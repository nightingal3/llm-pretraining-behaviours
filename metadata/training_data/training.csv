activation,attention_variant,batch_instances,batch_tokens,biases,block_type,deep_key,dimension,id,layer_norm_type,mlp_ratio,num_heads,positional_embeddings,sequence_length,weight_tying
swiglu,gqa,1024,4000000,none,sequential,0,8192,meta-llama/Llama-2-70b-hf,rmsnorm,2.666667,64,rope,4096,False
swiglu,full,1024,4000000,attn_only,sequential,0,4096,Qwen/Qwen-7B,rmsnorm,2.666667,32,rope,4096,False
swiglu,full,2048,4000000,ln_only,sequential,0,4096,openlm-research/open_llama_7b,parametric,2.666667,32,rope,2048,False
swiglu,gqa,0,0,none,sequential,0,4096,mistralai/Mixtral-8x7B-v0.1,rmsnorm,3.5,32,rope,4096,False
swiglu,full,1024,2000000,none,sequential,0,4096,EleutherAI/pythia-6.9b,parametric,2.666667,32,rope,2048,False
swiglu,gqa,1024,4000000,none,sequential,0,4096,meta-llama/Llama-2-7b,rmsnorm,2.666667,32,rope,4096,False
swiglu,full,2160,4000000,none,sequential,0,4096,allenai/OLMo-7B,non-paramteric,2.666667,32,rope,2048,False
gelu,mqa,2304,4000000,ln_only,parallel,0,4544,tiiuae/falcon-7b,parametric,4,71,rope,2048,False
0,0,0,0,0,0,0,0,openai-community/gpt2,0,0,0,0,0,0
swiglu,full,1024,4000000,none,sequential,0,4096,huggyllama/llama-7b,rmsnorm,2.666667,32,rope,2048,False
swiglu,gqa,1024,4000000,none,sequential,0,8192,meta-llama/Llama-2-70b-chat-hf,rmsnorm,2.666667,64,rope,4096,False
swiglu,gqa,0,0,none,sequential,0,4096,mistralai/Mistral-7B-v0.1,rmsnorm,3.5,32,rope,4096,False
