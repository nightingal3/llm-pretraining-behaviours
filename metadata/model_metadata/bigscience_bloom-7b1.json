{
    "id": "bigscience/bloom-7b1",
    "author": "bigscience",
    "sha": "6232703e399354503377bf59dfbb8397fd569e4a",
    "created_at": "2022-05-19T11:53:18+00:00",
    "last_modified": "2024-01-02T18:32:24+00:00",
    "private": false,
    "gated": false,
    "disabled": false,
    "library_name": "transformers",
    "tags": [
        "transformers",
        "pytorch",
        "jax",
        "safetensors",
        "bloom",
        "text-generation",
        "ak",
        "ar",
        "as",
        "bm",
        "bn",
        "ca",
        "code",
        "en",
        "es",
        "eu",
        "fon",
        "fr",
        "gu",
        "hi",
        "id",
        "ig",
        "ki",
        "kn",
        "lg",
        "ln",
        "ml",
        "mr",
        "ne",
        "nso",
        "ny",
        "or",
        "pa",
        "pt",
        "rn",
        "rw",
        "sn",
        "st",
        "sw",
        "ta",
        "te",
        "tn",
        "ts",
        "tum",
        "tw",
        "ur",
        "vi",
        "wo",
        "xh",
        "yo",
        "zh",
        "zhs",
        "zht",
        "zu",
        "arxiv:1909.08053",
        "arxiv:2110.02861",
        "arxiv:2108.12409",
        "license:bigscience-bloom-rail-1.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "text-generation-inference",
        "region:us"
    ],
    "pipeline_tag": "text-generation",
    "mask_token": null,
    "card_data": {},
    "model_index": null,
    "config": {
        "architectures": [
            "BloomForCausalLM"
        ],
        "model_type": "bloom",
        "tokenizer_config": {
            "unk_token": "<unk>",
            "eos_token": "</s>",
            "bos_token": "<s>",
            "pad_token": "<pad>"
        }
    },
    "transformers_info": {
        "auto_model": "AutoModelForCausalLM",
        "custom_class": null,
        "pipeline_tag": "text-generation",
        "processor": "AutoTokenizer"
    },
    "safetensors": {
        "parameters": {
            "F16": 7069016064
        },
        "total": 7069016064
    },
    "dimension": 4096,
    "weight_tying": false,
    "vocab_size": 250880
}