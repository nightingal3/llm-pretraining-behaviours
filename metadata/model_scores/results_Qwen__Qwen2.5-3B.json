{
    "model_name": "Qwen__Qwen2.5-3B",
    "last_updated": "2024-12-04 11:22:42.423236",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.8304093567251462,
                    "acc_stderr": 0.02878210810540171,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.46825396825396826,
                    "acc_stderr": 0.04463112720677173,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.02409347123262133,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.2636871508379888,
                    "acc_stderr": 0.01473692638376197,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.8438818565400844,
                    "acc_stderr": 0.02362715946031869,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.7196531791907514,
                    "acc_stderr": 0.024182427496577615,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.4791395045632334,
                    "acc_stderr": 0.012759117066518019,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.7791411042944786,
                    "acc_stderr": 0.03259177392742178,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.8431372549019608,
                    "acc_stderr": 0.02552472232455335,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.7170418006430869,
                    "acc_stderr": 0.025583062489984834,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.7685185185185185,
                    "acc_stderr": 0.04077494709252626,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.768595041322314,
                    "acc_stderr": 0.03849856098794088,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.7757575757575758,
                    "acc_stderr": 0.032568666616811015,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.8549222797927462,
                    "acc_stderr": 0.025416343096306422,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.7815126050420168,
                    "acc_stderr": 0.02684151432295893,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.8080808080808081,
                    "acc_stderr": 0.02805779167298901,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.8587155963302753,
                    "acc_stderr": 0.01493386898702808,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.7181818181818181,
                    "acc_stderr": 0.04309118709946458,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.84,
                    "acc_stderr": 0.036845294917747094,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.835820895522388,
                    "acc_stderr": 0.026193923544454115,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.7076923076923077,
                    "acc_stderr": 0.02306043838085775,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.7346938775510204,
                    "acc_stderr": 0.028263889943784603,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.6977124183006536,
                    "acc_stderr": 0.018579232711113895,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.7709923664122137,
                    "acc_stderr": 0.036853466317118506,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.5614035087719298,
                    "acc_stderr": 0.04668000738510455,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.8007662835249042,
                    "acc_stderr": 0.014283378044296413,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.8974358974358975,
                    "acc_stderr": 0.019875655027867447,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.7961165048543689,
                    "acc_stderr": 0.03989139859531772,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.7516339869281046,
                    "acc_stderr": 0.024739981355113592,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.04020151261036845,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.7040358744394619,
                    "acc_stderr": 0.030636591348699813,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.6580882352941176,
                    "acc_stderr": 0.02881472242225418,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.6820809248554913,
                    "acc_stderr": 0.035506839891655796,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.71,
                    "acc_stderr": 0.045604802157206845,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.7245283018867924,
                    "acc_stderr": 0.027495663683724046,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001974,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.4819277108433735,
                    "acc_stderr": 0.038899512528272166,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.4929078014184397,
                    "acc_stderr": 0.02982449855912901,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.49019607843137253,
                    "acc_stderr": 0.04974229460422817,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.45695364238410596,
                    "acc_stderr": 0.04067325174247443,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.8225806451612904,
                    "acc_stderr": 0.021732540689329272,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.7638888888888888,
                    "acc_stderr": 0.03551446610810826,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.6296296296296297,
                    "acc_stderr": 0.04171654161354544,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.04020151261036846,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.6,
                    "acc_stderr": 0.04923659639173309,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.743421052631579,
                    "acc_stderr": 0.035541803680256896,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.44,
                    "acc_stderr": 0.04988876515698589,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.6425531914893617,
                    "acc_stderr": 0.031329417894764254,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.048783173121456316,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.76,
                    "acc_stderr": 0.042923469599092816,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.4642857142857143,
                    "acc_stderr": 0.04733667890053756,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.6059113300492611,
                    "acc_stderr": 0.03438157967036545,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.625,
                    "acc_stderr": 0.033016908987210894,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.5978835978835979,
                    "acc_stderr": 0.02525303255499769,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.7034482758620689,
                    "acc_stderr": 0.03806142687309992,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.030485538042484616,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.5332764505119454,
                    "acc_stderr": 0.014578995859605799,
                    "acc_norm": 0.5750853242320819,
                    "acc_norm_stderr": 0.014445698968520767,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5538737303326031,
                    "acc_stderr": 0.004960732382255246,
                    "acc_norm": 0.7460665206134236,
                    "acc_norm_stderr": 0.004343704512380098,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.4893490828530172,
                    "acc_stderr": 0.014899604966354063,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 17.723506876309774,
                    "bleu_max_stderr": 0.6221890551789969,
                    "bleu_acc": 0.36474908200734396,
                    "bleu_acc_stderr": 0.016850961061720127,
                    "bleu_diff": -2.7917053834970753,
                    "bleu_diff_stderr": 0.5719675325277486,
                    "rouge1_max": 42.124187039438695,
                    "rouge1_max_stderr": 0.7609882906246943,
                    "rouge1_acc": 0.37209302325581395,
                    "rouge1_acc_stderr": 0.016921090118814035,
                    "rouge1_diff": -4.2070463723170395,
                    "rouge1_diff_stderr": 0.7335338436606067,
                    "rouge2_max": 28.977364786765644,
                    "rouge2_max_stderr": 0.8202368269256133,
                    "rouge2_acc": 0.32313341493268055,
                    "rouge2_acc_stderr": 0.016371836286454604,
                    "rouge2_diff": -5.148974861232573,
                    "rouge2_diff_stderr": 0.8363241355197695,
                    "rougeL_max": 39.37523397372098,
                    "rougeL_max_stderr": 0.7596826090671275,
                    "rougeL_acc": 0.3537331701346389,
                    "rougeL_acc_stderr": 0.01673781435884615,
                    "rougeL_diff": -4.5716762036035945,
                    "rougeL_diff_stderr": 0.7325385085991697,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.31701346389228885,
                    "acc_stderr": 0.016289203374403392,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7087608524072613,
                    "acc_stderr": 0.012769029305370702,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.7467778620166793,
                    "acc_stderr": 0.011978125194299678,
                    "timestamp": "2024-11-22T21-40-10.451010"
                }
            }
        }
    }
}