{
    "model_name": "Qwen__Qwen2.5-3B",
    "last_updated": "2024-12-19 13:38:01.135264",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.8187134502923976,
                "acc_stderr": 0.029547741687640038,
                "brier_score": 0.24563834735905882,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_formal_logic": {
                "acc": 0.49206349206349204,
                "acc_stderr": 0.044715725362943486,
                "brier_score": 0.6296355052360546,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_prehistory": {
                "acc": 0.7253086419753086,
                "acc_stderr": 0.024836057868294677,
                "brier_score": 0.3801587127112863,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2435754189944134,
                "acc_stderr": 0.01435591196476786,
                "brier_score": 0.962622291102738,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.8227848101265823,
                "acc_stderr": 0.02485636418450323,
                "brier_score": 0.24574863982281853,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_moral_disputes": {
                "acc": 0.6763005780346821,
                "acc_stderr": 0.025190181327608408,
                "brier_score": 0.4249982858165409,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_professional_law": {
                "acc": 0.4869621903520209,
                "acc_stderr": 0.012765893883835332,
                "brier_score": 0.6361156940456084,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.7914110429447853,
                "acc_stderr": 0.03192193448934725,
                "brier_score": 0.29033540862233154,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.8333333333333334,
                "acc_stderr": 0.026156867523931055,
                "brier_score": 0.2500351675424673,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_philosophy": {
                "acc": 0.7138263665594855,
                "acc_stderr": 0.025670259242188947,
                "brier_score": 0.3933299118993118,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_jurisprudence": {
                "acc": 0.8148148148148148,
                "acc_stderr": 0.03755265865037181,
                "brier_score": 0.27522692177544283,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_international_law": {
                "acc": 0.8264462809917356,
                "acc_stderr": 0.03457272836917669,
                "brier_score": 0.27296504783771897,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.7636363636363637,
                "acc_stderr": 0.033175059300091805,
                "brier_score": 0.33472155884729393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.8601036269430051,
                "acc_stderr": 0.025033870583015178,
                "brier_score": 0.19549088843628196,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.8067226890756303,
                "acc_stderr": 0.025649470265889193,
                "brier_score": 0.3036142514551187,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_geography": {
                "acc": 0.7929292929292929,
                "acc_stderr": 0.028869778460267066,
                "brier_score": 0.28880779032422577,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.8477064220183487,
                "acc_stderr": 0.015405084393157069,
                "brier_score": 0.20033463599748524,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_public_relations": {
                "acc": 0.7,
                "acc_stderr": 0.04389311454644287,
                "brier_score": 0.4009798205516975,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.83,
                "acc_stderr": 0.0377525168068637,
                "brier_score": 0.23176869938521993,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_sociology": {
                "acc": 0.8258706467661692,
                "acc_stderr": 0.026814951200421603,
                "brier_score": 0.2593726027414835,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.6820512820512821,
                "acc_stderr": 0.023610884308927865,
                "brier_score": 0.39070268561227034,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_security_studies": {
                "acc": 0.7306122448979592,
                "acc_stderr": 0.02840125202902294,
                "brier_score": 0.3572075223141421,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_professional_psychology": {
                "acc": 0.7124183006535948,
                "acc_stderr": 0.018311653053648222,
                "brier_score": 0.40299102758795385,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_human_sexuality": {
                "acc": 0.7480916030534351,
                "acc_stderr": 0.038073871163060866,
                "brier_score": 0.33656144859824927,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_econometrics": {
                "acc": 0.5263157894736842,
                "acc_stderr": 0.046970851366478626,
                "brier_score": 0.6314189534004948,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_miscellaneous": {
                "acc": 0.7994891443167306,
                "acc_stderr": 0.014317653708594209,
                "brier_score": 0.2675858404124839,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_marketing": {
                "acc": 0.8846153846153846,
                "acc_stderr": 0.02093019318517933,
                "brier_score": 0.17685456520590553,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_management": {
                "acc": 0.8155339805825242,
                "acc_stderr": 0.03840423627288276,
                "brier_score": 0.22934336958888674,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_nutrition": {
                "acc": 0.761437908496732,
                "acc_stderr": 0.024404394928087866,
                "brier_score": 0.3327031238651498,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_medical_genetics": {
                "acc": 0.73,
                "acc_stderr": 0.04461960433384741,
                "brier_score": 0.3168223886209323,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_human_aging": {
                "acc": 0.7399103139013453,
                "acc_stderr": 0.029442495585857473,
                "brier_score": 0.4084280177407385,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_professional_medicine": {
                "acc": 0.6580882352941176,
                "acc_stderr": 0.02881472242225418,
                "brier_score": 0.47004532244711683,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_college_medicine": {
                "acc": 0.6589595375722543,
                "acc_stderr": 0.036146654241808254,
                "brier_score": 0.4127036479388521,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_business_ethics": {
                "acc": 0.72,
                "acc_stderr": 0.04512608598542129,
                "brier_score": 0.3309723529677748,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.7320754716981132,
                "acc_stderr": 0.027257260322494845,
                "brier_score": 0.3467972625679778,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_global_facts": {
                "acc": 0.36,
                "acc_stderr": 0.04824181513244218,
                "brier_score": 0.7108003167393311,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_virology": {
                "acc": 0.5060240963855421,
                "acc_stderr": 0.03892212195333045,
                "brier_score": 0.7280065109273901,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_professional_accounting": {
                "acc": 0.5141843971631206,
                "acc_stderr": 0.02981549448368206,
                "brier_score": 0.5810576259817505,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_college_physics": {
                "acc": 0.5098039215686274,
                "acc_stderr": 0.04974229460422817,
                "brier_score": 0.5644665416176843,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_physics": {
                "acc": 0.41721854304635764,
                "acc_stderr": 0.040261414976346104,
                "brier_score": 0.6875609768063202,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_biology": {
                "acc": 0.8096774193548387,
                "acc_stderr": 0.022331707611823067,
                "brier_score": 0.26083194119588576,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_college_biology": {
                "acc": 0.7013888888888888,
                "acc_stderr": 0.03827052357950756,
                "brier_score": 0.33389341524347393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_anatomy": {
                "acc": 0.6222222222222222,
                "acc_stderr": 0.04188307537595853,
                "brier_score": 0.48947933132471644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_college_chemistry": {
                "acc": 0.49,
                "acc_stderr": 0.05024183937956913,
                "brier_score": 0.6023101953688079,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_computer_security": {
                "acc": 0.77,
                "acc_stderr": 0.04229525846816505,
                "brier_score": 0.3091005241781668,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_college_computer_science": {
                "acc": 0.58,
                "acc_stderr": 0.049604496374885836,
                "brier_score": 0.5087894407032691,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_astronomy": {
                "acc": 0.6842105263157895,
                "acc_stderr": 0.037827289808654685,
                "brier_score": 0.3960447226399447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_college_mathematics": {
                "acc": 0.4,
                "acc_stderr": 0.049236596391733084,
                "brier_score": 0.6804165912352971,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.676595744680851,
                "acc_stderr": 0.030579442773610344,
                "brier_score": 0.4625355830633662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.5,
                "acc_stderr": 0.050251890762960605,
                "brier_score": 0.6589979467745916,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.74,
                "acc_stderr": 0.04408440022768078,
                "brier_score": 0.3271117286082809,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_machine_learning": {
                "acc": 0.45535714285714285,
                "acc_stderr": 0.04726835553719099,
                "brier_score": 0.6080303321956573,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.5862068965517241,
                "acc_stderr": 0.03465304488406796,
                "brier_score": 0.5089021864446602,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.6111111111111112,
                "acc_stderr": 0.033247089118091176,
                "brier_score": 0.5083961832308744,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.5740740740740741,
                "acc_stderr": 0.025467149045469543,
                "brier_score": 0.4992870946014208,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.6413793103448275,
                "acc_stderr": 0.03996629574876718,
                "brier_score": 0.46942897274818185,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.5185185185185185,
                "acc_stderr": 0.03046462171889531,
                "brier_score": 0.6094137662078477,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-00-32.590210"
            },
            "arc_challenge": {
                "acc": 0.5332764505119454,
                "acc_stderr": 0.014578995859605799,
                "acc_norm": 0.5750853242320819,
                "acc_norm_stderr": 0.014445698968520767,
                "timestamp": "2024-11-22T21-40-10.451010"
            },
            "hellaswag": {
                "acc": 0.5538737303326031,
                "acc_stderr": 0.004960732382255246,
                "acc_norm": 0.7460665206134236,
                "acc_norm_stderr": 0.004343704512380098,
                "timestamp": "2024-11-22T21-40-10.451010"
            },
            "truthfulqa_mc2": {
                "acc": 0.4893490828530172,
                "acc_stderr": 0.014899604966354063,
                "timestamp": "2024-11-22T21-40-10.451010"
            },
            "truthfulqa_gen": {
                "bleu_max": 17.723506876309774,
                "bleu_max_stderr": 0.6221890551789969,
                "bleu_acc": 0.36474908200734396,
                "bleu_acc_stderr": 0.016850961061720127,
                "bleu_diff": -2.7917053834970753,
                "bleu_diff_stderr": 0.5719675325277486,
                "rouge1_max": 42.124187039438695,
                "rouge1_max_stderr": 0.7609882906246943,
                "rouge1_acc": 0.37209302325581395,
                "rouge1_acc_stderr": 0.016921090118814035,
                "rouge1_diff": -4.2070463723170395,
                "rouge1_diff_stderr": 0.7335338436606067,
                "rouge2_max": 28.977364786765644,
                "rouge2_max_stderr": 0.8202368269256133,
                "rouge2_acc": 0.32313341493268055,
                "rouge2_acc_stderr": 0.016371836286454604,
                "rouge2_diff": -5.148974861232573,
                "rouge2_diff_stderr": 0.8363241355197695,
                "rougeL_max": 39.37523397372098,
                "rougeL_max_stderr": 0.7596826090671275,
                "rougeL_acc": 0.3537331701346389,
                "rougeL_acc_stderr": 0.01673781435884615,
                "rougeL_diff": -4.5716762036035945,
                "rougeL_diff_stderr": 0.7325385085991697,
                "timestamp": "2024-11-22T21-40-10.451010"
            },
            "truthfulqa_mc1": {
                "acc": 0.31701346389228885,
                "acc_stderr": 0.016289203374403392,
                "timestamp": "2024-11-22T21-40-10.451010"
            },
            "winogrande": {
                "acc": 0.7087608524072613,
                "acc_stderr": 0.012769029305370702,
                "timestamp": "2024-11-22T21-40-10.451010"
            },
            "gsm8k": {
                "exact_match": 0.7467778620166793,
                "exact_match_stderr": 0.011978125194299678,
                "timestamp": "2024-11-22T21-40-10.451010"
            }
        }
    }
}