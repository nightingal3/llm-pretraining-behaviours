{
    "model_name": "EleutherAI/pythia-12b-deduped",
    "last_updated": "2023-07-19",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "acc": 0.0008389261744966443,
                    "acc_stderr": 0.0002964962989801232,
                    "f1": 0.04548238255033574,
                    "f1_stderr": 0.0011460514648967963,
                    "timestamp": "2023-10-21T20-55-12.299775"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.014404852160727824,
                    "acc_stderr": 0.003282055917136946,
                    "timestamp": "2023-10-21T20-55-12.299775"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.664561957379637,
                    "acc_stderr": 0.013269575904851425,
                    "timestamp": "2023-10-21T20-55-12.299775"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.378839590443686,
                    "acc_stderr": 0.01417591549000032,
                    "acc_norm": 0.4138225255972696,
                    "acc_norm_stderr": 0.014392730009221009,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5183230432184823,
                    "acc_stderr": 0.0049864298081467705,
                    "acc_norm": 0.7026488747261501,
                    "acc_norm_stderr": 0.004561582009834582,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.04560480215720684,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.04560480215720684,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.22962962962962963,
                    "acc_stderr": 0.03633384414073464,
                    "acc_norm": 0.22962962962962963,
                    "acc_norm_stderr": 0.03633384414073464,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.03583496176361063,
                    "acc_norm": 0.2631578947368421,
                    "acc_norm_stderr": 0.03583496176361063,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036846,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036846,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.26037735849056604,
                    "acc_stderr": 0.027008766090708104,
                    "acc_norm": 0.26037735849056604,
                    "acc_norm_stderr": 0.027008766090708104,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2638888888888889,
                    "acc_stderr": 0.03685651095897532,
                    "acc_norm": 0.2638888888888889,
                    "acc_norm_stderr": 0.03685651095897532,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.03942772444036625,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.03942772444036625,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.0416333199893227,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.0416333199893227,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542128,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542128,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2254335260115607,
                    "acc_stderr": 0.03186209851641143,
                    "acc_norm": 0.2254335260115607,
                    "acc_norm_stderr": 0.03186209851641143,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.18627450980392157,
                    "acc_stderr": 0.03873958714149351,
                    "acc_norm": 0.18627450980392157,
                    "acc_norm_stderr": 0.03873958714149351,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2680851063829787,
                    "acc_stderr": 0.028957342788342347,
                    "acc_norm": 0.2680851063829787,
                    "acc_norm_stderr": 0.028957342788342347,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.041424397194893596,
                    "acc_norm": 0.2631578947368421,
                    "acc_norm_stderr": 0.041424397194893596,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.22758620689655173,
                    "acc_stderr": 0.03493950380131183,
                    "acc_norm": 0.22758620689655173,
                    "acc_norm_stderr": 0.03493950380131183,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.26455026455026454,
                    "acc_stderr": 0.022717467897708614,
                    "acc_norm": 0.26455026455026454,
                    "acc_norm_stderr": 0.022717467897708614,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.19047619047619047,
                    "acc_stderr": 0.03512207412302054,
                    "acc_norm": 0.19047619047619047,
                    "acc_norm_stderr": 0.03512207412302054,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036846,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036846,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.23870967741935484,
                    "acc_stderr": 0.024251071262208834,
                    "acc_norm": 0.23870967741935484,
                    "acc_norm_stderr": 0.024251071262208834,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2660098522167488,
                    "acc_stderr": 0.031089826002937523,
                    "acc_norm": 0.2660098522167488,
                    "acc_norm_stderr": 0.031089826002937523,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.045126085985421276,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.045126085985421276,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.23636363636363636,
                    "acc_stderr": 0.03317505930009181,
                    "acc_norm": 0.23636363636363636,
                    "acc_norm_stderr": 0.03317505930009181,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.029620227874790486,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.029620227874790486,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.20725388601036268,
                    "acc_stderr": 0.02925282329180363,
                    "acc_norm": 0.20725388601036268,
                    "acc_norm_stderr": 0.02925282329180363,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.258974358974359,
                    "acc_stderr": 0.022211106810061665,
                    "acc_norm": 0.258974358974359,
                    "acc_norm_stderr": 0.022211106810061665,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2740740740740741,
                    "acc_stderr": 0.02719593480408562,
                    "acc_norm": 0.2740740740740741,
                    "acc_norm_stderr": 0.02719593480408562,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2815126050420168,
                    "acc_stderr": 0.02921354941437217,
                    "acc_norm": 0.2815126050420168,
                    "acc_norm_stderr": 0.02921354941437217,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2251655629139073,
                    "acc_stderr": 0.03410435282008937,
                    "acc_norm": 0.2251655629139073,
                    "acc_norm_stderr": 0.03410435282008937,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.2036697247706422,
                    "acc_stderr": 0.01726674208763079,
                    "acc_norm": 0.2036697247706422,
                    "acc_norm_stderr": 0.01726674208763079,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.16666666666666666,
                    "acc_stderr": 0.025416428388767478,
                    "acc_norm": 0.16666666666666666,
                    "acc_norm_stderr": 0.025416428388767478,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.24509803921568626,
                    "acc_stderr": 0.03019028245350195,
                    "acc_norm": 0.24509803921568626,
                    "acc_norm_stderr": 0.03019028245350195,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.2742616033755274,
                    "acc_stderr": 0.029041333510598035,
                    "acc_norm": 0.2742616033755274,
                    "acc_norm_stderr": 0.029041333510598035,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.36771300448430494,
                    "acc_stderr": 0.03236198350928275,
                    "acc_norm": 0.36771300448430494,
                    "acc_norm_stderr": 0.03236198350928275,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2595419847328244,
                    "acc_stderr": 0.03844876139785271,
                    "acc_norm": 0.2595419847328244,
                    "acc_norm_stderr": 0.03844876139785271,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.4628099173553719,
                    "acc_stderr": 0.04551711196104218,
                    "acc_norm": 0.4628099173553719,
                    "acc_norm_stderr": 0.04551711196104218,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2962962962962963,
                    "acc_stderr": 0.044143436668549335,
                    "acc_norm": 0.2962962962962963,
                    "acc_norm_stderr": 0.044143436668549335,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2331288343558282,
                    "acc_stderr": 0.03322015795776741,
                    "acc_norm": 0.2331288343558282,
                    "acc_norm_stderr": 0.03322015795776741,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04109974682633932,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04109974682633932,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.22330097087378642,
                    "acc_stderr": 0.04123553189891431,
                    "acc_norm": 0.22330097087378642,
                    "acc_norm_stderr": 0.04123553189891431,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.26495726495726496,
                    "acc_stderr": 0.028911208802749472,
                    "acc_norm": 0.26495726495726496,
                    "acc_norm_stderr": 0.028911208802749472,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.04093601807403326,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.04093601807403326,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.29246487867177523,
                    "acc_stderr": 0.016267000684598642,
                    "acc_norm": 0.29246487867177523,
                    "acc_norm_stderr": 0.016267000684598642,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.3092485549132948,
                    "acc_stderr": 0.024883140570071755,
                    "acc_norm": 0.3092485549132948,
                    "acc_norm_stderr": 0.024883140570071755,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2435754189944134,
                    "acc_stderr": 0.01435591196476786,
                    "acc_norm": 0.2435754189944134,
                    "acc_norm_stderr": 0.01435591196476786,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2973856209150327,
                    "acc_stderr": 0.02617390850671858,
                    "acc_norm": 0.2973856209150327,
                    "acc_norm_stderr": 0.02617390850671858,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.27009646302250806,
                    "acc_stderr": 0.025218040373410616,
                    "acc_norm": 0.27009646302250806,
                    "acc_norm_stderr": 0.025218040373410616,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.024383665531035457,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.024383665531035457,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.24822695035460993,
                    "acc_stderr": 0.025770015644290382,
                    "acc_norm": 0.24822695035460993,
                    "acc_norm_stderr": 0.025770015644290382,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.26140808344198174,
                    "acc_stderr": 0.011222528169771314,
                    "acc_norm": 0.26140808344198174,
                    "acc_norm_stderr": 0.011222528169771314,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.19852941176470587,
                    "acc_stderr": 0.024231013370541104,
                    "acc_norm": 0.19852941176470587,
                    "acc_norm_stderr": 0.024231013370541104,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.29248366013071897,
                    "acc_stderr": 0.01840341571010979,
                    "acc_norm": 0.29248366013071897,
                    "acc_norm_stderr": 0.01840341571010979,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.04172343038705383,
                    "acc_norm": 0.2545454545454545,
                    "acc_norm_stderr": 0.04172343038705383,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.2530612244897959,
                    "acc_stderr": 0.02783302387139968,
                    "acc_norm": 0.2530612244897959,
                    "acc_norm_stderr": 0.02783302387139968,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.26865671641791045,
                    "acc_stderr": 0.03134328358208954,
                    "acc_norm": 0.26865671641791045,
                    "acc_norm_stderr": 0.03134328358208954,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3192771084337349,
                    "acc_stderr": 0.0362933532994786,
                    "acc_norm": 0.3192771084337349,
                    "acc_norm_stderr": 0.0362933532994786,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.3508771929824561,
                    "acc_stderr": 0.03660298834049162,
                    "acc_norm": 0.3508771929824561,
                    "acc_norm_stderr": 0.03660298834049162,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.20930232558139536,
                    "mc1_stderr": 0.014241219434785828,
                    "mc2": 0.329988729051366,
                    "mc2_stderr": 0.013122098603854236,
                    "timestamp": "2023-07-19T18-15-42.026882"
                }
            }
        }
    }
}