{
    "model_name": "jb723/cross_lingual_epoch2",
    "last_updated": "2023-11-23",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.3293515358361775,
                    "acc_stderr": 0.013734057652635474,
                    "acc_norm": 0.3924914675767918,
                    "acc_norm_stderr": 0.014269634635670714,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.3392750448117905,
                    "acc_stderr": 0.004724956665879986,
                    "acc_norm": 0.47918741286596295,
                    "acc_norm_stderr": 0.004985456752161,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542128,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542128,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.4148148148148148,
                    "acc_stderr": 0.04256193767901407,
                    "acc_norm": 0.4148148148148148,
                    "acc_norm_stderr": 0.04256193767901407,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.3026315789473684,
                    "acc_stderr": 0.037385206761196686,
                    "acc_norm": 0.3026315789473684,
                    "acc_norm_stderr": 0.037385206761196686,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.04923659639173309,
                    "acc_norm": 0.4,
                    "acc_norm_stderr": 0.04923659639173309,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.4339622641509434,
                    "acc_stderr": 0.030503292013342596,
                    "acc_norm": 0.4339622641509434,
                    "acc_norm_stderr": 0.030503292013342596,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.3125,
                    "acc_stderr": 0.038760854559127644,
                    "acc_norm": 0.3125,
                    "acc_norm_stderr": 0.038760854559127644,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536955,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.038612291966536955,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621504,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621504,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.3583815028901734,
                    "acc_stderr": 0.03656343653353158,
                    "acc_norm": 0.3583815028901734,
                    "acc_norm_stderr": 0.03656343653353158,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.04023382273617747,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.04023382273617747,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.05021167315686781,
                    "acc_norm": 0.48,
                    "acc_norm_stderr": 0.05021167315686781,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3148936170212766,
                    "acc_stderr": 0.030363582197238167,
                    "acc_norm": 0.3148936170212766,
                    "acc_norm_stderr": 0.030363582197238167,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2543859649122807,
                    "acc_stderr": 0.04096985139843671,
                    "acc_norm": 0.2543859649122807,
                    "acc_norm_stderr": 0.04096985139843671,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.38620689655172413,
                    "acc_stderr": 0.04057324734419035,
                    "acc_norm": 0.38620689655172413,
                    "acc_norm_stderr": 0.04057324734419035,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2566137566137566,
                    "acc_stderr": 0.022494510767503154,
                    "acc_norm": 0.2566137566137566,
                    "acc_norm_stderr": 0.022494510767503154,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.30158730158730157,
                    "acc_stderr": 0.04104947269903394,
                    "acc_norm": 0.30158730158730157,
                    "acc_norm_stderr": 0.04104947269903394,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206824,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206824,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.4032258064516129,
                    "acc_stderr": 0.027906150826041143,
                    "acc_norm": 0.4032258064516129,
                    "acc_norm_stderr": 0.027906150826041143,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.24630541871921183,
                    "acc_stderr": 0.03031509928561773,
                    "acc_norm": 0.24630541871921183,
                    "acc_norm_stderr": 0.03031509928561773,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.0479372485441102,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.0479372485441102,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.3696969696969697,
                    "acc_stderr": 0.03769430314512568,
                    "acc_norm": 0.3696969696969697,
                    "acc_norm_stderr": 0.03769430314512568,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.3787878787878788,
                    "acc_stderr": 0.03456088731993747,
                    "acc_norm": 0.3787878787878788,
                    "acc_norm_stderr": 0.03456088731993747,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.49222797927461137,
                    "acc_stderr": 0.03608003225569653,
                    "acc_norm": 0.49222797927461137,
                    "acc_norm_stderr": 0.03608003225569653,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2923076923076923,
                    "acc_stderr": 0.023060438380857744,
                    "acc_norm": 0.2923076923076923,
                    "acc_norm_stderr": 0.023060438380857744,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2074074074074074,
                    "acc_stderr": 0.02472071319395216,
                    "acc_norm": 0.2074074074074074,
                    "acc_norm_stderr": 0.02472071319395216,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.36134453781512604,
                    "acc_stderr": 0.031204691225150023,
                    "acc_norm": 0.36134453781512604,
                    "acc_norm_stderr": 0.031204691225150023,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.23841059602649006,
                    "acc_stderr": 0.0347918557259966,
                    "acc_norm": 0.23841059602649006,
                    "acc_norm_stderr": 0.0347918557259966,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.43853211009174314,
                    "acc_stderr": 0.021274713073954572,
                    "acc_norm": 0.43853211009174314,
                    "acc_norm_stderr": 0.021274713073954572,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.18981481481481483,
                    "acc_stderr": 0.026744714834691926,
                    "acc_norm": 0.18981481481481483,
                    "acc_norm_stderr": 0.026744714834691926,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.37745098039215685,
                    "acc_stderr": 0.03402272044340703,
                    "acc_norm": 0.37745098039215685,
                    "acc_norm_stderr": 0.03402272044340703,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.4388185654008439,
                    "acc_stderr": 0.032302649315470375,
                    "acc_norm": 0.4388185654008439,
                    "acc_norm_stderr": 0.032302649315470375,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.49327354260089684,
                    "acc_stderr": 0.03355476596234354,
                    "acc_norm": 0.49327354260089684,
                    "acc_norm_stderr": 0.03355476596234354,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.40458015267175573,
                    "acc_stderr": 0.043046937953806645,
                    "acc_norm": 0.40458015267175573,
                    "acc_norm_stderr": 0.043046937953806645,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.5206611570247934,
                    "acc_stderr": 0.04560456086387235,
                    "acc_norm": 0.5206611570247934,
                    "acc_norm_stderr": 0.04560456086387235,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.4444444444444444,
                    "acc_stderr": 0.04803752235190193,
                    "acc_norm": 0.4444444444444444,
                    "acc_norm_stderr": 0.04803752235190193,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.3558282208588957,
                    "acc_stderr": 0.03761521380046734,
                    "acc_norm": 0.3558282208588957,
                    "acc_norm_stderr": 0.03761521380046734,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.3125,
                    "acc_stderr": 0.043994650575715215,
                    "acc_norm": 0.3125,
                    "acc_norm_stderr": 0.043994650575715215,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.36893203883495146,
                    "acc_stderr": 0.0477761518115674,
                    "acc_norm": 0.36893203883495146,
                    "acc_norm_stderr": 0.0477761518115674,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.6196581196581197,
                    "acc_stderr": 0.031804252043840985,
                    "acc_norm": 0.6196581196581197,
                    "acc_norm_stderr": 0.031804252043840985,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001975,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.04902071300001975,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.4725415070242657,
                    "acc_stderr": 0.017852981266633955,
                    "acc_norm": 0.4725415070242657,
                    "acc_norm_stderr": 0.017852981266633955,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.4046242774566474,
                    "acc_stderr": 0.02642481659400985,
                    "acc_norm": 0.4046242774566474,
                    "acc_norm_stderr": 0.02642481659400985,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.23798882681564246,
                    "acc_stderr": 0.014242630070574915,
                    "acc_norm": 0.23798882681564246,
                    "acc_norm_stderr": 0.014242630070574915,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.42810457516339867,
                    "acc_stderr": 0.028332397483664274,
                    "acc_norm": 0.42810457516339867,
                    "acc_norm_stderr": 0.028332397483664274,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.4758842443729904,
                    "acc_stderr": 0.028365041542564584,
                    "acc_norm": 0.4758842443729904,
                    "acc_norm_stderr": 0.028365041542564584,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.4537037037037037,
                    "acc_stderr": 0.027701228468542602,
                    "acc_norm": 0.4537037037037037,
                    "acc_norm_stderr": 0.027701228468542602,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2765957446808511,
                    "acc_stderr": 0.026684564340461,
                    "acc_norm": 0.2765957446808511,
                    "acc_norm_stderr": 0.026684564340461,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2985658409387223,
                    "acc_stderr": 0.011688060141794228,
                    "acc_norm": 0.2985658409387223,
                    "acc_norm_stderr": 0.011688060141794228,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.25735294117647056,
                    "acc_stderr": 0.026556519470041506,
                    "acc_norm": 0.25735294117647056,
                    "acc_norm_stderr": 0.026556519470041506,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.369281045751634,
                    "acc_stderr": 0.019524316744866356,
                    "acc_norm": 0.369281045751634,
                    "acc_norm_stderr": 0.019524316744866356,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.4636363636363636,
                    "acc_stderr": 0.04776449162396197,
                    "acc_norm": 0.4636363636363636,
                    "acc_norm_stderr": 0.04776449162396197,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.4122448979591837,
                    "acc_stderr": 0.03151236044674281,
                    "acc_norm": 0.4122448979591837,
                    "acc_norm_stderr": 0.03151236044674281,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.472636815920398,
                    "acc_stderr": 0.03530235517334682,
                    "acc_norm": 0.472636815920398,
                    "acc_norm_stderr": 0.03530235517334682,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.57,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.57,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3855421686746988,
                    "acc_stderr": 0.03789134424611548,
                    "acc_norm": 0.3855421686746988,
                    "acc_norm_stderr": 0.03789134424611548,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.43859649122807015,
                    "acc_stderr": 0.038057975055904594,
                    "acc_norm": 0.43859649122807015,
                    "acc_norm_stderr": 0.038057975055904594,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.23745410036719705,
                    "mc1_stderr": 0.01489627744104185,
                    "mc2": 0.4789867119861502,
                    "mc2_stderr": 0.016540775343672782,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.6211523283346487,
                    "acc_stderr": 0.013633724603180335,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.049601510067114093,
                    "em_stderr": 0.0022235145171999363,
                    "f1": 0.07294463087248305,
                    "f1_stderr": 0.002421427712218101,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.0,
                    "acc_stderr": 0.0,
                    "timestamp": "2023-11-23T20-42-48.019981"
                }
            }
        }
    }
}