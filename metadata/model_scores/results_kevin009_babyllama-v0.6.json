{
    "model_name": "kevin009/babyllama-v0.6",
    "last_updated": "2024-02-13",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.35238907849829354,
                    "acc_stderr": 0.013960142600598677,
                    "acc_norm": 0.3609215017064846,
                    "acc_norm_stderr": 0.014034761386175458,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.46335391356303524,
                    "acc_stderr": 0.004976361454341339,
                    "acc_norm": 0.6159131647082254,
                    "acc_norm_stderr": 0.0048538457503921415,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909284,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909284,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.17777777777777778,
                    "acc_stderr": 0.03302789859901717,
                    "acc_norm": 0.17777777777777778,
                    "acc_norm_stderr": 0.03302789859901717,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.17105263157894737,
                    "acc_stderr": 0.030643607071677077,
                    "acc_norm": 0.17105263157894737,
                    "acc_norm_stderr": 0.030643607071677077,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.042923469599092816,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2830188679245283,
                    "acc_stderr": 0.027724236492700904,
                    "acc_norm": 0.2830188679245283,
                    "acc_norm_stderr": 0.027724236492700904,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.24305555555555555,
                    "acc_stderr": 0.035868792800803406,
                    "acc_norm": 0.24305555555555555,
                    "acc_norm_stderr": 0.035868792800803406,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.0479372485441102,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.0479372485441102,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.19653179190751446,
                    "acc_stderr": 0.030299574664788147,
                    "acc_norm": 0.19653179190751446,
                    "acc_norm_stderr": 0.030299574664788147,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.19607843137254902,
                    "acc_stderr": 0.03950581861179961,
                    "acc_norm": 0.19607843137254902,
                    "acc_norm_stderr": 0.03950581861179961,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.28085106382978725,
                    "acc_stderr": 0.02937917046412482,
                    "acc_norm": 0.28085106382978725,
                    "acc_norm_stderr": 0.02937917046412482,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.041424397194893624,
                    "acc_norm": 0.2631578947368421,
                    "acc_norm_stderr": 0.041424397194893624,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.23448275862068965,
                    "acc_stderr": 0.035306258743465914,
                    "acc_norm": 0.23448275862068965,
                    "acc_norm_stderr": 0.035306258743465914,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.022569897074918417,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.022569897074918417,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.1984126984126984,
                    "acc_stderr": 0.03567016675276864,
                    "acc_norm": 0.1984126984126984,
                    "acc_norm_stderr": 0.03567016675276864,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.2645161290322581,
                    "acc_stderr": 0.025091892378859275,
                    "acc_norm": 0.2645161290322581,
                    "acc_norm_stderr": 0.025091892378859275,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2413793103448276,
                    "acc_stderr": 0.03010833071801162,
                    "acc_norm": 0.2413793103448276,
                    "acc_norm_stderr": 0.03010833071801162,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322695,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.041633319989322695,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.2909090909090909,
                    "acc_stderr": 0.03546563019624336,
                    "acc_norm": 0.2909090909090909,
                    "acc_norm_stderr": 0.03546563019624336,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.029620227874790486,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.029620227874790486,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.22279792746113988,
                    "acc_stderr": 0.03003114797764154,
                    "acc_norm": 0.22279792746113988,
                    "acc_norm_stderr": 0.03003114797764154,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.24615384615384617,
                    "acc_stderr": 0.021840866990423088,
                    "acc_norm": 0.24615384615384617,
                    "acc_norm_stderr": 0.021840866990423088,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.02696242432507384,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.02696242432507384,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.027553614467863818,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.027553614467863818,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2052980132450331,
                    "acc_stderr": 0.03297986648473836,
                    "acc_norm": 0.2052980132450331,
                    "acc_norm_stderr": 0.03297986648473836,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.24036697247706423,
                    "acc_stderr": 0.01832060732096407,
                    "acc_norm": 0.24036697247706423,
                    "acc_norm_stderr": 0.01832060732096407,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.38425925925925924,
                    "acc_stderr": 0.03317354514310742,
                    "acc_norm": 0.38425925925925924,
                    "acc_norm_stderr": 0.03317354514310742,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.24019607843137256,
                    "acc_stderr": 0.02998373305591362,
                    "acc_norm": 0.24019607843137256,
                    "acc_norm_stderr": 0.02998373305591362,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.25316455696202533,
                    "acc_stderr": 0.0283046579430353,
                    "acc_norm": 0.25316455696202533,
                    "acc_norm_stderr": 0.0283046579430353,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.3632286995515695,
                    "acc_stderr": 0.03227790442850499,
                    "acc_norm": 0.3632286995515695,
                    "acc_norm_stderr": 0.03227790442850499,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.037276735755969195,
                    "acc_norm": 0.2366412213740458,
                    "acc_norm_stderr": 0.037276735755969195,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.256198347107438,
                    "acc_stderr": 0.03984979653302871,
                    "acc_norm": 0.256198347107438,
                    "acc_norm_stderr": 0.03984979653302871,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.04236511258094633,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.04236511258094633,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.22085889570552147,
                    "acc_stderr": 0.032591773927421776,
                    "acc_norm": 0.22085889570552147,
                    "acc_norm_stderr": 0.032591773927421776,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.29464285714285715,
                    "acc_stderr": 0.04327040932578728,
                    "acc_norm": 0.29464285714285715,
                    "acc_norm_stderr": 0.04327040932578728,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2621359223300971,
                    "acc_stderr": 0.04354631077260597,
                    "acc_norm": 0.2621359223300971,
                    "acc_norm_stderr": 0.04354631077260597,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.28205128205128205,
                    "acc_stderr": 0.029480360549541194,
                    "acc_norm": 0.28205128205128205,
                    "acc_norm_stderr": 0.029480360549541194,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.04461960433384741,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.04461960433384741,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.280970625798212,
                    "acc_stderr": 0.01607312785122125,
                    "acc_norm": 0.280970625798212,
                    "acc_norm_stderr": 0.01607312785122125,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2254335260115607,
                    "acc_stderr": 0.022497230190967547,
                    "acc_norm": 0.2254335260115607,
                    "acc_norm_stderr": 0.022497230190967547,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.22681564245810057,
                    "acc_stderr": 0.014005843570897897,
                    "acc_norm": 0.22681564245810057,
                    "acc_norm_stderr": 0.014005843570897897,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.24183006535947713,
                    "acc_stderr": 0.024518195641879334,
                    "acc_norm": 0.24183006535947713,
                    "acc_norm_stderr": 0.024518195641879334,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.26688102893890675,
                    "acc_stderr": 0.02512263760881665,
                    "acc_norm": 0.26688102893890675,
                    "acc_norm_stderr": 0.02512263760881665,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2654320987654321,
                    "acc_stderr": 0.024569223600460845,
                    "acc_norm": 0.2654320987654321,
                    "acc_norm_stderr": 0.024569223600460845,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.25886524822695034,
                    "acc_stderr": 0.026129572527180848,
                    "acc_norm": 0.25886524822695034,
                    "acc_norm_stderr": 0.026129572527180848,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.23859191655801826,
                    "acc_stderr": 0.0108859297420022,
                    "acc_norm": 0.23859191655801826,
                    "acc_norm_stderr": 0.0108859297420022,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.20220588235294118,
                    "acc_stderr": 0.02439819298665492,
                    "acc_norm": 0.20220588235294118,
                    "acc_norm_stderr": 0.02439819298665492,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.26633986928104575,
                    "acc_stderr": 0.017883188134667192,
                    "acc_norm": 0.26633986928104575,
                    "acc_norm_stderr": 0.017883188134667192,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.3181818181818182,
                    "acc_stderr": 0.04461272175910507,
                    "acc_norm": 0.3181818181818182,
                    "acc_norm_stderr": 0.04461272175910507,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.17142857142857143,
                    "acc_stderr": 0.02412746346265015,
                    "acc_norm": 0.17142857142857143,
                    "acc_norm_stderr": 0.02412746346265015,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.23383084577114427,
                    "acc_stderr": 0.029929415408348384,
                    "acc_norm": 0.23383084577114427,
                    "acc_norm_stderr": 0.029929415408348384,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3192771084337349,
                    "acc_stderr": 0.03629335329947861,
                    "acc_norm": 0.3192771084337349,
                    "acc_norm_stderr": 0.03629335329947861,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.2807017543859649,
                    "acc_stderr": 0.034462962170884265,
                    "acc_norm": 0.2807017543859649,
                    "acc_norm_stderr": 0.034462962170884265,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.22031823745410037,
                    "mc1_stderr": 0.014509045171487295,
                    "mc2": 0.3584100057903431,
                    "mc2_stderr": 0.013776314892170112,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.6101026045777427,
                    "acc_stderr": 0.013707547317008463,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.01592115238817286,
                    "acc_stderr": 0.0034478192723890015,
                    "timestamp": "2024-02-13T10-06-30.565512"
                }
            }
        }
    }
}