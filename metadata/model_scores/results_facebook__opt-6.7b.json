{
    "model_name": "facebook/opt-6.7b",
    "last_updated": "2024-12-04 11:25:01.792655",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.003663003663003663,
                    "acc_stderr": 0.002587757368193444,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.010332950631458095,
                    "acc_stderr": 0.0034284443646836666,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.005555555555555556,
                    "acc_stderr": 0.0032015451273208887,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.004429678848283499,
                    "acc_stderr": 0.0022111531423787863,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.010438413361169102,
                    "acc_stderr": 0.004648627117184668,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.008438818565400843,
                    "acc_stderr": 0.004206007207713056,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.009267059814658803,
                    "acc_stderr": 0.002782319118488812,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.0015,
                    "acc_stderr": 0.0008655920660521483,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.002,
                    "acc_stderr": 0.0009992493430694893,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.0005,
                    "acc_stderr": 0.0005000000000000151,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.012,
                    "acc_stderr": 0.0024353573624298335,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.0205,
                    "acc_stderr": 0.00316936861988699,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.036,
                    "acc_stderr": 0.004166614973833173,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 0.0055,
                    "acc_stderr": 0.0016541593398342208,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.02880970432145565,
                    "acc_stderr": 0.004607484283767461,
                    "timestamp": "2024-06-06T13-32-05.282020"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.02577710386656558,
                    "acc_stderr": 0.00436504295362182,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.03188578017686398,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.16666666666666666,
                    "acc_stderr": 0.03333333333333337,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.27469135802469136,
                    "acc_stderr": 0.024836057868294674,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.2424581005586592,
                    "acc_stderr": 0.014333522059217887,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.25738396624472576,
                    "acc_stderr": 0.0284588209914603,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.23699421965317918,
                    "acc_stderr": 0.02289408248992599,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.23533246414602346,
                    "acc_stderr": 0.010834432543912238,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.24539877300613497,
                    "acc_stderr": 0.03380939813943354,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.24019607843137256,
                    "acc_stderr": 0.02998373305591361,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.2282958199356913,
                    "acc_stderr": 0.023839303311398215,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04186091791394607,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.256198347107438,
                    "acc_stderr": 0.03984979653302872,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.24242424242424243,
                    "acc_stderr": 0.033464098810559534,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.23316062176165803,
                    "acc_stderr": 0.03051611137147601,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.1722689075630252,
                    "acc_stderr": 0.024528664971305417,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.19696969696969696,
                    "acc_stderr": 0.02833560973246335,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.26972477064220185,
                    "acc_stderr": 0.019028486711115452,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.32727272727272727,
                    "acc_stderr": 0.0449429086625209,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322695,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.23383084577114427,
                    "acc_stderr": 0.029929415408348377,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.24358974358974358,
                    "acc_stderr": 0.021763733684173926,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.19183673469387755,
                    "acc_stderr": 0.025206963154225392,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.25163398692810457,
                    "acc_stderr": 0.017555818091322267,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.22137404580152673,
                    "acc_stderr": 0.0364129708131373,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.23684210526315788,
                    "acc_stderr": 0.039994238792813344,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.2835249042145594,
                    "acc_stderr": 0.016117318166832276,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.2948717948717949,
                    "acc_stderr": 0.02987257770889117,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.24271844660194175,
                    "acc_stderr": 0.04245022486384495,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.023805186524888135,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.3542600896860987,
                    "acc_stderr": 0.032100621541349864,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.02679956202488768,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.2543352601156069,
                    "acc_stderr": 0.0332055644308557,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036845,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.18867924528301888,
                    "acc_stderr": 0.02407999513006224,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.3313253012048193,
                    "acc_stderr": 0.03664314777288086,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.30141843971631205,
                    "acc_stderr": 0.027374128882631153,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.04092563958237655,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.16556291390728478,
                    "acc_stderr": 0.03034818341030361,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.23225806451612904,
                    "acc_stderr": 0.02402225613030824,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.03476590104304134,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.3333333333333333,
                    "acc_stderr": 0.04072314811876837,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909283,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.15789473684210525,
                    "acc_stderr": 0.029674167520101446,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.2553191489361702,
                    "acc_stderr": 0.02850485647051418,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909283,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.29464285714285715,
                    "acc_stderr": 0.04327040932578728,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.2512315270935961,
                    "acc_stderr": 0.030516530732694436,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.17592592592592593,
                    "acc_stderr": 0.025967420958258526,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.02201908001221789,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.2827586206896552,
                    "acc_stderr": 0.03752833958003337,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.2740740740740741,
                    "acc_stderr": 0.027195934804085626,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.35665529010238906,
                    "acc_stderr": 0.013998056902620199,
                    "acc_norm": 0.3856655290102389,
                    "acc_norm_stderr": 0.014224250973257175,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5072694682334197,
                    "acc_stderr": 0.004989254011895759,
                    "acc_norm": 0.6890061740689106,
                    "acc_norm_stderr": 0.004619542392006366,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.35095596246578176,
                    "acc_stderr": 0.013568219502617192,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 23.707553429599244,
                    "bleu_max_stderr": 0.7450122059703083,
                    "bleu_acc": 0.2876376988984088,
                    "bleu_acc_stderr": 0.015846315101394785,
                    "bleu_diff": -8.638098070838828,
                    "bleu_diff_stderr": 0.8005088298726831,
                    "rouge1_max": 48.60974313039175,
                    "rouge1_max_stderr": 0.858316293921586,
                    "rouge1_acc": 0.2594859241126071,
                    "rouge1_acc_stderr": 0.015345409485558008,
                    "rouge1_diff": -11.045582261019915,
                    "rouge1_diff_stderr": 0.8882795627003046,
                    "rouge2_max": 31.52878460707801,
                    "rouge2_max_stderr": 0.9802016635484709,
                    "rouge2_acc": 0.20195838433292534,
                    "rouge2_acc_stderr": 0.014053957441512376,
                    "rouge2_diff": -13.46757758334418,
                    "rouge2_diff_stderr": 1.035259887092465,
                    "rougeL_max": 45.74410864668455,
                    "rougeL_max_stderr": 0.8676424195118835,
                    "rougeL_acc": 0.2594859241126071,
                    "rougeL_acc_stderr": 0.015345409485558011,
                    "rougeL_diff": -11.266372071748116,
                    "rougeL_diff_stderr": 0.8999068306781938,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.2178702570379437,
                    "acc_stderr": 0.014450846714123899,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.654301499605367,
                    "acc_stderr": 0.013366596951934375,
                    "timestamp": "2024-11-21T16-41-59.547447"
                }
            }
        }
    }
}