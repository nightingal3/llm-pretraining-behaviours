{
    "model_name": "facebook/opt-6.7b",
    "last_updated": "2024-12-19 13:41:04.593268",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.003663003663003663,
                "exact_match_stderr": 0.002587757368193444,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.010332950631458095,
                "exact_match_stderr": 0.0034284443646836666,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.005555555555555556,
                "exact_match_stderr": 0.0032015451273208887,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.004429678848283499,
                "exact_match_stderr": 0.0022111531423787863,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "minerva_math_geometry": {
                "exact_match": 0.010438413361169102,
                "exact_match_stderr": 0.004648627117184668,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.008438818565400843,
                "exact_match_stderr": 0.004206007207713056,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "minerva_math_algebra": {
                "exact_match": 0.009267059814658803,
                "exact_match_stderr": 0.002782319118488812,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521483,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_3ds": {
                "acc": 0.002,
                "acc_stderr": 0.0009992493430694893,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_2ds": {
                "acc": 0.012,
                "acc_stderr": 0.0024353573624298335,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_1dc": {
                "acc": 0.0205,
                "acc_stderr": 0.00316936861988699,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_2dm": {
                "acc": 0.036,
                "acc_stderr": 0.004166614973833173,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "arithmetic_2da": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "gsm8k_cot": {
                "exact_match": 0.02880970432145565,
                "exact_match_stderr": 0.004607484283767461,
                "timestamp": "2024-06-06T13-32-05.282020"
            },
            "gsm8k": {
                "exact_match": 0.02577710386656558,
                "exact_match_stderr": 0.00436504295362182,
                "timestamp": "2024-11-21T16-41-59.547447"
            },
            "mmlu_world_religions": {
                "acc": 0.2573099415204678,
                "acc_stderr": 0.03352799844161865,
                "brier_score": 0.7552209591883933,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_formal_logic": {
                "acc": 0.1984126984126984,
                "acc_stderr": 0.035670166752768614,
                "brier_score": 0.7737676613415517,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_prehistory": {
                "acc": 0.2808641975308642,
                "acc_stderr": 0.025006469755799208,
                "brier_score": 0.7550137099246275,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.01442229220480884,
                "brier_score": 0.7847262404434647,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25738396624472576,
                "acc_stderr": 0.0284588209914603,
                "brier_score": 0.7608715004399518,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26878612716763006,
                "acc_stderr": 0.023868003262500097,
                "brier_score": 0.7530167216408132,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_professional_law": {
                "acc": 0.2685788787483703,
                "acc_stderr": 0.011320056629121734,
                "brier_score": 0.7745934802801677,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2883435582822086,
                "acc_stderr": 0.03559039531617342,
                "brier_score": 0.7562371230137225,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.029331162294251735,
                "brier_score": 0.7606373179788259,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_philosophy": {
                "acc": 0.2090032154340836,
                "acc_stderr": 0.023093140398374224,
                "brier_score": 0.7619163388633432,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_jurisprudence": {
                "acc": 0.21296296296296297,
                "acc_stderr": 0.03957835471980981,
                "brier_score": 0.7544655022994979,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_international_law": {
                "acc": 0.371900826446281,
                "acc_stderr": 0.04412015806624504,
                "brier_score": 0.738750265298197,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03225078108306289,
                "brier_score": 0.7596660922455263,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.23316062176165803,
                "acc_stderr": 0.03051611137147601,
                "brier_score": 0.7755860661040752,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.026265024608275882,
                "brier_score": 0.7775274996254388,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_geography": {
                "acc": 0.20707070707070707,
                "acc_stderr": 0.02886977846026705,
                "brier_score": 0.770352204182849,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23486238532110093,
                "acc_stderr": 0.018175110510343588,
                "brier_score": 0.7657261725880065,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_public_relations": {
                "acc": 0.3181818181818182,
                "acc_stderr": 0.044612721759105085,
                "brier_score": 0.7584306917793805,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.7602175169099713,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_sociology": {
                "acc": 0.22388059701492538,
                "acc_stderr": 0.029475250236017197,
                "brier_score": 0.7672048853971253,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.20256410256410257,
                "acc_stderr": 0.020377660970371383,
                "brier_score": 0.7789187510682279,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_security_studies": {
                "acc": 0.23673469387755103,
                "acc_stderr": 0.02721283588407316,
                "brier_score": 0.7692639395936833,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2696078431372549,
                "acc_stderr": 0.017952449196987866,
                "brier_score": 0.7547884905782574,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_human_sexuality": {
                "acc": 0.24427480916030533,
                "acc_stderr": 0.03768335959728744,
                "brier_score": 0.7678727074555681,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.04049339297748142,
                "brier_score": 0.7614188863435601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_miscellaneous": {
                "acc": 0.26436781609195403,
                "acc_stderr": 0.015769984840690518,
                "brier_score": 0.7540259391362917,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_marketing": {
                "acc": 0.28205128205128205,
                "acc_stderr": 0.02948036054954119,
                "brier_score": 0.7532337077509424,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_management": {
                "acc": 0.24271844660194175,
                "acc_stderr": 0.04245022486384493,
                "brier_score": 0.7671254179408622,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_nutrition": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.025261691219729484,
                "brier_score": 0.7612413425517579,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_medical_genetics": {
                "acc": 0.27,
                "acc_stderr": 0.044619604333847394,
                "brier_score": 0.7505691471150036,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_human_aging": {
                "acc": 0.242152466367713,
                "acc_stderr": 0.028751392398694755,
                "brier_score": 0.7536864157975758,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_professional_medicine": {
                "acc": 0.16176470588235295,
                "acc_stderr": 0.022368672562886754,
                "brier_score": 0.7799692276903291,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_college_medicine": {
                "acc": 0.2947976878612717,
                "acc_stderr": 0.034765996075164785,
                "brier_score": 0.7619002765455879,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_business_ethics": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.7504553219270095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.21509433962264152,
                "acc_stderr": 0.025288394502891366,
                "brier_score": 0.7661011361188558,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_global_facts": {
                "acc": 0.27,
                "acc_stderr": 0.044619604333847394,
                "brier_score": 0.7482995931248143,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_virology": {
                "acc": 0.26506024096385544,
                "acc_stderr": 0.03436024037944967,
                "brier_score": 0.7506023198628184,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2730496453900709,
                "acc_stderr": 0.02657786094330785,
                "brier_score": 0.7542366976498601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.04092563958237656,
                "brier_score": 0.7781365912609894,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2582781456953642,
                "acc_stderr": 0.035737053147634576,
                "brier_score": 0.7707688373280365,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2129032258064516,
                "acc_stderr": 0.023287665127268518,
                "brier_score": 0.7678464225240904,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_college_biology": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.03685651095897532,
                "brier_score": 0.7660270777982109,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_anatomy": {
                "acc": 0.3111111111111111,
                "acc_stderr": 0.03999262876617723,
                "brier_score": 0.7503755164314315,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_college_chemistry": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036845,
                "brier_score": 0.7845496059378259,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_computer_security": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7566757002307685,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_college_computer_science": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695235,
                "brier_score": 0.7504260902356191,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_astronomy": {
                "acc": 0.28289473684210525,
                "acc_stderr": 0.03665349695640767,
                "brier_score": 0.7661924313226414,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_college_mathematics": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7525634009133456,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.22127659574468084,
                "acc_stderr": 0.027136349602424066,
                "brier_score": 0.7635802144135204,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909282,
                "brier_score": 0.7477432389158422,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695236,
                "brier_score": 0.7467316005783833,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_machine_learning": {
                "acc": 0.25892857142857145,
                "acc_stderr": 0.04157751539865629,
                "brier_score": 0.7397021806186821,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.3054187192118227,
                "acc_stderr": 0.032406615658684086,
                "brier_score": 0.7459986258062615,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.21296296296296297,
                "acc_stderr": 0.027920963147993666,
                "brier_score": 0.7928295843656448,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.022019080012217883,
                "brier_score": 0.7598391214132458,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.03565998174135303,
                "brier_score": 0.7540268437561872,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.026067159222275794,
                "brier_score": 0.7597320922715378,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-53-28.313544"
            },
            "arc_challenge": {
                "acc": 0.35665529010238906,
                "acc_stderr": 0.013998056902620199,
                "acc_norm": 0.3856655290102389,
                "acc_norm_stderr": 0.014224250973257175,
                "timestamp": "2024-11-21T16-41-59.547447"
            },
            "hellaswag": {
                "acc": 0.5072694682334197,
                "acc_stderr": 0.004989254011895759,
                "acc_norm": 0.6890061740689106,
                "acc_norm_stderr": 0.004619542392006366,
                "timestamp": "2024-11-21T16-41-59.547447"
            },
            "truthfulqa_mc2": {
                "acc": 0.35095596246578176,
                "acc_stderr": 0.013568219502617192,
                "timestamp": "2024-11-21T16-41-59.547447"
            },
            "truthfulqa_gen": {
                "bleu_max": 23.707553429599244,
                "bleu_max_stderr": 0.7450122059703083,
                "bleu_acc": 0.2876376988984088,
                "bleu_acc_stderr": 0.015846315101394785,
                "bleu_diff": -8.638098070838828,
                "bleu_diff_stderr": 0.8005088298726831,
                "rouge1_max": 48.60974313039175,
                "rouge1_max_stderr": 0.858316293921586,
                "rouge1_acc": 0.2594859241126071,
                "rouge1_acc_stderr": 0.015345409485558008,
                "rouge1_diff": -11.045582261019915,
                "rouge1_diff_stderr": 0.8882795627003046,
                "rouge2_max": 31.52878460707801,
                "rouge2_max_stderr": 0.9802016635484709,
                "rouge2_acc": 0.20195838433292534,
                "rouge2_acc_stderr": 0.014053957441512376,
                "rouge2_diff": -13.46757758334418,
                "rouge2_diff_stderr": 1.035259887092465,
                "rougeL_max": 45.74410864668455,
                "rougeL_max_stderr": 0.8676424195118835,
                "rougeL_acc": 0.2594859241126071,
                "rougeL_acc_stderr": 0.015345409485558011,
                "rougeL_diff": -11.266372071748116,
                "rougeL_diff_stderr": 0.8999068306781938,
                "timestamp": "2024-11-21T16-41-59.547447"
            },
            "truthfulqa_mc1": {
                "acc": 0.2178702570379437,
                "acc_stderr": 0.014450846714123899,
                "timestamp": "2024-11-21T16-41-59.547447"
            },
            "winogrande": {
                "acc": 0.654301499605367,
                "acc_stderr": 0.013366596951934375,
                "timestamp": "2024-11-21T16-41-59.547447"
            }
        }
    }
}