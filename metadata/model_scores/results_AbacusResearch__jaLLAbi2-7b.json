{
    "model_name": "AbacusResearch/jaLLAbi2-7b",
    "last_updated": "2024-12-04 11:24:26.782586",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.07875457875457875,
                "exact_match_stderr": 0.0115379147687345,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.42479908151549944,
                "exact_match_stderr": 0.016758762397009255,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.15,
                "exact_match_stderr": 0.015380154912113008,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.07862679955703211,
                "exact_match_stderr": 0.008961894321625518,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "minerva_math_geometry": {
                "exact_match": 0.19206680584551147,
                "exact_match_stderr": 0.018017724185011356,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.21729957805907174,
                "exact_match_stderr": 0.01896254634032935,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "minerva_math_algebra": {
                "exact_match": 0.3201347935973041,
                "exact_match_stderr": 0.01354676204212894,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_3da": {
                "acc": 0.988,
                "acc_stderr": 0.002435357362429865,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_3ds": {
                "acc": 0.985,
                "acc_stderr": 0.002718675338799951,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_4da": {
                "acc": 0.9445,
                "acc_stderr": 0.005120838456077827,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_2ds": {
                "acc": 0.995,
                "acc_stderr": 0.0015775754727385177,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_5ds": {
                "acc": 0.9005,
                "acc_stderr": 0.0066949448200168534,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_5da": {
                "acc": 0.9245,
                "acc_stderr": 0.005909089072508007,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_1dc": {
                "acc": 0.769,
                "acc_stderr": 0.009426766782199622,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_4ds": {
                "acc": 0.9545,
                "acc_stderr": 0.004661087627253434,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_2dm": {
                "acc": 0.674,
                "acc_stderr": 0.01048412888509276,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "arithmetic_2da": {
                "acc": 1.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "gsm8k_cot": {
                "exact_match": 0.759666413949962,
                "exact_match_stderr": 0.011769580703836952,
                "timestamp": "2024-06-10T13-00-50.957455"
            },
            "gsm8k": {
                "exact_match": 0.7354056103108415,
                "exact_match_stderr": 0.012150554001563233,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "anli_r2": {
                "brier_score": 0.7735189327159094,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "anli_r3": {
                "brier_score": 0.8359426771055016,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "anli_r1": {
                "brier_score": 0.6783474989688411,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_eu": {
                "brier_score": 1.0950678701767658,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_vi": {
                "brier_score": 0.9853321581143759,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_ru": {
                "brier_score": 0.9458423472607614,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_zh": {
                "brier_score": 1.0653229084833014,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_tr": {
                "brier_score": 1.0516964598791096,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_fr": {
                "brier_score": 0.9213564341895442,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_en": {
                "brier_score": 0.7308615429117169,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_ur": {
                "brier_score": 1.1916179316711006,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_ar": {
                "brier_score": 1.2229906458740496,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_de": {
                "brier_score": 0.8943690517747402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_hi": {
                "brier_score": 0.9606897711791605,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_es": {
                "brier_score": 0.970678317354639,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_bg": {
                "brier_score": 0.9716211440198455,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_sw": {
                "brier_score": 1.04639506388785,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_el": {
                "brier_score": 0.9811536445855956,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "xnli_th": {
                "brier_score": 1.060485339632959,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "logiqa2": {
                "brier_score": 0.9724763634284794,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "mathqa": {
                "brier_score": 0.9560746958174166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T13-11-48.655040"
            },
            "lambada_standard": {
                "perplexity": 4.06627682379339,
                "perplexity_stderr": 0.10368161889943868,
                "acc": 0.6598098195226082,
                "acc_stderr": 0.006600583766063726,
                "timestamp": "2024-06-10T13-13-20.313891"
            },
            "lambada_openai": {
                "perplexity": 3.3960452191861408,
                "perplexity_stderr": 0.07798732129358217,
                "acc": 0.7135649136425384,
                "acc_stderr": 0.006298569473987373,
                "timestamp": "2024-06-10T13-13-20.313891"
            },
            "mmlu_world_religions": {
                "acc": 0.8362573099415205,
                "acc_stderr": 0.028380919596145866,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_formal_logic": {
                "acc": 0.4603174603174603,
                "acc_stderr": 0.04458029125470973,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_prehistory": {
                "acc": 0.7283950617283951,
                "acc_stderr": 0.02474862449053737,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.4581005586592179,
                "acc_stderr": 0.01666368329502053,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.8016877637130801,
                "acc_stderr": 0.02595502084162111,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_moral_disputes": {
                "acc": 0.7514450867052023,
                "acc_stderr": 0.023267528432100174,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_professional_law": {
                "acc": 0.4680573663624511,
                "acc_stderr": 0.012744149704869647,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.7668711656441718,
                "acc_stderr": 0.033220157957767414,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.8529411764705882,
                "acc_stderr": 0.024857478080250482,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_philosophy": {
                "acc": 0.7266881028938906,
                "acc_stderr": 0.02531176597542612,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_jurisprudence": {
                "acc": 0.7962962962962963,
                "acc_stderr": 0.03893542518824847,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_international_law": {
                "acc": 0.7851239669421488,
                "acc_stderr": 0.03749492448709696,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.7757575757575758,
                "acc_stderr": 0.03256866661681102,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.8860103626943006,
                "acc_stderr": 0.022935144053919426,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.6722689075630253,
                "acc_stderr": 0.03048991141767323,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_geography": {
                "acc": 0.8232323232323232,
                "acc_stderr": 0.027178752639044915,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.8403669724770643,
                "acc_stderr": 0.015703498348461752,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_public_relations": {
                "acc": 0.6727272727272727,
                "acc_stderr": 0.0449429086625209,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.86,
                "acc_stderr": 0.03487350880197769,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_sociology": {
                "acc": 0.835820895522388,
                "acc_stderr": 0.02619392354445414,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.658974358974359,
                "acc_stderr": 0.024035489676335082,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_security_studies": {
                "acc": 0.746938775510204,
                "acc_stderr": 0.0278330238713997,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_professional_psychology": {
                "acc": 0.6699346405228758,
                "acc_stderr": 0.019023726160724553,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_human_sexuality": {
                "acc": 0.7938931297709924,
                "acc_stderr": 0.03547771004159463,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_econometrics": {
                "acc": 0.49122807017543857,
                "acc_stderr": 0.04702880432049615,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_miscellaneous": {
                "acc": 0.8250319284802043,
                "acc_stderr": 0.013586619219903348,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_marketing": {
                "acc": 0.8803418803418803,
                "acc_stderr": 0.021262719400406957,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_management": {
                "acc": 0.7766990291262136,
                "acc_stderr": 0.04123553189891431,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_nutrition": {
                "acc": 0.7222222222222222,
                "acc_stderr": 0.02564686309713791,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_medical_genetics": {
                "acc": 0.72,
                "acc_stderr": 0.045126085985421276,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_human_aging": {
                "acc": 0.6995515695067265,
                "acc_stderr": 0.03076935200822914,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_professional_medicine": {
                "acc": 0.6801470588235294,
                "acc_stderr": 0.028332959514031236,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_college_medicine": {
                "acc": 0.6647398843930635,
                "acc_stderr": 0.03599586301247077,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_business_ethics": {
                "acc": 0.65,
                "acc_stderr": 0.047937248544110196,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.7132075471698113,
                "acc_stderr": 0.02783491252754407,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_global_facts": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695236,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_virology": {
                "acc": 0.5301204819277109,
                "acc_stderr": 0.03885425420866766,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_professional_accounting": {
                "acc": 0.5070921985815603,
                "acc_stderr": 0.02982449855912901,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_college_physics": {
                "acc": 0.39215686274509803,
                "acc_stderr": 0.048580835742663434,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_physics": {
                "acc": 0.37748344370860926,
                "acc_stderr": 0.0395802723112157,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_biology": {
                "acc": 0.7935483870967742,
                "acc_stderr": 0.023025899617188723,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_college_biology": {
                "acc": 0.7708333333333334,
                "acc_stderr": 0.035146974678623884,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_anatomy": {
                "acc": 0.6518518518518519,
                "acc_stderr": 0.041153246103369526,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_college_chemistry": {
                "acc": 0.47,
                "acc_stderr": 0.05016135580465919,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_computer_security": {
                "acc": 0.77,
                "acc_stderr": 0.04229525846816505,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_college_computer_science": {
                "acc": 0.52,
                "acc_stderr": 0.050211673156867795,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_astronomy": {
                "acc": 0.7039473684210527,
                "acc_stderr": 0.03715062154998905,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_college_mathematics": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.5787234042553191,
                "acc_stderr": 0.03227834510146268,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.36,
                "acc_stderr": 0.04824181513244218,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.7,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_machine_learning": {
                "acc": 0.4375,
                "acc_stderr": 0.04708567521880525,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.5123152709359606,
                "acc_stderr": 0.035169204442208966,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.49537037037037035,
                "acc_stderr": 0.03409825519163572,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.40476190476190477,
                "acc_stderr": 0.0252798503974049,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.5586206896551724,
                "acc_stderr": 0.04137931034482757,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.35185185185185186,
                "acc_stderr": 0.02911661760608301,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "arc_challenge": {
                "acc": 0.6919795221843004,
                "acc_stderr": 0.01349142951729204,
                "acc_norm": 0.7226962457337884,
                "acc_norm_stderr": 0.013082095839059374,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "hellaswag": {
                "acc": 0.7108145787691695,
                "acc_stderr": 0.0045245758929529185,
                "acc_norm": 0.8833897629954193,
                "acc_norm_stderr": 0.0032029933469910625,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "truthfulqa_mc2": {
                "acc": 0.7074488603697261,
                "acc_stderr": 0.01469831169732992,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "truthfulqa_gen": {
                "bleu_max": 16.994584405212713,
                "bleu_max_stderr": 0.5978346623164609,
                "bleu_acc": 0.5214198286413708,
                "bleu_acc_stderr": 0.01748743214471164,
                "bleu_diff": 2.5161099258317203,
                "bleu_diff_stderr": 0.4812096593307796,
                "rouge1_max": 43.149301424681205,
                "rouge1_max_stderr": 0.7451779721995008,
                "rouge1_acc": 0.5495716034271726,
                "rouge1_acc_stderr": 0.01741726437196764,
                "rouge1_diff": 4.598718031567308,
                "rouge1_diff_stderr": 0.7629498840038131,
                "rouge2_max": 28.509782535112226,
                "rouge2_max_stderr": 0.8264201912092511,
                "rouge2_acc": 0.4724602203182375,
                "rouge2_acc_stderr": 0.017476930190712187,
                "rouge2_diff": 4.175284034929332,
                "rouge2_diff_stderr": 0.8296229945452628,
                "rougeL_max": 39.81725800184248,
                "rougeL_max_stderr": 0.7558047743596359,
                "rougeL_acc": 0.5483476132190942,
                "rougeL_acc_stderr": 0.017421480300277643,
                "rougeL_diff": 4.509539289026502,
                "rougeL_diff_stderr": 0.764155345941694,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "truthfulqa_mc1": {
                "acc": 0.5520195838433293,
                "acc_stderr": 0.01740851306342291,
                "timestamp": "2024-11-06T23-38-53.002859"
            },
            "winogrande": {
                "acc": 0.835043409629045,
                "acc_stderr": 0.01043091746823742,
                "timestamp": "2024-11-06T23-38-53.002859"
            }
        }
    }
}