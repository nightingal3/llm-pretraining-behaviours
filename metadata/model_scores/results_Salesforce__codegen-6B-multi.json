{
    "model_name": "Salesforce/codegen-6B-multi",
    "last_updated": "2024-12-19 13:42:14.316186",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.01098901098901099,
                "exact_match_stderr": 0.004465618427331412,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.001148105625717566,
                "exact_match_stderr": 0.00114810562571757,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.001851851851851852,
                "exact_match_stderr": 0.001851851851851851,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.009966777408637873,
                "exact_match_stderr": 0.0033074934669720295,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0020876826722338203,
                "exact_match_stderr": 0.0020876826722338333,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.006329113924050633,
                "exact_match_stderr": 0.0036463820410650577,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "minerva_math_algebra": {
                "exact_match": 0.008424599831508003,
                "exact_match_stderr": 0.0026539648581165158,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_3da": {
                "acc": 0.004,
                "acc_stderr": 0.0014117352790976934,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_3ds": {
                "acc": 0.009,
                "acc_stderr": 0.002112280962711353,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_4da": {
                "acc": 0.002,
                "acc_stderr": 0.0009992493430694928,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_2ds": {
                "acc": 0.0615,
                "acc_stderr": 0.005373389214995324,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_1dc": {
                "acc": 0.093,
                "acc_stderr": 0.0064958908780204825,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000068,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_2dm": {
                "acc": 0.039,
                "acc_stderr": 0.0043299970481765525,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "arithmetic_2da": {
                "acc": 0.052,
                "acc_stderr": 0.004965916850399523,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "gsm8k_cot": {
                "exact_match": 0.026535253980288095,
                "exact_match_stderr": 0.004427045987265165,
                "timestamp": "2024-06-13T23-41-51.095936"
            },
            "gsm8k": {
                "exact_match": 0.022744503411675512,
                "exact_match_stderr": 0.004106620637749694,
                "timestamp": "2024-11-20T17-44-14.058126"
            },
            "anli_r2": {
                "brier_score": 0.7472883683198149,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "anli_r3": {
                "brier_score": 0.7776246952766512,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "anli_r1": {
                "brier_score": 0.7736621012350583,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_eu": {
                "brier_score": 1.0388889959896963,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_vi": {
                "brier_score": 1.2154173472405334,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_ru": {
                "brier_score": 0.8730313226525406,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_zh": {
                "brier_score": 0.9828620736086736,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_tr": {
                "brier_score": 1.0519111336327807,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_fr": {
                "brier_score": 1.0820785121586356,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_en": {
                "brier_score": 0.7349387098851814,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_ur": {
                "brier_score": 1.3175247071589906,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_ar": {
                "brier_score": 1.0337470838746723,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_de": {
                "brier_score": 0.9493973793623973,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_hi": {
                "brier_score": 0.9824796835327766,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_es": {
                "brier_score": 1.2007293365919631,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_bg": {
                "brier_score": 0.9574865362674579,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_sw": {
                "brier_score": 0.8975406853203322,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_el": {
                "brier_score": 0.8985200188087877,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "xnli_th": {
                "brier_score": 0.9106758374214105,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "logiqa2": {
                "brier_score": 1.1702528650156314,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "mathqa": {
                "brier_score": 0.959189809900026,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T23-54-19.404462"
            },
            "lambada_standard": {
                "perplexity": 71.09167247462723,
                "perplexity_stderr": 2.933823524943788,
                "acc": 0.25441490393945276,
                "acc_stderr": 0.006067809764031531,
                "timestamp": "2024-06-13T23-55-47.686622"
            },
            "lambada_openai": {
                "perplexity": 46.16956426063179,
                "perplexity_stderr": 1.8273643949515241,
                "acc": 0.3027362701339026,
                "acc_stderr": 0.006400926467529961,
                "timestamp": "2024-06-13T23-55-47.686622"
            },
            "mmlu_world_religions": {
                "acc": 0.28654970760233917,
                "acc_stderr": 0.03467826685703826,
                "brier_score": 0.7532546503632859,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_formal_logic": {
                "acc": 0.23015873015873015,
                "acc_stderr": 0.03764950879790605,
                "brier_score": 0.7963662753131191,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_prehistory": {
                "acc": 0.30864197530864196,
                "acc_stderr": 0.025702640260603756,
                "brier_score": 0.7634272189938609,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2435754189944134,
                "acc_stderr": 0.01435591196476786,
                "brier_score": 0.7995356694042249,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25316455696202533,
                "acc_stderr": 0.02830465794303529,
                "brier_score": 0.7783172821292649,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26011560693641617,
                "acc_stderr": 0.023618678310069356,
                "brier_score": 0.7578899242156162,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_professional_law": {
                "acc": 0.27249022164276404,
                "acc_stderr": 0.01137165829431152,
                "brier_score": 0.7811283194404477,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.24539877300613497,
                "acc_stderr": 0.03380939813943354,
                "brier_score": 0.7622586240232632,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.030778554678693254,
                "brier_score": 0.7733944586470429,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_philosophy": {
                "acc": 0.24437299035369775,
                "acc_stderr": 0.024406162094668903,
                "brier_score": 0.7801192892080534,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_jurisprudence": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.041331194402438376,
                "brier_score": 0.7768030237415023,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_international_law": {
                "acc": 0.2231404958677686,
                "acc_stderr": 0.03800754475228733,
                "brier_score": 0.7713861591060401,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03225078108306289,
                "brier_score": 0.7924488274974129,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.23834196891191708,
                "acc_stderr": 0.03074890536390989,
                "brier_score": 0.7750805919996284,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.20168067226890757,
                "acc_stderr": 0.026064313406304516,
                "brier_score": 0.791471356035099,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_geography": {
                "acc": 0.25252525252525254,
                "acc_stderr": 0.030954055470365928,
                "brier_score": 0.7784681107970716,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23853211009174313,
                "acc_stderr": 0.018272575810231867,
                "brier_score": 0.8000259036283063,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_public_relations": {
                "acc": 0.2909090909090909,
                "acc_stderr": 0.04350271442923243,
                "brier_score": 0.7603960425758571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.745432066974931,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_sociology": {
                "acc": 0.31343283582089554,
                "acc_stderr": 0.032801882053486435,
                "brier_score": 0.7595897474419355,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.21794871794871795,
                "acc_stderr": 0.02093244577446319,
                "brier_score": 0.7868715882088531,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_security_studies": {
                "acc": 0.22040816326530613,
                "acc_stderr": 0.026537045312145312,
                "brier_score": 0.7739147450326144,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_professional_psychology": {
                "acc": 0.28594771241830064,
                "acc_stderr": 0.01828048507295467,
                "brier_score": 0.7681059134757757,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2366412213740458,
                "acc_stderr": 0.03727673575596917,
                "brier_score": 0.7789988875767444,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_econometrics": {
                "acc": 0.2543859649122807,
                "acc_stderr": 0.040969851398436716,
                "brier_score": 0.7831895284106645,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_miscellaneous": {
                "acc": 0.26053639846743293,
                "acc_stderr": 0.015696008563807092,
                "brier_score": 0.7788589895933121,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_marketing": {
                "acc": 0.24786324786324787,
                "acc_stderr": 0.028286324075564386,
                "brier_score": 0.7764270748713787,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_management": {
                "acc": 0.20388349514563106,
                "acc_stderr": 0.03989139859531771,
                "brier_score": 0.8192650638978154,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_nutrition": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.024288619466046102,
                "brier_score": 0.7963416968181365,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_medical_genetics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "brier_score": 0.7655906441278336,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_human_aging": {
                "acc": 0.34080717488789236,
                "acc_stderr": 0.03181149747055362,
                "brier_score": 0.7586127872502773,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_professional_medicine": {
                "acc": 0.18382352941176472,
                "acc_stderr": 0.023529242185193106,
                "brier_score": 0.8281083010413719,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_college_medicine": {
                "acc": 0.18497109826589594,
                "acc_stderr": 0.029605623981771214,
                "brier_score": 0.7884521080102289,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_business_ethics": {
                "acc": 0.27,
                "acc_stderr": 0.04461960433384739,
                "brier_score": 0.7601396063483138,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.24528301886792453,
                "acc_stderr": 0.026480357179895688,
                "brier_score": 0.7768285415707459,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_global_facts": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768078,
                "brier_score": 0.7540050020585366,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_virology": {
                "acc": 0.27710843373493976,
                "acc_stderr": 0.03484331592680587,
                "brier_score": 0.7571894187167946,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_professional_accounting": {
                "acc": 0.28368794326241137,
                "acc_stderr": 0.02689170942834396,
                "brier_score": 0.7648754774992964,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_college_physics": {
                "acc": 0.28431372549019607,
                "acc_stderr": 0.04488482852329017,
                "brier_score": 0.7936455868848244,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_physics": {
                "acc": 0.23841059602649006,
                "acc_stderr": 0.0347918557259966,
                "brier_score": 0.8213075228649029,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_biology": {
                "acc": 0.25161290322580643,
                "acc_stderr": 0.024685979286239966,
                "brier_score": 0.7738057216716354,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_college_biology": {
                "acc": 0.2569444444444444,
                "acc_stderr": 0.03653946969442099,
                "brier_score": 0.7558366567479475,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_anatomy": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.03785714465066653,
                "brier_score": 0.7646735335299525,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_college_chemistry": {
                "acc": 0.19,
                "acc_stderr": 0.039427724440366234,
                "brier_score": 0.8108890731790384,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_computer_security": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.8018378565338942,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_college_computer_science": {
                "acc": 0.21,
                "acc_stderr": 0.04093601807403326,
                "brier_score": 0.8188608547002265,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_astronomy": {
                "acc": 0.20394736842105263,
                "acc_stderr": 0.0327900040631005,
                "brier_score": 0.806676012631122,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_college_mathematics": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909284,
                "brier_score": 0.8343213256984789,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2680851063829787,
                "acc_stderr": 0.028957342788342347,
                "brier_score": 0.767174864730072,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "brier_score": 0.8067165829480192,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7509139065340114,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_machine_learning": {
                "acc": 0.29464285714285715,
                "acc_stderr": 0.0432704093257873,
                "brier_score": 0.769449635318899,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2955665024630542,
                "acc_stderr": 0.032104944337514575,
                "brier_score": 0.7649732533889056,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.19444444444444445,
                "acc_stderr": 0.026991454502036726,
                "brier_score": 0.8146233490492436,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.24603174603174602,
                "acc_stderr": 0.022182037202948365,
                "brier_score": 0.824729102982892,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.30344827586206896,
                "acc_stderr": 0.038312260488503336,
                "brier_score": 0.7487325148507185,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2037037037037037,
                "acc_stderr": 0.02455617221914127,
                "brier_score": 0.8460906774646326,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-18-19.747795"
            },
            "arc_challenge": {
                "acc": 0.22440273037542663,
                "acc_stderr": 0.012191404938603838,
                "acc_norm": 0.26791808873720135,
                "acc_norm_stderr": 0.012942030195136433,
                "timestamp": "2024-11-20T17-44-14.058126"
            },
            "hellaswag": {
                "acc": 0.3339972117108146,
                "acc_stderr": 0.004706748152125333,
                "acc_norm": 0.4071898028281219,
                "acc_norm_stderr": 0.004903066639761948,
                "timestamp": "2024-11-20T17-44-14.058126"
            },
            "truthfulqa_mc2": {
                "acc": 0.456190954016092,
                "acc_stderr": 0.015179710279065738,
                "timestamp": "2024-11-20T17-44-14.058126"
            },
            "truthfulqa_gen": {
                "bleu_max": 18.4162863206651,
                "bleu_max_stderr": 0.6674446234293927,
                "bleu_acc": 0.4186046511627907,
                "bleu_acc_stderr": 0.017270015284476865,
                "bleu_diff": 2.0276870793796506,
                "bleu_diff_stderr": 0.7080695144029784,
                "rouge1_max": 39.01657959182641,
                "rouge1_max_stderr": 0.9129251342011889,
                "rouge1_acc": 0.3671970624235006,
                "rouge1_acc_stderr": 0.01687480500145318,
                "rouge1_diff": 0.7733335673342009,
                "rouge1_diff_stderr": 1.0566035740226651,
                "rouge2_max": 21.648507625466433,
                "rouge2_max_stderr": 1.002315340981092,
                "rouge2_acc": 0.23745410036719705,
                "rouge2_acc_stderr": 0.014896277441041845,
                "rouge2_diff": 1.2461063185143297,
                "rouge2_diff_stderr": 1.0682476326083845,
                "rougeL_max": 36.58025015195491,
                "rougeL_max_stderr": 0.9084853751956918,
                "rougeL_acc": 0.3671970624235006,
                "rougeL_acc_stderr": 0.01687480500145318,
                "rougeL_diff": 1.0039000965278462,
                "rougeL_diff_stderr": 1.069567876870668,
                "timestamp": "2024-11-20T17-44-14.058126"
            },
            "truthfulqa_mc1": {
                "acc": 0.26805385556915545,
                "acc_stderr": 0.015506204722834545,
                "timestamp": "2024-11-20T17-44-14.058126"
            },
            "winogrande": {
                "acc": 0.5414364640883977,
                "acc_stderr": 0.014004146853791899,
                "timestamp": "2024-11-20T17-44-14.058126"
            }
        }
    }
}