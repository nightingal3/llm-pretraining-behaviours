{
    "model_name": "Salesforce/codegen-350M-multi",
    "last_updated": "2024-12-04 11:23:50.264229",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.01098901098901099,
                "exact_match_stderr": 0.004465618427331418,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.01951779563719862,
                "exact_match_stderr": 0.004690029935284569,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.005555555555555556,
                "exact_match_stderr": 0.00320154512732093,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.012181616832779624,
                "exact_match_stderr": 0.0036524791938863663,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0020876826722338203,
                "exact_match_stderr": 0.0020876826722338333,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.008438818565400843,
                "exact_match_stderr": 0.004206007207713054,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "minerva_math_algebra": {
                "exact_match": 0.014321819713563605,
                "exact_match_stderr": 0.003450041570937017,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521454,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_3ds": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339515,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_4da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_2ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521503,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_1dc": {
                "acc": 0.029,
                "acc_stderr": 0.0037532044004605115,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_2dm": {
                "acc": 0.016,
                "acc_stderr": 0.002806410156941532,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "arithmetic_2da": {
                "acc": 0.004,
                "acc_stderr": 0.0014117352790977004,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "gsm8k_cot": {
                "exact_match": 0.022744503411675512,
                "exact_match_stderr": 0.004106620637749713,
                "timestamp": "2024-06-13T18-16-04.607115"
            },
            "gsm8k": {
                "exact_match": 0.02047005307050796,
                "exact_match_stderr": 0.003900413385915717,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "anli_r2": {
                "brier_score": 0.985719346196084,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "anli_r3": {
                "brier_score": 0.9732009467249916,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "anli_r1": {
                "brier_score": 0.9985649407847307,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_eu": {
                "brier_score": 1.2461840807929438,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_vi": {
                "brier_score": 1.1678074729947867,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_ru": {
                "brier_score": 0.9554534962904667,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_zh": {
                "brier_score": 1.0339034216345016,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_tr": {
                "brier_score": 1.255509308223099,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_fr": {
                "brier_score": 1.2357496951451048,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_en": {
                "brier_score": 0.8204070568367904,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_ur": {
                "brier_score": 1.2449382078214195,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_ar": {
                "brier_score": 1.026144943391084,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_de": {
                "brier_score": 0.9722273139773239,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_hi": {
                "brier_score": 1.0013653907261761,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_es": {
                "brier_score": 1.1864389653417817,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_bg": {
                "brier_score": 0.9590675242887678,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_sw": {
                "brier_score": 1.087228705558193,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_el": {
                "brier_score": 1.107009491211521,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "xnli_th": {
                "brier_score": 1.0205217980811283,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "logiqa2": {
                "brier_score": 1.1839026222831999,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "mathqa": {
                "brier_score": 1.0109975167317484,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T18-21-36.595519"
            },
            "lambada_standard": {
                "perplexity": 929.8432444456479,
                "perplexity_stderr": 44.08843957528335,
                "acc": 0.12012419949543955,
                "acc_stderr": 0.004529372226920103,
                "timestamp": "2024-06-13T18-22-36.360059"
            },
            "lambada_openai": {
                "perplexity": 1118.2514080440103,
                "perplexity_stderr": 58.16346545779122,
                "acc": 0.12652823597904134,
                "acc_stderr": 0.004631591355662349,
                "timestamp": "2024-06-13T18-22-36.360059"
            },
            "mmlu_world_religions": {
                "acc": 0.25146198830409355,
                "acc_stderr": 0.033275044238468436,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_formal_logic": {
                "acc": 0.18253968253968253,
                "acc_stderr": 0.03455071019102147,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_prehistory": {
                "acc": 0.22839506172839505,
                "acc_stderr": 0.023358211840626267,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2424581005586592,
                "acc_stderr": 0.014333522059217889,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2911392405063291,
                "acc_stderr": 0.029571601065753374,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_moral_disputes": {
                "acc": 0.23410404624277456,
                "acc_stderr": 0.022797110278071134,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_professional_law": {
                "acc": 0.2470664928292047,
                "acc_stderr": 0.011015752255279329,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2883435582822086,
                "acc_stderr": 0.035590395316173425,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.27450980392156865,
                "acc_stderr": 0.03132179803083289,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_philosophy": {
                "acc": 0.2572347266881029,
                "acc_stderr": 0.024826171289250888,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_jurisprudence": {
                "acc": 0.23148148148148148,
                "acc_stderr": 0.04077494709252626,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_international_law": {
                "acc": 0.2809917355371901,
                "acc_stderr": 0.04103203830514512,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.2909090909090909,
                "acc_stderr": 0.03546563019624336,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.36787564766839376,
                "acc_stderr": 0.034801756684660366,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.026653531596715473,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_geography": {
                "acc": 0.3484848484848485,
                "acc_stderr": 0.033948539651564025,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.30642201834862387,
                "acc_stderr": 0.019765517220458523,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_public_relations": {
                "acc": 0.14545454545454545,
                "acc_stderr": 0.0337689831983308,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_sociology": {
                "acc": 0.23383084577114427,
                "acc_stderr": 0.029929415408348384,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.32564102564102565,
                "acc_stderr": 0.02375966576741229,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_security_studies": {
                "acc": 0.30612244897959184,
                "acc_stderr": 0.02950489645459596,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.017630827375148383,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_human_sexuality": {
                "acc": 0.24427480916030533,
                "acc_stderr": 0.03768335959728743,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.039994238792813344,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_miscellaneous": {
                "acc": 0.24521072796934865,
                "acc_stderr": 0.01538435228454394,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_marketing": {
                "acc": 0.19658119658119658,
                "acc_stderr": 0.02603538609895129,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_management": {
                "acc": 0.2621359223300971,
                "acc_stderr": 0.04354631077260595,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_nutrition": {
                "acc": 0.26143790849673204,
                "acc_stderr": 0.025160998214292456,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_human_aging": {
                "acc": 0.2825112107623318,
                "acc_stderr": 0.030216831011508755,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4522058823529412,
                "acc_stderr": 0.030233758551596445,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_college_medicine": {
                "acc": 0.27167630057803466,
                "acc_stderr": 0.03391750322321659,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_business_ethics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2830188679245283,
                "acc_stderr": 0.027724236492700907,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_global_facts": {
                "acc": 0.16,
                "acc_stderr": 0.036845294917747094,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_virology": {
                "acc": 0.1686746987951807,
                "acc_stderr": 0.029152009627856544,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_professional_accounting": {
                "acc": 0.22695035460992907,
                "acc_stderr": 0.024987106365642976,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_college_physics": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.04158307533083286,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2781456953642384,
                "acc_stderr": 0.03658603262763743,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_biology": {
                "acc": 0.27419354838709675,
                "acc_stderr": 0.0253781399708852,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_college_biology": {
                "acc": 0.20833333333333334,
                "acc_stderr": 0.033961162058453336,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_anatomy": {
                "acc": 0.2518518518518518,
                "acc_stderr": 0.03749850709174023,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_college_chemistry": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_computer_security": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816506,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_college_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123394,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_college_mathematics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2425531914893617,
                "acc_stderr": 0.028020226271200217,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036623,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_machine_learning": {
                "acc": 0.33035714285714285,
                "acc_stderr": 0.04464285714285712,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.21674876847290642,
                "acc_stderr": 0.028990331252516235,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.46296296296296297,
                "acc_stderr": 0.034006036255382704,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2566137566137566,
                "acc_stderr": 0.022494510767503154,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.27586206896551724,
                "acc_stderr": 0.037245636197746304,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2814814814814815,
                "acc_stderr": 0.02742001935094527,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "arc_challenge": {
                "acc": 0.17320819112627986,
                "acc_stderr": 0.011058694183280328,
                "acc_norm": 0.21928327645051193,
                "acc_norm_stderr": 0.012091245787615716,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "hellaswag": {
                "acc": 0.27454690300736906,
                "acc_stderr": 0.004453735900947804,
                "acc_norm": 0.29356701852220674,
                "acc_norm_stderr": 0.0045446519760401006,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "truthfulqa_mc2": {
                "acc": 0.46878684418952904,
                "acc_stderr": 0.015919757808354366,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "truthfulqa_gen": {
                "bleu_max": 14.843134516314572,
                "bleu_max_stderr": 0.5728559782867888,
                "bleu_acc": 0.2864137086903305,
                "bleu_acc_stderr": 0.01582614243950239,
                "bleu_diff": -2.2717050031248,
                "bleu_diff_stderr": 0.43331168281195936,
                "rouge1_max": 35.03124298423832,
                "rouge1_max_stderr": 0.7866389432856384,
                "rouge1_acc": 0.28886168910648713,
                "rouge1_acc_stderr": 0.015866346401384315,
                "rouge1_diff": -4.893306750479266,
                "rouge1_diff_stderr": 0.5725461641851453,
                "rouge2_max": 19.758755174632952,
                "rouge2_max_stderr": 0.79627641363227,
                "rouge2_acc": 0.20685434516523868,
                "rouge2_acc_stderr": 0.014179591496728337,
                "rouge2_diff": -4.2748721462575014,
                "rouge2_diff_stderr": 0.5919097348223242,
                "rougeL_max": 32.14833428429696,
                "rougeL_max_stderr": 0.7758150933271327,
                "rougeL_acc": 0.2668298653610771,
                "rougeL_acc_stderr": 0.015483691939237277,
                "rougeL_diff": -4.921493483348142,
                "rougeL_diff_stderr": 0.5510430423849421,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "truthfulqa_mc1": {
                "acc": 0.27539779681762544,
                "acc_stderr": 0.01563813566777552,
                "timestamp": "2024-11-21T23-05-09.706261"
            },
            "winogrande": {
                "acc": 0.516179952644041,
                "acc_stderr": 0.0140451261309786,
                "timestamp": "2024-11-21T23-05-09.706261"
            }
        }
    }
}