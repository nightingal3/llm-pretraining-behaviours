{
    "model_name": "allenai/OLMo-7B-hf",
    "last_updated": "2024-12-04 11:23:32.075970",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.016483516483516484,
                "exact_match_stderr": 0.005454029764766746,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.03559127439724455,
                "exact_match_stderr": 0.006281201252709582,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.0052037049875126515,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.018826135105204873,
                "exact_match_stderr": 0.004525330498668475,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "minerva_math_geometry": {
                "exact_match": 0.018789144050104383,
                "exact_match_stderr": 0.006210416427997403,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.023206751054852322,
                "exact_match_stderr": 0.006922738487143303,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "minerva_math_algebra": {
                "exact_match": 0.012636899747262006,
                "exact_match_stderr": 0.003243518444352171,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521528,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_2ds": {
                "acc": 0.0155,
                "acc_stderr": 0.002762913651550328,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_1dc": {
                "acc": 0.0065,
                "acc_stderr": 0.0017973564602277768,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_2dm": {
                "acc": 0.029,
                "acc_stderr": 0.0037532044004605267,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "arithmetic_2da": {
                "acc": 0.016,
                "acc_stderr": 0.002806410156941529,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "gsm8k_cot": {
                "exact_match": 0.07733131159969674,
                "exact_match_stderr": 0.007357713523222347,
                "timestamp": "2024-06-11T07-11-43.974355"
            },
            "gsm8k": {
                "exact_match": 0.04700530705079606,
                "exact_match_stderr": 0.005829898355937165,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "anli_r2": {
                "brier_score": 0.727415360551483,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "anli_r3": {
                "brier_score": 0.7329111882888333,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "anli_r1": {
                "brier_score": 0.7459395081085325,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_eu": {
                "brier_score": 0.8502840947105932,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_vi": {
                "brier_score": 0.8697900492853695,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_ru": {
                "brier_score": 0.7888266397325421,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_zh": {
                "brier_score": 0.9170790959685605,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_tr": {
                "brier_score": 0.9126604079971902,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_fr": {
                "brier_score": 0.6871829801737326,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_en": {
                "brier_score": 0.6203086496227678,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_ur": {
                "brier_score": 1.1489733890229683,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_ar": {
                "brier_score": 1.2297706224773113,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_de": {
                "brier_score": 0.8009175976298082,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_hi": {
                "brier_score": 0.8077524633855024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_es": {
                "brier_score": 0.8277268635765521,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_bg": {
                "brier_score": 0.7504606344491516,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_sw": {
                "brier_score": 1.02550711834348,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_el": {
                "brier_score": 0.9957388418764128,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "xnli_th": {
                "brier_score": 1.0487110744801207,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "logiqa2": {
                "brier_score": 0.9608102017283243,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "mathqa": {
                "brier_score": 0.9668035927315314,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T07-42-39.674222"
            },
            "lambada_standard": {
                "perplexity": 5.030584263765433,
                "perplexity_stderr": 0.11130666457513855,
                "acc": 0.6405977100718029,
                "acc_stderr": 0.0066849041360329976,
                "timestamp": "2024-06-11T07-45-21.443883"
            },
            "lambada_openai": {
                "perplexity": 4.123391391255741,
                "perplexity_stderr": 0.08680886025197333,
                "acc": 0.6941587424801087,
                "acc_stderr": 0.006419327115892601,
                "timestamp": "2024-06-11T07-45-21.443883"
            },
            "mmlu_world_religions": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.033773102522091945,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_formal_logic": {
                "acc": 0.1984126984126984,
                "acc_stderr": 0.03567016675276864,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_prehistory": {
                "acc": 0.3117283950617284,
                "acc_stderr": 0.025773111169630436,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574903,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.270042194092827,
                "acc_stderr": 0.028900721906293426,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2658959537572254,
                "acc_stderr": 0.023786203255508297,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_professional_law": {
                "acc": 0.2607561929595828,
                "acc_stderr": 0.011213471559602313,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2392638036809816,
                "acc_stderr": 0.033519538795212696,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.22058823529411764,
                "acc_stderr": 0.02910225438967408,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_philosophy": {
                "acc": 0.3247588424437299,
                "acc_stderr": 0.026596782287697043,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_jurisprudence": {
                "acc": 0.21296296296296297,
                "acc_stderr": 0.039578354719809784,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_international_law": {
                "acc": 0.2644628099173554,
                "acc_stderr": 0.04026187527591206,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.03453131801885415,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.3316062176165803,
                "acc_stderr": 0.03397636541089116,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.3235294117647059,
                "acc_stderr": 0.03038835355188684,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_geography": {
                "acc": 0.23232323232323232,
                "acc_stderr": 0.030088629490217483,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24770642201834864,
                "acc_stderr": 0.018508143602547815,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_public_relations": {
                "acc": 0.32727272727272727,
                "acc_stderr": 0.04494290866252089,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909283,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_sociology": {
                "acc": 0.3582089552238806,
                "acc_stderr": 0.03390393042268814,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.3435897435897436,
                "acc_stderr": 0.024078696580635467,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_security_studies": {
                "acc": 0.4204081632653061,
                "acc_stderr": 0.03160106993449604,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_professional_psychology": {
                "acc": 0.22712418300653595,
                "acc_stderr": 0.01694985327921238,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2748091603053435,
                "acc_stderr": 0.03915345408847836,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_econometrics": {
                "acc": 0.2982456140350877,
                "acc_stderr": 0.04303684033537316,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2950191570881226,
                "acc_stderr": 0.016308363772932724,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_marketing": {
                "acc": 0.2863247863247863,
                "acc_stderr": 0.02961432369045666,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_management": {
                "acc": 0.27184466019417475,
                "acc_stderr": 0.044052680241409216,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_nutrition": {
                "acc": 0.33986928104575165,
                "acc_stderr": 0.027121956071388852,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_human_aging": {
                "acc": 0.3273542600896861,
                "acc_stderr": 0.03149384670994131,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4485294117647059,
                "acc_stderr": 0.0302114796091216,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_college_medicine": {
                "acc": 0.24277456647398843,
                "acc_stderr": 0.0326926380614177,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_business_ethics": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.27169811320754716,
                "acc_stderr": 0.027377706624670713,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_global_facts": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621504,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_virology": {
                "acc": 0.4036144578313253,
                "acc_stderr": 0.03819486140758398,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_professional_accounting": {
                "acc": 0.22340425531914893,
                "acc_stderr": 0.024847921358063962,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.04092563958237654,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2980132450331126,
                "acc_stderr": 0.037345356767871984,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_biology": {
                "acc": 0.34516129032258064,
                "acc_stderr": 0.027045746573534327,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_college_biology": {
                "acc": 0.24305555555555555,
                "acc_stderr": 0.03586879280080341,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_anatomy": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.03820169914517905,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_college_chemistry": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_college_computer_science": {
                "acc": 0.33,
                "acc_stderr": 0.047258156262526045,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_astronomy": {
                "acc": 0.26973684210526316,
                "acc_stderr": 0.03611780560284898,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_college_mathematics": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2851063829787234,
                "acc_stderr": 0.029513196625539345,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036845,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_machine_learning": {
                "acc": 0.2857142857142857,
                "acc_stderr": 0.04287858751340456,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.32019704433497537,
                "acc_stderr": 0.032826493853041504,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4537037037037037,
                "acc_stderr": 0.03395322726375797,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25396825396825395,
                "acc_stderr": 0.02241804289111394,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2896551724137931,
                "acc_stderr": 0.03780019230438014,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.27037037037037037,
                "acc_stderr": 0.027080372815145644,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "arc_challenge": {
                "acc": 0.43430034129692835,
                "acc_stderr": 0.014484703048857364,
                "acc_norm": 0.4616040955631399,
                "acc_norm_stderr": 0.01456824555029636,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "hellaswag": {
                "acc": 0.5726946823341964,
                "acc_stderr": 0.004936762568217072,
                "acc_norm": 0.7694682334196375,
                "acc_norm_stderr": 0.00420312448903718,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "truthfulqa_mc2": {
                "acc": 0.3585058314358672,
                "acc_stderr": 0.013802749984870202,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "truthfulqa_gen": {
                "bleu_max": 26.539400833287374,
                "bleu_max_stderr": 0.7779688490400837,
                "bleu_acc": 0.2937576499388005,
                "bleu_acc_stderr": 0.015945068581236614,
                "bleu_diff": -9.216867205020309,
                "bleu_diff_stderr": 0.7821192582719276,
                "rouge1_max": 51.946561103999834,
                "rouge1_max_stderr": 0.8300725318977166,
                "rouge1_acc": 0.31334149326805383,
                "rouge1_acc_stderr": 0.016238065069059605,
                "rouge1_diff": -11.3371350794366,
                "rouge1_diff_stderr": 0.8136895635471025,
                "rouge2_max": 36.413967516398216,
                "rouge2_max_stderr": 0.9679506184935703,
                "rouge2_acc": 0.2582619339045288,
                "rouge2_acc_stderr": 0.015321821688476187,
                "rouge2_diff": -13.565291787723291,
                "rouge2_diff_stderr": 1.009721378967521,
                "rougeL_max": 49.13417294832966,
                "rougeL_max_stderr": 0.8355931924931606,
                "rougeL_acc": 0.2937576499388005,
                "rougeL_acc_stderr": 0.015945068581236614,
                "rougeL_diff": -11.47267969384019,
                "rougeL_diff_stderr": 0.8120028631669012,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "truthfulqa_mc1": {
                "acc": 0.24479804161566707,
                "acc_stderr": 0.015051869486715,
                "timestamp": "2024-11-22T05-28-54.201828"
            },
            "winogrande": {
                "acc": 0.6953433307024467,
                "acc_stderr": 0.012935646499325312,
                "timestamp": "2024-11-22T05-28-54.201828"
            }
        }
    }
}