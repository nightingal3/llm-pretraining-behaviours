{
    "model_name": "facebook/xglm-2.9B",
    "last_updated": "2024-12-19 13:41:13.782953",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "minerva_math_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_3da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000061,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_3ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000116,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_2ds": {
                "acc": 0.0125,
                "acc_stderr": 0.00248494717876267,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_1dc": {
                "acc": 0.0025,
                "acc_stderr": 0.0011169148353275223,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_2dm": {
                "acc": 0.01,
                "acc_stderr": 0.002225415969682742,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "arithmetic_2da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000079,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "gsm8k_cot": {
                "exact_match": 0.024260803639120546,
                "exact_match_stderr": 0.004238007900001395,
                "timestamp": "2024-06-12T07-33-14.391506"
            },
            "gsm8k": {
                "exact_match": 0.015163002274450341,
                "exact_match_stderr": 0.0033660229497263377,
                "timestamp": "2024-11-20T02-09-39.233954"
            },
            "anli_r2": {
                "brier_score": 0.8903282853468429,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "anli_r3": {
                "brier_score": 0.8828546319214271,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "anli_r1": {
                "brier_score": 0.9090076784540243,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_eu": {
                "brier_score": 0.6945318464480982,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_vi": {
                "brier_score": 0.7198974352337865,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_ru": {
                "brier_score": 0.7940689223362921,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_zh": {
                "brier_score": 0.9662319296220725,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_tr": {
                "brier_score": 0.7985713408191671,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_fr": {
                "brier_score": 0.8777604442850733,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_en": {
                "brier_score": 0.6546699998277438,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_ur": {
                "brier_score": 0.9005216252993709,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_ar": {
                "brier_score": 1.2213926002441413,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_de": {
                "brier_score": 0.8098229929420445,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_hi": {
                "brier_score": 0.7612249719321225,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_es": {
                "brier_score": 0.8391711908916202,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_bg": {
                "brier_score": 0.7784193725622509,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_sw": {
                "brier_score": 0.752512116394542,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_el": {
                "brier_score": 0.8014402208550904,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "xnli_th": {
                "brier_score": 0.8209309976197092,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "logiqa2": {
                "brier_score": 1.0440698590170623,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "mathqa": {
                "brier_score": 1.0015941907404673,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T07-39-37.577568"
            },
            "lambada_standard": {
                "perplexity": 10.724228161680852,
                "perplexity_stderr": 0.3113611480259543,
                "acc": 0.5022317096836794,
                "acc_stderr": 0.006965908268351023,
                "timestamp": "2024-06-11T12-47-19.836663"
            },
            "lambada_openai": {
                "perplexity": 9.850349597657685,
                "perplexity_stderr": 0.28597160307157565,
                "acc": 0.49408111779545894,
                "acc_stderr": 0.0069654895595805955,
                "timestamp": "2024-06-11T12-47-19.836663"
            },
            "mmlu_world_religions": {
                "acc": 0.27485380116959063,
                "acc_stderr": 0.03424042924691584,
                "brier_score": 0.7784564619610387,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_formal_logic": {
                "acc": 0.29365079365079366,
                "acc_stderr": 0.04073524322147127,
                "brier_score": 0.7845963325351285,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_prehistory": {
                "acc": 0.2006172839506173,
                "acc_stderr": 0.022282313949774882,
                "brier_score": 0.7792786537846681,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.01424263007057489,
                "brier_score": 0.7949624902702473,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2869198312236287,
                "acc_stderr": 0.029443773022594693,
                "brier_score": 0.7594604098639599,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_moral_disputes": {
                "acc": 0.23410404624277456,
                "acc_stderr": 0.022797110278071134,
                "brier_score": 0.7971392662864681,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_professional_law": {
                "acc": 0.24902216427640156,
                "acc_stderr": 0.01104489226404077,
                "brier_score": 0.7638688880868297,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.24539877300613497,
                "acc_stderr": 0.03380939813943354,
                "brier_score": 0.8005802666734811,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.02977177522814563,
                "brier_score": 0.7712167399457017,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_philosophy": {
                "acc": 0.21221864951768488,
                "acc_stderr": 0.023222756797435126,
                "brier_score": 0.8130263770714379,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_jurisprudence": {
                "acc": 0.21296296296296297,
                "acc_stderr": 0.03957835471980977,
                "brier_score": 0.7672667124214475,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_international_law": {
                "acc": 0.24793388429752067,
                "acc_stderr": 0.03941897526516304,
                "brier_score": 0.7487661126399827,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.22424242424242424,
                "acc_stderr": 0.03256866661681102,
                "brier_score": 0.769036899521868,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21761658031088082,
                "acc_stderr": 0.02977866303775297,
                "brier_score": 0.8032216728797023,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.026653531596715487,
                "brier_score": 0.7999394859814347,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_geography": {
                "acc": 0.15151515151515152,
                "acc_stderr": 0.02554565042660363,
                "brier_score": 0.8162920169241648,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.22568807339449543,
                "acc_stderr": 0.017923087667803053,
                "brier_score": 0.8221072465700741,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_public_relations": {
                "acc": 0.2909090909090909,
                "acc_stderr": 0.04350271442923243,
                "brier_score": 0.7705212934293936,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "brier_score": 0.7728647597197729,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_sociology": {
                "acc": 0.22388059701492538,
                "acc_stderr": 0.02947525023601721,
                "brier_score": 0.8034163207123037,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.22564102564102564,
                "acc_stderr": 0.021193632525148526,
                "brier_score": 0.7978869228998441,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_security_studies": {
                "acc": 0.22040816326530613,
                "acc_stderr": 0.026537045312145287,
                "brier_score": 0.7820564396674529,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_professional_psychology": {
                "acc": 0.24509803921568626,
                "acc_stderr": 0.01740181671142766,
                "brier_score": 0.7721913891320545,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_human_sexuality": {
                "acc": 0.22900763358778625,
                "acc_stderr": 0.036853466317118506,
                "brier_score": 0.8245800855587376,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_econometrics": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.0414243971948936,
                "brier_score": 0.7891874971383447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_miscellaneous": {
                "acc": 0.25287356321839083,
                "acc_stderr": 0.015543377313719681,
                "brier_score": 0.7815964758465793,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_marketing": {
                "acc": 0.27350427350427353,
                "acc_stderr": 0.02920254015343117,
                "brier_score": 0.7637553614562805,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_management": {
                "acc": 0.2524271844660194,
                "acc_stderr": 0.043012503996908764,
                "brier_score": 0.8082520540534411,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_nutrition": {
                "acc": 0.23202614379084968,
                "acc_stderr": 0.024170840879341016,
                "brier_score": 0.8088553597902366,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_medical_genetics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "brier_score": 0.7579121532032091,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_human_aging": {
                "acc": 0.32286995515695066,
                "acc_stderr": 0.03138147637575498,
                "brier_score": 0.7320622941617274,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_professional_medicine": {
                "acc": 0.15441176470588236,
                "acc_stderr": 0.021950024722922023,
                "brier_score": 0.8381628978497615,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_college_medicine": {
                "acc": 0.1907514450867052,
                "acc_stderr": 0.029957851329869327,
                "brier_score": 0.8160227156496334,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_business_ethics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7699680125818698,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.20754716981132076,
                "acc_stderr": 0.024959918028911267,
                "brier_score": 0.8024709883934613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_global_facts": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.7755939405534948,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_virology": {
                "acc": 0.2891566265060241,
                "acc_stderr": 0.03529486801511115,
                "brier_score": 0.7675743285611036,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.026684564340460983,
                "brier_score": 0.7612596779770814,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_college_physics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.04023382273617747,
                "brier_score": 0.8147810206949243,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2052980132450331,
                "acc_stderr": 0.03297986648473834,
                "brier_score": 0.7850270619906705,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2032258064516129,
                "acc_stderr": 0.02289168798455495,
                "brier_score": 0.7879831869554097,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_college_biology": {
                "acc": 0.2569444444444444,
                "acc_stderr": 0.03653946969442099,
                "brier_score": 0.7667620151908258,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_anatomy": {
                "acc": 0.1925925925925926,
                "acc_stderr": 0.03406542058502653,
                "brier_score": 0.8159042810752004,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_college_chemistry": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653696,
                "brier_score": 0.8232613704210737,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_computer_security": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.7810317638069373,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_college_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816506,
                "brier_score": 0.7978820652141898,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_astronomy": {
                "acc": 0.18421052631578946,
                "acc_stderr": 0.0315469804508223,
                "brier_score": 0.8121393101079877,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_college_mathematics": {
                "acc": 0.22,
                "acc_stderr": 0.0416333199893227,
                "brier_score": 0.8056959732869093,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.26382978723404255,
                "acc_stderr": 0.028809989854102977,
                "brier_score": 0.7880471540031094,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.8258750816843782,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7564390065224648,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_machine_learning": {
                "acc": 0.3392857142857143,
                "acc_stderr": 0.04493949068613539,
                "brier_score": 0.7458255697862292,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.1921182266009852,
                "acc_stderr": 0.027719315709614778,
                "brier_score": 0.812417944584597,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.17592592592592593,
                "acc_stderr": 0.025967420958258526,
                "brier_score": 0.7912250369489995,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2566137566137566,
                "acc_stderr": 0.02249451076750315,
                "brier_score": 0.7938783769148282,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.03565998174135302,
                "brier_score": 0.8314571330920377,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24444444444444444,
                "acc_stderr": 0.02620276653465215,
                "brier_score": 0.7939871511609232,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-17.950634"
            },
            "arc_challenge": {
                "acc": 0.2764505119453925,
                "acc_stderr": 0.013069662474252427,
                "acc_norm": 0.30119453924914674,
                "acc_norm_stderr": 0.013406741767847632,
                "timestamp": "2024-11-20T02-09-39.233954"
            },
            "hellaswag": {
                "acc": 0.4106751643098984,
                "acc_stderr": 0.004909509538525162,
                "acc_norm": 0.5415255925114519,
                "acc_norm_stderr": 0.0049725431277678695,
                "timestamp": "2024-11-20T02-09-39.233954"
            },
            "truthfulqa_mc2": {
                "acc": 0.3582356473001071,
                "acc_stderr": 0.01375467497109615,
                "timestamp": "2024-11-20T02-09-39.233954"
            },
            "truthfulqa_gen": {
                "bleu_max": 3.6087558287053865,
                "bleu_max_stderr": 0.18670249668473277,
                "bleu_acc": 0.2631578947368421,
                "bleu_acc_stderr": 0.015415241740237002,
                "bleu_diff": -1.3731184892287656,
                "bleu_diff_stderr": 0.14371374046717808,
                "rouge1_max": 13.419021251081578,
                "rouge1_max_stderr": 0.3823807582971274,
                "rouge1_acc": 0.29498164014687883,
                "rouge1_acc_stderr": 0.015964400965589633,
                "rouge1_diff": -2.1593394128613332,
                "rouge1_diff_stderr": 0.22093851799676853,
                "rouge2_max": 7.604441562464007,
                "rouge2_max_stderr": 0.3269185048983413,
                "rouge2_acc": 0.19951040391676866,
                "rouge2_acc_stderr": 0.013989929967559657,
                "rouge2_diff": -2.7368765900678316,
                "rouge2_diff_stderr": 0.24573293145782651,
                "rougeL_max": 12.140731662361581,
                "rougeL_max_stderr": 0.3609507196982488,
                "rougeL_acc": 0.29008567931456547,
                "rougeL_acc_stderr": 0.015886236874209515,
                "rougeL_diff": -2.3125797772891734,
                "rougeL_diff_stderr": 0.22079372238444525,
                "timestamp": "2024-11-20T02-09-39.233954"
            },
            "truthfulqa_mc1": {
                "acc": 0.2141982864137087,
                "acc_stderr": 0.01436214815569047,
                "timestamp": "2024-11-20T02-09-39.233954"
            },
            "winogrande": {
                "acc": 0.5588003157063931,
                "acc_stderr": 0.013954975072834734,
                "timestamp": "2024-11-20T02-09-39.233954"
            }
        }
    }
}