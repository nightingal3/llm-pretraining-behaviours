{
    "model_name": "mosaicml/mpt-30b-instruct",
    "last_updated": "2023-10-14",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.5426621160409556,
                    "acc_stderr": 0.014558106543924067,
                    "acc_norm": 0.5844709897610921,
                    "acc_norm_stderr": 0.014401366641216384,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.652459669388568,
                    "acc_stderr": 0.004752158936871872,
                    "acc_norm": 0.8430591515634336,
                    "acc_norm_stderr": 0.0036300159898964026,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252606,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252606,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.4222222222222222,
                    "acc_stderr": 0.04266763404099582,
                    "acc_norm": 0.4222222222222222,
                    "acc_norm_stderr": 0.04266763404099582,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.04068942293855797,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.04068942293855797,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.050251890762960605,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.050251890762960605,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.5169811320754717,
                    "acc_stderr": 0.030755120364119905,
                    "acc_norm": 0.5169811320754717,
                    "acc_norm_stderr": 0.030755120364119905,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.5347222222222222,
                    "acc_stderr": 0.04171115858181618,
                    "acc_norm": 0.5347222222222222,
                    "acc_norm_stderr": 0.04171115858181618,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621503,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621503,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.47,
                    "acc_stderr": 0.050161355804659205,
                    "acc_norm": 0.47,
                    "acc_norm_stderr": 0.050161355804659205,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.4508670520231214,
                    "acc_stderr": 0.03794012674697028,
                    "acc_norm": 0.4508670520231214,
                    "acc_norm_stderr": 0.03794012674697028,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.35294117647058826,
                    "acc_stderr": 0.047551296160629475,
                    "acc_norm": 0.35294117647058826,
                    "acc_norm_stderr": 0.047551296160629475,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.64,
                    "acc_stderr": 0.04824181513244218,
                    "acc_norm": 0.64,
                    "acc_norm_stderr": 0.04824181513244218,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.46382978723404256,
                    "acc_stderr": 0.032600385118357715,
                    "acc_norm": 0.46382978723404256,
                    "acc_norm_stderr": 0.032600385118357715,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2894736842105263,
                    "acc_stderr": 0.04266339443159394,
                    "acc_norm": 0.2894736842105263,
                    "acc_norm_stderr": 0.04266339443159394,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.46206896551724136,
                    "acc_stderr": 0.041546596717075474,
                    "acc_norm": 0.46206896551724136,
                    "acc_norm_stderr": 0.041546596717075474,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.3201058201058201,
                    "acc_stderr": 0.0240268463928735,
                    "acc_norm": 0.3201058201058201,
                    "acc_norm_stderr": 0.0240268463928735,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.3492063492063492,
                    "acc_stderr": 0.04263906892795132,
                    "acc_norm": 0.3492063492063492,
                    "acc_norm_stderr": 0.04263906892795132,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.532258064516129,
                    "acc_stderr": 0.028384747788813332,
                    "acc_norm": 0.532258064516129,
                    "acc_norm_stderr": 0.028384747788813332,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.3793103448275862,
                    "acc_stderr": 0.03413963805906235,
                    "acc_norm": 0.3793103448275862,
                    "acc_norm_stderr": 0.03413963805906235,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.51,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.51,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.6484848484848484,
                    "acc_stderr": 0.037282069986826503,
                    "acc_norm": 0.6484848484848484,
                    "acc_norm_stderr": 0.037282069986826503,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.6464646464646465,
                    "acc_stderr": 0.03406086723547153,
                    "acc_norm": 0.6464646464646465,
                    "acc_norm_stderr": 0.03406086723547153,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.6735751295336787,
                    "acc_stderr": 0.033840286211432945,
                    "acc_norm": 0.6735751295336787,
                    "acc_norm_stderr": 0.033840286211432945,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.45897435897435895,
                    "acc_stderr": 0.025265525491284295,
                    "acc_norm": 0.45897435897435895,
                    "acc_norm_stderr": 0.025265525491284295,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.3296296296296296,
                    "acc_stderr": 0.028661201116524575,
                    "acc_norm": 0.3296296296296296,
                    "acc_norm_stderr": 0.028661201116524575,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.44537815126050423,
                    "acc_stderr": 0.0322841062671639,
                    "acc_norm": 0.44537815126050423,
                    "acc_norm_stderr": 0.0322841062671639,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.32450331125827814,
                    "acc_stderr": 0.038227469376587525,
                    "acc_norm": 0.32450331125827814,
                    "acc_norm_stderr": 0.038227469376587525,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.6587155963302752,
                    "acc_stderr": 0.020328612816592446,
                    "acc_norm": 0.6587155963302752,
                    "acc_norm_stderr": 0.020328612816592446,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.2916666666666667,
                    "acc_stderr": 0.03099866630456053,
                    "acc_norm": 0.2916666666666667,
                    "acc_norm_stderr": 0.03099866630456053,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.7205882352941176,
                    "acc_stderr": 0.031493281045079556,
                    "acc_norm": 0.7205882352941176,
                    "acc_norm_stderr": 0.031493281045079556,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.6877637130801688,
                    "acc_stderr": 0.030165137867847015,
                    "acc_norm": 0.6877637130801688,
                    "acc_norm_stderr": 0.030165137867847015,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.5919282511210763,
                    "acc_stderr": 0.03298574607842822,
                    "acc_norm": 0.5919282511210763,
                    "acc_norm_stderr": 0.03298574607842822,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.5877862595419847,
                    "acc_stderr": 0.04317171194870254,
                    "acc_norm": 0.5877862595419847,
                    "acc_norm_stderr": 0.04317171194870254,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.4462809917355372,
                    "acc_stderr": 0.0453793517794788,
                    "acc_norm": 0.4462809917355372,
                    "acc_norm_stderr": 0.0453793517794788,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.5740740740740741,
                    "acc_stderr": 0.047803436269367894,
                    "acc_norm": 0.5740740740740741,
                    "acc_norm_stderr": 0.047803436269367894,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.49079754601226994,
                    "acc_stderr": 0.039277056007874414,
                    "acc_norm": 0.49079754601226994,
                    "acc_norm_stderr": 0.039277056007874414,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.375,
                    "acc_stderr": 0.04595091388086298,
                    "acc_norm": 0.375,
                    "acc_norm_stderr": 0.04595091388086298,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.6116504854368932,
                    "acc_stderr": 0.0482572933735639,
                    "acc_norm": 0.6116504854368932,
                    "acc_norm_stderr": 0.0482572933735639,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.7051282051282052,
                    "acc_stderr": 0.02987257770889119,
                    "acc_norm": 0.7051282051282052,
                    "acc_norm_stderr": 0.02987257770889119,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.46,
                    "acc_stderr": 0.05009082659620333,
                    "acc_norm": 0.46,
                    "acc_norm_stderr": 0.05009082659620333,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.6922094508301405,
                    "acc_stderr": 0.01650604504515564,
                    "acc_norm": 0.6922094508301405,
                    "acc_norm_stderr": 0.01650604504515564,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.49710982658959535,
                    "acc_stderr": 0.026918645383239015,
                    "acc_norm": 0.49710982658959535,
                    "acc_norm_stderr": 0.026918645383239015,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.3027932960893855,
                    "acc_stderr": 0.015366860386397112,
                    "acc_norm": 0.3027932960893855,
                    "acc_norm_stderr": 0.015366860386397112,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.5163398692810458,
                    "acc_stderr": 0.028614624752805434,
                    "acc_norm": 0.5163398692810458,
                    "acc_norm_stderr": 0.028614624752805434,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.5659163987138264,
                    "acc_stderr": 0.028150232244535594,
                    "acc_norm": 0.5659163987138264,
                    "acc_norm_stderr": 0.028150232244535594,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.5771604938271605,
                    "acc_stderr": 0.027487472980871588,
                    "acc_norm": 0.5771604938271605,
                    "acc_norm_stderr": 0.027487472980871588,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.3900709219858156,
                    "acc_stderr": 0.02909767559946393,
                    "acc_norm": 0.3900709219858156,
                    "acc_norm_stderr": 0.02909767559946393,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.37027379400260757,
                    "acc_stderr": 0.012332930781256728,
                    "acc_norm": 0.37027379400260757,
                    "acc_norm_stderr": 0.012332930781256728,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4485294117647059,
                    "acc_stderr": 0.030211479609121596,
                    "acc_norm": 0.4485294117647059,
                    "acc_norm_stderr": 0.030211479609121596,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.49019607843137253,
                    "acc_stderr": 0.020223946005074305,
                    "acc_norm": 0.49019607843137253,
                    "acc_norm_stderr": 0.020223946005074305,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.5181818181818182,
                    "acc_stderr": 0.04785964010794916,
                    "acc_norm": 0.5181818181818182,
                    "acc_norm_stderr": 0.04785964010794916,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.563265306122449,
                    "acc_stderr": 0.031751952375833226,
                    "acc_norm": 0.563265306122449,
                    "acc_norm_stderr": 0.031751952375833226,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.5522388059701493,
                    "acc_stderr": 0.03516184772952167,
                    "acc_norm": 0.5522388059701493,
                    "acc_norm_stderr": 0.03516184772952167,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.69,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.69,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.463855421686747,
                    "acc_stderr": 0.03882310850890593,
                    "acc_norm": 0.463855421686747,
                    "acc_norm_stderr": 0.03882310850890593,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.6666666666666666,
                    "acc_stderr": 0.03615507630310935,
                    "acc_norm": 0.6666666666666666,
                    "acc_norm_stderr": 0.03615507630310935,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2582619339045288,
                    "mc1_stderr": 0.0153218216884762,
                    "mc2": 0.3804643219400445,
                    "mc2_stderr": 0.015216520266283156,
                    "timestamp": "2023-07-20T13-11-24.937399"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.3308515100671141,
                    "acc_stderr": 0.004818562129043009,
                    "f1": 0.38283766778523554,
                    "f1_stderr": 0.00472140525052066,
                    "timestamp": "2023-10-14T20-57-09.846204"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.15314632297194844,
                    "acc_stderr": 0.009919728152791466,
                    "timestamp": "2023-10-14T20-57-09.846204"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7513812154696132,
                    "acc_stderr": 0.012147314713403112,
                    "timestamp": "2023-10-14T20-57-09.846204"
                }
            }
        }
    }
}