{
    "model_name": "facebook/opt-2.7b",
    "last_updated": "2024-12-19 13:42:00.789242",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.023809523809523808,
                "exact_match_stderr": 0.006530469219761482,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.014925373134328358,
                "exact_match_stderr": 0.004110905928505601,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.005203704987512652,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0221483942414175,
                "exact_match_stderr": 0.00490009308861579,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "minerva_math_geometry": {
                "exact_match": 0.012526096033402923,
                "exact_match_stderr": 0.005086941389677978,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.010548523206751054,
                "exact_match_stderr": 0.004697453735376162,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "minerva_math_algebra": {
                "exact_match": 0.015164279696714406,
                "exact_match_stderr": 0.0035485460431325523,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521438,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_3ds": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339508,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_2ds": {
                "acc": 0.0135,
                "acc_stderr": 0.002581124968507327,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_1dc": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339562,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_2dm": {
                "acc": 0.0215,
                "acc_stderr": 0.0032440926417928078,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "arithmetic_2da": {
                "acc": 0.011,
                "acc_stderr": 0.002332856855993375,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "gsm8k_cot": {
                "exact_match": 0.02122820318423048,
                "exact_match_stderr": 0.003970449129848635,
                "timestamp": "2024-06-06T10-42-41.005082"
            },
            "gsm8k": {
                "exact_match": 0.021986353297952996,
                "exact_match_stderr": 0.004039162758110046,
                "timestamp": "2024-11-21T14-28-39.469398"
            },
            "anli_r2": {
                "brier_score": 0.9806575981542668,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "anli_r3": {
                "brier_score": 0.8989579823804109,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "anli_r1": {
                "brier_score": 0.9835816196646929,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_eu": {
                "brier_score": 1.0610127557895535,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_vi": {
                "brier_score": 0.9078406353013023,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_ru": {
                "brier_score": 0.8321734481428278,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_zh": {
                "brier_score": 1.1784647036176363,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_tr": {
                "brier_score": 0.8894841423618519,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_fr": {
                "brier_score": 0.8313152226484499,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_en": {
                "brier_score": 0.6116601747186954,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_ur": {
                "brier_score": 1.322704035007194,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_ar": {
                "brier_score": 0.9110827039215548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_de": {
                "brier_score": 0.8552219959870653,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_hi": {
                "brier_score": 0.9737996476054792,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_es": {
                "brier_score": 0.8705426231888244,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_bg": {
                "brier_score": 0.9735045902629904,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_sw": {
                "brier_score": 0.8732903791230823,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_el": {
                "brier_score": 1.1872540651461676,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "xnli_th": {
                "brier_score": 1.2687537528892334,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "logiqa2": {
                "brier_score": 1.2134396565953744,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "mathqa": {
                "brier_score": 0.9929214264531077,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T12-04-48.897848"
            },
            "lambada_standard": {
                "perplexity": 7.415927761731489,
                "perplexity_stderr": 0.19675668972521876,
                "acc": 0.5592858529012226,
                "acc_stderr": 0.0069168361138352225,
                "timestamp": "2024-06-06T12-08-58.689654"
            },
            "lambada_openai": {
                "perplexity": 5.119225735809997,
                "perplexity_stderr": 0.119744800718221,
                "acc": 0.6353580438579468,
                "acc_stderr": 0.0067058627120832725,
                "timestamp": "2024-06-06T12-08-58.689654"
            },
            "mmlu_world_religions": {
                "acc": 0.23391812865497075,
                "acc_stderr": 0.03246721765117828,
                "brier_score": 0.7878935837815728,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.03670066451047182,
                "brier_score": 0.7677482439006217,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_prehistory": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.025407197798890155,
                "brier_score": 0.7564602866799459,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.25139664804469275,
                "acc_stderr": 0.01450897945355397,
                "brier_score": 0.7658966744392373,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25738396624472576,
                "acc_stderr": 0.028458820991460288,
                "brier_score": 0.7797932422503456,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2861271676300578,
                "acc_stderr": 0.02433214677913413,
                "brier_score": 0.7667677037603893,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_professional_law": {
                "acc": 0.2516297262059974,
                "acc_stderr": 0.011083276280441905,
                "brier_score": 0.7707280847501268,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.3496932515337423,
                "acc_stderr": 0.03746668325470023,
                "brier_score": 0.7615028914354622,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2696078431372549,
                "acc_stderr": 0.031145570659486782,
                "brier_score": 0.766031691820315,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_philosophy": {
                "acc": 0.3054662379421222,
                "acc_stderr": 0.026160584450140485,
                "brier_score": 0.7496992228361217,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_jurisprudence": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.04133119440243839,
                "brier_score": 0.7733523691001012,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_international_law": {
                "acc": 0.34710743801652894,
                "acc_stderr": 0.04345724570292534,
                "brier_score": 0.7423423252676877,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.2787878787878788,
                "acc_stderr": 0.03501438706296781,
                "brier_score": 0.7671193652171602,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.3005181347150259,
                "acc_stderr": 0.0330881859441575,
                "brier_score": 0.754098993058133,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.24789915966386555,
                "acc_stderr": 0.028047967224176896,
                "brier_score": 0.7569618511834054,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_geography": {
                "acc": 0.2474747474747475,
                "acc_stderr": 0.03074630074212452,
                "brier_score": 0.7602981396993943,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.22752293577981653,
                "acc_stderr": 0.017974463578776502,
                "brier_score": 0.7592180999483095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_public_relations": {
                "acc": 0.24545454545454545,
                "acc_stderr": 0.041220665028782855,
                "brier_score": 0.7570539380574031,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.7705241951464472,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_sociology": {
                "acc": 0.25870646766169153,
                "acc_stderr": 0.030965903123573026,
                "brier_score": 0.7533474420589423,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.22564102564102564,
                "acc_stderr": 0.02119363252514854,
                "brier_score": 0.7593425818521468,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_security_studies": {
                "acc": 0.2653061224489796,
                "acc_stderr": 0.028263889943784603,
                "brier_score": 0.7580450235688722,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.0177408995091778,
                "brier_score": 0.7616363188926287,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_human_sexuality": {
                "acc": 0.24427480916030533,
                "acc_stderr": 0.03768335959728744,
                "brier_score": 0.7711793914236916,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_econometrics": {
                "acc": 0.21052631578947367,
                "acc_stderr": 0.03835153954399421,
                "brier_score": 0.767892062589693,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_miscellaneous": {
                "acc": 0.24904214559386972,
                "acc_stderr": 0.015464676163395951,
                "brier_score": 0.7727186122170707,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_marketing": {
                "acc": 0.2692307692307692,
                "acc_stderr": 0.029058588303748842,
                "brier_score": 0.7682489849238419,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_management": {
                "acc": 0.2524271844660194,
                "acc_stderr": 0.04301250399690878,
                "brier_score": 0.7675660008154402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_nutrition": {
                "acc": 0.25163398692810457,
                "acc_stderr": 0.024848018263875195,
                "brier_score": 0.7631766135670024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_medical_genetics": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.7673701717101398,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_human_aging": {
                "acc": 0.19730941704035873,
                "acc_stderr": 0.02670985334496796,
                "brier_score": 0.8047248026269651,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_professional_medicine": {
                "acc": 0.16544117647058823,
                "acc_stderr": 0.022571771025494757,
                "brier_score": 0.7947207604837336,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_college_medicine": {
                "acc": 0.24855491329479767,
                "acc_stderr": 0.03295304696818318,
                "brier_score": 0.7645409283951206,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_business_ethics": {
                "acc": 0.27,
                "acc_stderr": 0.044619604333847394,
                "brier_score": 0.767232076633074,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.23018867924528302,
                "acc_stderr": 0.02590789712240817,
                "brier_score": 0.7653957956215449,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_global_facts": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.7629692025072817,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_virology": {
                "acc": 0.22289156626506024,
                "acc_stderr": 0.03240004825594686,
                "brier_score": 0.7743406704718713,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_professional_accounting": {
                "acc": 0.26595744680851063,
                "acc_stderr": 0.02635806569888059,
                "brier_score": 0.7646076852898195,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.04092563958237656,
                "brier_score": 0.7529136910088495,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_physics": {
                "acc": 0.26490066225165565,
                "acc_stderr": 0.036030385453603826,
                "brier_score": 0.739006262173674,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_biology": {
                "acc": 0.25483870967741934,
                "acc_stderr": 0.024790118459332208,
                "brier_score": 0.7589112207960926,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_college_biology": {
                "acc": 0.2013888888888889,
                "acc_stderr": 0.03353647469713841,
                "brier_score": 0.7666553641033335,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_anatomy": {
                "acc": 0.32592592592592595,
                "acc_stderr": 0.040491220417025055,
                "brier_score": 0.7317195380722945,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_college_chemistry": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.7765942042217988,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_computer_security": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7561788166964886,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_college_computer_science": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695236,
                "brier_score": 0.7529124410986495,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_astronomy": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.03459777606810537,
                "brier_score": 0.7606069163529254,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_college_mathematics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542129,
                "brier_score": 0.7567010659399342,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.225531914893617,
                "acc_stderr": 0.02732107841738753,
                "brier_score": 0.7863535098918605,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768079,
                "brier_score": 0.7497993014925278,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "brier_score": 0.7427906928398942,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_machine_learning": {
                "acc": 0.25,
                "acc_stderr": 0.04109974682633932,
                "brier_score": 0.7688521974534093,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.3251231527093596,
                "acc_stderr": 0.03295797566311271,
                "brier_score": 0.7415130211082905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.03005820270430985,
                "brier_score": 0.747827956637948,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.23809523809523808,
                "acc_stderr": 0.021935878081184756,
                "brier_score": 0.7646395040474879,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2827586206896552,
                "acc_stderr": 0.03752833958003336,
                "brier_score": 0.7630012012413374,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.3037037037037037,
                "acc_stderr": 0.02803792996911499,
                "brier_score": 0.7425845374832011,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-48-44.848780"
            },
            "arc_challenge": {
                "acc": 0.31399317406143346,
                "acc_stderr": 0.01356269122472629,
                "acc_norm": 0.3455631399317406,
                "acc_norm_stderr": 0.01389693846114568,
                "timestamp": "2024-11-21T14-28-39.469398"
            },
            "hellaswag": {
                "acc": 0.4598685520812587,
                "acc_stderr": 0.004973683026202175,
                "acc_norm": 0.6177056363274248,
                "acc_norm_stderr": 0.004849547819134493,
                "timestamp": "2024-11-21T14-28-39.469398"
            },
            "truthfulqa_mc2": {
                "acc": 0.3763787086134506,
                "acc_stderr": 0.013809514742722533,
                "timestamp": "2024-11-21T14-28-39.469398"
            },
            "truthfulqa_gen": {
                "bleu_max": 21.936813599784074,
                "bleu_max_stderr": 0.6915463833520875,
                "bleu_acc": 0.2827417380660955,
                "bleu_acc_stderr": 0.01576477083677731,
                "bleu_diff": -7.93706461416589,
                "bleu_diff_stderr": 0.6942322145010175,
                "rouge1_max": 46.56471474176105,
                "rouge1_max_stderr": 0.8452800285319477,
                "rouge1_acc": 0.26193390452876375,
                "rouge1_acc_stderr": 0.015392118805015044,
                "rouge1_diff": -10.43421160381809,
                "rouge1_diff_stderr": 0.7865219397875223,
                "rouge2_max": 29.59058433337149,
                "rouge2_max_stderr": 0.9351396896455382,
                "rouge2_acc": 0.19216646266829865,
                "rouge2_acc_stderr": 0.013792870480628964,
                "rouge2_diff": -12.224344822664076,
                "rouge2_diff_stderr": 0.8785968206293998,
                "rougeL_max": 43.69031715340036,
                "rougeL_max_stderr": 0.8461881443382573,
                "rougeL_acc": 0.24724602203182375,
                "rougeL_acc_stderr": 0.015102404797359654,
                "rougeL_diff": -10.797286242336181,
                "rougeL_diff_stderr": 0.7775003821253601,
                "timestamp": "2024-11-21T14-28-39.469398"
            },
            "truthfulqa_mc1": {
                "acc": 0.22399020807833536,
                "acc_stderr": 0.014594964329474205,
                "timestamp": "2024-11-21T14-28-39.469398"
            },
            "winogrande": {
                "acc": 0.6164167324388319,
                "acc_stderr": 0.013666275889539017,
                "timestamp": "2024-11-21T14-28-39.469398"
            }
        }
    }
}