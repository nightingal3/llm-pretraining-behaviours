{
    "model_name": "froggeric/WestLake-10.7B-v2",
    "last_updated": "2024-12-04 11:24:58.976932",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.038461538461538464,
                "exact_match_stderr": 0.008237556478425377,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.27898966704936856,
                "exact_match_stderr": 0.015205656567297112,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.08148148148148149,
                "exact_match_stderr": 0.011783628281121663,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.03765227021040975,
                "exact_match_stderr": 0.006338088917730661,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "minerva_math_geometry": {
                "exact_match": 0.081419624217119,
                "exact_match_stderr": 0.012508613685688272,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0970464135021097,
                "exact_match_stderr": 0.013611058115372832,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "minerva_math_algebra": {
                "exact_match": 0.1979780960404381,
                "exact_match_stderr": 0.011570692228998754,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "fld_default": {
                "exact_match": 0.0002,
                "exact_match_stderr": 0.000199999999999999,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "fld_star": {
                "exact_match": 0.0002,
                "exact_match_stderr": 0.00019999999999999963,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_3da": {
                "acc": 0.954,
                "acc_stderr": 0.004685400355171849,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_3ds": {
                "acc": 0.975,
                "acc_stderr": 0.0034919331033682467,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_4da": {
                "acc": 0.8725,
                "acc_stderr": 0.007459872643009688,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_2ds": {
                "acc": 0.99,
                "acc_stderr": 0.0022254159696827483,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_5ds": {
                "acc": 0.8575,
                "acc_stderr": 0.007818403847292685,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_5da": {
                "acc": 0.81,
                "acc_stderr": 0.008774308761784195,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_1dc": {
                "acc": 0.6025,
                "acc_stderr": 0.010945628277499658,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_4ds": {
                "acc": 0.92,
                "acc_stderr": 0.006067817499282812,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_2dm": {
                "acc": 0.5035,
                "acc_stderr": 0.011182862030875628,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "arithmetic_2da": {
                "acc": 0.982,
                "acc_stderr": 0.0029736208922129122,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "gsm8k_cot": {
                "exact_match": 0.6550416982562547,
                "exact_match_stderr": 0.013093630133666224,
                "timestamp": "2024-06-10T16-39-52.124832"
            },
            "gsm8k": {
                "exact_match": 0.6209249431387415,
                "exact_match_stderr": 0.013363630295088347,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "anli_r2": {
                "brier_score": 0.8472615638756087,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "anli_r3": {
                "brier_score": 0.7853073888110543,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "anli_r1": {
                "brier_score": 0.6792771671674599,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_eu": {
                "brier_score": 1.065526750088975,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_vi": {
                "brier_score": 1.0439012064803812,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_ru": {
                "brier_score": 0.9928432729414078,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_zh": {
                "brier_score": 1.1708548798400407,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_tr": {
                "brier_score": 1.0415841664185104,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_fr": {
                "brier_score": 1.0902927110654848,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_en": {
                "brier_score": 0.7999870468682789,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_ur": {
                "brier_score": 1.203193103469732,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_ar": {
                "brier_score": 1.218269698711459,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_de": {
                "brier_score": 0.9566540615741678,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_hi": {
                "brier_score": 0.9931017430900577,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_es": {
                "brier_score": 1.0462014404303681,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_bg": {
                "brier_score": 0.9533770617239662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_sw": {
                "brier_score": 1.039675312596033,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_el": {
                "brier_score": 0.9991586293399103,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "xnli_th": {
                "brier_score": 1.2343337415593545,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "logiqa2": {
                "brier_score": 0.9135478490056801,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "mathqa": {
                "brier_score": 1.0613930870257773,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T16-53-58.666177"
            },
            "lambada_standard": {
                "perplexity": 7.47557563678433,
                "perplexity_stderr": 0.24808836554648117,
                "acc": 0.5486124587618862,
                "acc_stderr": 0.006932975888368619,
                "timestamp": "2024-06-10T16-55-51.282667"
            },
            "lambada_openai": {
                "perplexity": 5.457072232577165,
                "perplexity_stderr": 0.16173449273656423,
                "acc": 0.6037259848631865,
                "acc_stderr": 0.006814434238262821,
                "timestamp": "2024-06-10T16-55-51.282667"
            },
            "mmlu_world_religions": {
                "acc": 0.8070175438596491,
                "acc_stderr": 0.030267457554898465,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_formal_logic": {
                "acc": 0.4523809523809524,
                "acc_stderr": 0.044518079590553275,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_prehistory": {
                "acc": 0.7376543209876543,
                "acc_stderr": 0.024477222856135104,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.33519553072625696,
                "acc_stderr": 0.015788007190185888,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.810126582278481,
                "acc_stderr": 0.025530100460233497,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_moral_disputes": {
                "acc": 0.7052023121387283,
                "acc_stderr": 0.024547617794803838,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_professional_law": {
                "acc": 0.47979139504563234,
                "acc_stderr": 0.012759801427767562,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.7484662576687117,
                "acc_stderr": 0.034089978868575295,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.7941176470588235,
                "acc_stderr": 0.028379449451588667,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_philosophy": {
                "acc": 0.684887459807074,
                "acc_stderr": 0.026385273703464496,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_jurisprudence": {
                "acc": 0.8055555555555556,
                "acc_stderr": 0.03826076324884864,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_international_law": {
                "acc": 0.7768595041322314,
                "acc_stderr": 0.03800754475228733,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.7575757575757576,
                "acc_stderr": 0.03346409881055953,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.8911917098445595,
                "acc_stderr": 0.022473253332768763,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.6554621848739496,
                "acc_stderr": 0.030868682604121626,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_geography": {
                "acc": 0.7575757575757576,
                "acc_stderr": 0.030532892233932026,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.8348623853211009,
                "acc_stderr": 0.015919557829976037,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_public_relations": {
                "acc": 0.6636363636363637,
                "acc_stderr": 0.04525393596302506,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.86,
                "acc_stderr": 0.03487350880197769,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_sociology": {
                "acc": 0.8407960199004975,
                "acc_stderr": 0.02587064676616914,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.6384615384615384,
                "acc_stderr": 0.024359581465396987,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_security_studies": {
                "acc": 0.710204081632653,
                "acc_stderr": 0.02904308868330434,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_professional_psychology": {
                "acc": 0.6928104575163399,
                "acc_stderr": 0.01866335967146366,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_human_sexuality": {
                "acc": 0.732824427480916,
                "acc_stderr": 0.03880848301082396,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_econometrics": {
                "acc": 0.5087719298245614,
                "acc_stderr": 0.04702880432049615,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_miscellaneous": {
                "acc": 0.8186462324393359,
                "acc_stderr": 0.013778693778464102,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_marketing": {
                "acc": 0.8717948717948718,
                "acc_stderr": 0.02190190511507333,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_management": {
                "acc": 0.7572815533980582,
                "acc_stderr": 0.04245022486384495,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_nutrition": {
                "acc": 0.6830065359477124,
                "acc_stderr": 0.02664327847450875,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_medical_genetics": {
                "acc": 0.77,
                "acc_stderr": 0.04229525846816505,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_human_aging": {
                "acc": 0.7309417040358744,
                "acc_stderr": 0.029763779406874975,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_professional_medicine": {
                "acc": 0.6580882352941176,
                "acc_stderr": 0.028814722422254174,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_college_medicine": {
                "acc": 0.6416184971098265,
                "acc_stderr": 0.036563436533531606,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_business_ethics": {
                "acc": 0.67,
                "acc_stderr": 0.04725815626252609,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.6641509433962264,
                "acc_stderr": 0.029067220146644826,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_global_facts": {
                "acc": 0.4,
                "acc_stderr": 0.04923659639173309,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_virology": {
                "acc": 0.5301204819277109,
                "acc_stderr": 0.03885425420866768,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_professional_accounting": {
                "acc": 0.475177304964539,
                "acc_stderr": 0.029790719243829714,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_college_physics": {
                "acc": 0.37254901960784315,
                "acc_stderr": 0.04810840148082633,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_physics": {
                "acc": 0.40397350993377484,
                "acc_stderr": 0.040064856853653415,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_biology": {
                "acc": 0.7677419354838709,
                "acc_stderr": 0.024022256130308235,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_college_biology": {
                "acc": 0.7222222222222222,
                "acc_stderr": 0.037455547914624555,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_anatomy": {
                "acc": 0.562962962962963,
                "acc_stderr": 0.04284958639753401,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_college_chemistry": {
                "acc": 0.43,
                "acc_stderr": 0.04975698519562429,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_computer_security": {
                "acc": 0.73,
                "acc_stderr": 0.044619604333847394,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_college_computer_science": {
                "acc": 0.57,
                "acc_stderr": 0.049756985195624284,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_astronomy": {
                "acc": 0.7039473684210527,
                "acc_stderr": 0.037150621549989056,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_college_mathematics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542128,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.574468085106383,
                "acc_stderr": 0.03232146916224468,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695235,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.69,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_machine_learning": {
                "acc": 0.5,
                "acc_stderr": 0.04745789978762494,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.5073891625615764,
                "acc_stderr": 0.035176035403610105,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.5185185185185185,
                "acc_stderr": 0.03407632093854051,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.4312169312169312,
                "acc_stderr": 0.025506481698138204,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.5655172413793104,
                "acc_stderr": 0.041307408795554966,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.028742040903948482,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "arc_challenge": {
                "acc": 0.6672354948805461,
                "acc_stderr": 0.013769863046192307,
                "acc_norm": 0.6953924914675768,
                "acc_norm_stderr": 0.01344952210993249,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "hellaswag": {
                "acc": 0.7130053774148576,
                "acc_stderr": 0.004514345547780335,
                "acc_norm": 0.8808006373232424,
                "acc_norm_stderr": 0.0032336074238900206,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "truthfulqa_mc2": {
                "acc": 0.6502221311834233,
                "acc_stderr": 0.015728866880290463,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "truthfulqa_gen": {
                "bleu_max": 10.118772746100687,
                "bleu_max_stderr": 0.4033244046848448,
                "bleu_acc": 0.4602203182374541,
                "bleu_acc_stderr": 0.017448017223960884,
                "bleu_diff": 0.23678007300279724,
                "bleu_diff_stderr": 0.25435457707661413,
                "rouge1_max": 32.93967882301825,
                "rouge1_max_stderr": 0.6250936418056136,
                "rouge1_acc": 0.4883720930232558,
                "rouge1_acc_stderr": 0.017498767175740088,
                "rouge1_diff": 0.3499431820705031,
                "rouge1_diff_stderr": 0.43627397939161733,
                "rouge2_max": 18.885790230324695,
                "rouge2_max_stderr": 0.621447224079988,
                "rouge2_acc": 0.38310893512851896,
                "rouge2_acc_stderr": 0.017018461679389862,
                "rouge2_diff": -0.4377782614406173,
                "rouge2_diff_stderr": 0.4605252038625926,
                "rougeL_max": 28.92110479821841,
                "rougeL_max_stderr": 0.6163965826709437,
                "rougeL_acc": 0.46266829865361075,
                "rougeL_acc_stderr": 0.017454645150970588,
                "rougeL_diff": -0.21060189672916219,
                "rougeL_diff_stderr": 0.4195858986205983,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "truthfulqa_mc1": {
                "acc": 0.5116279069767442,
                "acc_stderr": 0.017498767175740084,
                "timestamp": "2024-11-26T00-08-52.629400"
            },
            "winogrande": {
                "acc": 0.8634569850039463,
                "acc_stderr": 0.009650242900291605,
                "timestamp": "2024-11-26T00-08-52.629400"
            }
        }
    }
}