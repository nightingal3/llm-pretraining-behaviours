{
    "model_name": "jisukim8873/falcon-7B-case-5",
    "last_updated": "2024-03-04",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.4462457337883959,
                    "acc_stderr": 0.014526705548539982,
                    "acc_norm": 0.48378839590443684,
                    "acc_norm_stderr": 0.014603708567414947,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5970922127066322,
                    "acc_stderr": 0.004894801119898607,
                    "acc_norm": 0.7851025692093209,
                    "acc_norm_stderr": 0.004099117122280895,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695236,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.04760952285695236,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.31851851851851853,
                    "acc_stderr": 0.040247784019771096,
                    "acc_norm": 0.31851851851851853,
                    "acc_norm_stderr": 0.040247784019771096,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.23684210526315788,
                    "acc_stderr": 0.03459777606810536,
                    "acc_norm": 0.23684210526315788,
                    "acc_norm_stderr": 0.03459777606810536,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.039427724440366234,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.039427724440366234,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.3169811320754717,
                    "acc_stderr": 0.028637235639800935,
                    "acc_norm": 0.3169811320754717,
                    "acc_norm_stderr": 0.028637235639800935,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2708333333333333,
                    "acc_stderr": 0.03716177437566017,
                    "acc_norm": 0.2708333333333333,
                    "acc_norm_stderr": 0.03716177437566017,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.16,
                    "acc_stderr": 0.03684529491774709,
                    "acc_norm": 0.16,
                    "acc_norm_stderr": 0.03684529491774709,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542127,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.044084400227680794,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.044084400227680794,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2832369942196532,
                    "acc_stderr": 0.034355680560478746,
                    "acc_norm": 0.2832369942196532,
                    "acc_norm_stderr": 0.034355680560478746,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.16666666666666666,
                    "acc_stderr": 0.03708284662416545,
                    "acc_norm": 0.16666666666666666,
                    "acc_norm_stderr": 0.03708284662416545,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.34893617021276596,
                    "acc_stderr": 0.031158522131357783,
                    "acc_norm": 0.34893617021276596,
                    "acc_norm_stderr": 0.031158522131357783,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.24561403508771928,
                    "acc_stderr": 0.040493392977481425,
                    "acc_norm": 0.24561403508771928,
                    "acc_norm_stderr": 0.040493392977481425,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2689655172413793,
                    "acc_stderr": 0.036951833116502325,
                    "acc_norm": 0.2689655172413793,
                    "acc_norm_stderr": 0.036951833116502325,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2671957671957672,
                    "acc_stderr": 0.02278967314577657,
                    "acc_norm": 0.2671957671957672,
                    "acc_norm_stderr": 0.02278967314577657,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.1984126984126984,
                    "acc_stderr": 0.03567016675276864,
                    "acc_norm": 0.1984126984126984,
                    "acc_norm_stderr": 0.03567016675276864,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.3419354838709677,
                    "acc_stderr": 0.02698528957655274,
                    "acc_norm": 0.3419354838709677,
                    "acc_norm_stderr": 0.02698528957655274,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.31527093596059114,
                    "acc_stderr": 0.03269080871970187,
                    "acc_norm": 0.31527093596059114,
                    "acc_norm_stderr": 0.03269080871970187,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001975,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.04902071300001975,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.34545454545454546,
                    "acc_stderr": 0.037131580674819135,
                    "acc_norm": 0.34545454545454546,
                    "acc_norm_stderr": 0.037131580674819135,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.2878787878787879,
                    "acc_stderr": 0.03225883512300993,
                    "acc_norm": 0.2878787878787879,
                    "acc_norm_stderr": 0.03225883512300993,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.2849740932642487,
                    "acc_stderr": 0.03257714077709662,
                    "acc_norm": 0.2849740932642487,
                    "acc_norm_stderr": 0.03257714077709662,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.26153846153846155,
                    "acc_stderr": 0.02228214120420442,
                    "acc_norm": 0.26153846153846155,
                    "acc_norm_stderr": 0.02228214120420442,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.25555555555555554,
                    "acc_stderr": 0.02659393910184408,
                    "acc_norm": 0.25555555555555554,
                    "acc_norm_stderr": 0.02659393910184408,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2857142857142857,
                    "acc_stderr": 0.02934457250063435,
                    "acc_norm": 0.2857142857142857,
                    "acc_norm_stderr": 0.02934457250063435,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.271523178807947,
                    "acc_stderr": 0.03631329803969654,
                    "acc_norm": 0.271523178807947,
                    "acc_norm_stderr": 0.03631329803969654,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.30091743119266057,
                    "acc_stderr": 0.019664751366802114,
                    "acc_norm": 0.30091743119266057,
                    "acc_norm_stderr": 0.019664751366802114,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.2037037037037037,
                    "acc_stderr": 0.027467401804057986,
                    "acc_norm": 0.2037037037037037,
                    "acc_norm_stderr": 0.027467401804057986,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.29411764705882354,
                    "acc_stderr": 0.03198001660115071,
                    "acc_norm": 0.29411764705882354,
                    "acc_norm_stderr": 0.03198001660115071,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.3333333333333333,
                    "acc_stderr": 0.0306858205966108,
                    "acc_norm": 0.3333333333333333,
                    "acc_norm_stderr": 0.0306858205966108,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.4260089686098655,
                    "acc_stderr": 0.033188332862172806,
                    "acc_norm": 0.4260089686098655,
                    "acc_norm_stderr": 0.033188332862172806,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2824427480916031,
                    "acc_stderr": 0.03948406125768361,
                    "acc_norm": 0.2824427480916031,
                    "acc_norm_stderr": 0.03948406125768361,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.36363636363636365,
                    "acc_stderr": 0.043913262867240704,
                    "acc_norm": 0.36363636363636365,
                    "acc_norm_stderr": 0.043913262867240704,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.3055555555555556,
                    "acc_stderr": 0.044531975073749834,
                    "acc_norm": 0.3055555555555556,
                    "acc_norm_stderr": 0.044531975073749834,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.25153374233128833,
                    "acc_stderr": 0.03408997886857529,
                    "acc_norm": 0.25153374233128833,
                    "acc_norm_stderr": 0.03408997886857529,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.33035714285714285,
                    "acc_stderr": 0.04464285714285713,
                    "acc_norm": 0.33035714285714285,
                    "acc_norm_stderr": 0.04464285714285713,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.30097087378640774,
                    "acc_stderr": 0.045416094465039476,
                    "acc_norm": 0.30097087378640774,
                    "acc_norm_stderr": 0.045416094465039476,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.36752136752136755,
                    "acc_stderr": 0.03158539157745637,
                    "acc_norm": 0.36752136752136755,
                    "acc_norm_stderr": 0.03158539157745637,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.3614303959131545,
                    "acc_stderr": 0.017179601328900736,
                    "acc_norm": 0.3614303959131545,
                    "acc_norm_stderr": 0.017179601328900736,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.3699421965317919,
                    "acc_stderr": 0.025992472029306383,
                    "acc_norm": 0.3699421965317919,
                    "acc_norm_stderr": 0.025992472029306383,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.24916201117318434,
                    "acc_stderr": 0.014465893829859924,
                    "acc_norm": 0.24916201117318434,
                    "acc_norm_stderr": 0.014465893829859924,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.3104575163398693,
                    "acc_stderr": 0.02649303322514589,
                    "acc_norm": 0.3104575163398693,
                    "acc_norm_stderr": 0.02649303322514589,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.33762057877813506,
                    "acc_stderr": 0.026858825879488558,
                    "acc_norm": 0.33762057877813506,
                    "acc_norm_stderr": 0.026858825879488558,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.3055555555555556,
                    "acc_stderr": 0.025630824975621344,
                    "acc_norm": 0.3055555555555556,
                    "acc_norm_stderr": 0.025630824975621344,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.24822695035460993,
                    "acc_stderr": 0.025770015644290392,
                    "acc_norm": 0.24822695035460993,
                    "acc_norm_stderr": 0.025770015644290392,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2907431551499348,
                    "acc_stderr": 0.011598062372851988,
                    "acc_norm": 0.2907431551499348,
                    "acc_norm_stderr": 0.011598062372851988,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.17647058823529413,
                    "acc_stderr": 0.023157468308559356,
                    "acc_norm": 0.17647058823529413,
                    "acc_norm_stderr": 0.023157468308559356,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.28921568627450983,
                    "acc_stderr": 0.018342529845275908,
                    "acc_norm": 0.28921568627450983,
                    "acc_norm_stderr": 0.018342529845275908,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.041723430387053825,
                    "acc_norm": 0.2545454545454545,
                    "acc_norm_stderr": 0.041723430387053825,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.2653061224489796,
                    "acc_stderr": 0.028263889943784596,
                    "acc_norm": 0.2653061224489796,
                    "acc_norm_stderr": 0.028263889943784596,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.31343283582089554,
                    "acc_stderr": 0.03280188205348643,
                    "acc_norm": 0.31343283582089554,
                    "acc_norm_stderr": 0.03280188205348643,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.42,
                    "acc_stderr": 0.049604496374885836,
                    "acc_norm": 0.42,
                    "acc_norm_stderr": 0.049604496374885836,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.37349397590361444,
                    "acc_stderr": 0.037658451171688624,
                    "acc_norm": 0.37349397590361444,
                    "acc_norm_stderr": 0.037658451171688624,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.3567251461988304,
                    "acc_stderr": 0.03674013002860954,
                    "acc_norm": 0.3567251461988304,
                    "acc_norm_stderr": 0.03674013002860954,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2484700122399021,
                    "mc1_stderr": 0.01512742709652069,
                    "mc2": 0.36034965375475575,
                    "mc2_stderr": 0.01417459244672947,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7182320441988951,
                    "acc_stderr": 0.012643326011852946,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.08491281273692192,
                    "acc_stderr": 0.007678212824450799,
                    "timestamp": "2024-03-04T02-58-17.035373"
                }
            }
        }
    }
}