{
    "model_name": "AbacusResearch/haLLawa4-7b",
    "last_updated": "2024-02-19",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.6936860068259386,
                    "acc_stderr": 0.013470584417276513,
                    "acc_norm": 0.7150170648464164,
                    "acc_norm_stderr": 0.013191348179838795,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.7127066321449911,
                    "acc_stderr": 0.004515748192605716,
                    "acc_norm": 0.8835889265086636,
                    "acc_norm_stderr": 0.0032006176493464752,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.048523658709391,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.048523658709391,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6444444444444445,
                    "acc_stderr": 0.04135176749720385,
                    "acc_norm": 0.6444444444444445,
                    "acc_norm_stderr": 0.04135176749720385,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.7039473684210527,
                    "acc_stderr": 0.03715062154998904,
                    "acc_norm": 0.7039473684210527,
                    "acc_norm_stderr": 0.03715062154998904,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.63,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.63,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.6943396226415094,
                    "acc_stderr": 0.028353298073322666,
                    "acc_norm": 0.6943396226415094,
                    "acc_norm_stderr": 0.028353298073322666,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.7569444444444444,
                    "acc_stderr": 0.03586879280080341,
                    "acc_norm": 0.7569444444444444,
                    "acc_norm_stderr": 0.03586879280080341,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.47,
                    "acc_stderr": 0.050161355804659205,
                    "acc_norm": 0.47,
                    "acc_norm_stderr": 0.050161355804659205,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.55,
                    "acc_stderr": 0.05,
                    "acc_norm": 0.55,
                    "acc_norm_stderr": 0.05,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.04560480215720684,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.04560480215720684,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.6416184971098265,
                    "acc_stderr": 0.03656343653353158,
                    "acc_norm": 0.6416184971098265,
                    "acc_norm_stderr": 0.03656343653353158,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.43137254901960786,
                    "acc_stderr": 0.04928099597287534,
                    "acc_norm": 0.43137254901960786,
                    "acc_norm_stderr": 0.04928099597287534,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.75,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.5659574468085107,
                    "acc_stderr": 0.03240038086792747,
                    "acc_norm": 0.5659574468085107,
                    "acc_norm_stderr": 0.03240038086792747,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.4824561403508772,
                    "acc_stderr": 0.04700708033551038,
                    "acc_norm": 0.4824561403508772,
                    "acc_norm_stderr": 0.04700708033551038,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5862068965517241,
                    "acc_stderr": 0.04104269211806232,
                    "acc_norm": 0.5862068965517241,
                    "acc_norm_stderr": 0.04104269211806232,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.41005291005291006,
                    "acc_stderr": 0.02533120243894443,
                    "acc_norm": 0.41005291005291006,
                    "acc_norm_stderr": 0.02533120243894443,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.49206349206349204,
                    "acc_stderr": 0.044715725362943486,
                    "acc_norm": 0.49206349206349204,
                    "acc_norm_stderr": 0.044715725362943486,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695235,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.04760952285695235,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7870967741935484,
                    "acc_stderr": 0.02328766512726854,
                    "acc_norm": 0.7870967741935484,
                    "acc_norm_stderr": 0.02328766512726854,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.5123152709359606,
                    "acc_stderr": 0.035169204442208966,
                    "acc_norm": 0.5123152709359606,
                    "acc_norm_stderr": 0.035169204442208966,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.7,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.7,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.7818181818181819,
                    "acc_stderr": 0.03225078108306289,
                    "acc_norm": 0.7818181818181819,
                    "acc_norm_stderr": 0.03225078108306289,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.7929292929292929,
                    "acc_stderr": 0.02886977846026705,
                    "acc_norm": 0.7929292929292929,
                    "acc_norm_stderr": 0.02886977846026705,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.9067357512953368,
                    "acc_stderr": 0.02098685459328973,
                    "acc_norm": 0.9067357512953368,
                    "acc_norm_stderr": 0.02098685459328973,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.6692307692307692,
                    "acc_stderr": 0.023854795680971125,
                    "acc_norm": 0.6692307692307692,
                    "acc_norm_stderr": 0.023854795680971125,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.3037037037037037,
                    "acc_stderr": 0.028037929969114993,
                    "acc_norm": 0.3037037037037037,
                    "acc_norm_stderr": 0.028037929969114993,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6722689075630253,
                    "acc_stderr": 0.03048991141767323,
                    "acc_norm": 0.6722689075630253,
                    "acc_norm_stderr": 0.03048991141767323,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.3576158940397351,
                    "acc_stderr": 0.03913453431177258,
                    "acc_norm": 0.3576158940397351,
                    "acc_norm_stderr": 0.03913453431177258,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8403669724770643,
                    "acc_stderr": 0.01570349834846177,
                    "acc_norm": 0.8403669724770643,
                    "acc_norm_stderr": 0.01570349834846177,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.5092592592592593,
                    "acc_stderr": 0.034093869469927006,
                    "acc_norm": 0.5092592592592593,
                    "acc_norm_stderr": 0.034093869469927006,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.8578431372549019,
                    "acc_stderr": 0.024509803921568603,
                    "acc_norm": 0.8578431372549019,
                    "acc_norm_stderr": 0.024509803921568603,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.810126582278481,
                    "acc_stderr": 0.02553010046023349,
                    "acc_norm": 0.810126582278481,
                    "acc_norm_stderr": 0.02553010046023349,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.6905829596412556,
                    "acc_stderr": 0.03102441174057221,
                    "acc_norm": 0.6905829596412556,
                    "acc_norm_stderr": 0.03102441174057221,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.7938931297709924,
                    "acc_stderr": 0.03547771004159465,
                    "acc_norm": 0.7938931297709924,
                    "acc_norm_stderr": 0.03547771004159465,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.7603305785123967,
                    "acc_stderr": 0.03896878985070416,
                    "acc_norm": 0.7603305785123967,
                    "acc_norm_stderr": 0.03896878985070416,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.04186091791394607,
                    "acc_norm": 0.75,
                    "acc_norm_stderr": 0.04186091791394607,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.754601226993865,
                    "acc_stderr": 0.03380939813943354,
                    "acc_norm": 0.754601226993865,
                    "acc_norm_stderr": 0.03380939813943354,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.41964285714285715,
                    "acc_stderr": 0.046840993210771065,
                    "acc_norm": 0.41964285714285715,
                    "acc_norm_stderr": 0.046840993210771065,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.7669902912621359,
                    "acc_stderr": 0.04185832598928315,
                    "acc_norm": 0.7669902912621359,
                    "acc_norm_stderr": 0.04185832598928315,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8717948717948718,
                    "acc_stderr": 0.021901905115073325,
                    "acc_norm": 0.8717948717948718,
                    "acc_norm_stderr": 0.021901905115073325,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.71,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.71,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.8339719029374202,
                    "acc_stderr": 0.013306478243066302,
                    "acc_norm": 0.8339719029374202,
                    "acc_norm_stderr": 0.013306478243066302,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7398843930635838,
                    "acc_stderr": 0.023618678310069363,
                    "acc_norm": 0.7398843930635838,
                    "acc_norm_stderr": 0.023618678310069363,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.441340782122905,
                    "acc_stderr": 0.016607021781050873,
                    "acc_norm": 0.441340782122905,
                    "acc_norm_stderr": 0.016607021781050873,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7352941176470589,
                    "acc_stderr": 0.025261691219729487,
                    "acc_norm": 0.7352941176470589,
                    "acc_norm_stderr": 0.025261691219729487,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.6945337620578779,
                    "acc_stderr": 0.026160584450140446,
                    "acc_norm": 0.6945337620578779,
                    "acc_norm_stderr": 0.026160584450140446,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.7376543209876543,
                    "acc_stderr": 0.024477222856135114,
                    "acc_norm": 0.7376543209876543,
                    "acc_norm_stderr": 0.024477222856135114,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.4929078014184397,
                    "acc_stderr": 0.02982449855912901,
                    "acc_norm": 0.4929078014184397,
                    "acc_norm_stderr": 0.02982449855912901,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.46936114732724904,
                    "acc_stderr": 0.012746237711716634,
                    "acc_norm": 0.46936114732724904,
                    "acc_norm_stderr": 0.012746237711716634,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.6691176470588235,
                    "acc_stderr": 0.02858270975389845,
                    "acc_norm": 0.6691176470588235,
                    "acc_norm_stderr": 0.02858270975389845,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.6683006535947712,
                    "acc_stderr": 0.01904748523936038,
                    "acc_norm": 0.6683006535947712,
                    "acc_norm_stderr": 0.01904748523936038,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6454545454545455,
                    "acc_stderr": 0.045820048415054174,
                    "acc_norm": 0.6454545454545455,
                    "acc_norm_stderr": 0.045820048415054174,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.7428571428571429,
                    "acc_stderr": 0.02797982353874455,
                    "acc_norm": 0.7428571428571429,
                    "acc_norm_stderr": 0.02797982353874455,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8308457711442786,
                    "acc_stderr": 0.02650859065623327,
                    "acc_norm": 0.8308457711442786,
                    "acc_norm_stderr": 0.02650859065623327,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.0348735088019777,
                    "acc_norm": 0.86,
                    "acc_norm_stderr": 0.0348735088019777,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5421686746987951,
                    "acc_stderr": 0.0387862677100236,
                    "acc_norm": 0.5421686746987951,
                    "acc_norm_stderr": 0.0387862677100236,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8245614035087719,
                    "acc_stderr": 0.029170885500727665,
                    "acc_norm": 0.8245614035087719,
                    "acc_norm_stderr": 0.029170885500727665,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.5789473684210527,
                    "mc1_stderr": 0.01728393624813648,
                    "mc2": 0.7427459589364643,
                    "mc2_stderr": 0.014232366890119735,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.823993685872139,
                    "acc_stderr": 0.010703090882320705,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.7050796057619408,
                    "acc_stderr": 0.012560698010954774,
                    "timestamp": "2024-02-19T19-33-51.734148"
                }
            }
        }
    }
}