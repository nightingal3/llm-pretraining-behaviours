{
    "model_name": "jisukim8873/falcon-7B-case-8",
    "last_updated": "2024-06-25 14:40:23.620750",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.4598976109215017,
                    "acc_stderr": 0.014564318856924848,
                    "acc_norm": 0.4948805460750853,
                    "acc_norm_stderr": 0.014610624890309157,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5965943039235212,
                    "acc_stderr": 0.004895782107786499,
                    "acc_norm": 0.7855008962358097,
                    "acc_norm_stderr": 0.0040963551251175165,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252605,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252605,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.3111111111111111,
                    "acc_stderr": 0.039992628766177214,
                    "acc_norm": 0.3111111111111111,
                    "acc_norm_stderr": 0.039992628766177214,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.2894736842105263,
                    "acc_stderr": 0.03690677986137283,
                    "acc_norm": 0.2894736842105263,
                    "acc_norm_stderr": 0.03690677986137283,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.3433962264150943,
                    "acc_stderr": 0.02922452646912479,
                    "acc_norm": 0.3433962264150943,
                    "acc_norm_stderr": 0.02922452646912479,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2847222222222222,
                    "acc_stderr": 0.037738099906869355,
                    "acc_norm": 0.2847222222222222,
                    "acc_norm_stderr": 0.037738099906869355,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.11,
                    "acc_stderr": 0.03144660377352203,
                    "acc_norm": 0.11,
                    "acc_norm_stderr": 0.03144660377352203,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542127,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2832369942196532,
                    "acc_stderr": 0.034355680560478746,
                    "acc_norm": 0.2832369942196532,
                    "acc_norm_stderr": 0.034355680560478746,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.04023382273617747,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.04023382273617747,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.04878317312145633,
                    "acc_norm": 0.38,
                    "acc_norm_stderr": 0.04878317312145633,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3276595744680851,
                    "acc_stderr": 0.030683020843231004,
                    "acc_norm": 0.3276595744680851,
                    "acc_norm_stderr": 0.030683020843231004,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.21052631578947367,
                    "acc_stderr": 0.0383515395439942,
                    "acc_norm": 0.21052631578947367,
                    "acc_norm_stderr": 0.0383515395439942,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2620689655172414,
                    "acc_stderr": 0.036646663372252565,
                    "acc_norm": 0.2620689655172414,
                    "acc_norm_stderr": 0.036646663372252565,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.022569897074918417,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.022569897074918417,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.18253968253968253,
                    "acc_stderr": 0.034550710191021496,
                    "acc_norm": 0.18253968253968253,
                    "acc_norm_stderr": 0.034550710191021496,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.3161290322580645,
                    "acc_stderr": 0.026450874489042774,
                    "acc_norm": 0.3161290322580645,
                    "acc_norm_stderr": 0.026450874489042774,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.33004926108374383,
                    "acc_stderr": 0.033085304262282574,
                    "acc_norm": 0.33004926108374383,
                    "acc_norm_stderr": 0.033085304262282574,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.38181818181818183,
                    "acc_stderr": 0.03793713171165634,
                    "acc_norm": 0.38181818181818183,
                    "acc_norm_stderr": 0.03793713171165634,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.3383838383838384,
                    "acc_stderr": 0.03371124142626302,
                    "acc_norm": 0.3383838383838384,
                    "acc_norm_stderr": 0.03371124142626302,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.27979274611398963,
                    "acc_stderr": 0.032396370467357036,
                    "acc_norm": 0.27979274611398963,
                    "acc_norm_stderr": 0.032396370467357036,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2641025641025641,
                    "acc_stderr": 0.02235219373745327,
                    "acc_norm": 0.2641025641025641,
                    "acc_norm_stderr": 0.02235219373745327,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.026719240783712163,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.026719240783712163,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.28991596638655465,
                    "acc_stderr": 0.029472485833136084,
                    "acc_norm": 0.28991596638655465,
                    "acc_norm_stderr": 0.029472485833136084,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31125827814569534,
                    "acc_stderr": 0.03780445850526732,
                    "acc_norm": 0.31125827814569534,
                    "acc_norm_stderr": 0.03780445850526732,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.29908256880733947,
                    "acc_stderr": 0.019630417285415175,
                    "acc_norm": 0.29908256880733947,
                    "acc_norm_stderr": 0.019630417285415175,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.22685185185185186,
                    "acc_stderr": 0.028561650102422263,
                    "acc_norm": 0.22685185185185186,
                    "acc_norm_stderr": 0.028561650102422263,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.28431372549019607,
                    "acc_stderr": 0.031660096793998116,
                    "acc_norm": 0.28431372549019607,
                    "acc_norm_stderr": 0.031660096793998116,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.29535864978902954,
                    "acc_stderr": 0.029696338713422876,
                    "acc_norm": 0.29535864978902954,
                    "acc_norm_stderr": 0.029696338713422876,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.3901345291479821,
                    "acc_stderr": 0.03273766725459157,
                    "acc_norm": 0.3901345291479821,
                    "acc_norm_stderr": 0.03273766725459157,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.3282442748091603,
                    "acc_stderr": 0.04118438565806298,
                    "acc_norm": 0.3282442748091603,
                    "acc_norm_stderr": 0.04118438565806298,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.371900826446281,
                    "acc_stderr": 0.044120158066245044,
                    "acc_norm": 0.371900826446281,
                    "acc_norm_stderr": 0.044120158066245044,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.32407407407407407,
                    "acc_stderr": 0.04524596007030048,
                    "acc_norm": 0.32407407407407407,
                    "acc_norm_stderr": 0.04524596007030048,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2883435582822086,
                    "acc_stderr": 0.035590395316173425,
                    "acc_norm": 0.2883435582822086,
                    "acc_norm_stderr": 0.035590395316173425,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.2857142857142857,
                    "acc_stderr": 0.04287858751340456,
                    "acc_norm": 0.2857142857142857,
                    "acc_norm_stderr": 0.04287858751340456,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.24271844660194175,
                    "acc_stderr": 0.04245022486384493,
                    "acc_norm": 0.24271844660194175,
                    "acc_norm_stderr": 0.04245022486384493,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.3504273504273504,
                    "acc_stderr": 0.03125610824421879,
                    "acc_norm": 0.3504273504273504,
                    "acc_norm_stderr": 0.03125610824421879,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.37292464878671777,
                    "acc_stderr": 0.01729286826945392,
                    "acc_norm": 0.37292464878671777,
                    "acc_norm_stderr": 0.01729286826945392,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.3670520231213873,
                    "acc_stderr": 0.025950054337654082,
                    "acc_norm": 0.3670520231213873,
                    "acc_norm_stderr": 0.025950054337654082,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.24022346368715083,
                    "acc_stderr": 0.014288343803925303,
                    "acc_norm": 0.24022346368715083,
                    "acc_norm_stderr": 0.014288343803925303,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.30392156862745096,
                    "acc_stderr": 0.026336613469046633,
                    "acc_norm": 0.30392156862745096,
                    "acc_norm_stderr": 0.026336613469046633,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.3504823151125402,
                    "acc_stderr": 0.027098652621301754,
                    "acc_norm": 0.3504823151125402,
                    "acc_norm_stderr": 0.027098652621301754,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.3148148148148148,
                    "acc_stderr": 0.025842248700902168,
                    "acc_norm": 0.3148148148148148,
                    "acc_norm_stderr": 0.025842248700902168,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2801418439716312,
                    "acc_stderr": 0.026789172351140242,
                    "acc_norm": 0.2801418439716312,
                    "acc_norm_stderr": 0.026789172351140242,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2737940026075619,
                    "acc_stderr": 0.011388612167979395,
                    "acc_norm": 0.2737940026075619,
                    "acc_norm_stderr": 0.011388612167979395,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.26838235294117646,
                    "acc_stderr": 0.026917481224377232,
                    "acc_norm": 0.26838235294117646,
                    "acc_norm_stderr": 0.026917481224377232,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.29901960784313725,
                    "acc_stderr": 0.018521756215423024,
                    "acc_norm": 0.29901960784313725,
                    "acc_norm_stderr": 0.018521756215423024,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.33636363636363636,
                    "acc_stderr": 0.04525393596302505,
                    "acc_norm": 0.33636363636363636,
                    "acc_norm_stderr": 0.04525393596302505,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.3183673469387755,
                    "acc_stderr": 0.029822533793982076,
                    "acc_norm": 0.3183673469387755,
                    "acc_norm_stderr": 0.029822533793982076,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.3880597014925373,
                    "acc_stderr": 0.0344578996436275,
                    "acc_norm": 0.3880597014925373,
                    "acc_norm_stderr": 0.0344578996436275,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.49,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3614457831325301,
                    "acc_stderr": 0.037400593820293204,
                    "acc_norm": 0.3614457831325301,
                    "acc_norm_stderr": 0.037400593820293204,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.3508771929824561,
                    "acc_stderr": 0.03660298834049162,
                    "acc_norm": 0.3508771929824561,
                    "acc_norm_stderr": 0.03660298834049162,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2582619339045288,
                    "mc1_stderr": 0.015321821688476196,
                    "mc2": 0.37575591444529527,
                    "mc2_stderr": 0.014304714495441502,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7048145224940805,
                    "acc_stderr": 0.012819410741754763,
                    "timestamp": "2024-03-09T20-50-47.085955"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.07505686125852919,
                    "acc_stderr": 0.007257633145486642,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.016483516483516484,
                    "acc_stderr": 0.005454029764766741,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.03673938002296211,
                    "acc_stderr": 0.006377907088210899,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.007407407407407408,
                    "acc_stderr": 0.0036933821684372575,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.015503875968992248,
                    "acc_stderr": 0.004113617238360444,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.010438413361169102,
                    "acc_stderr": 0.0046486271171846455,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.02109704641350211,
                    "acc_stderr": 0.0066076963656265374,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.019376579612468407,
                    "acc_stderr": 0.0040026474981053545,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.2185,
                    "acc_stderr": 0.009242379877114725,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.646,
                    "acc_stderr": 0.010695756149043483,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.0085,
                    "acc_stderr": 0.002053285901060994,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.6345,
                    "acc_stderr": 0.010770927603540937,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.056,
                    "acc_stderr": 0.005142491867889066,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.0025,
                    "acc_stderr": 0.0011169148353275223,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.053,
                    "acc_stderr": 0.0050107937521926766,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.1425,
                    "acc_stderr": 0.007818403847292692,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.2425,
                    "acc_stderr": 0.00958607434827748,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 0.716,
                    "acc_stderr": 0.010085775202269415,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.08946171341925702,
                    "acc_stderr": 0.00786158304993971,
                    "timestamp": "2024-06-07T14-05-19.237175"
                }
            },
            "anli_r2": {
                "0-shot": {
                    "brier_score": 0.8665276269305205,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "anli_r3": {
                "0-shot": {
                    "brier_score": 0.8318243198290138,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "anli_r1": {
                "0-shot": {
                    "brier_score": 0.8939690888764009,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_eu": {
                "0-shot": {
                    "brier_score": 1.0427344013967832,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_vi": {
                "0-shot": {
                    "brier_score": 1.0487135581024136,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_ru": {
                "0-shot": {
                    "brier_score": 0.8221328820374796,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_zh": {
                "0-shot": {
                    "brier_score": 1.0244308666680741,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_tr": {
                "0-shot": {
                    "brier_score": 0.9970781849876954,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_fr": {
                "0-shot": {
                    "brier_score": 0.7436567906026784,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_en": {
                "0-shot": {
                    "brier_score": 0.6425702304442446,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_ur": {
                "0-shot": {
                    "brier_score": 1.3117764974375834,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_ar": {
                "0-shot": {
                    "brier_score": 1.2508515010911752,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_de": {
                "0-shot": {
                    "brier_score": 0.8387305217972838,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_hi": {
                "0-shot": {
                    "brier_score": 1.1022494172288069,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_es": {
                "0-shot": {
                    "brier_score": 0.8133629952055023,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_bg": {
                "0-shot": {
                    "brier_score": 0.9484830535778097,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_sw": {
                "0-shot": {
                    "brier_score": 1.0888680824673866,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_el": {
                "0-shot": {
                    "brier_score": 0.9888240024756009,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "xnli_th": {
                "0-shot": {
                    "brier_score": 0.9812608603086016,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "brier_score": 1.059925394028478,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "mathqa": {
                "5-shot": {
                    "brier_score": 0.9414128848103127,
                    "timestamp": "2024-06-07T14-16-36.587884"
                }
            },
            "lambada_standard": {
                "0-shot": {
                    "perplexity": 4.213587010663793,
                    "perplexity_stderr": 0.0909206003026987,
                    "acc": 0.6557345235784979,
                    "acc_stderr": 0.006619464143312433,
                    "timestamp": "2024-06-07T14-17-55.951174"
                }
            },
            "lambada_openai": {
                "0-shot": {
                    "perplexity": 3.391177573570244,
                    "perplexity_stderr": 0.07192541543832812,
                    "acc": 0.7269551717446148,
                    "acc_stderr": 0.006207016010291535,
                    "timestamp": "2024-06-07T14-17-55.951174"
                }
            }
        }
    }
}