{
    "model_name": "Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b",
    "last_updated": "2023-10-15",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.5742320819112628,
                    "acc_stderr": 0.01444946427886881,
                    "acc_norm": 0.5964163822525598,
                    "acc_norm_stderr": 0.014337158914268445,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6027683728340968,
                    "acc_stderr": 0.004883246579496668,
                    "acc_norm": 0.799044015136427,
                    "acc_norm_stderr": 0.003998962580974816,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.04605661864718381,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.04605661864718381,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.45185185185185184,
                    "acc_stderr": 0.04299268905480864,
                    "acc_norm": 0.45185185185185184,
                    "acc_norm_stderr": 0.04299268905480864,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.5460526315789473,
                    "acc_stderr": 0.04051646342874142,
                    "acc_norm": 0.5460526315789473,
                    "acc_norm_stderr": 0.04051646342874142,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.61,
                    "acc_stderr": 0.04902071300001974,
                    "acc_norm": 0.61,
                    "acc_norm_stderr": 0.04902071300001974,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.5584905660377358,
                    "acc_stderr": 0.030561590426731833,
                    "acc_norm": 0.5584905660377358,
                    "acc_norm_stderr": 0.030561590426731833,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.5486111111111112,
                    "acc_stderr": 0.041614023984032786,
                    "acc_norm": 0.5486111111111112,
                    "acc_norm_stderr": 0.041614023984032786,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001975,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.04902071300001975,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.47,
                    "acc_stderr": 0.05016135580465919,
                    "acc_norm": 0.47,
                    "acc_norm_stderr": 0.05016135580465919,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.4797687861271676,
                    "acc_stderr": 0.03809342081273958,
                    "acc_norm": 0.4797687861271676,
                    "acc_norm_stderr": 0.03809342081273958,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.27450980392156865,
                    "acc_stderr": 0.04440521906179327,
                    "acc_norm": 0.27450980392156865,
                    "acc_norm_stderr": 0.04440521906179327,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.61,
                    "acc_stderr": 0.049020713000019756,
                    "acc_norm": 0.61,
                    "acc_norm_stderr": 0.049020713000019756,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.4723404255319149,
                    "acc_stderr": 0.03263597118409769,
                    "acc_norm": 0.4723404255319149,
                    "acc_norm_stderr": 0.03263597118409769,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2807017543859649,
                    "acc_stderr": 0.042270544512322004,
                    "acc_norm": 0.2807017543859649,
                    "acc_norm_stderr": 0.042270544512322004,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.4413793103448276,
                    "acc_stderr": 0.04137931034482758,
                    "acc_norm": 0.4413793103448276,
                    "acc_norm_stderr": 0.04137931034482758,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.3492063492063492,
                    "acc_stderr": 0.02455229220934265,
                    "acc_norm": 0.3492063492063492,
                    "acc_norm_stderr": 0.02455229220934265,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.36507936507936506,
                    "acc_stderr": 0.04306241259127153,
                    "acc_norm": 0.36507936507936506,
                    "acc_norm_stderr": 0.04306241259127153,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.048523658709391,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.048523658709391,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.6161290322580645,
                    "acc_stderr": 0.02766618207553964,
                    "acc_norm": 0.6161290322580645,
                    "acc_norm_stderr": 0.02766618207553964,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.39901477832512317,
                    "acc_stderr": 0.03445487686264716,
                    "acc_norm": 0.39901477832512317,
                    "acc_norm_stderr": 0.03445487686264716,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.55,
                    "acc_stderr": 0.05,
                    "acc_norm": 0.55,
                    "acc_norm_stderr": 0.05,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.6606060606060606,
                    "acc_stderr": 0.036974422050315946,
                    "acc_norm": 0.6606060606060606,
                    "acc_norm_stderr": 0.036974422050315946,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.7373737373737373,
                    "acc_stderr": 0.03135305009533084,
                    "acc_norm": 0.7373737373737373,
                    "acc_norm_stderr": 0.03135305009533084,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.7772020725388601,
                    "acc_stderr": 0.030031147977641538,
                    "acc_norm": 0.7772020725388601,
                    "acc_norm_stderr": 0.030031147977641538,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.02535100632816969,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.02535100632816969,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.02646611753895991,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.02646611753895991,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6092436974789915,
                    "acc_stderr": 0.031693802357129965,
                    "acc_norm": 0.6092436974789915,
                    "acc_norm_stderr": 0.031693802357129965,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.304635761589404,
                    "acc_stderr": 0.03757949922943343,
                    "acc_norm": 0.304635761589404,
                    "acc_norm_stderr": 0.03757949922943343,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.7321100917431193,
                    "acc_stderr": 0.018987462257978652,
                    "acc_norm": 0.7321100917431193,
                    "acc_norm_stderr": 0.018987462257978652,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.41203703703703703,
                    "acc_stderr": 0.03356787758160835,
                    "acc_norm": 0.41203703703703703,
                    "acc_norm_stderr": 0.03356787758160835,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.7647058823529411,
                    "acc_stderr": 0.029771775228145638,
                    "acc_norm": 0.7647058823529411,
                    "acc_norm_stderr": 0.029771775228145638,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.759493670886076,
                    "acc_stderr": 0.027820781981149685,
                    "acc_norm": 0.759493670886076,
                    "acc_norm_stderr": 0.027820781981149685,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.6502242152466368,
                    "acc_stderr": 0.03200736719484503,
                    "acc_norm": 0.6502242152466368,
                    "acc_norm_stderr": 0.03200736719484503,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.5572519083969466,
                    "acc_stderr": 0.043564472026650695,
                    "acc_norm": 0.5572519083969466,
                    "acc_norm_stderr": 0.043564472026650695,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.7107438016528925,
                    "acc_stderr": 0.041391127276354626,
                    "acc_norm": 0.7107438016528925,
                    "acc_norm_stderr": 0.041391127276354626,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.6759259259259259,
                    "acc_stderr": 0.045245960070300476,
                    "acc_norm": 0.6759259259259259,
                    "acc_norm_stderr": 0.045245960070300476,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.6257668711656442,
                    "acc_stderr": 0.03802068102899616,
                    "acc_norm": 0.6257668711656442,
                    "acc_norm_stderr": 0.03802068102899616,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.38392857142857145,
                    "acc_stderr": 0.04616143075028547,
                    "acc_norm": 0.38392857142857145,
                    "acc_norm_stderr": 0.04616143075028547,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.7378640776699029,
                    "acc_stderr": 0.04354631077260595,
                    "acc_norm": 0.7378640776699029,
                    "acc_norm_stderr": 0.04354631077260595,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8290598290598291,
                    "acc_stderr": 0.02466249684520981,
                    "acc_norm": 0.8290598290598291,
                    "acc_norm_stderr": 0.02466249684520981,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.63,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.63,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.7509578544061303,
                    "acc_stderr": 0.015464676163395958,
                    "acc_norm": 0.7509578544061303,
                    "acc_norm_stderr": 0.015464676163395958,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.5809248554913294,
                    "acc_stderr": 0.026564178111422632,
                    "acc_norm": 0.5809248554913294,
                    "acc_norm_stderr": 0.026564178111422632,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.34301675977653634,
                    "acc_stderr": 0.015876912673057752,
                    "acc_norm": 0.34301675977653634,
                    "acc_norm_stderr": 0.015876912673057752,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.5261437908496732,
                    "acc_stderr": 0.028590752958852394,
                    "acc_norm": 0.5261437908496732,
                    "acc_norm_stderr": 0.028590752958852394,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.6366559485530546,
                    "acc_stderr": 0.02731684767419271,
                    "acc_norm": 0.6366559485530546,
                    "acc_norm_stderr": 0.02731684767419271,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.6080246913580247,
                    "acc_stderr": 0.027163686038271146,
                    "acc_norm": 0.6080246913580247,
                    "acc_norm_stderr": 0.027163686038271146,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.40070921985815605,
                    "acc_stderr": 0.029233465745573086,
                    "acc_norm": 0.40070921985815605,
                    "acc_norm_stderr": 0.029233465745573086,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.4106910039113429,
                    "acc_stderr": 0.012564871542534349,
                    "acc_norm": 0.4106910039113429,
                    "acc_norm_stderr": 0.012564871542534349,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.5404411764705882,
                    "acc_stderr": 0.030273325077345755,
                    "acc_norm": 0.5404411764705882,
                    "acc_norm_stderr": 0.030273325077345755,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.565359477124183,
                    "acc_stderr": 0.020054269200726463,
                    "acc_norm": 0.565359477124183,
                    "acc_norm_stderr": 0.020054269200726463,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6454545454545455,
                    "acc_stderr": 0.045820048415054174,
                    "acc_norm": 0.6454545454545455,
                    "acc_norm_stderr": 0.045820048415054174,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.5836734693877551,
                    "acc_stderr": 0.031557828165561644,
                    "acc_norm": 0.5836734693877551,
                    "acc_norm_stderr": 0.031557828165561644,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.6766169154228856,
                    "acc_stderr": 0.03307615947979033,
                    "acc_norm": 0.6766169154228856,
                    "acc_norm_stderr": 0.03307615947979033,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.81,
                    "acc_stderr": 0.03942772444036623,
                    "acc_norm": 0.81,
                    "acc_norm_stderr": 0.03942772444036623,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.4819277108433735,
                    "acc_stderr": 0.03889951252827217,
                    "acc_norm": 0.4819277108433735,
                    "acc_norm_stderr": 0.03889951252827217,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.7368421052631579,
                    "acc_stderr": 0.03377310252209205,
                    "acc_norm": 0.7368421052631579,
                    "acc_norm_stderr": 0.03377310252209205,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.3843329253365973,
                    "mc1_stderr": 0.017028707301245196,
                    "mc2": 0.5594851543429306,
                    "mc2_stderr": 0.016227878204646183,
                    "timestamp": "2023-07-19T22-17-39.123351"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.24653942953020133,
                    "em_stderr": 0.004413804668718679,
                    "f1": 0.33164010067114214,
                    "f1_stderr": 0.004375317074606664,
                    "timestamp": "2023-10-15T18-31-20.676081"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.05534495830174375,
                    "acc_stderr": 0.006298221796179607,
                    "timestamp": "2023-10-15T18-31-20.676081"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7087608524072613,
                    "acc_stderr": 0.012769029305370699,
                    "timestamp": "2023-10-15T18-31-20.676081"
                }
            }
        }
    }
}