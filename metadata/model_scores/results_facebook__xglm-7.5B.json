{
    "model_name": "facebook/xglm-7.5B",
    "last_updated": "2024-12-19 13:37:56.869427",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "minerva_math_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_3da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000127,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_3ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_2ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_1dc": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_2dm": {
                "acc": 0.0045,
                "acc_stderr": 0.001496995490223334,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "arithmetic_2da": {
                "acc": 0.0015,
                "acc_stderr": 0.000865592066052149,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "gsm8k_cot": {
                "exact_match": 0.003032600454890068,
                "exact_match_stderr": 0.0015145735612245405,
                "timestamp": "2024-06-12T08-18-40.858953"
            },
            "gsm8k": {
                "exact_match": 0.01061410159211524,
                "exact_match_stderr": 0.0028227133223877035,
                "timestamp": "2024-11-24T16-59-05.111235"
            },
            "anli_r2": {
                "brier_score": 0.7252401406403721,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "anli_r3": {
                "brier_score": 0.7345576777518716,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "anli_r1": {
                "brier_score": 0.7403058522962276,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_eu": {
                "brier_score": 0.7249978471618856,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_vi": {
                "brier_score": 0.7449262360122225,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_ru": {
                "brier_score": 0.776968330898939,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_zh": {
                "brier_score": 1.0503581196137082,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_tr": {
                "brier_score": 0.7725432543659072,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_fr": {
                "brier_score": 0.8140567425559275,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_en": {
                "brier_score": 0.6517840395340417,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_ur": {
                "brier_score": 0.8831297423089415,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_ar": {
                "brier_score": 1.2554211575716623,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_de": {
                "brier_score": 0.7983353916849525,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_hi": {
                "brier_score": 0.7335594240977125,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_es": {
                "brier_score": 0.8237671750458347,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_bg": {
                "brier_score": 0.8170940878498713,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_sw": {
                "brier_score": 0.7365080187868047,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_el": {
                "brier_score": 0.8709162316623921,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "xnli_th": {
                "brier_score": 0.8133822575764703,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "logiqa2": {
                "brier_score": 1.0735932488634556,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "mathqa": {
                "brier_score": 0.988707059624341,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-58-16.302652"
            },
            "lambada_standard": {
                "perplexity": 8.331266240940502,
                "perplexity_stderr": 0.22169161559199443,
                "acc": 0.5460896565107705,
                "acc_stderr": 0.006936319475444719,
                "timestamp": "2024-06-09T16-01-07.820079"
            },
            "lambada_openai": {
                "perplexity": 7.454260315964878,
                "perplexity_stderr": 0.19833941098139538,
                "acc": 0.5561808655152338,
                "acc_stderr": 0.006921864695286302,
                "timestamp": "2024-06-09T16-01-07.820079"
            },
            "mmlu_world_religions": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.03565079670708312,
                "brier_score": 0.7478836978346121,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.03670066451047181,
                "brier_score": 0.7711352778112898,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_prehistory": {
                "acc": 0.25617283950617287,
                "acc_stderr": 0.0242885336377261,
                "brier_score": 0.7602271097108688,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23687150837988827,
                "acc_stderr": 0.01421957078810398,
                "brier_score": 0.7704458434686978,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.24472573839662448,
                "acc_stderr": 0.02798569938703641,
                "brier_score": 0.7612080943321962,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_moral_disputes": {
                "acc": 0.24277456647398843,
                "acc_stderr": 0.023083658586984204,
                "brier_score": 0.7620153279156361,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_professional_law": {
                "acc": 0.2457627118644068,
                "acc_stderr": 0.010996156635142692,
                "brier_score": 0.7593523563741399,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.26993865030674846,
                "acc_stderr": 0.034878251684978906,
                "brier_score": 0.7643975216702128,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2107843137254902,
                "acc_stderr": 0.0286265479124374,
                "brier_score": 0.7736428893138484,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_philosophy": {
                "acc": 0.27009646302250806,
                "acc_stderr": 0.025218040373410616,
                "brier_score": 0.7652509223419405,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_jurisprudence": {
                "acc": 0.28703703703703703,
                "acc_stderr": 0.04373313040914761,
                "brier_score": 0.7492958135151253,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_international_law": {
                "acc": 0.256198347107438,
                "acc_stderr": 0.03984979653302871,
                "brier_score": 0.7704808114213275,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.20606060606060606,
                "acc_stderr": 0.03158415324047708,
                "brier_score": 0.7849059442327416,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.23834196891191708,
                "acc_stderr": 0.03074890536390989,
                "brier_score": 0.7804161088743681,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.25630252100840334,
                "acc_stderr": 0.02835962087053395,
                "brier_score": 0.7618685447381729,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_geography": {
                "acc": 0.1919191919191919,
                "acc_stderr": 0.02805779167298901,
                "brier_score": 0.7856532168185606,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.21100917431192662,
                "acc_stderr": 0.01749392240411265,
                "brier_score": 0.7825017004964188,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_public_relations": {
                "acc": 0.3090909090909091,
                "acc_stderr": 0.044262946482000985,
                "brier_score": 0.7554740125877594,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.26,
                "acc_stderr": 0.044084400227680794,
                "brier_score": 0.758873841872607,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_sociology": {
                "acc": 0.23383084577114427,
                "acc_stderr": 0.029929415408348398,
                "brier_score": 0.7768714367937326,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2358974358974359,
                "acc_stderr": 0.021525965407408726,
                "brier_score": 0.763402389499434,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_security_studies": {
                "acc": 0.3142857142857143,
                "acc_stderr": 0.02971932942241748,
                "brier_score": 0.744842735795794,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25326797385620914,
                "acc_stderr": 0.017593486895366835,
                "brier_score": 0.7584474284269817,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2748091603053435,
                "acc_stderr": 0.039153454088478354,
                "brier_score": 0.7664670824349369,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_econometrics": {
                "acc": 0.21052631578947367,
                "acc_stderr": 0.038351539543994194,
                "brier_score": 0.755762572716479,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.015302380123542094,
                "brier_score": 0.7665063300029615,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_marketing": {
                "acc": 0.26495726495726496,
                "acc_stderr": 0.028911208802749458,
                "brier_score": 0.7462265461625658,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_management": {
                "acc": 0.23300970873786409,
                "acc_stderr": 0.04185832598928315,
                "brier_score": 0.757532453734775,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_nutrition": {
                "acc": 0.24183006535947713,
                "acc_stderr": 0.024518195641879334,
                "brier_score": 0.7661987299388116,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7445708685279325,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_human_aging": {
                "acc": 0.32286995515695066,
                "acc_stderr": 0.03138147637575499,
                "brier_score": 0.7165132661444996,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_professional_medicine": {
                "acc": 0.1875,
                "acc_stderr": 0.023709788253811766,
                "brier_score": 0.8085025990033413,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_college_medicine": {
                "acc": 0.27167630057803466,
                "acc_stderr": 0.03391750322321659,
                "brier_score": 0.7591816490931492,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_business_ethics": {
                "acc": 0.21,
                "acc_stderr": 0.04093601807403326,
                "brier_score": 0.7598132441980852,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.23773584905660378,
                "acc_stderr": 0.02619980880756191,
                "brier_score": 0.7633841188520737,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_global_facts": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7547408714872293,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_virology": {
                "acc": 0.27710843373493976,
                "acc_stderr": 0.034843315926805875,
                "brier_score": 0.7484904151371046,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_professional_accounting": {
                "acc": 0.23404255319148937,
                "acc_stderr": 0.025257861359432397,
                "brier_score": 0.7687279343923068,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_college_physics": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.04336432707993176,
                "brier_score": 0.7717036537544136,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2847682119205298,
                "acc_stderr": 0.03684881521389023,
                "brier_score": 0.7465195131792135,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2645161290322581,
                "acc_stderr": 0.02509189237885928,
                "brier_score": 0.7642024621618707,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_college_biology": {
                "acc": 0.2361111111111111,
                "acc_stderr": 0.03551446610810826,
                "brier_score": 0.7616838225475571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_anatomy": {
                "acc": 0.28888888888888886,
                "acc_stderr": 0.0391545063041425,
                "brier_score": 0.7611247992529132,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_college_chemistry": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816506,
                "brier_score": 0.7606467477392989,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_computer_security": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621504,
                "brier_score": 0.7515781654981363,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_college_computer_science": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768077,
                "brier_score": 0.7662889507808877,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_astronomy": {
                "acc": 0.2894736842105263,
                "acc_stderr": 0.036906779861372814,
                "brier_score": 0.774228413944404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_college_mathematics": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036623,
                "brier_score": 0.7655113831829723,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.28936170212765955,
                "acc_stderr": 0.02964400657700962,
                "brier_score": 0.7505745859976238,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 0.7628138033789061,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.26,
                "acc_stderr": 0.044084400227680794,
                "brier_score": 0.7548514925906404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_machine_learning": {
                "acc": 0.20535714285714285,
                "acc_stderr": 0.03834241021419072,
                "brier_score": 0.7589028909280103,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.21674876847290642,
                "acc_stderr": 0.028990331252516235,
                "brier_score": 0.7688436179237746,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2824074074074074,
                "acc_stderr": 0.030701372111510934,
                "brier_score": 0.7584148541273571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.02201908001221791,
                "brier_score": 0.7639398310366289,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2482758620689655,
                "acc_stderr": 0.036001056927277696,
                "brier_score": 0.7610945759264104,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2814814814814815,
                "acc_stderr": 0.02742001935094527,
                "brier_score": 0.76561675444009,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-20-50.109941"
            },
            "arc_challenge": {
                "acc": 0.310580204778157,
                "acc_stderr": 0.013522292098053047,
                "acc_norm": 0.3387372013651877,
                "acc_norm_stderr": 0.01383056892797433,
                "timestamp": "2024-11-24T16-59-05.111235"
            },
            "hellaswag": {
                "acc": 0.44961163114917346,
                "acc_stderr": 0.004964378762425233,
                "acc_norm": 0.6113324039036049,
                "acc_norm_stderr": 0.0048645132621943,
                "timestamp": "2024-11-24T16-59-05.111235"
            },
            "truthfulqa_mc2": {
                "acc": 0.36382995217114145,
                "acc_stderr": 0.01364982350224989,
                "timestamp": "2024-11-24T16-59-05.111235"
            },
            "truthfulqa_gen": {
                "bleu_max": 6.098360177088277,
                "bleu_max_stderr": 0.3118236431976185,
                "bleu_acc": 0.2558139534883721,
                "bleu_acc_stderr": 0.01527417621928335,
                "bleu_diff": -2.2599605981406405,
                "bleu_diff_stderr": 0.2506337182789708,
                "rouge1_max": 21.779416715306077,
                "rouge1_max_stderr": 0.5244944853770919,
                "rouge1_acc": 0.3072215422276622,
                "rouge1_acc_stderr": 0.016150201321323006,
                "rouge1_diff": -3.390869478818047,
                "rouge1_diff_stderr": 0.39043003996311954,
                "rouge2_max": 11.203254328402076,
                "rouge2_max_stderr": 0.4899639367721538,
                "rouge2_acc": 0.18482252141982863,
                "rouge2_acc_stderr": 0.013588091176049533,
                "rouge2_diff": -4.243657550945619,
                "rouge2_diff_stderr": 0.4037168891317911,
                "rougeL_max": 19.716590648355854,
                "rougeL_max_stderr": 0.5021969423588765,
                "rougeL_acc": 0.2864137086903305,
                "rougeL_acc_stderr": 0.01582614243950234,
                "rougeL_diff": -3.591440069116983,
                "rougeL_diff_stderr": 0.37059524653156883,
                "timestamp": "2024-11-24T16-59-05.111235"
            },
            "truthfulqa_mc1": {
                "acc": 0.20685434516523868,
                "acc_stderr": 0.014179591496728337,
                "timestamp": "2024-11-24T16-59-05.111235"
            },
            "winogrande": {
                "acc": 0.584846093133386,
                "acc_stderr": 0.01384868408665859,
                "timestamp": "2024-11-24T16-59-05.111235"
            }
        }
    }
}