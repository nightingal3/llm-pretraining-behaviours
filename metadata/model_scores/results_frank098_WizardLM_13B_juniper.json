{
    "model_name": "frank098/WizardLM_13B_juniper",
    "last_updated": "2023-07-24",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "em": 0.00576761744966443,
                    "em_stderr": 0.0007755000442814698,
                    "f1": 0.07442428691275203,
                    "f1_stderr": 0.001635445995042788,
                    "timestamp": "2023-10-29T18-18-55.569728"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.0803639120545868,
                    "acc_stderr": 0.007488258573239077,
                    "timestamp": "2023-10-29T18-18-55.569728"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7111286503551697,
                    "acc_stderr": 0.012738241271018446,
                    "timestamp": "2023-10-29T18-18-55.569728"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.5366894197952219,
                    "acc_stderr": 0.014572000527756989,
                    "acc_norm": 0.5537542662116041,
                    "acc_norm_stderr": 0.014526705548539982,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5796654052977495,
                    "acc_stderr": 0.004926038197714513,
                    "acc_norm": 0.7719577773351922,
                    "acc_norm_stderr": 0.004187124964848515,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621503,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621503,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.4148148148148148,
                    "acc_stderr": 0.042561937679014075,
                    "acc_norm": 0.4148148148148148,
                    "acc_norm_stderr": 0.042561937679014075,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.4276315789473684,
                    "acc_stderr": 0.04026097083296559,
                    "acc_norm": 0.4276315789473684,
                    "acc_norm_stderr": 0.04026097083296559,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.048783173121456316,
                    "acc_norm": 0.38,
                    "acc_norm_stderr": 0.048783173121456316,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.45660377358490567,
                    "acc_stderr": 0.030656748696739435,
                    "acc_norm": 0.45660377358490567,
                    "acc_norm_stderr": 0.030656748696739435,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.4305555555555556,
                    "acc_stderr": 0.04140685639111502,
                    "acc_norm": 0.4305555555555556,
                    "acc_norm_stderr": 0.04140685639111502,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.04878317312145632,
                    "acc_norm": 0.38,
                    "acc_norm_stderr": 0.04878317312145632,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.048241815132442176,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.048241815132442176,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.3583815028901734,
                    "acc_stderr": 0.036563436533531585,
                    "acc_norm": 0.3583815028901734,
                    "acc_norm_stderr": 0.036563436533531585,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.04092563958237655,
                    "acc_norm": 0.21568627450980393,
                    "acc_norm_stderr": 0.04092563958237655,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.61,
                    "acc_stderr": 0.04902071300001974,
                    "acc_norm": 0.61,
                    "acc_norm_stderr": 0.04902071300001974,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.34893617021276596,
                    "acc_stderr": 0.031158522131357783,
                    "acc_norm": 0.34893617021276596,
                    "acc_norm_stderr": 0.031158522131357783,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.30701754385964913,
                    "acc_stderr": 0.043391383225798615,
                    "acc_norm": 0.30701754385964913,
                    "acc_norm_stderr": 0.043391383225798615,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.3586206896551724,
                    "acc_stderr": 0.03996629574876719,
                    "acc_norm": 0.3586206896551724,
                    "acc_norm_stderr": 0.03996629574876719,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2857142857142857,
                    "acc_stderr": 0.02326651221373056,
                    "acc_norm": 0.2857142857142857,
                    "acc_norm_stderr": 0.02326651221373056,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.2698412698412698,
                    "acc_stderr": 0.03970158273235172,
                    "acc_norm": 0.2698412698412698,
                    "acc_norm_stderr": 0.03970158273235172,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.048241815132442176,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.048241815132442176,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.535483870967742,
                    "acc_stderr": 0.028372287797962935,
                    "acc_norm": 0.535483870967742,
                    "acc_norm_stderr": 0.028372287797962935,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.35467980295566504,
                    "acc_stderr": 0.0336612448905145,
                    "acc_norm": 0.35467980295566504,
                    "acc_norm_stderr": 0.0336612448905145,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.51,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.51,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.4666666666666667,
                    "acc_stderr": 0.03895658065271846,
                    "acc_norm": 0.4666666666666667,
                    "acc_norm_stderr": 0.03895658065271846,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.5454545454545454,
                    "acc_stderr": 0.03547601494006937,
                    "acc_norm": 0.5454545454545454,
                    "acc_norm_stderr": 0.03547601494006937,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.6113989637305699,
                    "acc_stderr": 0.03517739796373132,
                    "acc_norm": 0.6113989637305699,
                    "acc_norm_stderr": 0.03517739796373132,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.44871794871794873,
                    "acc_stderr": 0.02521731518484648,
                    "acc_norm": 0.44871794871794873,
                    "acc_norm_stderr": 0.02521731518484648,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26296296296296295,
                    "acc_stderr": 0.02684205787383371,
                    "acc_norm": 0.26296296296296295,
                    "acc_norm_stderr": 0.02684205787383371,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.3907563025210084,
                    "acc_stderr": 0.031693802357129965,
                    "acc_norm": 0.3907563025210084,
                    "acc_norm_stderr": 0.031693802357129965,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.3708609271523179,
                    "acc_stderr": 0.03943966699183629,
                    "acc_norm": 0.3708609271523179,
                    "acc_norm_stderr": 0.03943966699183629,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.6495412844036698,
                    "acc_stderr": 0.02045607759982446,
                    "acc_norm": 0.6495412844036698,
                    "acc_norm_stderr": 0.02045607759982446,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.32407407407407407,
                    "acc_stderr": 0.03191923445686186,
                    "acc_norm": 0.32407407407407407,
                    "acc_norm_stderr": 0.03191923445686186,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.5490196078431373,
                    "acc_stderr": 0.03492406104163613,
                    "acc_norm": 0.5490196078431373,
                    "acc_norm_stderr": 0.03492406104163613,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.5780590717299579,
                    "acc_stderr": 0.03214814630240369,
                    "acc_norm": 0.5780590717299579,
                    "acc_norm_stderr": 0.03214814630240369,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.5022421524663677,
                    "acc_stderr": 0.03355746535223264,
                    "acc_norm": 0.5022421524663677,
                    "acc_norm_stderr": 0.03355746535223264,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.5190839694656488,
                    "acc_stderr": 0.04382094705550988,
                    "acc_norm": 0.5190839694656488,
                    "acc_norm_stderr": 0.04382094705550988,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.6115702479338843,
                    "acc_stderr": 0.04449270350068383,
                    "acc_norm": 0.6115702479338843,
                    "acc_norm_stderr": 0.04449270350068383,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.5740740740740741,
                    "acc_stderr": 0.0478034362693679,
                    "acc_norm": 0.5740740740740741,
                    "acc_norm_stderr": 0.0478034362693679,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.4662576687116564,
                    "acc_stderr": 0.03919415545048411,
                    "acc_norm": 0.4662576687116564,
                    "acc_norm_stderr": 0.03919415545048411,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.26785714285714285,
                    "acc_stderr": 0.04203277291467763,
                    "acc_norm": 0.26785714285714285,
                    "acc_norm_stderr": 0.04203277291467763,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.6310679611650486,
                    "acc_stderr": 0.0477761518115674,
                    "acc_norm": 0.6310679611650486,
                    "acc_norm_stderr": 0.0477761518115674,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.6794871794871795,
                    "acc_stderr": 0.03057281131029961,
                    "acc_norm": 0.6794871794871795,
                    "acc_norm_stderr": 0.03057281131029961,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.050251890762960605,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.050251890762960605,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.6564495530012772,
                    "acc_stderr": 0.016982145632652466,
                    "acc_norm": 0.6564495530012772,
                    "acc_norm_stderr": 0.016982145632652466,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.49710982658959535,
                    "acc_stderr": 0.02691864538323901,
                    "acc_norm": 0.49710982658959535,
                    "acc_norm_stderr": 0.02691864538323901,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.24581005586592178,
                    "acc_stderr": 0.014400296429225632,
                    "acc_norm": 0.24581005586592178,
                    "acc_norm_stderr": 0.014400296429225632,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.5326797385620915,
                    "acc_stderr": 0.02856869975222587,
                    "acc_norm": 0.5326797385620915,
                    "acc_norm_stderr": 0.02856869975222587,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.4662379421221865,
                    "acc_stderr": 0.028333277109562797,
                    "acc_norm": 0.4662379421221865,
                    "acc_norm_stderr": 0.028333277109562797,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.4876543209876543,
                    "acc_stderr": 0.027812262269327242,
                    "acc_norm": 0.4876543209876543,
                    "acc_norm_stderr": 0.027812262269327242,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.36879432624113473,
                    "acc_stderr": 0.02878222756134724,
                    "acc_norm": 0.36879432624113473,
                    "acc_norm_stderr": 0.02878222756134724,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.3578878748370274,
                    "acc_stderr": 0.012243563850490314,
                    "acc_norm": 0.3578878748370274,
                    "acc_norm_stderr": 0.012243563850490314,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4632352941176471,
                    "acc_stderr": 0.030290619180485694,
                    "acc_norm": 0.4632352941176471,
                    "acc_norm_stderr": 0.030290619180485694,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.45098039215686275,
                    "acc_stderr": 0.020130388312904528,
                    "acc_norm": 0.45098039215686275,
                    "acc_norm_stderr": 0.020130388312904528,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.5636363636363636,
                    "acc_stderr": 0.04750185058907296,
                    "acc_norm": 0.5636363636363636,
                    "acc_norm_stderr": 0.04750185058907296,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.5428571428571428,
                    "acc_stderr": 0.031891418324213966,
                    "acc_norm": 0.5428571428571428,
                    "acc_norm_stderr": 0.031891418324213966,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.6318407960199005,
                    "acc_stderr": 0.03410410565495301,
                    "acc_norm": 0.6318407960199005,
                    "acc_norm_stderr": 0.03410410565495301,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.66,
                    "acc_stderr": 0.04760952285695237,
                    "acc_norm": 0.66,
                    "acc_norm_stderr": 0.04760952285695237,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3674698795180723,
                    "acc_stderr": 0.03753267402120574,
                    "acc_norm": 0.3674698795180723,
                    "acc_norm_stderr": 0.03753267402120574,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.6549707602339181,
                    "acc_stderr": 0.03645981377388806,
                    "acc_norm": 0.6549707602339181,
                    "acc_norm_stderr": 0.03645981377388806,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.3537331701346389,
                    "mc1_stderr": 0.016737814358846147,
                    "mc2": 0.5149590028669258,
                    "mc2_stderr": 0.01603000361270752,
                    "timestamp": "2023-07-24T12-54-22.349435"
                }
            }
        }
    }
}