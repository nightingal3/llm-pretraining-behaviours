{
    "model_name": "01-ai/Yi-34B",
    "last_updated": "2023-11-02",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.6160409556313993,
                    "acc_stderr": 0.01421244498065189,
                    "acc_norm": 0.6459044368600683,
                    "acc_norm_stderr": 0.01397545412275656,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.656144194383589,
                    "acc_stderr": 0.004740229212473466,
                    "acc_norm": 0.8569010157339175,
                    "acc_norm_stderr": 0.003494581076398525,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.45,
                    "acc_stderr": 0.049999999999999996,
                    "acc_norm": 0.45,
                    "acc_norm_stderr": 0.049999999999999996,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.7481481481481481,
                    "acc_stderr": 0.03749850709174021,
                    "acc_norm": 0.7481481481481481,
                    "acc_norm_stderr": 0.03749850709174021,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.9013157894736842,
                    "acc_stderr": 0.024270227737522715,
                    "acc_norm": 0.9013157894736842,
                    "acc_norm_stderr": 0.024270227737522715,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.79,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.79,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.7924528301886793,
                    "acc_stderr": 0.02495991802891127,
                    "acc_norm": 0.7924528301886793,
                    "acc_norm_stderr": 0.02495991802891127,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.8819444444444444,
                    "acc_stderr": 0.026983346503309354,
                    "acc_norm": 0.8819444444444444,
                    "acc_norm_stderr": 0.026983346503309354,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.49,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.65,
                    "acc_stderr": 0.04793724854411019,
                    "acc_norm": 0.65,
                    "acc_norm_stderr": 0.04793724854411019,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.48,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.7109826589595376,
                    "acc_stderr": 0.03456425745086999,
                    "acc_norm": 0.7109826589595376,
                    "acc_norm_stderr": 0.03456425745086999,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.04975185951049946,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.04975185951049946,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.03861229196653694,
                    "acc_norm": 0.82,
                    "acc_norm_stderr": 0.03861229196653694,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.7702127659574468,
                    "acc_stderr": 0.02750175294441242,
                    "acc_norm": 0.7702127659574468,
                    "acc_norm_stderr": 0.02750175294441242,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.5526315789473685,
                    "acc_stderr": 0.04677473004491199,
                    "acc_norm": 0.5526315789473685,
                    "acc_norm_stderr": 0.04677473004491199,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.0333333333333333,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.0333333333333333,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.656084656084656,
                    "acc_stderr": 0.024464426625596437,
                    "acc_norm": 0.656084656084656,
                    "acc_norm_stderr": 0.024464426625596437,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.5634920634920635,
                    "acc_stderr": 0.04435932892851466,
                    "acc_norm": 0.5634920634920635,
                    "acc_norm_stderr": 0.04435932892851466,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.8806451612903226,
                    "acc_stderr": 0.018443411325315393,
                    "acc_norm": 0.8806451612903226,
                    "acc_norm_stderr": 0.018443411325315393,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.645320197044335,
                    "acc_stderr": 0.03366124489051449,
                    "acc_norm": 0.645320197044335,
                    "acc_norm_stderr": 0.03366124489051449,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.038612291966536955,
                    "acc_norm": 0.82,
                    "acc_norm_stderr": 0.038612291966536955,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.8666666666666667,
                    "acc_stderr": 0.026544435312706473,
                    "acc_norm": 0.8666666666666667,
                    "acc_norm_stderr": 0.026544435312706473,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.8939393939393939,
                    "acc_stderr": 0.021938047738853106,
                    "acc_norm": 0.8939393939393939,
                    "acc_norm_stderr": 0.021938047738853106,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.9792746113989638,
                    "acc_stderr": 0.010281417011909042,
                    "acc_norm": 0.9792746113989638,
                    "acc_norm_stderr": 0.010281417011909042,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.020280805062535726,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.020280805062535726,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.4444444444444444,
                    "acc_stderr": 0.030296771286067323,
                    "acc_norm": 0.4444444444444444,
                    "acc_norm_stderr": 0.030296771286067323,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.8571428571428571,
                    "acc_stderr": 0.02273020811930654,
                    "acc_norm": 0.8571428571428571,
                    "acc_norm_stderr": 0.02273020811930654,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.5165562913907285,
                    "acc_stderr": 0.04080244185628972,
                    "acc_norm": 0.5165562913907285,
                    "acc_norm_stderr": 0.04080244185628972,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.9155963302752294,
                    "acc_stderr": 0.011918819327334877,
                    "acc_norm": 0.9155963302752294,
                    "acc_norm_stderr": 0.011918819327334877,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.6388888888888888,
                    "acc_stderr": 0.032757734861009996,
                    "acc_norm": 0.6388888888888888,
                    "acc_norm_stderr": 0.032757734861009996,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.9166666666666666,
                    "acc_stderr": 0.019398452135813905,
                    "acc_norm": 0.9166666666666666,
                    "acc_norm_stderr": 0.019398452135813905,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.919831223628692,
                    "acc_stderr": 0.017676679991891625,
                    "acc_norm": 0.919831223628692,
                    "acc_norm_stderr": 0.017676679991891625,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.7937219730941704,
                    "acc_stderr": 0.027157150479563824,
                    "acc_norm": 0.7937219730941704,
                    "acc_norm_stderr": 0.027157150479563824,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.8625954198473282,
                    "acc_stderr": 0.030194823996804475,
                    "acc_norm": 0.8625954198473282,
                    "acc_norm_stderr": 0.030194823996804475,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.9090909090909091,
                    "acc_stderr": 0.02624319405407388,
                    "acc_norm": 0.9090909090909091,
                    "acc_norm_stderr": 0.02624319405407388,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8888888888888888,
                    "acc_stderr": 0.03038159675665167,
                    "acc_norm": 0.8888888888888888,
                    "acc_norm_stderr": 0.03038159675665167,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.8834355828220859,
                    "acc_stderr": 0.025212327210507108,
                    "acc_norm": 0.8834355828220859,
                    "acc_norm_stderr": 0.025212327210507108,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.5982142857142857,
                    "acc_stderr": 0.04653333146973647,
                    "acc_norm": 0.5982142857142857,
                    "acc_norm_stderr": 0.04653333146973647,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.912621359223301,
                    "acc_stderr": 0.027960689125970654,
                    "acc_norm": 0.912621359223301,
                    "acc_norm_stderr": 0.027960689125970654,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.9316239316239316,
                    "acc_stderr": 0.01653462768431136,
                    "acc_norm": 0.9316239316239316,
                    "acc_norm_stderr": 0.01653462768431136,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.87,
                    "acc_stderr": 0.033799766898963086,
                    "acc_norm": 0.87,
                    "acc_norm_stderr": 0.033799766898963086,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.9054916985951469,
                    "acc_stderr": 0.01046101533819307,
                    "acc_norm": 0.9054916985951469,
                    "acc_norm_stderr": 0.01046101533819307,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.8294797687861272,
                    "acc_stderr": 0.020247961569303728,
                    "acc_norm": 0.8294797687861272,
                    "acc_norm_stderr": 0.020247961569303728,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.6446927374301676,
                    "acc_stderr": 0.016006989934803192,
                    "acc_norm": 0.6446927374301676,
                    "acc_norm_stderr": 0.016006989934803192,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.8627450980392157,
                    "acc_stderr": 0.01970403918385981,
                    "acc_norm": 0.8627450980392157,
                    "acc_norm_stderr": 0.01970403918385981,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.8392282958199357,
                    "acc_stderr": 0.020862388082391888,
                    "acc_norm": 0.8392282958199357,
                    "acc_norm_stderr": 0.020862388082391888,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.8827160493827161,
                    "acc_stderr": 0.01790311261528112,
                    "acc_norm": 0.8827160493827161,
                    "acc_norm_stderr": 0.01790311261528112,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.6702127659574468,
                    "acc_stderr": 0.02804594694204241,
                    "acc_norm": 0.6702127659574468,
                    "acc_norm_stderr": 0.02804594694204241,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.6049543676662321,
                    "acc_stderr": 0.01248572781325157,
                    "acc_norm": 0.6049543676662321,
                    "acc_norm_stderr": 0.01248572781325157,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.8125,
                    "acc_stderr": 0.023709788253811766,
                    "acc_norm": 0.8125,
                    "acc_norm_stderr": 0.023709788253811766,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.8186274509803921,
                    "acc_stderr": 0.015588643495370457,
                    "acc_norm": 0.8186274509803921,
                    "acc_norm_stderr": 0.015588643495370457,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.7363636363636363,
                    "acc_stderr": 0.04220224692971987,
                    "acc_norm": 0.7363636363636363,
                    "acc_norm_stderr": 0.04220224692971987,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.8448979591836735,
                    "acc_stderr": 0.0231747988612186,
                    "acc_norm": 0.8448979591836735,
                    "acc_norm_stderr": 0.0231747988612186,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8905472636815921,
                    "acc_stderr": 0.022076326101824657,
                    "acc_norm": 0.8905472636815921,
                    "acc_norm_stderr": 0.022076326101824657,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.9,
                    "acc_stderr": 0.030151134457776334,
                    "acc_norm": 0.9,
                    "acc_norm_stderr": 0.030151134457776334,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5783132530120482,
                    "acc_stderr": 0.038444531817709175,
                    "acc_norm": 0.5783132530120482,
                    "acc_norm_stderr": 0.038444531817709175,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8771929824561403,
                    "acc_stderr": 0.02517298435015578,
                    "acc_norm": 0.8771929824561403,
                    "acc_norm_stderr": 0.02517298435015578,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.40758873929008566,
                    "mc1_stderr": 0.017201949234553107,
                    "mc2": 0.5623083932983032,
                    "mc2_stderr": 0.015165963671039869,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.6081166107382551,
                    "em_stderr": 0.004999326629880105,
                    "f1": 0.6419882550335565,
                    "f1_stderr": 0.004748239351156368,
                    "timestamp": "2023-11-08T19-46-38.378007"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.5064442759666414,
                    "acc_stderr": 0.013771340765699767,
                    "timestamp": "2023-11-08T19-46-38.378007"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.8303078137332282,
                    "acc_stderr": 0.010549542647363686,
                    "timestamp": "2023-11-08T19-46-38.378007"
                }
            }
        }
    }
}