{
    "model_name": "AbacusResearch/haLLAwa2",
    "last_updated": "2024-12-04 11:23:18.783926",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.06043956043956044,
                "exact_match_stderr": 0.010207626216646904,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.3099885189437428,
                "exact_match_stderr": 0.015679829530316276,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.08888888888888889,
                "exact_match_stderr": 0.012257870465567294,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.07641196013289037,
                "exact_match_stderr": 0.008845381151645904,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "minerva_math_geometry": {
                "exact_match": 0.1336116910229645,
                "exact_match_stderr": 0.015561969995340364,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.13924050632911392,
                "exact_match_stderr": 0.01591816995536744,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "minerva_math_algebra": {
                "exact_match": 0.22662173546756528,
                "exact_match_stderr": 0.012156384192199525,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_3da": {
                "acc": 0.9685,
                "acc_stderr": 0.003906597720891784,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_3ds": {
                "acc": 0.97,
                "acc_stderr": 0.003815400193861728,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_4da": {
                "acc": 0.9135,
                "acc_stderr": 0.006287180554084638,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_2ds": {
                "acc": 0.9955,
                "acc_stderr": 0.0014969954902233223,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_5ds": {
                "acc": 0.8495,
                "acc_stderr": 0.00799730288451757,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_5da": {
                "acc": 0.8895,
                "acc_stderr": 0.007012093819243022,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_1dc": {
                "acc": 0.6425,
                "acc_stderr": 0.010719343597608066,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_4ds": {
                "acc": 0.9125,
                "acc_stderr": 0.006319956164639155,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_2dm": {
                "acc": 0.617,
                "acc_stderr": 0.010872654105766934,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "arithmetic_2da": {
                "acc": 0.986,
                "acc_stderr": 0.0026278228110667933,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "gsm8k_cot": {
                "exact_match": 0.5701288855193328,
                "exact_match_stderr": 0.013636344017393732,
                "timestamp": "2024-06-10T10-29-02.405831"
            },
            "gsm8k": {
                "exact_match": 0.5276724791508719,
                "exact_match_stderr": 0.013751375538801326,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "anli_r2": {
                "brier_score": 1.0924707676244783,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "anli_r3": {
                "brier_score": 1.0236139706947502,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "anli_r1": {
                "brier_score": 1.0837828290354423,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_eu": {
                "brier_score": 1.1439712925050591,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_vi": {
                "brier_score": 1.0246823649930676,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_ru": {
                "brier_score": 0.9703116231053286,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_zh": {
                "brier_score": 1.2234280607922878,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_tr": {
                "brier_score": 1.0282731990983742,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_fr": {
                "brier_score": 0.9242344176415858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_en": {
                "brier_score": 0.6881897992605256,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_ur": {
                "brier_score": 1.2513617048946404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_ar": {
                "brier_score": 1.305527995518514,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_de": {
                "brier_score": 1.018285594258703,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_hi": {
                "brier_score": 1.023238689228211,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_es": {
                "brier_score": 0.9770376302469205,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_bg": {
                "brier_score": 0.9690174071509365,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_sw": {
                "brier_score": 0.9981986621227679,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_el": {
                "brier_score": 0.9569683975606962,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "xnli_th": {
                "brier_score": 1.0281918906994378,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "logiqa2": {
                "brier_score": 0.9982039832917935,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "mathqa": {
                "brier_score": 0.9685303356773713,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T10-39-51.764112"
            },
            "lambada_standard": {
                "perplexity": 3.7062962855818853,
                "perplexity_stderr": 0.11093935427091443,
                "acc": 0.6743644478944304,
                "acc_stderr": 0.006528678957835463,
                "timestamp": "2024-06-10T10-41-20.515217"
            },
            "lambada_openai": {
                "perplexity": 2.8051253790007746,
                "perplexity_stderr": 0.07002379774910768,
                "acc": 0.74345041723268,
                "acc_stderr": 0.006084483727167678,
                "timestamp": "2024-06-10T10-41-20.515217"
            },
            "mmlu_world_religions": {
                "acc": 0.8362573099415205,
                "acc_stderr": 0.028380919596145866,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_formal_logic": {
                "acc": 0.40476190476190477,
                "acc_stderr": 0.043902592653775614,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_prehistory": {
                "acc": 0.7160493827160493,
                "acc_stderr": 0.02508947852376513,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.4223463687150838,
                "acc_stderr": 0.016519594275297114,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.810126582278481,
                "acc_stderr": 0.025530100460233497,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_moral_disputes": {
                "acc": 0.7369942196531792,
                "acc_stderr": 0.023703099525258172,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_professional_law": {
                "acc": 0.45827900912646674,
                "acc_stderr": 0.012725701656953642,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.7607361963190185,
                "acc_stderr": 0.0335195387952127,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.8137254901960784,
                "acc_stderr": 0.027325470966716333,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_philosophy": {
                "acc": 0.7266881028938906,
                "acc_stderr": 0.025311765975426115,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_jurisprudence": {
                "acc": 0.8055555555555556,
                "acc_stderr": 0.038260763248848646,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_international_law": {
                "acc": 0.8181818181818182,
                "acc_stderr": 0.035208939510976506,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.7636363636363637,
                "acc_stderr": 0.03317505930009181,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.8704663212435233,
                "acc_stderr": 0.02423353229775872,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.6512605042016807,
                "acc_stderr": 0.030956636328566545,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_geography": {
                "acc": 0.7777777777777778,
                "acc_stderr": 0.02962022787479047,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.8256880733944955,
                "acc_stderr": 0.01626567563201037,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_public_relations": {
                "acc": 0.6454545454545455,
                "acc_stderr": 0.045820048415054174,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.89,
                "acc_stderr": 0.03144660377352203,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_sociology": {
                "acc": 0.8606965174129353,
                "acc_stderr": 0.02448448716291397,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.6128205128205129,
                "acc_stderr": 0.024697216930878937,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_security_studies": {
                "acc": 0.726530612244898,
                "acc_stderr": 0.02853556033712844,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_professional_psychology": {
                "acc": 0.6519607843137255,
                "acc_stderr": 0.019270998708223977,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_human_sexuality": {
                "acc": 0.7557251908396947,
                "acc_stderr": 0.03768335959728744,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_econometrics": {
                "acc": 0.4824561403508772,
                "acc_stderr": 0.04700708033551038,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_miscellaneous": {
                "acc": 0.8084291187739464,
                "acc_stderr": 0.014072859310451949,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_marketing": {
                "acc": 0.8803418803418803,
                "acc_stderr": 0.021262719400406957,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_management": {
                "acc": 0.7961165048543689,
                "acc_stderr": 0.039891398595317706,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_nutrition": {
                "acc": 0.7516339869281046,
                "acc_stderr": 0.024739981355113592,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_medical_genetics": {
                "acc": 0.67,
                "acc_stderr": 0.04725815626252607,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_human_aging": {
                "acc": 0.6995515695067265,
                "acc_stderr": 0.030769352008229136,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_professional_medicine": {
                "acc": 0.6948529411764706,
                "acc_stderr": 0.0279715413701706,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_college_medicine": {
                "acc": 0.6011560693641619,
                "acc_stderr": 0.037336266553835096,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_business_ethics": {
                "acc": 0.56,
                "acc_stderr": 0.04988876515698589,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.6981132075471698,
                "acc_stderr": 0.028254200344438662,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_global_facts": {
                "acc": 0.4,
                "acc_stderr": 0.04923659639173309,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_virology": {
                "acc": 0.5421686746987951,
                "acc_stderr": 0.038786267710023595,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_professional_accounting": {
                "acc": 0.44680851063829785,
                "acc_stderr": 0.029658235097666907,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_college_physics": {
                "acc": 0.4411764705882353,
                "acc_stderr": 0.04940635630605659,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2847682119205298,
                "acc_stderr": 0.03684881521389024,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_biology": {
                "acc": 0.7419354838709677,
                "acc_stderr": 0.024892469172462836,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_college_biology": {
                "acc": 0.7222222222222222,
                "acc_stderr": 0.037455547914624576,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_anatomy": {
                "acc": 0.6222222222222222,
                "acc_stderr": 0.04188307537595853,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_college_chemistry": {
                "acc": 0.46,
                "acc_stderr": 0.05009082659620332,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_computer_security": {
                "acc": 0.77,
                "acc_stderr": 0.04229525846816506,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_college_computer_science": {
                "acc": 0.52,
                "acc_stderr": 0.050211673156867795,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_astronomy": {
                "acc": 0.6710526315789473,
                "acc_stderr": 0.03823428969926604,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_college_mathematics": {
                "acc": 0.36,
                "acc_stderr": 0.048241815132442176,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.5872340425531914,
                "acc_stderr": 0.03218471141400351,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.35,
                "acc_stderr": 0.047937248544110196,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.67,
                "acc_stderr": 0.04725815626252607,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_machine_learning": {
                "acc": 0.4732142857142857,
                "acc_stderr": 0.047389751192741546,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.4876847290640394,
                "acc_stderr": 0.035169204442208966,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.48148148148148145,
                "acc_stderr": 0.03407632093854051,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.3835978835978836,
                "acc_stderr": 0.025043757318520196,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.5310344827586206,
                "acc_stderr": 0.04158632762097828,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.3814814814814815,
                "acc_stderr": 0.029616718927497596,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "arc_challenge": {
                "acc": 0.6023890784982935,
                "acc_stderr": 0.014301752223279538,
                "acc_norm": 0.6407849829351536,
                "acc_norm_stderr": 0.014020224155839154,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "hellaswag": {
                "acc": 0.68123879705238,
                "acc_stderr": 0.004650438781745276,
                "acc_norm": 0.8464449312885879,
                "acc_norm_stderr": 0.00359784913981503,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "truthfulqa_mc2": {
                "acc": 0.47657125753705704,
                "acc_stderr": 0.01563309801578014,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "truthfulqa_gen": {
                "bleu_max": 26.979448196778783,
                "bleu_max_stderr": 0.7893178897204924,
                "bleu_acc": 0.3929008567931457,
                "bleu_acc_stderr": 0.017097248285233065,
                "bleu_diff": -2.604511700508371,
                "bleu_diff_stderr": 0.8015257472068844,
                "rouge1_max": 52.66916062467333,
                "rouge1_max_stderr": 0.8511635282334032,
                "rouge1_acc": 0.3953488372093023,
                "rouge1_acc_stderr": 0.017115815632418204,
                "rouge1_diff": -4.016130326745816,
                "rouge1_diff_stderr": 0.974035197973408,
                "rouge2_max": 37.88807807215924,
                "rouge2_max_stderr": 0.9893255272073741,
                "rouge2_acc": 0.35495716034271724,
                "rouge2_acc_stderr": 0.0167508623813759,
                "rouge2_diff": -4.403291370670143,
                "rouge2_diff_stderr": 1.1060638545857444,
                "rougeL_max": 49.48769052738361,
                "rougeL_max_stderr": 0.8679658445507986,
                "rougeL_acc": 0.3708690330477356,
                "rougeL_acc_stderr": 0.01690969358024884,
                "rougeL_diff": -4.579117328573877,
                "rougeL_diff_stderr": 0.9886743725581556,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "truthfulqa_mc1": {
                "acc": 0.33414932680538556,
                "acc_stderr": 0.01651253067715055,
                "timestamp": "2024-11-07T04-59-50.627973"
            },
            "winogrande": {
                "acc": 0.760852407261247,
                "acc_stderr": 0.011988541844843903,
                "timestamp": "2024-11-07T04-59-50.627973"
            }
        }
    }
}