{
    "model_name": "jb723/llama2-ko-7B-model",
    "last_updated": "2024-12-04 11:24:11.063799",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.5349829351535836,
                    "acc_stderr": 0.014575583922019674,
                    "acc_norm": 0.5631399317406144,
                    "acc_norm_stderr": 0.014494421584256532,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6069508066122287,
                    "acc_stderr": 0.004874293964843518,
                    "acc_norm": 0.7893845847440749,
                    "acc_norm_stderr": 0.004069123905324906,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.45925925925925926,
                    "acc_stderr": 0.04304979692464242,
                    "acc_norm": 0.45925925925925926,
                    "acc_norm_stderr": 0.04304979692464242,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.4605263157894737,
                    "acc_stderr": 0.04056242252249034,
                    "acc_norm": 0.4605263157894737,
                    "acc_norm_stderr": 0.04056242252249034,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.45,
                    "acc_stderr": 0.05,
                    "acc_norm": 0.45,
                    "acc_norm_stderr": 0.05,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.45660377358490567,
                    "acc_stderr": 0.030656748696739438,
                    "acc_norm": 0.45660377358490567,
                    "acc_norm_stderr": 0.030656748696739438,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.4375,
                    "acc_stderr": 0.04148415739394154,
                    "acc_norm": 0.4375,
                    "acc_norm_stderr": 0.04148415739394154,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.04824181513244218,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.04824181513244218,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252604,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.04852365870939098,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.04852365870939098,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.4046242774566474,
                    "acc_stderr": 0.03742461193887248,
                    "acc_norm": 0.4046242774566474,
                    "acc_norm_stderr": 0.03742461193887248,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.14705882352941177,
                    "acc_stderr": 0.035240689515674495,
                    "acc_norm": 0.14705882352941177,
                    "acc_norm_stderr": 0.035240689515674495,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.05021167315686779,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.05021167315686779,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.4127659574468085,
                    "acc_stderr": 0.03218471141400351,
                    "acc_norm": 0.4127659574468085,
                    "acc_norm_stderr": 0.03218471141400351,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.32456140350877194,
                    "acc_stderr": 0.04404556157374767,
                    "acc_norm": 0.32456140350877194,
                    "acc_norm_stderr": 0.04404556157374767,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.4482758620689655,
                    "acc_stderr": 0.04144311810878151,
                    "acc_norm": 0.4482758620689655,
                    "acc_norm_stderr": 0.04144311810878151,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.31746031746031744,
                    "acc_stderr": 0.023973861998992072,
                    "acc_norm": 0.31746031746031744,
                    "acc_norm_stderr": 0.023973861998992072,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.23809523809523808,
                    "acc_stderr": 0.03809523809523812,
                    "acc_norm": 0.23809523809523808,
                    "acc_norm_stderr": 0.03809523809523812,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001974,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.04902071300001974,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.4935483870967742,
                    "acc_stderr": 0.02844163823354051,
                    "acc_norm": 0.4935483870967742,
                    "acc_norm_stderr": 0.02844163823354051,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.33004926108374383,
                    "acc_stderr": 0.033085304262282574,
                    "acc_norm": 0.33004926108374383,
                    "acc_norm_stderr": 0.033085304262282574,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.41,
                    "acc_stderr": 0.04943110704237102,
                    "acc_norm": 0.41,
                    "acc_norm_stderr": 0.04943110704237102,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.5878787878787879,
                    "acc_stderr": 0.03843566993588717,
                    "acc_norm": 0.5878787878787879,
                    "acc_norm_stderr": 0.03843566993588717,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.5757575757575758,
                    "acc_stderr": 0.035212249088415845,
                    "acc_norm": 0.5757575757575758,
                    "acc_norm_stderr": 0.035212249088415845,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.689119170984456,
                    "acc_stderr": 0.033403619062765864,
                    "acc_norm": 0.689119170984456,
                    "acc_norm_stderr": 0.033403619062765864,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.4461538461538462,
                    "acc_stderr": 0.02520357177302833,
                    "acc_norm": 0.4461538461538462,
                    "acc_norm_stderr": 0.02520357177302833,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.026719240783712173,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.026719240783712173,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.40756302521008403,
                    "acc_stderr": 0.03191863374478466,
                    "acc_norm": 0.40756302521008403,
                    "acc_norm_stderr": 0.03191863374478466,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31788079470198677,
                    "acc_stderr": 0.03802039760107903,
                    "acc_norm": 0.31788079470198677,
                    "acc_norm_stderr": 0.03802039760107903,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.6146788990825688,
                    "acc_stderr": 0.02086585085279412,
                    "acc_norm": 0.6146788990825688,
                    "acc_norm_stderr": 0.02086585085279412,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.28703703703703703,
                    "acc_stderr": 0.030851992993257017,
                    "acc_norm": 0.28703703703703703,
                    "acc_norm_stderr": 0.030851992993257017,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.5833333333333334,
                    "acc_stderr": 0.03460228327239171,
                    "acc_norm": 0.5833333333333334,
                    "acc_norm_stderr": 0.03460228327239171,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.6666666666666666,
                    "acc_stderr": 0.030685820596610795,
                    "acc_norm": 0.6666666666666666,
                    "acc_norm_stderr": 0.030685820596610795,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.5246636771300448,
                    "acc_stderr": 0.03351695167652628,
                    "acc_norm": 0.5246636771300448,
                    "acc_norm_stderr": 0.03351695167652628,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.5343511450381679,
                    "acc_stderr": 0.04374928560599738,
                    "acc_norm": 0.5343511450381679,
                    "acc_norm_stderr": 0.04374928560599738,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.6446280991735537,
                    "acc_stderr": 0.0436923632657398,
                    "acc_norm": 0.6446280991735537,
                    "acc_norm_stderr": 0.0436923632657398,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.04833682445228318,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.04833682445228318,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.44785276073619634,
                    "acc_stderr": 0.03906947479456602,
                    "acc_norm": 0.44785276073619634,
                    "acc_norm_stderr": 0.03906947479456602,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.29464285714285715,
                    "acc_stderr": 0.043270409325787296,
                    "acc_norm": 0.29464285714285715,
                    "acc_norm_stderr": 0.043270409325787296,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.5631067961165048,
                    "acc_stderr": 0.04911147107365777,
                    "acc_norm": 0.5631067961165048,
                    "acc_norm_stderr": 0.04911147107365777,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.6752136752136753,
                    "acc_stderr": 0.030679022765498828,
                    "acc_norm": 0.6752136752136753,
                    "acc_norm_stderr": 0.030679022765498828,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.53,
                    "acc_stderr": 0.05016135580465919,
                    "acc_norm": 0.53,
                    "acc_norm_stderr": 0.05016135580465919,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.6577266922094508,
                    "acc_stderr": 0.016967031766413624,
                    "acc_norm": 0.6577266922094508,
                    "acc_norm_stderr": 0.016967031766413624,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.5202312138728323,
                    "acc_stderr": 0.026897049996382868,
                    "acc_norm": 0.5202312138728323,
                    "acc_norm_stderr": 0.026897049996382868,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.23798882681564246,
                    "acc_stderr": 0.014242630070574915,
                    "acc_norm": 0.23798882681564246,
                    "acc_norm_stderr": 0.014242630070574915,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.477124183006536,
                    "acc_stderr": 0.028599936776089782,
                    "acc_norm": 0.477124183006536,
                    "acc_norm_stderr": 0.028599936776089782,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.5562700964630225,
                    "acc_stderr": 0.028217683556652315,
                    "acc_norm": 0.5562700964630225,
                    "acc_norm_stderr": 0.028217683556652315,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.5216049382716049,
                    "acc_stderr": 0.027794760105008736,
                    "acc_norm": 0.5216049382716049,
                    "acc_norm_stderr": 0.027794760105008736,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.3475177304964539,
                    "acc_stderr": 0.02840662780959095,
                    "acc_norm": 0.3475177304964539,
                    "acc_norm_stderr": 0.02840662780959095,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.35528031290743156,
                    "acc_stderr": 0.012223623364044037,
                    "acc_norm": 0.35528031290743156,
                    "acc_norm_stderr": 0.012223623364044037,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4411764705882353,
                    "acc_stderr": 0.03016191193076711,
                    "acc_norm": 0.4411764705882353,
                    "acc_norm_stderr": 0.03016191193076711,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.4526143790849673,
                    "acc_stderr": 0.020136790918492523,
                    "acc_norm": 0.4526143790849673,
                    "acc_norm_stderr": 0.020136790918492523,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.5272727272727272,
                    "acc_stderr": 0.04782001791380061,
                    "acc_norm": 0.5272727272727272,
                    "acc_norm_stderr": 0.04782001791380061,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.4326530612244898,
                    "acc_stderr": 0.031717528240626645,
                    "acc_norm": 0.4326530612244898,
                    "acc_norm_stderr": 0.031717528240626645,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.572139303482587,
                    "acc_stderr": 0.03498541988407795,
                    "acc_norm": 0.572139303482587,
                    "acc_norm_stderr": 0.03498541988407795,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.63,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.63,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.42168674698795183,
                    "acc_stderr": 0.03844453181770917,
                    "acc_norm": 0.42168674698795183,
                    "acc_norm_stderr": 0.03844453181770917,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.6842105263157895,
                    "acc_stderr": 0.03565079670708311,
                    "acc_norm": 0.6842105263157895,
                    "acc_norm_stderr": 0.03565079670708311,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.28151774785801714,
                    "mc1_stderr": 0.015744027248256045,
                    "mc2": 0.4097811489004275,
                    "mc2_stderr": 0.015552291335837638,
                    "timestamp": "2023-09-22T03-46-09.444345"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.23427013422818793,
                    "acc_stderr": 0.004337464243138509,
                    "f1": 0.3152516778523505,
                    "f1_stderr": 0.004353725712557671,
                    "timestamp": "2023-10-28T09-35-48.028758"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.0803639120545868,
                    "acc_stderr": 0.007488258573239077,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7000789265982637,
                    "acc_stderr": 0.012878347526636072,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.672514619883041,
                    "acc_stderr": 0.035993357714560276,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.24603174603174602,
                    "acc_stderr": 0.038522733649243156,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.5154320987654321,
                    "acc_stderr": 0.027807490044276184,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.23798882681564246,
                    "acc_stderr": 0.014242630070574885,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.6751054852320675,
                    "acc_stderr": 0.030486039389105307,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.5260115606936416,
                    "acc_stderr": 0.02688264343402289,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.3546284224250326,
                    "acc_stderr": 0.012218576439090174,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.4785276073619632,
                    "acc_stderr": 0.0392474687675113,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.5637254901960784,
                    "acc_stderr": 0.03480693138457038,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.5594855305466238,
                    "acc_stderr": 0.02819640057419743,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.49074074074074076,
                    "acc_stderr": 0.04832853553437055,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.6033057851239669,
                    "acc_stderr": 0.04465869780531009,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.5757575757575758,
                    "acc_stderr": 0.038592681420702636,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.6787564766839378,
                    "acc_stderr": 0.033699508685490674,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.41596638655462187,
                    "acc_stderr": 0.03201650100739615,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.5909090909090909,
                    "acc_stderr": 0.03502975799413007,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.6201834862385321,
                    "acc_stderr": 0.020808825617866244,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.5545454545454546,
                    "acc_stderr": 0.047605488214603246,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.64,
                    "acc_stderr": 0.04824181513244218,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.5870646766169154,
                    "acc_stderr": 0.03481520803367348,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.44358974358974357,
                    "acc_stderr": 0.025189149894764194,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.4489795918367347,
                    "acc_stderr": 0.03184213866687579,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.4477124183006536,
                    "acc_stderr": 0.020116925347422425,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.549618320610687,
                    "acc_stderr": 0.04363643698524779,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.32456140350877194,
                    "acc_stderr": 0.04404556157374767,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.6360153256704981,
                    "acc_stderr": 0.017205684809032232,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.6623931623931624,
                    "acc_stderr": 0.030980296992618558,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.5825242718446602,
                    "acc_stderr": 0.048828405482122375,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.4803921568627451,
                    "acc_stderr": 0.028607893699576063,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.56,
                    "acc_stderr": 0.049888765156985884,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.5336322869955157,
                    "acc_stderr": 0.03348180017060306,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.4338235294117647,
                    "acc_stderr": 0.030105636570016636,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.43352601156069365,
                    "acc_stderr": 0.03778621079092056,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.4679245283018868,
                    "acc_stderr": 0.030709486992556552,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001974,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.4397590361445783,
                    "acc_stderr": 0.03864139923699122,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.3617021276595745,
                    "acc_stderr": 0.028663820147199492,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.14705882352941177,
                    "acc_stderr": 0.035240689515674495,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.31788079470198677,
                    "acc_stderr": 0.038020397601079024,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.028444006199428714,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.4652777777777778,
                    "acc_stderr": 0.04171115858181618,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.4444444444444444,
                    "acc_stderr": 0.04292596718256981,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.048523658709391,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.51,
                    "acc_stderr": 0.05024183937956911,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.047937248544110196,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.4407894736842105,
                    "acc_stderr": 0.04040311062490437,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.03202563076101735,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.41,
                    "acc_stderr": 0.04943110704237102,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.32142857142857145,
                    "acc_stderr": 0.04432804055291518,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.32019704433497537,
                    "acc_stderr": 0.032826493853041504,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.2824074074074074,
                    "acc_stderr": 0.03070137211151092,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.31746031746031744,
                    "acc_stderr": 0.023973861998992072,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.47586206896551725,
                    "acc_stderr": 0.0416180850350153,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.026466117538959912,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.5136518771331058,
                    "acc_stderr": 0.014605943429860942,
                    "acc_norm": 0.53839590443686,
                    "acc_norm_stderr": 0.014568245550296365,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.4095609976995192,
                    "acc_stderr": 0.015544249293655412,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 20.89104935353322,
                    "bleu_max_stderr": 0.690580917763846,
                    "bleu_acc": 0.34394124847001223,
                    "bleu_acc_stderr": 0.016629087514276816,
                    "bleu_diff": -4.982714805919928,
                    "bleu_diff_stderr": 0.6525563274154834,
                    "rouge1_max": 46.0179025495015,
                    "rouge1_max_stderr": 0.8157872400574628,
                    "rouge1_acc": 0.34149326805385555,
                    "rouge1_acc_stderr": 0.016600688619950833,
                    "rouge1_diff": -7.817434391713708,
                    "rouge1_diff_stderr": 0.7537330659697064,
                    "rouge2_max": 30.284883181541517,
                    "rouge2_max_stderr": 0.885344085472441,
                    "rouge2_acc": 0.2913096695226438,
                    "rouge2_acc_stderr": 0.015905987048184824,
                    "rouge2_diff": -8.624090028516706,
                    "rouge2_diff_stderr": 0.8953473056334376,
                    "rougeL_max": 42.39510902130912,
                    "rougeL_max_stderr": 0.8131112049425046,
                    "rougeL_acc": 0.31334149326805383,
                    "rougeL_acc_stderr": 0.016238065069059573,
                    "rougeL_diff": -8.169098689998256,
                    "rougeL_diff_stderr": 0.7490350953113237,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.2876376988984088,
                    "acc_stderr": 0.015846315101394795,
                    "timestamp": "2024-11-21T09-38-35.283575"
                }
            }
        }
    }
}