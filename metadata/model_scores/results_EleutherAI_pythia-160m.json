{
    "model_name": "EleutherAI/pythia-160m",
    "last_updated": "2023-10-19",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.19027303754266212,
                    "acc_stderr": 0.011470424179225697,
                    "acc_norm": 0.22781569965870307,
                    "acc_norm_stderr": 0.012256708602326905,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.2874925313682533,
                    "acc_stderr": 0.0045166819538790745,
                    "acc_norm": 0.30342561242780325,
                    "acc_norm_stderr": 0.004587978625582475,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932268,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932268,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.03749850709174021,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.03749850709174021,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.17763157894736842,
                    "acc_stderr": 0.031103182383123398,
                    "acc_norm": 0.17763157894736842,
                    "acc_norm_stderr": 0.031103182383123398,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2037735849056604,
                    "acc_stderr": 0.0247907845017754,
                    "acc_norm": 0.2037735849056604,
                    "acc_norm_stderr": 0.0247907845017754,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.24305555555555555,
                    "acc_stderr": 0.03586879280080342,
                    "acc_norm": 0.24305555555555555,
                    "acc_norm_stderr": 0.03586879280080342,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036845,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036845,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768079,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768079,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.23699421965317918,
                    "acc_stderr": 0.03242414757483098,
                    "acc_norm": 0.23699421965317918,
                    "acc_norm_stderr": 0.03242414757483098,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.04092563958237654,
                    "acc_norm": 0.21568627450980393,
                    "acc_norm_stderr": 0.04092563958237654,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536975,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.038612291966536975,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2680851063829787,
                    "acc_stderr": 0.028957342788342343,
                    "acc_norm": 0.2680851063829787,
                    "acc_norm_stderr": 0.028957342788342343,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.24561403508771928,
                    "acc_stderr": 0.04049339297748141,
                    "acc_norm": 0.24561403508771928,
                    "acc_norm_stderr": 0.04049339297748141,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2482758620689655,
                    "acc_stderr": 0.036001056927277716,
                    "acc_norm": 0.2482758620689655,
                    "acc_norm_stderr": 0.036001056927277716,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.25132275132275134,
                    "acc_stderr": 0.022340482339643898,
                    "acc_norm": 0.25132275132275134,
                    "acc_norm_stderr": 0.022340482339643898,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.1984126984126984,
                    "acc_stderr": 0.03567016675276863,
                    "acc_norm": 0.1984126984126984,
                    "acc_norm_stderr": 0.03567016675276863,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536934,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.038612291966536934,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.31290322580645163,
                    "acc_stderr": 0.02637756702864586,
                    "acc_norm": 0.31290322580645163,
                    "acc_norm_stderr": 0.02637756702864586,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.270935960591133,
                    "acc_stderr": 0.03127090713297698,
                    "acc_norm": 0.270935960591133,
                    "acc_norm_stderr": 0.03127090713297698,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.040201512610368466,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.040201512610368466,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.24848484848484848,
                    "acc_stderr": 0.033744026441394036,
                    "acc_norm": 0.24848484848484848,
                    "acc_norm_stderr": 0.033744026441394036,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.23232323232323232,
                    "acc_stderr": 0.030088629490217483,
                    "acc_norm": 0.23232323232323232,
                    "acc_norm_stderr": 0.030088629490217483,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.24352331606217617,
                    "acc_stderr": 0.03097543638684543,
                    "acc_norm": 0.24352331606217617,
                    "acc_norm_stderr": 0.03097543638684543,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.24615384615384617,
                    "acc_stderr": 0.021840866990423077,
                    "acc_norm": 0.24615384615384617,
                    "acc_norm_stderr": 0.021840866990423077,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.02646611753895991,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.02646611753895991,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.27310924369747897,
                    "acc_stderr": 0.02894200404099817,
                    "acc_norm": 0.27310924369747897,
                    "acc_norm_stderr": 0.02894200404099817,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.271523178807947,
                    "acc_stderr": 0.03631329803969654,
                    "acc_norm": 0.271523178807947,
                    "acc_norm_stderr": 0.03631329803969654,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.22385321100917432,
                    "acc_stderr": 0.017871217767790215,
                    "acc_norm": 0.22385321100917432,
                    "acc_norm_stderr": 0.017871217767790215,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.47685185185185186,
                    "acc_stderr": 0.03406315360711507,
                    "acc_norm": 0.47685185185185186,
                    "acc_norm_stderr": 0.03406315360711507,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.25980392156862747,
                    "acc_stderr": 0.03077855467869327,
                    "acc_norm": 0.25980392156862747,
                    "acc_norm_stderr": 0.03077855467869327,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.29535864978902954,
                    "acc_stderr": 0.029696338713422876,
                    "acc_norm": 0.29535864978902954,
                    "acc_norm_stderr": 0.029696338713422876,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.3094170403587444,
                    "acc_stderr": 0.03102441174057222,
                    "acc_norm": 0.3094170403587444,
                    "acc_norm_stderr": 0.03102441174057222,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.037276735755969174,
                    "acc_norm": 0.2366412213740458,
                    "acc_norm_stderr": 0.037276735755969174,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.2231404958677686,
                    "acc_stderr": 0.03800754475228733,
                    "acc_norm": 0.2231404958677686,
                    "acc_norm_stderr": 0.03800754475228733,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.043300437496507437,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.043300437496507437,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2085889570552147,
                    "acc_stderr": 0.03192193448934724,
                    "acc_norm": 0.2085889570552147,
                    "acc_norm_stderr": 0.03192193448934724,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.1875,
                    "acc_stderr": 0.0370468111477387,
                    "acc_norm": 0.1875,
                    "acc_norm_stderr": 0.0370468111477387,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.1941747572815534,
                    "acc_stderr": 0.03916667762822585,
                    "acc_norm": 0.1941747572815534,
                    "acc_norm_stderr": 0.03916667762822585,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.18376068376068377,
                    "acc_stderr": 0.025372139671722933,
                    "acc_norm": 0.18376068376068377,
                    "acc_norm_stderr": 0.025372139671722933,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621504,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621504,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.25287356321839083,
                    "acc_stderr": 0.01554337731371968,
                    "acc_norm": 0.25287356321839083,
                    "acc_norm_stderr": 0.01554337731371968,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2630057803468208,
                    "acc_stderr": 0.023703099525258172,
                    "acc_norm": 0.2630057803468208,
                    "acc_norm_stderr": 0.023703099525258172,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2770949720670391,
                    "acc_stderr": 0.01496877243581215,
                    "acc_norm": 0.2770949720670391,
                    "acc_norm_stderr": 0.01496877243581215,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.024288619466046112,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.024288619466046112,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.18971061093247588,
                    "acc_stderr": 0.02226819625878323,
                    "acc_norm": 0.18971061093247588,
                    "acc_norm_stderr": 0.02226819625878323,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.024383665531035447,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.024383665531035447,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2801418439716312,
                    "acc_stderr": 0.026789172351140242,
                    "acc_norm": 0.2801418439716312,
                    "acc_norm_stderr": 0.026789172351140242,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.23728813559322035,
                    "acc_stderr": 0.010865436690780272,
                    "acc_norm": 0.23728813559322035,
                    "acc_norm_stderr": 0.010865436690780272,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4485294117647059,
                    "acc_stderr": 0.030211479609121593,
                    "acc_norm": 0.4485294117647059,
                    "acc_norm_stderr": 0.030211479609121593,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.25163398692810457,
                    "acc_stderr": 0.01755581809132226,
                    "acc_norm": 0.25163398692810457,
                    "acc_norm_stderr": 0.01755581809132226,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.04172343038705383,
                    "acc_norm": 0.2545454545454545,
                    "acc_norm_stderr": 0.04172343038705383,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.24489795918367346,
                    "acc_stderr": 0.027529637440174937,
                    "acc_norm": 0.24489795918367346,
                    "acc_norm_stderr": 0.027529637440174937,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.24378109452736318,
                    "acc_stderr": 0.03036049015401465,
                    "acc_norm": 0.24378109452736318,
                    "acc_norm_stderr": 0.03036049015401465,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768079,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768079,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.18072289156626506,
                    "acc_stderr": 0.02995573785581014,
                    "acc_norm": 0.18072289156626506,
                    "acc_norm_stderr": 0.02995573785581014,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.2046783625730994,
                    "acc_stderr": 0.03094445977853322,
                    "acc_norm": 0.2046783625730994,
                    "acc_norm_stderr": 0.03094445977853322,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.25091799265605874,
                    "mc1_stderr": 0.015176985027707687,
                    "mc2": 0.44263082100781215,
                    "mc2_stderr": 0.014953643384240857,
                    "timestamp": "2023-07-19T14-01-14.258064"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.0012583892617449664,
                    "em_stderr": 0.0003630560893119131,
                    "f1": 0.03449874161073832,
                    "f1_stderr": 0.0010696643616809897,
                    "timestamp": "2023-10-19T00-42-11.960734"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.002274450341167551,
                    "acc_stderr": 0.0013121578148674164,
                    "timestamp": "2023-10-19T00-42-11.960734"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5153906866614049,
                    "acc_stderr": 0.01404582678978366,
                    "timestamp": "2023-10-19T00-42-11.960734"
                }
            }
        }
    }
}