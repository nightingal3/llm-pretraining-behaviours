{
    "model_name": "google__gemma-7b",
    "last_updated": "2024-12-19 13:41:20.232884",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.8362573099415205,
                "acc_stderr": 0.028380919596145866,
                "brier_score": 0.23192067938649402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_formal_logic": {
                "acc": 0.4523809523809524,
                "acc_stderr": 0.044518079590553275,
                "brier_score": 0.6492991823803103,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_prehistory": {
                "acc": 0.7067901234567902,
                "acc_stderr": 0.02532988817190092,
                "brier_score": 0.36084739252752485,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24916201117318434,
                "acc_stderr": 0.014465893829859933,
                "brier_score": 0.8291813080992225,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.8227848101265823,
                "acc_stderr": 0.024856364184503234,
                "brier_score": 0.26232318467685783,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_moral_disputes": {
                "acc": 0.653179190751445,
                "acc_stderr": 0.02562472399403046,
                "brier_score": 0.4428304040800636,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_professional_law": {
                "acc": 0.470013037809648,
                "acc_stderr": 0.01274724896707906,
                "brier_score": 0.6545306376603126,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.7484662576687117,
                "acc_stderr": 0.03408997886857529,
                "brier_score": 0.3275554200833385,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.7745098039215687,
                "acc_stderr": 0.029331162294251735,
                "brier_score": 0.2932653335661421,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_philosophy": {
                "acc": 0.7363344051446945,
                "acc_stderr": 0.02502553850053234,
                "brier_score": 0.37893514894575386,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_jurisprudence": {
                "acc": 0.7222222222222222,
                "acc_stderr": 0.043300437496507416,
                "brier_score": 0.37148916048633085,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_international_law": {
                "acc": 0.7933884297520661,
                "acc_stderr": 0.036959801280988254,
                "brier_score": 0.30941704283786436,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.7757575757575758,
                "acc_stderr": 0.03256866661681102,
                "brier_score": 0.3419807011294865,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.844559585492228,
                "acc_stderr": 0.026148483469153324,
                "brier_score": 0.20123268843700662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.6302521008403361,
                "acc_stderr": 0.031357095996135904,
                "brier_score": 0.5082437444997995,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_geography": {
                "acc": 0.8080808080808081,
                "acc_stderr": 0.028057791672989017,
                "brier_score": 0.26562395906533987,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.8128440366972477,
                "acc_stderr": 0.016722684526200144,
                "brier_score": 0.26215102561607717,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_public_relations": {
                "acc": 0.6363636363636364,
                "acc_stderr": 0.04607582090719976,
                "brier_score": 0.4314491872734749,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.87,
                "acc_stderr": 0.03379976689896309,
                "brier_score": 0.1993096348099571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_sociology": {
                "acc": 0.7910447761194029,
                "acc_stderr": 0.028748298931728658,
                "brier_score": 0.26865300694882494,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.6256410256410256,
                "acc_stderr": 0.024537591572830517,
                "brier_score": 0.4942070941658129,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_security_studies": {
                "acc": 0.7387755102040816,
                "acc_stderr": 0.028123429335142797,
                "brier_score": 0.3675529070865265,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_professional_psychology": {
                "acc": 0.6601307189542484,
                "acc_stderr": 0.019162418588623553,
                "brier_score": 0.4584601558942431,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_human_sexuality": {
                "acc": 0.816793893129771,
                "acc_stderr": 0.033927709264947335,
                "brier_score": 0.33801186706674946,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_econometrics": {
                "acc": 0.47368421052631576,
                "acc_stderr": 0.046970851366478626,
                "brier_score": 0.6289216779161965,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_miscellaneous": {
                "acc": 0.8148148148148148,
                "acc_stderr": 0.013890862162876164,
                "brier_score": 0.254727901877134,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_marketing": {
                "acc": 0.8675213675213675,
                "acc_stderr": 0.022209309073165626,
                "brier_score": 0.1880570882030185,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_management": {
                "acc": 0.8446601941747572,
                "acc_stderr": 0.03586594738573975,
                "brier_score": 0.21054564782275975,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_nutrition": {
                "acc": 0.6993464052287581,
                "acc_stderr": 0.026256053835718964,
                "brier_score": 0.3912421136861796,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_medical_genetics": {
                "acc": 0.72,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 0.3520073746839315,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_human_aging": {
                "acc": 0.6905829596412556,
                "acc_stderr": 0.031024411740572203,
                "brier_score": 0.41113365407849767,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_professional_medicine": {
                "acc": 0.6102941176470589,
                "acc_stderr": 0.02962466358115969,
                "brier_score": 0.49532887960076133,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_college_medicine": {
                "acc": 0.6069364161849711,
                "acc_stderr": 0.03724249595817731,
                "brier_score": 0.4769229917453633,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_business_ethics": {
                "acc": 0.65,
                "acc_stderr": 0.0479372485441102,
                "brier_score": 0.45469680318876826,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.7132075471698113,
                "acc_stderr": 0.027834912527544074,
                "brier_score": 0.398876507521786,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_global_facts": {
                "acc": 0.36,
                "acc_stderr": 0.048241815132442176,
                "brier_score": 0.693773285966367,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_virology": {
                "acc": 0.5180722891566265,
                "acc_stderr": 0.038899512528272166,
                "brier_score": 0.6993771685259159,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_professional_accounting": {
                "acc": 0.5212765957446809,
                "acc_stderr": 0.029800481645628693,
                "brier_score": 0.5919019586393015,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_college_physics": {
                "acc": 0.3431372549019608,
                "acc_stderr": 0.04724007352383889,
                "brier_score": 0.666476923757466,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_physics": {
                "acc": 0.36423841059602646,
                "acc_stderr": 0.03929111781242742,
                "brier_score": 0.6936531576790718,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_biology": {
                "acc": 0.7612903225806451,
                "acc_stderr": 0.024251071262208837,
                "brier_score": 0.3246457688347233,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_college_biology": {
                "acc": 0.7708333333333334,
                "acc_stderr": 0.035146974678623884,
                "brier_score": 0.3102445873248153,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_anatomy": {
                "acc": 0.5703703703703704,
                "acc_stderr": 0.04276349494376599,
                "brier_score": 0.5425841934637702,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_college_chemistry": {
                "acc": 0.45,
                "acc_stderr": 0.04999999999999999,
                "brier_score": 0.6260057241013216,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_computer_security": {
                "acc": 0.69,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.41333249631795255,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_college_computer_science": {
                "acc": 0.55,
                "acc_stderr": 0.049999999999999996,
                "brier_score": 0.5801512561734236,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_astronomy": {
                "acc": 0.7302631578947368,
                "acc_stderr": 0.03611780560284898,
                "brier_score": 0.37540527560539916,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_college_mathematics": {
                "acc": 0.39,
                "acc_stderr": 0.04902071300001974,
                "brier_score": 0.7107035113276393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.574468085106383,
                "acc_stderr": 0.03232146916224469,
                "brier_score": 0.5277388052784279,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.7316091176070936,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.68,
                "acc_stderr": 0.04688261722621504,
                "brier_score": 0.4363723609398192,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_machine_learning": {
                "acc": 0.49107142857142855,
                "acc_stderr": 0.04745033255489123,
                "brier_score": 0.6216939745012614,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.5221674876847291,
                "acc_stderr": 0.035145285621750094,
                "brier_score": 0.560523454378963,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.5462962962962963,
                "acc_stderr": 0.03395322726375797,
                "brier_score": 0.5618044455674843,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.4312169312169312,
                "acc_stderr": 0.025506481698138215,
                "brier_score": 0.6506498011362429,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.5448275862068965,
                "acc_stderr": 0.04149886942192117,
                "brier_score": 0.5214244847001162,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.35185185185185186,
                "acc_stderr": 0.029116617606083018,
                "brier_score": 0.6953923313703757,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-19-36.388618"
            },
            "arc_challenge": {
                "acc": 0.5708191126279863,
                "acc_stderr": 0.014464085894870657,
                "acc_norm": 0.6049488054607508,
                "acc_norm_stderr": 0.014285898292938169,
                "timestamp": "2024-11-22T16-58-09.137194"
            },
            "hellaswag": {
                "acc": 0.6207926707827126,
                "acc_stderr": 0.00484198197351532,
                "acc_norm": 0.8228440549691296,
                "acc_norm_stderr": 0.003810203308901054,
                "timestamp": "2024-11-22T16-58-09.137194"
            },
            "truthfulqa_mc2": {
                "acc": 0.45109982374873114,
                "acc_stderr": 0.014690361721297288,
                "timestamp": "2024-11-22T16-58-09.137194"
            },
            "truthfulqa_gen": {
                "bleu_max": 27.01069206803005,
                "bleu_max_stderr": 0.8195476991507818,
                "bleu_acc": 0.4394124847001224,
                "bleu_acc_stderr": 0.01737452048251371,
                "bleu_diff": 0.7951375918699185,
                "bleu_diff_stderr": 0.9114193943215877,
                "rouge1_max": 52.929721104206294,
                "rouge1_max_stderr": 0.8888037925222474,
                "rouge1_acc": 0.44063647490820074,
                "rouge1_acc_stderr": 0.017379697555437446,
                "rouge1_diff": 1.0282214491941382,
                "rouge1_diff_stderr": 1.1490330858956697,
                "rouge2_max": 37.759767751388836,
                "rouge2_max_stderr": 1.0663490449012212,
                "rouge2_acc": 0.3671970624235006,
                "rouge2_acc_stderr": 0.01687480500145318,
                "rouge2_diff": -0.45206688687486213,
                "rouge2_diff_stderr": 1.3041898041246565,
                "rougeL_max": 49.90048034353921,
                "rougeL_max_stderr": 0.9075990783414511,
                "rougeL_acc": 0.42105263157894735,
                "rougeL_acc_stderr": 0.017283936248136483,
                "rougeL_diff": 0.7823563764856337,
                "rougeL_diff_stderr": 1.1650307856683864,
                "timestamp": "2024-11-22T16-58-09.137194"
            },
            "truthfulqa_mc1": {
                "acc": 0.3108935128518972,
                "acc_stderr": 0.016203316673559693,
                "timestamp": "2024-11-22T16-58-09.137194"
            },
            "winogrande": {
                "acc": 0.760852407261247,
                "acc_stderr": 0.011988541844843909,
                "timestamp": "2024-11-22T16-58-09.137194"
            },
            "gsm8k": {
                "exact_match": 0.5382865807429871,
                "exact_match_stderr": 0.013732048227016685,
                "timestamp": "2024-11-22T16-58-09.137194"
            }
        }
    }
}