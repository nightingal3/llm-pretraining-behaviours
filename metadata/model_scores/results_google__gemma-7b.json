{
    "model_name": "google__gemma-7b",
    "last_updated": "2024-12-04 11:25:15.123019",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.8421052631578947,
                    "acc_stderr": 0.027966785859160865,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.04472135954999579,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.7314814814814815,
                    "acc_stderr": 0.024659685185967273,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.31508379888268156,
                    "acc_stderr": 0.015536850852473635,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.8227848101265823,
                    "acc_stderr": 0.024856364184503228,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.6676300578034682,
                    "acc_stderr": 0.02536116874968822,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.4661016949152542,
                    "acc_stderr": 0.012740853872949832,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.7484662576687117,
                    "acc_stderr": 0.03408997886857529,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.7843137254901961,
                    "acc_stderr": 0.028867431449849313,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.7009646302250804,
                    "acc_stderr": 0.026003301117885128,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.04186091791394607,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.8512396694214877,
                    "acc_stderr": 0.03248470083807195,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.7575757575757576,
                    "acc_stderr": 0.03346409881055953,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.8860103626943006,
                    "acc_stderr": 0.022935144053919436,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.6470588235294118,
                    "acc_stderr": 0.031041941304059278,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.803030303030303,
                    "acc_stderr": 0.028335609732463362,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.8128440366972477,
                    "acc_stderr": 0.01672268452620016,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.6545454545454545,
                    "acc_stderr": 0.04554619617541054,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.87,
                    "acc_stderr": 0.03379976689896308,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.835820895522388,
                    "acc_stderr": 0.026193923544454125,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.6307692307692307,
                    "acc_stderr": 0.02446861524147891,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.7183673469387755,
                    "acc_stderr": 0.028795185574291296,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.6895424836601307,
                    "acc_stderr": 0.018718067052623213,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.7251908396946565,
                    "acc_stderr": 0.03915345408847836,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.45614035087719296,
                    "acc_stderr": 0.04685473041907789,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.8416347381864623,
                    "acc_stderr": 0.013055346753516743,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.8931623931623932,
                    "acc_stderr": 0.020237149008990943,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.8252427184466019,
                    "acc_stderr": 0.03760178006026621,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.7418300653594772,
                    "acc_stderr": 0.025058503316958147,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.04351941398892446,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.7085201793721974,
                    "acc_stderr": 0.03050028317654584,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.5772058823529411,
                    "acc_stderr": 0.03000856284500347,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.6473988439306358,
                    "acc_stderr": 0.03643037168958548,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.64,
                    "acc_stderr": 0.048241815132442176,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.660377358490566,
                    "acc_stderr": 0.029146904747798328,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.0479372485441102,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.5240963855421686,
                    "acc_stderr": 0.03887971849597264,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.46099290780141844,
                    "acc_stderr": 0.02973659252642444,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.38235294117647056,
                    "acc_stderr": 0.04835503696107223,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.3973509933774834,
                    "acc_stderr": 0.03995524007681681,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.02275520495954294,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.7291666666666666,
                    "acc_stderr": 0.03716177437566017,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.5259259259259259,
                    "acc_stderr": 0.04313531696750575,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.04351941398892446,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.050241839379569095,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.6907894736842105,
                    "acc_stderr": 0.03761070869867479,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.04878317312145632,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.5829787234042553,
                    "acc_stderr": 0.03223276266711712,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909282,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.65,
                    "acc_stderr": 0.047937248544110196,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.5535714285714286,
                    "acc_stderr": 0.047184714852195865,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.5024630541871922,
                    "acc_stderr": 0.03517945038691063,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.48148148148148145,
                    "acc_stderr": 0.034076320938540516,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.4603174603174603,
                    "acc_stderr": 0.025670080636909186,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.6344827586206897,
                    "acc_stderr": 0.04013124195424386,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.362962962962963,
                    "acc_stderr": 0.02931820364520686,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.5708191126279863,
                    "acc_stderr": 0.014464085894870657,
                    "acc_norm": 0.6049488054607508,
                    "acc_norm_stderr": 0.014285898292938169,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6207926707827126,
                    "acc_stderr": 0.00484198197351532,
                    "acc_norm": 0.8228440549691296,
                    "acc_norm_stderr": 0.003810203308901054,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.45109982374873114,
                    "acc_stderr": 0.014690361721297288,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 27.01069206803005,
                    "bleu_max_stderr": 0.8195476991507818,
                    "bleu_acc": 0.4394124847001224,
                    "bleu_acc_stderr": 0.01737452048251371,
                    "bleu_diff": 0.7951375918699185,
                    "bleu_diff_stderr": 0.9114193943215877,
                    "rouge1_max": 52.929721104206294,
                    "rouge1_max_stderr": 0.8888037925222474,
                    "rouge1_acc": 0.44063647490820074,
                    "rouge1_acc_stderr": 0.017379697555437446,
                    "rouge1_diff": 1.0282214491941382,
                    "rouge1_diff_stderr": 1.1490330858956697,
                    "rouge2_max": 37.759767751388836,
                    "rouge2_max_stderr": 1.0663490449012212,
                    "rouge2_acc": 0.3671970624235006,
                    "rouge2_acc_stderr": 0.01687480500145318,
                    "rouge2_diff": -0.45206688687486213,
                    "rouge2_diff_stderr": 1.3041898041246565,
                    "rougeL_max": 49.90048034353921,
                    "rougeL_max_stderr": 0.9075990783414511,
                    "rougeL_acc": 0.42105263157894735,
                    "rougeL_acc_stderr": 0.017283936248136483,
                    "rougeL_diff": 0.7823563764856337,
                    "rougeL_diff_stderr": 1.1650307856683864,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.3108935128518972,
                    "acc_stderr": 0.016203316673559693,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.760852407261247,
                    "acc_stderr": 0.011988541844843909,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.5382865807429871,
                    "acc_stderr": 0.013732048227016685,
                    "timestamp": "2024-11-22T16-58-09.137194"
                }
            }
        }
    }
}