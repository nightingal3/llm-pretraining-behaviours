{
    "model_name": "EleutherAI/gpt-neox-20b",
    "last_updated": "2023-12-03",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.42662116040955633,
                    "acc_stderr": 0.014453185592920293,
                    "acc_norm": 0.45733788395904434,
                    "acc_norm_stderr": 0.014558106543924063,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5416251742680741,
                    "acc_stderr": 0.004972460206842309,
                    "acc_norm": 0.73451503684525,
                    "acc_norm_stderr": 0.004406886100685863,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.23703703703703705,
                    "acc_stderr": 0.03673731683969506,
                    "acc_norm": 0.23703703703703705,
                    "acc_norm_stderr": 0.03673731683969506,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.23026315789473684,
                    "acc_stderr": 0.03426059424403165,
                    "acc_norm": 0.23026315789473684,
                    "acc_norm_stderr": 0.03426059424403165,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252604,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.22264150943396227,
                    "acc_stderr": 0.025604233470899095,
                    "acc_norm": 0.22264150943396227,
                    "acc_norm_stderr": 0.025604233470899095,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.03621034121889507,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.03621034121889507,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322695,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.041633319989322695,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.21965317919075145,
                    "acc_stderr": 0.031568093627031744,
                    "acc_norm": 0.21965317919075145,
                    "acc_norm_stderr": 0.031568093627031744,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.041583075330832865,
                    "acc_norm": 0.22549019607843138,
                    "acc_norm_stderr": 0.041583075330832865,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3617021276595745,
                    "acc_stderr": 0.0314108219759624,
                    "acc_norm": 0.3617021276595745,
                    "acc_norm_stderr": 0.0314108219759624,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.0414243971948936,
                    "acc_norm": 0.2631578947368421,
                    "acc_norm_stderr": 0.0414243971948936,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.20689655172413793,
                    "acc_stderr": 0.03375672449560554,
                    "acc_norm": 0.20689655172413793,
                    "acc_norm_stderr": 0.03375672449560554,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.022569897074918417,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.022569897074918417,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.23015873015873015,
                    "acc_stderr": 0.03764950879790607,
                    "acc_norm": 0.23015873015873015,
                    "acc_norm_stderr": 0.03764950879790607,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.2064516129032258,
                    "acc_stderr": 0.023025899617188712,
                    "acc_norm": 0.2064516129032258,
                    "acc_norm_stderr": 0.023025899617188712,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.1477832512315271,
                    "acc_stderr": 0.024969621333521277,
                    "acc_norm": 0.1477832512315271,
                    "acc_norm_stderr": 0.024969621333521277,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.04461960433384741,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.04461960433384741,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.21212121212121213,
                    "acc_stderr": 0.031922715695483,
                    "acc_norm": 0.21212121212121213,
                    "acc_norm_stderr": 0.031922715695483,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.18686868686868688,
                    "acc_stderr": 0.02777253333421898,
                    "acc_norm": 0.18686868686868688,
                    "acc_norm_stderr": 0.02777253333421898,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.22797927461139897,
                    "acc_stderr": 0.030276909945178256,
                    "acc_norm": 0.22797927461139897,
                    "acc_norm_stderr": 0.030276909945178256,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.020280805062535722,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.020280805062535722,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.22962962962962963,
                    "acc_stderr": 0.025644108639267634,
                    "acc_norm": 0.22962962962962963,
                    "acc_norm_stderr": 0.025644108639267634,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2184873949579832,
                    "acc_stderr": 0.026841514322958924,
                    "acc_norm": 0.2184873949579832,
                    "acc_norm_stderr": 0.026841514322958924,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.24503311258278146,
                    "acc_stderr": 0.035118075718047245,
                    "acc_norm": 0.24503311258278146,
                    "acc_norm_stderr": 0.035118075718047245,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.21467889908256882,
                    "acc_stderr": 0.01760430414925649,
                    "acc_norm": 0.21467889908256882,
                    "acc_norm_stderr": 0.01760430414925649,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.17592592592592593,
                    "acc_stderr": 0.025967420958258533,
                    "acc_norm": 0.17592592592592593,
                    "acc_norm_stderr": 0.025967420958258533,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.29411764705882354,
                    "acc_stderr": 0.03198001660115071,
                    "acc_norm": 0.29411764705882354,
                    "acc_norm_stderr": 0.03198001660115071,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.26582278481012656,
                    "acc_stderr": 0.02875679962965834,
                    "acc_norm": 0.26582278481012656,
                    "acc_norm_stderr": 0.02875679962965834,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.34977578475336324,
                    "acc_stderr": 0.03200736719484503,
                    "acc_norm": 0.34977578475336324,
                    "acc_norm_stderr": 0.03200736719484503,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.25190839694656486,
                    "acc_stderr": 0.03807387116306086,
                    "acc_norm": 0.25190839694656486,
                    "acc_norm_stderr": 0.03807387116306086,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.2727272727272727,
                    "acc_stderr": 0.04065578140908705,
                    "acc_norm": 0.2727272727272727,
                    "acc_norm_stderr": 0.04065578140908705,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.04330043749650744,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.04330043749650744,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2392638036809816,
                    "acc_stderr": 0.033519538795212696,
                    "acc_norm": 0.2392638036809816,
                    "acc_norm_stderr": 0.033519538795212696,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04109974682633932,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04109974682633932,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.24271844660194175,
                    "acc_stderr": 0.04245022486384495,
                    "acc_norm": 0.24271844660194175,
                    "acc_norm_stderr": 0.04245022486384495,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.2863247863247863,
                    "acc_stderr": 0.02961432369045665,
                    "acc_norm": 0.2863247863247863,
                    "acc_norm_stderr": 0.02961432369045665,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.2656449553001277,
                    "acc_stderr": 0.01579430248788871,
                    "acc_norm": 0.2656449553001277,
                    "acc_norm_stderr": 0.01579430248788871,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2630057803468208,
                    "acc_stderr": 0.023703099525258172,
                    "acc_norm": 0.2630057803468208,
                    "acc_norm_stderr": 0.023703099525258172,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2446927374301676,
                    "acc_stderr": 0.014378169884098443,
                    "acc_norm": 0.2446927374301676,
                    "acc_norm_stderr": 0.014378169884098443,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.27450980392156865,
                    "acc_stderr": 0.025553169991826507,
                    "acc_norm": 0.27450980392156865,
                    "acc_norm_stderr": 0.025553169991826507,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.2765273311897106,
                    "acc_stderr": 0.025403832978179604,
                    "acc_norm": 0.2765273311897106,
                    "acc_norm_stderr": 0.025403832978179604,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.02438366553103545,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.02438366553103545,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.25886524822695034,
                    "acc_stderr": 0.026129572527180848,
                    "acc_norm": 0.25886524822695034,
                    "acc_norm_stderr": 0.026129572527180848,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.23859191655801826,
                    "acc_stderr": 0.010885929742002207,
                    "acc_norm": 0.23859191655801826,
                    "acc_norm_stderr": 0.010885929742002207,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.19117647058823528,
                    "acc_stderr": 0.02388688192244034,
                    "acc_norm": 0.19117647058823528,
                    "acc_norm_stderr": 0.02388688192244034,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.25980392156862747,
                    "acc_stderr": 0.017740899509177795,
                    "acc_norm": 0.25980392156862747,
                    "acc_norm_stderr": 0.017740899509177795,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.04172343038705383,
                    "acc_norm": 0.2545454545454545,
                    "acc_norm_stderr": 0.04172343038705383,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.20408163265306123,
                    "acc_stderr": 0.025801283475090496,
                    "acc_norm": 0.20408163265306123,
                    "acc_norm_stderr": 0.025801283475090496,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.2885572139303483,
                    "acc_stderr": 0.032038410402133226,
                    "acc_norm": 0.2885572139303483,
                    "acc_norm_stderr": 0.032038410402133226,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322695,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.041633319989322695,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3072289156626506,
                    "acc_stderr": 0.03591566797824662,
                    "acc_norm": 0.3072289156626506,
                    "acc_norm_stderr": 0.03591566797824662,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.3333333333333333,
                    "acc_stderr": 0.036155076303109344,
                    "acc_norm": 0.3333333333333333,
                    "acc_norm_stderr": 0.036155076303109344,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.193390452876377,
                    "mc1_stderr": 0.013826240752599066,
                    "mc2": 0.3161314596733849,
                    "mc2_stderr": 0.013022756719177409,
                    "timestamp": "2023-07-20T10-44-54.391639"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.001363255033557047,
                    "acc_stderr": 0.00037786091964606844,
                    "f1": 0.050428901006711505,
                    "f1_stderr": 0.0012240402281522937,
                    "timestamp": "2023-09-08T18-29-20.429481"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.05458680818802123,
                    "acc_stderr": 0.00625744403791253,
                    "timestamp": "2023-12-03T17-14-42.607420"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.6890292028413575,
                    "acc_stderr": 0.013009534736286067,
                    "timestamp": "2023-09-08T18-29-20.429481"
                }
            }
        }
    }
}