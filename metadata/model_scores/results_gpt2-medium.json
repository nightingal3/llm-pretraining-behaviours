{
    "model_name": "gpt2-medium",
    "last_updated": "2023-07-24",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.22098976109215018,
                    "acc_stderr": 0.012124929206818258,
                    "acc_norm": 0.27047781569965873,
                    "acc_norm_stderr": 0.012980954547659554,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.33061143198566023,
                    "acc_stderr": 0.004694718918225759,
                    "acc_norm": 0.4017128062139016,
                    "acc_norm_stderr": 0.004892425356375716,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768081,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768081,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.24444444444444444,
                    "acc_stderr": 0.03712537833614867,
                    "acc_norm": 0.24444444444444444,
                    "acc_norm_stderr": 0.03712537833614867,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.3026315789473684,
                    "acc_stderr": 0.03738520676119668,
                    "acc_norm": 0.3026315789473684,
                    "acc_norm_stderr": 0.03738520676119668,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.15,
                    "acc_stderr": 0.035887028128263686,
                    "acc_norm": 0.15,
                    "acc_norm_stderr": 0.035887028128263686,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.3018867924528302,
                    "acc_stderr": 0.02825420034443867,
                    "acc_norm": 0.3018867924528302,
                    "acc_norm_stderr": 0.02825420034443867,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2708333333333333,
                    "acc_stderr": 0.03716177437566016,
                    "acc_norm": 0.2708333333333333,
                    "acc_norm_stderr": 0.03716177437566016,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.046482319871173156,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.046482319871173156,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.04824181513244218,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.04824181513244218,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2658959537572254,
                    "acc_stderr": 0.033687629322594316,
                    "acc_norm": 0.2658959537572254,
                    "acc_norm_stderr": 0.033687629322594316,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.1568627450980392,
                    "acc_stderr": 0.036186648199362466,
                    "acc_norm": 0.1568627450980392,
                    "acc_norm_stderr": 0.036186648199362466,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.042295258468165044,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.042295258468165044,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2936170212765957,
                    "acc_stderr": 0.029771642712491227,
                    "acc_norm": 0.2936170212765957,
                    "acc_norm_stderr": 0.029771642712491227,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.30701754385964913,
                    "acc_stderr": 0.0433913832257986,
                    "acc_norm": 0.30701754385964913,
                    "acc_norm_stderr": 0.0433913832257986,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2482758620689655,
                    "acc_stderr": 0.03600105692727771,
                    "acc_norm": 0.2482758620689655,
                    "acc_norm_stderr": 0.03600105692727771,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.022569897074918424,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.022569897074918424,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.15873015873015872,
                    "acc_stderr": 0.03268454013011742,
                    "acc_norm": 0.15873015873015872,
                    "acc_norm_stderr": 0.03268454013011742,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.24193548387096775,
                    "acc_stderr": 0.0243625996930311,
                    "acc_norm": 0.24193548387096775,
                    "acc_norm_stderr": 0.0243625996930311,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.3054187192118227,
                    "acc_stderr": 0.03240661565868408,
                    "acc_norm": 0.3054187192118227,
                    "acc_norm_stderr": 0.03240661565868408,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542127,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.03453131801885415,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.03453131801885415,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.35858585858585856,
                    "acc_stderr": 0.03416903640391521,
                    "acc_norm": 0.35858585858585856,
                    "acc_norm_stderr": 0.03416903640391521,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.3160621761658031,
                    "acc_stderr": 0.033553973696861736,
                    "acc_norm": 0.3160621761658031,
                    "acc_norm_stderr": 0.033553973696861736,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.3076923076923077,
                    "acc_stderr": 0.023400928918310502,
                    "acc_norm": 0.3076923076923077,
                    "acc_norm_stderr": 0.023400928918310502,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26296296296296295,
                    "acc_stderr": 0.026842057873833706,
                    "acc_norm": 0.26296296296296295,
                    "acc_norm_stderr": 0.026842057873833706,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.3025210084033613,
                    "acc_stderr": 0.02983796238829192,
                    "acc_norm": 0.3025210084033613,
                    "acc_norm_stderr": 0.02983796238829192,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.3443708609271523,
                    "acc_stderr": 0.038796870240733264,
                    "acc_norm": 0.3443708609271523,
                    "acc_norm_stderr": 0.038796870240733264,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.3357798165137615,
                    "acc_stderr": 0.020248081396752937,
                    "acc_norm": 0.3357798165137615,
                    "acc_norm_stderr": 0.020248081396752937,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4074074074074074,
                    "acc_stderr": 0.033509916046960436,
                    "acc_norm": 0.4074074074074074,
                    "acc_norm_stderr": 0.033509916046960436,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.02933116229425173,
                    "acc_norm": 0.22549019607843138,
                    "acc_norm_stderr": 0.02933116229425173,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.2109704641350211,
                    "acc_stderr": 0.02655837250266192,
                    "acc_norm": 0.2109704641350211,
                    "acc_norm_stderr": 0.02655837250266192,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.2062780269058296,
                    "acc_stderr": 0.02715715047956382,
                    "acc_norm": 0.2062780269058296,
                    "acc_norm_stderr": 0.02715715047956382,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2595419847328244,
                    "acc_stderr": 0.03844876139785271,
                    "acc_norm": 0.2595419847328244,
                    "acc_norm_stderr": 0.03844876139785271,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.1652892561983471,
                    "acc_stderr": 0.03390780612972777,
                    "acc_norm": 0.1652892561983471,
                    "acc_norm_stderr": 0.03390780612972777,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2037037037037037,
                    "acc_stderr": 0.03893542518824847,
                    "acc_norm": 0.2037037037037037,
                    "acc_norm_stderr": 0.03893542518824847,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.26380368098159507,
                    "acc_stderr": 0.03462419931615624,
                    "acc_norm": 0.26380368098159507,
                    "acc_norm_stderr": 0.03462419931615624,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.19642857142857142,
                    "acc_stderr": 0.03770970049347019,
                    "acc_norm": 0.19642857142857142,
                    "acc_norm_stderr": 0.03770970049347019,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.3592233009708738,
                    "acc_stderr": 0.04750458399041692,
                    "acc_norm": 0.3592233009708738,
                    "acc_norm_stderr": 0.04750458399041692,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.20512820512820512,
                    "acc_stderr": 0.026453508054040353,
                    "acc_norm": 0.20512820512820512,
                    "acc_norm_stderr": 0.026453508054040353,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.045126085985421296,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.045126085985421296,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.24010217113665389,
                    "acc_stderr": 0.015274685213734191,
                    "acc_norm": 0.24010217113665389,
                    "acc_norm_stderr": 0.015274685213734191,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.24277456647398843,
                    "acc_stderr": 0.0230836585869842,
                    "acc_norm": 0.24277456647398843,
                    "acc_norm_stderr": 0.0230836585869842,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2424581005586592,
                    "acc_stderr": 0.014333522059217889,
                    "acc_norm": 0.2424581005586592,
                    "acc_norm_stderr": 0.014333522059217889,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2679738562091503,
                    "acc_stderr": 0.025360603796242557,
                    "acc_norm": 0.2679738562091503,
                    "acc_norm_stderr": 0.025360603796242557,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.24115755627009647,
                    "acc_stderr": 0.02429659403476343,
                    "acc_norm": 0.24115755627009647,
                    "acc_norm_stderr": 0.02429659403476343,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2191358024691358,
                    "acc_stderr": 0.023016705640262196,
                    "acc_norm": 0.2191358024691358,
                    "acc_norm_stderr": 0.023016705640262196,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.25886524822695034,
                    "acc_stderr": 0.02612957252718085,
                    "acc_norm": 0.25886524822695034,
                    "acc_norm_stderr": 0.02612957252718085,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.24511082138200782,
                    "acc_stderr": 0.010986307870045524,
                    "acc_norm": 0.24511082138200782,
                    "acc_norm_stderr": 0.010986307870045524,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4522058823529412,
                    "acc_stderr": 0.030233758551596455,
                    "acc_norm": 0.4522058823529412,
                    "acc_norm_stderr": 0.030233758551596455,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.01716058723504635,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.01716058723504635,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2636363636363636,
                    "acc_stderr": 0.04220224692971987,
                    "acc_norm": 0.2636363636363636,
                    "acc_norm_stderr": 0.04220224692971987,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.3346938775510204,
                    "acc_stderr": 0.030209235226242307,
                    "acc_norm": 0.3346938775510204,
                    "acc_norm_stderr": 0.030209235226242307,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.22885572139303484,
                    "acc_stderr": 0.029705284056772432,
                    "acc_norm": 0.22885572139303484,
                    "acc_norm_stderr": 0.029705284056772432,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.045126085985421276,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.045126085985421276,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.20481927710843373,
                    "acc_stderr": 0.03141784291663926,
                    "acc_norm": 0.20481927710843373,
                    "acc_norm_stderr": 0.03141784291663926,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.25146198830409355,
                    "acc_stderr": 0.033275044238468436,
                    "acc_norm": 0.25146198830409355,
                    "acc_norm_stderr": 0.033275044238468436,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2252141982864137,
                    "mc1_stderr": 0.014623240768023505,
                    "mc2": 0.4075602335796246,
                    "mc2_stderr": 0.014596763158762415,
                    "timestamp": "2023-07-24T09-55-17.325605"
                }
            }
        }
    }
}