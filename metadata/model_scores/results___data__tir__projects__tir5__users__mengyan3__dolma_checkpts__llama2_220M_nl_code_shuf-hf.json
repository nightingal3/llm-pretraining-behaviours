{
    "model_name": "__data__tir__projects__tir5__users__mengyan3__dolma_checkpts__llama2_220M_nl_code_shuf-hf",
    "last_updated": "2024-12-04 11:24:38.377574",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.2982456140350877,
                "acc_stderr": 0.03508771929824561,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_formal_logic": {
                "acc": 0.15079365079365079,
                "acc_stderr": 0.03200686497287394,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_prehistory": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.02378858355165854,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24581005586592178,
                "acc_stderr": 0.014400296429225613,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.24050632911392406,
                "acc_stderr": 0.027820781981149678,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_moral_disputes": {
                "acc": 0.18208092485549132,
                "acc_stderr": 0.020776761102512992,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_professional_law": {
                "acc": 0.2405475880052151,
                "acc_stderr": 0.010916406735478947,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2331288343558282,
                "acc_stderr": 0.033220157957767414,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.030964517926923393,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_philosophy": {
                "acc": 0.19935691318327975,
                "acc_stderr": 0.022691033780549652,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.0401910747255735,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_international_law": {
                "acc": 0.2231404958677686,
                "acc_stderr": 0.03800754475228733,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.296969696969697,
                "acc_stderr": 0.03567969772268049,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.22279792746113988,
                "acc_stderr": 0.030031147977641545,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.24789915966386555,
                "acc_stderr": 0.028047967224176896,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_geography": {
                "acc": 0.20202020202020202,
                "acc_stderr": 0.028606204289229893,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23486238532110093,
                "acc_stderr": 0.01817511051034358,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_sociology": {
                "acc": 0.24378109452736318,
                "acc_stderr": 0.03036049015401466,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2794871794871795,
                "acc_stderr": 0.022752388839776823,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_security_studies": {
                "acc": 0.27346938775510204,
                "acc_stderr": 0.02853556033712844,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2581699346405229,
                "acc_stderr": 0.01770453165325007,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2748091603053435,
                "acc_stderr": 0.039153454088478354,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_econometrics": {
                "acc": 0.2719298245614035,
                "acc_stderr": 0.04185774424022056,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2681992337164751,
                "acc_stderr": 0.015842430835269438,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_marketing": {
                "acc": 0.21367521367521367,
                "acc_stderr": 0.02685345037700916,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_management": {
                "acc": 0.1941747572815534,
                "acc_stderr": 0.03916667762822583,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_nutrition": {
                "acc": 0.22875816993464052,
                "acc_stderr": 0.024051029739912258,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_human_aging": {
                "acc": 0.22869955156950672,
                "acc_stderr": 0.028188240046929193,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4485294117647059,
                "acc_stderr": 0.030211479609121596,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_college_medicine": {
                "acc": 0.2658959537572254,
                "acc_stderr": 0.03368762932259431,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_business_ethics": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206824,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.22264150943396227,
                "acc_stderr": 0.025604233470899098,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_global_facts": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036625,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_virology": {
                "acc": 0.23493975903614459,
                "acc_stderr": 0.03300533186128922,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_professional_accounting": {
                "acc": 0.26595744680851063,
                "acc_stderr": 0.026358065698880592,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_college_physics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.040233822736177455,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2582781456953642,
                "acc_stderr": 0.035737053147634576,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2806451612903226,
                "acc_stderr": 0.02556060472102289,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_college_biology": {
                "acc": 0.24305555555555555,
                "acc_stderr": 0.03586879280080341,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_anatomy": {
                "acc": 0.28888888888888886,
                "acc_stderr": 0.03915450630414251,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_college_chemistry": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_computer_security": {
                "acc": 0.27,
                "acc_stderr": 0.04461960433384741,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_college_computer_science": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036622,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123387,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_college_mathematics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2553191489361702,
                "acc_stderr": 0.028504856470514196,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421296,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_machine_learning": {
                "acc": 0.22321428571428573,
                "acc_stderr": 0.03952301967702511,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2660098522167488,
                "acc_stderr": 0.03108982600293753,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.37037037037037035,
                "acc_stderr": 0.03293377139415191,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.23809523809523808,
                "acc_stderr": 0.021935878081184763,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2896551724137931,
                "acc_stderr": 0.03780019230438014,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.29259259259259257,
                "acc_stderr": 0.02773896963217609,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "arc_challenge": {
                "acc": 0.20136518771331058,
                "acc_stderr": 0.011718927477444262,
                "acc_norm": 0.23890784982935154,
                "acc_norm_stderr": 0.01246107137631661,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "hellaswag": {
                "acc": 0.30083648675562635,
                "acc_stderr": 0.004576844407429688,
                "acc_norm": 0.3286197968532165,
                "acc_norm_stderr": 0.00468751470834533,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "truthfulqa_mc2": {
                "acc": 0.4449089670353243,
                "acc_stderr": 0.015142694363046506,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "truthfulqa_gen": {
                "bleu_max": 15.293326250001726,
                "bleu_max_stderr": 0.513234397164096,
                "bleu_acc": 0.3880048959608323,
                "bleu_acc_stderr": 0.017058761501347976,
                "bleu_diff": -1.124785455285198,
                "bleu_diff_stderr": 0.48395860317043926,
                "rouge1_max": 35.88339320580419,
                "rouge1_max_stderr": 0.7759164759269833,
                "rouge1_acc": 0.3329253365973072,
                "rouge1_acc_stderr": 0.016497402382012052,
                "rouge1_diff": -2.978599244361346,
                "rouge1_diff_stderr": 0.7637855921800153,
                "rouge2_max": 18.22410194896594,
                "rouge2_max_stderr": 0.8084438166951625,
                "rouge2_acc": 0.18604651162790697,
                "rouge2_acc_stderr": 0.013622771770442053,
                "rouge2_diff": -3.450080480635194,
                "rouge2_diff_stderr": 0.7201480121874273,
                "rougeL_max": 33.19168845229128,
                "rougeL_max_stderr": 0.7536889121182447,
                "rougeL_acc": 0.35495716034271724,
                "rougeL_acc_stderr": 0.016750862381375905,
                "rougeL_diff": -2.5759988820935593,
                "rougeL_diff_stderr": 0.7520527963416354,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "truthfulqa_mc1": {
                "acc": 0.2484700122399021,
                "acc_stderr": 0.015127427096520683,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "winogrande": {
                "acc": 0.5288082083662194,
                "acc_stderr": 0.014029141615909612,
                "timestamp": "2024-11-22T00-47-29.035355"
            },
            "gsm8k": {
                "exact_match": 0.013646702047005308,
                "exact_match_stderr": 0.0031957470754808395,
                "timestamp": "2024-11-22T00-47-29.035355"
            }
        }
    }
}