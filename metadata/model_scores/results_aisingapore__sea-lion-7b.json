{
    "model_name": "aisingapore/sea-lion-7b",
    "last_updated": "2024-12-04 11:24:01.901713",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.018315018315018316,
                "exact_match_stderr": 0.00574369673165366,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.02640642939150402,
                "exact_match_stderr": 0.005436057762573988,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.018518518518518517,
                "exact_match_stderr": 0.0058069728079122715,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.02547065337763012,
                "exact_match_stderr": 0.0052458302725593275,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.005861462425818019,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.008438818565400843,
                "exact_match_stderr": 0.004206007207713057,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_algebra": {
                "exact_match": 0.020219039595619208,
                "exact_match_stderr": 0.004086979080518444,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_3da": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_3ds": {
                "acc": 0.017,
                "acc_stderr": 0.002891311093590548,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_2ds": {
                "acc": 0.1115,
                "acc_stderr": 0.0070397907871727824,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_1dc": {
                "acc": 0.068,
                "acc_stderr": 0.005630617366325325,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000043,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_2dm": {
                "acc": 0.0935,
                "acc_stderr": 0.006511534000335054,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_2da": {
                "acc": 0.087,
                "acc_stderr": 0.006303599581496369,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "gsm8k_cot": {
                "exact_match": 0.03335860500379075,
                "exact_match_stderr": 0.0049462826491737735,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "gsm8k": {
                "exact_match": 0.02577710386656558,
                "exact_match_stderr": 0.004365042953621813,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "anli_r2": {
                "brier_score": 0.7371776965553825,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "anli_r3": {
                "brier_score": 0.7142007390796328,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "anli_r1": {
                "brier_score": 0.7412739329209331,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_eu": {
                "brier_score": 1.113847354553935,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_vi": {
                "brier_score": 0.7514185268043483,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_ru": {
                "brier_score": 0.8063321258911998,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_zh": {
                "brier_score": 1.0744744185180415,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_tr": {
                "brier_score": 0.8904322785337762,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_fr": {
                "brier_score": 0.7973751206359644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_en": {
                "brier_score": 0.6864404290885256,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_ur": {
                "brier_score": 1.3105023198255918,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_ar": {
                "brier_score": 0.8566459473172638,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_de": {
                "brier_score": 0.8048918665563971,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_hi": {
                "brier_score": 1.0018933981610276,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_es": {
                "brier_score": 0.9309039010456585,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_bg": {
                "brier_score": 0.7926756354946493,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_sw": {
                "brier_score": 0.9224405259782638,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_el": {
                "brier_score": 1.1955451021051349,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_th": {
                "brier_score": 0.7636640796291084,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "logiqa2": {
                "brier_score": 1.1108267492611301,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "mathqa": {
                "brier_score": 0.9547448253077948,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "lambada_standard": {
                "perplexity": 6.716223440457293,
                "perplexity_stderr": 0.171380584266027,
                "acc": 0.5767514069474092,
                "acc_stderr": 0.006883418357084482,
                "timestamp": "2024-06-12T10-07-23.284318"
            },
            "lambada_openai": {
                "perplexity": 5.278455783743519,
                "perplexity_stderr": 0.1252746999189692,
                "acc": 0.638074907820687,
                "acc_stderr": 0.00669510284767741,
                "timestamp": "2024-06-12T10-07-23.284318"
            },
            "mmlu_world_religions": {
                "acc": 0.30994152046783624,
                "acc_stderr": 0.03546976959393164,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_formal_logic": {
                "acc": 0.2619047619047619,
                "acc_stderr": 0.039325376803928724,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_prehistory": {
                "acc": 0.25,
                "acc_stderr": 0.02409347123262133,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.27262569832402234,
                "acc_stderr": 0.014893391735249603,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.28270042194092826,
                "acc_stderr": 0.029312814153955917,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2976878612716763,
                "acc_stderr": 0.024617055388677003,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_professional_law": {
                "acc": 0.288135593220339,
                "acc_stderr": 0.011567140661324568,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.25153374233128833,
                "acc_stderr": 0.034089978868575295,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.03077855467869326,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_philosophy": {
                "acc": 0.3054662379421222,
                "acc_stderr": 0.026160584450140485,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_jurisprudence": {
                "acc": 0.28703703703703703,
                "acc_stderr": 0.043733130409147614,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_international_law": {
                "acc": 0.21487603305785125,
                "acc_stderr": 0.037494924487096966,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.03453131801885415,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.29015544041450775,
                "acc_stderr": 0.03275264467791515,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23109243697478993,
                "acc_stderr": 0.027381406927868973,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_geography": {
                "acc": 0.35353535353535354,
                "acc_stderr": 0.03406086723547153,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.27339449541284405,
                "acc_stderr": 0.019109299846098278,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_public_relations": {
                "acc": 0.2818181818181818,
                "acc_stderr": 0.043091187099464606,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_sociology": {
                "acc": 0.2885572139303483,
                "acc_stderr": 0.03203841040213322,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2923076923076923,
                "acc_stderr": 0.023060438380857747,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_security_studies": {
                "acc": 0.44081632653061226,
                "acc_stderr": 0.03178419114175363,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2630718954248366,
                "acc_stderr": 0.017812676542320657,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2824427480916031,
                "acc_stderr": 0.03948406125768361,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_econometrics": {
                "acc": 0.2807017543859649,
                "acc_stderr": 0.042270544512321984,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_miscellaneous": {
                "acc": 0.26053639846743293,
                "acc_stderr": 0.01569600856380709,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_marketing": {
                "acc": 0.2606837606837607,
                "acc_stderr": 0.028760348956523414,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_management": {
                "acc": 0.23300970873786409,
                "acc_stderr": 0.04185832598928315,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_nutrition": {
                "acc": 0.28104575163398693,
                "acc_stderr": 0.02573885479781871,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_medical_genetics": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816506,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_human_aging": {
                "acc": 0.20179372197309417,
                "acc_stderr": 0.026936111912802256,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_professional_medicine": {
                "acc": 0.19852941176470587,
                "acc_stderr": 0.024231013370541073,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_college_medicine": {
                "acc": 0.3583815028901734,
                "acc_stderr": 0.03656343653353159,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_business_ethics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2943396226415094,
                "acc_stderr": 0.028049186315695248,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_global_facts": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_virology": {
                "acc": 0.25903614457831325,
                "acc_stderr": 0.034106466140718564,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2695035460992908,
                "acc_stderr": 0.026469036818590634,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_college_physics": {
                "acc": 0.1568627450980392,
                "acc_stderr": 0.036186648199362466,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_physics": {
                "acc": 0.23178807947019867,
                "acc_stderr": 0.034454062719870546,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_biology": {
                "acc": 0.23225806451612904,
                "acc_stderr": 0.02402225613030824,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_college_biology": {
                "acc": 0.3263888888888889,
                "acc_stderr": 0.03921067198982266,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_anatomy": {
                "acc": 0.22962962962962963,
                "acc_stderr": 0.03633384414073465,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_college_chemistry": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_computer_security": {
                "acc": 0.31,
                "acc_stderr": 0.046482319871173156,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_college_computer_science": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_astronomy": {
                "acc": 0.26973684210526316,
                "acc_stderr": 0.03611780560284898,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_college_mathematics": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.23829787234042554,
                "acc_stderr": 0.027851252973889795,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_machine_learning": {
                "acc": 0.25892857142857145,
                "acc_stderr": 0.04157751539865629,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.22660098522167488,
                "acc_stderr": 0.02945486383529298,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.25462962962962965,
                "acc_stderr": 0.029711275860005357,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25132275132275134,
                "acc_stderr": 0.022340482339643895,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.31724137931034485,
                "acc_stderr": 0.03878352372138622,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.22592592592592592,
                "acc_stderr": 0.02549753263960955,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "arc_challenge": {
                "acc": 0.35921501706484643,
                "acc_stderr": 0.014020224155839154,
                "acc_norm": 0.39590443686006827,
                "acc_norm_stderr": 0.014291228393536587,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "hellaswag": {
                "acc": 0.5088627763393746,
                "acc_stderr": 0.0049889974671344945,
                "acc_norm": 0.6869149571798446,
                "acc_norm_stderr": 0.004628008661955106,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "truthfulqa_mc2": {
                "acc": 0.35126194842988767,
                "acc_stderr": 0.013458543099827976,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "truthfulqa_gen": {
                "bleu_max": 22.656533340238912,
                "bleu_max_stderr": 0.7120387543349416,
                "bleu_acc": 0.29865361077111385,
                "bleu_acc_stderr": 0.016021570613768542,
                "bleu_diff": -9.533528953913965,
                "bleu_diff_stderr": 0.7845698517561239,
                "rouge1_max": 47.07013430195858,
                "rouge1_max_stderr": 0.8588618247484433,
                "rouge1_acc": 0.2521419828641371,
                "rouge1_acc_stderr": 0.015201522246299969,
                "rouge1_diff": -11.523044856861965,
                "rouge1_diff_stderr": 0.8632903667062516,
                "rouge2_max": 29.865710554472003,
                "rouge2_max_stderr": 0.9675510003865858,
                "rouge2_acc": 0.193390452876377,
                "rouge2_acc_stderr": 0.013826240752599066,
                "rouge2_diff": -14.306988948573508,
                "rouge2_diff_stderr": 1.009761759343484,
                "rougeL_max": 44.404211913477454,
                "rougeL_max_stderr": 0.8669444299614717,
                "rougeL_acc": 0.24724602203182375,
                "rougeL_acc_stderr": 0.015102404797359652,
                "rougeL_diff": -11.962363986877257,
                "rougeL_diff_stderr": 0.869977470816951,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "truthfulqa_mc1": {
                "acc": 0.204406364749082,
                "acc_stderr": 0.01411717433743262,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "winogrande": {
                "acc": 0.6156274664561957,
                "acc_stderr": 0.013671567600836194,
                "timestamp": "2024-11-21T21-51-41.609919"
            }
        }
    }
}