{
    "model_name": "aisingapore/sea-lion-7b",
    "last_updated": "2024-12-19 13:39:27.519814",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.018315018315018316,
                "exact_match_stderr": 0.00574369673165366,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.02640642939150402,
                "exact_match_stderr": 0.005436057762573988,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.018518518518518517,
                "exact_match_stderr": 0.0058069728079122715,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.02547065337763012,
                "exact_match_stderr": 0.0052458302725593275,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.005861462425818019,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.008438818565400843,
                "exact_match_stderr": 0.004206007207713057,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "minerva_math_algebra": {
                "exact_match": 0.020219039595619208,
                "exact_match_stderr": 0.004086979080518444,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_3da": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_3ds": {
                "acc": 0.017,
                "acc_stderr": 0.002891311093590548,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_2ds": {
                "acc": 0.1115,
                "acc_stderr": 0.0070397907871727824,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_1dc": {
                "acc": 0.068,
                "acc_stderr": 0.005630617366325325,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000043,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_2dm": {
                "acc": 0.0935,
                "acc_stderr": 0.006511534000335054,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "arithmetic_2da": {
                "acc": 0.087,
                "acc_stderr": 0.006303599581496369,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "gsm8k_cot": {
                "exact_match": 0.03335860500379075,
                "exact_match_stderr": 0.0049462826491737735,
                "timestamp": "2024-06-12T09-56-29.000309"
            },
            "gsm8k": {
                "exact_match": 0.02577710386656558,
                "exact_match_stderr": 0.004365042953621813,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "anli_r2": {
                "brier_score": 0.7371776965553825,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "anli_r3": {
                "brier_score": 0.7142007390796328,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "anli_r1": {
                "brier_score": 0.7412739329209331,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_eu": {
                "brier_score": 1.113847354553935,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_vi": {
                "brier_score": 0.7514185268043483,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_ru": {
                "brier_score": 0.8063321258911998,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_zh": {
                "brier_score": 1.0744744185180415,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_tr": {
                "brier_score": 0.8904322785337762,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_fr": {
                "brier_score": 0.7973751206359644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_en": {
                "brier_score": 0.6864404290885256,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_ur": {
                "brier_score": 1.3105023198255918,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_ar": {
                "brier_score": 0.8566459473172638,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_de": {
                "brier_score": 0.8048918665563971,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_hi": {
                "brier_score": 1.0018933981610276,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_es": {
                "brier_score": 0.9309039010456585,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_bg": {
                "brier_score": 0.7926756354946493,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_sw": {
                "brier_score": 0.9224405259782638,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_el": {
                "brier_score": 1.1955451021051349,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "xnli_th": {
                "brier_score": 0.7636640796291084,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "logiqa2": {
                "brier_score": 1.1108267492611301,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "mathqa": {
                "brier_score": 0.9547448253077948,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-12T10-06-03.947520"
            },
            "lambada_standard": {
                "perplexity": 6.716223440457293,
                "perplexity_stderr": 0.171380584266027,
                "acc": 0.5767514069474092,
                "acc_stderr": 0.006883418357084482,
                "timestamp": "2024-06-12T10-07-23.284318"
            },
            "lambada_openai": {
                "perplexity": 5.278455783743519,
                "perplexity_stderr": 0.1252746999189692,
                "acc": 0.638074907820687,
                "acc_stderr": 0.00669510284767741,
                "timestamp": "2024-06-12T10-07-23.284318"
            },
            "mmlu_world_religions": {
                "acc": 0.22807017543859648,
                "acc_stderr": 0.03218093795602357,
                "brier_score": 0.7752774085491143,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_formal_logic": {
                "acc": 0.30158730158730157,
                "acc_stderr": 0.04104947269903394,
                "brier_score": 0.7502519936902966,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_prehistory": {
                "acc": 0.3055555555555556,
                "acc_stderr": 0.025630824975621344,
                "brier_score": 0.7520781517274742,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2435754189944134,
                "acc_stderr": 0.01435591196476786,
                "brier_score": 0.757214810611251,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2616033755274262,
                "acc_stderr": 0.028609516716994934,
                "brier_score": 0.7731902924413044,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2745664739884393,
                "acc_stderr": 0.024027745155265012,
                "brier_score": 0.7576887106989154,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_professional_law": {
                "acc": 0.2607561929595828,
                "acc_stderr": 0.01121347155960232,
                "brier_score": 0.7605541627446022,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.31901840490797545,
                "acc_stderr": 0.03661997551073836,
                "brier_score": 0.7547284277689329,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.03058759135160425,
                "brier_score": 0.7724803077641238,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_philosophy": {
                "acc": 0.3215434083601286,
                "acc_stderr": 0.026527724079528872,
                "brier_score": 0.738986747794473,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_jurisprudence": {
                "acc": 0.3425925925925926,
                "acc_stderr": 0.04587904741301812,
                "brier_score": 0.7502170737990164,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_international_law": {
                "acc": 0.30578512396694213,
                "acc_stderr": 0.04205953933884124,
                "brier_score": 0.7544421936349635,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.296969696969697,
                "acc_stderr": 0.03567969772268049,
                "brier_score": 0.7638611557055202,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.22279792746113988,
                "acc_stderr": 0.03003114797764154,
                "brier_score": 0.7592438626250126,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.226890756302521,
                "acc_stderr": 0.027205371538279483,
                "brier_score": 0.7520425312177341,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_geography": {
                "acc": 0.23232323232323232,
                "acc_stderr": 0.030088629490217483,
                "brier_score": 0.7559098136708166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23853211009174313,
                "acc_stderr": 0.01827257581023186,
                "brier_score": 0.7614810618696795,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_public_relations": {
                "acc": 0.23636363636363636,
                "acc_stderr": 0.04069306319721376,
                "brier_score": 0.7607146453285218,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252606,
                "brier_score": 0.7384479897630887,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_sociology": {
                "acc": 0.25870646766169153,
                "acc_stderr": 0.030965903123573037,
                "brier_score": 0.7578779619880223,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24358974358974358,
                "acc_stderr": 0.02176373368417392,
                "brier_score": 0.7472250881282662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_security_studies": {
                "acc": 0.4,
                "acc_stderr": 0.03136250240935893,
                "brier_score": 0.7162989736314286,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_professional_psychology": {
                "acc": 0.27124183006535946,
                "acc_stderr": 0.017986615304030312,
                "brier_score": 0.7546982787291897,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_human_sexuality": {
                "acc": 0.31297709923664124,
                "acc_stderr": 0.04066962905677698,
                "brier_score": 0.7554849086865097,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_econometrics": {
                "acc": 0.2543859649122807,
                "acc_stderr": 0.0409698513984367,
                "brier_score": 0.7646334716879251,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_miscellaneous": {
                "acc": 0.26309067688378035,
                "acc_stderr": 0.015745497169049046,
                "brier_score": 0.7643217133064008,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_marketing": {
                "acc": 0.23504273504273504,
                "acc_stderr": 0.027778835904935423,
                "brier_score": 0.7756730657667739,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_management": {
                "acc": 0.24271844660194175,
                "acc_stderr": 0.04245022486384493,
                "brier_score": 0.7511358086600124,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_nutrition": {
                "acc": 0.3104575163398693,
                "acc_stderr": 0.0264930332251459,
                "brier_score": 0.7471881741064178,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_medical_genetics": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.7655001909042981,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_human_aging": {
                "acc": 0.26905829596412556,
                "acc_stderr": 0.029763779406874975,
                "brier_score": 0.7839268773478784,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_professional_medicine": {
                "acc": 0.20220588235294118,
                "acc_stderr": 0.024398192986654924,
                "brier_score": 0.7528840585295138,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_college_medicine": {
                "acc": 0.2543352601156069,
                "acc_stderr": 0.0332055644308557,
                "brier_score": 0.7539792708412846,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_business_ethics": {
                "acc": 0.16,
                "acc_stderr": 0.036845294917747094,
                "brier_score": 0.7729585342994129,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.27169811320754716,
                "acc_stderr": 0.02737770662467071,
                "brier_score": 0.7556143704191626,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_global_facts": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "brier_score": 0.7480811312516856,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_virology": {
                "acc": 0.29518072289156627,
                "acc_stderr": 0.0355092018568963,
                "brier_score": 0.7591723780375526,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_professional_accounting": {
                "acc": 0.25886524822695034,
                "acc_stderr": 0.026129572527180848,
                "brier_score": 0.7633880028569879,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_college_physics": {
                "acc": 0.27450980392156865,
                "acc_stderr": 0.044405219061793254,
                "brier_score": 0.7585494145393273,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_physics": {
                "acc": 0.32450331125827814,
                "acc_stderr": 0.03822746937658753,
                "brier_score": 0.7394946142667588,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_biology": {
                "acc": 0.27741935483870966,
                "acc_stderr": 0.025470196835900055,
                "brier_score": 0.7485462690398437,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "brier_score": 0.763165184784987,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_anatomy": {
                "acc": 0.3111111111111111,
                "acc_stderr": 0.03999262876617724,
                "brier_score": 0.734966060147871,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_college_chemistry": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.7627976595239333,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_computer_security": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 0.7738218912003072,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_college_computer_science": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.747236755807785,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_astronomy": {
                "acc": 0.3223684210526316,
                "acc_stderr": 0.038035102483515854,
                "brier_score": 0.7375677113730162,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_college_mathematics": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.756023191070122,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.225531914893617,
                "acc_stderr": 0.027321078417387533,
                "brier_score": 0.7741473196300603,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909284,
                "brier_score": 0.7697823399701942,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.750020760483219,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_machine_learning": {
                "acc": 0.29464285714285715,
                "acc_stderr": 0.04327040932578728,
                "brier_score": 0.7839518900665153,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.32019704433497537,
                "acc_stderr": 0.032826493853041504,
                "brier_score": 0.7354908609195902,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2175925925925926,
                "acc_stderr": 0.02813968944485967,
                "brier_score": 0.7414971120687751,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.28835978835978837,
                "acc_stderr": 0.023330654054535882,
                "brier_score": 0.7529959049377857,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.3103448275862069,
                "acc_stderr": 0.03855289616378949,
                "brier_score": 0.7518379881767014,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2740740740740741,
                "acc_stderr": 0.027195934804085622,
                "brier_score": 0.7608680277559734,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-58-13.342162"
            },
            "arc_challenge": {
                "acc": 0.35921501706484643,
                "acc_stderr": 0.014020224155839154,
                "acc_norm": 0.39590443686006827,
                "acc_norm_stderr": 0.014291228393536587,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "hellaswag": {
                "acc": 0.5088627763393746,
                "acc_stderr": 0.0049889974671344945,
                "acc_norm": 0.6869149571798446,
                "acc_norm_stderr": 0.004628008661955106,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "truthfulqa_mc2": {
                "acc": 0.35126194842988767,
                "acc_stderr": 0.013458543099827976,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "truthfulqa_gen": {
                "bleu_max": 22.656533340238912,
                "bleu_max_stderr": 0.7120387543349416,
                "bleu_acc": 0.29865361077111385,
                "bleu_acc_stderr": 0.016021570613768542,
                "bleu_diff": -9.533528953913965,
                "bleu_diff_stderr": 0.7845698517561239,
                "rouge1_max": 47.07013430195858,
                "rouge1_max_stderr": 0.8588618247484433,
                "rouge1_acc": 0.2521419828641371,
                "rouge1_acc_stderr": 0.015201522246299969,
                "rouge1_diff": -11.523044856861965,
                "rouge1_diff_stderr": 0.8632903667062516,
                "rouge2_max": 29.865710554472003,
                "rouge2_max_stderr": 0.9675510003865858,
                "rouge2_acc": 0.193390452876377,
                "rouge2_acc_stderr": 0.013826240752599066,
                "rouge2_diff": -14.306988948573508,
                "rouge2_diff_stderr": 1.009761759343484,
                "rougeL_max": 44.404211913477454,
                "rougeL_max_stderr": 0.8669444299614717,
                "rougeL_acc": 0.24724602203182375,
                "rougeL_acc_stderr": 0.015102404797359652,
                "rougeL_diff": -11.962363986877257,
                "rougeL_diff_stderr": 0.869977470816951,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "truthfulqa_mc1": {
                "acc": 0.204406364749082,
                "acc_stderr": 0.01411717433743262,
                "timestamp": "2024-11-21T21-51-41.609919"
            },
            "winogrande": {
                "acc": 0.6156274664561957,
                "acc_stderr": 0.013671567600836194,
                "timestamp": "2024-11-21T21-51-41.609919"
            }
        }
    }
}