{
    "model_name": "EleutherAI/pythia-6.9b-deduped",
    "last_updated": "2023-10-22",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.3728668941979522,
                    "acc_stderr": 0.014131176760131162,
                    "acc_norm": 0.4129692832764505,
                    "acc_norm_stderr": 0.014388344935398326,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.492531368253336,
                    "acc_stderr": 0.004989224715784536,
                    "acc_norm": 0.6704839673371839,
                    "acc_norm_stderr": 0.004690768393854471,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847415,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847415,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.23703703703703705,
                    "acc_stderr": 0.03673731683969506,
                    "acc_norm": 0.23703703703703705,
                    "acc_norm_stderr": 0.03673731683969506,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.23684210526315788,
                    "acc_stderr": 0.03459777606810535,
                    "acc_norm": 0.23684210526315788,
                    "acc_norm_stderr": 0.03459777606810535,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.040201512610368445,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.040201512610368445,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2528301886792453,
                    "acc_stderr": 0.026749899771241238,
                    "acc_norm": 0.2528301886792453,
                    "acc_norm_stderr": 0.026749899771241238,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2569444444444444,
                    "acc_stderr": 0.03653946969442099,
                    "acc_norm": 0.2569444444444444,
                    "acc_norm_stderr": 0.03653946969442099,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.03861229196653697,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.03861229196653697,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.047937248544110196,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.047937248544110196,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.19653179190751446,
                    "acc_stderr": 0.03029957466478814,
                    "acc_norm": 0.19653179190751446,
                    "acc_norm_stderr": 0.03029957466478814,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.04023382273617748,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.04023382273617748,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909283,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909283,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2936170212765957,
                    "acc_stderr": 0.029771642712491227,
                    "acc_norm": 0.2936170212765957,
                    "acc_norm_stderr": 0.029771642712491227,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.22807017543859648,
                    "acc_stderr": 0.03947152782669415,
                    "acc_norm": 0.22807017543859648,
                    "acc_norm_stderr": 0.03947152782669415,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.30344827586206896,
                    "acc_stderr": 0.038312260488503336,
                    "acc_norm": 0.30344827586206896,
                    "acc_norm_stderr": 0.038312260488503336,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.0220190800122179,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.0220190800122179,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.21428571428571427,
                    "acc_stderr": 0.03670066451047181,
                    "acc_norm": 0.21428571428571427,
                    "acc_norm_stderr": 0.03670066451047181,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.23870967741935484,
                    "acc_stderr": 0.024251071262208837,
                    "acc_norm": 0.23870967741935484,
                    "acc_norm_stderr": 0.024251071262208837,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.3054187192118227,
                    "acc_stderr": 0.03240661565868408,
                    "acc_norm": 0.3054187192118227,
                    "acc_norm_stderr": 0.03240661565868408,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.039427724440366255,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.039427724440366255,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.23636363636363636,
                    "acc_stderr": 0.033175059300091805,
                    "acc_norm": 0.23636363636363636,
                    "acc_norm_stderr": 0.033175059300091805,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.23737373737373738,
                    "acc_stderr": 0.030313710538198892,
                    "acc_norm": 0.23737373737373738,
                    "acc_norm_stderr": 0.030313710538198892,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.2538860103626943,
                    "acc_stderr": 0.0314102478056532,
                    "acc_norm": 0.2538860103626943,
                    "acc_norm_stderr": 0.0314102478056532,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.25384615384615383,
                    "acc_stderr": 0.022066054378726257,
                    "acc_norm": 0.25384615384615383,
                    "acc_norm_stderr": 0.022066054378726257,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.026962424325073828,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.026962424325073828,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2184873949579832,
                    "acc_stderr": 0.026841514322958938,
                    "acc_norm": 0.2184873949579832,
                    "acc_norm_stderr": 0.026841514322958938,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31125827814569534,
                    "acc_stderr": 0.03780445850526731,
                    "acc_norm": 0.31125827814569534,
                    "acc_norm_stderr": 0.03780445850526731,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.21834862385321102,
                    "acc_stderr": 0.017712600528722727,
                    "acc_norm": 0.21834862385321102,
                    "acc_norm_stderr": 0.017712600528722727,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.44907407407407407,
                    "acc_stderr": 0.03392238405321617,
                    "acc_norm": 0.44907407407407407,
                    "acc_norm_stderr": 0.03392238405321617,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.2549019607843137,
                    "acc_stderr": 0.030587591351604246,
                    "acc_norm": 0.2549019607843137,
                    "acc_norm_stderr": 0.030587591351604246,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.22784810126582278,
                    "acc_stderr": 0.027303484599069432,
                    "acc_norm": 0.22784810126582278,
                    "acc_norm_stderr": 0.027303484599069432,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.37668161434977576,
                    "acc_stderr": 0.03252113489929188,
                    "acc_norm": 0.37668161434977576,
                    "acc_norm_stderr": 0.03252113489929188,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.03727673575596918,
                    "acc_norm": 0.2366412213740458,
                    "acc_norm_stderr": 0.03727673575596918,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.3305785123966942,
                    "acc_stderr": 0.04294340845212095,
                    "acc_norm": 0.3305785123966942,
                    "acc_norm_stderr": 0.04294340845212095,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.28703703703703703,
                    "acc_stderr": 0.043733130409147614,
                    "acc_norm": 0.28703703703703703,
                    "acc_norm_stderr": 0.043733130409147614,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.3128834355828221,
                    "acc_stderr": 0.036429145782924055,
                    "acc_norm": 0.3128834355828221,
                    "acc_norm_stderr": 0.036429145782924055,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.33035714285714285,
                    "acc_stderr": 0.04464285714285713,
                    "acc_norm": 0.33035714285714285,
                    "acc_norm_stderr": 0.04464285714285713,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2815533980582524,
                    "acc_stderr": 0.04453254836326468,
                    "acc_norm": 0.2815533980582524,
                    "acc_norm_stderr": 0.04453254836326468,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.24786324786324787,
                    "acc_stderr": 0.028286324075564397,
                    "acc_norm": 0.24786324786324787,
                    "acc_norm_stderr": 0.028286324075564397,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909283,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909283,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.28991060025542786,
                    "acc_stderr": 0.016225017944770954,
                    "acc_norm": 0.28991060025542786,
                    "acc_norm_stderr": 0.016225017944770954,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2138728323699422,
                    "acc_stderr": 0.02207570925175718,
                    "acc_norm": 0.2138728323699422,
                    "acc_norm_stderr": 0.02207570925175718,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2435754189944134,
                    "acc_stderr": 0.014355911964767865,
                    "acc_norm": 0.2435754189944134,
                    "acc_norm_stderr": 0.014355911964767865,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.25163398692810457,
                    "acc_stderr": 0.024848018263875202,
                    "acc_norm": 0.25163398692810457,
                    "acc_norm_stderr": 0.024848018263875202,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.2861736334405145,
                    "acc_stderr": 0.02567025924218894,
                    "acc_norm": 0.2861736334405145,
                    "acc_norm_stderr": 0.02567025924218894,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2654320987654321,
                    "acc_stderr": 0.024569223600460842,
                    "acc_norm": 0.2654320987654321,
                    "acc_norm_stderr": 0.024569223600460842,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2801418439716312,
                    "acc_stderr": 0.026789172351140242,
                    "acc_norm": 0.2801418439716312,
                    "acc_norm_stderr": 0.026789172351140242,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.25554106910039115,
                    "acc_stderr": 0.011139857833598521,
                    "acc_norm": 0.25554106910039115,
                    "acc_norm_stderr": 0.011139857833598521,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.21323529411764705,
                    "acc_stderr": 0.024880971512294278,
                    "acc_norm": 0.21323529411764705,
                    "acc_norm_stderr": 0.024880971512294278,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.01812022425148458,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.01812022425148458,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.21818181818181817,
                    "acc_stderr": 0.03955932861795833,
                    "acc_norm": 0.21818181818181817,
                    "acc_norm_stderr": 0.03955932861795833,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.31020408163265306,
                    "acc_stderr": 0.029613459872484378,
                    "acc_norm": 0.31020408163265306,
                    "acc_norm_stderr": 0.029613459872484378,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.23880597014925373,
                    "acc_stderr": 0.030147775935409217,
                    "acc_norm": 0.23880597014925373,
                    "acc_norm_stderr": 0.030147775935409217,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542127,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3493975903614458,
                    "acc_stderr": 0.03711725190740751,
                    "acc_norm": 0.3493975903614458,
                    "acc_norm_stderr": 0.03711725190740751,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.2982456140350877,
                    "acc_stderr": 0.03508771929824563,
                    "acc_norm": 0.2982456140350877,
                    "acc_norm_stderr": 0.03508771929824563,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.21542227662178703,
                    "mc1_stderr": 0.014391902652427683,
                    "mc2": 0.3519458488266109,
                    "mc2_stderr": 0.013230951607864657,
                    "timestamp": "2023-07-19T17-40-55.095296"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.0007340604026845638,
                    "em_stderr": 0.0002773614457335642,
                    "f1": 0.04495805369127533,
                    "f1_stderr": 0.0011424943224633687,
                    "timestamp": "2023-10-22T01-47-10.144336"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.016679302501895376,
                    "acc_stderr": 0.003527595888722438,
                    "timestamp": "2023-10-22T01-47-10.144336"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.6408839779005525,
                    "acc_stderr": 0.013483115202120236,
                    "timestamp": "2023-10-22T01-47-10.144336"
                }
            }
        }
    }
}