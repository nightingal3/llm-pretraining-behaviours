{
    "model_name": "cerebras/Cerebras-GPT-1.3B",
    "last_updated": "2024-12-04 11:24:08.682755",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.009157509157509158,
                    "acc_stderr": 0.004080306065048968,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.012629161882893225,
                    "acc_stderr": 0.0037858882182630017,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.012962962962962963,
                    "acc_stderr": 0.004872192984581503,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.015503875968992248,
                    "acc_stderr": 0.004113617238360454,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.006263048016701462,
                    "acc_stderr": 0.003608399732887888,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.010548523206751054,
                    "acc_stderr": 0.004697453735376145,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.009267059814658803,
                    "acc_stderr": 0.0027823191184887844,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.001,
                    "acc_stderr": 0.0007069298939339458,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.0015,
                    "acc_stderr": 0.0008655920660521539,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.0005,
                    "acc_stderr": 0.0005000000000000151,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.013,
                    "acc_stderr": 0.0025335171905233197,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.0275,
                    "acc_stderr": 0.0036576719757437743,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.022,
                    "acc_stderr": 0.0032807593162018913,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 0.007,
                    "acc_stderr": 0.0018647355360237512,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.014404852160727824,
                    "acc_stderr": 0.003282055917136958,
                    "timestamp": "2024-06-11T18-28-01.677806"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.02122820318423048,
                    "acc_stderr": 0.003970449129848635,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "anli_r2": {
                "0-shot": {
                    "brier_score": 0.8160206698968121,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "anli_r3": {
                "0-shot": {
                    "brier_score": 0.7564486854420894,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "anli_r1": {
                "0-shot": {
                    "brier_score": 0.8453036236123399,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_eu": {
                "0-shot": {
                    "brier_score": 1.203122941977538,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_vi": {
                "0-shot": {
                    "brier_score": 0.9581888474081791,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_ru": {
                "0-shot": {
                    "brier_score": 0.8555274102354892,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_zh": {
                "0-shot": {
                    "brier_score": 0.9525134852831153,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_tr": {
                "0-shot": {
                    "brier_score": 0.8488424294763287,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_fr": {
                "0-shot": {
                    "brier_score": 0.9004072134991292,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_en": {
                "0-shot": {
                    "brier_score": 0.676735865099788,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_ur": {
                "0-shot": {
                    "brier_score": 1.119118044439167,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_ar": {
                "0-shot": {
                    "brier_score": 1.0334678129216435,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_de": {
                "0-shot": {
                    "brier_score": 0.8840107844662859,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_hi": {
                "0-shot": {
                    "brier_score": 0.9489724257727796,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_es": {
                "0-shot": {
                    "brier_score": 0.9940572286363858,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_bg": {
                "0-shot": {
                    "brier_score": 0.7972760255131902,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_sw": {
                "0-shot": {
                    "brier_score": 0.9917977124323483,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_el": {
                "0-shot": {
                    "brier_score": 1.2932784189714346,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "xnli_th": {
                "0-shot": {
                    "brier_score": 0.7809459010580413,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "brier_score": 1.2019264190394812,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "mathqa": {
                "5-shot": {
                    "brier_score": 0.9930432217011608,
                    "timestamp": "2024-06-11T18-38-40.786526"
                }
            },
            "lambada_standard": {
                "0-shot": {
                    "perplexity": 30.331273764614334,
                    "perplexity_stderr": 1.0890794817806768,
                    "acc": 0.3584319813700757,
                    "acc_stderr": 0.006680928173680374,
                    "timestamp": "2024-06-11T18-40-02.959579"
                }
            },
            "lambada_openai": {
                "0-shot": {
                    "perplexity": 14.11714896411139,
                    "perplexity_stderr": 0.4545475556587265,
                    "acc": 0.4614787502425771,
                    "acc_stderr": 0.006945273445805861,
                    "timestamp": "2024-06-11T18-40-02.959579"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.30409356725146197,
                    "acc_stderr": 0.03528211258245231,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.2619047619047619,
                    "acc_stderr": 0.039325376803928724,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.2623456790123457,
                    "acc_stderr": 0.024477222856135124,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.23687150837988827,
                    "acc_stderr": 0.014219570788103982,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.2742616033755274,
                    "acc_stderr": 0.029041333510598046,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.26878612716763006,
                    "acc_stderr": 0.02386800326250011,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.2379400260756193,
                    "acc_stderr": 0.01087570078769423,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.3006134969325153,
                    "acc_stderr": 0.03602511318806771,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.029331162294251728,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.26688102893890675,
                    "acc_stderr": 0.025122637608816643,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.041331194402438376,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.2727272727272727,
                    "acc_stderr": 0.04065578140908705,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.03453131801885415,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.34196891191709844,
                    "acc_stderr": 0.03423465100104282,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.22268907563025211,
                    "acc_stderr": 0.027025433498882357,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.31313131313131315,
                    "acc_stderr": 0.03304205087813652,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.3431192660550459,
                    "acc_stderr": 0.020354777736086037,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.16363636363636364,
                    "acc_stderr": 0.035434330542986794,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036843,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.22885572139303484,
                    "acc_stderr": 0.02970528405677245,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.3230769230769231,
                    "acc_stderr": 0.023710888501970565,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.23265306122448978,
                    "acc_stderr": 0.02704925791589618,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.272875816993464,
                    "acc_stderr": 0.01802047414839358,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.03727673575596918,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.24561403508771928,
                    "acc_stderr": 0.04049339297748141,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.24776500638569604,
                    "acc_stderr": 0.015438083080568965,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.2905982905982906,
                    "acc_stderr": 0.029745048572674057,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.20388349514563106,
                    "acc_stderr": 0.0398913985953177,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.2973856209150327,
                    "acc_stderr": 0.02617390850671858,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.242152466367713,
                    "acc_stderr": 0.028751392398694755,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.41911764705882354,
                    "acc_stderr": 0.02997280717046463,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.2774566473988439,
                    "acc_stderr": 0.034140140070440354,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322716,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.27169811320754716,
                    "acc_stderr": 0.027377706624670713,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.3313253012048193,
                    "acc_stderr": 0.03664314777288085,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.2765957446808511,
                    "acc_stderr": 0.026684564340461008,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.04158307533083286,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.23841059602649006,
                    "acc_stderr": 0.0347918557259966,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.2032258064516129,
                    "acc_stderr": 0.022891687984554963,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.2361111111111111,
                    "acc_stderr": 0.03551446610810826,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.2074074074074074,
                    "acc_stderr": 0.03502553170678317,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909284,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.04093601807403325,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695235,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.21052631578947367,
                    "acc_stderr": 0.033176727875331574,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.2851063829787234,
                    "acc_stderr": 0.02951319662553935,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.04560480215720684,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.29464285714285715,
                    "acc_stderr": 0.04327040932578728,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.23645320197044334,
                    "acc_stderr": 0.029896114291733552,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.4722222222222222,
                    "acc_stderr": 0.0340470532865388,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.0220190800122179,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.23448275862068965,
                    "acc_stderr": 0.035306258743465914,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.26296296296296295,
                    "acc_stderr": 0.026842057873833706,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.24744027303754265,
                    "acc_stderr": 0.01261035266329267,
                    "acc_norm": 0.2738907849829352,
                    "acc_norm_stderr": 0.013032004972989503,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.32433778131846247,
                    "acc_stderr": 0.004671701705567258,
                    "acc_norm": 0.38946425014937264,
                    "acc_norm_stderr": 0.004866322258335968,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.4270182875007654,
                    "acc_stderr": 0.014896443872403905,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 15.75896364576763,
                    "bleu_max_stderr": 0.5364357683647447,
                    "bleu_acc": 0.3598531211750306,
                    "bleu_acc_stderr": 0.016801860466677174,
                    "bleu_diff": -3.3445863219845346,
                    "bleu_diff_stderr": 0.5222101463879798,
                    "rouge1_max": 36.903769503650345,
                    "rouge1_max_stderr": 0.7998635375855746,
                    "rouge1_acc": 0.2913096695226438,
                    "rouge1_acc_stderr": 0.01590598704818482,
                    "rouge1_diff": -6.460703210665295,
                    "rouge1_diff_stderr": 0.7328034463145772,
                    "rouge2_max": 19.326584406586324,
                    "rouge2_max_stderr": 0.8077506474590068,
                    "rouge2_acc": 0.18237454100367198,
                    "rouge2_acc_stderr": 0.013518055636187212,
                    "rouge2_diff": -6.646991226024485,
                    "rouge2_diff_stderr": 0.7429900455011789,
                    "rougeL_max": 33.790056148847405,
                    "rougeL_max_stderr": 0.78334334845043,
                    "rougeL_acc": 0.28518971848225216,
                    "rougeL_acc_stderr": 0.015805827874454892,
                    "rougeL_diff": -6.286118787315412,
                    "rougeL_diff_stderr": 0.7141317626988594,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.24479804161566707,
                    "acc_stderr": 0.015051869486714997,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5248618784530387,
                    "acc_stderr": 0.01403510288362775,
                    "timestamp": "2024-11-21T01-52-17.626746"
                }
            }
        }
    }
}