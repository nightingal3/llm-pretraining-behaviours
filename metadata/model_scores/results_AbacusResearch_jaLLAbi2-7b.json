{
    "model_name": "AbacusResearch/jaLLAbi2-7b",
    "last_updated": "2024-02-20",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.6885665529010239,
                    "acc_stderr": 0.01353247209985094,
                    "acc_norm": 0.7167235494880546,
                    "acc_norm_stderr": 0.013167478735134575,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.7123083051185023,
                    "acc_stderr": 0.004517614647703243,
                    "acc_norm": 0.8828918542123083,
                    "acc_norm_stderr": 0.003208919510309925,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.0479372485441102,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.0479372485441102,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6518518518518519,
                    "acc_stderr": 0.041153246103369526,
                    "acc_norm": 0.6518518518518519,
                    "acc_norm_stderr": 0.041153246103369526,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.6907894736842105,
                    "acc_stderr": 0.037610708698674805,
                    "acc_norm": 0.6907894736842105,
                    "acc_norm_stderr": 0.037610708698674805,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.64,
                    "acc_stderr": 0.04824181513244218,
                    "acc_norm": 0.64,
                    "acc_norm_stderr": 0.04824181513244218,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.6943396226415094,
                    "acc_stderr": 0.028353298073322666,
                    "acc_norm": 0.6943396226415094,
                    "acc_norm_stderr": 0.028353298073322666,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.7638888888888888,
                    "acc_stderr": 0.03551446610810826,
                    "acc_norm": 0.7638888888888888,
                    "acc_norm_stderr": 0.03551446610810826,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.48,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.53,
                    "acc_stderr": 0.050161355804659205,
                    "acc_norm": 0.53,
                    "acc_norm_stderr": 0.050161355804659205,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.6647398843930635,
                    "acc_stderr": 0.03599586301247077,
                    "acc_norm": 0.6647398843930635,
                    "acc_norm_stderr": 0.03599586301247077,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.43137254901960786,
                    "acc_stderr": 0.04928099597287534,
                    "acc_norm": 0.43137254901960786,
                    "acc_norm_stderr": 0.04928099597287534,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.77,
                    "acc_stderr": 0.042295258468165065,
                    "acc_norm": 0.77,
                    "acc_norm_stderr": 0.042295258468165065,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.5829787234042553,
                    "acc_stderr": 0.03223276266711712,
                    "acc_norm": 0.5829787234042553,
                    "acc_norm_stderr": 0.03223276266711712,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.4824561403508772,
                    "acc_stderr": 0.04700708033551038,
                    "acc_norm": 0.4824561403508772,
                    "acc_norm_stderr": 0.04700708033551038,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5517241379310345,
                    "acc_stderr": 0.04144311810878152,
                    "acc_norm": 0.5517241379310345,
                    "acc_norm_stderr": 0.04144311810878152,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.42328042328042326,
                    "acc_stderr": 0.025446365634406783,
                    "acc_norm": 0.42328042328042326,
                    "acc_norm_stderr": 0.025446365634406783,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.48412698412698413,
                    "acc_stderr": 0.04469881854072606,
                    "acc_norm": 0.48412698412698413,
                    "acc_norm_stderr": 0.04469881854072606,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.04793724854411019,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.04793724854411019,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7870967741935484,
                    "acc_stderr": 0.02328766512726854,
                    "acc_norm": 0.7870967741935484,
                    "acc_norm_stderr": 0.02328766512726854,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.5123152709359606,
                    "acc_stderr": 0.035169204442208966,
                    "acc_norm": 0.5123152709359606,
                    "acc_norm_stderr": 0.035169204442208966,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.69,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.69,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.7818181818181819,
                    "acc_stderr": 0.03225078108306289,
                    "acc_norm": 0.7818181818181819,
                    "acc_norm_stderr": 0.03225078108306289,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.8131313131313131,
                    "acc_stderr": 0.027772533334218967,
                    "acc_norm": 0.8131313131313131,
                    "acc_norm_stderr": 0.027772533334218967,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.8963730569948186,
                    "acc_stderr": 0.02199531196364424,
                    "acc_norm": 0.8963730569948186,
                    "acc_norm_stderr": 0.02199531196364424,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.6666666666666666,
                    "acc_stderr": 0.023901157979402538,
                    "acc_norm": 0.6666666666666666,
                    "acc_norm_stderr": 0.023901157979402538,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.3296296296296296,
                    "acc_stderr": 0.028661201116524565,
                    "acc_norm": 0.3296296296296296,
                    "acc_norm_stderr": 0.028661201116524565,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6764705882352942,
                    "acc_stderr": 0.03038835355188679,
                    "acc_norm": 0.6764705882352942,
                    "acc_norm_stderr": 0.03038835355188679,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.3576158940397351,
                    "acc_stderr": 0.03913453431177258,
                    "acc_norm": 0.3576158940397351,
                    "acc_norm_stderr": 0.03913453431177258,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8458715596330275,
                    "acc_stderr": 0.015480826865374303,
                    "acc_norm": 0.8458715596330275,
                    "acc_norm_stderr": 0.015480826865374303,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.5092592592592593,
                    "acc_stderr": 0.034093869469927006,
                    "acc_norm": 0.5092592592592593,
                    "acc_norm_stderr": 0.034093869469927006,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.8578431372549019,
                    "acc_stderr": 0.02450980392156862,
                    "acc_norm": 0.8578431372549019,
                    "acc_norm_stderr": 0.02450980392156862,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.8227848101265823,
                    "acc_stderr": 0.02485636418450322,
                    "acc_norm": 0.8227848101265823,
                    "acc_norm_stderr": 0.02485636418450322,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.6905829596412556,
                    "acc_stderr": 0.03102441174057221,
                    "acc_norm": 0.6905829596412556,
                    "acc_norm_stderr": 0.03102441174057221,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.8015267175572519,
                    "acc_stderr": 0.03498149385462472,
                    "acc_norm": 0.8015267175572519,
                    "acc_norm_stderr": 0.03498149385462472,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.7603305785123967,
                    "acc_stderr": 0.03896878985070416,
                    "acc_norm": 0.7603305785123967,
                    "acc_norm_stderr": 0.03896878985070416,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.7870370370370371,
                    "acc_stderr": 0.0395783547198098,
                    "acc_norm": 0.7870370370370371,
                    "acc_norm_stderr": 0.0395783547198098,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.7668711656441718,
                    "acc_stderr": 0.0332201579577674,
                    "acc_norm": 0.7668711656441718,
                    "acc_norm_stderr": 0.0332201579577674,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.44642857142857145,
                    "acc_stderr": 0.04718471485219588,
                    "acc_norm": 0.44642857142857145,
                    "acc_norm_stderr": 0.04718471485219588,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.7766990291262136,
                    "acc_stderr": 0.04123553189891431,
                    "acc_norm": 0.7766990291262136,
                    "acc_norm_stderr": 0.04123553189891431,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8760683760683761,
                    "acc_stderr": 0.021586494001281376,
                    "acc_norm": 0.8760683760683761,
                    "acc_norm_stderr": 0.021586494001281376,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.7,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.7,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.8288633461047255,
                    "acc_stderr": 0.0134682016140663,
                    "acc_norm": 0.8288633461047255,
                    "acc_norm_stderr": 0.0134682016140663,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7543352601156069,
                    "acc_stderr": 0.023176298203992,
                    "acc_norm": 0.7543352601156069,
                    "acc_norm_stderr": 0.023176298203992,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.46256983240223465,
                    "acc_stderr": 0.016675578687308082,
                    "acc_norm": 0.46256983240223465,
                    "acc_norm_stderr": 0.016675578687308082,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7287581699346405,
                    "acc_stderr": 0.025457756696667878,
                    "acc_norm": 0.7287581699346405,
                    "acc_norm_stderr": 0.025457756696667878,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.7331189710610932,
                    "acc_stderr": 0.02512263760881666,
                    "acc_norm": 0.7331189710610932,
                    "acc_norm_stderr": 0.02512263760881666,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.7407407407407407,
                    "acc_stderr": 0.02438366553103545,
                    "acc_norm": 0.7407407407407407,
                    "acc_norm_stderr": 0.02438366553103545,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.5035460992907801,
                    "acc_stderr": 0.02982674915328092,
                    "acc_norm": 0.5035460992907801,
                    "acc_norm_stderr": 0.02982674915328092,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.4726205997392438,
                    "acc_stderr": 0.012751075788015058,
                    "acc_norm": 0.4726205997392438,
                    "acc_norm_stderr": 0.012751075788015058,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.6764705882352942,
                    "acc_stderr": 0.02841820861940676,
                    "acc_norm": 0.6764705882352942,
                    "acc_norm_stderr": 0.02841820861940676,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.6764705882352942,
                    "acc_stderr": 0.018926082916083383,
                    "acc_norm": 0.6764705882352942,
                    "acc_norm_stderr": 0.018926082916083383,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6727272727272727,
                    "acc_stderr": 0.0449429086625209,
                    "acc_norm": 0.6727272727272727,
                    "acc_norm_stderr": 0.0449429086625209,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.7387755102040816,
                    "acc_stderr": 0.028123429335142783,
                    "acc_norm": 0.7387755102040816,
                    "acc_norm_stderr": 0.028123429335142783,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.835820895522388,
                    "acc_stderr": 0.026193923544454125,
                    "acc_norm": 0.835820895522388,
                    "acc_norm_stderr": 0.026193923544454125,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.0348735088019777,
                    "acc_norm": 0.86,
                    "acc_norm_stderr": 0.0348735088019777,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5240963855421686,
                    "acc_stderr": 0.03887971849597264,
                    "acc_norm": 0.5240963855421686,
                    "acc_norm_stderr": 0.03887971849597264,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8304093567251462,
                    "acc_stderr": 0.02878210810540171,
                    "acc_norm": 0.8304093567251462,
                    "acc_norm_stderr": 0.02878210810540171,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.5422276621787026,
                    "mc1_stderr": 0.017440965712482125,
                    "mc2": 0.701571568751529,
                    "mc2_stderr": 0.014731078396613723,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.8334648776637726,
                    "acc_stderr": 0.0104707964967811,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.7194844579226687,
                    "acc_stderr": 0.012374608490929553,
                    "timestamp": "2024-02-20T08-55-37.448857"
                }
            }
        }
    }
}