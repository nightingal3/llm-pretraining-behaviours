{
    "model_name": "Monero/WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b",
    "last_updated": "2023-07-19",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "em": 0.23898909395973153,
                    "em_stderr": 0.004367411698321815,
                    "f1": 0.33218645134228264,
                    "f1_stderr": 0.0042948501285767545,
                    "timestamp": "2023-09-16T20-24-34.064678"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.03411675511751327,
                    "acc_stderr": 0.005000212600773271,
                    "timestamp": "2023-09-16T20-24-34.064678"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7237569060773481,
                    "acc_stderr": 0.012566815015698158,
                    "timestamp": "2023-09-16T20-24-34.064678"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.5264505119453925,
                    "acc_stderr": 0.014590931358120169,
                    "acc_norm": 0.5554607508532423,
                    "acc_norm_stderr": 0.01452122640562708,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.59699263095001,
                    "acc_stderr": 0.004894997736719044,
                    "acc_norm": 0.8037243576976698,
                    "acc_norm_stderr": 0.003963677261161228,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542129,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542129,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.4444444444444444,
                    "acc_stderr": 0.04292596718256981,
                    "acc_norm": 0.4444444444444444,
                    "acc_norm_stderr": 0.04292596718256981,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.5789473684210527,
                    "acc_stderr": 0.04017901275981749,
                    "acc_norm": 0.5789473684210527,
                    "acc_norm_stderr": 0.04017901275981749,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.59,
                    "acc_stderr": 0.049431107042371025,
                    "acc_norm": 0.59,
                    "acc_norm_stderr": 0.049431107042371025,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.5849056603773585,
                    "acc_stderr": 0.03032594578928611,
                    "acc_norm": 0.5849056603773585,
                    "acc_norm_stderr": 0.03032594578928611,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.5625,
                    "acc_stderr": 0.04148415739394154,
                    "acc_norm": 0.5625,
                    "acc_norm_stderr": 0.04148415739394154,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.04878317312145633,
                    "acc_norm": 0.38,
                    "acc_norm_stderr": 0.04878317312145633,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.43,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.43,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695235,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.04760952285695235,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.45664739884393063,
                    "acc_stderr": 0.03798106566014498,
                    "acc_norm": 0.45664739884393063,
                    "acc_norm_stderr": 0.03798106566014498,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.27450980392156865,
                    "acc_stderr": 0.044405219061793275,
                    "acc_norm": 0.27450980392156865,
                    "acc_norm_stderr": 0.044405219061793275,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.61,
                    "acc_stderr": 0.04902071300001975,
                    "acc_norm": 0.61,
                    "acc_norm_stderr": 0.04902071300001975,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.4978723404255319,
                    "acc_stderr": 0.03268572658667492,
                    "acc_norm": 0.4978723404255319,
                    "acc_norm_stderr": 0.03268572658667492,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.3157894736842105,
                    "acc_stderr": 0.04372748290278006,
                    "acc_norm": 0.3157894736842105,
                    "acc_norm_stderr": 0.04372748290278006,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.4068965517241379,
                    "acc_stderr": 0.040937939812662374,
                    "acc_norm": 0.4068965517241379,
                    "acc_norm_stderr": 0.040937939812662374,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.34656084656084657,
                    "acc_stderr": 0.024508777521028435,
                    "acc_norm": 0.34656084656084657,
                    "acc_norm_stderr": 0.024508777521028435,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.3412698412698413,
                    "acc_stderr": 0.042407993275749255,
                    "acc_norm": 0.3412698412698413,
                    "acc_norm_stderr": 0.042407993275749255,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001975,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.04902071300001975,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.6193548387096774,
                    "acc_stderr": 0.027621717832907032,
                    "acc_norm": 0.6193548387096774,
                    "acc_norm_stderr": 0.027621717832907032,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.39901477832512317,
                    "acc_stderr": 0.03445487686264716,
                    "acc_norm": 0.39901477832512317,
                    "acc_norm_stderr": 0.03445487686264716,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.53,
                    "acc_stderr": 0.050161355804659205,
                    "acc_norm": 0.53,
                    "acc_norm_stderr": 0.050161355804659205,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.6303030303030303,
                    "acc_stderr": 0.03769430314512568,
                    "acc_norm": 0.6303030303030303,
                    "acc_norm_stderr": 0.03769430314512568,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.7171717171717171,
                    "acc_stderr": 0.0320877955878675,
                    "acc_norm": 0.7171717171717171,
                    "acc_norm_stderr": 0.0320877955878675,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.7564766839378239,
                    "acc_stderr": 0.030975436386845443,
                    "acc_norm": 0.7564766839378239,
                    "acc_norm_stderr": 0.030975436386845443,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.49230769230769234,
                    "acc_stderr": 0.025348006031534778,
                    "acc_norm": 0.49230769230769234,
                    "acc_norm_stderr": 0.025348006031534778,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.02730914058823018,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.02730914058823018,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.592436974789916,
                    "acc_stderr": 0.03191863374478465,
                    "acc_norm": 0.592436974789916,
                    "acc_norm_stderr": 0.03191863374478465,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31125827814569534,
                    "acc_stderr": 0.03780445850526732,
                    "acc_norm": 0.31125827814569534,
                    "acc_norm_stderr": 0.03780445850526732,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.7412844036697248,
                    "acc_stderr": 0.018776052319619634,
                    "acc_norm": 0.7412844036697248,
                    "acc_norm_stderr": 0.018776052319619634,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4027777777777778,
                    "acc_stderr": 0.033448873829978666,
                    "acc_norm": 0.4027777777777778,
                    "acc_norm_stderr": 0.033448873829978666,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.03039153369274154,
                    "acc_norm": 0.75,
                    "acc_norm_stderr": 0.03039153369274154,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.7426160337552743,
                    "acc_stderr": 0.0284588209914603,
                    "acc_norm": 0.7426160337552743,
                    "acc_norm_stderr": 0.0284588209914603,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.6322869955156951,
                    "acc_stderr": 0.032361983509282745,
                    "acc_norm": 0.6322869955156951,
                    "acc_norm_stderr": 0.032361983509282745,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.549618320610687,
                    "acc_stderr": 0.04363643698524779,
                    "acc_norm": 0.549618320610687,
                    "acc_norm_stderr": 0.04363643698524779,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.6776859504132231,
                    "acc_stderr": 0.042664163633521685,
                    "acc_norm": 0.6776859504132231,
                    "acc_norm_stderr": 0.042664163633521685,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.6574074074074074,
                    "acc_stderr": 0.0458790474130181,
                    "acc_norm": 0.6574074074074074,
                    "acc_norm_stderr": 0.0458790474130181,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.6134969325153374,
                    "acc_stderr": 0.038258255488486076,
                    "acc_norm": 0.6134969325153374,
                    "acc_norm_stderr": 0.038258255488486076,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.45535714285714285,
                    "acc_stderr": 0.047268355537191,
                    "acc_norm": 0.45535714285714285,
                    "acc_norm_stderr": 0.047268355537191,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.7281553398058253,
                    "acc_stderr": 0.044052680241409216,
                    "acc_norm": 0.7281553398058253,
                    "acc_norm_stderr": 0.044052680241409216,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8162393162393162,
                    "acc_stderr": 0.025372139671722933,
                    "acc_norm": 0.8162393162393162,
                    "acc_norm_stderr": 0.025372139671722933,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.67,
                    "acc_stderr": 0.047258156262526094,
                    "acc_norm": 0.67,
                    "acc_norm_stderr": 0.047258156262526094,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.7420178799489144,
                    "acc_stderr": 0.01564583018834895,
                    "acc_norm": 0.7420178799489144,
                    "acc_norm_stderr": 0.01564583018834895,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.5664739884393064,
                    "acc_stderr": 0.026680134761679217,
                    "acc_norm": 0.5664739884393064,
                    "acc_norm_stderr": 0.026680134761679217,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.29720670391061454,
                    "acc_stderr": 0.015285313353641597,
                    "acc_norm": 0.29720670391061454,
                    "acc_norm_stderr": 0.015285313353641597,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.5294117647058824,
                    "acc_stderr": 0.02858034106513829,
                    "acc_norm": 0.5294117647058824,
                    "acc_norm_stderr": 0.02858034106513829,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.6270096463022508,
                    "acc_stderr": 0.027466610213140105,
                    "acc_norm": 0.6270096463022508,
                    "acc_norm_stderr": 0.027466610213140105,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.6018518518518519,
                    "acc_stderr": 0.027237415094592488,
                    "acc_norm": 0.6018518518518519,
                    "acc_norm_stderr": 0.027237415094592488,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.40425531914893614,
                    "acc_stderr": 0.029275532159704725,
                    "acc_norm": 0.40425531914893614,
                    "acc_norm_stderr": 0.029275532159704725,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.3983050847457627,
                    "acc_stderr": 0.01250331056516625,
                    "acc_norm": 0.3983050847457627,
                    "acc_norm_stderr": 0.01250331056516625,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.5294117647058824,
                    "acc_stderr": 0.03032024326500413,
                    "acc_norm": 0.5294117647058824,
                    "acc_norm_stderr": 0.03032024326500413,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.5718954248366013,
                    "acc_stderr": 0.020017629214213097,
                    "acc_norm": 0.5718954248366013,
                    "acc_norm_stderr": 0.020017629214213097,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6636363636363637,
                    "acc_stderr": 0.04525393596302505,
                    "acc_norm": 0.6636363636363637,
                    "acc_norm_stderr": 0.04525393596302505,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.5551020408163265,
                    "acc_stderr": 0.031814251181977865,
                    "acc_norm": 0.5551020408163265,
                    "acc_norm_stderr": 0.031814251181977865,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.681592039800995,
                    "acc_stderr": 0.032941184790540944,
                    "acc_norm": 0.681592039800995,
                    "acc_norm_stderr": 0.032941184790540944,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.81,
                    "acc_stderr": 0.03942772444036623,
                    "acc_norm": 0.81,
                    "acc_norm_stderr": 0.03942772444036623,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.46987951807228917,
                    "acc_stderr": 0.03885425420866766,
                    "acc_norm": 0.46987951807228917,
                    "acc_norm_stderr": 0.03885425420866766,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.7426900584795322,
                    "acc_stderr": 0.03352799844161865,
                    "acc_norm": 0.7426900584795322,
                    "acc_norm_stderr": 0.03352799844161865,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.34516523867809057,
                    "mc1_stderr": 0.01664310331927494,
                    "mc2": 0.5130419747449761,
                    "mc2_stderr": 0.015788452657821323,
                    "timestamp": "2023-07-19T22-53-40.714431"
                }
            }
        }
    }
}