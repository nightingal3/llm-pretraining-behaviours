{
    "model_name": "mosaicml/mpt-7b-storywriter",
    "last_updated": "2023-09-22",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.17320819112627986,
                    "acc_stderr": 0.011058694183280334,
                    "acc_norm": 0.20392491467576793,
                    "acc_norm_stderr": 0.011774262478702249,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.2633937462656841,
                    "acc_stderr": 0.00439573949568858,
                    "acc_norm": 0.27454690300736906,
                    "acc_norm_stderr": 0.004453735900947829,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.32592592592592595,
                    "acc_stderr": 0.040491220417025055,
                    "acc_norm": 0.32592592592592595,
                    "acc_norm_stderr": 0.040491220417025055,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.18421052631578946,
                    "acc_stderr": 0.0315469804508223,
                    "acc_norm": 0.18421052631578946,
                    "acc_norm_stderr": 0.0315469804508223,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.22264150943396227,
                    "acc_stderr": 0.0256042334708991,
                    "acc_norm": 0.22264150943396227,
                    "acc_norm_stderr": 0.0256042334708991,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.034765901043041336,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.034765901043041336,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542127,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.15,
                    "acc_stderr": 0.03588702812826372,
                    "acc_norm": 0.15,
                    "acc_norm_stderr": 0.03588702812826372,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036846,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036846,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2138728323699422,
                    "acc_stderr": 0.03126511206173043,
                    "acc_norm": 0.2138728323699422,
                    "acc_norm_stderr": 0.03126511206173043,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.28431372549019607,
                    "acc_stderr": 0.04488482852329017,
                    "acc_norm": 0.28431372549019607,
                    "acc_norm_stderr": 0.04488482852329017,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816505,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816505,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2297872340425532,
                    "acc_stderr": 0.027501752944412424,
                    "acc_norm": 0.2297872340425532,
                    "acc_norm_stderr": 0.027501752944412424,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.22807017543859648,
                    "acc_stderr": 0.03947152782669415,
                    "acc_norm": 0.22807017543859648,
                    "acc_norm_stderr": 0.03947152782669415,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2620689655172414,
                    "acc_stderr": 0.03664666337225257,
                    "acc_norm": 0.2620689655172414,
                    "acc_norm_stderr": 0.03664666337225257,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.022569897074918424,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.022569897074918424,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.20634920634920634,
                    "acc_stderr": 0.036196045241242515,
                    "acc_norm": 0.20634920634920634,
                    "acc_norm_stderr": 0.036196045241242515,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.3096774193548387,
                    "acc_stderr": 0.026302774983517418,
                    "acc_norm": 0.3096774193548387,
                    "acc_norm_stderr": 0.026302774983517418,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2512315270935961,
                    "acc_stderr": 0.030516530732694436,
                    "acc_norm": 0.2512315270935961,
                    "acc_norm_stderr": 0.030516530732694436,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036845,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036845,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.23636363636363636,
                    "acc_stderr": 0.03317505930009179,
                    "acc_norm": 0.23636363636363636,
                    "acc_norm_stderr": 0.03317505930009179,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.2474747474747475,
                    "acc_stderr": 0.030746300742124488,
                    "acc_norm": 0.2474747474747475,
                    "acc_norm_stderr": 0.030746300742124488,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.22797927461139897,
                    "acc_stderr": 0.030276909945178256,
                    "acc_norm": 0.22797927461139897,
                    "acc_norm_stderr": 0.030276909945178256,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.20512820512820512,
                    "acc_stderr": 0.02047323317355198,
                    "acc_norm": 0.20512820512820512,
                    "acc_norm_stderr": 0.02047323317355198,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.026067159222275788,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.026067159222275788,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.23109243697478993,
                    "acc_stderr": 0.027381406927868966,
                    "acc_norm": 0.23109243697478993,
                    "acc_norm_stderr": 0.027381406927868966,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2052980132450331,
                    "acc_stderr": 0.03297986648473835,
                    "acc_norm": 0.2052980132450331,
                    "acc_norm_stderr": 0.03297986648473835,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.20917431192660552,
                    "acc_stderr": 0.017437937173343226,
                    "acc_norm": 0.20917431192660552,
                    "acc_norm_stderr": 0.017437937173343226,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4398148148148148,
                    "acc_stderr": 0.03385177976044811,
                    "acc_norm": 0.4398148148148148,
                    "acc_norm_stderr": 0.03385177976044811,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.24509803921568626,
                    "acc_stderr": 0.030190282453501947,
                    "acc_norm": 0.24509803921568626,
                    "acc_norm_stderr": 0.030190282453501947,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.23628691983122363,
                    "acc_stderr": 0.027652153144159263,
                    "acc_norm": 0.23628691983122363,
                    "acc_norm_stderr": 0.027652153144159263,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.2242152466367713,
                    "acc_stderr": 0.027991534258519527,
                    "acc_norm": 0.2242152466367713,
                    "acc_norm_stderr": 0.027991534258519527,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.25190839694656486,
                    "acc_stderr": 0.03807387116306086,
                    "acc_norm": 0.25190839694656486,
                    "acc_norm_stderr": 0.03807387116306086,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.24793388429752067,
                    "acc_stderr": 0.03941897526516303,
                    "acc_norm": 0.24793388429752067,
                    "acc_norm_stderr": 0.03941897526516303,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.21296296296296297,
                    "acc_stderr": 0.0395783547198098,
                    "acc_norm": 0.21296296296296297,
                    "acc_norm_stderr": 0.0395783547198098,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.25153374233128833,
                    "acc_stderr": 0.034089978868575295,
                    "acc_norm": 0.25153374233128833,
                    "acc_norm_stderr": 0.034089978868575295,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.19642857142857142,
                    "acc_stderr": 0.03770970049347018,
                    "acc_norm": 0.19642857142857142,
                    "acc_norm_stderr": 0.03770970049347018,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.1941747572815534,
                    "acc_stderr": 0.03916667762822585,
                    "acc_norm": 0.1941747572815534,
                    "acc_norm_stderr": 0.03916667762822585,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.23504273504273504,
                    "acc_stderr": 0.02777883590493543,
                    "acc_norm": 0.23504273504273504,
                    "acc_norm_stderr": 0.02777883590493543,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.01567100600933957,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.01567100600933957,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.21676300578034682,
                    "acc_stderr": 0.022183477668412856,
                    "acc_norm": 0.21676300578034682,
                    "acc_norm_stderr": 0.022183477668412856,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.23798882681564246,
                    "acc_stderr": 0.014242630070574915,
                    "acc_norm": 0.23798882681564246,
                    "acc_norm_stderr": 0.014242630070574915,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.02392915551735129,
                    "acc_norm": 0.22549019607843138,
                    "acc_norm_stderr": 0.02392915551735129,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.22508038585209003,
                    "acc_stderr": 0.023720088516179027,
                    "acc_norm": 0.22508038585209003,
                    "acc_norm_stderr": 0.023720088516179027,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.02409347123262133,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.02409347123262133,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2375886524822695,
                    "acc_stderr": 0.025389512552729903,
                    "acc_norm": 0.2375886524822695,
                    "acc_norm_stderr": 0.025389512552729903,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.24967405475880053,
                    "acc_stderr": 0.011054538377832324,
                    "acc_norm": 0.24967405475880053,
                    "acc_norm_stderr": 0.011054538377832324,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.24632352941176472,
                    "acc_stderr": 0.02617343857052,
                    "acc_norm": 0.24632352941176472,
                    "acc_norm_stderr": 0.02617343857052,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.25980392156862747,
                    "acc_stderr": 0.01774089950917779,
                    "acc_norm": 0.25980392156862747,
                    "acc_norm_stderr": 0.01774089950917779,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.04172343038705383,
                    "acc_norm": 0.2545454545454545,
                    "acc_norm_stderr": 0.04172343038705383,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.2653061224489796,
                    "acc_stderr": 0.028263889943784593,
                    "acc_norm": 0.2653061224489796,
                    "acc_norm_stderr": 0.028263889943784593,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.27860696517412936,
                    "acc_stderr": 0.031700561834973086,
                    "acc_norm": 0.27860696517412936,
                    "acc_norm_stderr": 0.031700561834973086,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.2469879518072289,
                    "acc_stderr": 0.03357351982064536,
                    "acc_norm": 0.2469879518072289,
                    "acc_norm_stderr": 0.03357351982064536,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.2807017543859649,
                    "acc_stderr": 0.03446296217088427,
                    "acc_norm": 0.2807017543859649,
                    "acc_norm_stderr": 0.03446296217088427,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.23623011015911874,
                    "mc1_stderr": 0.014869755015871091,
                    "mc2": 0.48439801230953305,
                    "mc2_stderr": 0.016279319966019105,
                    "timestamp": "2023-10-03T22-53-23.133729"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.0006291946308724832,
                    "acc_stderr": 0.00025680027497237983,
                    "f1": 0.0032026006711409396,
                    "f1_stderr": 0.0005040610386397096,
                    "timestamp": "2023-10-16T08-53-05.263222"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2023-10-16T08-53-05.263222"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5114443567482242,
                    "acc_stderr": 0.014048804199859325,
                    "timestamp": "2023-10-16T08-53-05.263222"
                }
            }
        }
    }
}