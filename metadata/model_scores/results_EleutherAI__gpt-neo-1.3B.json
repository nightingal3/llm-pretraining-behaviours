{
    "model_name": "EleutherAI/gpt-neo-1.3B",
    "last_updated": "2024-12-04 11:23:47.036521",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.01282051282051282,
                "exact_match_stderr": 0.004818950982487613,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.01951779563719862,
                "exact_match_stderr": 0.004690029935284562,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.003703703703703704,
                "exact_match_stderr": 0.002616483457231187,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.016611295681063124,
                "exact_match_stderr": 0.004255602872194619,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "minerva_math_geometry": {
                "exact_match": 0.020876826722338204,
                "exact_match_stderr": 0.0065393857958139425,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0189873417721519,
                "exact_match_stderr": 0.006275362513989611,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "minerva_math_algebra": {
                "exact_match": 0.017691659646166806,
                "exact_match_stderr": 0.003827946497642315,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_2ds": {
                "acc": 0.0125,
                "acc_stderr": 0.00248494717876267,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_1dc": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_2dm": {
                "acc": 0.022,
                "acc_stderr": 0.0032807593162018913,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "arithmetic_2da": {
                "acc": 0.0075,
                "acc_stderr": 0.0019296986470519835,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "gsm8k_cot": {
                "exact_match": 0.014404852160727824,
                "exact_match_stderr": 0.0032820559171369834,
                "timestamp": "2024-06-05T17-14-37.407106"
            },
            "gsm8k": {
                "exact_match": 0.017437452615617893,
                "exact_match_stderr": 0.003605486867998249,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "anli_r2": {
                "brier_score": 0.8796036508016054,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "anli_r3": {
                "brier_score": 0.8324430010683331,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "anli_r1": {
                "brier_score": 0.917864024592765,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_eu": {
                "brier_score": 1.2200574234266282,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_vi": {
                "brier_score": 0.7903674559535868,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_ru": {
                "brier_score": 0.7566305493567674,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_zh": {
                "brier_score": 0.9856404379148058,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_tr": {
                "brier_score": 0.9169373031276504,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_fr": {
                "brier_score": 0.856853328987711,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_en": {
                "brier_score": 0.6629460290225301,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_ur": {
                "brier_score": 0.9441652578494679,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_ar": {
                "brier_score": 1.2124809306829618,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_de": {
                "brier_score": 0.8893252434461513,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_hi": {
                "brier_score": 0.8044741628382988,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_es": {
                "brier_score": 0.9012945391331052,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_bg": {
                "brier_score": 0.8686133483895749,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_sw": {
                "brier_score": 0.9058789893603033,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_el": {
                "brier_score": 1.0604737322480473,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "xnli_th": {
                "brier_score": 0.7762030553378716,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "logiqa2": {
                "brier_score": 1.225701255044906,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "mathqa": {
                "brier_score": 0.988187749870061,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-05T20-17-24.884212"
            },
            "lambada_standard": {
                "perplexity": 14.582486374632817,
                "perplexity_stderr": 0.46752693319723554,
                "acc": 0.45294003493110807,
                "acc_stderr": 0.0069350547518701846,
                "timestamp": "2024-06-05T20-30-50.006352"
            },
            "lambada_openai": {
                "perplexity": 7.497819400340795,
                "perplexity_stderr": 0.19958274093516393,
                "acc": 0.5720939258684261,
                "acc_stderr": 0.006893185516930775,
                "timestamp": "2024-06-05T20-30-50.006352"
            },
            "mmlu_world_religions": {
                "acc": 0.3216374269005848,
                "acc_stderr": 0.03582529442573122,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_formal_logic": {
                "acc": 0.25396825396825395,
                "acc_stderr": 0.038932596106046706,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_prehistory": {
                "acc": 0.23148148148148148,
                "acc_stderr": 0.023468429832451166,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23910614525139665,
                "acc_stderr": 0.014265554192331154,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.28270042194092826,
                "acc_stderr": 0.02931281415395592,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2514450867052023,
                "acc_stderr": 0.023357365785874037,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_professional_law": {
                "acc": 0.2516297262059974,
                "acc_stderr": 0.011083276280441907,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2392638036809816,
                "acc_stderr": 0.033519538795212696,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.23039215686274508,
                "acc_stderr": 0.029554292605695066,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_philosophy": {
                "acc": 0.1864951768488746,
                "acc_stderr": 0.02212243977248077,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_jurisprudence": {
                "acc": 0.25,
                "acc_stderr": 0.04186091791394607,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_international_law": {
                "acc": 0.30578512396694213,
                "acc_stderr": 0.04205953933884122,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03225078108306289,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.20725388601036268,
                "acc_stderr": 0.029252823291803644,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23109243697478993,
                "acc_stderr": 0.027381406927868963,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_geography": {
                "acc": 0.17676767676767677,
                "acc_stderr": 0.027178752639044915,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.1944954128440367,
                "acc_stderr": 0.016970289090458026,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_sociology": {
                "acc": 0.24378109452736318,
                "acc_stderr": 0.030360490154014652,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.3128205128205128,
                "acc_stderr": 0.023507579020645365,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_security_studies": {
                "acc": 0.2163265306122449,
                "acc_stderr": 0.026358916334904028,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_professional_psychology": {
                "acc": 0.24836601307189543,
                "acc_stderr": 0.017479487001364764,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.03999423879281334,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_miscellaneous": {
                "acc": 0.22349936143039592,
                "acc_stderr": 0.014897235229450708,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_marketing": {
                "acc": 0.2948717948717949,
                "acc_stderr": 0.029872577708891172,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_nutrition": {
                "acc": 0.22875816993464052,
                "acc_stderr": 0.024051029739912258,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_medical_genetics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_human_aging": {
                "acc": 0.3094170403587444,
                "acc_stderr": 0.031024411740572206,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_professional_medicine": {
                "acc": 0.16544117647058823,
                "acc_stderr": 0.02257177102549476,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_college_medicine": {
                "acc": 0.2023121387283237,
                "acc_stderr": 0.03063114553919882,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_business_ethics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2679245283018868,
                "acc_stderr": 0.02725726032249485,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_global_facts": {
                "acc": 0.19,
                "acc_stderr": 0.039427724440366234,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_virology": {
                "acc": 0.27710843373493976,
                "acc_stderr": 0.034843315926805875,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2375886524822695,
                "acc_stderr": 0.025389512552729903,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.040925639582376556,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2582781456953642,
                "acc_stderr": 0.035737053147634576,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2,
                "acc_stderr": 0.022755204959542936,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_college_biology": {
                "acc": 0.2569444444444444,
                "acc_stderr": 0.03653946969442099,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_anatomy": {
                "acc": 0.14814814814814814,
                "acc_stderr": 0.03068864761035268,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_college_chemistry": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542128,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_college_computer_science": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123398,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_college_mathematics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720683,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2851063829787234,
                "acc_stderr": 0.029513196625539355,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_machine_learning": {
                "acc": 0.30357142857142855,
                "acc_stderr": 0.04364226155841044,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.19704433497536947,
                "acc_stderr": 0.02798672466673622,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4027777777777778,
                "acc_stderr": 0.033448873829978666,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25396825396825395,
                "acc_stderr": 0.02241804289111394,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2827586206896552,
                "acc_stderr": 0.037528339580033376,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2518518518518518,
                "acc_stderr": 0.026466117538959905,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "arc_challenge": {
                "acc": 0.27559726962457337,
                "acc_stderr": 0.01305716965576184,
                "acc_norm": 0.30887372013651876,
                "acc_norm_stderr": 0.013501770929344004,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "hellaswag": {
                "acc": 0.38787094204341765,
                "acc_stderr": 0.004862690594815702,
                "acc_norm": 0.4863572993427604,
                "acc_norm_stderr": 0.0049879236366285554,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "truthfulqa_mc2": {
                "acc": 0.3961377966394332,
                "acc_stderr": 0.014266511238016675,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "truthfulqa_gen": {
                "bleu_max": 22.087978514504556,
                "bleu_max_stderr": 0.710438739697618,
                "bleu_acc": 0.32313341493268055,
                "bleu_acc_stderr": 0.016371836286454607,
                "bleu_diff": -3.357699260688641,
                "bleu_diff_stderr": 0.7725285332901728,
                "rouge1_max": 46.248363981939576,
                "rouge1_max_stderr": 0.870416787403499,
                "rouge1_acc": 0.29253365973072215,
                "rouge1_acc_stderr": 0.015925597445286165,
                "rouge1_diff": -4.926670357978892,
                "rouge1_diff_stderr": 0.9736757701753777,
                "rouge2_max": 29.894549790277882,
                "rouge2_max_stderr": 0.991318388653234,
                "rouge2_acc": 0.2423500611995104,
                "rouge2_acc_stderr": 0.015000674373570342,
                "rouge2_diff": -5.599665381566807,
                "rouge2_diff_stderr": 1.0842752003655973,
                "rougeL_max": 43.635205520666176,
                "rougeL_max_stderr": 0.8764442787347267,
                "rougeL_acc": 0.2839657282741738,
                "rougeL_acc_stderr": 0.015785370858396736,
                "rougeL_diff": -4.913623343185734,
                "rougeL_diff_stderr": 0.9748055918882317,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "truthfulqa_mc1": {
                "acc": 0.23133414932680538,
                "acc_stderr": 0.014761945174862665,
                "timestamp": "2024-11-21T07-33-41.442625"
            },
            "winogrande": {
                "acc": 0.5564325177584846,
                "acc_stderr": 0.0139626949076204,
                "timestamp": "2024-11-21T07-33-41.442625"
            }
        }
    }
}