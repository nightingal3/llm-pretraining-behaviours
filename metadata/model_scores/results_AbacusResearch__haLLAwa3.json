{
    "model_name": "AbacusResearch/haLLAwa3",
    "last_updated": "2024-12-04 11:24:18.589864",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0695970695970696,
                "exact_match_stderr": 0.010900157182653379,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.38920780711825487,
                "exact_match_stderr": 0.016530191465345578,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.11296296296296296,
                "exact_match_stderr": 0.013634666880074291,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.08305647840531562,
                "exact_match_stderr": 0.009188714988985457,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "minerva_math_geometry": {
                "exact_match": 0.162839248434238,
                "exact_match_stderr": 0.016887681356465453,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.17721518987341772,
                "exact_match_stderr": 0.017557514449364223,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "minerva_math_algebra": {
                "exact_match": 0.2923336141533277,
                "exact_match_stderr": 0.013207217104051101,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_3da": {
                "acc": 0.983,
                "acc_stderr": 0.002891311093590562,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_3ds": {
                "acc": 0.978,
                "acc_stderr": 0.0032807593162018905,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_4da": {
                "acc": 0.932,
                "acc_stderr": 0.0056306173663253235,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_2ds": {
                "acc": 0.993,
                "acc_stderr": 0.0018647355360237657,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_5ds": {
                "acc": 0.883,
                "acc_stderr": 0.0071889735477559495,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_5da": {
                "acc": 0.906,
                "acc_stderr": 0.006527120471603569,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_1dc": {
                "acc": 0.7045,
                "acc_stderr": 0.010204996128024123,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_4ds": {
                "acc": 0.934,
                "acc_stderr": 0.00555314493862308,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_2dm": {
                "acc": 0.639,
                "acc_stderr": 0.010742308811391417,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "arithmetic_2da": {
                "acc": 0.9965,
                "acc_stderr": 0.0013208888574315811,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "gsm8k_cot": {
                "exact_match": 0.7134192570128886,
                "exact_match_stderr": 0.01245484166833771,
                "timestamp": "2024-06-10T12-03-36.976432"
            },
            "gsm8k": {
                "exact_match": 0.6823351023502654,
                "exact_match_stderr": 0.012824066621488845,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "anli_r2": {
                "brier_score": 0.7822788074399861,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "anli_r3": {
                "brier_score": 0.8264083512806276,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "anli_r1": {
                "brier_score": 0.7075167500716043,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_eu": {
                "brier_score": 1.1457721381774524,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_vi": {
                "brier_score": 0.9869158803328254,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_ru": {
                "brier_score": 0.8716743740754499,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_zh": {
                "brier_score": 1.090951879740664,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_tr": {
                "brier_score": 1.0701876481592625,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_fr": {
                "brier_score": 0.8327048649607862,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_en": {
                "brier_score": 0.712823188151288,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_ur": {
                "brier_score": 1.239951455555465,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_ar": {
                "brier_score": 1.2690967154108113,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_de": {
                "brier_score": 0.9143433937106298,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_hi": {
                "brier_score": 0.9583571701761374,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_es": {
                "brier_score": 0.9546231307238467,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_bg": {
                "brier_score": 0.9223977711612795,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_sw": {
                "brier_score": 1.0030011650642259,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_el": {
                "brier_score": 0.9462798720866608,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "xnli_th": {
                "brier_score": 1.0379297621590724,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "logiqa2": {
                "brier_score": 0.9965660380454943,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "mathqa": {
                "brier_score": 0.9484637194920996,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T12-14-41.839035"
            },
            "lambada_standard": {
                "perplexity": 3.8098181547092502,
                "perplexity_stderr": 0.10283604883136323,
                "acc": 0.6586454492528624,
                "acc_stderr": 0.0066060334598652065,
                "timestamp": "2024-06-10T12-16-03.776971"
            },
            "lambada_openai": {
                "perplexity": 3.1571427801436447,
                "perplexity_stderr": 0.07480572143016524,
                "acc": 0.7143411604890355,
                "acc_stderr": 0.0062934493900561085,
                "timestamp": "2024-06-10T12-16-03.776971"
            },
            "mmlu_world_religions": {
                "acc": 0.8362573099415205,
                "acc_stderr": 0.028380919596145866,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_formal_logic": {
                "acc": 0.47619047619047616,
                "acc_stderr": 0.04467062628403273,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_prehistory": {
                "acc": 0.7222222222222222,
                "acc_stderr": 0.024922001168886324,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.4692737430167598,
                "acc_stderr": 0.01669089616194438,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.8143459915611815,
                "acc_stderr": 0.025310495376944856,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_moral_disputes": {
                "acc": 0.7398843930635838,
                "acc_stderr": 0.023618678310069367,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_professional_law": {
                "acc": 0.4667535853976532,
                "acc_stderr": 0.01274197433389723,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.7668711656441718,
                "acc_stderr": 0.0332201579577674,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.8333333333333334,
                "acc_stderr": 0.026156867523931038,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_philosophy": {
                "acc": 0.7106109324758842,
                "acc_stderr": 0.025755865922632935,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_jurisprudence": {
                "acc": 0.8425925925925926,
                "acc_stderr": 0.03520703990517965,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_international_law": {
                "acc": 0.768595041322314,
                "acc_stderr": 0.03849856098794088,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.7696969696969697,
                "acc_stderr": 0.03287666758603489,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.8808290155440415,
                "acc_stderr": 0.023381935348121455,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.6596638655462185,
                "acc_stderr": 0.030778057422931673,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_geography": {
                "acc": 0.7929292929292929,
                "acc_stderr": 0.02886977846026704,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.8385321100917431,
                "acc_stderr": 0.015776239256163244,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_public_relations": {
                "acc": 0.6272727272727273,
                "acc_stderr": 0.04631381319425465,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.85,
                "acc_stderr": 0.0358870281282637,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_sociology": {
                "acc": 0.8507462686567164,
                "acc_stderr": 0.025196929874827072,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.6717948717948717,
                "acc_stderr": 0.023807633198657262,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_security_studies": {
                "acc": 0.726530612244898,
                "acc_stderr": 0.028535560337128448,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_professional_psychology": {
                "acc": 0.6486928104575164,
                "acc_stderr": 0.01931267606578655,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_human_sexuality": {
                "acc": 0.7862595419847328,
                "acc_stderr": 0.0359546161177469,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_econometrics": {
                "acc": 0.45614035087719296,
                "acc_stderr": 0.046854730419077895,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_miscellaneous": {
                "acc": 0.8263090676883781,
                "acc_stderr": 0.013547415658662269,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_marketing": {
                "acc": 0.8803418803418803,
                "acc_stderr": 0.02126271940040698,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_management": {
                "acc": 0.8155339805825242,
                "acc_stderr": 0.03840423627288276,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_nutrition": {
                "acc": 0.7483660130718954,
                "acc_stderr": 0.024848018263875195,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_medical_genetics": {
                "acc": 0.68,
                "acc_stderr": 0.046882617226215034,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_human_aging": {
                "acc": 0.6905829596412556,
                "acc_stderr": 0.03102441174057221,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_professional_medicine": {
                "acc": 0.6801470588235294,
                "acc_stderr": 0.02833295951403121,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_college_medicine": {
                "acc": 0.6358381502890174,
                "acc_stderr": 0.03669072477416905,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_business_ethics": {
                "acc": 0.6,
                "acc_stderr": 0.049236596391733084,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.7018867924528301,
                "acc_stderr": 0.02815283794249386,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_global_facts": {
                "acc": 0.35,
                "acc_stderr": 0.04793724854411019,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_virology": {
                "acc": 0.536144578313253,
                "acc_stderr": 0.038823108508905954,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_professional_accounting": {
                "acc": 0.46808510638297873,
                "acc_stderr": 0.029766675075873866,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_college_physics": {
                "acc": 0.38235294117647056,
                "acc_stderr": 0.04835503696107224,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_physics": {
                "acc": 0.31788079470198677,
                "acc_stderr": 0.038020397601079024,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_biology": {
                "acc": 0.7612903225806451,
                "acc_stderr": 0.02425107126220884,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_college_biology": {
                "acc": 0.7361111111111112,
                "acc_stderr": 0.03685651095897532,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_anatomy": {
                "acc": 0.6148148148148148,
                "acc_stderr": 0.04203921040156279,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_college_chemistry": {
                "acc": 0.46,
                "acc_stderr": 0.05009082659620333,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_computer_security": {
                "acc": 0.79,
                "acc_stderr": 0.04093601807403326,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_college_computer_science": {
                "acc": 0.55,
                "acc_stderr": 0.05,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_astronomy": {
                "acc": 0.6973684210526315,
                "acc_stderr": 0.037385206761196665,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_college_mathematics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.574468085106383,
                "acc_stderr": 0.03232146916224468,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695235,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.68,
                "acc_stderr": 0.04688261722621505,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_machine_learning": {
                "acc": 0.45535714285714285,
                "acc_stderr": 0.04726835553719099,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.5024630541871922,
                "acc_stderr": 0.03517945038691063,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.5046296296296297,
                "acc_stderr": 0.03409825519163572,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.41005291005291006,
                "acc_stderr": 0.02533120243894443,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.5655172413793104,
                "acc_stderr": 0.041307408795554966,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.36666666666666664,
                "acc_stderr": 0.029381620726465073,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "arc_challenge": {
                "acc": 0.6399317406143344,
                "acc_stderr": 0.01402751681458519,
                "acc_norm": 0.6800341296928327,
                "acc_norm_stderr": 0.013631345807016193,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "hellaswag": {
                "acc": 0.7013543118900617,
                "acc_stderr": 0.004567287775700555,
                "acc_norm": 0.8702449711212906,
                "acc_norm_stderr": 0.003353469625027663,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "truthfulqa_mc2": {
                "acc": 0.6405912523524916,
                "acc_stderr": 0.015307419874400935,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "truthfulqa_gen": {
                "bleu_max": 26.60703488440451,
                "bleu_max_stderr": 0.8059789583751055,
                "bleu_acc": 0.5458996328029376,
                "bleu_acc_stderr": 0.01742959309132352,
                "bleu_diff": 8.310410176146629,
                "bleu_diff_stderr": 0.8246277137737857,
                "rouge1_max": 53.367218868579776,
                "rouge1_max_stderr": 0.8757933437838162,
                "rouge1_acc": 0.5667074663402693,
                "rouge1_acc_stderr": 0.01734702445010748,
                "rouge1_diff": 11.738773993934803,
                "rouge1_diff_stderr": 1.1820094019339218,
                "rouge2_max": 40.14406041744083,
                "rouge2_max_stderr": 1.0363145440664192,
                "rouge2_acc": 0.5226438188494492,
                "rouge2_acc_stderr": 0.01748554225848964,
                "rouge2_diff": 11.844606657455826,
                "rouge2_diff_stderr": 1.2675724960588215,
                "rougeL_max": 50.376439907152886,
                "rougeL_max_stderr": 0.9112608531974777,
                "rougeL_acc": 0.5471236230110159,
                "rougeL_acc_stderr": 0.01742558984831402,
                "rougeL_diff": 11.36007163528697,
                "rougeL_diff_stderr": 1.1945981073742722,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "truthfulqa_mc1": {
                "acc": 0.46511627906976744,
                "acc_stderr": 0.017460849975873965,
                "timestamp": "2024-11-21T20-42-08.958721"
            },
            "winogrande": {
                "acc": 0.7916337805840569,
                "acc_stderr": 0.011414554399987729,
                "timestamp": "2024-11-21T20-42-08.958721"
            }
        }
    }
}