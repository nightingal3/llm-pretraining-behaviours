{
    "model_name": "facebook/xglm-564M",
    "last_updated": "2024-12-04 11:22:45.502176",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.001148105625717566,
                "exact_match_stderr": 0.0011481056257175673,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_algebra": {
                "exact_match": 0.0008424599831508003,
                "exact_match_stderr": 0.0008424599831507863,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_3ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000116,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_2ds": {
                "acc": 0.008,
                "acc_stderr": 0.001992482118488464,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_1dc": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339569,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_2dm": {
                "acc": 0.0185,
                "acc_stderr": 0.0030138707185866456,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_2da": {
                "acc": 0.0065,
                "acc_stderr": 0.0017973564602277766,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "gsm8k_cot": {
                "exact_match": 0.02122820318423048,
                "exact_match_stderr": 0.003970449129848635,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "gsm8k": {
                "exact_match": 0.016679302501895376,
                "exact_match_stderr": 0.0035275958887224573,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "anli_r2": {
                "brier_score": 0.8949833517502548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "anli_r3": {
                "brier_score": 0.8360668742052519,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "anli_r1": {
                "brier_score": 0.8884687642270159,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_eu": {
                "brier_score": 0.7371713039069413,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_vi": {
                "brier_score": 0.8654369273137161,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_ru": {
                "brier_score": 0.7606713225425753,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_zh": {
                "brier_score": 1.0486949265688723,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_tr": {
                "brier_score": 0.8214109452628465,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_fr": {
                "brier_score": 0.7815638841837665,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_en": {
                "brier_score": 0.7112428085311449,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_ur": {
                "brier_score": 1.062454393810576,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_ar": {
                "brier_score": 1.296895724941598,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_de": {
                "brier_score": 0.8351648045281843,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_hi": {
                "brier_score": 0.8394046981053793,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_es": {
                "brier_score": 0.8566611033916195,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_bg": {
                "brier_score": 0.7676392908318753,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_sw": {
                "brier_score": 0.8344826636346988,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_el": {
                "brier_score": 0.8773823206071238,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_th": {
                "brier_score": 0.8581311641212404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "logiqa2": {
                "brier_score": 1.1939318152210763,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "mathqa": {
                "brier_score": 1.0334681136329835,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "lambada_standard": {
                "perplexity": 42.939212075145306,
                "perplexity_stderr": 1.641006871749824,
                "acc": 0.3291286629148069,
                "acc_stderr": 0.0065465809755531025,
                "timestamp": "2024-06-09T15-17-16.795144"
            },
            "lambada_openai": {
                "perplexity": 28.56367062584693,
                "perplexity_stderr": 1.0264515146295097,
                "acc": 0.3584319813700757,
                "acc_stderr": 0.006680928173680378,
                "timestamp": "2024-06-09T15-17-16.795144"
            },
            "mmlu_world_religions": {
                "acc": 0.22807017543859648,
                "acc_stderr": 0.032180937956023566,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_formal_logic": {
                "acc": 0.14285714285714285,
                "acc_stderr": 0.03129843185743809,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_prehistory": {
                "acc": 0.21604938271604937,
                "acc_stderr": 0.02289916291844581,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2424581005586592,
                "acc_stderr": 0.014333522059217892,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2489451476793249,
                "acc_stderr": 0.028146970599422644,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_moral_disputes": {
                "acc": 0.24855491329479767,
                "acc_stderr": 0.023267528432100174,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_professional_law": {
                "acc": 0.2457627118644068,
                "acc_stderr": 0.01099615663514269,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.26380368098159507,
                "acc_stderr": 0.03462419931615624,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.27941176470588236,
                "acc_stderr": 0.031493281045079556,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_philosophy": {
                "acc": 0.1832797427652733,
                "acc_stderr": 0.021974198848265812,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_jurisprudence": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.04236511258094631,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_international_law": {
                "acc": 0.32231404958677684,
                "acc_stderr": 0.042664163633521664,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.03453131801885415,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.2694300518134715,
                "acc_stderr": 0.03201867122877794,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.02755361446786379,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_geography": {
                "acc": 0.18686868686868688,
                "acc_stderr": 0.02777253333421898,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.1926605504587156,
                "acc_stderr": 0.016909276884936087,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816507,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_sociology": {
                "acc": 0.22885572139303484,
                "acc_stderr": 0.029705284056772436,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2076923076923077,
                "acc_stderr": 0.0205675395672468,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_security_studies": {
                "acc": 0.2612244897959184,
                "acc_stderr": 0.028123429335142787,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25,
                "acc_stderr": 0.01751781884501444,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_econometrics": {
                "acc": 0.2719298245614035,
                "acc_stderr": 0.04185774424022056,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_miscellaneous": {
                "acc": 0.23754789272030652,
                "acc_stderr": 0.015218733046150191,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_marketing": {
                "acc": 0.2948717948717949,
                "acc_stderr": 0.02987257770889115,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_management": {
                "acc": 0.18446601941747573,
                "acc_stderr": 0.03840423627288276,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_nutrition": {
                "acc": 0.21895424836601307,
                "acc_stderr": 0.02367908986180772,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_human_aging": {
                "acc": 0.33183856502242154,
                "acc_stderr": 0.03160295143776679,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_professional_medicine": {
                "acc": 0.44485294117647056,
                "acc_stderr": 0.030187532060329383,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_college_medicine": {
                "acc": 0.2138728323699422,
                "acc_stderr": 0.03126511206173042,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_business_ethics": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421296,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2037735849056604,
                "acc_stderr": 0.024790784501775406,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653694,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370519,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2801418439716312,
                "acc_stderr": 0.026789172351140245,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_college_physics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.04220773659171453,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2052980132450331,
                "acc_stderr": 0.03297986648473835,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2709677419354839,
                "acc_stderr": 0.02528441611490016,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_college_biology": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.03476590104304134,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_anatomy": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.03944624162501116,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_college_chemistry": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036845,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_computer_security": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_college_computer_science": {
                "acc": 0.16,
                "acc_stderr": 0.036845294917747094,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123387,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_college_mathematics": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909282,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.30638297872340425,
                "acc_stderr": 0.03013590647851756,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.2,
                "acc_stderr": 0.040201512610368445,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.31,
                "acc_stderr": 0.046482319871173156,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_machine_learning": {
                "acc": 0.3392857142857143,
                "acc_stderr": 0.04493949068613539,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2955665024630542,
                "acc_stderr": 0.032104944337514575,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4583333333333333,
                "acc_stderr": 0.03398110890294636,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2566137566137566,
                "acc_stderr": 0.022494510767503154,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.25517241379310346,
                "acc_stderr": 0.03632984052707842,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.23703703703703705,
                "acc_stderr": 0.02592887613276614,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "arc_challenge": {
                "acc": 0.20648464163822525,
                "acc_stderr": 0.011828865619002316,
                "acc_norm": 0.25426621160409557,
                "acc_norm_stderr": 0.012724999945157732,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "hellaswag": {
                "acc": 0.30481975702051384,
                "acc_stderr": 0.004593902601979336,
                "acc_norm": 0.34933280223063135,
                "acc_norm_stderr": 0.004757849023411965,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "truthfulqa_mc2": {
                "acc": 0.4037316135738844,
                "acc_stderr": 0.014865718962222852,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "truthfulqa_gen": {
                "bleu_max": 1.3466045659305554,
                "bleu_max_stderr": 0.06990039606064374,
                "bleu_acc": 0.32802937576499386,
                "bleu_acc_stderr": 0.016435632932815022,
                "bleu_diff": -0.24132198144345063,
                "bleu_diff_stderr": 0.04226398695222224,
                "rouge1_max": 6.215723953891085,
                "rouge1_max_stderr": 0.1887917253922373,
                "rouge1_acc": 0.3561811505507956,
                "rouge1_acc_stderr": 0.016763790728446325,
                "rouge1_diff": -0.5530378301879756,
                "rouge1_diff_stderr": 0.10492155060893699,
                "rouge2_max": 3.0775068077414733,
                "rouge2_max_stderr": 0.1443089347191126,
                "rouge2_acc": 0.23990208078335373,
                "rouge2_acc_stderr": 0.01494881267906214,
                "rouge2_diff": -0.6499181850303296,
                "rouge2_diff_stderr": 0.0934320138734598,
                "rougeL_max": 5.696483921597016,
                "rougeL_max_stderr": 0.1816196852303411,
                "rougeL_acc": 0.3463892288861689,
                "rougeL_acc_stderr": 0.016656997109125136,
                "rougeL_diff": -0.5754493671860949,
                "rougeL_diff_stderr": 0.10036316110705906,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "truthfulqa_mc1": {
                "acc": 0.22888616891064872,
                "acc_stderr": 0.014706994909055027,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "winogrande": {
                "acc": 0.5130228887134964,
                "acc_stderr": 0.014047718393997667,
                "timestamp": "2024-11-22T20-36-49.643550"
            }
        }
    }
}