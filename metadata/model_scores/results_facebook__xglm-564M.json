{
    "model_name": "facebook/xglm-564M",
    "last_updated": "2024-12-19 13:38:05.443940",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.001148105625717566,
                "exact_match_stderr": 0.0011481056257175673,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "minerva_math_algebra": {
                "exact_match": 0.0008424599831508003,
                "exact_match_stderr": 0.0008424599831507863,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_3ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000116,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_2ds": {
                "acc": 0.008,
                "acc_stderr": 0.001992482118488464,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_1dc": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339569,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_2dm": {
                "acc": 0.0185,
                "acc_stderr": 0.0030138707185866456,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "arithmetic_2da": {
                "acc": 0.0065,
                "acc_stderr": 0.0017973564602277766,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "gsm8k_cot": {
                "exact_match": 0.02122820318423048,
                "exact_match_stderr": 0.003970449129848635,
                "timestamp": "2024-06-09T15-11-08.178008"
            },
            "gsm8k": {
                "exact_match": 0.016679302501895376,
                "exact_match_stderr": 0.0035275958887224573,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "anli_r2": {
                "brier_score": 0.8949833517502548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "anli_r3": {
                "brier_score": 0.8360668742052519,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "anli_r1": {
                "brier_score": 0.8884687642270159,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_eu": {
                "brier_score": 0.7371713039069413,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_vi": {
                "brier_score": 0.8654369273137161,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_ru": {
                "brier_score": 0.7606713225425753,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_zh": {
                "brier_score": 1.0486949265688723,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_tr": {
                "brier_score": 0.8214109452628465,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_fr": {
                "brier_score": 0.7815638841837665,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_en": {
                "brier_score": 0.7112428085311449,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_ur": {
                "brier_score": 1.062454393810576,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_ar": {
                "brier_score": 1.296895724941598,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_de": {
                "brier_score": 0.8351648045281843,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_hi": {
                "brier_score": 0.8394046981053793,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_es": {
                "brier_score": 0.8566611033916195,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_bg": {
                "brier_score": 0.7676392908318753,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_sw": {
                "brier_score": 0.8344826636346988,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_el": {
                "brier_score": 0.8773823206071238,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "xnli_th": {
                "brier_score": 0.8581311641212404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "logiqa2": {
                "brier_score": 1.1939318152210763,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "mathqa": {
                "brier_score": 1.0334681136329835,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-16-07.126503"
            },
            "lambada_standard": {
                "perplexity": 42.939212075145306,
                "perplexity_stderr": 1.641006871749824,
                "acc": 0.3291286629148069,
                "acc_stderr": 0.0065465809755531025,
                "timestamp": "2024-06-09T15-17-16.795144"
            },
            "lambada_openai": {
                "perplexity": 28.56367062584693,
                "perplexity_stderr": 1.0264515146295097,
                "acc": 0.3584319813700757,
                "acc_stderr": 0.006680928173680378,
                "timestamp": "2024-06-09T15-17-16.795144"
            },
            "mmlu_world_religions": {
                "acc": 0.3216374269005848,
                "acc_stderr": 0.03582529442573122,
                "brier_score": 1.0742204944517322,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_formal_logic": {
                "acc": 0.29365079365079366,
                "acc_stderr": 0.04073524322147127,
                "brier_score": 0.9756313169804313,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_prehistory": {
                "acc": 0.21296296296296297,
                "acc_stderr": 0.022779719088733396,
                "brier_score": 1.1572943708648094,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574885,
                "brier_score": 1.0254183250723512,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.270042194092827,
                "acc_stderr": 0.028900721906293426,
                "brier_score": 1.018241284346488,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_moral_disputes": {
                "acc": 0.24855491329479767,
                "acc_stderr": 0.023267528432100174,
                "brier_score": 0.9919887099294075,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_professional_law": {
                "acc": 0.2457627118644068,
                "acc_stderr": 0.01099615663514269,
                "brier_score": 1.14760371837571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.22085889570552147,
                "acc_stderr": 0.032591773927421776,
                "brier_score": 1.1609613230884086,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25,
                "acc_stderr": 0.03039153369274154,
                "brier_score": 1.0642376995987706,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_philosophy": {
                "acc": 0.1864951768488746,
                "acc_stderr": 0.02212243977248077,
                "brier_score": 1.0991696264667123,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_jurisprudence": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.04236511258094634,
                "brier_score": 1.1133413634160059,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_international_law": {
                "acc": 0.2396694214876033,
                "acc_stderr": 0.03896878985070417,
                "brier_score": 1.1945862952663897,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03225078108306289,
                "brier_score": 1.0930421099365542,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.19689119170984457,
                "acc_stderr": 0.02869787397186069,
                "brier_score": 1.2726744501976759,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.21008403361344538,
                "acc_stderr": 0.026461398717471874,
                "brier_score": 1.1642584812060601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_geography": {
                "acc": 0.17676767676767677,
                "acc_stderr": 0.027178752639044915,
                "brier_score": 1.2243462537350878,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.1926605504587156,
                "acc_stderr": 0.016909276884936073,
                "brier_score": 1.1864906805081876,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "brier_score": 1.171621547436488,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 1.041866918047384,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_sociology": {
                "acc": 0.24378109452736318,
                "acc_stderr": 0.030360490154014652,
                "brier_score": 1.1864462264687594,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.20256410256410257,
                "acc_stderr": 0.020377660970371397,
                "brier_score": 1.144645957965444,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_security_studies": {
                "acc": 0.1836734693877551,
                "acc_stderr": 0.024789071332007643,
                "brier_score": 1.0750865427602068,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25326797385620914,
                "acc_stderr": 0.017593486895366835,
                "brier_score": 1.10367136949964,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "brier_score": 1.1555577803376484,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.039994238792813386,
                "brier_score": 1.1043201571671586,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_miscellaneous": {
                "acc": 0.23754789272030652,
                "acc_stderr": 0.015218733046150195,
                "brier_score": 1.1747437386100705,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_marketing": {
                "acc": 0.2905982905982906,
                "acc_stderr": 0.029745048572674057,
                "brier_score": 1.017501207234622,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "brier_score": 1.3234052284173319,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_nutrition": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.023929155517351284,
                "brier_score": 1.0904194849137472,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 1.105144331505844,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_human_aging": {
                "acc": 0.3094170403587444,
                "acc_stderr": 0.031024411740572206,
                "brier_score": 1.0500046831441932,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_professional_medicine": {
                "acc": 0.18382352941176472,
                "acc_stderr": 0.02352924218519311,
                "brier_score": 1.3024801814973317,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_college_medicine": {
                "acc": 0.20809248554913296,
                "acc_stderr": 0.030952890217749884,
                "brier_score": 1.127110928046614,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_business_ethics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.9902005073726969,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.21509433962264152,
                "acc_stderr": 0.025288394502891377,
                "brier_score": 1.1457897742784553,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.038612291966536955,
                "brier_score": 1.2337053484077747,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370518,
                "brier_score": 1.0801259462709316,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_professional_accounting": {
                "acc": 0.23404255319148937,
                "acc_stderr": 0.025257861359432407,
                "brier_score": 1.1444845677900901,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.040925639582376556,
                "brier_score": 1.254457287294448,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_physics": {
                "acc": 0.1986754966887417,
                "acc_stderr": 0.032578473844367746,
                "brier_score": 1.2603638914713362,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_biology": {
                "acc": 0.1774193548387097,
                "acc_stderr": 0.021732540689329265,
                "brier_score": 1.1962168644512337,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_college_biology": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.03685651095897532,
                "brier_score": 1.10328888433029,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_anatomy": {
                "acc": 0.18518518518518517,
                "acc_stderr": 0.03355677216313142,
                "brier_score": 1.2379006196342983,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_college_chemistry": {
                "acc": 0.2,
                "acc_stderr": 0.040201512610368445,
                "brier_score": 1.1775779464579272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 1.0988299111046593,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_college_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 1.1186786193930078,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123398,
                "brier_score": 1.2905734218280234,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_college_mathematics": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 1.192045879915838,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.26382978723404255,
                "acc_stderr": 0.02880998985410298,
                "brier_score": 1.06926722842903,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 1.1232698972017345,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 1.1409648640225867,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_machine_learning": {
                "acc": 0.3125,
                "acc_stderr": 0.043994650575715215,
                "brier_score": 0.9704242816152965,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.15270935960591134,
                "acc_stderr": 0.025308904539380624,
                "brier_score": 1.197587214007225,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1527777777777778,
                "acc_stderr": 0.02453632602613422,
                "brier_score": 1.3412212646761488,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.20899470899470898,
                "acc_stderr": 0.020940481565334835,
                "brier_score": 1.2361802328965794,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.03565998174135302,
                "brier_score": 1.1751296553870207,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2111111111111111,
                "acc_stderr": 0.02488211685765508,
                "brier_score": 1.249433087090846,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-21-59.412791"
            },
            "arc_challenge": {
                "acc": 0.20648464163822525,
                "acc_stderr": 0.011828865619002316,
                "acc_norm": 0.25426621160409557,
                "acc_norm_stderr": 0.012724999945157732,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "hellaswag": {
                "acc": 0.30481975702051384,
                "acc_stderr": 0.004593902601979336,
                "acc_norm": 0.34933280223063135,
                "acc_norm_stderr": 0.004757849023411965,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "truthfulqa_mc2": {
                "acc": 0.4037316135738844,
                "acc_stderr": 0.014865718962222852,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "truthfulqa_gen": {
                "bleu_max": 1.3466045659305554,
                "bleu_max_stderr": 0.06990039606064374,
                "bleu_acc": 0.32802937576499386,
                "bleu_acc_stderr": 0.016435632932815022,
                "bleu_diff": -0.24132198144345063,
                "bleu_diff_stderr": 0.04226398695222224,
                "rouge1_max": 6.215723953891085,
                "rouge1_max_stderr": 0.1887917253922373,
                "rouge1_acc": 0.3561811505507956,
                "rouge1_acc_stderr": 0.016763790728446325,
                "rouge1_diff": -0.5530378301879756,
                "rouge1_diff_stderr": 0.10492155060893699,
                "rouge2_max": 3.0775068077414733,
                "rouge2_max_stderr": 0.1443089347191126,
                "rouge2_acc": 0.23990208078335373,
                "rouge2_acc_stderr": 0.01494881267906214,
                "rouge2_diff": -0.6499181850303296,
                "rouge2_diff_stderr": 0.0934320138734598,
                "rougeL_max": 5.696483921597016,
                "rougeL_max_stderr": 0.1816196852303411,
                "rougeL_acc": 0.3463892288861689,
                "rougeL_acc_stderr": 0.016656997109125136,
                "rougeL_diff": -0.5754493671860949,
                "rougeL_diff_stderr": 0.10036316110705906,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "truthfulqa_mc1": {
                "acc": 0.22888616891064872,
                "acc_stderr": 0.014706994909055027,
                "timestamp": "2024-11-22T20-36-49.643550"
            },
            "winogrande": {
                "acc": 0.5130228887134964,
                "acc_stderr": 0.014047718393997667,
                "timestamp": "2024-11-22T20-36-49.643550"
            }
        }
    }
}