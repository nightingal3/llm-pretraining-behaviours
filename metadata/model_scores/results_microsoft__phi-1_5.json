{
    "model_name": "microsoft__phi-1_5",
    "last_updated": "2024-12-19 13:42:10.917708",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.42105263157894735,
                "acc_stderr": 0.037867207062342145,
                "brier_score": 0.7360008752497482,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_formal_logic": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.040061680838488774,
                "brier_score": 0.777206013151806,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_prehistory": {
                "acc": 0.4166666666666667,
                "acc_stderr": 0.02743162372241501,
                "brier_score": 0.7380412963405232,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.21899441340782122,
                "acc_stderr": 0.013831676687303195,
                "brier_score": 0.7901934382936933,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.5485232067510548,
                "acc_stderr": 0.032393600173974704,
                "brier_score": 0.5792982228967096,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_moral_disputes": {
                "acc": 0.5173410404624278,
                "acc_stderr": 0.026902900458666647,
                "brier_score": 0.6423391106948374,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_professional_law": {
                "acc": 0.3318122555410691,
                "acc_stderr": 0.012026088259897623,
                "brier_score": 0.7996198436303776,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.50920245398773,
                "acc_stderr": 0.03927705600787443,
                "brier_score": 0.6745861358113816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.49019607843137253,
                "acc_stderr": 0.035086373586305716,
                "brier_score": 0.6738082218665467,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_philosophy": {
                "acc": 0.4662379421221865,
                "acc_stderr": 0.0283332771095628,
                "brier_score": 0.7074817938782464,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_jurisprudence": {
                "acc": 0.5555555555555556,
                "acc_stderr": 0.04803752235190193,
                "brier_score": 0.6202091131314537,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_international_law": {
                "acc": 0.6363636363636364,
                "acc_stderr": 0.043913262867240704,
                "brier_score": 0.515450487912183,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.5212121212121212,
                "acc_stderr": 0.03900828913737301,
                "brier_score": 0.6271142903227849,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.5492227979274611,
                "acc_stderr": 0.03590910952235524,
                "brier_score": 0.6016571539143739,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.4327731092436975,
                "acc_stderr": 0.03218358107742613,
                "brier_score": 0.6787214859797679,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_geography": {
                "acc": 0.5202020202020202,
                "acc_stderr": 0.03559443565563918,
                "brier_score": 0.6260096931634569,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.5779816513761468,
                "acc_stderr": 0.021174991407763175,
                "brier_score": 0.5846518035857755,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_public_relations": {
                "acc": 0.509090909090909,
                "acc_stderr": 0.0478833976870286,
                "brier_score": 0.6436576965735664,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.64,
                "acc_stderr": 0.048241815132442176,
                "brier_score": 0.5065862216963781,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_sociology": {
                "acc": 0.6716417910447762,
                "acc_stderr": 0.033206858897443244,
                "brier_score": 0.4555626101107058,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.4153846153846154,
                "acc_stderr": 0.02498535492310234,
                "brier_score": 0.7287925023449086,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_security_studies": {
                "acc": 0.4775510204081633,
                "acc_stderr": 0.03197694118713672,
                "brier_score": 0.6596316885420239,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_professional_psychology": {
                "acc": 0.39705882352941174,
                "acc_stderr": 0.01979448890002411,
                "brier_score": 0.7788372440904848,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_human_sexuality": {
                "acc": 0.5114503816793893,
                "acc_stderr": 0.043841400240780176,
                "brier_score": 0.6196890231613316,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_econometrics": {
                "acc": 0.2719298245614035,
                "acc_stderr": 0.04185774424022056,
                "brier_score": 0.8722626035349442,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_miscellaneous": {
                "acc": 0.5057471264367817,
                "acc_stderr": 0.01787878232612923,
                "brier_score": 0.6169403423513001,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_marketing": {
                "acc": 0.6794871794871795,
                "acc_stderr": 0.03057281131029961,
                "brier_score": 0.4510237014621223,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_management": {
                "acc": 0.6116504854368932,
                "acc_stderr": 0.04825729337356389,
                "brier_score": 0.5332099122676459,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_nutrition": {
                "acc": 0.5163398692810458,
                "acc_stderr": 0.028614624752805434,
                "brier_score": 0.6350133525853426,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_medical_genetics": {
                "acc": 0.44,
                "acc_stderr": 0.049888765156985884,
                "brier_score": 0.6881471618921275,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_human_aging": {
                "acc": 0.45739910313901344,
                "acc_stderr": 0.033435777055830646,
                "brier_score": 0.7105188295097965,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_professional_medicine": {
                "acc": 0.3272058823529412,
                "acc_stderr": 0.028501452860396563,
                "brier_score": 0.8125044459404857,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_college_medicine": {
                "acc": 0.3930635838150289,
                "acc_stderr": 0.0372424959581773,
                "brier_score": 0.763439234194539,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_business_ethics": {
                "acc": 0.5,
                "acc_stderr": 0.050251890762960605,
                "brier_score": 0.6084030677575967,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.4641509433962264,
                "acc_stderr": 0.030693675018458006,
                "brier_score": 0.7057383144243409,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_global_facts": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.7998431789880189,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_virology": {
                "acc": 0.41566265060240964,
                "acc_stderr": 0.038367221765980515,
                "brier_score": 0.7769035591355941,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2907801418439716,
                "acc_stderr": 0.027090664368353178,
                "brier_score": 0.8232895523294619,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_college_physics": {
                "acc": 0.30392156862745096,
                "acc_stderr": 0.045766654032077636,
                "brier_score": 0.8065082489214286,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_physics": {
                "acc": 0.31788079470198677,
                "acc_stderr": 0.03802039760107903,
                "brier_score": 0.7968941338535283,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_biology": {
                "acc": 0.47419354838709676,
                "acc_stderr": 0.02840609505765332,
                "brier_score": 0.6673893585036331,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_college_biology": {
                "acc": 0.3541666666666667,
                "acc_stderr": 0.039994111357535424,
                "brier_score": 0.7600764132559394,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_anatomy": {
                "acc": 0.4222222222222222,
                "acc_stderr": 0.04266763404099582,
                "brier_score": 0.7251975156324822,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_college_chemistry": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "brier_score": 0.7605508640162124,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_computer_security": {
                "acc": 0.5,
                "acc_stderr": 0.050251890762960605,
                "brier_score": 0.685223148320539,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_college_computer_science": {
                "acc": 0.37,
                "acc_stderr": 0.048523658709391,
                "brier_score": 0.7721012067814597,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_astronomy": {
                "acc": 0.39473684210526316,
                "acc_stderr": 0.039777499346220734,
                "brier_score": 0.7257436739204995,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_college_mathematics": {
                "acc": 0.37,
                "acc_stderr": 0.048523658709391,
                "brier_score": 0.7138852319996118,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.3659574468085106,
                "acc_stderr": 0.031489558297455304,
                "brier_score": 0.8308913625317457,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.8358822212811206,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.49,
                "acc_stderr": 0.05024183937956911,
                "brier_score": 0.6243291870440001,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_machine_learning": {
                "acc": 0.39285714285714285,
                "acc_stderr": 0.04635550135609976,
                "brier_score": 0.7403036536446453,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.33004926108374383,
                "acc_stderr": 0.03308530426228258,
                "brier_score": 0.7808990504625262,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.030225226160012393,
                "brier_score": 0.8587462682397424,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.30952380952380953,
                "acc_stderr": 0.023809523809523867,
                "brier_score": 0.7900592112702733,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.46206896551724136,
                "acc_stderr": 0.041546596717075474,
                "brier_score": 0.6533778994999127,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24444444444444444,
                "acc_stderr": 0.02620276653465215,
                "brier_score": 0.7963735663036272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-10-38.843036"
            },
            "arc_challenge": {
                "acc": 0.5008532423208191,
                "acc_stderr": 0.014611369529813262,
                "acc_norm": 0.5315699658703071,
                "acc_norm_stderr": 0.014582236460866977,
                "timestamp": "2024-11-10T18-48-26.589447"
            },
            "hellaswag": {
                "acc": 0.48615813582951606,
                "acc_stderr": 0.004987868988630005,
                "acc_norm": 0.6392152957578172,
                "acc_norm_stderr": 0.0047924672558997605,
                "timestamp": "2024-11-10T18-48-26.589447"
            },
            "truthfulqa_mc2": {
                "acc": 0.4085779509507484,
                "acc_stderr": 0.014840269110411395,
                "timestamp": "2024-11-10T18-48-26.589447"
            },
            "truthfulqa_gen": {
                "bleu_max": 27.567668355661375,
                "bleu_max_stderr": 0.7836996753793046,
                "bleu_acc": 0.31946144430844553,
                "bleu_acc_stderr": 0.016322644182960505,
                "bleu_diff": -4.901645580186722,
                "bleu_diff_stderr": 0.877157693134709,
                "rouge1_max": 52.930362387533215,
                "rouge1_max_stderr": 0.8338583348805911,
                "rouge1_acc": 0.32802937576499386,
                "rouge1_acc_stderr": 0.016435632932815,
                "rouge1_diff": -5.2936235823035105,
                "rouge1_diff_stderr": 1.0149100288222572,
                "rouge2_max": 38.05023260311765,
                "rouge2_max_stderr": 0.9792456124605255,
                "rouge2_acc": 0.28151774785801714,
                "rouge2_acc_stderr": 0.015744027248256045,
                "rouge2_diff": -6.717662093135759,
                "rouge2_diff_stderr": 1.1866608093259197,
                "rougeL_max": 50.36955243236768,
                "rougeL_max_stderr": 0.8580936496769774,
                "rougeL_acc": 0.3157894736842105,
                "rougeL_acc_stderr": 0.016272287957916885,
                "rougeL_diff": -5.513065735562071,
                "rougeL_diff_stderr": 1.0278544546573318,
                "timestamp": "2024-11-10T18-48-26.589447"
            },
            "truthfulqa_mc1": {
                "acc": 0.27050183598531213,
                "acc_stderr": 0.0155507783328429,
                "timestamp": "2024-11-10T18-48-26.589447"
            },
            "winogrande": {
                "acc": 0.7103393843725335,
                "acc_stderr": 0.012748550807638261,
                "timestamp": "2024-11-10T18-48-26.589447"
            },
            "gsm8k": {
                "exact_match": 0.3206974981046247,
                "exact_match_stderr": 0.012856468433722292,
                "timestamp": "2024-11-10T18-48-26.589447"
            }
        }
    }
}