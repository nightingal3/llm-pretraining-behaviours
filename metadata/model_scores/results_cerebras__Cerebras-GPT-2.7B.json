{
    "model_name": "cerebras/Cerebras-GPT-2.7B",
    "last_updated": "2024-12-04 11:24:35.537170",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.02197802197802198,
                "exact_match_stderr": 0.0062801549282525335,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.02525832376578645,
                "exact_match_stderr": 0.005319703220303023,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.011111111111111112,
                "exact_match_stderr": 0.004515003707694652,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.014396456256921373,
                "exact_match_stderr": 0.003966209590910285,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.005861462425818025,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0189873417721519,
                "exact_match_stderr": 0.006275362513989594,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_algebra": {
                "exact_match": 0.009267059814658803,
                "exact_match_stderr": 0.002782319118488812,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_2ds": {
                "acc": 0.0125,
                "acc_stderr": 0.00248494717876267,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_1dc": {
                "acc": 0.0135,
                "acc_stderr": 0.0025811249685072746,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_2dm": {
                "acc": 0.022,
                "acc_stderr": 0.0032807593162018913,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_2da": {
                "acc": 0.007,
                "acc_stderr": 0.0018647355360237512,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "gsm8k_cot": {
                "exact_match": 0.027293404094010616,
                "exact_match_stderr": 0.004488095380209752,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "gsm8k": {
                "exact_match": 0.02047005307050796,
                "exact_match_stderr": 0.003900413385915723,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "anli_r2": {
                "brier_score": 0.7441656976970576,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "anli_r3": {
                "brier_score": 0.7347729274486875,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "anli_r1": {
                "brier_score": 0.7559520884886023,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_eu": {
                "brier_score": 1.1749589886887095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_vi": {
                "brier_score": 0.7928664423474424,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_ru": {
                "brier_score": 0.7994578741034748,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_zh": {
                "brier_score": 0.940473825271897,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_tr": {
                "brier_score": 0.8423086576456662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_fr": {
                "brier_score": 0.8332420930715818,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_en": {
                "brier_score": 0.6809222796174058,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_ur": {
                "brier_score": 1.016116798585533,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_ar": {
                "brier_score": 1.0259751211437746,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_de": {
                "brier_score": 0.9495039241411007,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_hi": {
                "brier_score": 0.7931095115018697,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_es": {
                "brier_score": 0.8553823700128562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_bg": {
                "brier_score": 0.7603246399463625,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_sw": {
                "brier_score": 0.8740183453743544,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_el": {
                "brier_score": 1.1103931638852953,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_th": {
                "brier_score": 0.8312683414736515,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "logiqa2": {
                "brier_score": 1.1527212168892447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "mathqa": {
                "brier_score": 0.9819685448386691,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "lambada_standard": {
                "perplexity": 12.37585271202106,
                "perplexity_stderr": 0.3686403362745451,
                "acc": 0.46807684843780323,
                "acc_stderr": 0.006951765275756158,
                "timestamp": "2024-06-11T17-43-37.534801"
            },
            "lambada_openai": {
                "perplexity": 7.735706433404285,
                "perplexity_stderr": 0.20963908845319387,
                "acc": 0.5651077042499515,
                "acc_stderr": 0.006906667423619273,
                "timestamp": "2024-06-11T17-43-37.534801"
            },
            "mmlu_world_religions": {
                "acc": 0.26900584795321636,
                "acc_stderr": 0.0340105262010409,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_formal_logic": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.03718489006818114,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_prehistory": {
                "acc": 0.23765432098765432,
                "acc_stderr": 0.02368359183700856,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.27150837988826815,
                "acc_stderr": 0.014874252168095271,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25738396624472576,
                "acc_stderr": 0.028458820991460288,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26878612716763006,
                "acc_stderr": 0.023868003262500104,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_professional_law": {
                "acc": 0.2633637548891786,
                "acc_stderr": 0.011249506403605284,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.31901840490797545,
                "acc_stderr": 0.03661997551073836,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2696078431372549,
                "acc_stderr": 0.031145570659486782,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_philosophy": {
                "acc": 0.27009646302250806,
                "acc_stderr": 0.025218040373410622,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_jurisprudence": {
                "acc": 0.18518518518518517,
                "acc_stderr": 0.037552658650371835,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_international_law": {
                "acc": 0.2396694214876033,
                "acc_stderr": 0.03896878985070417,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.22424242424242424,
                "acc_stderr": 0.03256866661681102,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21761658031088082,
                "acc_stderr": 0.02977866303775296,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.027553614467863804,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_geography": {
                "acc": 0.22727272727272727,
                "acc_stderr": 0.02985751567338641,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23302752293577983,
                "acc_stderr": 0.018125669180861503,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_public_relations": {
                "acc": 0.16363636363636364,
                "acc_stderr": 0.035434330542986774,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720683,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_sociology": {
                "acc": 0.2537313432835821,
                "acc_stderr": 0.03076944496729601,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24102564102564103,
                "acc_stderr": 0.021685546665333195,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_security_studies": {
                "acc": 0.32653061224489793,
                "acc_stderr": 0.030021056238440307,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2581699346405229,
                "acc_stderr": 0.017704531653250078,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2366412213740458,
                "acc_stderr": 0.037276735755969174,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.040493392977481425,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_miscellaneous": {
                "acc": 0.26181353767560667,
                "acc_stderr": 0.01572083867844527,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_marketing": {
                "acc": 0.2564102564102564,
                "acc_stderr": 0.02860595370200425,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_management": {
                "acc": 0.22330097087378642,
                "acc_stderr": 0.04123553189891431,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_nutrition": {
                "acc": 0.2679738562091503,
                "acc_stderr": 0.025360603796242553,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_medical_genetics": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_human_aging": {
                "acc": 0.242152466367713,
                "acc_stderr": 0.028751392398694755,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_professional_medicine": {
                "acc": 0.25,
                "acc_stderr": 0.026303648393696036,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_college_medicine": {
                "acc": 0.23121387283236994,
                "acc_stderr": 0.032147373020294696,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_business_ethics": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.21509433962264152,
                "acc_stderr": 0.025288394502891373,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.038612291966536955,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370519,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2695035460992908,
                "acc_stderr": 0.026469036818590634,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_college_physics": {
                "acc": 0.17647058823529413,
                "acc_stderr": 0.037932811853078105,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2781456953642384,
                "acc_stderr": 0.03658603262763743,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_biology": {
                "acc": 0.24838709677419354,
                "acc_stderr": 0.024580028921481003,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_college_biology": {
                "acc": 0.2708333333333333,
                "acc_stderr": 0.03716177437566016,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_anatomy": {
                "acc": 0.2814814814814815,
                "acc_stderr": 0.03885004245800255,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_college_chemistry": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036623,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_computer_security": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_college_computer_science": {
                "acc": 0.37,
                "acc_stderr": 0.048523658709391,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_astronomy": {
                "acc": 0.19736842105263158,
                "acc_stderr": 0.03238981601699397,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_college_mathematics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.25957446808510637,
                "acc_stderr": 0.028659179374292316,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165044,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.39,
                "acc_stderr": 0.04902071300001975,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_machine_learning": {
                "acc": 0.3392857142857143,
                "acc_stderr": 0.04493949068613539,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2660098522167488,
                "acc_stderr": 0.03108982600293752,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2824074074074074,
                "acc_stderr": 0.030701372111510927,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2037037037037037,
                "acc_stderr": 0.020742740560122656,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2482758620689655,
                "acc_stderr": 0.036001056927277716,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.02696242432507384,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "arc_challenge": {
                "acc": 0.26109215017064846,
                "acc_stderr": 0.01283552390947384,
                "acc_norm": 0.2883959044368601,
                "acc_norm_stderr": 0.013238394422428175,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "hellaswag": {
                "acc": 0.38388767177853017,
                "acc_stderr": 0.004853371646239244,
                "acc_norm": 0.4924317864967138,
                "acc_norm_stderr": 0.00498920977074323,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "truthfulqa_mc2": {
                "acc": 0.413716384386627,
                "acc_stderr": 0.014440498911294912,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "truthfulqa_gen": {
                "bleu_max": 23.035243335291693,
                "bleu_max_stderr": 0.7624195895417043,
                "bleu_acc": 0.4467564259485924,
                "bleu_acc_stderr": 0.017403977522557144,
                "bleu_diff": 2.6100154453645934,
                "bleu_diff_stderr": 0.9107727769465377,
                "rouge1_max": 45.78174774695968,
                "rouge1_max_stderr": 0.9626102741710353,
                "rouge1_acc": 0.38555691554467564,
                "rouge1_acc_stderr": 0.017038839010591667,
                "rouge1_diff": 3.3839378780303964,
                "rouge1_diff_stderr": 1.307720244891136,
                "rouge2_max": 29.129612331742987,
                "rouge2_max_stderr": 1.1201208139741414,
                "rouge2_acc": 0.2839657282741738,
                "rouge2_acc_stderr": 0.01578537085839671,
                "rouge2_diff": 2.646752250018767,
                "rouge2_diff_stderr": 1.3612360250946927,
                "rougeL_max": 43.715314843247654,
                "rougeL_max_stderr": 0.971141942560561,
                "rougeL_acc": 0.390452876376989,
                "rougeL_acc_stderr": 0.017078230743431448,
                "rougeL_diff": 3.7178865764791853,
                "rougeL_diff_stderr": 1.3006816230973743,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "truthfulqa_mc1": {
                "acc": 0.2460220318237454,
                "acc_stderr": 0.015077219200662587,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "winogrande": {
                "acc": 0.5603788476716653,
                "acc_stderr": 0.013949649776015692,
                "timestamp": "2024-11-07T10-27-52.672195"
            }
        }
    }
}