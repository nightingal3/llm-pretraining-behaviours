{
    "model_name": "cerebras/Cerebras-GPT-2.7B",
    "last_updated": "2024-12-19 13:40:29.112136",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.02197802197802198,
                "exact_match_stderr": 0.0062801549282525335,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.02525832376578645,
                "exact_match_stderr": 0.005319703220303023,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.011111111111111112,
                "exact_match_stderr": 0.004515003707694652,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.014396456256921373,
                "exact_match_stderr": 0.003966209590910285,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.005861462425818025,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0189873417721519,
                "exact_match_stderr": 0.006275362513989594,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "minerva_math_algebra": {
                "exact_match": 0.009267059814658803,
                "exact_match_stderr": 0.002782319118488812,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_2ds": {
                "acc": 0.0125,
                "acc_stderr": 0.00248494717876267,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_1dc": {
                "acc": 0.0135,
                "acc_stderr": 0.0025811249685072746,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_2dm": {
                "acc": 0.022,
                "acc_stderr": 0.0032807593162018913,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "arithmetic_2da": {
                "acc": 0.007,
                "acc_stderr": 0.0018647355360237512,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "gsm8k_cot": {
                "exact_match": 0.027293404094010616,
                "exact_match_stderr": 0.004488095380209752,
                "timestamp": "2024-06-11T17-24-42.692973"
            },
            "gsm8k": {
                "exact_match": 0.02047005307050796,
                "exact_match_stderr": 0.003900413385915723,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "anli_r2": {
                "brier_score": 0.7441656976970576,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "anli_r3": {
                "brier_score": 0.7347729274486875,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "anli_r1": {
                "brier_score": 0.7559520884886023,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_eu": {
                "brier_score": 1.1749589886887095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_vi": {
                "brier_score": 0.7928664423474424,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_ru": {
                "brier_score": 0.7994578741034748,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_zh": {
                "brier_score": 0.940473825271897,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_tr": {
                "brier_score": 0.8423086576456662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_fr": {
                "brier_score": 0.8332420930715818,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_en": {
                "brier_score": 0.6809222796174058,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_ur": {
                "brier_score": 1.016116798585533,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_ar": {
                "brier_score": 1.0259751211437746,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_de": {
                "brier_score": 0.9495039241411007,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_hi": {
                "brier_score": 0.7931095115018697,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_es": {
                "brier_score": 0.8553823700128562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_bg": {
                "brier_score": 0.7603246399463625,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_sw": {
                "brier_score": 0.8740183453743544,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_el": {
                "brier_score": 1.1103931638852953,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "xnli_th": {
                "brier_score": 0.8312683414736515,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "logiqa2": {
                "brier_score": 1.1527212168892447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "mathqa": {
                "brier_score": 0.9819685448386691,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T17-41-47.713877"
            },
            "lambada_standard": {
                "perplexity": 12.37585271202106,
                "perplexity_stderr": 0.3686403362745451,
                "acc": 0.46807684843780323,
                "acc_stderr": 0.006951765275756158,
                "timestamp": "2024-06-11T17-43-37.534801"
            },
            "lambada_openai": {
                "perplexity": 7.735706433404285,
                "perplexity_stderr": 0.20963908845319387,
                "acc": 0.5651077042499515,
                "acc_stderr": 0.006906667423619273,
                "timestamp": "2024-06-11T17-43-37.534801"
            },
            "mmlu_world_religions": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.03615507630310935,
                "brier_score": 0.7476930573228682,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_formal_logic": {
                "acc": 0.18253968253968253,
                "acc_stderr": 0.03455071019102147,
                "brier_score": 0.7756165109851708,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_prehistory": {
                "acc": 0.22839506172839505,
                "acc_stderr": 0.023358211840626267,
                "brier_score": 0.7923180096902048,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574885,
                "brier_score": 0.7560601311261977,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25738396624472576,
                "acc_stderr": 0.028458820991460295,
                "brier_score": 0.7574709963177562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2543352601156069,
                "acc_stderr": 0.02344582627654555,
                "brier_score": 0.7863398783390683,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_professional_law": {
                "acc": 0.258148631029987,
                "acc_stderr": 0.011176923719313397,
                "brier_score": 0.7834504398219595,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2147239263803681,
                "acc_stderr": 0.03226219377286774,
                "brier_score": 0.7749737316033809,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.028379449451588667,
                "brier_score": 0.7730385015882367,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_philosophy": {
                "acc": 0.19935691318327975,
                "acc_stderr": 0.022691033780549656,
                "brier_score": 0.8200560029886024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_jurisprudence": {
                "acc": 0.28703703703703703,
                "acc_stderr": 0.04373313040914761,
                "brier_score": 0.781378343322981,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_international_law": {
                "acc": 0.24793388429752067,
                "acc_stderr": 0.039418975265163025,
                "brier_score": 0.7768501435131513,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03225078108306289,
                "brier_score": 0.7776240256500807,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21761658031088082,
                "acc_stderr": 0.02977866303775296,
                "brier_score": 0.7999517058536764,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.026265024608275882,
                "brier_score": 0.7860553522313176,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_geography": {
                "acc": 0.19696969696969696,
                "acc_stderr": 0.02833560973246335,
                "brier_score": 0.8000606853746892,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23853211009174313,
                "acc_stderr": 0.01827257581023187,
                "brier_score": 0.7696250170238662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_public_relations": {
                "acc": 0.20909090909090908,
                "acc_stderr": 0.03895091015724137,
                "brier_score": 0.7735527423605206,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542128,
                "brier_score": 0.7691975852419901,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_sociology": {
                "acc": 0.22388059701492538,
                "acc_stderr": 0.029475250236017193,
                "brier_score": 0.8032467232239617,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2128205128205128,
                "acc_stderr": 0.020752423722128027,
                "brier_score": 0.7881431774671177,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_security_studies": {
                "acc": 0.2,
                "acc_stderr": 0.025607375986579153,
                "brier_score": 0.7926343764752054,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.017740899509177795,
                "brier_score": 0.7721471100019798,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_human_sexuality": {
                "acc": 0.24427480916030533,
                "acc_stderr": 0.03768335959728744,
                "brier_score": 0.7740153632877359,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.039994238792813344,
                "brier_score": 0.7599040869797429,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_miscellaneous": {
                "acc": 0.280970625798212,
                "acc_stderr": 0.016073127851221235,
                "brier_score": 0.7657858959409427,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_marketing": {
                "acc": 0.31196581196581197,
                "acc_stderr": 0.03035152732334493,
                "brier_score": 0.7525991848995963,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_management": {
                "acc": 0.2912621359223301,
                "acc_stderr": 0.044986763205729224,
                "brier_score": 0.7733049014327111,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_nutrition": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.023929155517351284,
                "brier_score": 0.7840834700358891,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_medical_genetics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720683,
                "brier_score": 0.7580529964562934,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_human_aging": {
                "acc": 0.29596412556053814,
                "acc_stderr": 0.0306365913486998,
                "brier_score": 0.7562483844482,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_professional_medicine": {
                "acc": 0.17279411764705882,
                "acc_stderr": 0.02296606758558178,
                "brier_score": 0.776164881685414,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.03242414757483099,
                "brier_score": 0.7794146792011398,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_business_ethics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7558271806493747,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.20754716981132076,
                "acc_stderr": 0.02495991802891127,
                "brier_score": 0.7972495925387602,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_global_facts": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036844,
                "brier_score": 0.7775920033083306,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_virology": {
                "acc": 0.2469879518072289,
                "acc_stderr": 0.03357351982064536,
                "brier_score": 0.7758619736761486,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_professional_accounting": {
                "acc": 0.24468085106382978,
                "acc_stderr": 0.025645553622266733,
                "brier_score": 0.7720524096989436,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_college_physics": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.04336432707993179,
                "brier_score": 0.749662073099727,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2980132450331126,
                "acc_stderr": 0.037345356767871984,
                "brier_score": 0.7472706507270679,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_biology": {
                "acc": 0.23548387096774193,
                "acc_stderr": 0.02413763242933771,
                "brier_score": 0.7690663879971471,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_college_biology": {
                "acc": 0.2708333333333333,
                "acc_stderr": 0.037161774375660164,
                "brier_score": 0.7683342643108664,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_anatomy": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.035914440841969694,
                "brier_score": 0.7780246383757022,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_college_chemistry": {
                "acc": 0.14,
                "acc_stderr": 0.03487350880197771,
                "brier_score": 0.7790738942811067,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_computer_security": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768079,
                "brier_score": 0.7572718501114037,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_college_computer_science": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421296,
                "brier_score": 0.7470019487908016,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_astronomy": {
                "acc": 0.27631578947368424,
                "acc_stderr": 0.03639057569952924,
                "brier_score": 0.7481578957001758,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_college_mathematics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7615635626380802,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.28085106382978725,
                "acc_stderr": 0.029379170464124825,
                "brier_score": 0.7747178620837013,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.041633319989322695,
                "brier_score": 0.7949552265673998,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252604,
                "brier_score": 0.736408727816062,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_machine_learning": {
                "acc": 0.3482142857142857,
                "acc_stderr": 0.04521829902833585,
                "brier_score": 0.7449644713984921,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.24630541871921183,
                "acc_stderr": 0.030315099285617736,
                "brier_score": 0.7678783424089564,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.19444444444444445,
                "acc_stderr": 0.02699145450203673,
                "brier_score": 0.7702784574532471,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.02256989707491842,
                "brier_score": 0.7596261911717037,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.03565998174135302,
                "brier_score": 0.7637082530045226,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.02696242432507385,
                "brier_score": 0.7582533211051161,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-09-24.727491"
            },
            "arc_challenge": {
                "acc": 0.26109215017064846,
                "acc_stderr": 0.01283552390947384,
                "acc_norm": 0.2883959044368601,
                "acc_norm_stderr": 0.013238394422428175,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "hellaswag": {
                "acc": 0.38388767177853017,
                "acc_stderr": 0.004853371646239244,
                "acc_norm": 0.4924317864967138,
                "acc_norm_stderr": 0.00498920977074323,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "truthfulqa_mc2": {
                "acc": 0.413716384386627,
                "acc_stderr": 0.014440498911294912,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "truthfulqa_gen": {
                "bleu_max": 23.035243335291693,
                "bleu_max_stderr": 0.7624195895417043,
                "bleu_acc": 0.4467564259485924,
                "bleu_acc_stderr": 0.017403977522557144,
                "bleu_diff": 2.6100154453645934,
                "bleu_diff_stderr": 0.9107727769465377,
                "rouge1_max": 45.78174774695968,
                "rouge1_max_stderr": 0.9626102741710353,
                "rouge1_acc": 0.38555691554467564,
                "rouge1_acc_stderr": 0.017038839010591667,
                "rouge1_diff": 3.3839378780303964,
                "rouge1_diff_stderr": 1.307720244891136,
                "rouge2_max": 29.129612331742987,
                "rouge2_max_stderr": 1.1201208139741414,
                "rouge2_acc": 0.2839657282741738,
                "rouge2_acc_stderr": 0.01578537085839671,
                "rouge2_diff": 2.646752250018767,
                "rouge2_diff_stderr": 1.3612360250946927,
                "rougeL_max": 43.715314843247654,
                "rougeL_max_stderr": 0.971141942560561,
                "rougeL_acc": 0.390452876376989,
                "rougeL_acc_stderr": 0.017078230743431448,
                "rougeL_diff": 3.7178865764791853,
                "rougeL_diff_stderr": 1.3006816230973743,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "truthfulqa_mc1": {
                "acc": 0.2460220318237454,
                "acc_stderr": 0.015077219200662587,
                "timestamp": "2024-11-07T10-27-52.672195"
            },
            "winogrande": {
                "acc": 0.5603788476716653,
                "acc_stderr": 0.013949649776015692,
                "timestamp": "2024-11-07T10-27-52.672195"
            }
        }
    }
}