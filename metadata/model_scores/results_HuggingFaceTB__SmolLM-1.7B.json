{
    "model_name": "HuggingFaceTB__SmolLM-1.7B",
    "last_updated": "2024-12-19 13:41:49.711330",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.30409356725146197,
                "acc_stderr": 0.03528211258245232,
                "brier_score": 0.7551636436309582,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.03670066451047182,
                "brier_score": 0.7801281626265447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_prehistory": {
                "acc": 0.3117283950617284,
                "acc_stderr": 0.025773111169630433,
                "brier_score": 0.7327440907985833,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.014422292204808838,
                "brier_score": 0.7665170648069967,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.32489451476793246,
                "acc_stderr": 0.030486039389105296,
                "brier_score": 0.7430802455771087,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_moral_disputes": {
                "acc": 0.32947976878612717,
                "acc_stderr": 0.025305258131879723,
                "brier_score": 0.7487056188535807,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_professional_law": {
                "acc": 0.2666232073011734,
                "acc_stderr": 0.01129383603161214,
                "brier_score": 0.7597029785420658,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.25766871165644173,
                "acc_stderr": 0.03436150827846917,
                "brier_score": 0.7527711239704512,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.30392156862745096,
                "acc_stderr": 0.03228210387037892,
                "brier_score": 0.7470402138960359,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_philosophy": {
                "acc": 0.33440514469453375,
                "acc_stderr": 0.026795422327893937,
                "brier_score": 0.7318698523628182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_jurisprudence": {
                "acc": 0.32407407407407407,
                "acc_stderr": 0.04524596007030048,
                "brier_score": 0.7467307283787081,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_international_law": {
                "acc": 0.4214876033057851,
                "acc_stderr": 0.045077322787750944,
                "brier_score": 0.7196812416252452,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.30303030303030304,
                "acc_stderr": 0.035886248000917095,
                "brier_score": 0.7466180394615518,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.2538860103626943,
                "acc_stderr": 0.03141024780565319,
                "brier_score": 0.7520148085554117,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.02755361446786381,
                "brier_score": 0.7706544205273947,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_geography": {
                "acc": 0.2676767676767677,
                "acc_stderr": 0.03154449888270286,
                "brier_score": 0.7539202035996062,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.25688073394495414,
                "acc_stderr": 0.018732492928342444,
                "brier_score": 0.7540537370882963,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_public_relations": {
                "acc": 0.3090909090909091,
                "acc_stderr": 0.044262946482000985,
                "brier_score": 0.741642452100577,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7395676551979159,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_sociology": {
                "acc": 0.2885572139303483,
                "acc_stderr": 0.03203841040213321,
                "brier_score": 0.7494414822803905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.26153846153846155,
                "acc_stderr": 0.022282141204204423,
                "brier_score": 0.7690134510038235,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_security_studies": {
                "acc": 0.24897959183673468,
                "acc_stderr": 0.027682979522960227,
                "brier_score": 0.760649006945786,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_professional_psychology": {
                "acc": 0.27124183006535946,
                "acc_stderr": 0.01798661530403032,
                "brier_score": 0.7559105031184763,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_human_sexuality": {
                "acc": 0.29770992366412213,
                "acc_stderr": 0.040103589424622034,
                "brier_score": 0.740708587662364,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_econometrics": {
                "acc": 0.21052631578947367,
                "acc_stderr": 0.038351539543994194,
                "brier_score": 0.7576774223011613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_miscellaneous": {
                "acc": 0.31417624521072796,
                "acc_stderr": 0.0165992917358849,
                "brier_score": 0.7455891824240866,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_marketing": {
                "acc": 0.3162393162393162,
                "acc_stderr": 0.030463656747340244,
                "brier_score": 0.7519586934939722,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_management": {
                "acc": 0.2815533980582524,
                "acc_stderr": 0.04453254836326469,
                "brier_score": 0.759008869776314,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_nutrition": {
                "acc": 0.29411764705882354,
                "acc_stderr": 0.02609016250427905,
                "brier_score": 0.7566521828898929,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_medical_genetics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542128,
                "brier_score": 0.7584183884384231,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_human_aging": {
                "acc": 0.28699551569506726,
                "acc_stderr": 0.03036037971029196,
                "brier_score": 0.7601582506539263,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_professional_medicine": {
                "acc": 0.2536764705882353,
                "acc_stderr": 0.02643132987078954,
                "brier_score": 0.7525283294492598,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.03242414757483098,
                "brier_score": 0.7557866419534296,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_business_ethics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542128,
                "brier_score": 0.7593669275619496,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2641509433962264,
                "acc_stderr": 0.02713429162874172,
                "brier_score": 0.7559920747609482,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_global_facts": {
                "acc": 0.36,
                "acc_stderr": 0.04824181513244218,
                "brier_score": 0.7280857545855145,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_virology": {
                "acc": 0.2891566265060241,
                "acc_stderr": 0.03529486801511115,
                "brier_score": 0.7417908992843658,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2553191489361702,
                "acc_stderr": 0.026011992930902006,
                "brier_score": 0.7609243491199253,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_college_physics": {
                "acc": 0.19607843137254902,
                "acc_stderr": 0.03950581861179963,
                "brier_score": 0.7712757539051257,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2847682119205298,
                "acc_stderr": 0.03684881521389024,
                "brier_score": 0.7527327446093819,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2903225806451613,
                "acc_stderr": 0.025822106119415898,
                "brier_score": 0.7558582574449789,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_college_biology": {
                "acc": 0.2569444444444444,
                "acc_stderr": 0.03653946969442099,
                "brier_score": 0.7742859530237306,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_anatomy": {
                "acc": 0.32592592592592595,
                "acc_stderr": 0.040491220417025055,
                "brier_score": 0.7313849868720731,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_college_chemistry": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7727946797892771,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_computer_security": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "brier_score": 0.740939384922402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_college_computer_science": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036846,
                "brier_score": 0.7795700638432723,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_astronomy": {
                "acc": 0.3223684210526316,
                "acc_stderr": 0.03803510248351586,
                "brier_score": 0.7392204275275199,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_college_mathematics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.76147059626292,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2936170212765957,
                "acc_stderr": 0.029771642712491227,
                "brier_score": 0.7448651469298134,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "brier_score": 0.7693922391618123,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621504,
                "brier_score": 0.7305897004268131,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_machine_learning": {
                "acc": 0.25,
                "acc_stderr": 0.04109974682633932,
                "brier_score": 0.7620884806930387,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2660098522167488,
                "acc_stderr": 0.03108982600293752,
                "brier_score": 0.7493879817389563,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.17592592592592593,
                "acc_stderr": 0.025967420958258526,
                "brier_score": 0.7940560528283264,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2724867724867725,
                "acc_stderr": 0.02293097307163334,
                "brier_score": 0.7668511170348982,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2689655172413793,
                "acc_stderr": 0.036951833116502325,
                "brier_score": 0.7554425454586065,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.26296296296296295,
                "acc_stderr": 0.026842057873833706,
                "brier_score": 0.7690530697397449,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-02-17.509556"
            },
            "arc_challenge": {
                "acc": 0.4616040955631399,
                "acc_stderr": 0.014568245550296358,
                "acc_norm": 0.48976109215017066,
                "acc_norm_stderr": 0.014608326906285012,
                "timestamp": "2024-11-21T21-51-42.336660"
            },
            "hellaswag": {
                "acc": 0.4985062736506672,
                "acc_stderr": 0.004989759144812283,
                "acc_norm": 0.673770165305716,
                "acc_norm_stderr": 0.004678743563766636,
                "timestamp": "2024-11-21T21-51-42.336660"
            },
            "truthfulqa_mc2": {
                "acc": 0.3850766766083665,
                "acc_stderr": 0.0141318041269147,
                "timestamp": "2024-11-21T21-51-42.336660"
            },
            "truthfulqa_gen": {
                "bleu_max": 25.637781985326768,
                "bleu_max_stderr": 0.7862985780149876,
                "bleu_acc": 0.3243574051407589,
                "bleu_acc_stderr": 0.016387976779647935,
                "bleu_diff": -7.032993037032112,
                "bleu_diff_stderr": 0.8260513065264542,
                "rouge1_max": 50.02333005649277,
                "rouge1_max_stderr": 0.888601487283363,
                "rouge1_acc": 0.2974296205630355,
                "rouge1_acc_stderr": 0.016002651487361005,
                "rouge1_diff": -9.43923743956951,
                "rouge1_diff_stderr": 0.9261892834631157,
                "rouge2_max": 33.70014727089389,
                "rouge2_max_stderr": 1.0124553331073005,
                "rouge2_acc": 0.25458996328029376,
                "rouge2_acc_stderr": 0.015250117079156487,
                "rouge2_diff": -11.30035982468476,
                "rouge2_diff_stderr": 1.0978598755923092,
                "rougeL_max": 47.21056335206833,
                "rougeL_max_stderr": 0.8970416486465508,
                "rougeL_acc": 0.2839657282741738,
                "rougeL_acc_stderr": 0.015785370858396718,
                "rougeL_diff": -9.830444388716177,
                "rougeL_diff_stderr": 0.9254935688315172,
                "timestamp": "2024-11-21T21-51-42.336660"
            },
            "truthfulqa_mc1": {
                "acc": 0.24479804161566707,
                "acc_stderr": 0.015051869486715006,
                "timestamp": "2024-11-21T21-51-42.336660"
            },
            "winogrande": {
                "acc": 0.611681136543015,
                "acc_stderr": 0.013697456658457228,
                "timestamp": "2024-11-21T21-51-42.336660"
            },
            "gsm8k": {
                "exact_match": 0.052312357846853674,
                "exact_match_stderr": 0.006133057708959227,
                "timestamp": "2024-11-21T21-51-42.336660"
            }
        }
    }
}