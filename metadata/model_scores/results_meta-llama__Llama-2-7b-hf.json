{
    "model_name": "meta-llama/Llama-2-7b-hf",
    "last_updated": "2024-12-04 11:24:15.676077",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.018315018315018316,
                "exact_match_stderr": 0.00574369673165366,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.0711825487944891,
                "exact_match_stderr": 0.008717507390607558,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.018518518518518517,
                "exact_match_stderr": 0.005806972807912266,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.021040974529346623,
                "exact_match_stderr": 0.004778723623319674,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "minerva_math_geometry": {
                "exact_match": 0.037578288100208766,
                "exact_match_stderr": 0.008698357509032981,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.02531645569620253,
                "exact_match_stderr": 0.007222751926043707,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "minerva_math_algebra": {
                "exact_match": 0.03201347935973041,
                "exact_match_stderr": 0.005111622218276094,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_3da": {
                "acc": 0.8525,
                "acc_stderr": 0.007931161747394468,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_3ds": {
                "acc": 0.439,
                "acc_stderr": 0.011099599116647338,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_4da": {
                "acc": 0.6725,
                "acc_stderr": 0.010496521494368456,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_2ds": {
                "acc": 0.5025,
                "acc_stderr": 0.01118299623099078,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_5ds": {
                "acc": 0.2475,
                "acc_stderr": 0.00965238101349167,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_5da": {
                "acc": 0.405,
                "acc_stderr": 0.010979425025334411,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_1dc": {
                "acc": 0.1845,
                "acc_stderr": 0.008675684915577382,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_4ds": {
                "acc": 0.363,
                "acc_stderr": 0.010755153958374458,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_2dm": {
                "acc": 0.1545,
                "acc_stderr": 0.008083783073189481,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "arithmetic_2da": {
                "acc": 0.8885,
                "acc_stderr": 0.007039790787172794,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "gsm8k_cot": {
                "exact_match": 0.14404852160727824,
                "exact_match_stderr": 0.009672110973065277,
                "timestamp": "2024-06-11T12-11-22.933030"
            },
            "gsm8k": {
                "exact_match": 0.14025777103866566,
                "exact_match_stderr": 0.009565108281428644,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "anli_r2": {
                "brier_score": 0.6728312849024087,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "anli_r3": {
                "brier_score": 0.675924163095268,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "anli_r1": {
                "brier_score": 0.680913669643727,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_eu": {
                "brier_score": 1.0405008687131228,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_vi": {
                "brier_score": 0.9640582331183833,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_ru": {
                "brier_score": 0.8785393767177124,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_zh": {
                "brier_score": 0.9451672393270157,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_tr": {
                "brier_score": 0.8861324289870143,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_fr": {
                "brier_score": 0.7371020830089063,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_en": {
                "brier_score": 0.6309192441010487,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_ur": {
                "brier_score": 1.2513457477843288,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_ar": {
                "brier_score": 1.0275160054419725,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_de": {
                "brier_score": 0.7952906725023694,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_hi": {
                "brier_score": 0.9207229324542658,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_es": {
                "brier_score": 0.9614303254112365,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_bg": {
                "brier_score": 0.8810276928274537,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_sw": {
                "brier_score": 0.8940776492925108,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_el": {
                "brier_score": 0.8013356806216894,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "xnli_th": {
                "brier_score": 0.9641360198328498,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "logiqa2": {
                "brier_score": 0.9310549072453368,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "mathqa": {
                "brier_score": 0.8984744744150647,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T12-22-24.374330"
            },
            "lambada_standard": {
                "perplexity": 4.129570848882444,
                "perplexity_stderr": 0.08224461333530494,
                "acc": 0.6821269163594023,
                "acc_stderr": 0.006487412955192984,
                "timestamp": "2024-06-11T12-24-02.494796"
            },
            "lambada_openai": {
                "perplexity": 3.395129491760032,
                "perplexity_stderr": 0.06674310180079371,
                "acc": 0.7391810595769455,
                "acc_stderr": 0.006117261570238603,
                "timestamp": "2024-06-11T12-24-02.494796"
            },
            "mmlu_world_religions": {
                "acc": 0.695906432748538,
                "acc_stderr": 0.03528211258245232,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_formal_logic": {
                "acc": 0.29365079365079366,
                "acc_stderr": 0.04073524322147126,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_prehistory": {
                "acc": 0.5030864197530864,
                "acc_stderr": 0.02782021415859437,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24022346368715083,
                "acc_stderr": 0.014288343803925308,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.6371308016877637,
                "acc_stderr": 0.031299208255302136,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_moral_disputes": {
                "acc": 0.5057803468208093,
                "acc_stderr": 0.026917296179149123,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_professional_law": {
                "acc": 0.3670143415906128,
                "acc_stderr": 0.012310264244842129,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.5153374233128835,
                "acc_stderr": 0.03926522378708843,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.5441176470588235,
                "acc_stderr": 0.03495624522015478,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_philosophy": {
                "acc": 0.5916398713826366,
                "acc_stderr": 0.027917050748484634,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_jurisprudence": {
                "acc": 0.5277777777777778,
                "acc_stderr": 0.04826217294139894,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_international_law": {
                "acc": 0.6528925619834711,
                "acc_stderr": 0.043457245702925335,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.6,
                "acc_stderr": 0.03825460278380026,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.6787564766839378,
                "acc_stderr": 0.033699508685490674,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.4327731092436975,
                "acc_stderr": 0.03218358107742613,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_geography": {
                "acc": 0.494949494949495,
                "acc_stderr": 0.035621707606254015,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.6238532110091743,
                "acc_stderr": 0.020769231968205074,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_public_relations": {
                "acc": 0.5636363636363636,
                "acc_stderr": 0.04750185058907296,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.62,
                "acc_stderr": 0.04878317312145633,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_sociology": {
                "acc": 0.6517412935323383,
                "acc_stderr": 0.033687874661154596,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.4512820512820513,
                "acc_stderr": 0.025230381238934833,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_security_studies": {
                "acc": 0.47346938775510206,
                "acc_stderr": 0.03196412734523272,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_professional_psychology": {
                "acc": 0.4444444444444444,
                "acc_stderr": 0.02010258389588718,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_human_sexuality": {
                "acc": 0.549618320610687,
                "acc_stderr": 0.04363643698524779,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_econometrics": {
                "acc": 0.2719298245614035,
                "acc_stderr": 0.04185774424022056,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_miscellaneous": {
                "acc": 0.6436781609195402,
                "acc_stderr": 0.017125853762755886,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_marketing": {
                "acc": 0.6965811965811965,
                "acc_stderr": 0.030118210106942666,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_management": {
                "acc": 0.5436893203883495,
                "acc_stderr": 0.049318019942204146,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_nutrition": {
                "acc": 0.49673202614379086,
                "acc_stderr": 0.02862930519400354,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_medical_genetics": {
                "acc": 0.51,
                "acc_stderr": 0.05024183937956911,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_human_aging": {
                "acc": 0.5650224215246636,
                "acc_stderr": 0.03327283370271345,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_professional_medicine": {
                "acc": 0.5183823529411765,
                "acc_stderr": 0.03035230339535196,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_college_medicine": {
                "acc": 0.41040462427745666,
                "acc_stderr": 0.03750757044895537,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_business_ethics": {
                "acc": 0.53,
                "acc_stderr": 0.05016135580465919,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.46037735849056605,
                "acc_stderr": 0.030676096599389188,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_global_facts": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621505,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_virology": {
                "acc": 0.42168674698795183,
                "acc_stderr": 0.038444531817709175,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_professional_accounting": {
                "acc": 0.3475177304964539,
                "acc_stderr": 0.02840662780959095,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_college_physics": {
                "acc": 0.24509803921568626,
                "acc_stderr": 0.04280105837364395,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_physics": {
                "acc": 0.31788079470198677,
                "acc_stderr": 0.03802039760107903,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_biology": {
                "acc": 0.4967741935483871,
                "acc_stderr": 0.02844341422643833,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_college_biology": {
                "acc": 0.4722222222222222,
                "acc_stderr": 0.04174752578923185,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_anatomy": {
                "acc": 0.4666666666666667,
                "acc_stderr": 0.043097329010363554,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_college_chemistry": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695236,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_computer_security": {
                "acc": 0.61,
                "acc_stderr": 0.04902071300001975,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_college_computer_science": {
                "acc": 0.34,
                "acc_stderr": 0.047609522856952344,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_astronomy": {
                "acc": 0.3881578947368421,
                "acc_stderr": 0.03965842097512744,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_college_mathematics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.4127659574468085,
                "acc_stderr": 0.03218471141400351,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720683,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.39,
                "acc_stderr": 0.049020713000019756,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_machine_learning": {
                "acc": 0.375,
                "acc_stderr": 0.04595091388086298,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.37438423645320196,
                "acc_stderr": 0.03405155380561952,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.030058202704309846,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2671957671957672,
                "acc_stderr": 0.022789673145776564,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.47586206896551725,
                "acc_stderr": 0.041618085035015295,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.3,
                "acc_stderr": 0.02794045713622841,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "arc_challenge": {
                "acc": 0.4854948805460751,
                "acc_stderr": 0.014605241081370053,
                "acc_norm": 0.5290102389078498,
                "acc_norm_stderr": 0.014586776355294323,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "hellaswag": {
                "acc": 0.5868352917745469,
                "acc_stderr": 0.004913955705080136,
                "acc_norm": 0.7866958773152758,
                "acc_norm_stderr": 0.004088034745195409,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "truthfulqa_mc2": {
                "acc": 0.38965409945411716,
                "acc_stderr": 0.013577349838309216,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "truthfulqa_gen": {
                "bleu_max": 30.794481532169428,
                "bleu_max_stderr": 0.8287919046619883,
                "bleu_acc": 0.34394124847001223,
                "bleu_acc_stderr": 0.016629087514276816,
                "bleu_diff": -5.993291829792142,
                "bleu_diff_stderr": 0.9641520859323807,
                "rouge1_max": 56.463597234122524,
                "rouge1_max_stderr": 0.8547674057001485,
                "rouge1_acc": 0.3219094247246022,
                "rouge1_acc_stderr": 0.01635556761196043,
                "rouge1_diff": -7.067120288688261,
                "rouge1_diff_stderr": 1.0624470832704904,
                "rouge2_max": 42.08844049121037,
                "rouge2_max_stderr": 1.020295723228426,
                "rouge2_acc": 0.30599755201958384,
                "rouge2_acc_stderr": 0.01613222972815502,
                "rouge2_diff": -8.293387411017884,
                "rouge2_diff_stderr": 1.2550691006847554,
                "rougeL_max": 53.62297062326941,
                "rougeL_max_stderr": 0.8821248084779468,
                "rougeL_acc": 0.3243574051407589,
                "rougeL_acc_stderr": 0.01638797677964794,
                "rougeL_diff": -7.257582732253812,
                "rougeL_diff_stderr": 1.0716212651200012,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "truthfulqa_mc1": {
                "acc": 0.2521419828641371,
                "acc_stderr": 0.015201522246299962,
                "timestamp": "2024-11-21T04-23-19.383457"
            },
            "winogrande": {
                "acc": 0.7458563535911602,
                "acc_stderr": 0.012236307219708269,
                "timestamp": "2024-11-21T04-23-19.383457"
            }
        }
    }
}