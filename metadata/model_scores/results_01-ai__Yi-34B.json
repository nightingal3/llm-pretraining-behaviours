{
    "model_name": "01-ai/Yi-34B",
    "last_updated": "2024-12-19 13:39:23.602577",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.6160409556313993,
                    "acc_stderr": 0.01421244498065189,
                    "acc_norm": 0.6459044368600683,
                    "acc_norm_stderr": 0.01397545412275656,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hellaswag": {
                "acc": 0.6568412666799442,
                "acc_stderr": 0.004737936758047645,
                "acc_norm": 0.8558056164110734,
                "acc_norm_stderr": 0.0035056879433872637,
                "timestamp": "2024-11-27T03-38-49.243482"
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.45,
                    "acc_stderr": 0.049999999999999996,
                    "acc_norm": 0.45,
                    "acc_norm_stderr": 0.049999999999999996,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.7481481481481481,
                    "acc_stderr": 0.03749850709174021,
                    "acc_norm": 0.7481481481481481,
                    "acc_norm_stderr": 0.03749850709174021,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.9013157894736842,
                    "acc_stderr": 0.024270227737522715,
                    "acc_norm": 0.9013157894736842,
                    "acc_norm_stderr": 0.024270227737522715,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.79,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.79,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.7924528301886793,
                    "acc_stderr": 0.02495991802891127,
                    "acc_norm": 0.7924528301886793,
                    "acc_norm_stderr": 0.02495991802891127,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.8819444444444444,
                    "acc_stderr": 0.026983346503309354,
                    "acc_norm": 0.8819444444444444,
                    "acc_norm_stderr": 0.026983346503309354,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.49,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.65,
                    "acc_stderr": 0.04793724854411019,
                    "acc_norm": 0.65,
                    "acc_norm_stderr": 0.04793724854411019,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.48,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.7109826589595376,
                    "acc_stderr": 0.03456425745086999,
                    "acc_norm": 0.7109826589595376,
                    "acc_norm_stderr": 0.03456425745086999,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.04975185951049946,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.04975185951049946,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.03861229196653694,
                    "acc_norm": 0.82,
                    "acc_norm_stderr": 0.03861229196653694,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.7702127659574468,
                    "acc_stderr": 0.02750175294441242,
                    "acc_norm": 0.7702127659574468,
                    "acc_norm_stderr": 0.02750175294441242,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.5526315789473685,
                    "acc_stderr": 0.04677473004491199,
                    "acc_norm": 0.5526315789473685,
                    "acc_norm_stderr": 0.04677473004491199,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.0333333333333333,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.0333333333333333,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.656084656084656,
                    "acc_stderr": 0.024464426625596437,
                    "acc_norm": 0.656084656084656,
                    "acc_norm_stderr": 0.024464426625596437,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.5634920634920635,
                    "acc_stderr": 0.04435932892851466,
                    "acc_norm": 0.5634920634920635,
                    "acc_norm_stderr": 0.04435932892851466,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.8806451612903226,
                    "acc_stderr": 0.018443411325315393,
                    "acc_norm": 0.8806451612903226,
                    "acc_norm_stderr": 0.018443411325315393,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.645320197044335,
                    "acc_stderr": 0.03366124489051449,
                    "acc_norm": 0.645320197044335,
                    "acc_norm_stderr": 0.03366124489051449,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.038612291966536955,
                    "acc_norm": 0.82,
                    "acc_norm_stderr": 0.038612291966536955,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.8666666666666667,
                    "acc_stderr": 0.026544435312706473,
                    "acc_norm": 0.8666666666666667,
                    "acc_norm_stderr": 0.026544435312706473,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.8939393939393939,
                    "acc_stderr": 0.021938047738853106,
                    "acc_norm": 0.8939393939393939,
                    "acc_norm_stderr": 0.021938047738853106,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.9792746113989638,
                    "acc_stderr": 0.010281417011909042,
                    "acc_norm": 0.9792746113989638,
                    "acc_norm_stderr": 0.010281417011909042,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.020280805062535726,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.020280805062535726,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.4444444444444444,
                    "acc_stderr": 0.030296771286067323,
                    "acc_norm": 0.4444444444444444,
                    "acc_norm_stderr": 0.030296771286067323,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.8571428571428571,
                    "acc_stderr": 0.02273020811930654,
                    "acc_norm": 0.8571428571428571,
                    "acc_norm_stderr": 0.02273020811930654,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.5165562913907285,
                    "acc_stderr": 0.04080244185628972,
                    "acc_norm": 0.5165562913907285,
                    "acc_norm_stderr": 0.04080244185628972,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.9155963302752294,
                    "acc_stderr": 0.011918819327334877,
                    "acc_norm": 0.9155963302752294,
                    "acc_norm_stderr": 0.011918819327334877,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.6388888888888888,
                    "acc_stderr": 0.032757734861009996,
                    "acc_norm": 0.6388888888888888,
                    "acc_norm_stderr": 0.032757734861009996,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.9166666666666666,
                    "acc_stderr": 0.019398452135813905,
                    "acc_norm": 0.9166666666666666,
                    "acc_norm_stderr": 0.019398452135813905,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.919831223628692,
                    "acc_stderr": 0.017676679991891625,
                    "acc_norm": 0.919831223628692,
                    "acc_norm_stderr": 0.017676679991891625,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.7937219730941704,
                    "acc_stderr": 0.027157150479563824,
                    "acc_norm": 0.7937219730941704,
                    "acc_norm_stderr": 0.027157150479563824,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.8625954198473282,
                    "acc_stderr": 0.030194823996804475,
                    "acc_norm": 0.8625954198473282,
                    "acc_norm_stderr": 0.030194823996804475,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.9090909090909091,
                    "acc_stderr": 0.02624319405407388,
                    "acc_norm": 0.9090909090909091,
                    "acc_norm_stderr": 0.02624319405407388,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8888888888888888,
                    "acc_stderr": 0.03038159675665167,
                    "acc_norm": 0.8888888888888888,
                    "acc_norm_stderr": 0.03038159675665167,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.8834355828220859,
                    "acc_stderr": 0.025212327210507108,
                    "acc_norm": 0.8834355828220859,
                    "acc_norm_stderr": 0.025212327210507108,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.5982142857142857,
                    "acc_stderr": 0.04653333146973647,
                    "acc_norm": 0.5982142857142857,
                    "acc_norm_stderr": 0.04653333146973647,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.912621359223301,
                    "acc_stderr": 0.027960689125970654,
                    "acc_norm": 0.912621359223301,
                    "acc_norm_stderr": 0.027960689125970654,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.9316239316239316,
                    "acc_stderr": 0.01653462768431136,
                    "acc_norm": 0.9316239316239316,
                    "acc_norm_stderr": 0.01653462768431136,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.87,
                    "acc_stderr": 0.033799766898963086,
                    "acc_norm": 0.87,
                    "acc_norm_stderr": 0.033799766898963086,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.9054916985951469,
                    "acc_stderr": 0.01046101533819307,
                    "acc_norm": 0.9054916985951469,
                    "acc_norm_stderr": 0.01046101533819307,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.8294797687861272,
                    "acc_stderr": 0.020247961569303728,
                    "acc_norm": 0.8294797687861272,
                    "acc_norm_stderr": 0.020247961569303728,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.6446927374301676,
                    "acc_stderr": 0.016006989934803192,
                    "acc_norm": 0.6446927374301676,
                    "acc_norm_stderr": 0.016006989934803192,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.8627450980392157,
                    "acc_stderr": 0.01970403918385981,
                    "acc_norm": 0.8627450980392157,
                    "acc_norm_stderr": 0.01970403918385981,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.8392282958199357,
                    "acc_stderr": 0.020862388082391888,
                    "acc_norm": 0.8392282958199357,
                    "acc_norm_stderr": 0.020862388082391888,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.8827160493827161,
                    "acc_stderr": 0.01790311261528112,
                    "acc_norm": 0.8827160493827161,
                    "acc_norm_stderr": 0.01790311261528112,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.6702127659574468,
                    "acc_stderr": 0.02804594694204241,
                    "acc_norm": 0.6702127659574468,
                    "acc_norm_stderr": 0.02804594694204241,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.6049543676662321,
                    "acc_stderr": 0.01248572781325157,
                    "acc_norm": 0.6049543676662321,
                    "acc_norm_stderr": 0.01248572781325157,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.8125,
                    "acc_stderr": 0.023709788253811766,
                    "acc_norm": 0.8125,
                    "acc_norm_stderr": 0.023709788253811766,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.8186274509803921,
                    "acc_stderr": 0.015588643495370457,
                    "acc_norm": 0.8186274509803921,
                    "acc_norm_stderr": 0.015588643495370457,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.7363636363636363,
                    "acc_stderr": 0.04220224692971987,
                    "acc_norm": 0.7363636363636363,
                    "acc_norm_stderr": 0.04220224692971987,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.8448979591836735,
                    "acc_stderr": 0.0231747988612186,
                    "acc_norm": 0.8448979591836735,
                    "acc_norm_stderr": 0.0231747988612186,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8905472636815921,
                    "acc_stderr": 0.022076326101824657,
                    "acc_norm": 0.8905472636815921,
                    "acc_norm_stderr": 0.022076326101824657,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.9,
                    "acc_stderr": 0.030151134457776334,
                    "acc_norm": 0.9,
                    "acc_norm_stderr": 0.030151134457776334,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5783132530120482,
                    "acc_stderr": 0.038444531817709175,
                    "acc_norm": 0.5783132530120482,
                    "acc_norm_stderr": 0.038444531817709175,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8771929824561403,
                    "acc_stderr": 0.02517298435015578,
                    "acc_norm": 0.8771929824561403,
                    "acc_norm_stderr": 0.02517298435015578,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.40758873929008566,
                    "mc1_stderr": 0.017201949234553107,
                    "mc2": 0.5623083932983032,
                    "mc2_stderr": 0.015165963671039869,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.6081166107382551,
                    "em_stderr": 0.004999326629880105,
                    "f1": 0.6419882550335565,
                    "f1_stderr": 0.004748239351156368,
                    "timestamp": "2023-11-08T19-46-38.378007"
                }
            },
            "gsm8k": {
                "exact_match": 0.6557998483699773,
                "exact_match_stderr": 0.013086800426693782,
                "timestamp": "2024-11-27T03-38-49.243482"
            },
            "winogrande": {
                "acc": 0.8287292817679558,
                "acc_stderr": 0.010588417294962526,
                "timestamp": "2024-11-27T03-38-49.243482"
            },
            "mmlu_world_religions": {
                "acc": 0.8713450292397661,
                "acc_stderr": 0.02567934272327694,
                "brier_score": 0.18327084828640616,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_formal_logic": {
                "acc": 0.5317460317460317,
                "acc_stderr": 0.04463112720677172,
                "brier_score": 0.586843042091581,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_prehistory": {
                "acc": 0.8487654320987654,
                "acc_stderr": 0.019935086092149876,
                "brier_score": 0.22676479805196784,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.43687150837988825,
                "acc_stderr": 0.016588680864530626,
                "brier_score": 0.6715835727205642,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.890295358649789,
                "acc_stderr": 0.020343400734868847,
                "brier_score": 0.16060559995313187,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_moral_disputes": {
                "acc": 0.8005780346820809,
                "acc_stderr": 0.021511900654252555,
                "brier_score": 0.2933084466158301,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_professional_law": {
                "acc": 0.6016949152542372,
                "acc_stderr": 0.012503310565166239,
                "brier_score": 0.5424167959759115,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.8711656441717791,
                "acc_stderr": 0.026321383198783636,
                "brier_score": 0.20225943773288335,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.9019607843137255,
                "acc_stderr": 0.020871118455552097,
                "brier_score": 0.14855776488105746,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_philosophy": {
                "acc": 0.8070739549839229,
                "acc_stderr": 0.02241151678091136,
                "brier_score": 0.26327499807735083,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_jurisprudence": {
                "acc": 0.9259259259259259,
                "acc_stderr": 0.025317997297209738,
                "brier_score": 0.1481508666174445,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_international_law": {
                "acc": 0.859504132231405,
                "acc_stderr": 0.031722334260021585,
                "brier_score": 0.17724953898088328,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.8545454545454545,
                "acc_stderr": 0.02753019635506658,
                "brier_score": 0.2116949199581668,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.9689119170984456,
                "acc_stderr": 0.012525310625527019,
                "brier_score": 0.046998485404674356,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.8235294117647058,
                "acc_stderr": 0.024762902678057926,
                "brier_score": 0.22790712051886977,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_geography": {
                "acc": 0.9141414141414141,
                "acc_stderr": 0.019960225563172885,
                "brier_score": 0.1433233477967982,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.9027522935779817,
                "acc_stderr": 0.01270353340854038,
                "brier_score": 0.14856247232733205,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_public_relations": {
                "acc": 0.7545454545454545,
                "acc_stderr": 0.04122066502878284,
                "brier_score": 0.3290300957956976,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.94,
                "acc_stderr": 0.0238683256575942,
                "brier_score": 0.10569846644231891,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_sociology": {
                "acc": 0.8756218905472637,
                "acc_stderr": 0.023335401790166327,
                "brier_score": 0.1946006826671885,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.7974358974358975,
                "acc_stderr": 0.020377660970371393,
                "brier_score": 0.28320335308971284,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_security_studies": {
                "acc": 0.8081632653061225,
                "acc_stderr": 0.02520696315422539,
                "brier_score": 0.2731915635613184,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_professional_psychology": {
                "acc": 0.7990196078431373,
                "acc_stderr": 0.01621193888965559,
                "brier_score": 0.27985172892589205,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_human_sexuality": {
                "acc": 0.8244274809160306,
                "acc_stderr": 0.03336820338476076,
                "brier_score": 0.21410963942217057,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_econometrics": {
                "acc": 0.6140350877192983,
                "acc_stderr": 0.04579639422070434,
                "brier_score": 0.5066114633945582,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_miscellaneous": {
                "acc": 0.896551724137931,
                "acc_stderr": 0.010890452544691481,
                "brier_score": 0.14592820623528896,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_marketing": {
                "acc": 0.9145299145299145,
                "acc_stderr": 0.018315891685625838,
                "brier_score": 0.1259947955560078,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_management": {
                "acc": 0.8640776699029126,
                "acc_stderr": 0.033932957297610096,
                "brier_score": 0.16745701098951601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_nutrition": {
                "acc": 0.8398692810457516,
                "acc_stderr": 0.0209987409303623,
                "brier_score": 0.22269453920155644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_medical_genetics": {
                "acc": 0.89,
                "acc_stderr": 0.03144660377352202,
                "brier_score": 0.18956379020345207,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_human_aging": {
                "acc": 0.7668161434977578,
                "acc_stderr": 0.028380391147094713,
                "brier_score": 0.3350636038155653,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_professional_medicine": {
                "acc": 0.8088235294117647,
                "acc_stderr": 0.023886881922440352,
                "brier_score": 0.2711944775698018,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_college_medicine": {
                "acc": 0.6878612716763006,
                "acc_stderr": 0.03533133389323657,
                "brier_score": 0.37305027047556816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_business_ethics": {
                "acc": 0.77,
                "acc_stderr": 0.04229525846816507,
                "brier_score": 0.2994103903421135,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.7811320754716982,
                "acc_stderr": 0.0254478638251086,
                "brier_score": 0.27156114148483634,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_global_facts": {
                "acc": 0.62,
                "acc_stderr": 0.04878317312145634,
                "brier_score": 0.5321935454798382,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_virology": {
                "acc": 0.572289156626506,
                "acc_stderr": 0.038515976837185335,
                "brier_score": 0.6847042867927003,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_professional_accounting": {
                "acc": 0.6099290780141844,
                "acc_stderr": 0.029097675599463926,
                "brier_score": 0.47774400145574714,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_college_physics": {
                "acc": 0.5,
                "acc_stderr": 0.04975185951049946,
                "brier_score": 0.6111197699586813,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_physics": {
                "acc": 0.5099337748344371,
                "acc_stderr": 0.04081677107248436,
                "brier_score": 0.6474363239128381,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_biology": {
                "acc": 0.8774193548387097,
                "acc_stderr": 0.018656720991789402,
                "brier_score": 0.18133846283863253,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_college_biology": {
                "acc": 0.8611111111111112,
                "acc_stderr": 0.028919802956134905,
                "brier_score": 0.19358888814122588,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_anatomy": {
                "acc": 0.7555555555555555,
                "acc_stderr": 0.037125378336148665,
                "brier_score": 0.34125616897932837,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_college_chemistry": {
                "acc": 0.52,
                "acc_stderr": 0.050211673156867795,
                "brier_score": 0.5641430565145782,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_computer_security": {
                "acc": 0.81,
                "acc_stderr": 0.039427724440366234,
                "brier_score": 0.23630050012470782,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_college_computer_science": {
                "acc": 0.64,
                "acc_stderr": 0.04824181513244218,
                "brier_score": 0.4467921110077767,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_astronomy": {
                "acc": 0.8552631578947368,
                "acc_stderr": 0.028631951845930387,
                "brier_score": 0.21622829183700756,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_college_mathematics": {
                "acc": 0.41,
                "acc_stderr": 0.049431107042371025,
                "brier_score": 0.678893353888229,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.723404255319149,
                "acc_stderr": 0.029241883869628817,
                "brier_score": 0.3747238590806247,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.47,
                "acc_stderr": 0.05016135580465919,
                "brier_score": 0.6528285560015504,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.8,
                "acc_stderr": 0.04020151261036847,
                "brier_score": 0.257800700904363,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_machine_learning": {
                "acc": 0.6160714285714286,
                "acc_stderr": 0.04616143075028547,
                "brier_score": 0.4957008287428955,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.6354679802955665,
                "acc_stderr": 0.0338640574606209,
                "brier_score": 0.4977746266472533,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.6620370370370371,
                "acc_stderr": 0.03225941352631295,
                "brier_score": 0.44800329761941105,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.5529100529100529,
                "acc_stderr": 0.025606723995777028,
                "brier_score": 0.5190951310769079,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.7586206896551724,
                "acc_stderr": 0.03565998174135303,
                "brier_score": 0.35460163341746903,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.37777777777777777,
                "acc_stderr": 0.029560707392465718,
                "brier_score": 0.6790299812588423,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-58-08.159671"
            },
            "arc_challenge": {
                "acc": 0.6049488054607508,
                "acc_stderr": 0.014285898292938163,
                "acc_norm": 0.6459044368600683,
                "acc_norm_stderr": 0.013975454122756555,
                "timestamp": "2024-11-27T03-38-49.243482"
            },
            "truthfulqa_mc2": {
                "acc": 0.562494191768382,
                "acc_stderr": 0.015156214731267605,
                "timestamp": "2024-11-27T03-38-49.243482"
            },
            "truthfulqa_gen": {
                "bleu_max": 20.330647080409832,
                "bleu_max_stderr": 0.7024796684315535,
                "bleu_acc": 0.48714810281517745,
                "bleu_acc_stderr": 0.01749771794429982,
                "bleu_diff": 1.2034859228193233,
                "bleu_diff_stderr": 0.588166050547877,
                "rouge1_max": 45.71688577672627,
                "rouge1_max_stderr": 0.8227479818577124,
                "rouge1_acc": 0.5140758873929009,
                "rouge1_acc_stderr": 0.017496563717042796,
                "rouge1_diff": 2.5276609445648797,
                "rouge1_diff_stderr": 0.781731669558233,
                "rouge2_max": 29.910346423305118,
                "rouge2_max_stderr": 0.9393337389294315,
                "rouge2_acc": 0.3769889840881273,
                "rouge2_acc_stderr": 0.016965517578930358,
                "rouge2_diff": 0.485430202097819,
                "rouge2_diff_stderr": 0.8945392225724594,
                "rougeL_max": 42.9851750782942,
                "rougeL_max_stderr": 0.8216423235693433,
                "rougeL_acc": 0.5361077111383109,
                "rougeL_acc_stderr": 0.017457800422268622,
                "rougeL_diff": 2.5484074342195404,
                "rougeL_diff_stderr": 0.7888693977638205,
                "timestamp": "2024-11-27T03-38-49.243482"
            },
            "truthfulqa_mc1": {
                "acc": 0.40758873929008566,
                "acc_stderr": 0.01720194923455311,
                "timestamp": "2024-11-27T03-38-49.243482"
            }
        }
    }
}