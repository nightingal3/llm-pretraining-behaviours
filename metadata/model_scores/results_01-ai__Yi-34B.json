{
    "model_name": "01-ai/Yi-34B",
    "last_updated": "2024-12-04 11:23:57.995135",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.6160409556313993,
                    "acc_stderr": 0.01421244498065189,
                    "acc_norm": 0.6459044368600683,
                    "acc_norm_stderr": 0.01397545412275656,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6568412666799442,
                    "acc_stderr": 0.004737936758047645,
                    "acc_norm": 0.8558056164110734,
                    "acc_norm_stderr": 0.0035056879433872637,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.45,
                    "acc_stderr": 0.049999999999999996,
                    "acc_norm": 0.45,
                    "acc_norm_stderr": 0.049999999999999996,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.7481481481481481,
                    "acc_stderr": 0.03749850709174021,
                    "acc_norm": 0.7481481481481481,
                    "acc_norm_stderr": 0.03749850709174021,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.9013157894736842,
                    "acc_stderr": 0.024270227737522715,
                    "acc_norm": 0.9013157894736842,
                    "acc_norm_stderr": 0.024270227737522715,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.79,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.79,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.7924528301886793,
                    "acc_stderr": 0.02495991802891127,
                    "acc_norm": 0.7924528301886793,
                    "acc_norm_stderr": 0.02495991802891127,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.8819444444444444,
                    "acc_stderr": 0.026983346503309354,
                    "acc_norm": 0.8819444444444444,
                    "acc_norm_stderr": 0.026983346503309354,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.49,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.65,
                    "acc_stderr": 0.04793724854411019,
                    "acc_norm": 0.65,
                    "acc_norm_stderr": 0.04793724854411019,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.48,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.7109826589595376,
                    "acc_stderr": 0.03456425745086999,
                    "acc_norm": 0.7109826589595376,
                    "acc_norm_stderr": 0.03456425745086999,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.04975185951049946,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.04975185951049946,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.03861229196653694,
                    "acc_norm": 0.82,
                    "acc_norm_stderr": 0.03861229196653694,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.7702127659574468,
                    "acc_stderr": 0.02750175294441242,
                    "acc_norm": 0.7702127659574468,
                    "acc_norm_stderr": 0.02750175294441242,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.5526315789473685,
                    "acc_stderr": 0.04677473004491199,
                    "acc_norm": 0.5526315789473685,
                    "acc_norm_stderr": 0.04677473004491199,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.0333333333333333,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.0333333333333333,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.656084656084656,
                    "acc_stderr": 0.024464426625596437,
                    "acc_norm": 0.656084656084656,
                    "acc_norm_stderr": 0.024464426625596437,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.5634920634920635,
                    "acc_stderr": 0.04435932892851466,
                    "acc_norm": 0.5634920634920635,
                    "acc_norm_stderr": 0.04435932892851466,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.8806451612903226,
                    "acc_stderr": 0.018443411325315393,
                    "acc_norm": 0.8806451612903226,
                    "acc_norm_stderr": 0.018443411325315393,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.645320197044335,
                    "acc_stderr": 0.03366124489051449,
                    "acc_norm": 0.645320197044335,
                    "acc_norm_stderr": 0.03366124489051449,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.038612291966536955,
                    "acc_norm": 0.82,
                    "acc_norm_stderr": 0.038612291966536955,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.8666666666666667,
                    "acc_stderr": 0.026544435312706473,
                    "acc_norm": 0.8666666666666667,
                    "acc_norm_stderr": 0.026544435312706473,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.8939393939393939,
                    "acc_stderr": 0.021938047738853106,
                    "acc_norm": 0.8939393939393939,
                    "acc_norm_stderr": 0.021938047738853106,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.9792746113989638,
                    "acc_stderr": 0.010281417011909042,
                    "acc_norm": 0.9792746113989638,
                    "acc_norm_stderr": 0.010281417011909042,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.020280805062535726,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.020280805062535726,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.4444444444444444,
                    "acc_stderr": 0.030296771286067323,
                    "acc_norm": 0.4444444444444444,
                    "acc_norm_stderr": 0.030296771286067323,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.8571428571428571,
                    "acc_stderr": 0.02273020811930654,
                    "acc_norm": 0.8571428571428571,
                    "acc_norm_stderr": 0.02273020811930654,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.5165562913907285,
                    "acc_stderr": 0.04080244185628972,
                    "acc_norm": 0.5165562913907285,
                    "acc_norm_stderr": 0.04080244185628972,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.9155963302752294,
                    "acc_stderr": 0.011918819327334877,
                    "acc_norm": 0.9155963302752294,
                    "acc_norm_stderr": 0.011918819327334877,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.6388888888888888,
                    "acc_stderr": 0.032757734861009996,
                    "acc_norm": 0.6388888888888888,
                    "acc_norm_stderr": 0.032757734861009996,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.9166666666666666,
                    "acc_stderr": 0.019398452135813905,
                    "acc_norm": 0.9166666666666666,
                    "acc_norm_stderr": 0.019398452135813905,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.919831223628692,
                    "acc_stderr": 0.017676679991891625,
                    "acc_norm": 0.919831223628692,
                    "acc_norm_stderr": 0.017676679991891625,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.7937219730941704,
                    "acc_stderr": 0.027157150479563824,
                    "acc_norm": 0.7937219730941704,
                    "acc_norm_stderr": 0.027157150479563824,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.8625954198473282,
                    "acc_stderr": 0.030194823996804475,
                    "acc_norm": 0.8625954198473282,
                    "acc_norm_stderr": 0.030194823996804475,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.9090909090909091,
                    "acc_stderr": 0.02624319405407388,
                    "acc_norm": 0.9090909090909091,
                    "acc_norm_stderr": 0.02624319405407388,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8888888888888888,
                    "acc_stderr": 0.03038159675665167,
                    "acc_norm": 0.8888888888888888,
                    "acc_norm_stderr": 0.03038159675665167,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.8834355828220859,
                    "acc_stderr": 0.025212327210507108,
                    "acc_norm": 0.8834355828220859,
                    "acc_norm_stderr": 0.025212327210507108,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.5982142857142857,
                    "acc_stderr": 0.04653333146973647,
                    "acc_norm": 0.5982142857142857,
                    "acc_norm_stderr": 0.04653333146973647,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.912621359223301,
                    "acc_stderr": 0.027960689125970654,
                    "acc_norm": 0.912621359223301,
                    "acc_norm_stderr": 0.027960689125970654,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.9316239316239316,
                    "acc_stderr": 0.01653462768431136,
                    "acc_norm": 0.9316239316239316,
                    "acc_norm_stderr": 0.01653462768431136,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.87,
                    "acc_stderr": 0.033799766898963086,
                    "acc_norm": 0.87,
                    "acc_norm_stderr": 0.033799766898963086,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.9054916985951469,
                    "acc_stderr": 0.01046101533819307,
                    "acc_norm": 0.9054916985951469,
                    "acc_norm_stderr": 0.01046101533819307,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.8294797687861272,
                    "acc_stderr": 0.020247961569303728,
                    "acc_norm": 0.8294797687861272,
                    "acc_norm_stderr": 0.020247961569303728,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.6446927374301676,
                    "acc_stderr": 0.016006989934803192,
                    "acc_norm": 0.6446927374301676,
                    "acc_norm_stderr": 0.016006989934803192,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.8627450980392157,
                    "acc_stderr": 0.01970403918385981,
                    "acc_norm": 0.8627450980392157,
                    "acc_norm_stderr": 0.01970403918385981,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.8392282958199357,
                    "acc_stderr": 0.020862388082391888,
                    "acc_norm": 0.8392282958199357,
                    "acc_norm_stderr": 0.020862388082391888,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.8827160493827161,
                    "acc_stderr": 0.01790311261528112,
                    "acc_norm": 0.8827160493827161,
                    "acc_norm_stderr": 0.01790311261528112,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.6702127659574468,
                    "acc_stderr": 0.02804594694204241,
                    "acc_norm": 0.6702127659574468,
                    "acc_norm_stderr": 0.02804594694204241,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.6049543676662321,
                    "acc_stderr": 0.01248572781325157,
                    "acc_norm": 0.6049543676662321,
                    "acc_norm_stderr": 0.01248572781325157,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.8125,
                    "acc_stderr": 0.023709788253811766,
                    "acc_norm": 0.8125,
                    "acc_norm_stderr": 0.023709788253811766,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.8186274509803921,
                    "acc_stderr": 0.015588643495370457,
                    "acc_norm": 0.8186274509803921,
                    "acc_norm_stderr": 0.015588643495370457,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.7363636363636363,
                    "acc_stderr": 0.04220224692971987,
                    "acc_norm": 0.7363636363636363,
                    "acc_norm_stderr": 0.04220224692971987,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.8448979591836735,
                    "acc_stderr": 0.0231747988612186,
                    "acc_norm": 0.8448979591836735,
                    "acc_norm_stderr": 0.0231747988612186,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8905472636815921,
                    "acc_stderr": 0.022076326101824657,
                    "acc_norm": 0.8905472636815921,
                    "acc_norm_stderr": 0.022076326101824657,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.9,
                    "acc_stderr": 0.030151134457776334,
                    "acc_norm": 0.9,
                    "acc_norm_stderr": 0.030151134457776334,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5783132530120482,
                    "acc_stderr": 0.038444531817709175,
                    "acc_norm": 0.5783132530120482,
                    "acc_norm_stderr": 0.038444531817709175,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8771929824561403,
                    "acc_stderr": 0.02517298435015578,
                    "acc_norm": 0.8771929824561403,
                    "acc_norm_stderr": 0.02517298435015578,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.40758873929008566,
                    "mc1_stderr": 0.017201949234553107,
                    "mc2": 0.5623083932983032,
                    "mc2_stderr": 0.015165963671039869,
                    "timestamp": "2023-11-02T14-57-09.483407"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.6081166107382551,
                    "acc_stderr": 0.004999326629880105,
                    "f1": 0.6419882550335565,
                    "f1_stderr": 0.004748239351156368,
                    "timestamp": "2023-11-08T19-46-38.378007"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.6557998483699773,
                    "acc_stderr": 0.013086800426693782,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.8287292817679558,
                    "acc_stderr": 0.010588417294962526,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.8654970760233918,
                    "acc_stderr": 0.026168221344662294,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.5555555555555556,
                    "acc_stderr": 0.04444444444444449,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.8734567901234568,
                    "acc_stderr": 0.018498600558790917,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.6458100558659218,
                    "acc_stderr": 0.015995644947299225,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.9240506329113924,
                    "acc_stderr": 0.017244633251065688,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.8323699421965318,
                    "acc_stderr": 0.02011057991973484,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.5971316818774446,
                    "acc_stderr": 0.01252695557711801,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.8834355828220859,
                    "acc_stderr": 0.025212327210507094,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.9117647058823529,
                    "acc_stderr": 0.019907399791316942,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.8392282958199357,
                    "acc_stderr": 0.02086238808239191,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.8981481481481481,
                    "acc_stderr": 0.02923927267563274,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.9173553719008265,
                    "acc_stderr": 0.02513538235660422,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.8666666666666667,
                    "acc_stderr": 0.026544435312706473,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.9792746113989638,
                    "acc_stderr": 0.010281417011909025,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.8529411764705882,
                    "acc_stderr": 0.023005459446673957,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.8888888888888888,
                    "acc_stderr": 0.022390787638216763,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.9155963302752294,
                    "acc_stderr": 0.011918819327334889,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.7363636363636363,
                    "acc_stderr": 0.04220224692971987,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.91,
                    "acc_stderr": 0.028762349126466125,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.8855721393034826,
                    "acc_stderr": 0.022509345325101703,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.7871794871794872,
                    "acc_stderr": 0.020752423722128013,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.8530612244897959,
                    "acc_stderr": 0.02266540041721763,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.8235294117647058,
                    "acc_stderr": 0.015422512066262547,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.8625954198473282,
                    "acc_stderr": 0.030194823996804475,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.5526315789473685,
                    "acc_stderr": 0.04677473004491199,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.9054916985951469,
                    "acc_stderr": 0.010461015338193071,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.9273504273504274,
                    "acc_stderr": 0.017004368568132353,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.912621359223301,
                    "acc_stderr": 0.027960689125970658,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.869281045751634,
                    "acc_stderr": 0.019301873624215298,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.87,
                    "acc_stderr": 0.03379976689896309,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.7892376681614349,
                    "acc_stderr": 0.027373095500540186,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.8235294117647058,
                    "acc_stderr": 0.02315746830855936,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.7052023121387283,
                    "acc_stderr": 0.03476599607516478,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.79,
                    "acc_stderr": 0.040936018074033256,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.7924528301886793,
                    "acc_stderr": 0.02495991802891127,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.572289156626506,
                    "acc_stderr": 0.038515976837185335,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.6631205673758865,
                    "acc_stderr": 0.028195534873966734,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.4803921568627451,
                    "acc_stderr": 0.04971358884367406,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.5231788079470199,
                    "acc_stderr": 0.04078093859163084,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.8806451612903226,
                    "acc_stderr": 0.01844341132531541,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.875,
                    "acc_stderr": 0.02765610492929436,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.7481481481481481,
                    "acc_stderr": 0.037498507091740206,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.03861229196653695,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.64,
                    "acc_stderr": 0.04824181513244218,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.9013157894736842,
                    "acc_stderr": 0.024270227737522732,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.774468085106383,
                    "acc_stderr": 0.02732107841738753,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.44,
                    "acc_stderr": 0.04988876515698589,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.82,
                    "acc_stderr": 0.038612291966536934,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.6160714285714286,
                    "acc_stderr": 0.04616143075028547,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.6403940886699507,
                    "acc_stderr": 0.03376458246509567,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.6481481481481481,
                    "acc_stderr": 0.032568505702936464,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.6507936507936508,
                    "acc_stderr": 0.02455229220934265,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.033333333333333305,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.4666666666666667,
                    "acc_stderr": 0.03041771696171748,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.6049488054607508,
                    "acc_stderr": 0.014285898292938163,
                    "acc_norm": 0.6459044368600683,
                    "acc_norm_stderr": 0.013975454122756555,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.562494191768382,
                    "acc_stderr": 0.015156214731267605,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 20.330647080409832,
                    "bleu_max_stderr": 0.7024796684315535,
                    "bleu_acc": 0.48714810281517745,
                    "bleu_acc_stderr": 0.01749771794429982,
                    "bleu_diff": 1.2034859228193233,
                    "bleu_diff_stderr": 0.588166050547877,
                    "rouge1_max": 45.71688577672627,
                    "rouge1_max_stderr": 0.8227479818577124,
                    "rouge1_acc": 0.5140758873929009,
                    "rouge1_acc_stderr": 0.017496563717042796,
                    "rouge1_diff": 2.5276609445648797,
                    "rouge1_diff_stderr": 0.781731669558233,
                    "rouge2_max": 29.910346423305118,
                    "rouge2_max_stderr": 0.9393337389294315,
                    "rouge2_acc": 0.3769889840881273,
                    "rouge2_acc_stderr": 0.016965517578930358,
                    "rouge2_diff": 0.485430202097819,
                    "rouge2_diff_stderr": 0.8945392225724594,
                    "rougeL_max": 42.9851750782942,
                    "rougeL_max_stderr": 0.8216423235693433,
                    "rougeL_acc": 0.5361077111383109,
                    "rougeL_acc_stderr": 0.017457800422268622,
                    "rougeL_diff": 2.5484074342195404,
                    "rougeL_diff_stderr": 0.7888693977638205,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.40758873929008566,
                    "acc_stderr": 0.01720194923455311,
                    "timestamp": "2024-11-27T03-38-49.243482"
                }
            }
        }
    }
}