{
    "model_name": "tiiuae/falcon-7b",
    "last_updated": "2023-07-19",
    "results": {
        "harness": {
            "gsm8k": {
                "5-shot": {
                    "acc": 0.04624715693707354,
                    "acc_stderr": 0.005784991662691836,
                    "timestamp": "2023-12-03T17-58-16.188347"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.0010486577181208054,
                    "em_stderr": 0.00033145814652193653,
                    "f1": 0.04824664429530208,
                    "f1_stderr": 0.0012232481165562455,
                    "timestamp": "2023-09-17T10-06-45.584443"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7237569060773481,
                    "acc_stderr": 0.01256681501569816,
                    "timestamp": "2023-09-17T10-06-45.584443"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.43600682593856654,
                    "acc_stderr": 0.014491225699230914,
                    "acc_norm": 0.4786689419795222,
                    "acc_norm_stderr": 0.014598087973127102,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5797649870543716,
                    "acc_stderr": 0.004925877705771197,
                    "acc_norm": 0.7813184624576778,
                    "acc_norm_stderr": 0.004125072816630354,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.03455473702325438,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.03455473702325438,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.24342105263157895,
                    "acc_stderr": 0.034923496688842384,
                    "acc_norm": 0.24342105263157895,
                    "acc_norm_stderr": 0.034923496688842384,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2981132075471698,
                    "acc_stderr": 0.028152837942493868,
                    "acc_norm": 0.2981132075471698,
                    "acc_norm_stderr": 0.028152837942493868,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2361111111111111,
                    "acc_stderr": 0.03551446610810826,
                    "acc_norm": 0.2361111111111111,
                    "acc_norm_stderr": 0.03551446610810826,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.24855491329479767,
                    "acc_stderr": 0.03295304696818318,
                    "acc_norm": 0.24855491329479767,
                    "acc_norm_stderr": 0.03295304696818318,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.04092563958237656,
                    "acc_norm": 0.21568627450980393,
                    "acc_norm_stderr": 0.04092563958237656,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542128,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542128,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2851063829787234,
                    "acc_stderr": 0.029513196625539355,
                    "acc_norm": 0.2851063829787234,
                    "acc_norm_stderr": 0.029513196625539355,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.04142439719489362,
                    "acc_norm": 0.2631578947368421,
                    "acc_norm_stderr": 0.04142439719489362,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2896551724137931,
                    "acc_stderr": 0.03780019230438015,
                    "acc_norm": 0.2896551724137931,
                    "acc_norm_stderr": 0.03780019230438015,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.24867724867724866,
                    "acc_stderr": 0.022261817692400168,
                    "acc_norm": 0.24867724867724866,
                    "acc_norm_stderr": 0.022261817692400168,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.21428571428571427,
                    "acc_stderr": 0.03670066451047181,
                    "acc_norm": 0.21428571428571427,
                    "acc_norm_stderr": 0.03670066451047181,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.25161290322580643,
                    "acc_stderr": 0.02468597928623996,
                    "acc_norm": 0.25161290322580643,
                    "acc_norm_stderr": 0.02468597928623996,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2512315270935961,
                    "acc_stderr": 0.030516530732694433,
                    "acc_norm": 0.2512315270935961,
                    "acc_norm_stderr": 0.030516530732694433,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.24242424242424243,
                    "acc_stderr": 0.03346409881055953,
                    "acc_norm": 0.24242424242424243,
                    "acc_norm_stderr": 0.03346409881055953,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.19696969696969696,
                    "acc_stderr": 0.028335609732463348,
                    "acc_norm": 0.19696969696969696,
                    "acc_norm_stderr": 0.028335609732463348,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.25906735751295334,
                    "acc_stderr": 0.03161877917935411,
                    "acc_norm": 0.25906735751295334,
                    "acc_norm_stderr": 0.03161877917935411,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.24358974358974358,
                    "acc_stderr": 0.021763733684173926,
                    "acc_norm": 0.24358974358974358,
                    "acc_norm_stderr": 0.021763733684173926,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26296296296296295,
                    "acc_stderr": 0.026842057873833706,
                    "acc_norm": 0.26296296296296295,
                    "acc_norm_stderr": 0.026842057873833706,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.27310924369747897,
                    "acc_stderr": 0.028942004040998167,
                    "acc_norm": 0.27310924369747897,
                    "acc_norm_stderr": 0.028942004040998167,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31125827814569534,
                    "acc_stderr": 0.03780445850526733,
                    "acc_norm": 0.31125827814569534,
                    "acc_norm_stderr": 0.03780445850526733,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.23486238532110093,
                    "acc_stderr": 0.01817511051034357,
                    "acc_norm": 0.23486238532110093,
                    "acc_norm_stderr": 0.01817511051034357,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.1527777777777778,
                    "acc_stderr": 0.02453632602613422,
                    "acc_norm": 0.1527777777777778,
                    "acc_norm_stderr": 0.02453632602613422,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.31862745098039214,
                    "acc_stderr": 0.03270287181482079,
                    "acc_norm": 0.31862745098039214,
                    "acc_norm_stderr": 0.03270287181482079,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.26582278481012656,
                    "acc_stderr": 0.028756799629658342,
                    "acc_norm": 0.26582278481012656,
                    "acc_norm_stderr": 0.028756799629658342,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.45739910313901344,
                    "acc_stderr": 0.033435777055830646,
                    "acc_norm": 0.45739910313901344,
                    "acc_norm_stderr": 0.033435777055830646,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2900763358778626,
                    "acc_stderr": 0.03980066246467765,
                    "acc_norm": 0.2900763358778626,
                    "acc_norm_stderr": 0.03980066246467765,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.2396694214876033,
                    "acc_stderr": 0.038968789850704164,
                    "acc_norm": 0.2396694214876033,
                    "acc_norm_stderr": 0.038968789850704164,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.3148148148148148,
                    "acc_stderr": 0.04489931073591311,
                    "acc_norm": 0.3148148148148148,
                    "acc_norm_stderr": 0.04489931073591311,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.25766871165644173,
                    "acc_stderr": 0.03436150827846917,
                    "acc_norm": 0.25766871165644173,
                    "acc_norm_stderr": 0.03436150827846917,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.38392857142857145,
                    "acc_stderr": 0.04616143075028547,
                    "acc_norm": 0.38392857142857145,
                    "acc_norm_stderr": 0.04616143075028547,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2524271844660194,
                    "acc_stderr": 0.04301250399690875,
                    "acc_norm": 0.2524271844660194,
                    "acc_norm_stderr": 0.04301250399690875,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.31196581196581197,
                    "acc_stderr": 0.030351527323344948,
                    "acc_norm": 0.31196581196581197,
                    "acc_norm_stderr": 0.030351527323344948,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.04560480215720683,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.04560480215720683,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.3065134099616858,
                    "acc_stderr": 0.016486952893041515,
                    "acc_norm": 0.3065134099616858,
                    "acc_norm_stderr": 0.016486952893041515,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.28901734104046245,
                    "acc_stderr": 0.024405173935783238,
                    "acc_norm": 0.28901734104046245,
                    "acc_norm_stderr": 0.024405173935783238,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.23910614525139665,
                    "acc_stderr": 0.01426555419233115,
                    "acc_norm": 0.23910614525139665,
                    "acc_norm_stderr": 0.01426555419233115,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.28104575163398693,
                    "acc_stderr": 0.025738854797818716,
                    "acc_norm": 0.28104575163398693,
                    "acc_norm_stderr": 0.025738854797818716,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.2958199356913183,
                    "acc_stderr": 0.025922371788818777,
                    "acc_norm": 0.2958199356913183,
                    "acc_norm_stderr": 0.025922371788818777,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.32098765432098764,
                    "acc_stderr": 0.025976566010862737,
                    "acc_norm": 0.32098765432098764,
                    "acc_norm_stderr": 0.025976566010862737,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.30141843971631205,
                    "acc_stderr": 0.02737412888263116,
                    "acc_norm": 0.30141843971631205,
                    "acc_norm_stderr": 0.02737412888263116,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.24967405475880053,
                    "acc_stderr": 0.011054538377832325,
                    "acc_norm": 0.24967405475880053,
                    "acc_norm_stderr": 0.011054538377832325,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.27941176470588236,
                    "acc_stderr": 0.027257202606114948,
                    "acc_norm": 0.27941176470588236,
                    "acc_norm_stderr": 0.027257202606114948,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2679738562091503,
                    "acc_stderr": 0.017917974069594722,
                    "acc_norm": 0.2679738562091503,
                    "acc_norm_stderr": 0.017917974069594722,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.35454545454545455,
                    "acc_stderr": 0.045820048415054174,
                    "acc_norm": 0.35454545454545455,
                    "acc_norm_stderr": 0.045820048415054174,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.2693877551020408,
                    "acc_stderr": 0.02840125202902294,
                    "acc_norm": 0.2693877551020408,
                    "acc_norm_stderr": 0.02840125202902294,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.35323383084577115,
                    "acc_stderr": 0.03379790611796777,
                    "acc_norm": 0.35323383084577115,
                    "acc_norm_stderr": 0.03379790611796777,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001975,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.04902071300001975,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3795180722891566,
                    "acc_stderr": 0.03777798822748017,
                    "acc_norm": 0.3795180722891566,
                    "acc_norm_stderr": 0.03777798822748017,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.34502923976608185,
                    "acc_stderr": 0.036459813773888065,
                    "acc_norm": 0.34502923976608185,
                    "acc_norm_stderr": 0.036459813773888065,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.22399020807833536,
                    "mc1_stderr": 0.014594964329474202,
                    "mc2": 0.34263825539848,
                    "mc2_stderr": 0.01327555829964236,
                    "timestamp": "2023-07-19T10-51-47.706539"
                }
            }
        }
    }
}