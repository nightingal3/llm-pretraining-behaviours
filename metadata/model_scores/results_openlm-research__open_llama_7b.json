{
    "model_name": "openlm-research/open_llama_7b",
    "last_updated": "2024-12-19 13:40:38.142595",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.027472527472527472,
                "exact_match_stderr": 0.007001675776712246,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.05625717566016074,
                "exact_match_stderr": 0.0078118908661403115,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.016666666666666666,
                "exact_match_stderr": 0.0055141728150896,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.017718715393133997,
                "exact_match_stderr": 0.0043926922934928985,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "minerva_math_geometry": {
                "exact_match": 0.025052192066805846,
                "exact_match_stderr": 0.007148247838013856,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.035864978902953586,
                "exact_match_stderr": 0.008550148322989818,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "minerva_math_algebra": {
                "exact_match": 0.03369839932603201,
                "exact_match_stderr": 0.005239847423284803,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_3da": {
                "acc": 0.578,
                "acc_stderr": 0.011046221503516772,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_3ds": {
                "acc": 0.387,
                "acc_stderr": 0.010893798117218193,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_4da": {
                "acc": 0.328,
                "acc_stderr": 0.010500625294037556,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_2ds": {
                "acc": 0.471,
                "acc_stderr": 0.01116431014037372,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_5ds": {
                "acc": 0.1505,
                "acc_stderr": 0.007997302884517568,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_5da": {
                "acc": 0.1685,
                "acc_stderr": 0.00837191253297178,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_1dc": {
                "acc": 0.066,
                "acc_stderr": 0.005553144938623083,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_4ds": {
                "acc": 0.2865,
                "acc_stderr": 0.010112368911511354,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_2dm": {
                "acc": 0.167,
                "acc_stderr": 0.008342079785495497,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "arithmetic_2da": {
                "acc": 0.786,
                "acc_stderr": 0.0091730077965745,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "gsm8k_cot": {
                "exact_match": 0.060652009097801364,
                "exact_match_stderr": 0.0065747333814058185,
                "timestamp": "2024-06-07T07-41-33.524912"
            },
            "gsm8k": {
                "exact_match": 0.04700530705079606,
                "exact_match_stderr": 0.005829898355937191,
                "timestamp": "2024-11-19T19-17-59.185280"
            },
            "anli_r2": {
                "brier_score": 0.7315652755987271,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "anli_r3": {
                "brier_score": 0.7126834543344663,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "anli_r1": {
                "brier_score": 0.7524228409842385,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_eu": {
                "brier_score": 1.0796208109080396,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_vi": {
                "brier_score": 0.9749295501186998,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_ru": {
                "brier_score": 0.9068002030674046,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_zh": {
                "brier_score": 0.9572324638810987,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_tr": {
                "brier_score": 0.8002787399839949,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_fr": {
                "brier_score": 0.7601606558680033,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_en": {
                "brier_score": 0.6418456490546002,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_ur": {
                "brier_score": 1.3165241257894102,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_ar": {
                "brier_score": 1.0829988270046658,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_de": {
                "brier_score": 0.8950139994287923,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_hi": {
                "brier_score": 0.8444982412224025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_es": {
                "brier_score": 0.8538788249083591,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_bg": {
                "brier_score": 0.9184994666520818,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_sw": {
                "brier_score": 0.8773492580411928,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_el": {
                "brier_score": 0.9607628544532029,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "xnli_th": {
                "brier_score": 0.9321633089585535,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "logiqa2": {
                "brier_score": 1.0022188312341742,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "mathqa": {
                "brier_score": 0.9390200290590519,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T07-55-16.110596"
            },
            "lambada_standard": {
                "perplexity": 5.162686665204201,
                "perplexity_stderr": 0.11470587722922684,
                "acc": 0.6374927226858141,
                "acc_stderr": 0.00669742799676281,
                "timestamp": "2024-06-07T07-56-45.755035"
            },
            "lambada_openai": {
                "perplexity": 3.96851611110645,
                "perplexity_stderr": 0.08521608041380699,
                "acc": 0.7036677663496992,
                "acc_stderr": 0.0063618781796918695,
                "timestamp": "2024-06-07T07-56-45.755035"
            },
            "mmlu_world_religions": {
                "acc": 0.38011695906432746,
                "acc_stderr": 0.037229657413855394,
                "brier_score": 0.7115781518397424,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_formal_logic": {
                "acc": 0.23015873015873015,
                "acc_stderr": 0.037649508797906066,
                "brier_score": 0.7582250938895074,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_prehistory": {
                "acc": 0.28703703703703703,
                "acc_stderr": 0.02517104191530968,
                "brier_score": 0.7433593871803341,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.01442229220480884,
                "brier_score": 0.7733353691881525,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.3206751054852321,
                "acc_stderr": 0.030381931949990407,
                "brier_score": 0.7400741865893943,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_moral_disputes": {
                "acc": 0.3208092485549133,
                "acc_stderr": 0.025131000233647886,
                "brier_score": 0.7367835273826375,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_professional_law": {
                "acc": 0.24967405475880053,
                "acc_stderr": 0.011054538377832303,
                "brier_score": 0.7541500718985059,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.27607361963190186,
                "acc_stderr": 0.0351238528370505,
                "brier_score": 0.7540611090931192,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.31862745098039214,
                "acc_stderr": 0.032702871814820796,
                "brier_score": 0.737559011238905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_philosophy": {
                "acc": 0.2958199356913183,
                "acc_stderr": 0.025922371788818774,
                "brier_score": 0.7431260488193858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_jurisprudence": {
                "acc": 0.3055555555555556,
                "acc_stderr": 0.044531975073749834,
                "brier_score": 0.7462423092896351,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_international_law": {
                "acc": 0.3305785123966942,
                "acc_stderr": 0.04294340845212094,
                "brier_score": 0.7250984481128241,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.30303030303030304,
                "acc_stderr": 0.03588624800091707,
                "brier_score": 0.7400033896016402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.35751295336787564,
                "acc_stderr": 0.03458816042181004,
                "brier_score": 0.7329061845026962,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.29831932773109243,
                "acc_stderr": 0.029719142876342856,
                "brier_score": 0.7411659475083163,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_geography": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.03358618145732524,
                "brier_score": 0.7393613997273552,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.25137614678899084,
                "acc_stderr": 0.01859920636028741,
                "brier_score": 0.7567749368816743,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_public_relations": {
                "acc": 0.3090909090909091,
                "acc_stderr": 0.044262946482000985,
                "brier_score": 0.7349220211468389,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.4,
                "acc_stderr": 0.04923659639173309,
                "brier_score": 0.7344245727291141,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_sociology": {
                "acc": 0.26865671641791045,
                "acc_stderr": 0.031343283582089536,
                "brier_score": 0.7514521170912215,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.30256410256410254,
                "acc_stderr": 0.02329088805377273,
                "brier_score": 0.7377887604031166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_security_studies": {
                "acc": 0.2938775510204082,
                "acc_stderr": 0.029162738410249786,
                "brier_score": 0.7441710476153711,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_professional_psychology": {
                "acc": 0.32679738562091504,
                "acc_stderr": 0.01897542792050722,
                "brier_score": 0.7451067887273528,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "brier_score": 0.7491208222192818,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_econometrics": {
                "acc": 0.32456140350877194,
                "acc_stderr": 0.044045561573747685,
                "brier_score": 0.7515400174292252,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_miscellaneous": {
                "acc": 0.36015325670498083,
                "acc_stderr": 0.017166362471369295,
                "brier_score": 0.7287497853643151,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_marketing": {
                "acc": 0.31196581196581197,
                "acc_stderr": 0.03035152732334495,
                "brier_score": 0.7373180884734252,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_management": {
                "acc": 0.21359223300970873,
                "acc_stderr": 0.040580420156460344,
                "brier_score": 0.7665639456583346,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_nutrition": {
                "acc": 0.2875816993464052,
                "acc_stderr": 0.02591780611714716,
                "brier_score": 0.7541034532974272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7361997183161302,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_human_aging": {
                "acc": 0.3632286995515695,
                "acc_stderr": 0.03227790442850499,
                "brier_score": 0.7280599282684578,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_professional_medicine": {
                "acc": 0.16911764705882354,
                "acc_stderr": 0.022770868010113018,
                "brier_score": 0.76751002970653,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_college_medicine": {
                "acc": 0.26011560693641617,
                "acc_stderr": 0.03345036916788991,
                "brier_score": 0.7487159013561581,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_business_ethics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "brier_score": 0.7461751890143767,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.32452830188679244,
                "acc_stderr": 0.028815615713432104,
                "brier_score": 0.741708963885023,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_global_facts": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "brier_score": 0.728265985349061,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_virology": {
                "acc": 0.3072289156626506,
                "acc_stderr": 0.035915667978246635,
                "brier_score": 0.7483456929513479,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.026684564340460987,
                "brier_score": 0.753348568316132,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_college_physics": {
                "acc": 0.11764705882352941,
                "acc_stderr": 0.03205907733144526,
                "brier_score": 0.7722211942164457,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_physics": {
                "acc": 0.31788079470198677,
                "acc_stderr": 0.03802039760107903,
                "brier_score": 0.7402329444666135,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_biology": {
                "acc": 0.3096774193548387,
                "acc_stderr": 0.026302774983517418,
                "brier_score": 0.7455413082159084,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_college_biology": {
                "acc": 0.2708333333333333,
                "acc_stderr": 0.03716177437566018,
                "brier_score": 0.7436999721198967,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_anatomy": {
                "acc": 0.28888888888888886,
                "acc_stderr": 0.0391545063041425,
                "brier_score": 0.7370266037272393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_college_chemistry": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909282,
                "brier_score": 0.7552559560723006,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_computer_security": {
                "acc": 0.37,
                "acc_stderr": 0.048523658709391,
                "brier_score": 0.7338082445157076,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_college_computer_science": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "brier_score": 0.7521525063788139,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_astronomy": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.037827289808654685,
                "brier_score": 0.750488644502129,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_college_mathematics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7377399289135675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.30638297872340425,
                "acc_stderr": 0.030135906478517563,
                "brier_score": 0.7397898067329776,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7618169165467457,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.33,
                "acc_stderr": 0.047258156262526045,
                "brier_score": 0.7296516482112015,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_machine_learning": {
                "acc": 0.1875,
                "acc_stderr": 0.0370468111477387,
                "brier_score": 0.7644975489531513,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2512315270935961,
                "acc_stderr": 0.030516530732694436,
                "brier_score": 0.7448182748498786,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.03054674526495318,
                "brier_score": 0.748414786538635,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.022569897074918424,
                "brier_score": 0.75592354186923,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.27586206896551724,
                "acc_stderr": 0.037245636197746325,
                "brier_score": 0.744755125515667,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.29259259259259257,
                "acc_stderr": 0.027738969632176088,
                "brier_score": 0.7486140129083726,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-45.007907"
            },
            "arc_challenge": {
                "acc": 0.4334470989761092,
                "acc_stderr": 0.014481376224558902,
                "acc_norm": 0.4735494880546075,
                "acc_norm_stderr": 0.014590931358120169,
                "timestamp": "2024-11-19T19-17-59.185280"
            },
            "hellaswag": {
                "acc": 0.5382393945429197,
                "acc_stderr": 0.004975167382061824,
                "acc_norm": 0.7168890659231228,
                "acc_norm_stderr": 0.0044958914405194466,
                "timestamp": "2024-11-19T19-17-59.185280"
            },
            "truthfulqa_mc2": {
                "acc": 0.3514211609144709,
                "acc_stderr": 0.01355096005061055,
                "timestamp": "2024-11-19T19-17-59.185280"
            },
            "truthfulqa_gen": {
                "bleu_max": 27.152576174533763,
                "bleu_max_stderr": 0.8095916667785428,
                "bleu_acc": 0.32558139534883723,
                "bleu_acc_stderr": 0.01640398946990785,
                "bleu_diff": -7.3095593587963394,
                "bleu_diff_stderr": 0.886002774295994,
                "rouge1_max": 52.08716441858857,
                "rouge1_max_stderr": 0.8595002597325482,
                "rouge1_acc": 0.3072215422276622,
                "rouge1_acc_stderr": 0.01615020132132304,
                "rouge1_diff": -10.058290429072063,
                "rouge1_diff_stderr": 0.9747973784961548,
                "rouge2_max": 36.143564978969415,
                "rouge2_max_stderr": 1.0316941608267736,
                "rouge2_acc": 0.25458996328029376,
                "rouge2_acc_stderr": 0.015250117079156496,
                "rouge2_diff": -11.837884926202605,
                "rouge2_diff_stderr": 1.164433626396958,
                "rougeL_max": 49.3236769159205,
                "rougeL_max_stderr": 0.8804470956979424,
                "rougeL_acc": 0.2974296205630355,
                "rougeL_acc_stderr": 0.01600265148736098,
                "rougeL_diff": -10.229350835912319,
                "rougeL_diff_stderr": 0.9840130390427738,
                "timestamp": "2024-11-19T19-17-59.185280"
            },
            "truthfulqa_mc1": {
                "acc": 0.23133414932680538,
                "acc_stderr": 0.014761945174862658,
                "timestamp": "2024-11-19T19-17-59.185280"
            },
            "winogrande": {
                "acc": 0.6858721389108129,
                "acc_stderr": 0.01304541671607257,
                "timestamp": "2024-11-19T19-17-59.185280"
            }
        }
    }
}