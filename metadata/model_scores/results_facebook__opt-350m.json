{
    "model_name": "facebook/opt-350m",
    "last_updated": "2024-12-04 11:25:31.530440",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.003663003663003663,
                "exact_match_stderr": 0.0025877573681934458,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.021814006888633754,
                "exact_match_stderr": 0.00495243536887352,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0033222591362126247,
                "exact_match_stderr": 0.0019159795218657448,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0020876826722338203,
                "exact_match_stderr": 0.0020876826722338315,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.002109704641350211,
                "exact_match_stderr": 0.0021097046413502104,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "minerva_math_algebra": {
                "exact_match": 0.015164279696714406,
                "exact_match_stderr": 0.0035485460431325393,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_4da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_2ds": {
                "acc": 0.0095,
                "acc_stderr": 0.00216961485391003,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_1dc": {
                "acc": 0.002,
                "acc_stderr": 0.0009992493430694997,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_2dm": {
                "acc": 0.0185,
                "acc_stderr": 0.0030138707185866456,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "arithmetic_2da": {
                "acc": 0.0045,
                "acc_stderr": 0.0014969954902233223,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "gsm8k_cot": {
                "exact_match": 0.016679302501895376,
                "exact_match_stderr": 0.0035275958887224556,
                "timestamp": "2024-06-06T04-30-26.898550"
            },
            "gsm8k": {
                "exact_match": 0.01819560272934041,
                "exact_match_stderr": 0.003681611894073872,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "anli_r2": {
                "brier_score": 0.8622047830185462,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "anli_r3": {
                "brier_score": 0.8573734867626938,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "anli_r1": {
                "brier_score": 0.875763881825301,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_eu": {
                "brier_score": 1.2350745588223613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_vi": {
                "brier_score": 1.187962184278827,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_ru": {
                "brier_score": 0.9510442144388731,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_zh": {
                "brier_score": 1.2604084317893178,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_tr": {
                "brier_score": 0.8578078728939282,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_fr": {
                "brier_score": 0.8643029753225795,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_en": {
                "brier_score": 0.7004467657014397,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_ur": {
                "brier_score": 1.294615717341209,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_ar": {
                "brier_score": 0.9077213776188938,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_de": {
                "brier_score": 0.9042343874682501,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_hi": {
                "brier_score": 1.2717666974720738,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_es": {
                "brier_score": 0.9307819515890197,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_bg": {
                "brier_score": 1.2325643903521082,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_sw": {
                "brier_score": 1.141700829535198,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_el": {
                "brier_score": 1.0571822860452535,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "xnli_th": {
                "brier_score": 1.294957782543984,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "logiqa2": {
                "brier_score": 1.1601959747659683,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "mathqa": {
                "brier_score": 1.0103387666530812,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T06-40-01.162162"
            },
            "lambada_standard": {
                "perplexity": 38.15763443103525,
                "perplexity_stderr": 1.5528711068483714,
                "acc": 0.3578497962352028,
                "acc_stderr": 0.00667852832588857,
                "timestamp": "2024-06-06T08-49-49.969484"
            },
            "lambada_openai": {
                "perplexity": 16.398847797098593,
                "perplexity_stderr": 0.555542932914263,
                "acc": 0.4513875412381137,
                "acc_stderr": 0.00693297588836862,
                "timestamp": "2024-06-06T08-49-49.969484"
            },
            "mmlu_world_religions": {
                "acc": 0.18128654970760233,
                "acc_stderr": 0.02954774168764002,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_formal_logic": {
                "acc": 0.23015873015873015,
                "acc_stderr": 0.03764950879790605,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_prehistory": {
                "acc": 0.2716049382716049,
                "acc_stderr": 0.024748624490537382,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24022346368715083,
                "acc_stderr": 0.014288343803925307,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.20675105485232068,
                "acc_stderr": 0.0263616516683891,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26878612716763006,
                "acc_stderr": 0.023868003262500107,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_professional_law": {
                "acc": 0.23989569752281617,
                "acc_stderr": 0.010906282617981634,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.24539877300613497,
                "acc_stderr": 0.03380939813943354,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.03077855467869328,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_philosophy": {
                "acc": 0.18971061093247588,
                "acc_stderr": 0.022268196258783218,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.040191074725573483,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_international_law": {
                "acc": 0.371900826446281,
                "acc_stderr": 0.04412015806624504,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.03453131801885415,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.3160621761658031,
                "acc_stderr": 0.03355397369686173,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.2773109243697479,
                "acc_stderr": 0.029079374539480007,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_geography": {
                "acc": 0.35858585858585856,
                "acc_stderr": 0.03416903640391521,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.3431192660550459,
                "acc_stderr": 0.02035477773608604,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_public_relations": {
                "acc": 0.20909090909090908,
                "acc_stderr": 0.038950910157241364,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_sociology": {
                "acc": 0.2537313432835821,
                "acc_stderr": 0.030769444967296004,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2794871794871795,
                "acc_stderr": 0.022752388839776826,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_security_studies": {
                "acc": 0.4,
                "acc_stderr": 0.031362502409358936,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_professional_psychology": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.017160587235046345,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_human_sexuality": {
                "acc": 0.26717557251908397,
                "acc_stderr": 0.038808483010823944,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_econometrics": {
                "acc": 0.22807017543859648,
                "acc_stderr": 0.03947152782669415,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_miscellaneous": {
                "acc": 0.20945083014048532,
                "acc_stderr": 0.0145513105681437,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_marketing": {
                "acc": 0.20512820512820512,
                "acc_stderr": 0.026453508054040332,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_management": {
                "acc": 0.1941747572815534,
                "acc_stderr": 0.03916667762822584,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_nutrition": {
                "acc": 0.2581699346405229,
                "acc_stderr": 0.025058503316958147,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_medical_genetics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_human_aging": {
                "acc": 0.16143497757847533,
                "acc_stderr": 0.024693957899128472,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4485294117647059,
                "acc_stderr": 0.030211479609121593,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.03242414757483098,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_business_ethics": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.26037735849056604,
                "acc_stderr": 0.02700876609070808,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_global_facts": {
                "acc": 0.19,
                "acc_stderr": 0.039427724440366234,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_virology": {
                "acc": 0.18674698795180722,
                "acc_stderr": 0.030338749144500594,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_professional_accounting": {
                "acc": 0.24468085106382978,
                "acc_stderr": 0.025645553622266733,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_college_physics": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.04158307533083286,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_physics": {
                "acc": 0.33774834437086093,
                "acc_stderr": 0.03861557546255168,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2967741935483871,
                "acc_stderr": 0.02598850079241189,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_college_biology": {
                "acc": 0.20833333333333334,
                "acc_stderr": 0.033961162058453336,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_anatomy": {
                "acc": 0.26666666666666666,
                "acc_stderr": 0.038201699145179055,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_college_chemistry": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_computer_security": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653697,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_college_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123398,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_college_mathematics": {
                "acc": 0.27,
                "acc_stderr": 0.044619604333847394,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.30638297872340425,
                "acc_stderr": 0.030135906478517563,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036625,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_machine_learning": {
                "acc": 0.23214285714285715,
                "acc_stderr": 0.04007341809755805,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2955665024630542,
                "acc_stderr": 0.032104944337514575,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4722222222222222,
                "acc_stderr": 0.0340470532865388,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2566137566137566,
                "acc_stderr": 0.022494510767503154,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.31724137931034485,
                "acc_stderr": 0.038783523721386236,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.27037037037037037,
                "acc_stderr": 0.027080372815145658,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "arc_challenge": {
                "acc": 0.20733788395904437,
                "acc_stderr": 0.011846905782971335,
                "acc_norm": 0.23976109215017063,
                "acc_norm_stderr": 0.01247630412745394,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "hellaswag": {
                "acc": 0.32304321848237405,
                "acc_stderr": 0.004666833452796162,
                "acc_norm": 0.3694483170683131,
                "acc_norm_stderr": 0.004816690123209759,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "truthfulqa_mc2": {
                "acc": 0.4101397354990324,
                "acc_stderr": 0.014705665829993402,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "truthfulqa_gen": {
                "bleu_max": 16.445503288865865,
                "bleu_max_stderr": 0.5837064468134793,
                "bleu_acc": 0.390452876376989,
                "bleu_acc_stderr": 0.017078230743431462,
                "bleu_diff": -2.0622603654652294,
                "bleu_diff_stderr": 0.549702898772285,
                "rouge1_max": 35.862459464442495,
                "rouge1_max_stderr": 0.8635314811502294,
                "rouge1_acc": 0.2839657282741738,
                "rouge1_acc_stderr": 0.015785370858396756,
                "rouge1_diff": -5.37391209618839,
                "rouge1_diff_stderr": 0.7800863393842812,
                "rouge2_max": 18.920614186005043,
                "rouge2_max_stderr": 0.8752002965989032,
                "rouge2_acc": 0.18115055079559364,
                "rouge2_acc_stderr": 0.013482697187817888,
                "rouge2_diff": -4.905442542549504,
                "rouge2_diff_stderr": 0.7931073647795669,
                "rougeL_max": 33.326011225852476,
                "rougeL_max_stderr": 0.8434189971154012,
                "rougeL_acc": 0.2962056303549572,
                "rougeL_acc_stderr": 0.015983595101811385,
                "rougeL_diff": -4.789359756965765,
                "rougeL_diff_stderr": 0.7670328005263544,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "truthfulqa_mc1": {
                "acc": 0.2350061199510404,
                "acc_stderr": 0.014843061507731606,
                "timestamp": "2024-11-22T15-23-45.050539"
            },
            "winogrande": {
                "acc": 0.5130228887134964,
                "acc_stderr": 0.014047718393997667,
                "timestamp": "2024-11-22T15-23-45.050539"
            }
        }
    }
}