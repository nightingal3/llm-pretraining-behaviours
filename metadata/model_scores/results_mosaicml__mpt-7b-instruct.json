{
    "model_name": "mosaicml/mpt-7b-instruct",
    "last_updated": "2024-12-19 13:38:36.368712",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.029304029304029304,
                "exact_match_stderr": 0.007224487305459686,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.0677382319173364,
                "exact_match_stderr": 0.008519737992709985,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.044444444444444446,
                "exact_match_stderr": 0.00887651168786704,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.03211517165005537,
                "exact_match_stderr": 0.005870345955796336,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "minerva_math_geometry": {
                "exact_match": 0.05010438413361169,
                "exact_match_stderr": 0.009978421784323616,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.05907172995780591,
                "exact_match_stderr": 0.010840205941655379,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "minerva_math_algebra": {
                "exact_match": 0.040438079191238416,
                "exact_match_stderr": 0.005719912921553342,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_3da": {
                "acc": 0.0505,
                "acc_stderr": 0.004897639067368751,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_3ds": {
                "acc": 0.0445,
                "acc_stderr": 0.004611996341621302,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_4da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521504,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_2ds": {
                "acc": 0.2525,
                "acc_stderr": 0.009716948314273846,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_5da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000026,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_1dc": {
                "acc": 0.0135,
                "acc_stderr": 0.002581124968507327,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_4ds": {
                "acc": 0.002,
                "acc_stderr": 0.00099924934306949,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_2dm": {
                "acc": 0.067,
                "acc_stderr": 0.005592060046868742,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "arithmetic_2da": {
                "acc": 0.1985,
                "acc_stderr": 0.008921248193760107,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "gsm8k_cot": {
                "exact_match": 0.06974981046247157,
                "exact_match_stderr": 0.007016389571013853,
                "timestamp": "2024-06-10T04-54-35.917402"
            },
            "gsm8k": {
                "exact_match": 0.0712661106899166,
                "exact_match_stderr": 0.0070864621279544925,
                "timestamp": "2024-11-07T02-15-50.682989"
            },
            "anli_r2": {
                "brier_score": 0.7896846312992308,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "anli_r3": {
                "brier_score": 0.7829638521132672,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "anli_r1": {
                "brier_score": 0.815948761399128,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_eu": {
                "brier_score": 1.0863038087250385,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_vi": {
                "brier_score": 1.0752130995950049,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_ru": {
                "brier_score": 0.8232916030715194,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_zh": {
                "brier_score": 1.0965246448802113,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_tr": {
                "brier_score": 0.9706318085768089,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_fr": {
                "brier_score": 0.7747493883608759,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_en": {
                "brier_score": 0.6437717014643014,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_ur": {
                "brier_score": 1.31440196299888,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_ar": {
                "brier_score": 1.264810737749815,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_de": {
                "brier_score": 0.8408303311766104,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_hi": {
                "brier_score": 1.0110715039028983,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_es": {
                "brier_score": 0.8694036206904238,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_bg": {
                "brier_score": 0.880489846093007,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_sw": {
                "brier_score": 1.1727384594477277,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_el": {
                "brier_score": 1.037015650900093,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "xnli_th": {
                "brier_score": 0.9186444184570115,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "logiqa2": {
                "brier_score": 1.0058818905060056,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "mathqa": {
                "brier_score": 0.9402290653146276,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-10T05-04-20.841528"
            },
            "lambada_standard": {
                "perplexity": 4.596967497234068,
                "perplexity_stderr": 0.11312431016784812,
                "acc": 0.6262371434116049,
                "acc_stderr": 0.0067403050865517955,
                "timestamp": "2024-06-09T17-10-00.585190"
            },
            "lambada_openai": {
                "perplexity": 3.6623178667968417,
                "perplexity_stderr": 0.08650055440703999,
                "acc": 0.673394139336309,
                "acc_stderr": 0.0065336930212616965,
                "timestamp": "2024-06-09T17-10-00.585190"
            },
            "mmlu_world_religions": {
                "acc": 0.4269005847953216,
                "acc_stderr": 0.03793620616529918,
                "brier_score": 0.7257681021251722,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_formal_logic": {
                "acc": 0.2619047619047619,
                "acc_stderr": 0.039325376803928704,
                "brier_score": 0.7638751497517091,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_prehistory": {
                "acc": 0.35185185185185186,
                "acc_stderr": 0.02657148348071997,
                "brier_score": 0.7306776171304961,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2581005586592179,
                "acc_stderr": 0.014635185616527829,
                "brier_score": 0.7662986423904314,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.33755274261603374,
                "acc_stderr": 0.03078154910202623,
                "brier_score": 0.7421268302824199,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_moral_disputes": {
                "acc": 0.33236994219653176,
                "acc_stderr": 0.02536116874968821,
                "brier_score": 0.7561425416002233,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_professional_law": {
                "acc": 0.3057366362451108,
                "acc_stderr": 0.011766973847072917,
                "brier_score": 0.7542538520161718,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.25766871165644173,
                "acc_stderr": 0.03436150827846917,
                "brier_score": 0.8284424973497988,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.31862745098039214,
                "acc_stderr": 0.032702871814820796,
                "brier_score": 0.7372789272582697,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_philosophy": {
                "acc": 0.39228295819935693,
                "acc_stderr": 0.027731258647012,
                "brier_score": 0.721029565430574,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_jurisprudence": {
                "acc": 0.37962962962962965,
                "acc_stderr": 0.04691521224077742,
                "brier_score": 0.726041299898626,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_international_law": {
                "acc": 0.33884297520661155,
                "acc_stderr": 0.0432076780753667,
                "brier_score": 0.7047682053011297,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.3696969696969697,
                "acc_stderr": 0.03769430314512568,
                "brier_score": 0.7132406579401299,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.46113989637305697,
                "acc_stderr": 0.03597524411734578,
                "brier_score": 0.6654367489833136,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.3739495798319328,
                "acc_stderr": 0.031429466378837076,
                "brier_score": 0.7127620035581791,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_geography": {
                "acc": 0.3686868686868687,
                "acc_stderr": 0.034373055019806184,
                "brier_score": 0.7379190244649863,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.3431192660550459,
                "acc_stderr": 0.02035477773608604,
                "brier_score": 0.7530211886673183,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_public_relations": {
                "acc": 0.39090909090909093,
                "acc_stderr": 0.04673752333670239,
                "brier_score": 0.7280283275482469,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.44,
                "acc_stderr": 0.049888765156985884,
                "brier_score": 0.6790437427997017,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_sociology": {
                "acc": 0.3880597014925373,
                "acc_stderr": 0.0344578996436275,
                "brier_score": 0.6893012864485502,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.023901157979402544,
                "brier_score": 0.7434676816538845,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_security_studies": {
                "acc": 0.42857142857142855,
                "acc_stderr": 0.03168091161233883,
                "brier_score": 0.7022607912797364,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_professional_psychology": {
                "acc": 0.30392156862745096,
                "acc_stderr": 0.018607552131279827,
                "brier_score": 0.7529173430663462,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_human_sexuality": {
                "acc": 0.3511450381679389,
                "acc_stderr": 0.04186445163013751,
                "brier_score": 0.7067376678367041,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_econometrics": {
                "acc": 0.2982456140350877,
                "acc_stderr": 0.04303684033537315,
                "brier_score": 0.8098249857307726,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_miscellaneous": {
                "acc": 0.438058748403576,
                "acc_stderr": 0.017742232238257237,
                "brier_score": 0.6926811262238252,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_marketing": {
                "acc": 0.42735042735042733,
                "acc_stderr": 0.03240847393516327,
                "brier_score": 0.6712686523189929,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_management": {
                "acc": 0.2912621359223301,
                "acc_stderr": 0.04498676320572922,
                "brier_score": 0.7691905852991028,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_nutrition": {
                "acc": 0.31699346405228757,
                "acc_stderr": 0.02664327847450875,
                "brier_score": 0.7576935523056313,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_medical_genetics": {
                "acc": 0.37,
                "acc_stderr": 0.048523658709391,
                "brier_score": 0.7319577359874132,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_human_aging": {
                "acc": 0.42152466367713004,
                "acc_stderr": 0.033141902221106564,
                "brier_score": 0.6962093368385792,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_professional_medicine": {
                "acc": 0.25,
                "acc_stderr": 0.026303648393696036,
                "brier_score": 0.7660407492028689,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_college_medicine": {
                "acc": 0.3063583815028902,
                "acc_stderr": 0.03514942551267437,
                "brier_score": 0.7834385880245865,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_business_ethics": {
                "acc": 0.41,
                "acc_stderr": 0.049431107042371025,
                "brier_score": 0.7095796975594699,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.35094339622641507,
                "acc_stderr": 0.02937364625323469,
                "brier_score": 0.7387591157712322,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_global_facts": {
                "acc": 0.36,
                "acc_stderr": 0.04824181513244218,
                "brier_score": 0.7227671849826451,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_virology": {
                "acc": 0.42168674698795183,
                "acc_stderr": 0.03844453181770917,
                "brier_score": 0.7084750245331242,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_professional_accounting": {
                "acc": 0.31560283687943264,
                "acc_stderr": 0.027724989449509317,
                "brier_score": 0.768002782091526,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_college_physics": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.041583075330832865,
                "brier_score": 0.7977170964249326,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2980132450331126,
                "acc_stderr": 0.037345356767871984,
                "brier_score": 0.7633345278522208,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_biology": {
                "acc": 0.35161290322580646,
                "acc_stderr": 0.027162537826948458,
                "brier_score": 0.7338681018242579,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_college_biology": {
                "acc": 0.3194444444444444,
                "acc_stderr": 0.038990736873573344,
                "brier_score": 0.7382610461426873,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_anatomy": {
                "acc": 0.32592592592592595,
                "acc_stderr": 0.040491220417025055,
                "brier_score": 0.7194668604811008,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_college_chemistry": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768079,
                "brier_score": 0.7800486692301324,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_computer_security": {
                "acc": 0.46,
                "acc_stderr": 0.05009082659620333,
                "brier_score": 0.6713774316992404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_college_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.8094644862958944,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_astronomy": {
                "acc": 0.3355263157894737,
                "acc_stderr": 0.03842498559395268,
                "brier_score": 0.7766442411477616,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_college_mathematics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.7854849117108706,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.3021276595744681,
                "acc_stderr": 0.030017554471880557,
                "brier_score": 0.777047512305051,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.27,
                "acc_stderr": 0.04461960433384741,
                "brier_score": 0.8814857314331535,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.736066824294179,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_machine_learning": {
                "acc": 0.35714285714285715,
                "acc_stderr": 0.04547960999764376,
                "brier_score": 0.779959477294934,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2660098522167488,
                "acc_stderr": 0.03108982600293753,
                "brier_score": 0.7725297810535339,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.031141447823536023,
                "brier_score": 0.7689269329033727,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2857142857142857,
                "acc_stderr": 0.023266512213730564,
                "brier_score": 0.8022422868333021,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.35172413793103446,
                "acc_stderr": 0.0397923663749741,
                "brier_score": 0.7397187719586187,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.22592592592592592,
                "acc_stderr": 0.025497532639609553,
                "brier_score": 0.8140482353582547,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-39-47.488962"
            },
            "arc_challenge": {
                "acc": 0.44795221843003413,
                "acc_stderr": 0.01453201149821167,
                "acc_norm": 0.49658703071672355,
                "acc_norm_stderr": 0.014611050403244084,
                "timestamp": "2024-11-07T02-15-50.682989"
            },
            "hellaswag": {
                "acc": 0.5801633140808604,
                "acc_stderr": 0.004925233680511583,
                "acc_norm": 0.7784305915156343,
                "acc_norm_stderr": 0.004144540263219874,
                "timestamp": "2024-11-07T02-15-50.682989"
            },
            "truthfulqa_mc2": {
                "acc": 0.35155810772535084,
                "acc_stderr": 0.013782411103738377,
                "timestamp": "2024-11-07T02-15-50.682989"
            },
            "truthfulqa_gen": {
                "bleu_max": 22.020255818812327,
                "bleu_max_stderr": 0.7190998542826786,
                "bleu_acc": 0.3072215422276622,
                "bleu_acc_stderr": 0.016150201321323047,
                "bleu_diff": -7.310042815538973,
                "bleu_diff_stderr": 0.7057991080780003,
                "rouge1_max": 45.74195109027613,
                "rouge1_max_stderr": 0.8459789920841821,
                "rouge1_acc": 0.2962056303549572,
                "rouge1_acc_stderr": 0.01598359510181139,
                "rouge1_diff": -10.270084976230358,
                "rouge1_diff_stderr": 0.8292877682668228,
                "rouge2_max": 29.204194016485438,
                "rouge2_max_stderr": 0.9485549352289394,
                "rouge2_acc": 0.22276621787025705,
                "rouge2_acc_stderr": 0.01456650696139674,
                "rouge2_diff": -11.797169376853871,
                "rouge2_diff_stderr": 0.9223640474246654,
                "rougeL_max": 42.95963549103364,
                "rougeL_max_stderr": 0.8505381892909781,
                "rougeL_acc": 0.2864137086903305,
                "rougeL_acc_stderr": 0.015826142439502377,
                "rougeL_diff": -10.523682874537876,
                "rougeL_diff_stderr": 0.8392765224735843,
                "timestamp": "2024-11-07T02-15-50.682989"
            },
            "truthfulqa_mc1": {
                "acc": 0.22643818849449204,
                "acc_stderr": 0.014651337324602576,
                "timestamp": "2024-11-07T02-15-50.682989"
            },
            "winogrande": {
                "acc": 0.7079715864246251,
                "acc_stderr": 0.012779198491754021,
                "timestamp": "2024-11-07T02-15-50.682989"
            }
        }
    }
}