{
    "model_name": "AbacusResearch/haLLAwa3",
    "last_updated": "2024-02-13",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.643344709897611,
                    "acc_stderr": 0.013998056902620197,
                    "acc_norm": 0.6783276450511946,
                    "acc_norm_stderr": 0.013650488084494162,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.7039434375622386,
                    "acc_stderr": 0.004555832462774594,
                    "acc_norm": 0.8702449711212906,
                    "acc_norm_stderr": 0.003353469625027664,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695236,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.04760952285695236,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6148148148148148,
                    "acc_stderr": 0.04203921040156279,
                    "acc_norm": 0.6148148148148148,
                    "acc_norm_stderr": 0.04203921040156279,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.6776315789473685,
                    "acc_stderr": 0.03803510248351585,
                    "acc_norm": 0.6776315789473685,
                    "acc_norm_stderr": 0.03803510248351585,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.6,
                    "acc_stderr": 0.04923659639173309,
                    "acc_norm": 0.6,
                    "acc_norm_stderr": 0.04923659639173309,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.7056603773584905,
                    "acc_stderr": 0.02804918631569525,
                    "acc_norm": 0.7056603773584905,
                    "acc_norm_stderr": 0.02804918631569525,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.7291666666666666,
                    "acc_stderr": 0.037161774375660185,
                    "acc_norm": 0.7291666666666666,
                    "acc_norm_stderr": 0.037161774375660185,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.46,
                    "acc_stderr": 0.05009082659620332,
                    "acc_norm": 0.46,
                    "acc_norm_stderr": 0.05009082659620332,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.55,
                    "acc_stderr": 0.05,
                    "acc_norm": 0.55,
                    "acc_norm_stderr": 0.05,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.6416184971098265,
                    "acc_stderr": 0.036563436533531585,
                    "acc_norm": 0.6416184971098265,
                    "acc_norm_stderr": 0.036563436533531585,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.4019607843137255,
                    "acc_stderr": 0.048786087144669955,
                    "acc_norm": 0.4019607843137255,
                    "acc_norm_stderr": 0.048786087144669955,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.04020151261036845,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.04020151261036845,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.5659574468085107,
                    "acc_stderr": 0.03240038086792747,
                    "acc_norm": 0.5659574468085107,
                    "acc_norm_stderr": 0.03240038086792747,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.4649122807017544,
                    "acc_stderr": 0.046920083813689104,
                    "acc_norm": 0.4649122807017544,
                    "acc_norm_stderr": 0.046920083813689104,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5517241379310345,
                    "acc_stderr": 0.04144311810878152,
                    "acc_norm": 0.5517241379310345,
                    "acc_norm_stderr": 0.04144311810878152,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.4126984126984127,
                    "acc_stderr": 0.025355741263055263,
                    "acc_norm": 0.4126984126984127,
                    "acc_norm_stderr": 0.025355741263055263,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.47619047619047616,
                    "acc_stderr": 0.04467062628403273,
                    "acc_norm": 0.47619047619047616,
                    "acc_norm_stderr": 0.04467062628403273,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7645161290322581,
                    "acc_stderr": 0.02413763242933771,
                    "acc_norm": 0.7645161290322581,
                    "acc_norm_stderr": 0.02413763242933771,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.5024630541871922,
                    "acc_stderr": 0.035179450386910616,
                    "acc_norm": 0.5024630541871922,
                    "acc_norm_stderr": 0.035179450386910616,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.66,
                    "acc_stderr": 0.04760952285695237,
                    "acc_norm": 0.66,
                    "acc_norm_stderr": 0.04760952285695237,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.7575757575757576,
                    "acc_stderr": 0.03346409881055953,
                    "acc_norm": 0.7575757575757576,
                    "acc_norm_stderr": 0.03346409881055953,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.8080808080808081,
                    "acc_stderr": 0.028057791672989017,
                    "acc_norm": 0.8080808080808081,
                    "acc_norm_stderr": 0.028057791672989017,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.8860103626943006,
                    "acc_stderr": 0.022935144053919443,
                    "acc_norm": 0.8860103626943006,
                    "acc_norm_stderr": 0.022935144053919443,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.6794871794871795,
                    "acc_stderr": 0.02366129639396428,
                    "acc_norm": 0.6794871794871795,
                    "acc_norm_stderr": 0.02366129639396428,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.34074074074074073,
                    "acc_stderr": 0.02889774874113115,
                    "acc_norm": 0.34074074074074073,
                    "acc_norm_stderr": 0.02889774874113115,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6596638655462185,
                    "acc_stderr": 0.030778057422931673,
                    "acc_norm": 0.6596638655462185,
                    "acc_norm_stderr": 0.030778057422931673,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31125827814569534,
                    "acc_stderr": 0.03780445850526732,
                    "acc_norm": 0.31125827814569534,
                    "acc_norm_stderr": 0.03780445850526732,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8403669724770643,
                    "acc_stderr": 0.015703498348461763,
                    "acc_norm": 0.8403669724770643,
                    "acc_norm_stderr": 0.015703498348461763,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.5092592592592593,
                    "acc_stderr": 0.034093869469927006,
                    "acc_norm": 0.5092592592592593,
                    "acc_norm_stderr": 0.034093869469927006,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.8382352941176471,
                    "acc_stderr": 0.02584501798692692,
                    "acc_norm": 0.8382352941176471,
                    "acc_norm_stderr": 0.02584501798692692,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.8227848101265823,
                    "acc_stderr": 0.024856364184503224,
                    "acc_norm": 0.8227848101265823,
                    "acc_norm_stderr": 0.024856364184503224,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.695067264573991,
                    "acc_stderr": 0.030898610882477515,
                    "acc_norm": 0.695067264573991,
                    "acc_norm_stderr": 0.030898610882477515,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.8015267175572519,
                    "acc_stderr": 0.03498149385462472,
                    "acc_norm": 0.8015267175572519,
                    "acc_norm_stderr": 0.03498149385462472,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.7603305785123967,
                    "acc_stderr": 0.03896878985070416,
                    "acc_norm": 0.7603305785123967,
                    "acc_norm_stderr": 0.03896878985070416,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8425925925925926,
                    "acc_stderr": 0.03520703990517963,
                    "acc_norm": 0.8425925925925926,
                    "acc_norm_stderr": 0.03520703990517963,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.7730061349693251,
                    "acc_stderr": 0.03291099578615769,
                    "acc_norm": 0.7730061349693251,
                    "acc_norm_stderr": 0.03291099578615769,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.4732142857142857,
                    "acc_stderr": 0.047389751192741546,
                    "acc_norm": 0.4732142857142857,
                    "acc_norm_stderr": 0.047389751192741546,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.7961165048543689,
                    "acc_stderr": 0.039891398595317706,
                    "acc_norm": 0.7961165048543689,
                    "acc_norm_stderr": 0.039891398595317706,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8760683760683761,
                    "acc_stderr": 0.021586494001281372,
                    "acc_norm": 0.8760683760683761,
                    "acc_norm_stderr": 0.021586494001281372,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.68,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.68,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.822477650063857,
                    "acc_stderr": 0.013664230995834834,
                    "acc_norm": 0.822477650063857,
                    "acc_norm_stderr": 0.013664230995834834,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7427745664739884,
                    "acc_stderr": 0.023532925431044283,
                    "acc_norm": 0.7427745664739884,
                    "acc_norm_stderr": 0.023532925431044283,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.47262569832402235,
                    "acc_stderr": 0.016697420650642752,
                    "acc_norm": 0.47262569832402235,
                    "acc_norm_stderr": 0.016697420650642752,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7418300653594772,
                    "acc_stderr": 0.02505850331695814,
                    "acc_norm": 0.7418300653594772,
                    "acc_norm_stderr": 0.02505850331695814,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.7202572347266881,
                    "acc_stderr": 0.025494259350694912,
                    "acc_norm": 0.7202572347266881,
                    "acc_norm_stderr": 0.025494259350694912,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.7253086419753086,
                    "acc_stderr": 0.024836057868294677,
                    "acc_norm": 0.7253086419753086,
                    "acc_norm_stderr": 0.024836057868294677,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.46099290780141844,
                    "acc_stderr": 0.02973659252642444,
                    "acc_norm": 0.46099290780141844,
                    "acc_norm_stderr": 0.02973659252642444,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.4661016949152542,
                    "acc_stderr": 0.01274085387294983,
                    "acc_norm": 0.4661016949152542,
                    "acc_norm_stderr": 0.01274085387294983,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.6875,
                    "acc_stderr": 0.02815637344037142,
                    "acc_norm": 0.6875,
                    "acc_norm_stderr": 0.02815637344037142,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.6552287581699346,
                    "acc_stderr": 0.01922832201869664,
                    "acc_norm": 0.6552287581699346,
                    "acc_norm_stderr": 0.01922832201869664,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6363636363636364,
                    "acc_stderr": 0.04607582090719976,
                    "acc_norm": 0.6363636363636364,
                    "acc_norm_stderr": 0.04607582090719976,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.7142857142857143,
                    "acc_stderr": 0.028920583220675602,
                    "acc_norm": 0.7142857142857143,
                    "acc_norm_stderr": 0.028920583220675602,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8407960199004975,
                    "acc_stderr": 0.02587064676616913,
                    "acc_norm": 0.8407960199004975,
                    "acc_norm_stderr": 0.02587064676616913,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.0348735088019777,
                    "acc_norm": 0.86,
                    "acc_norm_stderr": 0.0348735088019777,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.536144578313253,
                    "acc_stderr": 0.038823108508905954,
                    "acc_norm": 0.536144578313253,
                    "acc_norm_stderr": 0.038823108508905954,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8187134502923976,
                    "acc_stderr": 0.029547741687640038,
                    "acc_norm": 0.8187134502923976,
                    "acc_norm_stderr": 0.029547741687640038,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.4602203182374541,
                    "mc1_stderr": 0.017448017223960884,
                    "mc2": 0.6371489024278089,
                    "mc2_stderr": 0.015333679187240897,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.8050513022888713,
                    "acc_stderr": 0.011134099415938282,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.6474601971190296,
                    "acc_stderr": 0.013159909755930333,
                    "timestamp": "2024-02-13T09-17-16.010723"
                }
            }
        }
    }
}