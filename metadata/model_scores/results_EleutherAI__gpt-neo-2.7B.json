{
    "model_name": "EleutherAI/gpt-neo-2.7B",
    "last_updated": "2024-12-19 13:40:46.199004",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.029304029304029304,
                "exact_match_stderr": 0.007224487305459686,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.010332950631458095,
                "exact_match_stderr": 0.0034284443646836614,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.025925925925925925,
                "exact_match_stderr": 0.0068449258444466055,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.019933554817275746,
                "exact_match_stderr": 0.004653898088316119,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.005861462425818025,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.012658227848101266,
                "exact_match_stderr": 0.005140313889578842,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_algebra": {
                "exact_match": 0.016006739679865205,
                "exact_match_stderr": 0.0036442247924417257,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_2ds": {
                "acc": 0.012,
                "acc_stderr": 0.0024353573624298214,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_1dc": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339602,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_2dm": {
                "acc": 0.02,
                "acc_stderr": 0.0031312780858980494,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_2da": {
                "acc": 0.0065,
                "acc_stderr": 0.0017973564602277766,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "gsm8k_cot": {
                "exact_match": 0.01819560272934041,
                "exact_match_stderr": 0.0036816118940738727,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "gsm8k": {
                "exact_match": 0.02047005307050796,
                "exact_match_stderr": 0.0039004133859157192,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "anli_r2": {
                "brier_score": 0.8345390393015258,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "anli_r3": {
                "brier_score": 0.7689959499686069,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "anli_r1": {
                "brier_score": 0.8666257668901235,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_eu": {
                "brier_score": 1.1095833790701086,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_vi": {
                "brier_score": 0.7461864238715018,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_ru": {
                "brier_score": 0.7999015977427434,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_zh": {
                "brier_score": 0.9806971205870976,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_tr": {
                "brier_score": 0.8960590433909379,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_fr": {
                "brier_score": 0.8146332923707491,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_en": {
                "brier_score": 0.6628158350379858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_ur": {
                "brier_score": 1.0717135016627397,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_ar": {
                "brier_score": 1.273910719834552,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_de": {
                "brier_score": 0.8583968193026806,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_hi": {
                "brier_score": 0.8196953013215778,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_es": {
                "brier_score": 0.8843937740416182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_bg": {
                "brier_score": 0.8937202436821879,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_sw": {
                "brier_score": 0.7992106097885577,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_el": {
                "brier_score": 0.9270116356402289,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_th": {
                "brier_score": 0.7847320508164434,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "logiqa2": {
                "brier_score": 1.1416601553118555,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "mathqa": {
                "brier_score": 0.9872909262605601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "lambada_standard": {
                "perplexity": 9.472999881770361,
                "perplexity_stderr": 0.26574628102110154,
                "acc": 0.5175625849019988,
                "acc_stderr": 0.00696167909747916,
                "timestamp": "2024-06-14T08-32-02.667192"
            },
            "lambada_openai": {
                "perplexity": 5.625735040282958,
                "perplexity_stderr": 0.13851851129115517,
                "acc": 0.622355909179119,
                "acc_stderr": 0.006754183076526671,
                "timestamp": "2024-06-14T08-32-02.667192"
            },
            "mmlu_world_religions": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.03377310252209197,
                "brier_score": 0.7658346958459136,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_formal_logic": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.037184890068181146,
                "brier_score": 0.7794135178208829,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_prehistory": {
                "acc": 0.24382716049382716,
                "acc_stderr": 0.023891879541959614,
                "brier_score": 0.7603184815640671,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.27262569832402234,
                "acc_stderr": 0.014893391735249603,
                "brier_score": 0.750880388733267,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.23628691983122363,
                "acc_stderr": 0.027652153144159274,
                "brier_score": 0.752884223832844,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26011560693641617,
                "acc_stderr": 0.023618678310069367,
                "brier_score": 0.7615525596388655,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_professional_law": {
                "acc": 0.23598435462842243,
                "acc_stderr": 0.010844802669662694,
                "brier_score": 0.7670687906909094,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2883435582822086,
                "acc_stderr": 0.035590395316173425,
                "brier_score": 0.7539889704237094,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.28431372549019607,
                "acc_stderr": 0.03166009679399812,
                "brier_score": 0.7625875980655249,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_philosophy": {
                "acc": 0.20257234726688103,
                "acc_stderr": 0.02282731749105968,
                "brier_score": 0.778702284090794,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_jurisprudence": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.042365112580946336,
                "brier_score": 0.759013828336546,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_international_law": {
                "acc": 0.256198347107438,
                "acc_stderr": 0.03984979653302871,
                "brier_score": 0.7520643776248898,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.18787878787878787,
                "acc_stderr": 0.030501934059429144,
                "brier_score": 0.7758275559833471,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21243523316062177,
                "acc_stderr": 0.029519282616817258,
                "brier_score": 0.7830004814871769,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.24369747899159663,
                "acc_stderr": 0.027886828078380565,
                "brier_score": 0.7596873547074825,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_geography": {
                "acc": 0.25252525252525254,
                "acc_stderr": 0.030954055470365907,
                "brier_score": 0.7673749782882908,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24036697247706423,
                "acc_stderr": 0.01832060732096407,
                "brier_score": 0.7678113442767674,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_public_relations": {
                "acc": 0.2909090909090909,
                "acc_stderr": 0.04350271442923243,
                "brier_score": 0.7594777018665088,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7619763802399224,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_sociology": {
                "acc": 0.263681592039801,
                "acc_stderr": 0.03115715086935557,
                "brier_score": 0.7685246417582672,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2205128205128205,
                "acc_stderr": 0.0210206726808279,
                "brier_score": 0.7621025466387151,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_security_studies": {
                "acc": 0.19183673469387755,
                "acc_stderr": 0.02520696315422539,
                "brier_score": 0.791629502080558,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_professional_psychology": {
                "acc": 0.28431372549019607,
                "acc_stderr": 0.018249024411207654,
                "brier_score": 0.749931090787944,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "brier_score": 0.7653318578140017,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_econometrics": {
                "acc": 0.21929824561403508,
                "acc_stderr": 0.03892431106518754,
                "brier_score": 0.7547973992786374,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2988505747126437,
                "acc_stderr": 0.01636925681509313,
                "brier_score": 0.7496301587887607,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_marketing": {
                "acc": 0.20512820512820512,
                "acc_stderr": 0.026453508054040346,
                "brier_score": 0.7526073931650051,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "brier_score": 0.7826546414599174,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_nutrition": {
                "acc": 0.22875816993464052,
                "acc_stderr": 0.02405102973991225,
                "brier_score": 0.759395961830524,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_medical_genetics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "brier_score": 0.7474368668384139,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_human_aging": {
                "acc": 0.24663677130044842,
                "acc_stderr": 0.028930413120910877,
                "brier_score": 0.745585259751547,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_professional_medicine": {
                "acc": 0.19852941176470587,
                "acc_stderr": 0.024231013370541073,
                "brier_score": 0.7964218925708318,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_college_medicine": {
                "acc": 0.19653179190751446,
                "acc_stderr": 0.03029957466478814,
                "brier_score": 0.7652411263996571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_business_ethics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7525584188309182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2981132075471698,
                "acc_stderr": 0.028152837942493857,
                "brier_score": 0.7492196086319309,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_global_facts": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "brier_score": 0.7337682181369112,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_virology": {
                "acc": 0.25301204819277107,
                "acc_stderr": 0.033844291552331346,
                "brier_score": 0.7498117129055916,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_professional_accounting": {
                "acc": 0.28368794326241137,
                "acc_stderr": 0.02689170942834396,
                "brier_score": 0.7608069606813758,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_college_physics": {
                "acc": 0.37254901960784315,
                "acc_stderr": 0.04810840148082635,
                "brier_score": 0.7493495275284697,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_physics": {
                "acc": 0.24503311258278146,
                "acc_stderr": 0.035118075718047245,
                "brier_score": 0.7608706788430074,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_biology": {
                "acc": 0.22258064516129034,
                "acc_stderr": 0.02366421667164252,
                "brier_score": 0.7621223319968164,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_college_biology": {
                "acc": 0.22916666666666666,
                "acc_stderr": 0.035146974678623884,
                "brier_score": 0.76157444703166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_anatomy": {
                "acc": 0.31851851851851853,
                "acc_stderr": 0.040247784019771096,
                "brier_score": 0.7453968548735045,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_college_chemistry": {
                "acc": 0.26,
                "acc_stderr": 0.044084400227680794,
                "brier_score": 0.7698692994140525,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_computer_security": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7556765474161725,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_college_computer_science": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7501910451686946,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_astronomy": {
                "acc": 0.2236842105263158,
                "acc_stderr": 0.033911609343436025,
                "brier_score": 0.7611459185050019,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_college_mathematics": {
                "acc": 0.26,
                "acc_stderr": 0.044084400227680794,
                "brier_score": 0.7618346898017976,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.22127659574468084,
                "acc_stderr": 0.027136349602424063,
                "brier_score": 0.7569627671463691,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7624468731602159,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.755932464895756,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_machine_learning": {
                "acc": 0.22321428571428573,
                "acc_stderr": 0.03952301967702511,
                "brier_score": 0.7564926169126464,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.26108374384236455,
                "acc_stderr": 0.030903796952114503,
                "brier_score": 0.7478953532965273,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1712962962962963,
                "acc_stderr": 0.025695341643824688,
                "brier_score": 0.7791131334160182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25396825396825395,
                "acc_stderr": 0.022418042891113942,
                "brier_score": 0.7597662971310662,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2206896551724138,
                "acc_stderr": 0.03455930201924811,
                "brier_score": 0.7641719279038703,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2851851851851852,
                "acc_stderr": 0.027528599210340492,
                "brier_score": 0.7541280656672071,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-09-00.779237"
            },
            "arc_challenge": {
                "acc": 0.3199658703071672,
                "acc_stderr": 0.013631345807016196,
                "acc_norm": 0.34897610921501704,
                "acc_norm_stderr": 0.013928933461382504,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "hellaswag": {
                "acc": 0.4266082453694483,
                "acc_stderr": 0.004935735300348859,
                "acc_norm": 0.561840270862378,
                "acc_norm_stderr": 0.004951470301995878,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "truthfulqa_mc2": {
                "acc": 0.39862631918419716,
                "acc_stderr": 0.014042065906814954,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.047333287791275,
                "bleu_max_stderr": 0.7526012282674005,
                "bleu_acc": 0.35862913096695226,
                "bleu_acc_stderr": 0.016789289499502022,
                "bleu_diff": -2.089423502619666,
                "bleu_diff_stderr": 0.8400708361193482,
                "rouge1_max": 49.67982165948041,
                "rouge1_max_stderr": 0.8665227117461761,
                "rouge1_acc": 0.34149326805385555,
                "rouge1_acc_stderr": 0.01660068861995083,
                "rouge1_diff": -1.963312198645994,
                "rouge1_diff_stderr": 1.0594843696396452,
                "rouge2_max": 32.68414904349889,
                "rouge2_max_stderr": 1.029790643453569,
                "rouge2_acc": 0.26193390452876375,
                "rouge2_acc_stderr": 0.015392118805015016,
                "rouge2_diff": -3.8823758442208036,
                "rouge2_diff_stderr": 1.1699264928139463,
                "rougeL_max": 46.885237240783155,
                "rougeL_max_stderr": 0.885979367776478,
                "rougeL_acc": 0.3427172582619339,
                "rougeL_acc_stderr": 0.016614949385347046,
                "rougeL_diff": -2.1065494987322304,
                "rougeL_diff_stderr": 1.0736368489730006,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "truthfulqa_mc1": {
                "acc": 0.2386780905752754,
                "acc_stderr": 0.014922629695456416,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "winogrande": {
                "acc": 0.5777426992896606,
                "acc_stderr": 0.013881582030658557,
                "timestamp": "2024-11-21T20-01-14.372897"
            }
        }
    }
}