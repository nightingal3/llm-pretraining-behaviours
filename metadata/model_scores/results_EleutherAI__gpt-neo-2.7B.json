{
    "model_name": "EleutherAI/gpt-neo-2.7B",
    "last_updated": "2024-12-04 11:24:49.074976",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.029304029304029304,
                "exact_match_stderr": 0.007224487305459686,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.010332950631458095,
                "exact_match_stderr": 0.0034284443646836614,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.025925925925925925,
                "exact_match_stderr": 0.0068449258444466055,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.019933554817275746,
                "exact_match_stderr": 0.004653898088316119,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.005861462425818025,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.012658227848101266,
                "exact_match_stderr": 0.005140313889578842,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "minerva_math_algebra": {
                "exact_match": 0.016006739679865205,
                "exact_match_stderr": 0.0036442247924417257,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_2ds": {
                "acc": 0.012,
                "acc_stderr": 0.0024353573624298214,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_1dc": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339602,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_2dm": {
                "acc": 0.02,
                "acc_stderr": 0.0031312780858980494,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "arithmetic_2da": {
                "acc": 0.0065,
                "acc_stderr": 0.0017973564602277766,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "gsm8k_cot": {
                "exact_match": 0.01819560272934041,
                "exact_match_stderr": 0.0036816118940738727,
                "timestamp": "2024-06-14T19-26-30.348205"
            },
            "gsm8k": {
                "exact_match": 0.02047005307050796,
                "exact_match_stderr": 0.0039004133859157192,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "anli_r2": {
                "brier_score": 0.8345390393015258,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "anli_r3": {
                "brier_score": 0.7689959499686069,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "anli_r1": {
                "brier_score": 0.8666257668901235,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_eu": {
                "brier_score": 1.1095833790701086,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_vi": {
                "brier_score": 0.7461864238715018,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_ru": {
                "brier_score": 0.7999015977427434,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_zh": {
                "brier_score": 0.9806971205870976,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_tr": {
                "brier_score": 0.8960590433909379,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_fr": {
                "brier_score": 0.8146332923707491,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_en": {
                "brier_score": 0.6628158350379858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_ur": {
                "brier_score": 1.0717135016627397,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_ar": {
                "brier_score": 1.273910719834552,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_de": {
                "brier_score": 0.8583968193026806,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_hi": {
                "brier_score": 0.8196953013215778,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_es": {
                "brier_score": 0.8843937740416182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_bg": {
                "brier_score": 0.8937202436821879,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_sw": {
                "brier_score": 0.7992106097885577,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_el": {
                "brier_score": 0.9270116356402289,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "xnli_th": {
                "brier_score": 0.7847320508164434,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "logiqa2": {
                "brier_score": 1.1416601553118555,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "mathqa": {
                "brier_score": 0.9872909262605601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T19-46-05.025312"
            },
            "lambada_standard": {
                "perplexity": 9.472999881770361,
                "perplexity_stderr": 0.26574628102110154,
                "acc": 0.5175625849019988,
                "acc_stderr": 0.00696167909747916,
                "timestamp": "2024-06-14T08-32-02.667192"
            },
            "lambada_openai": {
                "perplexity": 5.625735040282958,
                "perplexity_stderr": 0.13851851129115517,
                "acc": 0.622355909179119,
                "acc_stderr": 0.006754183076526671,
                "timestamp": "2024-06-14T08-32-02.667192"
            },
            "mmlu_world_religions": {
                "acc": 0.27485380116959063,
                "acc_stderr": 0.03424042924691584,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_formal_logic": {
                "acc": 0.18253968253968253,
                "acc_stderr": 0.03455071019102149,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_prehistory": {
                "acc": 0.3117283950617284,
                "acc_stderr": 0.02577311116963045,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.27039106145251396,
                "acc_stderr": 0.014854993938010085,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.22362869198312235,
                "acc_stderr": 0.02712329820522997,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_moral_disputes": {
                "acc": 0.24566473988439305,
                "acc_stderr": 0.023176298203992005,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_professional_law": {
                "acc": 0.2457627118644068,
                "acc_stderr": 0.01099615663514269,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.25766871165644173,
                "acc_stderr": 0.03436150827846917,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.19607843137254902,
                "acc_stderr": 0.027865942286639318,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_philosophy": {
                "acc": 0.3247588424437299,
                "acc_stderr": 0.026596782287697046,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.04414343666854933,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_international_law": {
                "acc": 0.23140495867768596,
                "acc_stderr": 0.03849856098794088,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.23030303030303031,
                "acc_stderr": 0.0328766675860349,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.26424870466321243,
                "acc_stderr": 0.03182155050916646,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.027553614467863804,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_geography": {
                "acc": 0.3282828282828283,
                "acc_stderr": 0.03345678422756775,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.30642201834862387,
                "acc_stderr": 0.01976551722045852,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_public_relations": {
                "acc": 0.18181818181818182,
                "acc_stderr": 0.036942843353378,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_sociology": {
                "acc": 0.22885572139303484,
                "acc_stderr": 0.02970528405677243,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.3487179487179487,
                "acc_stderr": 0.024162780284017717,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_security_studies": {
                "acc": 0.2816326530612245,
                "acc_stderr": 0.02879518557429127,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2679738562091503,
                "acc_stderr": 0.017917974069594726,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2748091603053435,
                "acc_stderr": 0.03915345408847836,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_econometrics": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.04142439719489361,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2388250319284802,
                "acc_stderr": 0.015246803197398682,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_marketing": {
                "acc": 0.2606837606837607,
                "acc_stderr": 0.028760348956523414,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_management": {
                "acc": 0.27184466019417475,
                "acc_stderr": 0.044052680241409216,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_nutrition": {
                "acc": 0.3104575163398693,
                "acc_stderr": 0.026493033225145894,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_medical_genetics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_human_aging": {
                "acc": 0.18385650224215247,
                "acc_stderr": 0.02599837909235651,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_professional_medicine": {
                "acc": 0.43014705882352944,
                "acc_stderr": 0.030074971917302875,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.032424147574830996,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_business_ethics": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2641509433962264,
                "acc_stderr": 0.027134291628741713,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_global_facts": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_virology": {
                "acc": 0.3253012048192771,
                "acc_stderr": 0.03647168523683229,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2553191489361702,
                "acc_stderr": 0.026011992930902013,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.04092563958237657,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_physics": {
                "acc": 0.23841059602649006,
                "acc_stderr": 0.0347918557259966,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_biology": {
                "acc": 0.24838709677419354,
                "acc_stderr": 0.024580028921481003,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_college_biology": {
                "acc": 0.2569444444444444,
                "acc_stderr": 0.03653946969442099,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_anatomy": {
                "acc": 0.2,
                "acc_stderr": 0.03455473702325436,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_college_chemistry": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_college_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_astronomy": {
                "acc": 0.19078947368421054,
                "acc_stderr": 0.031975658210325004,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_college_mathematics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.029241883869628817,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_machine_learning": {
                "acc": 0.16964285714285715,
                "acc_stderr": 0.0356236785009539,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.030108330718011625,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4074074074074074,
                "acc_stderr": 0.03350991604696043,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.29365079365079366,
                "acc_stderr": 0.02345603738398203,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.25517241379310346,
                "acc_stderr": 0.03632984052707842,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24814814814814815,
                "acc_stderr": 0.0263357394040558,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "arc_challenge": {
                "acc": 0.3199658703071672,
                "acc_stderr": 0.013631345807016196,
                "acc_norm": 0.34897610921501704,
                "acc_norm_stderr": 0.013928933461382504,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "hellaswag": {
                "acc": 0.4266082453694483,
                "acc_stderr": 0.004935735300348859,
                "acc_norm": 0.561840270862378,
                "acc_norm_stderr": 0.004951470301995878,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "truthfulqa_mc2": {
                "acc": 0.39862631918419716,
                "acc_stderr": 0.014042065906814954,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.047333287791275,
                "bleu_max_stderr": 0.7526012282674005,
                "bleu_acc": 0.35862913096695226,
                "bleu_acc_stderr": 0.016789289499502022,
                "bleu_diff": -2.089423502619666,
                "bleu_diff_stderr": 0.8400708361193482,
                "rouge1_max": 49.67982165948041,
                "rouge1_max_stderr": 0.8665227117461761,
                "rouge1_acc": 0.34149326805385555,
                "rouge1_acc_stderr": 0.01660068861995083,
                "rouge1_diff": -1.963312198645994,
                "rouge1_diff_stderr": 1.0594843696396452,
                "rouge2_max": 32.68414904349889,
                "rouge2_max_stderr": 1.029790643453569,
                "rouge2_acc": 0.26193390452876375,
                "rouge2_acc_stderr": 0.015392118805015016,
                "rouge2_diff": -3.8823758442208036,
                "rouge2_diff_stderr": 1.1699264928139463,
                "rougeL_max": 46.885237240783155,
                "rougeL_max_stderr": 0.885979367776478,
                "rougeL_acc": 0.3427172582619339,
                "rougeL_acc_stderr": 0.016614949385347046,
                "rougeL_diff": -2.1065494987322304,
                "rougeL_diff_stderr": 1.0736368489730006,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "truthfulqa_mc1": {
                "acc": 0.2386780905752754,
                "acc_stderr": 0.014922629695456416,
                "timestamp": "2024-11-21T20-01-14.372897"
            },
            "winogrande": {
                "acc": 0.5777426992896606,
                "acc_stderr": 0.013881582030658557,
                "timestamp": "2024-11-21T20-01-14.372897"
            }
        }
    }
}