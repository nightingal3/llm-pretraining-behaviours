{
    "model_name": "AbacusResearch/RasGulla1-7b",
    "last_updated": "2024-03-04",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.659556313993174,
                    "acc_stderr": 0.013847460518892976,
                    "acc_norm": 0.697098976109215,
                    "acc_norm_stderr": 0.013428241573185349,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6983668591913962,
                    "acc_stderr": 0.0045802887281959775,
                    "acc_norm": 0.8740290778729337,
                    "acc_norm_stderr": 0.0033113844981586408,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6518518518518519,
                    "acc_stderr": 0.041153246103369526,
                    "acc_norm": 0.6518518518518519,
                    "acc_norm_stderr": 0.041153246103369526,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.6907894736842105,
                    "acc_stderr": 0.037610708698674805,
                    "acc_norm": 0.6907894736842105,
                    "acc_norm_stderr": 0.037610708698674805,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.61,
                    "acc_stderr": 0.04902071300001975,
                    "acc_norm": 0.61,
                    "acc_norm_stderr": 0.04902071300001975,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.7132075471698113,
                    "acc_stderr": 0.02783491252754406,
                    "acc_norm": 0.7132075471698113,
                    "acc_norm_stderr": 0.02783491252754406,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.7708333333333334,
                    "acc_stderr": 0.03514697467862388,
                    "acc_norm": 0.7708333333333334,
                    "acc_norm_stderr": 0.03514697467862388,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.47,
                    "acc_stderr": 0.050161355804659205,
                    "acc_norm": 0.47,
                    "acc_norm_stderr": 0.050161355804659205,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.57,
                    "acc_stderr": 0.04975698519562428,
                    "acc_norm": 0.57,
                    "acc_norm_stderr": 0.04975698519562428,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.6878612716763006,
                    "acc_stderr": 0.03533133389323657,
                    "acc_norm": 0.6878612716763006,
                    "acc_norm_stderr": 0.03533133389323657,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.4117647058823529,
                    "acc_stderr": 0.048971049527263666,
                    "acc_norm": 0.4117647058823529,
                    "acc_norm_stderr": 0.048971049527263666,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.77,
                    "acc_stderr": 0.04229525846816508,
                    "acc_norm": 0.77,
                    "acc_norm_stderr": 0.04229525846816508,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.5829787234042553,
                    "acc_stderr": 0.03223276266711712,
                    "acc_norm": 0.5829787234042553,
                    "acc_norm_stderr": 0.03223276266711712,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.47368421052631576,
                    "acc_stderr": 0.046970851366478626,
                    "acc_norm": 0.47368421052631576,
                    "acc_norm_stderr": 0.046970851366478626,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5517241379310345,
                    "acc_stderr": 0.04144311810878152,
                    "acc_norm": 0.5517241379310345,
                    "acc_norm_stderr": 0.04144311810878152,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.4021164021164021,
                    "acc_stderr": 0.025253032554997692,
                    "acc_norm": 0.4021164021164021,
                    "acc_norm_stderr": 0.025253032554997692,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.49206349206349204,
                    "acc_stderr": 0.044715725362943486,
                    "acc_norm": 0.49206349206349204,
                    "acc_norm_stderr": 0.044715725362943486,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621505,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621505,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7774193548387097,
                    "acc_stderr": 0.023664216671642518,
                    "acc_norm": 0.7774193548387097,
                    "acc_norm_stderr": 0.023664216671642518,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.4827586206896552,
                    "acc_stderr": 0.035158955511656986,
                    "acc_norm": 0.4827586206896552,
                    "acc_norm_stderr": 0.035158955511656986,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.69,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.69,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.7818181818181819,
                    "acc_stderr": 0.03225078108306289,
                    "acc_norm": 0.7818181818181819,
                    "acc_norm_stderr": 0.03225078108306289,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.8080808080808081,
                    "acc_stderr": 0.028057791672989017,
                    "acc_norm": 0.8080808080808081,
                    "acc_norm_stderr": 0.028057791672989017,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.9015544041450777,
                    "acc_stderr": 0.021500249576033456,
                    "acc_norm": 0.9015544041450777,
                    "acc_norm_stderr": 0.021500249576033456,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.6666666666666666,
                    "acc_stderr": 0.023901157979402534,
                    "acc_norm": 0.6666666666666666,
                    "acc_norm_stderr": 0.023901157979402534,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.34814814814814815,
                    "acc_stderr": 0.029045600290616255,
                    "acc_norm": 0.34814814814814815,
                    "acc_norm_stderr": 0.029045600290616255,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6680672268907563,
                    "acc_stderr": 0.03058869701378364,
                    "acc_norm": 0.6680672268907563,
                    "acc_norm_stderr": 0.03058869701378364,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.3443708609271523,
                    "acc_stderr": 0.038796870240733264,
                    "acc_norm": 0.3443708609271523,
                    "acc_norm_stderr": 0.038796870240733264,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8477064220183487,
                    "acc_stderr": 0.015405084393157074,
                    "acc_norm": 0.8477064220183487,
                    "acc_norm_stderr": 0.015405084393157074,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.5046296296296297,
                    "acc_stderr": 0.03409825519163572,
                    "acc_norm": 0.5046296296296297,
                    "acc_norm_stderr": 0.03409825519163572,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.8529411764705882,
                    "acc_stderr": 0.024857478080250458,
                    "acc_norm": 0.8529411764705882,
                    "acc_norm_stderr": 0.024857478080250458,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.8143459915611815,
                    "acc_stderr": 0.025310495376944856,
                    "acc_norm": 0.8143459915611815,
                    "acc_norm_stderr": 0.025310495376944856,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.695067264573991,
                    "acc_stderr": 0.030898610882477515,
                    "acc_norm": 0.695067264573991,
                    "acc_norm_stderr": 0.030898610882477515,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.7938931297709924,
                    "acc_stderr": 0.03547771004159464,
                    "acc_norm": 0.7938931297709924,
                    "acc_norm_stderr": 0.03547771004159464,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.7933884297520661,
                    "acc_stderr": 0.03695980128098824,
                    "acc_norm": 0.7933884297520661,
                    "acc_norm_stderr": 0.03695980128098824,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8055555555555556,
                    "acc_stderr": 0.038260763248848646,
                    "acc_norm": 0.8055555555555556,
                    "acc_norm_stderr": 0.038260763248848646,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.7668711656441718,
                    "acc_stderr": 0.0332201579577674,
                    "acc_norm": 0.7668711656441718,
                    "acc_norm_stderr": 0.0332201579577674,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.42857142857142855,
                    "acc_stderr": 0.04697113923010212,
                    "acc_norm": 0.42857142857142855,
                    "acc_norm_stderr": 0.04697113923010212,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.7864077669902912,
                    "acc_stderr": 0.040580420156460344,
                    "acc_norm": 0.7864077669902912,
                    "acc_norm_stderr": 0.040580420156460344,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8803418803418803,
                    "acc_stderr": 0.021262719400406957,
                    "acc_norm": 0.8803418803418803,
                    "acc_norm_stderr": 0.021262719400406957,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.71,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.71,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.8237547892720306,
                    "acc_stderr": 0.013625556907993462,
                    "acc_norm": 0.8237547892720306,
                    "acc_norm_stderr": 0.013625556907993462,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7485549132947977,
                    "acc_stderr": 0.02335736578587403,
                    "acc_norm": 0.7485549132947977,
                    "acc_norm_stderr": 0.02335736578587403,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.4581005586592179,
                    "acc_stderr": 0.016663683295020527,
                    "acc_norm": 0.4581005586592179,
                    "acc_norm_stderr": 0.016663683295020527,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7320261437908496,
                    "acc_stderr": 0.025360603796242557,
                    "acc_norm": 0.7320261437908496,
                    "acc_norm_stderr": 0.025360603796242557,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.7138263665594855,
                    "acc_stderr": 0.025670259242188933,
                    "acc_norm": 0.7138263665594855,
                    "acc_norm_stderr": 0.025670259242188933,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.7314814814814815,
                    "acc_stderr": 0.024659685185967284,
                    "acc_norm": 0.7314814814814815,
                    "acc_norm_stderr": 0.024659685185967284,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.4645390070921986,
                    "acc_stderr": 0.02975238965742705,
                    "acc_norm": 0.4645390070921986,
                    "acc_norm_stderr": 0.02975238965742705,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.46870925684485004,
                    "acc_stderr": 0.012745204626083133,
                    "acc_norm": 0.46870925684485004,
                    "acc_norm_stderr": 0.012745204626083133,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.6838235294117647,
                    "acc_stderr": 0.028245687391462923,
                    "acc_norm": 0.6838235294117647,
                    "acc_norm_stderr": 0.028245687391462923,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.6797385620915033,
                    "acc_stderr": 0.018875682938069443,
                    "acc_norm": 0.6797385620915033,
                    "acc_norm_stderr": 0.018875682938069443,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6727272727272727,
                    "acc_stderr": 0.0449429086625209,
                    "acc_norm": 0.6727272727272727,
                    "acc_norm_stderr": 0.0449429086625209,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.7346938775510204,
                    "acc_stderr": 0.028263889943784593,
                    "acc_norm": 0.7346938775510204,
                    "acc_norm_stderr": 0.028263889943784593,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8557213930348259,
                    "acc_stderr": 0.024845753212306053,
                    "acc_norm": 0.8557213930348259,
                    "acc_norm_stderr": 0.024845753212306053,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.0348735088019777,
                    "acc_norm": 0.86,
                    "acc_norm_stderr": 0.0348735088019777,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5481927710843374,
                    "acc_stderr": 0.03874371556587953,
                    "acc_norm": 0.5481927710843374,
                    "acc_norm_stderr": 0.03874371556587953,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8421052631578947,
                    "acc_stderr": 0.027966785859160893,
                    "acc_norm": 0.8421052631578947,
                    "acc_norm_stderr": 0.027966785859160893,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.4589963280293758,
                    "mc1_stderr": 0.017444544447661192,
                    "mc2": 0.633141301315501,
                    "mc2_stderr": 0.015119391252537636,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.8089976322020521,
                    "acc_stderr": 0.011047808761510427,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.7172100075815011,
                    "acc_stderr": 0.012405020417873619,
                    "timestamp": "2024-03-04T19-34-43.069322"
                }
            }
        }
    }
}