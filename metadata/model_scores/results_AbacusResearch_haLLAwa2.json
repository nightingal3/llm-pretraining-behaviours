{
    "model_name": "AbacusResearch/haLLAwa2",
    "last_updated": "2024-02-12",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.6015358361774744,
                    "acc_stderr": 0.014306946052735565,
                    "acc_norm": 0.6331058020477816,
                    "acc_norm_stderr": 0.014084133118104298,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6836287592113125,
                    "acc_stderr": 0.004641092001425291,
                    "acc_norm": 0.8450507866958773,
                    "acc_norm_stderr": 0.003611167302959773,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.048523658709391,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.048523658709391,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6074074074074074,
                    "acc_stderr": 0.04218506215368881,
                    "acc_norm": 0.6074074074074074,
                    "acc_norm_stderr": 0.04218506215368881,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.6710526315789473,
                    "acc_stderr": 0.038234289699266046,
                    "acc_norm": 0.6710526315789473,
                    "acc_norm_stderr": 0.038234289699266046,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.57,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.57,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.690566037735849,
                    "acc_stderr": 0.02845015479411864,
                    "acc_norm": 0.690566037735849,
                    "acc_norm_stderr": 0.02845015479411864,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.7152777777777778,
                    "acc_stderr": 0.037738099906869334,
                    "acc_norm": 0.7152777777777778,
                    "acc_norm_stderr": 0.037738099906869334,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.47,
                    "acc_stderr": 0.050161355804659205,
                    "acc_norm": 0.47,
                    "acc_norm_stderr": 0.050161355804659205,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.48,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.48,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.047937248544110196,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.047937248544110196,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.630057803468208,
                    "acc_stderr": 0.0368122963339432,
                    "acc_norm": 0.630057803468208,
                    "acc_norm_stderr": 0.0368122963339432,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.43137254901960786,
                    "acc_stderr": 0.04928099597287533,
                    "acc_norm": 0.43137254901960786,
                    "acc_norm_stderr": 0.04928099597287533,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.77,
                    "acc_stderr": 0.04229525846816506,
                    "acc_norm": 0.77,
                    "acc_norm_stderr": 0.04229525846816506,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.5787234042553191,
                    "acc_stderr": 0.03227834510146268,
                    "acc_norm": 0.5787234042553191,
                    "acc_norm_stderr": 0.03227834510146268,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.47368421052631576,
                    "acc_stderr": 0.046970851366478626,
                    "acc_norm": 0.47368421052631576,
                    "acc_norm_stderr": 0.046970851366478626,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5241379310344828,
                    "acc_stderr": 0.0416180850350153,
                    "acc_norm": 0.5241379310344828,
                    "acc_norm_stderr": 0.0416180850350153,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.4021164021164021,
                    "acc_stderr": 0.025253032554997695,
                    "acc_norm": 0.4021164021164021,
                    "acc_norm_stderr": 0.025253032554997695,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.40476190476190477,
                    "acc_stderr": 0.04390259265377562,
                    "acc_norm": 0.40476190476190477,
                    "acc_norm_stderr": 0.04390259265377562,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7419354838709677,
                    "acc_stderr": 0.024892469172462836,
                    "acc_norm": 0.7419354838709677,
                    "acc_norm_stderr": 0.024892469172462836,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.47783251231527096,
                    "acc_stderr": 0.035145285621750094,
                    "acc_norm": 0.47783251231527096,
                    "acc_norm_stderr": 0.035145285621750094,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.67,
                    "acc_stderr": 0.047258156262526066,
                    "acc_norm": 0.67,
                    "acc_norm_stderr": 0.047258156262526066,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.7575757575757576,
                    "acc_stderr": 0.03346409881055953,
                    "acc_norm": 0.7575757575757576,
                    "acc_norm_stderr": 0.03346409881055953,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.7828282828282829,
                    "acc_stderr": 0.029376616484945633,
                    "acc_norm": 0.7828282828282829,
                    "acc_norm_stderr": 0.029376616484945633,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.8601036269430051,
                    "acc_stderr": 0.025033870583015184,
                    "acc_norm": 0.8601036269430051,
                    "acc_norm_stderr": 0.025033870583015184,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.6205128205128205,
                    "acc_stderr": 0.024603626924097417,
                    "acc_norm": 0.6205128205128205,
                    "acc_norm_stderr": 0.024603626924097417,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.362962962962963,
                    "acc_stderr": 0.029318203645206865,
                    "acc_norm": 0.362962962962963,
                    "acc_norm_stderr": 0.029318203645206865,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6470588235294118,
                    "acc_stderr": 0.031041941304059288,
                    "acc_norm": 0.6470588235294118,
                    "acc_norm_stderr": 0.031041941304059288,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2913907284768212,
                    "acc_stderr": 0.037101857261199946,
                    "acc_norm": 0.2913907284768212,
                    "acc_norm_stderr": 0.037101857261199946,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8311926605504587,
                    "acc_stderr": 0.01606005626853035,
                    "acc_norm": 0.8311926605504587,
                    "acc_norm_stderr": 0.01606005626853035,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.5231481481481481,
                    "acc_stderr": 0.03406315360711507,
                    "acc_norm": 0.5231481481481481,
                    "acc_norm_stderr": 0.03406315360711507,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.8137254901960784,
                    "acc_stderr": 0.027325470966716312,
                    "acc_norm": 0.8137254901960784,
                    "acc_norm_stderr": 0.027325470966716312,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.7974683544303798,
                    "acc_stderr": 0.026160568246601453,
                    "acc_norm": 0.7974683544303798,
                    "acc_norm_stderr": 0.026160568246601453,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.6995515695067265,
                    "acc_stderr": 0.030769352008229143,
                    "acc_norm": 0.6995515695067265,
                    "acc_norm_stderr": 0.030769352008229143,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.7557251908396947,
                    "acc_stderr": 0.03768335959728742,
                    "acc_norm": 0.7557251908396947,
                    "acc_norm_stderr": 0.03768335959728742,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.8099173553719008,
                    "acc_stderr": 0.03581796951709282,
                    "acc_norm": 0.8099173553719008,
                    "acc_norm_stderr": 0.03581796951709282,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8148148148148148,
                    "acc_stderr": 0.03755265865037181,
                    "acc_norm": 0.8148148148148148,
                    "acc_norm_stderr": 0.03755265865037181,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.7730061349693251,
                    "acc_stderr": 0.03291099578615769,
                    "acc_norm": 0.7730061349693251,
                    "acc_norm_stderr": 0.03291099578615769,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.48214285714285715,
                    "acc_stderr": 0.047427623612430116,
                    "acc_norm": 0.48214285714285715,
                    "acc_norm_stderr": 0.047427623612430116,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.7864077669902912,
                    "acc_stderr": 0.04058042015646034,
                    "acc_norm": 0.7864077669902912,
                    "acc_norm_stderr": 0.04058042015646034,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8803418803418803,
                    "acc_stderr": 0.021262719400406957,
                    "acc_norm": 0.8803418803418803,
                    "acc_norm_stderr": 0.021262719400406957,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.67,
                    "acc_stderr": 0.047258156262526094,
                    "acc_norm": 0.67,
                    "acc_norm_stderr": 0.047258156262526094,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.8135376756066411,
                    "acc_stderr": 0.013927751372001512,
                    "acc_norm": 0.8135376756066411,
                    "acc_norm_stderr": 0.013927751372001512,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7196531791907514,
                    "acc_stderr": 0.02418242749657761,
                    "acc_norm": 0.7196531791907514,
                    "acc_norm_stderr": 0.02418242749657761,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.4223463687150838,
                    "acc_stderr": 0.016519594275297114,
                    "acc_norm": 0.4223463687150838,
                    "acc_norm_stderr": 0.016519594275297114,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7450980392156863,
                    "acc_stderr": 0.02495418432487991,
                    "acc_norm": 0.7450980392156863,
                    "acc_norm_stderr": 0.02495418432487991,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.7363344051446945,
                    "acc_stderr": 0.02502553850053234,
                    "acc_norm": 0.7363344051446945,
                    "acc_norm_stderr": 0.02502553850053234,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.7067901234567902,
                    "acc_stderr": 0.02532988817190092,
                    "acc_norm": 0.7067901234567902,
                    "acc_norm_stderr": 0.02532988817190092,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.450354609929078,
                    "acc_stderr": 0.029680105565029036,
                    "acc_norm": 0.450354609929078,
                    "acc_norm_stderr": 0.029680105565029036,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.4602346805736636,
                    "acc_stderr": 0.01272978538659856,
                    "acc_norm": 0.4602346805736636,
                    "acc_norm_stderr": 0.01272978538659856,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.6801470588235294,
                    "acc_stderr": 0.02833295951403121,
                    "acc_norm": 0.6801470588235294,
                    "acc_norm_stderr": 0.02833295951403121,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.6470588235294118,
                    "acc_stderr": 0.019333142020797157,
                    "acc_norm": 0.6470588235294118,
                    "acc_norm_stderr": 0.019333142020797157,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6363636363636364,
                    "acc_stderr": 0.04607582090719976,
                    "acc_norm": 0.6363636363636364,
                    "acc_norm_stderr": 0.04607582090719976,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.7306122448979592,
                    "acc_stderr": 0.02840125202902294,
                    "acc_norm": 0.7306122448979592,
                    "acc_norm_stderr": 0.02840125202902294,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.845771144278607,
                    "acc_stderr": 0.025538433368578337,
                    "acc_norm": 0.845771144278607,
                    "acc_norm_stderr": 0.025538433368578337,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.88,
                    "acc_stderr": 0.03265986323710906,
                    "acc_norm": 0.88,
                    "acc_norm_stderr": 0.03265986323710906,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5421686746987951,
                    "acc_stderr": 0.03878626771002361,
                    "acc_norm": 0.5421686746987951,
                    "acc_norm_stderr": 0.03878626771002361,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8304093567251462,
                    "acc_stderr": 0.02878210810540171,
                    "acc_norm": 0.8304093567251462,
                    "acc_norm_stderr": 0.02878210810540171,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.33047735618115054,
                    "mc1_stderr": 0.016466769613698303,
                    "mc2": 0.4737549402479496,
                    "mc2_stderr": 0.015584581777910896,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7584846093133386,
                    "acc_stderr": 0.012028983782011875,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.5208491281273692,
                    "acc_stderr": 0.013760506094029868,
                    "timestamp": "2024-02-12T13-50-58.490257"
                }
            }
        }
    }
}