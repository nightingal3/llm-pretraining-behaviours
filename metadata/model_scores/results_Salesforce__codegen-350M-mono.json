{
    "model_name": "Salesforce/codegen-350M-mono",
    "last_updated": "2024-12-19 13:40:35.085267",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0018315018315018315,
                "exact_match_stderr": 0.0018315018315018335,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.004592422502870264,
                "exact_match_stderr": 0.0022922488477037863,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.001851851851851852,
                "exact_match_stderr": 0.0018518518518518504,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0022148394241417496,
                "exact_match_stderr": 0.0015652595934070638,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0020876826722338203,
                "exact_match_stderr": 0.002087682672233835,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.006329113924050633,
                "exact_match_stderr": 0.003646382041065048,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_algebra": {
                "exact_match": 0.006739679865206402,
                "exact_match_stderr": 0.0023757942810498744,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521431,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_2ds": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_1dc": {
                "acc": 0.0525,
                "acc_stderr": 0.004988418302285789,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000043,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_2dm": {
                "acc": 0.0205,
                "acc_stderr": 0.0031693686198869895,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_2da": {
                "acc": 0.013,
                "acc_stderr": 0.002533517190523328,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "gsm8k_cot": {
                "exact_match": 0.019711902956785442,
                "exact_match_stderr": 0.0038289829787357165,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "gsm8k": {
                "exact_match": 0.017437452615617893,
                "exact_match_stderr": 0.0036054868679982334,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "anli_r2": {
                "brier_score": 0.8673139893251594,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "anli_r3": {
                "brier_score": 0.8478090822960272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "anli_r1": {
                "brier_score": 0.8706127110280617,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_eu": {
                "brier_score": 1.2682920496708858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_vi": {
                "brier_score": 1.2752175230857816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_ru": {
                "brier_score": 0.8045751050884926,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_zh": {
                "brier_score": 1.2459315046432844,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_tr": {
                "brier_score": 1.1658554315468868,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_fr": {
                "brier_score": 1.1841733511077182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_en": {
                "brier_score": 0.7473727928562213,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_ur": {
                "brier_score": 1.2271500572114633,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_ar": {
                "brier_score": 1.0042596344795447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_de": {
                "brier_score": 1.014094216947774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_hi": {
                "brier_score": 1.1401151042767916,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_es": {
                "brier_score": 1.0787926125248246,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_bg": {
                "brier_score": 1.1483978030694448,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_sw": {
                "brier_score": 0.9825310566958355,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_el": {
                "brier_score": 1.0833010621729644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_th": {
                "brier_score": 1.0701844940260412,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "logiqa2": {
                "brier_score": 1.2332295009469143,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "mathqa": {
                "brier_score": 0.9914231048948398,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "lambada_standard": {
                "perplexity": 647.5605869289026,
                "perplexity_stderr": 29.442408532236634,
                "acc": 0.1340966427323889,
                "acc_stderr": 0.004747399033321078,
                "timestamp": "2024-06-13T19-08-46.936760"
            },
            "lambada_openai": {
                "perplexity": 727.0942907672784,
                "perplexity_stderr": 37.34894050370883,
                "acc": 0.14147098777411216,
                "acc_stderr": 0.004855380319784803,
                "timestamp": "2024-06-13T19-08-46.936760"
            },
            "mmlu_world_religions": {
                "acc": 0.29239766081871343,
                "acc_stderr": 0.03488647713457922,
                "brier_score": 0.8396180714465893,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_formal_logic": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.04006168083848877,
                "brier_score": 0.8348683170976435,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_prehistory": {
                "acc": 0.2191358024691358,
                "acc_stderr": 0.023016705640262213,
                "brier_score": 0.898568305536742,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.25251396648044694,
                "acc_stderr": 0.01453033020146864,
                "brier_score": 0.797675676064862,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.26582278481012656,
                "acc_stderr": 0.028756799629658335,
                "brier_score": 0.8232921151238497,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2630057803468208,
                "acc_stderr": 0.023703099525258165,
                "brier_score": 0.8652584497055802,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_professional_law": {
                "acc": 0.2457627118644068,
                "acc_stderr": 0.01099615663514269,
                "brier_score": 0.8576270672615085,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.22699386503067484,
                "acc_stderr": 0.032910995786157686,
                "brier_score": 0.9149466079655856,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.24019607843137256,
                "acc_stderr": 0.02998373305591361,
                "brier_score": 0.8328847382132503,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_philosophy": {
                "acc": 0.19292604501607716,
                "acc_stderr": 0.022411516780911363,
                "brier_score": 1.0091701908373931,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.04414343666854933,
                "brier_score": 0.8145598918889535,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_international_law": {
                "acc": 0.23140495867768596,
                "acc_stderr": 0.03849856098794088,
                "brier_score": 0.9721588320326391,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.23636363636363636,
                "acc_stderr": 0.03317505930009181,
                "brier_score": 0.8278142947763675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.20207253886010362,
                "acc_stderr": 0.02897908979429673,
                "brier_score": 0.959848409778557,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.21008403361344538,
                "acc_stderr": 0.026461398717471874,
                "brier_score": 0.928203227541235,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_geography": {
                "acc": 0.18181818181818182,
                "acc_stderr": 0.027479603010538804,
                "brier_score": 0.9445360520364772,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.2,
                "acc_stderr": 0.01714985851425095,
                "brier_score": 0.9262547542438837,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_public_relations": {
                "acc": 0.20909090909090908,
                "acc_stderr": 0.038950910157241364,
                "brier_score": 0.872707172893249,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.8664993053477293,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_sociology": {
                "acc": 0.24378109452736318,
                "acc_stderr": 0.030360490154014652,
                "brier_score": 1.0189674683196024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2230769230769231,
                "acc_stderr": 0.021107730127243995,
                "brier_score": 0.9392181616810883,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_security_studies": {
                "acc": 0.17551020408163265,
                "acc_stderr": 0.024352800722970015,
                "brier_score": 0.9534796271553114,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_professional_psychology": {
                "acc": 0.272875816993464,
                "acc_stderr": 0.018020474148393577,
                "brier_score": 0.9046486802299831,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_human_sexuality": {
                "acc": 0.22900763358778625,
                "acc_stderr": 0.036853466317118506,
                "brier_score": 0.9188173589415536,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.039994238792813365,
                "brier_score": 0.8627957595734717,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2515964240102171,
                "acc_stderr": 0.015517322365529631,
                "brier_score": 0.8501331895844128,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_marketing": {
                "acc": 0.28205128205128205,
                "acc_stderr": 0.02948036054954119,
                "brier_score": 0.8850419108960376,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "brier_score": 0.9488170330428574,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_nutrition": {
                "acc": 0.26143790849673204,
                "acc_stderr": 0.025160998214292456,
                "brier_score": 0.8710136973501563,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_medical_genetics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.8155383796623732,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_human_aging": {
                "acc": 0.3273542600896861,
                "acc_stderr": 0.03149384670994131,
                "brier_score": 0.8526030027294819,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_professional_medicine": {
                "acc": 0.20220588235294118,
                "acc_stderr": 0.024398192986654924,
                "brier_score": 0.8277778329217774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_college_medicine": {
                "acc": 0.20809248554913296,
                "acc_stderr": 0.030952890217749895,
                "brier_score": 0.9308911708837603,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_business_ethics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.8362462742598932,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.19622641509433963,
                "acc_stderr": 0.02444238813110082,
                "brier_score": 0.9179332108618161,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_global_facts": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.8688533991922952,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370518,
                "brier_score": 0.8792122675212317,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.026684564340460997,
                "brier_score": 0.8605899062455952,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_college_physics": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.0433643270799318,
                "brier_score": 0.8501212667575395,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_physics": {
                "acc": 0.26490066225165565,
                "acc_stderr": 0.036030385453603826,
                "brier_score": 0.8608175490841444,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_biology": {
                "acc": 0.20967741935483872,
                "acc_stderr": 0.023157879349083532,
                "brier_score": 0.8865733013303182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_college_biology": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.03685651095897532,
                "brier_score": 0.8503616280461068,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_anatomy": {
                "acc": 0.2,
                "acc_stderr": 0.03455473702325437,
                "brier_score": 0.8940578241939349,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_college_chemistry": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653695,
                "brier_score": 0.8989931116226676,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_computer_security": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.8061276351309151,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_college_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816505,
                "brier_score": 0.8625106698190292,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_astronomy": {
                "acc": 0.19078947368421054,
                "acc_stderr": 0.031975658210325,
                "brier_score": 0.8972433159767368,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_college_mathematics": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909282,
                "brier_score": 0.8340920973781174,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.24680851063829787,
                "acc_stderr": 0.028185441301234116,
                "brier_score": 0.8605806596292578,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.8899781796093954,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.837163754480655,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_machine_learning": {
                "acc": 0.26785714285714285,
                "acc_stderr": 0.04203277291467762,
                "brier_score": 0.8136091820043374,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.20689655172413793,
                "acc_stderr": 0.028501378167893946,
                "brier_score": 0.9127406347974003,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.19907407407407407,
                "acc_stderr": 0.027232298462690232,
                "brier_score": 0.9278457068781704,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25132275132275134,
                "acc_stderr": 0.022340482339643895,
                "brier_score": 0.8694753783996194,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2482758620689655,
                "acc_stderr": 0.03600105692727771,
                "brier_score": 0.8792815661151909,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24444444444444444,
                "acc_stderr": 0.02620276653465215,
                "brier_score": 0.8594830956920774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-11-02.066995"
            },
            "arc_challenge": {
                "acc": 0.17491467576791808,
                "acc_stderr": 0.011101562501828222,
                "acc_norm": 0.2090443686006826,
                "acc_norm_stderr": 0.011882746987406451,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "hellaswag": {
                "acc": 0.27185819557857,
                "acc_stderr": 0.004440079173277,
                "acc_norm": 0.2898824935271858,
                "acc_norm_stderr": 0.004527804016253785,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "truthfulqa_mc2": {
                "acc": 0.46361368860111973,
                "acc_stderr": 0.015624637210450354,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "truthfulqa_gen": {
                "bleu_max": 17.866027572769028,
                "bleu_max_stderr": 0.6178956466724292,
                "bleu_acc": 0.33414932680538556,
                "bleu_acc_stderr": 0.016512530677150555,
                "bleu_diff": -2.1708615982970616,
                "bleu_diff_stderr": 0.4770014144933768,
                "rouge1_max": 40.82248996851368,
                "rouge1_max_stderr": 0.78495594086117,
                "rouge1_acc": 0.3047735618115055,
                "rouge1_acc_stderr": 0.01611412415688242,
                "rouge1_diff": -4.957623701292929,
                "rouge1_diff_stderr": 0.6248240530678496,
                "rouge2_max": 24.375831054930796,
                "rouge2_max_stderr": 0.8575180753868562,
                "rouge2_acc": 0.23011015911872704,
                "rouge2_acc_stderr": 0.014734557959807763,
                "rouge2_diff": -4.351761806675208,
                "rouge2_diff_stderr": 0.6492003146837445,
                "rougeL_max": 37.79851538845665,
                "rougeL_max_stderr": 0.7858244663149364,
                "rougeL_acc": 0.3023255813953488,
                "rougeL_acc_stderr": 0.01607750926613303,
                "rougeL_diff": -4.666501260483323,
                "rougeL_diff_stderr": 0.6089020246878603,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "truthfulqa_mc1": {
                "acc": 0.2521419828641371,
                "acc_stderr": 0.015201522246299956,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "winogrande": {
                "acc": 0.5193370165745856,
                "acc_stderr": 0.014041972733712976,
                "timestamp": "2024-11-12T18-20-13.928795"
            }
        }
    }
}