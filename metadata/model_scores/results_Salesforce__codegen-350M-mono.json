{
    "model_name": "Salesforce/codegen-350M-mono",
    "last_updated": "2024-12-04 11:24:41.286914",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0018315018315018315,
                "exact_match_stderr": 0.0018315018315018335,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.004592422502870264,
                "exact_match_stderr": 0.0022922488477037863,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.001851851851851852,
                "exact_match_stderr": 0.0018518518518518504,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0022148394241417496,
                "exact_match_stderr": 0.0015652595934070638,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0020876826722338203,
                "exact_match_stderr": 0.002087682672233835,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.006329113924050633,
                "exact_match_stderr": 0.003646382041065048,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "minerva_math_algebra": {
                "exact_match": 0.006739679865206402,
                "exact_match_stderr": 0.0023757942810498744,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521431,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_2ds": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_1dc": {
                "acc": 0.0525,
                "acc_stderr": 0.004988418302285789,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000043,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_2dm": {
                "acc": 0.0205,
                "acc_stderr": 0.0031693686198869895,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "arithmetic_2da": {
                "acc": 0.013,
                "acc_stderr": 0.002533517190523328,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "gsm8k_cot": {
                "exact_match": 0.019711902956785442,
                "exact_match_stderr": 0.0038289829787357165,
                "timestamp": "2024-06-13T19-02-29.474320"
            },
            "gsm8k": {
                "exact_match": 0.017437452615617893,
                "exact_match_stderr": 0.0036054868679982334,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "anli_r2": {
                "brier_score": 0.8673139893251594,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "anli_r3": {
                "brier_score": 0.8478090822960272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "anli_r1": {
                "brier_score": 0.8706127110280617,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_eu": {
                "brier_score": 1.2682920496708858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_vi": {
                "brier_score": 1.2752175230857816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_ru": {
                "brier_score": 0.8045751050884926,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_zh": {
                "brier_score": 1.2459315046432844,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_tr": {
                "brier_score": 1.1658554315468868,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_fr": {
                "brier_score": 1.1841733511077182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_en": {
                "brier_score": 0.7473727928562213,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_ur": {
                "brier_score": 1.2271500572114633,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_ar": {
                "brier_score": 1.0042596344795447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_de": {
                "brier_score": 1.014094216947774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_hi": {
                "brier_score": 1.1401151042767916,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_es": {
                "brier_score": 1.0787926125248246,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_bg": {
                "brier_score": 1.1483978030694448,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_sw": {
                "brier_score": 0.9825310566958355,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_el": {
                "brier_score": 1.0833010621729644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "xnli_th": {
                "brier_score": 1.0701844940260412,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "logiqa2": {
                "brier_score": 1.2332295009469143,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "mathqa": {
                "brier_score": 0.9914231048948398,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T19-07-50.346128"
            },
            "lambada_standard": {
                "perplexity": 647.5605869289026,
                "perplexity_stderr": 29.442408532236634,
                "acc": 0.1340966427323889,
                "acc_stderr": 0.004747399033321078,
                "timestamp": "2024-06-13T19-08-46.936760"
            },
            "lambada_openai": {
                "perplexity": 727.0942907672784,
                "perplexity_stderr": 37.34894050370883,
                "acc": 0.14147098777411216,
                "acc_stderr": 0.004855380319784803,
                "timestamp": "2024-06-13T19-08-46.936760"
            },
            "mmlu_world_religions": {
                "acc": 0.29239766081871343,
                "acc_stderr": 0.03488647713457922,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.036700664510471825,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_prehistory": {
                "acc": 0.20679012345679013,
                "acc_stderr": 0.022535006705942818,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574885,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2320675105485232,
                "acc_stderr": 0.027479744550808514,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_moral_disputes": {
                "acc": 0.18497109826589594,
                "acc_stderr": 0.020903975842083033,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_professional_law": {
                "acc": 0.24902216427640156,
                "acc_stderr": 0.01104489226404077,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.25766871165644173,
                "acc_stderr": 0.03436150827846917,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.030778554678693264,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_philosophy": {
                "acc": 0.24115755627009647,
                "acc_stderr": 0.02429659403476343,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_jurisprudence": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.04133119440243838,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_international_law": {
                "acc": 0.24793388429752067,
                "acc_stderr": 0.03941897526516303,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.24242424242424243,
                "acc_stderr": 0.03346409881055953,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.3626943005181347,
                "acc_stderr": 0.03469713791704372,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.028657491285071973,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_geography": {
                "acc": 0.35353535353535354,
                "acc_stderr": 0.03406086723547153,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.30091743119266057,
                "acc_stderr": 0.019664751366802114,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_public_relations": {
                "acc": 0.20909090909090908,
                "acc_stderr": 0.03895091015724138,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_sociology": {
                "acc": 0.23880597014925373,
                "acc_stderr": 0.030147775935409224,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.33589743589743587,
                "acc_stderr": 0.023946724741563976,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_security_studies": {
                "acc": 0.27346938775510204,
                "acc_stderr": 0.02853556033712845,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2434640522875817,
                "acc_stderr": 0.017362473762146644,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2366412213740458,
                "acc_stderr": 0.03727673575596918,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_econometrics": {
                "acc": 0.21052631578947367,
                "acc_stderr": 0.0383515395439942,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_miscellaneous": {
                "acc": 0.25287356321839083,
                "acc_stderr": 0.01554337731371968,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_marketing": {
                "acc": 0.23931623931623933,
                "acc_stderr": 0.027951826808924333,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_management": {
                "acc": 0.20388349514563106,
                "acc_stderr": 0.03989139859531772,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_nutrition": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.024954184324879912,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_human_aging": {
                "acc": 0.21973094170403587,
                "acc_stderr": 0.027790177064383602,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_professional_medicine": {
                "acc": 0.3602941176470588,
                "acc_stderr": 0.029163128570670733,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_college_medicine": {
                "acc": 0.2543352601156069,
                "acc_stderr": 0.0332055644308557,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_business_ethics": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252604,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2792452830188679,
                "acc_stderr": 0.027611163402399715,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.038612291966536955,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_virology": {
                "acc": 0.1927710843373494,
                "acc_stderr": 0.03070982405056527,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.026684564340460997,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_college_physics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.040233822736177476,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2781456953642384,
                "acc_stderr": 0.03658603262763743,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_biology": {
                "acc": 0.29354838709677417,
                "acc_stderr": 0.02590608702131929,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_anatomy": {
                "acc": 0.24444444444444444,
                "acc_stderr": 0.037125378336148665,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_college_chemistry": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_computer_security": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_college_computer_science": {
                "acc": 0.38,
                "acc_stderr": 0.048783173121456316,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_astronomy": {
                "acc": 0.18421052631578946,
                "acc_stderr": 0.0315469804508223,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_college_mathematics": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.33617021276595743,
                "acc_stderr": 0.030881618520676942,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_machine_learning": {
                "acc": 0.25892857142857145,
                "acc_stderr": 0.04157751539865629,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.28078817733990147,
                "acc_stderr": 0.03161856335358609,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4444444444444444,
                "acc_stderr": 0.03388857118502326,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.25132275132275134,
                "acc_stderr": 0.022340482339643895,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2482758620689655,
                "acc_stderr": 0.03600105692727771,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2518518518518518,
                "acc_stderr": 0.026466117538959905,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "arc_challenge": {
                "acc": 0.17491467576791808,
                "acc_stderr": 0.011101562501828222,
                "acc_norm": 0.2090443686006826,
                "acc_norm_stderr": 0.011882746987406451,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "hellaswag": {
                "acc": 0.27185819557857,
                "acc_stderr": 0.004440079173277,
                "acc_norm": 0.2898824935271858,
                "acc_norm_stderr": 0.004527804016253785,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "truthfulqa_mc2": {
                "acc": 0.46361368860111973,
                "acc_stderr": 0.015624637210450354,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "truthfulqa_gen": {
                "bleu_max": 17.866027572769028,
                "bleu_max_stderr": 0.6178956466724292,
                "bleu_acc": 0.33414932680538556,
                "bleu_acc_stderr": 0.016512530677150555,
                "bleu_diff": -2.1708615982970616,
                "bleu_diff_stderr": 0.4770014144933768,
                "rouge1_max": 40.82248996851368,
                "rouge1_max_stderr": 0.78495594086117,
                "rouge1_acc": 0.3047735618115055,
                "rouge1_acc_stderr": 0.01611412415688242,
                "rouge1_diff": -4.957623701292929,
                "rouge1_diff_stderr": 0.6248240530678496,
                "rouge2_max": 24.375831054930796,
                "rouge2_max_stderr": 0.8575180753868562,
                "rouge2_acc": 0.23011015911872704,
                "rouge2_acc_stderr": 0.014734557959807763,
                "rouge2_diff": -4.351761806675208,
                "rouge2_diff_stderr": 0.6492003146837445,
                "rougeL_max": 37.79851538845665,
                "rougeL_max_stderr": 0.7858244663149364,
                "rougeL_acc": 0.3023255813953488,
                "rougeL_acc_stderr": 0.01607750926613303,
                "rougeL_diff": -4.666501260483323,
                "rougeL_diff_stderr": 0.6089020246878603,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "truthfulqa_mc1": {
                "acc": 0.2521419828641371,
                "acc_stderr": 0.015201522246299956,
                "timestamp": "2024-11-12T18-20-13.928795"
            },
            "winogrande": {
                "acc": 0.5193370165745856,
                "acc_stderr": 0.014041972733712976,
                "timestamp": "2024-11-12T18-20-13.928795"
            }
        }
    }
}