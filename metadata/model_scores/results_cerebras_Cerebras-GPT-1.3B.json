{
    "model_name": "cerebras/Cerebras-GPT-1.3B",
    "last_updated": "2023-10-16",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.23720136518771331,
                    "acc_stderr": 0.01243039982926084,
                    "acc_norm": 0.2627986348122867,
                    "acc_norm_stderr": 0.012862523175351333,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.32901812387970525,
                    "acc_stderr": 0.004688963175758139,
                    "acc_norm": 0.385381398127863,
                    "acc_norm_stderr": 0.004856906473719379,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2074074074074074,
                    "acc_stderr": 0.03502553170678316,
                    "acc_norm": 0.2074074074074074,
                    "acc_norm_stderr": 0.03502553170678316,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.21052631578947367,
                    "acc_stderr": 0.033176727875331574,
                    "acc_norm": 0.21052631578947367,
                    "acc_norm_stderr": 0.033176727875331574,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322695,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.041633319989322695,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2679245283018868,
                    "acc_stderr": 0.02725726032249485,
                    "acc_norm": 0.2679245283018868,
                    "acc_norm_stderr": 0.02725726032249485,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.24305555555555555,
                    "acc_stderr": 0.03586879280080343,
                    "acc_norm": 0.24305555555555555,
                    "acc_norm_stderr": 0.03586879280080343,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909284,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909284,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695235,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.04760952285695235,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2658959537572254,
                    "acc_stderr": 0.03368762932259431,
                    "acc_norm": 0.2658959537572254,
                    "acc_norm_stderr": 0.03368762932259431,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.041583075330832865,
                    "acc_norm": 0.22549019607843138,
                    "acc_norm_stderr": 0.041583075330832865,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.04093601807403326,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.04093601807403326,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2765957446808511,
                    "acc_stderr": 0.029241883869628806,
                    "acc_norm": 0.2765957446808511,
                    "acc_norm_stderr": 0.029241883869628806,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.24561403508771928,
                    "acc_stderr": 0.040493392977481404,
                    "acc_norm": 0.24561403508771928,
                    "acc_norm_stderr": 0.040493392977481404,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.23448275862068965,
                    "acc_stderr": 0.035306258743465914,
                    "acc_norm": 0.23448275862068965,
                    "acc_norm_stderr": 0.035306258743465914,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.022019080012217893,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.022019080012217893,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.2619047619047619,
                    "acc_stderr": 0.039325376803928704,
                    "acc_norm": 0.2619047619047619,
                    "acc_norm_stderr": 0.039325376803928704,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.2032258064516129,
                    "acc_stderr": 0.022891687984554963,
                    "acc_norm": 0.2032258064516129,
                    "acc_norm_stderr": 0.022891687984554963,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.22660098522167488,
                    "acc_stderr": 0.029454863835292965,
                    "acc_norm": 0.22660098522167488,
                    "acc_norm_stderr": 0.029454863835292965,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.2606060606060606,
                    "acc_stderr": 0.03427743175816524,
                    "acc_norm": 0.2606060606060606,
                    "acc_norm_stderr": 0.03427743175816524,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.31313131313131315,
                    "acc_stderr": 0.033042050878136525,
                    "acc_norm": 0.31313131313131315,
                    "acc_norm_stderr": 0.033042050878136525,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.3471502590673575,
                    "acc_stderr": 0.034356961683613546,
                    "acc_norm": 0.3471502590673575,
                    "acc_norm_stderr": 0.034356961683613546,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.3230769230769231,
                    "acc_stderr": 0.023710888501970562,
                    "acc_norm": 0.3230769230769231,
                    "acc_norm_stderr": 0.023710888501970562,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.026719240783712166,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.026719240783712166,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.22268907563025211,
                    "acc_stderr": 0.027025433498882364,
                    "acc_norm": 0.22268907563025211,
                    "acc_norm_stderr": 0.027025433498882364,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.23841059602649006,
                    "acc_stderr": 0.03479185572599659,
                    "acc_norm": 0.23841059602649006,
                    "acc_norm_stderr": 0.03479185572599659,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.3431192660550459,
                    "acc_stderr": 0.02035477773608604,
                    "acc_norm": 0.3431192660550459,
                    "acc_norm_stderr": 0.02035477773608604,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4722222222222222,
                    "acc_stderr": 0.0340470532865388,
                    "acc_norm": 0.4722222222222222,
                    "acc_norm_stderr": 0.0340470532865388,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.22058823529411764,
                    "acc_stderr": 0.02910225438967409,
                    "acc_norm": 0.22058823529411764,
                    "acc_norm_stderr": 0.02910225438967409,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.2742616033755274,
                    "acc_stderr": 0.029041333510598014,
                    "acc_norm": 0.2742616033755274,
                    "acc_norm_stderr": 0.029041333510598014,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.23318385650224216,
                    "acc_stderr": 0.028380391147094716,
                    "acc_norm": 0.23318385650224216,
                    "acc_norm_stderr": 0.028380391147094716,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.037276735755969174,
                    "acc_norm": 0.2366412213740458,
                    "acc_norm_stderr": 0.037276735755969174,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.2809917355371901,
                    "acc_stderr": 0.04103203830514512,
                    "acc_norm": 0.2809917355371901,
                    "acc_norm_stderr": 0.04103203830514512,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.04133119440243839,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.04133119440243839,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.3006134969325153,
                    "acc_stderr": 0.03602511318806771,
                    "acc_norm": 0.3006134969325153,
                    "acc_norm_stderr": 0.03602511318806771,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.30357142857142855,
                    "acc_stderr": 0.04364226155841044,
                    "acc_norm": 0.30357142857142855,
                    "acc_norm_stderr": 0.04364226155841044,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.1941747572815534,
                    "acc_stderr": 0.039166677628225836,
                    "acc_norm": 0.1941747572815534,
                    "acc_norm_stderr": 0.039166677628225836,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.2948717948717949,
                    "acc_stderr": 0.029872577708891155,
                    "acc_norm": 0.2948717948717949,
                    "acc_norm_stderr": 0.029872577708891155,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542129,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542129,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.24648786717752236,
                    "acc_stderr": 0.015411308769686929,
                    "acc_norm": 0.24648786717752236,
                    "acc_norm_stderr": 0.015411308769686929,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2630057803468208,
                    "acc_stderr": 0.023703099525258172,
                    "acc_norm": 0.2630057803468208,
                    "acc_norm_stderr": 0.023703099525258172,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.23687150837988827,
                    "acc_stderr": 0.014219570788103982,
                    "acc_norm": 0.23687150837988827,
                    "acc_norm_stderr": 0.014219570788103982,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.29411764705882354,
                    "acc_stderr": 0.026090162504279042,
                    "acc_norm": 0.29411764705882354,
                    "acc_norm_stderr": 0.026090162504279042,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.26366559485530544,
                    "acc_stderr": 0.02502553850053234,
                    "acc_norm": 0.26366559485530544,
                    "acc_norm_stderr": 0.02502553850053234,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2654320987654321,
                    "acc_stderr": 0.02456922360046085,
                    "acc_norm": 0.2654320987654321,
                    "acc_norm_stderr": 0.02456922360046085,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2801418439716312,
                    "acc_stderr": 0.02678917235114023,
                    "acc_norm": 0.2801418439716312,
                    "acc_norm_stderr": 0.02678917235114023,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2379400260756193,
                    "acc_stderr": 0.010875700787694242,
                    "acc_norm": 0.2379400260756193,
                    "acc_norm_stderr": 0.010875700787694242,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.41911764705882354,
                    "acc_stderr": 0.02997280717046462,
                    "acc_norm": 0.41911764705882354,
                    "acc_norm_stderr": 0.02997280717046462,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.018120224251484587,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.018120224251484587,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.16363636363636364,
                    "acc_stderr": 0.03543433054298678,
                    "acc_norm": 0.16363636363636364,
                    "acc_norm_stderr": 0.03543433054298678,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.23673469387755103,
                    "acc_stderr": 0.027212835884073142,
                    "acc_norm": 0.23673469387755103,
                    "acc_norm_stderr": 0.027212835884073142,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.22885572139303484,
                    "acc_stderr": 0.029705284056772422,
                    "acc_norm": 0.22885572139303484,
                    "acc_norm_stderr": 0.029705284056772422,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.03942772444036625,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.03942772444036625,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3313253012048193,
                    "acc_stderr": 0.036643147772880864,
                    "acc_norm": 0.3313253012048193,
                    "acc_norm_stderr": 0.036643147772880864,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.30409356725146197,
                    "acc_stderr": 0.03528211258245232,
                    "acc_norm": 0.30409356725146197,
                    "acc_norm_stderr": 0.03528211258245232,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.24479804161566707,
                    "mc1_stderr": 0.01505186948671501,
                    "mc2": 0.4269871364718513,
                    "mc2_stderr": 0.014897248723095273,
                    "timestamp": "2023-07-18T11-08-05.365000"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.00041946308724832214,
                    "em_stderr": 0.00020969854707829098,
                    "f1": 0.03696203859060411,
                    "f1_stderr": 0.0010536462556224307,
                    "timestamp": "2023-10-16T21-53-03.543660"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.002274450341167551,
                    "acc_stderr": 0.0013121578148673984,
                    "timestamp": "2023-10-16T21-53-03.543660"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5343330702446725,
                    "acc_stderr": 0.014019317531542565,
                    "timestamp": "2023-10-16T21-53-03.543660"
                }
            }
        }
    }
}