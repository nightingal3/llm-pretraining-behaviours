{
    "model_name": "Dampish/StellarX-4B-V0",
    "last_updated": "2023-10-25",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.32337883959044367,
                    "acc_stderr": 0.013669421630012125,
                    "acc_norm": 0.36945392491467577,
                    "acc_norm_stderr": 0.0141045783664919,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.4584744074885481,
                    "acc_stderr": 0.004972543127767877,
                    "acc_norm": 0.6190001991635132,
                    "acc_norm_stderr": 0.004846400325585233,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.047609522856952365,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.047609522856952365,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.03749850709174022,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.03749850709174022,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.34210526315789475,
                    "acc_stderr": 0.03860731599316092,
                    "acc_norm": 0.34210526315789475,
                    "acc_norm_stderr": 0.03860731599316092,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816506,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816506,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2830188679245283,
                    "acc_stderr": 0.0277242364927009,
                    "acc_norm": 0.2830188679245283,
                    "acc_norm_stderr": 0.0277242364927009,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.24305555555555555,
                    "acc_stderr": 0.03586879280080341,
                    "acc_norm": 0.24305555555555555,
                    "acc_norm_stderr": 0.03586879280080341,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.0440844002276808,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.0440844002276808,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252606,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252606,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.04824181513244218,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.04824181513244218,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.26011560693641617,
                    "acc_stderr": 0.03345036916788991,
                    "acc_norm": 0.26011560693641617,
                    "acc_norm_stderr": 0.03345036916788991,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.04023382273617747,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.04023382273617747,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.045126085985421276,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.045126085985421276,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2851063829787234,
                    "acc_stderr": 0.02951319662553935,
                    "acc_norm": 0.2851063829787234,
                    "acc_norm_stderr": 0.02951319662553935,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2719298245614035,
                    "acc_stderr": 0.04185774424022056,
                    "acc_norm": 0.2719298245614035,
                    "acc_norm_stderr": 0.04185774424022056,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.296551724137931,
                    "acc_stderr": 0.03806142687309993,
                    "acc_norm": 0.296551724137931,
                    "acc_norm_stderr": 0.03806142687309993,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.26455026455026454,
                    "acc_stderr": 0.022717467897708617,
                    "acc_norm": 0.26455026455026454,
                    "acc_norm_stderr": 0.022717467897708617,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.15079365079365079,
                    "acc_stderr": 0.03200686497287394,
                    "acc_norm": 0.15079365079365079,
                    "acc_norm_stderr": 0.03200686497287394,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.22903225806451613,
                    "acc_stderr": 0.02390491431178265,
                    "acc_norm": 0.22903225806451613,
                    "acc_norm_stderr": 0.02390491431178265,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2512315270935961,
                    "acc_stderr": 0.030516530732694433,
                    "acc_norm": 0.2512315270935961,
                    "acc_norm_stderr": 0.030516530732694433,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909282,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909282,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.23030303030303031,
                    "acc_stderr": 0.03287666758603489,
                    "acc_norm": 0.23030303030303031,
                    "acc_norm_stderr": 0.03287666758603489,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.35353535353535354,
                    "acc_stderr": 0.03406086723547153,
                    "acc_norm": 0.35353535353535354,
                    "acc_norm_stderr": 0.03406086723547153,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.26424870466321243,
                    "acc_stderr": 0.031821550509166484,
                    "acc_norm": 0.26424870466321243,
                    "acc_norm_stderr": 0.031821550509166484,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2512820512820513,
                    "acc_stderr": 0.02199201666237056,
                    "acc_norm": 0.2512820512820513,
                    "acc_norm_stderr": 0.02199201666237056,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.025348097468097838,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.025348097468097838,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.19327731092436976,
                    "acc_stderr": 0.0256494702658892,
                    "acc_norm": 0.19327731092436976,
                    "acc_norm_stderr": 0.0256494702658892,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.271523178807947,
                    "acc_stderr": 0.03631329803969653,
                    "acc_norm": 0.271523178807947,
                    "acc_norm_stderr": 0.03631329803969653,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.3284403669724771,
                    "acc_stderr": 0.020135902797298395,
                    "acc_norm": 0.3284403669724771,
                    "acc_norm_stderr": 0.020135902797298395,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.3287037037037037,
                    "acc_stderr": 0.032036140846700596,
                    "acc_norm": 0.3287037037037037,
                    "acc_norm_stderr": 0.032036140846700596,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.24019607843137256,
                    "acc_stderr": 0.02998373305591362,
                    "acc_norm": 0.24019607843137256,
                    "acc_norm_stderr": 0.02998373305591362,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.270042194092827,
                    "acc_stderr": 0.028900721906293426,
                    "acc_norm": 0.270042194092827,
                    "acc_norm_stderr": 0.028900721906293426,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.11659192825112108,
                    "acc_stderr": 0.02153963981624447,
                    "acc_norm": 0.11659192825112108,
                    "acc_norm_stderr": 0.02153963981624447,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.24427480916030533,
                    "acc_stderr": 0.03768335959728744,
                    "acc_norm": 0.24427480916030533,
                    "acc_norm_stderr": 0.03768335959728744,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.4049586776859504,
                    "acc_stderr": 0.044811377559424694,
                    "acc_norm": 0.4049586776859504,
                    "acc_norm_stderr": 0.044811377559424694,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04186091791394607,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04186091791394607,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.24539877300613497,
                    "acc_stderr": 0.03380939813943354,
                    "acc_norm": 0.24539877300613497,
                    "acc_norm_stderr": 0.03380939813943354,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.2767857142857143,
                    "acc_stderr": 0.04246624336697625,
                    "acc_norm": 0.2767857142857143,
                    "acc_norm_stderr": 0.04246624336697625,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.30097087378640774,
                    "acc_stderr": 0.045416094465039476,
                    "acc_norm": 0.30097087378640774,
                    "acc_norm_stderr": 0.045416094465039476,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.29914529914529914,
                    "acc_stderr": 0.029996951858349483,
                    "acc_norm": 0.29914529914529914,
                    "acc_norm_stderr": 0.029996951858349483,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.28735632183908044,
                    "acc_stderr": 0.0161824107306827,
                    "acc_norm": 0.28735632183908044,
                    "acc_norm_stderr": 0.0161824107306827,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.24855491329479767,
                    "acc_stderr": 0.023267528432100174,
                    "acc_norm": 0.24855491329479767,
                    "acc_norm_stderr": 0.023267528432100174,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.27262569832402234,
                    "acc_stderr": 0.014893391735249588,
                    "acc_norm": 0.27262569832402234,
                    "acc_norm_stderr": 0.014893391735249588,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2549019607843137,
                    "acc_stderr": 0.02495418432487991,
                    "acc_norm": 0.2549019607843137,
                    "acc_norm_stderr": 0.02495418432487991,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.28938906752411575,
                    "acc_stderr": 0.02575586592263294,
                    "acc_norm": 0.28938906752411575,
                    "acc_norm_stderr": 0.02575586592263294,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2654320987654321,
                    "acc_stderr": 0.024569223600460852,
                    "acc_norm": 0.2654320987654321,
                    "acc_norm_stderr": 0.024569223600460852,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2695035460992908,
                    "acc_stderr": 0.026469036818590627,
                    "acc_norm": 0.2695035460992908,
                    "acc_norm_stderr": 0.026469036818590627,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2633637548891786,
                    "acc_stderr": 0.011249506403605291,
                    "acc_norm": 0.2633637548891786,
                    "acc_norm_stderr": 0.011249506403605291,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.2426470588235294,
                    "acc_stderr": 0.02604066247420126,
                    "acc_norm": 0.2426470588235294,
                    "acc_norm_stderr": 0.02604066247420126,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.24019607843137256,
                    "acc_stderr": 0.017282760695167414,
                    "acc_norm": 0.24019607843137256,
                    "acc_norm_stderr": 0.017282760695167414,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.33636363636363636,
                    "acc_stderr": 0.04525393596302506,
                    "acc_norm": 0.33636363636363636,
                    "acc_norm_stderr": 0.04525393596302506,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.23673469387755103,
                    "acc_stderr": 0.02721283588407314,
                    "acc_norm": 0.23673469387755103,
                    "acc_norm_stderr": 0.02721283588407314,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.24378109452736318,
                    "acc_stderr": 0.03036049015401468,
                    "acc_norm": 0.24378109452736318,
                    "acc_norm_stderr": 0.03036049015401468,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768078,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768078,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.23493975903614459,
                    "acc_stderr": 0.03300533186128922,
                    "acc_norm": 0.23493975903614459,
                    "acc_norm_stderr": 0.03300533186128922,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.29239766081871343,
                    "acc_stderr": 0.034886477134579215,
                    "acc_norm": 0.29239766081871343,
                    "acc_norm_stderr": 0.034886477134579215,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.20685434516523868,
                    "mc1_stderr": 0.014179591496728343,
                    "mc2": 0.34296822571733665,
                    "mc2_stderr": 0.013628027163865984,
                    "timestamp": "2023-10-03T17-57-03.227360"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.0269505033557047,
                    "em_stderr": 0.0016584048452624597,
                    "f1": 0.1094840604026844,
                    "f1_stderr": 0.002280673263690395,
                    "timestamp": "2023-10-25T05-01-15.065069"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.0,
                    "acc_stderr": 0.0,
                    "timestamp": "2023-10-25T05-01-15.065069"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.6385161799526441,
                    "acc_stderr": 0.013502479670791294,
                    "timestamp": "2023-10-25T05-01-15.065069"
                }
            }
        }
    }
}