{
    "model_name": "EleutherAI/pythia-6.9b",
    "last_updated": "2024-12-04 11:24:23.872880",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.003663003663003663,
                "exact_match_stderr": 0.0025877573681934475,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.029850746268656716,
                "exact_match_stderr": 0.0057694876379482535,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.005555555555555556,
                "exact_match_stderr": 0.0032015451273209095,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.018826135105204873,
                "exact_match_stderr": 0.004525330498668475,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_geometry": {
                "exact_match": 0.018789144050104383,
                "exact_match_stderr": 0.006210416427997405,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.014767932489451477,
                "exact_match_stderr": 0.005546238589668472,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_algebra": {
                "exact_match": 0.016006739679865205,
                "exact_match_stderr": 0.0036442247924417305,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_3da": {
                "acc": 0.007,
                "acc_stderr": 0.0018647355360237436,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_3ds": {
                "acc": 0.0085,
                "acc_stderr": 0.002053285901060985,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_2ds": {
                "acc": 0.093,
                "acc_stderr": 0.006495890878020431,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_1dc": {
                "acc": 0.0565,
                "acc_stderr": 0.0051640302675624965,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.000500000000000002,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_2dm": {
                "acc": 0.054,
                "acc_stderr": 0.005055173329243414,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_2da": {
                "acc": 0.053,
                "acc_stderr": 0.005010793752192678,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "gsm8k_cot": {
                "exact_match": 0.029567854435178165,
                "exact_match_stderr": 0.004665893134220779,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "gsm8k": {
                "exact_match": 0.02880970432145565,
                "exact_match_stderr": 0.004607484283767438,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "anli_r2": {
                "brier_score": 0.8480871011410138,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "anli_r3": {
                "brier_score": 0.8179776021045503,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "anli_r1": {
                "brier_score": 0.8722694615179778,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_eu": {
                "brier_score": 0.8999839016489077,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_vi": {
                "brier_score": 0.8204605652395003,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_ru": {
                "brier_score": 0.8106878224944725,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_zh": {
                "brier_score": 1.09036580135667,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_tr": {
                "brier_score": 0.8878553632357027,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_fr": {
                "brier_score": 0.7961302677354256,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_en": {
                "brier_score": 0.653954455047553,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_ur": {
                "brier_score": 1.191915875742246,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_ar": {
                "brier_score": 1.2272435798439718,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_de": {
                "brier_score": 0.828333282541149,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_hi": {
                "brier_score": 0.801853810502381,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_es": {
                "brier_score": 0.8599668349851329,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_bg": {
                "brier_score": 0.7686484176938864,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_sw": {
                "brier_score": 0.8502894613384985,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_el": {
                "brier_score": 0.8562714843114025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_th": {
                "brier_score": 0.7751103774504331,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "logiqa2": {
                "brier_score": 1.165272169859693,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "mathqa": {
                "brier_score": 0.95247880187882,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "lambada_standard": {
                "perplexity": 6.937810377810337,
                "perplexity_stderr": 0.1757119002721059,
                "acc": 0.5587036677663497,
                "acc_stderr": 0.006917799856218216,
                "timestamp": "2024-06-07T21-53-35.445617"
            },
            "lambada_openai": {
                "perplexity": 4.459300840447834,
                "perplexity_stderr": 0.10000526645129007,
                "acc": 0.6710653987968174,
                "acc_stderr": 0.006545597195850585,
                "timestamp": "2024-06-07T21-53-35.445617"
            },
            "mmlu_world_religions": {
                "acc": 0.29239766081871343,
                "acc_stderr": 0.034886477134579215,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_formal_logic": {
                "acc": 0.15873015873015872,
                "acc_stderr": 0.03268454013011744,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_prehistory": {
                "acc": 0.23765432098765432,
                "acc_stderr": 0.02368359183700856,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.014422292204808838,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2742616033755274,
                "acc_stderr": 0.02904133351059804,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2658959537572254,
                "acc_stderr": 0.023786203255508283,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_professional_law": {
                "acc": 0.23859191655801826,
                "acc_stderr": 0.010885929742002212,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.294478527607362,
                "acc_stderr": 0.03581165790474082,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.03096451792692339,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_philosophy": {
                "acc": 0.2733118971061093,
                "acc_stderr": 0.025311765975426115,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_jurisprudence": {
                "acc": 0.3055555555555556,
                "acc_stderr": 0.044531975073749834,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_international_law": {
                "acc": 0.38016528925619836,
                "acc_stderr": 0.04431324501968432,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.24848484848484848,
                "acc_stderr": 0.03374402644139405,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.24352331606217617,
                "acc_stderr": 0.03097543638684543,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.22268907563025211,
                "acc_stderr": 0.02702543349888236,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_geography": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.02962022787479048,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23119266055045873,
                "acc_stderr": 0.018075750241633163,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_public_relations": {
                "acc": 0.35454545454545455,
                "acc_stderr": 0.045820048415054174,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816508,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_sociology": {
                "acc": 0.25870646766169153,
                "acc_stderr": 0.030965903123573026,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.30512820512820515,
                "acc_stderr": 0.023346335293325887,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_security_studies": {
                "acc": 0.17551020408163265,
                "acc_stderr": 0.024352800722970015,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2679738562091503,
                "acc_stderr": 0.017917974069594722,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_human_sexuality": {
                "acc": 0.24427480916030533,
                "acc_stderr": 0.037683359597287434,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_econometrics": {
                "acc": 0.21929824561403508,
                "acc_stderr": 0.03892431106518753,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_miscellaneous": {
                "acc": 0.29246487867177523,
                "acc_stderr": 0.016267000684598645,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_marketing": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.0272360139461967,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_management": {
                "acc": 0.27184466019417475,
                "acc_stderr": 0.044052680241409216,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_nutrition": {
                "acc": 0.2581699346405229,
                "acc_stderr": 0.025058503316958157,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_medical_genetics": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909281,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_human_aging": {
                "acc": 0.28699551569506726,
                "acc_stderr": 0.030360379710291954,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_professional_medicine": {
                "acc": 0.3125,
                "acc_stderr": 0.02815637344037142,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_college_medicine": {
                "acc": 0.19653179190751446,
                "acc_stderr": 0.030299574664788147,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_business_ethics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.25660377358490566,
                "acc_stderr": 0.02688064788905197,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_global_facts": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_virology": {
                "acc": 0.3192771084337349,
                "acc_stderr": 0.036293353299478595,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_professional_accounting": {
                "acc": 0.29432624113475175,
                "acc_stderr": 0.027187127011503796,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_college_physics": {
                "acc": 0.24509803921568626,
                "acc_stderr": 0.04280105837364396,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_physics": {
                "acc": 0.25165562913907286,
                "acc_stderr": 0.035433042343899844,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_biology": {
                "acc": 0.24838709677419354,
                "acc_stderr": 0.024580028921480992,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_college_biology": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.03745554791462457,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_anatomy": {
                "acc": 0.34814814814814815,
                "acc_stderr": 0.041153246103369526,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_college_chemistry": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653696,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_college_computer_science": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_astronomy": {
                "acc": 0.29605263157894735,
                "acc_stderr": 0.03715062154998905,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_college_mathematics": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.32340425531914896,
                "acc_stderr": 0.030579442773610348,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_machine_learning": {
                "acc": 0.26785714285714285,
                "acc_stderr": 0.04203277291467762,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2512315270935961,
                "acc_stderr": 0.030516530732694436,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.3425925925925926,
                "acc_stderr": 0.03236585252602157,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2830687830687831,
                "acc_stderr": 0.023201392938194978,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.27586206896551724,
                "acc_stderr": 0.037245636197746325,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2111111111111111,
                "acc_stderr": 0.02488211685765508,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "arc_challenge": {
                "acc": 0.3575085324232082,
                "acc_stderr": 0.014005494275916576,
                "acc_norm": 0.39590443686006827,
                "acc_norm_stderr": 0.014291228393536587,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "hellaswag": {
                "acc": 0.4816769567815176,
                "acc_stderr": 0.004986429808146771,
                "acc_norm": 0.6514638518223461,
                "acc_norm_stderr": 0.004755329243976686,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "truthfulqa_mc2": {
                "acc": 0.3515928441032545,
                "acc_stderr": 0.013622121924448312,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.822049251809254,
                "bleu_max_stderr": 0.7441915100142679,
                "bleu_acc": 0.2876376988984088,
                "bleu_acc_stderr": 0.015846315101394795,
                "bleu_diff": -8.72973777939928,
                "bleu_diff_stderr": 0.799560880574795,
                "rouge1_max": 50.64779573586556,
                "rouge1_max_stderr": 0.8314230941524794,
                "rouge1_acc": 0.2692778457772338,
                "rouge1_acc_stderr": 0.015528566637087264,
                "rouge1_diff": -11.027700507612522,
                "rouge1_diff_stderr": 0.8410289087382377,
                "rouge2_max": 33.75326270225714,
                "rouge2_max_stderr": 0.9630024578384584,
                "rouge2_acc": 0.22643818849449204,
                "rouge2_acc_stderr": 0.014651337324602585,
                "rouge2_diff": -13.190984697589347,
                "rouge2_diff_stderr": 1.019946221960148,
                "rougeL_max": 47.78257310566027,
                "rougeL_max_stderr": 0.8446585069903312,
                "rougeL_acc": 0.2594859241126071,
                "rougeL_acc_stderr": 0.015345409485558,
                "rougeL_diff": -11.191544484143854,
                "rougeL_diff_stderr": 0.859662499239064,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "truthfulqa_mc1": {
                "acc": 0.2178702570379437,
                "acc_stderr": 0.014450846714123899,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "winogrande": {
                "acc": 0.6400947119179163,
                "acc_stderr": 0.013489609590266797,
                "timestamp": "2024-11-12T22-04-18.010150"
            }
        }
    }
}