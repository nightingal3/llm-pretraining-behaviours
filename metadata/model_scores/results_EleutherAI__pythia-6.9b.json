{
    "model_name": "EleutherAI/pythia-6.9b",
    "last_updated": "2024-12-19 13:40:15.926814",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.003663003663003663,
                "exact_match_stderr": 0.0025877573681934475,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.029850746268656716,
                "exact_match_stderr": 0.0057694876379482535,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.005555555555555556,
                "exact_match_stderr": 0.0032015451273209095,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.018826135105204873,
                "exact_match_stderr": 0.004525330498668475,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_geometry": {
                "exact_match": 0.018789144050104383,
                "exact_match_stderr": 0.006210416427997405,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.014767932489451477,
                "exact_match_stderr": 0.005546238589668472,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "minerva_math_algebra": {
                "exact_match": 0.016006739679865205,
                "exact_match_stderr": 0.0036442247924417305,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_3da": {
                "acc": 0.007,
                "acc_stderr": 0.0018647355360237436,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_3ds": {
                "acc": 0.0085,
                "acc_stderr": 0.002053285901060985,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_2ds": {
                "acc": 0.093,
                "acc_stderr": 0.006495890878020431,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_1dc": {
                "acc": 0.0565,
                "acc_stderr": 0.0051640302675624965,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.000500000000000002,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_2dm": {
                "acc": 0.054,
                "acc_stderr": 0.005055173329243414,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "arithmetic_2da": {
                "acc": 0.053,
                "acc_stderr": 0.005010793752192678,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "gsm8k_cot": {
                "exact_match": 0.029567854435178165,
                "exact_match_stderr": 0.004665893134220779,
                "timestamp": "2024-06-06T00-02-44.928696"
            },
            "gsm8k": {
                "exact_match": 0.02880970432145565,
                "exact_match_stderr": 0.004607484283767438,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "anli_r2": {
                "brier_score": 0.8480871011410138,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "anli_r3": {
                "brier_score": 0.8179776021045503,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "anli_r1": {
                "brier_score": 0.8722694615179778,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_eu": {
                "brier_score": 0.8999839016489077,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_vi": {
                "brier_score": 0.8204605652395003,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_ru": {
                "brier_score": 0.8106878224944725,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_zh": {
                "brier_score": 1.09036580135667,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_tr": {
                "brier_score": 0.8878553632357027,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_fr": {
                "brier_score": 0.7961302677354256,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_en": {
                "brier_score": 0.653954455047553,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_ur": {
                "brier_score": 1.191915875742246,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_ar": {
                "brier_score": 1.2272435798439718,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_de": {
                "brier_score": 0.828333282541149,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_hi": {
                "brier_score": 0.801853810502381,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_es": {
                "brier_score": 0.8599668349851329,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_bg": {
                "brier_score": 0.7686484176938864,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_sw": {
                "brier_score": 0.8502894613384985,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_el": {
                "brier_score": 0.8562714843114025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "xnli_th": {
                "brier_score": 0.7751103774504331,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "logiqa2": {
                "brier_score": 1.165272169859693,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "mathqa": {
                "brier_score": 0.95247880187882,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-06T01-36-28.878651"
            },
            "lambada_standard": {
                "perplexity": 6.937810377810337,
                "perplexity_stderr": 0.1757119002721059,
                "acc": 0.5587036677663497,
                "acc_stderr": 0.006917799856218216,
                "timestamp": "2024-06-07T21-53-35.445617"
            },
            "lambada_openai": {
                "perplexity": 4.459300840447834,
                "perplexity_stderr": 0.10000526645129007,
                "acc": 0.6710653987968174,
                "acc_stderr": 0.006545597195850585,
                "timestamp": "2024-06-07T21-53-35.445617"
            },
            "mmlu_world_religions": {
                "acc": 0.30994152046783624,
                "acc_stderr": 0.03546976959393163,
                "brier_score": 0.7607541360437958,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_formal_logic": {
                "acc": 0.24603174603174602,
                "acc_stderr": 0.03852273364924317,
                "brier_score": 0.7558897175145493,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_prehistory": {
                "acc": 0.2808641975308642,
                "acc_stderr": 0.025006469755799215,
                "brier_score": 0.7552343974599415,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.014422292204808842,
                "brier_score": 0.7551737995162493,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.24050632911392406,
                "acc_stderr": 0.027820781981149678,
                "brier_score": 0.76716563357048,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_moral_disputes": {
                "acc": 0.27167630057803466,
                "acc_stderr": 0.02394851290546835,
                "brier_score": 0.7518613678618872,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_professional_law": {
                "acc": 0.24119947848761408,
                "acc_stderr": 0.010926496102034952,
                "brier_score": 0.7695844539373945,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.3128834355828221,
                "acc_stderr": 0.036429145782924055,
                "brier_score": 0.7493157501646129,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.27941176470588236,
                "acc_stderr": 0.031493281045079556,
                "brier_score": 0.7631207153722228,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_philosophy": {
                "acc": 0.33440514469453375,
                "acc_stderr": 0.02679542232789395,
                "brier_score": 0.7436655510942353,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_jurisprudence": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.04284467968052192,
                "brier_score": 0.7533622182470998,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_international_law": {
                "acc": 0.35537190082644626,
                "acc_stderr": 0.0436923632657398,
                "brier_score": 0.7329535247802648,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21212121212121213,
                "acc_stderr": 0.031922715695482974,
                "brier_score": 0.7645629191281306,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.27979274611398963,
                "acc_stderr": 0.032396370467357036,
                "brier_score": 0.7541351413172132,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.026653531596715494,
                "brier_score": 0.7603567565580419,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_geography": {
                "acc": 0.25757575757575757,
                "acc_stderr": 0.031156269519646847,
                "brier_score": 0.7437409782398885,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24403669724770644,
                "acc_stderr": 0.018415286351416406,
                "brier_score": 0.7664984382654108,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "brier_score": 0.759405754385259,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.7606951982879532,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_sociology": {
                "acc": 0.2537313432835821,
                "acc_stderr": 0.030769444967296007,
                "brier_score": 0.7634450081849491,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24871794871794872,
                "acc_stderr": 0.021916957709213796,
                "brier_score": 0.7574546160387061,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_security_studies": {
                "acc": 0.22040816326530613,
                "acc_stderr": 0.026537045312145277,
                "brier_score": 0.752254816581363,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_professional_psychology": {
                "acc": 0.26633986928104575,
                "acc_stderr": 0.017883188134667206,
                "brier_score": 0.7565649200831607,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_human_sexuality": {
                "acc": 0.24427480916030533,
                "acc_stderr": 0.037683359597287434,
                "brier_score": 0.7652780053590433,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_econometrics": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.0414243971948936,
                "brier_score": 0.7652218900149195,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_miscellaneous": {
                "acc": 0.26309067688378035,
                "acc_stderr": 0.015745497169049046,
                "brier_score": 0.7654908629478401,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_marketing": {
                "acc": 0.28205128205128205,
                "acc_stderr": 0.02948036054954119,
                "brier_score": 0.7552332681529889,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_management": {
                "acc": 0.2524271844660194,
                "acc_stderr": 0.04301250399690878,
                "brier_score": 0.7600245221684531,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_nutrition": {
                "acc": 0.25163398692810457,
                "acc_stderr": 0.024848018263875195,
                "brier_score": 0.7640597340305756,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_medical_genetics": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.7762216678033689,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_human_aging": {
                "acc": 0.19282511210762332,
                "acc_stderr": 0.026478240960489365,
                "brier_score": 0.7800610696771287,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_professional_medicine": {
                "acc": 0.22426470588235295,
                "acc_stderr": 0.025336848563332362,
                "brier_score": 0.7635379068383641,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.03242414757483098,
                "brier_score": 0.7679840913432142,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_business_ethics": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.7781903236182292,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.19622641509433963,
                "acc_stderr": 0.02444238813110083,
                "brier_score": 0.7681818346745,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_global_facts": {
                "acc": 0.33,
                "acc_stderr": 0.047258156262526045,
                "brier_score": 0.7487369092626431,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_virology": {
                "acc": 0.2469879518072289,
                "acc_stderr": 0.03357351982064536,
                "brier_score": 0.7695191849068174,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2375886524822695,
                "acc_stderr": 0.025389512552729906,
                "brier_score": 0.7730920632947392,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_college_physics": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.041583075330832865,
                "brier_score": 0.7921966035717108,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2781456953642384,
                "acc_stderr": 0.03658603262763743,
                "brier_score": 0.7524004818521565,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_biology": {
                "acc": 0.23870967741935484,
                "acc_stderr": 0.024251071262208834,
                "brier_score": 0.7597133258999512,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_college_biology": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.03685651095897532,
                "brier_score": 0.7732549139379925,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_anatomy": {
                "acc": 0.34074074074074073,
                "acc_stderr": 0.040943762699967946,
                "brier_score": 0.7401866268979679,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_college_chemistry": {
                "acc": 0.18,
                "acc_stderr": 0.038612291966536955,
                "brier_score": 0.7873479975290243,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_computer_security": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7573995889302786,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_college_computer_science": {
                "acc": 0.27,
                "acc_stderr": 0.04461960433384741,
                "brier_score": 0.7698551965779535,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_astronomy": {
                "acc": 0.3223684210526316,
                "acc_stderr": 0.038035102483515854,
                "brier_score": 0.732576931227996,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_college_mathematics": {
                "acc": 0.26,
                "acc_stderr": 0.044084400227680794,
                "brier_score": 0.7955437721978141,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.20425531914893616,
                "acc_stderr": 0.026355158413349414,
                "brier_score": 0.7729307589058161,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7534459394903233,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7555446493228442,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_machine_learning": {
                "acc": 0.23214285714285715,
                "acc_stderr": 0.04007341809755805,
                "brier_score": 0.7669895465473596,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.30049261083743845,
                "acc_stderr": 0.03225799476233486,
                "brier_score": 0.7544466344426068,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.03114144782353604,
                "brier_score": 0.7459779406514874,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2619047619047619,
                "acc_stderr": 0.022644212615525218,
                "brier_score": 0.7902749238124076,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.31724137931034485,
                "acc_stderr": 0.038783523721386215,
                "brier_score": 0.7439372516923184,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.26296296296296295,
                "acc_stderr": 0.026842057873833706,
                "brier_score": 0.8017153999088171,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-13-55.298554"
            },
            "arc_challenge": {
                "acc": 0.3575085324232082,
                "acc_stderr": 0.014005494275916576,
                "acc_norm": 0.39590443686006827,
                "acc_norm_stderr": 0.014291228393536587,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "hellaswag": {
                "acc": 0.4816769567815176,
                "acc_stderr": 0.004986429808146771,
                "acc_norm": 0.6514638518223461,
                "acc_norm_stderr": 0.004755329243976686,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "truthfulqa_mc2": {
                "acc": 0.3515928441032545,
                "acc_stderr": 0.013622121924448312,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.822049251809254,
                "bleu_max_stderr": 0.7441915100142679,
                "bleu_acc": 0.2876376988984088,
                "bleu_acc_stderr": 0.015846315101394795,
                "bleu_diff": -8.72973777939928,
                "bleu_diff_stderr": 0.799560880574795,
                "rouge1_max": 50.64779573586556,
                "rouge1_max_stderr": 0.8314230941524794,
                "rouge1_acc": 0.2692778457772338,
                "rouge1_acc_stderr": 0.015528566637087264,
                "rouge1_diff": -11.027700507612522,
                "rouge1_diff_stderr": 0.8410289087382377,
                "rouge2_max": 33.75326270225714,
                "rouge2_max_stderr": 0.9630024578384584,
                "rouge2_acc": 0.22643818849449204,
                "rouge2_acc_stderr": 0.014651337324602585,
                "rouge2_diff": -13.190984697589347,
                "rouge2_diff_stderr": 1.019946221960148,
                "rougeL_max": 47.78257310566027,
                "rougeL_max_stderr": 0.8446585069903312,
                "rougeL_acc": 0.2594859241126071,
                "rougeL_acc_stderr": 0.015345409485558,
                "rougeL_diff": -11.191544484143854,
                "rougeL_diff_stderr": 0.859662499239064,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "truthfulqa_mc1": {
                "acc": 0.2178702570379437,
                "acc_stderr": 0.014450846714123899,
                "timestamp": "2024-11-12T22-04-18.010150"
            },
            "winogrande": {
                "acc": 0.6400947119179163,
                "acc_stderr": 0.013489609590266797,
                "timestamp": "2024-11-12T22-04-18.010150"
            }
        }
    }
}