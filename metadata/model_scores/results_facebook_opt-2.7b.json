{
    "model_name": "facebook/opt-2.7b",
    "last_updated": "2023-07-19",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "acc": 0.0010486577181208054,
                    "acc_stderr": 0.0003314581465219369,
                    "f1": 0.04767407718120815,
                    "f1_stderr": 0.0011986644527763738,
                    "timestamp": "2023-10-19T03-26-05.209079"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.002274450341167551,
                    "acc_stderr": 0.0013121578148673927,
                    "timestamp": "2023-10-19T03-26-05.209079"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.6195737963693765,
                    "acc_stderr": 0.01364472790865682,
                    "timestamp": "2023-10-19T03-26-05.209079"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.3097269624573379,
                    "acc_stderr": 0.01351205841523836,
                    "acc_norm": 0.3395904436860068,
                    "acc_norm_stderr": 0.01383903976282016,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.4600677155945031,
                    "acc_stderr": 0.004973842670559798,
                    "acc_norm": 0.6143198566022705,
                    "acc_norm_stderr": 0.004857607641160633,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036846,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036846,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.037498507091740234,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.037498507091740234,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.17105263157894737,
                    "acc_stderr": 0.030643607071677084,
                    "acc_norm": 0.17105263157894737,
                    "acc_norm_stderr": 0.030643607071677084,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.22264150943396227,
                    "acc_stderr": 0.025604233470899098,
                    "acc_norm": 0.22264150943396227,
                    "acc_norm_stderr": 0.025604233470899098,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.20833333333333334,
                    "acc_stderr": 0.03396116205845334,
                    "acc_norm": 0.20833333333333334,
                    "acc_norm_stderr": 0.03396116205845334,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.03942772444036625,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.03942772444036625,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816505,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816505,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.24855491329479767,
                    "acc_stderr": 0.03295304696818317,
                    "acc_norm": 0.24855491329479767,
                    "acc_norm_stderr": 0.03295304696818317,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.04092563958237656,
                    "acc_norm": 0.21568627450980393,
                    "acc_norm_stderr": 0.04092563958237656,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768078,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768078,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.225531914893617,
                    "acc_stderr": 0.027321078417387536,
                    "acc_norm": 0.225531914893617,
                    "acc_norm_stderr": 0.027321078417387536,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.24561403508771928,
                    "acc_stderr": 0.04049339297748141,
                    "acc_norm": 0.24561403508771928,
                    "acc_norm_stderr": 0.04049339297748141,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2827586206896552,
                    "acc_stderr": 0.03752833958003336,
                    "acc_norm": 0.2827586206896552,
                    "acc_norm_stderr": 0.03752833958003336,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.22486772486772486,
                    "acc_stderr": 0.02150209607822914,
                    "acc_norm": 0.22486772486772486,
                    "acc_norm_stderr": 0.02150209607822914,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.19047619047619047,
                    "acc_stderr": 0.03512207412302054,
                    "acc_norm": 0.19047619047619047,
                    "acc_norm_stderr": 0.03512207412302054,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.2161290322580645,
                    "acc_stderr": 0.023415293433568525,
                    "acc_norm": 0.2161290322580645,
                    "acc_norm_stderr": 0.023415293433568525,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2512315270935961,
                    "acc_stderr": 0.03051653073269444,
                    "acc_norm": 0.2512315270935961,
                    "acc_norm_stderr": 0.03051653073269444,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.23636363636363636,
                    "acc_stderr": 0.03317505930009181,
                    "acc_norm": 0.23636363636363636,
                    "acc_norm_stderr": 0.03317505930009181,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.21717171717171718,
                    "acc_stderr": 0.029376616484945633,
                    "acc_norm": 0.21717171717171718,
                    "acc_norm_stderr": 0.029376616484945633,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.33678756476683935,
                    "acc_stderr": 0.03410780251836182,
                    "acc_norm": 0.33678756476683935,
                    "acc_norm_stderr": 0.03410780251836182,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.3487179487179487,
                    "acc_stderr": 0.02416278028401772,
                    "acc_norm": 0.3487179487179487,
                    "acc_norm_stderr": 0.02416278028401772,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2814814814814815,
                    "acc_stderr": 0.027420019350945277,
                    "acc_norm": 0.2814814814814815,
                    "acc_norm_stderr": 0.027420019350945277,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.21428571428571427,
                    "acc_stderr": 0.02665353159671548,
                    "acc_norm": 0.21428571428571427,
                    "acc_norm_stderr": 0.02665353159671548,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31788079470198677,
                    "acc_stderr": 0.03802039760107903,
                    "acc_norm": 0.31788079470198677,
                    "acc_norm_stderr": 0.03802039760107903,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.326605504587156,
                    "acc_stderr": 0.020106990889937303,
                    "acc_norm": 0.326605504587156,
                    "acc_norm_stderr": 0.020106990889937303,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4074074074074074,
                    "acc_stderr": 0.033509916046960436,
                    "acc_norm": 0.4074074074074074,
                    "acc_norm_stderr": 0.033509916046960436,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.03039153369274154,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.03039153369274154,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.23628691983122363,
                    "acc_stderr": 0.02765215314415927,
                    "acc_norm": 0.23628691983122363,
                    "acc_norm_stderr": 0.02765215314415927,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.20179372197309417,
                    "acc_stderr": 0.02693611191280226,
                    "acc_norm": 0.20179372197309417,
                    "acc_norm_stderr": 0.02693611191280226,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.1984732824427481,
                    "acc_stderr": 0.03498149385462472,
                    "acc_norm": 0.1984732824427481,
                    "acc_norm_stderr": 0.03498149385462472,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.24793388429752067,
                    "acc_stderr": 0.03941897526516302,
                    "acc_norm": 0.24793388429752067,
                    "acc_norm_stderr": 0.03941897526516302,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.28703703703703703,
                    "acc_stderr": 0.043733130409147614,
                    "acc_norm": 0.28703703703703703,
                    "acc_norm_stderr": 0.043733130409147614,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.22699386503067484,
                    "acc_stderr": 0.03291099578615769,
                    "acc_norm": 0.22699386503067484,
                    "acc_norm_stderr": 0.03291099578615769,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.2857142857142857,
                    "acc_stderr": 0.04287858751340456,
                    "acc_norm": 0.2857142857142857,
                    "acc_norm_stderr": 0.04287858751340456,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.36893203883495146,
                    "acc_stderr": 0.047776151811567386,
                    "acc_norm": 0.36893203883495146,
                    "acc_norm_stderr": 0.047776151811567386,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.23931623931623933,
                    "acc_stderr": 0.027951826808924333,
                    "acc_norm": 0.23931623931623933,
                    "acc_norm_stderr": 0.027951826808924333,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.047937248544110196,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.047937248544110196,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.24648786717752236,
                    "acc_stderr": 0.01541130876968693,
                    "acc_norm": 0.24648786717752236,
                    "acc_norm_stderr": 0.01541130876968693,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.24277456647398843,
                    "acc_stderr": 0.023083658586984204,
                    "acc_norm": 0.24277456647398843,
                    "acc_norm_stderr": 0.023083658586984204,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.23575418994413408,
                    "acc_stderr": 0.014196375686290804,
                    "acc_norm": 0.23575418994413408,
                    "acc_norm_stderr": 0.014196375686290804,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.19281045751633988,
                    "acc_stderr": 0.02258931888817676,
                    "acc_norm": 0.19281045751633988,
                    "acc_norm_stderr": 0.02258931888817676,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.31511254019292606,
                    "acc_stderr": 0.026385273703464482,
                    "acc_norm": 0.31511254019292606,
                    "acc_norm_stderr": 0.026385273703464482,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.02378858355165854,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.02378858355165854,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2978723404255319,
                    "acc_stderr": 0.02728160834446942,
                    "acc_norm": 0.2978723404255319,
                    "acc_norm_stderr": 0.02728160834446942,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2522816166883963,
                    "acc_stderr": 0.011092789056875243,
                    "acc_norm": 0.2522816166883963,
                    "acc_norm_stderr": 0.011092789056875243,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.31985294117647056,
                    "acc_stderr": 0.028332959514031218,
                    "acc_norm": 0.31985294117647056,
                    "acc_norm_stderr": 0.028332959514031218,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2581699346405229,
                    "acc_stderr": 0.01770453165325007,
                    "acc_norm": 0.2581699346405229,
                    "acc_norm_stderr": 0.01770453165325007,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.21818181818181817,
                    "acc_stderr": 0.03955932861795833,
                    "acc_norm": 0.21818181818181817,
                    "acc_norm_stderr": 0.03955932861795833,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.20816326530612245,
                    "acc_stderr": 0.025991117672813296,
                    "acc_norm": 0.20816326530612245,
                    "acc_norm_stderr": 0.025991117672813296,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.2537313432835821,
                    "acc_stderr": 0.03076944496729602,
                    "acc_norm": 0.2537313432835821,
                    "acc_norm_stderr": 0.03076944496729602,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.23493975903614459,
                    "acc_stderr": 0.03300533186128922,
                    "acc_norm": 0.23493975903614459,
                    "acc_norm_stderr": 0.03300533186128922,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.17543859649122806,
                    "acc_stderr": 0.029170885500727654,
                    "acc_norm": 0.17543859649122806,
                    "acc_norm_stderr": 0.029170885500727654,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.22766217870257038,
                    "mc1_stderr": 0.01467925503211107,
                    "mc2": 0.37425417990290183,
                    "mc2_stderr": 0.013823032731766478,
                    "timestamp": "2023-07-19T16-25-28.050181"
                }
            }
        }
    }
}