{
    "model_name": "jisukim8873/falcon-7B-case-c",
    "last_updated": "2024-03-07",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.454778156996587,
                    "acc_stderr": 0.014551507060836353,
                    "acc_norm": 0.4854948805460751,
                    "acc_norm_stderr": 0.014605241081370056,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5984863572993427,
                    "acc_stderr": 0.004892026457294713,
                    "acc_norm": 0.7866958773152758,
                    "acc_norm_stderr": 0.004088034745195346,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2814814814814815,
                    "acc_stderr": 0.03885004245800254,
                    "acc_norm": 0.2814814814814815,
                    "acc_norm_stderr": 0.03885004245800254,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.24342105263157895,
                    "acc_stderr": 0.034923496688842384,
                    "acc_norm": 0.24342105263157895,
                    "acc_norm_stderr": 0.034923496688842384,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036845,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036845,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2981132075471698,
                    "acc_stderr": 0.028152837942493854,
                    "acc_norm": 0.2981132075471698,
                    "acc_norm_stderr": 0.028152837942493854,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2708333333333333,
                    "acc_stderr": 0.03716177437566017,
                    "acc_norm": 0.2708333333333333,
                    "acc_norm_stderr": 0.03716177437566017,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.03942772444036624,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.03942772444036624,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.3179190751445087,
                    "acc_stderr": 0.03550683989165582,
                    "acc_norm": 0.3179190751445087,
                    "acc_norm_stderr": 0.03550683989165582,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.04023382273617749,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.04023382273617749,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252605,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252605,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.32340425531914896,
                    "acc_stderr": 0.030579442773610334,
                    "acc_norm": 0.32340425531914896,
                    "acc_norm_stderr": 0.030579442773610334,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2719298245614035,
                    "acc_stderr": 0.04185774424022056,
                    "acc_norm": 0.2719298245614035,
                    "acc_norm_stderr": 0.04185774424022056,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2413793103448276,
                    "acc_stderr": 0.03565998174135302,
                    "acc_norm": 0.2413793103448276,
                    "acc_norm_stderr": 0.03565998174135302,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2698412698412698,
                    "acc_stderr": 0.02286083830923207,
                    "acc_norm": 0.2698412698412698,
                    "acc_norm_stderr": 0.02286083830923207,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.18253968253968253,
                    "acc_stderr": 0.034550710191021496,
                    "acc_norm": 0.18253968253968253,
                    "acc_norm_stderr": 0.034550710191021496,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.31290322580645163,
                    "acc_stderr": 0.02637756702864586,
                    "acc_norm": 0.31290322580645163,
                    "acc_norm_stderr": 0.02637756702864586,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.29064039408866993,
                    "acc_stderr": 0.03194740072265541,
                    "acc_norm": 0.29064039408866993,
                    "acc_norm_stderr": 0.03194740072265541,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.04824181513244218,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.04824181513244218,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.32727272727272727,
                    "acc_stderr": 0.03663974994391242,
                    "acc_norm": 0.32727272727272727,
                    "acc_norm_stderr": 0.03663974994391242,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.29292929292929293,
                    "acc_stderr": 0.032424979581788166,
                    "acc_norm": 0.29292929292929293,
                    "acc_norm_stderr": 0.032424979581788166,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.31088082901554404,
                    "acc_stderr": 0.03340361906276585,
                    "acc_norm": 0.31088082901554404,
                    "acc_norm_stderr": 0.03340361906276585,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.022421273612923714,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.022421273612923714,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.026719240783712166,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.026719240783712166,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2815126050420168,
                    "acc_stderr": 0.029213549414372177,
                    "acc_norm": 0.2815126050420168,
                    "acc_norm_stderr": 0.029213549414372177,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.304635761589404,
                    "acc_stderr": 0.03757949922943342,
                    "acc_norm": 0.304635761589404,
                    "acc_norm_stderr": 0.03757949922943342,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.26788990825688075,
                    "acc_stderr": 0.018987462257978652,
                    "acc_norm": 0.26788990825688075,
                    "acc_norm_stderr": 0.018987462257978652,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.20833333333333334,
                    "acc_stderr": 0.02769691071309394,
                    "acc_norm": 0.20833333333333334,
                    "acc_norm_stderr": 0.02769691071309394,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.27450980392156865,
                    "acc_stderr": 0.0313217980308329,
                    "acc_norm": 0.27450980392156865,
                    "acc_norm_stderr": 0.0313217980308329,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.3459915611814346,
                    "acc_stderr": 0.03096481058878671,
                    "acc_norm": 0.3459915611814346,
                    "acc_norm_stderr": 0.03096481058878671,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.29596412556053814,
                    "acc_stderr": 0.030636591348699803,
                    "acc_norm": 0.29596412556053814,
                    "acc_norm_stderr": 0.030636591348699803,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.1984732824427481,
                    "acc_stderr": 0.03498149385462472,
                    "acc_norm": 0.1984732824427481,
                    "acc_norm_stderr": 0.03498149385462472,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.32231404958677684,
                    "acc_stderr": 0.042664163633521664,
                    "acc_norm": 0.32231404958677684,
                    "acc_norm_stderr": 0.042664163633521664,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.32407407407407407,
                    "acc_stderr": 0.04524596007030049,
                    "acc_norm": 0.32407407407407407,
                    "acc_norm_stderr": 0.04524596007030049,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.31901840490797545,
                    "acc_stderr": 0.03661997551073836,
                    "acc_norm": 0.31901840490797545,
                    "acc_norm_stderr": 0.03661997551073836,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.29464285714285715,
                    "acc_stderr": 0.0432704093257873,
                    "acc_norm": 0.29464285714285715,
                    "acc_norm_stderr": 0.0432704093257873,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2524271844660194,
                    "acc_stderr": 0.04301250399690878,
                    "acc_norm": 0.2524271844660194,
                    "acc_norm_stderr": 0.04301250399690878,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.3034188034188034,
                    "acc_stderr": 0.03011821010694262,
                    "acc_norm": 0.3034188034188034,
                    "acc_norm_stderr": 0.03011821010694262,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206824,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206824,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.38058748403575987,
                    "acc_stderr": 0.017362564126075425,
                    "acc_norm": 0.38058748403575987,
                    "acc_norm_stderr": 0.017362564126075425,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.33815028901734107,
                    "acc_stderr": 0.02546977014940018,
                    "acc_norm": 0.33815028901734107,
                    "acc_norm_stderr": 0.02546977014940018,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2569832402234637,
                    "acc_stderr": 0.01461446582196632,
                    "acc_norm": 0.2569832402234637,
                    "acc_norm_stderr": 0.01461446582196632,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.28431372549019607,
                    "acc_stderr": 0.025829163272757468,
                    "acc_norm": 0.28431372549019607,
                    "acc_norm_stderr": 0.025829163272757468,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.3536977491961415,
                    "acc_stderr": 0.027155208103200875,
                    "acc_norm": 0.3536977491961415,
                    "acc_norm_stderr": 0.027155208103200875,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.30864197530864196,
                    "acc_stderr": 0.025702640260603746,
                    "acc_norm": 0.30864197530864196,
                    "acc_norm_stderr": 0.025702640260603746,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2553191489361702,
                    "acc_stderr": 0.026011992930902,
                    "acc_norm": 0.2553191489361702,
                    "acc_norm_stderr": 0.026011992930902,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2737940026075619,
                    "acc_stderr": 0.0113886121679794,
                    "acc_norm": 0.2737940026075619,
                    "acc_norm_stderr": 0.0113886121679794,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.1948529411764706,
                    "acc_stderr": 0.024060599423487414,
                    "acc_norm": 0.1948529411764706,
                    "acc_norm_stderr": 0.024060599423487414,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.28594771241830064,
                    "acc_stderr": 0.01828048507295468,
                    "acc_norm": 0.28594771241830064,
                    "acc_norm_stderr": 0.01828048507295468,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.23636363636363636,
                    "acc_stderr": 0.040693063197213775,
                    "acc_norm": 0.23636363636363636,
                    "acc_norm_stderr": 0.040693063197213775,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.22448979591836735,
                    "acc_stderr": 0.026711430555538408,
                    "acc_norm": 0.22448979591836735,
                    "acc_norm_stderr": 0.026711430555538408,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.29850746268656714,
                    "acc_stderr": 0.03235743789355043,
                    "acc_norm": 0.29850746268656714,
                    "acc_norm_stderr": 0.03235743789355043,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.41,
                    "acc_stderr": 0.04943110704237102,
                    "acc_norm": 0.41,
                    "acc_norm_stderr": 0.04943110704237102,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.35542168674698793,
                    "acc_stderr": 0.03726214354322415,
                    "acc_norm": 0.35542168674698793,
                    "acc_norm_stderr": 0.03726214354322415,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.3742690058479532,
                    "acc_stderr": 0.03711601185389481,
                    "acc_norm": 0.3742690058479532,
                    "acc_norm_stderr": 0.03711601185389481,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2668298653610771,
                    "mc1_stderr": 0.01548369193923727,
                    "mc2": 0.3826492372963991,
                    "mc2_stderr": 0.014401921891655947,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7008681925808997,
                    "acc_stderr": 0.012868639066091536,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.07429871114480667,
                    "acc_stderr": 0.007223844172845588,
                    "timestamp": "2024-03-07T11-30-30.635454"
                }
            }
        }
    }
}