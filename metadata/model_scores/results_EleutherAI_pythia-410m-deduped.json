{
    "model_name": "EleutherAI/pythia-410m-deduped",
    "last_updated": "2023-07-19",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "em": 0.0014681208053691276,
                    "em_stderr": 0.0003921042190298293,
                    "f1": 0.042572357382550524,
                    "f1_stderr": 0.0011637772390608397,
                    "timestamp": "2023-10-18T09-27-36.064128"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.003032600454890068,
                    "acc_stderr": 0.0015145735612245436,
                    "timestamp": "2023-10-18T09-27-36.064128"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5438042620363063,
                    "acc_stderr": 0.013998453610924331,
                    "timestamp": "2023-10-18T09-27-36.064128"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.22610921501706485,
                    "acc_stderr": 0.012224202097063295,
                    "acc_norm": 0.24829351535836178,
                    "acc_norm_stderr": 0.012624912868089762,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.344353714399522,
                    "acc_stderr": 0.004741859753178411,
                    "acc_norm": 0.41286596295558653,
                    "acc_norm_stderr": 0.00491342901055906,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.03861229196653697,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.03861229196653697,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2740740740740741,
                    "acc_stderr": 0.03853254836552003,
                    "acc_norm": 0.2740740740740741,
                    "acc_norm_stderr": 0.03853254836552003,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.1513157894736842,
                    "acc_stderr": 0.029162631596843975,
                    "acc_norm": 0.1513157894736842,
                    "acc_norm_stderr": 0.029162631596843975,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2490566037735849,
                    "acc_stderr": 0.026616482980501715,
                    "acc_norm": 0.2490566037735849,
                    "acc_norm_stderr": 0.026616482980501715,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.3263888888888889,
                    "acc_stderr": 0.03921067198982266,
                    "acc_norm": 0.3263888888888889,
                    "acc_norm_stderr": 0.03921067198982266,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932268,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932268,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2023121387283237,
                    "acc_stderr": 0.030631145539198823,
                    "acc_norm": 0.2023121387283237,
                    "acc_norm_stderr": 0.030631145539198823,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.04220773659171452,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.04220773659171452,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2297872340425532,
                    "acc_stderr": 0.027501752944412424,
                    "acc_norm": 0.2297872340425532,
                    "acc_norm_stderr": 0.027501752944412424,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.21052631578947367,
                    "acc_stderr": 0.038351539543994194,
                    "acc_norm": 0.21052631578947367,
                    "acc_norm_stderr": 0.038351539543994194,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2482758620689655,
                    "acc_stderr": 0.036001056927277716,
                    "acc_norm": 0.2482758620689655,
                    "acc_norm_stderr": 0.036001056927277716,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2619047619047619,
                    "acc_stderr": 0.022644212615525218,
                    "acc_norm": 0.2619047619047619,
                    "acc_norm_stderr": 0.022644212615525218,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.30952380952380953,
                    "acc_stderr": 0.04134913018303316,
                    "acc_norm": 0.30952380952380953,
                    "acc_norm_stderr": 0.04134913018303316,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909281,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909281,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.25161290322580643,
                    "acc_stderr": 0.024685979286239952,
                    "acc_norm": 0.25161290322580643,
                    "acc_norm_stderr": 0.024685979286239952,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2512315270935961,
                    "acc_stderr": 0.030516530732694436,
                    "acc_norm": 0.2512315270935961,
                    "acc_norm_stderr": 0.030516530732694436,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909284,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909284,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.22424242424242424,
                    "acc_stderr": 0.032568666616811015,
                    "acc_norm": 0.22424242424242424,
                    "acc_norm_stderr": 0.032568666616811015,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.23232323232323232,
                    "acc_stderr": 0.030088629490217483,
                    "acc_norm": 0.23232323232323232,
                    "acc_norm_stderr": 0.030088629490217483,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.22797927461139897,
                    "acc_stderr": 0.030276909945178256,
                    "acc_norm": 0.22797927461139897,
                    "acc_norm_stderr": 0.030276909945178256,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.3128205128205128,
                    "acc_stderr": 0.02350757902064535,
                    "acc_norm": 0.3128205128205128,
                    "acc_norm_stderr": 0.02350757902064535,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.026962424325073828,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.026962424325073828,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.20168067226890757,
                    "acc_stderr": 0.02606431340630452,
                    "acc_norm": 0.20168067226890757,
                    "acc_norm_stderr": 0.02606431340630452,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.25165562913907286,
                    "acc_stderr": 0.035433042343899844,
                    "acc_norm": 0.25165562913907286,
                    "acc_norm_stderr": 0.035433042343899844,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.21651376146788992,
                    "acc_stderr": 0.01765871059444315,
                    "acc_norm": 0.21651376146788992,
                    "acc_norm_stderr": 0.01765871059444315,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4722222222222222,
                    "acc_stderr": 0.0340470532865388,
                    "acc_norm": 0.4722222222222222,
                    "acc_norm_stderr": 0.0340470532865388,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.029771775228145628,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.029771775228145628,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.27848101265822783,
                    "acc_stderr": 0.029178682304842565,
                    "acc_norm": 0.27848101265822783,
                    "acc_norm_stderr": 0.029178682304842565,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.336322869955157,
                    "acc_stderr": 0.031708824268455,
                    "acc_norm": 0.336322869955157,
                    "acc_norm_stderr": 0.031708824268455,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.24427480916030533,
                    "acc_stderr": 0.03768335959728743,
                    "acc_norm": 0.24427480916030533,
                    "acc_norm_stderr": 0.03768335959728743,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.32231404958677684,
                    "acc_stderr": 0.04266416363352167,
                    "acc_norm": 0.32231404958677684,
                    "acc_norm_stderr": 0.04266416363352167,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2962962962962963,
                    "acc_stderr": 0.044143436668549335,
                    "acc_norm": 0.2962962962962963,
                    "acc_norm_stderr": 0.044143436668549335,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.22699386503067484,
                    "acc_stderr": 0.032910995786157686,
                    "acc_norm": 0.22699386503067484,
                    "acc_norm_stderr": 0.032910995786157686,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.3392857142857143,
                    "acc_stderr": 0.04493949068613539,
                    "acc_norm": 0.3392857142857143,
                    "acc_norm_stderr": 0.04493949068613539,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2815533980582524,
                    "acc_stderr": 0.044532548363264673,
                    "acc_norm": 0.2815533980582524,
                    "acc_norm_stderr": 0.044532548363264673,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.2264957264957265,
                    "acc_stderr": 0.027421007295392916,
                    "acc_norm": 0.2264957264957265,
                    "acc_norm_stderr": 0.027421007295392916,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.26181353767560667,
                    "acc_stderr": 0.01572083867844526,
                    "acc_norm": 0.26181353767560667,
                    "acc_norm_stderr": 0.01572083867844526,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2514450867052023,
                    "acc_stderr": 0.02335736578587404,
                    "acc_norm": 0.2514450867052023,
                    "acc_norm_stderr": 0.02335736578587404,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.24134078212290502,
                    "acc_stderr": 0.014310999547961459,
                    "acc_norm": 0.24134078212290502,
                    "acc_norm_stderr": 0.014310999547961459,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.22875816993464052,
                    "acc_stderr": 0.024051029739912255,
                    "acc_norm": 0.22875816993464052,
                    "acc_norm_stderr": 0.024051029739912255,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.1864951768488746,
                    "acc_stderr": 0.022122439772480774,
                    "acc_norm": 0.1864951768488746,
                    "acc_norm_stderr": 0.022122439772480774,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2037037037037037,
                    "acc_stderr": 0.022409674547304168,
                    "acc_norm": 0.2037037037037037,
                    "acc_norm_stderr": 0.022409674547304168,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.24468085106382978,
                    "acc_stderr": 0.02564555362226673,
                    "acc_norm": 0.24468085106382978,
                    "acc_norm_stderr": 0.02564555362226673,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.23989569752281617,
                    "acc_stderr": 0.010906282617981653,
                    "acc_norm": 0.23989569752281617,
                    "acc_norm_stderr": 0.010906282617981653,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.3235294117647059,
                    "acc_stderr": 0.02841820861940679,
                    "acc_norm": 0.3235294117647059,
                    "acc_norm_stderr": 0.02841820861940679,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.24509803921568626,
                    "acc_stderr": 0.01740181671142766,
                    "acc_norm": 0.24509803921568626,
                    "acc_norm_stderr": 0.01740181671142766,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2909090909090909,
                    "acc_stderr": 0.04350271442923243,
                    "acc_norm": 0.2909090909090909,
                    "acc_norm_stderr": 0.04350271442923243,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.3510204081632653,
                    "acc_stderr": 0.03055531675557364,
                    "acc_norm": 0.3510204081632653,
                    "acc_norm_stderr": 0.03055531675557364,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.27860696517412936,
                    "acc_stderr": 0.031700561834973086,
                    "acc_norm": 0.27860696517412936,
                    "acc_norm_stderr": 0.031700561834973086,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.26506024096385544,
                    "acc_stderr": 0.03436024037944968,
                    "acc_norm": 0.26506024096385544,
                    "acc_norm_stderr": 0.03436024037944968,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.28654970760233917,
                    "acc_stderr": 0.03467826685703826,
                    "acc_norm": 0.28654970760233917,
                    "acc_norm_stderr": 0.03467826685703826,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.23745410036719705,
                    "mc1_stderr": 0.01489627744104184,
                    "mc2": 0.40947578592206774,
                    "mc2_stderr": 0.014566854286767683,
                    "timestamp": "2023-07-19T14-23-02.980263"
                }
            }
        }
    }
}