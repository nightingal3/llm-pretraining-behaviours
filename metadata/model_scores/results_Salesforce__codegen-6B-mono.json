{
    "model_name": "Salesforce/codegen-6B-mono",
    "last_updated": "2024-12-19 13:40:26.028990",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.016483516483516484,
                "exact_match_stderr": 0.005454029764766752,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.011481056257175661,
                "exact_match_stderr": 0.0036118008378658315,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.005203704987512651,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.006644518272425249,
                "exact_match_stderr": 0.0027050844483854065,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_geometry": {
                "exact_match": 0.006263048016701462,
                "exact_match_stderr": 0.0036083997328878823,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.012658227848101266,
                "exact_match_stderr": 0.005140313889578834,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_algebra": {
                "exact_match": 0.010109519797809604,
                "exact_match_stderr": 0.0029048017186508964,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_3da": {
                "acc": 0.0035,
                "acc_stderr": 0.0013208888574315787,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_3ds": {
                "acc": 0.01,
                "acc_stderr": 0.0022254159696827483,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_4da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339444,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_2ds": {
                "acc": 0.0865,
                "acc_stderr": 0.006287180554084609,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_1dc": {
                "acc": 0.112,
                "acc_stderr": 0.007053571892184721,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_2dm": {
                "acc": 0.061,
                "acc_stderr": 0.005352926948264491,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_2da": {
                "acc": 0.064,
                "acc_stderr": 0.005474210764278855,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "gsm8k_cot": {
                "exact_match": 0.028051554207733132,
                "exact_match_stderr": 0.004548229533836338,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "gsm8k": {
                "exact_match": 0.026535253980288095,
                "exact_match_stderr": 0.004427045987265169,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "anli_r2": {
                "brier_score": 0.8876594910752693,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "anli_r3": {
                "brier_score": 0.9495729853127675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "anli_r1": {
                "brier_score": 0.9382038423017862,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_eu": {
                "brier_score": 1.2520379152089613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_vi": {
                "brier_score": 1.1171458537014292,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_ru": {
                "brier_score": 0.8728299042741055,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_zh": {
                "brier_score": 0.921242092190122,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_tr": {
                "brier_score": 0.9486527913660737,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_fr": {
                "brier_score": 0.9557543141847247,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_en": {
                "brier_score": 0.7389286092568025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_ur": {
                "brier_score": 1.3111515844555548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_ar": {
                "brier_score": 1.1548071078890445,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_de": {
                "brier_score": 0.9638964115536973,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_hi": {
                "brier_score": 0.7796514751173046,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_es": {
                "brier_score": 1.0355571799263816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_bg": {
                "brier_score": 0.9432814063972743,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_sw": {
                "brier_score": 1.1131222005854853,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_el": {
                "brier_score": 0.8760007535828703,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_th": {
                "brier_score": 0.9361718072793239,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "logiqa2": {
                "brier_score": 1.132872371789095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "mathqa": {
                "brier_score": 0.9827803171269597,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "lambada_standard": {
                "perplexity": 87.13890878986679,
                "perplexity_stderr": 3.482329775710741,
                "acc": 0.2342324859305259,
                "acc_stderr": 0.005900436024282866,
                "timestamp": "2024-06-14T01-05-27.879801"
            },
            "lambada_openai": {
                "perplexity": 67.5368425039748,
                "perplexity_stderr": 2.8888291352437236,
                "acc": 0.27983698816223557,
                "acc_stderr": 0.006254319132119315,
                "timestamp": "2024-06-14T01-05-27.879801"
            },
            "mmlu_world_religions": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.03565079670708312,
                "brier_score": 0.7780453162294215,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_formal_logic": {
                "acc": 0.3412698412698413,
                "acc_stderr": 0.04240799327574924,
                "brier_score": 0.7584902825096467,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_prehistory": {
                "acc": 0.21604938271604937,
                "acc_stderr": 0.022899162918445806,
                "brier_score": 0.8522473735930954,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574885,
                "brier_score": 0.8018951873309403,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.270042194092827,
                "acc_stderr": 0.028900721906293426,
                "brier_score": 0.8322130716486043,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2514450867052023,
                "acc_stderr": 0.023357365785874037,
                "brier_score": 0.8062744853068,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_professional_law": {
                "acc": 0.24511082138200782,
                "acc_stderr": 0.010986307870045509,
                "brier_score": 0.8031523145939087,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2147239263803681,
                "acc_stderr": 0.03226219377286774,
                "brier_score": 0.8443732327706741,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25,
                "acc_stderr": 0.03039153369274154,
                "brier_score": 0.8459099121028086,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_philosophy": {
                "acc": 0.19292604501607716,
                "acc_stderr": 0.022411516780911363,
                "brier_score": 0.8363464662368304,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_jurisprudence": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.04284467968052192,
                "brier_score": 0.7999006000971114,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_international_law": {
                "acc": 0.24793388429752067,
                "acc_stderr": 0.03941897526516303,
                "brier_score": 0.8350664431932452,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.20606060606060606,
                "acc_stderr": 0.0315841532404771,
                "brier_score": 0.8866890339705624,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.19689119170984457,
                "acc_stderr": 0.02869787397186069,
                "brier_score": 0.8399415509436173,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.20168067226890757,
                "acc_stderr": 0.026064313406304527,
                "brier_score": 0.8356456450973151,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_geography": {
                "acc": 0.17676767676767677,
                "acc_stderr": 0.027178752639044915,
                "brier_score": 0.8601059320404222,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.1944954128440367,
                "acc_stderr": 0.01697028909045803,
                "brier_score": 0.8793552688694627,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "brier_score": 0.8392366871864418,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.26,
                "acc_stderr": 0.044084400227680794,
                "brier_score": 0.8079258933629248,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_sociology": {
                "acc": 0.24875621890547264,
                "acc_stderr": 0.030567675938916718,
                "brier_score": 0.8280539163246622,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.20512820512820512,
                "acc_stderr": 0.020473233173551965,
                "brier_score": 0.8476793974498708,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_security_studies": {
                "acc": 0.19183673469387755,
                "acc_stderr": 0.025206963154225395,
                "brier_score": 0.8042266307100163,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_professional_psychology": {
                "acc": 0.24836601307189543,
                "acc_stderr": 0.017479487001364764,
                "brier_score": 0.8331333456314579,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "brier_score": 0.8131038913189726,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.03999423879281338,
                "brier_score": 0.81680580181963,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_miscellaneous": {
                "acc": 0.23754789272030652,
                "acc_stderr": 0.015218733046150193,
                "brier_score": 0.8326763568305254,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_marketing": {
                "acc": 0.2863247863247863,
                "acc_stderr": 0.02961432369045666,
                "brier_score": 0.7995292177441591,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "brier_score": 0.8966846263949297,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_nutrition": {
                "acc": 0.22875816993464052,
                "acc_stderr": 0.024051029739912255,
                "brier_score": 0.8368563375087384,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_medical_genetics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7830689974921099,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_human_aging": {
                "acc": 0.3094170403587444,
                "acc_stderr": 0.031024411740572213,
                "brier_score": 0.7777714690289778,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_professional_medicine": {
                "acc": 0.1875,
                "acc_stderr": 0.023709788253811766,
                "brier_score": 0.8164722350932145,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_college_medicine": {
                "acc": 0.2023121387283237,
                "acc_stderr": 0.03063114553919882,
                "brier_score": 0.831745806610858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_business_ethics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7825093139684296,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.20754716981132076,
                "acc_stderr": 0.02495991802891127,
                "brier_score": 0.8186797089881754,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_global_facts": {
                "acc": 0.19,
                "acc_stderr": 0.039427724440366234,
                "brier_score": 0.797798978773303,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_virology": {
                "acc": 0.26506024096385544,
                "acc_stderr": 0.03436024037944967,
                "brier_score": 0.7869259397242031,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_professional_accounting": {
                "acc": 0.24113475177304963,
                "acc_stderr": 0.025518731049537766,
                "brier_score": 0.8298314768200078,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.040925639582376556,
                "brier_score": 0.8203894402472959,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_physics": {
                "acc": 0.23178807947019867,
                "acc_stderr": 0.03445406271987053,
                "brier_score": 0.8125147376482349,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_biology": {
                "acc": 0.1967741935483871,
                "acc_stderr": 0.02261640942074203,
                "brier_score": 0.8417325335916167,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_college_biology": {
                "acc": 0.2569444444444444,
                "acc_stderr": 0.03653946969442099,
                "brier_score": 0.8147040991164677,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_anatomy": {
                "acc": 0.17777777777777778,
                "acc_stderr": 0.03302789859901718,
                "brier_score": 0.8490402739177968,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_college_chemistry": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.8065386643467295,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 0.8036688727566377,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_college_computer_science": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909283,
                "brier_score": 0.8542327416264095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_astronomy": {
                "acc": 0.19736842105263158,
                "acc_stderr": 0.03238981601699397,
                "brier_score": 0.8461938805111608,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_college_mathematics": {
                "acc": 0.22,
                "acc_stderr": 0.041633319989322695,
                "brier_score": 0.8212076844205903,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.029241883869628837,
                "brier_score": 0.7958415984856299,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.27,
                "acc_stderr": 0.04461960433384741,
                "brier_score": 0.8162146327227109,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909282,
                "brier_score": 0.8478307013393968,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_machine_learning": {
                "acc": 0.32142857142857145,
                "acc_stderr": 0.04432804055291519,
                "brier_score": 0.7652253306062621,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.1724137931034483,
                "acc_stderr": 0.02657767218303657,
                "brier_score": 0.8502229478591244,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.14814814814814814,
                "acc_stderr": 0.024227629273728353,
                "brier_score": 0.8688155406417832,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.02141168439369418,
                "brier_score": 0.8255646328708173,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.03565998174135302,
                "brier_score": 0.8134734193210041,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24814814814814815,
                "acc_stderr": 0.026335739404055803,
                "brier_score": 0.8201107377136791,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-12-28.954906"
            },
            "arc_challenge": {
                "acc": 0.21416382252559726,
                "acc_stderr": 0.011988383205966513,
                "acc_norm": 0.24061433447098976,
                "acc_norm_stderr": 0.012491468532390582,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "hellaswag": {
                "acc": 0.311292571200956,
                "acc_stderr": 0.004620758579628644,
                "acc_norm": 0.358195578570006,
                "acc_norm_stderr": 0.004784901248558693,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "truthfulqa_mc2": {
                "acc": 0.4328537953113665,
                "acc_stderr": 0.015447836565101179,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "truthfulqa_gen": {
                "bleu_max": 17.25064603206814,
                "bleu_max_stderr": 0.6130969229491668,
                "bleu_acc": 0.34149326805385555,
                "bleu_acc_stderr": 0.016600688619950836,
                "bleu_diff": -1.7382338560986075,
                "bleu_diff_stderr": 0.5783719458598973,
                "rouge1_max": 39.52252821322913,
                "rouge1_max_stderr": 0.8483702442864192,
                "rouge1_acc": 0.3108935128518972,
                "rouge1_acc_stderr": 0.016203316673559696,
                "rouge1_diff": -3.784504356456098,
                "rouge1_diff_stderr": 0.8036994596352391,
                "rouge2_max": 22.32239340912172,
                "rouge2_max_stderr": 0.8946073218671986,
                "rouge2_acc": 0.204406364749082,
                "rouge2_acc_stderr": 0.014117174337432621,
                "rouge2_diff": -3.2879573679240153,
                "rouge2_diff_stderr": 0.8115132417345173,
                "rougeL_max": 36.318101595189205,
                "rougeL_max_stderr": 0.8312620370594777,
                "rougeL_acc": 0.29865361077111385,
                "rougeL_acc_stderr": 0.01602157061376854,
                "rougeL_diff": -3.7476560334607485,
                "rougeL_diff_stderr": 0.7943779862877182,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "truthfulqa_mc1": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.015415241740237009,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "winogrande": {
                "acc": 0.5390686661404893,
                "acc_stderr": 0.014009521680980314,
                "timestamp": "2024-11-21T16-50-41.823534"
            }
        }
    }
}