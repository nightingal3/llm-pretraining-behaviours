{
    "model_name": "Salesforce/codegen-6B-mono",
    "last_updated": "2024-12-04 11:24:32.569700",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.016483516483516484,
                "exact_match_stderr": 0.005454029764766752,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.011481056257175661,
                "exact_match_stderr": 0.0036118008378658315,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.005203704987512651,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.006644518272425249,
                "exact_match_stderr": 0.0027050844483854065,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_geometry": {
                "exact_match": 0.006263048016701462,
                "exact_match_stderr": 0.0036083997328878823,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.012658227848101266,
                "exact_match_stderr": 0.005140313889578834,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "minerva_math_algebra": {
                "exact_match": 0.010109519797809604,
                "exact_match_stderr": 0.0029048017186508964,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_3da": {
                "acc": 0.0035,
                "acc_stderr": 0.0013208888574315787,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_3ds": {
                "acc": 0.01,
                "acc_stderr": 0.0022254159696827483,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_4da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339444,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_2ds": {
                "acc": 0.0865,
                "acc_stderr": 0.006287180554084609,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_1dc": {
                "acc": 0.112,
                "acc_stderr": 0.007053571892184721,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_2dm": {
                "acc": 0.061,
                "acc_stderr": 0.005352926948264491,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "arithmetic_2da": {
                "acc": 0.064,
                "acc_stderr": 0.005474210764278855,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "gsm8k_cot": {
                "exact_match": 0.028051554207733132,
                "exact_match_stderr": 0.004548229533836338,
                "timestamp": "2024-06-14T00-51-28.732086"
            },
            "gsm8k": {
                "exact_match": 0.026535253980288095,
                "exact_match_stderr": 0.004427045987265169,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "anli_r2": {
                "brier_score": 0.8876594910752693,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "anli_r3": {
                "brier_score": 0.9495729853127675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "anli_r1": {
                "brier_score": 0.9382038423017862,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_eu": {
                "brier_score": 1.2520379152089613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_vi": {
                "brier_score": 1.1171458537014292,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_ru": {
                "brier_score": 0.8728299042741055,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_zh": {
                "brier_score": 0.921242092190122,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_tr": {
                "brier_score": 0.9486527913660737,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_fr": {
                "brier_score": 0.9557543141847247,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_en": {
                "brier_score": 0.7389286092568025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_ur": {
                "brier_score": 1.3111515844555548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_ar": {
                "brier_score": 1.1548071078890445,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_de": {
                "brier_score": 0.9638964115536973,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_hi": {
                "brier_score": 0.7796514751173046,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_es": {
                "brier_score": 1.0355571799263816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_bg": {
                "brier_score": 0.9432814063972743,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_sw": {
                "brier_score": 1.1131222005854853,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_el": {
                "brier_score": 0.8760007535828703,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "xnli_th": {
                "brier_score": 0.9361718072793239,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "logiqa2": {
                "brier_score": 1.132872371789095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "mathqa": {
                "brier_score": 0.9827803171269597,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T01-04-02.219231"
            },
            "lambada_standard": {
                "perplexity": 87.13890878986679,
                "perplexity_stderr": 3.482329775710741,
                "acc": 0.2342324859305259,
                "acc_stderr": 0.005900436024282866,
                "timestamp": "2024-06-14T01-05-27.879801"
            },
            "lambada_openai": {
                "perplexity": 67.5368425039748,
                "perplexity_stderr": 2.8888291352437236,
                "acc": 0.27983698816223557,
                "acc_stderr": 0.006254319132119315,
                "timestamp": "2024-06-14T01-05-27.879801"
            },
            "mmlu_world_religions": {
                "acc": 0.29239766081871343,
                "acc_stderr": 0.034886477134579215,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_formal_logic": {
                "acc": 0.29365079365079366,
                "acc_stderr": 0.04073524322147127,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_prehistory": {
                "acc": 0.2808641975308642,
                "acc_stderr": 0.025006469755799208,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.014422292204808836,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25738396624472576,
                "acc_stderr": 0.028458820991460285,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_moral_disputes": {
                "acc": 0.25722543352601157,
                "acc_stderr": 0.023532925431044283,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_professional_law": {
                "acc": 0.24185136897001303,
                "acc_stderr": 0.010936550813827054,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.27607361963190186,
                "acc_stderr": 0.0351238528370505,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.24019607843137256,
                "acc_stderr": 0.02998373305591361,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_philosophy": {
                "acc": 0.2282958199356913,
                "acc_stderr": 0.02383930331139822,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_jurisprudence": {
                "acc": 0.28703703703703703,
                "acc_stderr": 0.043733130409147614,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_international_law": {
                "acc": 0.256198347107438,
                "acc_stderr": 0.03984979653302872,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.23636363636363636,
                "acc_stderr": 0.033175059300091805,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.23834196891191708,
                "acc_stderr": 0.03074890536390989,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.3277310924369748,
                "acc_stderr": 0.03048991141767323,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_geography": {
                "acc": 0.26262626262626265,
                "acc_stderr": 0.031353050095330834,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23302752293577983,
                "acc_stderr": 0.018125669180861496,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_public_relations": {
                "acc": 0.19090909090909092,
                "acc_stderr": 0.03764425585984926,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252604,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_sociology": {
                "acc": 0.263681592039801,
                "acc_stderr": 0.03115715086935557,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.28717948717948716,
                "acc_stderr": 0.022939925418530616,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_security_studies": {
                "acc": 0.2857142857142857,
                "acc_stderr": 0.028920583220675578,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2679738562091503,
                "acc_stderr": 0.017917974069594726,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_human_sexuality": {
                "acc": 0.25190839694656486,
                "acc_stderr": 0.03807387116306087,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.04049339297748142,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2681992337164751,
                "acc_stderr": 0.015842430835269428,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_marketing": {
                "acc": 0.25213675213675213,
                "acc_stderr": 0.02844796547623101,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_management": {
                "acc": 0.1553398058252427,
                "acc_stderr": 0.035865947385739734,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_nutrition": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.02495418432487991,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_medical_genetics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_human_aging": {
                "acc": 0.25112107623318386,
                "acc_stderr": 0.02910522083322461,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_professional_medicine": {
                "acc": 0.3786764705882353,
                "acc_stderr": 0.029465133639776125,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_college_medicine": {
                "acc": 0.2023121387283237,
                "acc_stderr": 0.030631145539198823,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_business_ethics": {
                "acc": 0.35,
                "acc_stderr": 0.047937248544110196,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2339622641509434,
                "acc_stderr": 0.02605529690115292,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_global_facts": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036844,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_virology": {
                "acc": 0.2710843373493976,
                "acc_stderr": 0.034605799075530276,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_professional_accounting": {
                "acc": 0.25886524822695034,
                "acc_stderr": 0.026129572527180848,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_college_physics": {
                "acc": 0.16666666666666666,
                "acc_stderr": 0.03708284662416545,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2781456953642384,
                "acc_stderr": 0.03658603262763743,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_biology": {
                "acc": 0.20967741935483872,
                "acc_stderr": 0.02315787934908353,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_anatomy": {
                "acc": 0.1925925925925926,
                "acc_stderr": 0.034065420585026526,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_college_chemistry": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_college_computer_science": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621504,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_astronomy": {
                "acc": 0.19736842105263158,
                "acc_stderr": 0.03238981601699397,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_college_mathematics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.30638297872340425,
                "acc_stderr": 0.030135906478517563,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768077,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_machine_learning": {
                "acc": 0.29464285714285715,
                "acc_stderr": 0.0432704093257873,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.27586206896551724,
                "acc_stderr": 0.031447125816782405,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.375,
                "acc_stderr": 0.033016908987210894,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2328042328042328,
                "acc_stderr": 0.021765961672154537,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.20689655172413793,
                "acc_stderr": 0.03375672449560553,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.027309140588230172,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "arc_challenge": {
                "acc": 0.21416382252559726,
                "acc_stderr": 0.011988383205966513,
                "acc_norm": 0.24061433447098976,
                "acc_norm_stderr": 0.012491468532390582,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "hellaswag": {
                "acc": 0.311292571200956,
                "acc_stderr": 0.004620758579628644,
                "acc_norm": 0.358195578570006,
                "acc_norm_stderr": 0.004784901248558693,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "truthfulqa_mc2": {
                "acc": 0.4328537953113665,
                "acc_stderr": 0.015447836565101179,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "truthfulqa_gen": {
                "bleu_max": 17.25064603206814,
                "bleu_max_stderr": 0.6130969229491668,
                "bleu_acc": 0.34149326805385555,
                "bleu_acc_stderr": 0.016600688619950836,
                "bleu_diff": -1.7382338560986075,
                "bleu_diff_stderr": 0.5783719458598973,
                "rouge1_max": 39.52252821322913,
                "rouge1_max_stderr": 0.8483702442864192,
                "rouge1_acc": 0.3108935128518972,
                "rouge1_acc_stderr": 0.016203316673559696,
                "rouge1_diff": -3.784504356456098,
                "rouge1_diff_stderr": 0.8036994596352391,
                "rouge2_max": 22.32239340912172,
                "rouge2_max_stderr": 0.8946073218671986,
                "rouge2_acc": 0.204406364749082,
                "rouge2_acc_stderr": 0.014117174337432621,
                "rouge2_diff": -3.2879573679240153,
                "rouge2_diff_stderr": 0.8115132417345173,
                "rougeL_max": 36.318101595189205,
                "rougeL_max_stderr": 0.8312620370594777,
                "rougeL_acc": 0.29865361077111385,
                "rougeL_acc_stderr": 0.01602157061376854,
                "rougeL_diff": -3.7476560334607485,
                "rougeL_diff_stderr": 0.7943779862877182,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "truthfulqa_mc1": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.015415241740237009,
                "timestamp": "2024-11-21T16-50-41.823534"
            },
            "winogrande": {
                "acc": 0.5390686661404893,
                "acc_stderr": 0.014009521680980314,
                "timestamp": "2024-11-21T16-50-41.823534"
            }
        }
    }
}