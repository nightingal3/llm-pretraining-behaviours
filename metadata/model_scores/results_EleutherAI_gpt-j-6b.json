{
    "model_name": "EleutherAI/gpt-j-6b",
    "last_updated": "2023-07-18",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.36860068259385664,
                    "acc_stderr": 0.014097810678042184,
                    "acc_norm": 0.4138225255972696,
                    "acc_norm_stderr": 0.014392730009221007,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.4945230033857797,
                    "acc_stderr": 0.00498948204061011,
                    "acc_norm": 0.675363473411671,
                    "acc_norm_stderr": 0.004672819355838537,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.03820169914517904,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.03820169914517904,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.26973684210526316,
                    "acc_stderr": 0.03611780560284898,
                    "acc_norm": 0.26973684210526316,
                    "acc_norm_stderr": 0.03611780560284898,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542126,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542126,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.26037735849056604,
                    "acc_stderr": 0.027008766090708104,
                    "acc_norm": 0.26037735849056604,
                    "acc_norm_stderr": 0.027008766090708104,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.24305555555555555,
                    "acc_stderr": 0.03586879280080339,
                    "acc_norm": 0.24305555555555555,
                    "acc_norm_stderr": 0.03586879280080339,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.17,
                    "acc_stderr": 0.0377525168068637,
                    "acc_norm": 0.17,
                    "acc_norm_stderr": 0.0377525168068637,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816508,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816508,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621505,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621505,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2832369942196532,
                    "acc_stderr": 0.03435568056047875,
                    "acc_norm": 0.2832369942196532,
                    "acc_norm_stderr": 0.03435568056047875,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.04092563958237655,
                    "acc_norm": 0.21568627450980393,
                    "acc_norm_stderr": 0.04092563958237655,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.41,
                    "acc_stderr": 0.04943110704237101,
                    "acc_norm": 0.41,
                    "acc_norm_stderr": 0.04943110704237101,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3404255319148936,
                    "acc_stderr": 0.03097669299853443,
                    "acc_norm": 0.3404255319148936,
                    "acc_norm_stderr": 0.03097669299853443,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2807017543859649,
                    "acc_stderr": 0.042270544512322,
                    "acc_norm": 0.2807017543859649,
                    "acc_norm_stderr": 0.042270544512322,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2896551724137931,
                    "acc_stderr": 0.037800192304380135,
                    "acc_norm": 0.2896551724137931,
                    "acc_norm_stderr": 0.037800192304380135,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.23015873015873015,
                    "acc_stderr": 0.021679219663693145,
                    "acc_norm": 0.23015873015873015,
                    "acc_norm_stderr": 0.021679219663693145,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.18253968253968253,
                    "acc_stderr": 0.03455071019102149,
                    "acc_norm": 0.18253968253968253,
                    "acc_norm_stderr": 0.03455071019102149,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.2032258064516129,
                    "acc_stderr": 0.022891687984554966,
                    "acc_norm": 0.2032258064516129,
                    "acc_norm_stderr": 0.022891687984554966,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.24630541871921183,
                    "acc_stderr": 0.030315099285617732,
                    "acc_norm": 0.24630541871921183,
                    "acc_norm_stderr": 0.030315099285617732,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.15,
                    "acc_stderr": 0.03588702812826369,
                    "acc_norm": 0.15,
                    "acc_norm_stderr": 0.03588702812826369,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.28484848484848485,
                    "acc_stderr": 0.035243908445117836,
                    "acc_norm": 0.28484848484848485,
                    "acc_norm_stderr": 0.035243908445117836,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.23232323232323232,
                    "acc_stderr": 0.030088629490217483,
                    "acc_norm": 0.23232323232323232,
                    "acc_norm_stderr": 0.030088629490217483,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.22279792746113988,
                    "acc_stderr": 0.03003114797764154,
                    "acc_norm": 0.22279792746113988,
                    "acc_norm_stderr": 0.03003114797764154,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2230769230769231,
                    "acc_stderr": 0.021107730127243984,
                    "acc_norm": 0.2230769230769231,
                    "acc_norm_stderr": 0.021107730127243984,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2740740740740741,
                    "acc_stderr": 0.027195934804085622,
                    "acc_norm": 0.2740740740740741,
                    "acc_norm_stderr": 0.027195934804085622,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.25630252100840334,
                    "acc_stderr": 0.02835962087053395,
                    "acc_norm": 0.25630252100840334,
                    "acc_norm_stderr": 0.02835962087053395,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.25165562913907286,
                    "acc_stderr": 0.035433042343899844,
                    "acc_norm": 0.25165562913907286,
                    "acc_norm_stderr": 0.035433042343899844,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.22568807339449543,
                    "acc_stderr": 0.017923087667803057,
                    "acc_norm": 0.22568807339449543,
                    "acc_norm_stderr": 0.017923087667803057,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.16203703703703703,
                    "acc_stderr": 0.025130453652268455,
                    "acc_norm": 0.16203703703703703,
                    "acc_norm_stderr": 0.025130453652268455,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.28921568627450983,
                    "acc_stderr": 0.03182231867647555,
                    "acc_norm": 0.28921568627450983,
                    "acc_norm_stderr": 0.03182231867647555,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.2869198312236287,
                    "acc_stderr": 0.02944377302259469,
                    "acc_norm": 0.2869198312236287,
                    "acc_norm_stderr": 0.02944377302259469,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.336322869955157,
                    "acc_stderr": 0.031708824268455005,
                    "acc_norm": 0.336322869955157,
                    "acc_norm_stderr": 0.031708824268455005,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.21374045801526717,
                    "acc_stderr": 0.0359546161177469,
                    "acc_norm": 0.21374045801526717,
                    "acc_norm_stderr": 0.0359546161177469,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.24793388429752067,
                    "acc_stderr": 0.03941897526516302,
                    "acc_norm": 0.24793388429752067,
                    "acc_norm_stderr": 0.03941897526516302,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.28703703703703703,
                    "acc_stderr": 0.043733130409147614,
                    "acc_norm": 0.28703703703703703,
                    "acc_norm_stderr": 0.043733130409147614,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.25153374233128833,
                    "acc_stderr": 0.03408997886857529,
                    "acc_norm": 0.25153374233128833,
                    "acc_norm_stderr": 0.03408997886857529,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.38392857142857145,
                    "acc_stderr": 0.04616143075028547,
                    "acc_norm": 0.38392857142857145,
                    "acc_norm_stderr": 0.04616143075028547,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.21359223300970873,
                    "acc_stderr": 0.040580420156460344,
                    "acc_norm": 0.21359223300970873,
                    "acc_norm_stderr": 0.040580420156460344,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.26495726495726496,
                    "acc_stderr": 0.028911208802749482,
                    "acc_norm": 0.26495726495726496,
                    "acc_norm_stderr": 0.028911208802749482,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.31417624521072796,
                    "acc_stderr": 0.016599291735884904,
                    "acc_norm": 0.31417624521072796,
                    "acc_norm_stderr": 0.016599291735884904,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.27167630057803466,
                    "acc_stderr": 0.023948512905468358,
                    "acc_norm": 0.27167630057803466,
                    "acc_norm_stderr": 0.023948512905468358,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2435754189944134,
                    "acc_stderr": 0.014355911964767864,
                    "acc_norm": 0.2435754189944134,
                    "acc_norm_stderr": 0.014355911964767864,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.025261691219729494,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.025261691219729494,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.2604501607717042,
                    "acc_stderr": 0.024926723224845543,
                    "acc_norm": 0.2604501607717042,
                    "acc_norm_stderr": 0.024926723224845543,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.3117283950617284,
                    "acc_stderr": 0.02577311116963045,
                    "acc_norm": 0.3117283950617284,
                    "acc_norm_stderr": 0.02577311116963045,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.28368794326241137,
                    "acc_stderr": 0.02689170942834396,
                    "acc_norm": 0.28368794326241137,
                    "acc_norm_stderr": 0.02689170942834396,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2894393741851369,
                    "acc_stderr": 0.011582659702210252,
                    "acc_norm": 0.2894393741851369,
                    "acc_norm_stderr": 0.011582659702210252,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.2426470588235294,
                    "acc_stderr": 0.02604066247420127,
                    "acc_norm": 0.2426470588235294,
                    "acc_norm_stderr": 0.02604066247420127,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.27941176470588236,
                    "acc_stderr": 0.018152871051538816,
                    "acc_norm": 0.27941176470588236,
                    "acc_norm_stderr": 0.018152871051538816,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.34545454545454546,
                    "acc_stderr": 0.04554619617541054,
                    "acc_norm": 0.34545454545454546,
                    "acc_norm_stderr": 0.04554619617541054,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.35918367346938773,
                    "acc_stderr": 0.030713560455108493,
                    "acc_norm": 0.35918367346938773,
                    "acc_norm_stderr": 0.030713560455108493,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.27860696517412936,
                    "acc_stderr": 0.031700561834973086,
                    "acc_norm": 0.27860696517412936,
                    "acc_norm_stderr": 0.031700561834973086,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.04560480215720684,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.04560480215720684,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3373493975903614,
                    "acc_stderr": 0.03680783690727581,
                    "acc_norm": 0.3373493975903614,
                    "acc_norm_stderr": 0.03680783690727581,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.34502923976608185,
                    "acc_stderr": 0.03645981377388807,
                    "acc_norm": 0.34502923976608185,
                    "acc_norm_stderr": 0.03645981377388807,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.20195838433292534,
                    "mc1_stderr": 0.014053957441512348,
                    "mc2": 0.35962472949507807,
                    "mc2_stderr": 0.013462019520008167,
                    "timestamp": "2023-08-29T19-41-28.653242"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.029567854435178165,
                    "acc_stderr": 0.004665893134220772,
                    "timestamp": "2023-12-03T16-52-26.173919"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.0009437919463087249,
                    "acc_stderr": 0.00031446531194132096,
                    "f1": 0.0461545721476511,
                    "f1_stderr": 0.0011697500055632092,
                    "timestamp": "2023-09-08T17-46-12.907701"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.65982636148382,
                    "acc_stderr": 0.0133152187624174,
                    "timestamp": "2023-09-08T17-46-12.907701"
                }
            }
        }
    }
}