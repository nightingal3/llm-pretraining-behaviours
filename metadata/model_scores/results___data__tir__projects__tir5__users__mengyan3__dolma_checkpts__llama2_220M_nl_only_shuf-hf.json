{
    "model_name": "__data__tir__projects__tir5__users__mengyan3__dolma_checkpts__llama2_220M_nl_only_shuf-hf",
    "last_updated": "2024-12-04 11:24:29.195992",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.23391812865497075,
                "acc_stderr": 0.03246721765117826,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_formal_logic": {
                "acc": 0.19047619047619047,
                "acc_stderr": 0.035122074123020514,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_prehistory": {
                "acc": 0.21604938271604937,
                "acc_stderr": 0.02289916291844581,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2424581005586592,
                "acc_stderr": 0.014333522059217892,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25738396624472576,
                "acc_stderr": 0.028458820991460285,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2514450867052023,
                "acc_stderr": 0.023357365785874037,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_professional_law": {
                "acc": 0.24445893089960888,
                "acc_stderr": 0.010976425013113904,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2822085889570552,
                "acc_stderr": 0.03536117886664743,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.029771775228145638,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_philosophy": {
                "acc": 0.19614147909967847,
                "acc_stderr": 0.02255244778047802,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_jurisprudence": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.04133119440243839,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_international_law": {
                "acc": 0.2644628099173554,
                "acc_stderr": 0.04026187527591205,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.03477691162163659,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.18652849740932642,
                "acc_stderr": 0.028112091210117474,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.22268907563025211,
                "acc_stderr": 0.02702543349888238,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_geography": {
                "acc": 0.2828282828282828,
                "acc_stderr": 0.03208779558786751,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.1834862385321101,
                "acc_stderr": 0.01659525971039933,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_public_relations": {
                "acc": 0.22727272727272727,
                "acc_stderr": 0.040139645540727756,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695235,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_sociology": {
                "acc": 0.22885572139303484,
                "acc_stderr": 0.02970528405677244,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2358974358974359,
                "acc_stderr": 0.021525965407408726,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_security_studies": {
                "acc": 0.24081632653061225,
                "acc_stderr": 0.027372942201788163,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25,
                "acc_stderr": 0.01751781884501444,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2824427480916031,
                "acc_stderr": 0.03948406125768362,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_econometrics": {
                "acc": 0.22807017543859648,
                "acc_stderr": 0.03947152782669415,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_miscellaneous": {
                "acc": 0.24648786717752236,
                "acc_stderr": 0.015411308769686922,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_marketing": {
                "acc": 0.19230769230769232,
                "acc_stderr": 0.025819233256483717,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_nutrition": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.023152722439402303,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_human_aging": {
                "acc": 0.32286995515695066,
                "acc_stderr": 0.03138147637575499,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4411764705882353,
                "acc_stderr": 0.030161911930767105,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_college_medicine": {
                "acc": 0.2023121387283237,
                "acc_stderr": 0.030631145539198823,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_business_ethics": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2188679245283019,
                "acc_stderr": 0.02544786382510861,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_global_facts": {
                "acc": 0.17,
                "acc_stderr": 0.0377525168068637,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_virology": {
                "acc": 0.2710843373493976,
                "acc_stderr": 0.03460579907553027,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2198581560283688,
                "acc_stderr": 0.024706141070705477,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_college_physics": {
                "acc": 0.27450980392156865,
                "acc_stderr": 0.044405219061793254,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2185430463576159,
                "acc_stderr": 0.03374235550425694,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2838709677419355,
                "acc_stderr": 0.02564938106302926,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_college_biology": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.03476590104304134,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_anatomy": {
                "acc": 0.31851851851851853,
                "acc_stderr": 0.040247784019771096,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_college_chemistry": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036845,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_computer_security": {
                "acc": 0.35,
                "acc_stderr": 0.04793724854411018,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_college_computer_science": {
                "acc": 0.15,
                "acc_stderr": 0.03588702812826371,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123387,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_college_mathematics": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816508,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2680851063829787,
                "acc_stderr": 0.028957342788342347,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.21,
                "acc_stderr": 0.04093601807403326,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_machine_learning": {
                "acc": 0.29464285714285715,
                "acc_stderr": 0.04327040932578729,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.270935960591133,
                "acc_stderr": 0.031270907132976984,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.3148148148148148,
                "acc_stderr": 0.03167468706828979,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.24867724867724866,
                "acc_stderr": 0.022261817692400175,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.20689655172413793,
                "acc_stderr": 0.03375672449560554,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.26296296296296295,
                "acc_stderr": 0.026842057873833706,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "arc_challenge": {
                "acc": 0.1885665529010239,
                "acc_stderr": 0.011430897647675808,
                "acc_norm": 0.2363481228668942,
                "acc_norm_stderr": 0.012414960524301839,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "hellaswag": {
                "acc": 0.3080063732324238,
                "acc_stderr": 0.0046072567529318875,
                "acc_norm": 0.34993029277036447,
                "acc_norm_stderr": 0.004759729267943187,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "truthfulqa_mc2": {
                "acc": 0.42516012703712225,
                "acc_stderr": 0.014980156437707953,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "truthfulqa_gen": {
                "bleu_max": 13.245193057292528,
                "bleu_max_stderr": 0.5392567154704238,
                "bleu_acc": 0.31211750305997554,
                "bleu_acc_stderr": 0.016220756769520932,
                "bleu_diff": -2.1038506421564245,
                "bleu_diff_stderr": 0.4272897488054713,
                "rouge1_max": 32.83986416573394,
                "rouge1_max_stderr": 0.7972024559888017,
                "rouge1_acc": 0.2876376988984088,
                "rouge1_acc_stderr": 0.015846315101394805,
                "rouge1_diff": -5.300496494078776,
                "rouge1_diff_stderr": 0.5971929544496777,
                "rouge2_max": 16.524233679867464,
                "rouge2_max_stderr": 0.7783960296759083,
                "rouge2_acc": 0.18849449204406366,
                "rouge2_acc_stderr": 0.013691467148835361,
                "rouge2_diff": -4.2703421807378,
                "rouge2_diff_stderr": 0.6210440741887263,
                "rougeL_max": 30.097535619702846,
                "rougeL_max_stderr": 0.7730685592985541,
                "rougeL_acc": 0.2913096695226438,
                "rougeL_acc_stderr": 0.015905987048184828,
                "rougeL_diff": -5.181069664548509,
                "rougeL_diff_stderr": 0.5868656105739015,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "truthfulqa_mc1": {
                "acc": 0.23990208078335373,
                "acc_stderr": 0.014948812679062137,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "winogrande": {
                "acc": 0.5256511444356748,
                "acc_stderr": 0.014033980956108557,
                "timestamp": "2024-11-21T22-52-13.985176"
            },
            "gsm8k": {
                "exact_match": 0.012130401819560273,
                "exact_match_stderr": 0.00301529424289095,
                "timestamp": "2024-11-21T22-52-13.985176"
            }
        }
    }
}