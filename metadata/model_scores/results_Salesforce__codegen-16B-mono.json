{
    "model_name": "Salesforce/codegen-16B-mono",
    "last_updated": "2024-12-04 11:23:07.771650",
    "results": {
        "harness": {
            "anli_r2": {
                "brier_score": 0.8253824795424403,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "anli_r3": {
                "brier_score": 0.7940354639454789,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "anli_r1": {
                "brier_score": 0.8429583560477731,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_eu": {
                "brier_score": 1.0481738865590222,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_vi": {
                "brier_score": 1.0622085188319774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_ru": {
                "brier_score": 0.8720664022610117,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_zh": {
                "brier_score": 0.9990022713619396,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_tr": {
                "brier_score": 0.9070160533682529,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_fr": {
                "brier_score": 0.938508643895241,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_en": {
                "brier_score": 0.7700595581688592,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_ur": {
                "brier_score": 1.3209693093229593,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_ar": {
                "brier_score": 1.082490658523419,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_de": {
                "brier_score": 1.043589609239351,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_hi": {
                "brier_score": 0.9979601232052799,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_es": {
                "brier_score": 0.9189766913317707,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_bg": {
                "brier_score": 0.9981098092050139,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_sw": {
                "brier_score": 0.9858592239662022,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_el": {
                "brier_score": 0.8942569399409489,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "xnli_th": {
                "brier_score": 0.8555323255757882,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "logiqa2": {
                "brier_score": 1.0900452349971228,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "mathqa": {
                "brier_score": 0.9631942925356739,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-40-07.971762"
            },
            "lambada_standard": {
                "perplexity": 40.81985306298772,
                "perplexity_stderr": 1.485975941096083,
                "acc": 0.3052590723850184,
                "acc_stderr": 0.006415903230922692,
                "timestamp": "2024-06-14T04-42-09.817508"
            },
            "lambada_openai": {
                "perplexity": 27.000998548239398,
                "perplexity_stderr": 0.9699976694126962,
                "acc": 0.3592082282165729,
                "acc_stderr": 0.006684111319975825,
                "timestamp": "2024-06-14T04-42-09.817508"
            },
            "mmlu_world_religions": {
                "acc": 0.27485380116959063,
                "acc_stderr": 0.03424042924691583,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_formal_logic": {
                "acc": 0.20634920634920634,
                "acc_stderr": 0.0361960452412425,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_prehistory": {
                "acc": 0.2839506172839506,
                "acc_stderr": 0.025089478523765137,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23575418994413408,
                "acc_stderr": 0.014196375686290804,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.25316455696202533,
                "acc_stderr": 0.02830465794303529,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26011560693641617,
                "acc_stderr": 0.023618678310069363,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_professional_law": {
                "acc": 0.25358539765319427,
                "acc_stderr": 0.011111715336101132,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.22085889570552147,
                "acc_stderr": 0.032591773927421776,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25,
                "acc_stderr": 0.03039153369274154,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_philosophy": {
                "acc": 0.27009646302250806,
                "acc_stderr": 0.025218040373410605,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_jurisprudence": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.04284467968052191,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_international_law": {
                "acc": 0.2644628099173554,
                "acc_stderr": 0.04026187527591206,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.22424242424242424,
                "acc_stderr": 0.03256866661681102,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.20725388601036268,
                "acc_stderr": 0.02925282329180364,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.02755361446786379,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_geography": {
                "acc": 0.1919191919191919,
                "acc_stderr": 0.028057791672989017,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23302752293577983,
                "acc_stderr": 0.0181256691808615,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_public_relations": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.04265792110940589,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909281,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_sociology": {
                "acc": 0.29850746268656714,
                "acc_stderr": 0.03235743789355041,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.22564102564102564,
                "acc_stderr": 0.021193632525148522,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_security_studies": {
                "acc": 0.22040816326530613,
                "acc_stderr": 0.026537045312145284,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.017740899509177795,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2748091603053435,
                "acc_stderr": 0.03915345408847836,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.040493392977481425,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2656449553001277,
                "acc_stderr": 0.01579430248788872,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_marketing": {
                "acc": 0.2863247863247863,
                "acc_stderr": 0.029614323690456648,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_management": {
                "acc": 0.22330097087378642,
                "acc_stderr": 0.04123553189891431,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_nutrition": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.02495418432487991,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_medical_genetics": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909281,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_human_aging": {
                "acc": 0.3542600896860987,
                "acc_stderr": 0.032100621541349864,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_professional_medicine": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.024562204314142317,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_college_medicine": {
                "acc": 0.2254335260115607,
                "acc_stderr": 0.03186209851641144,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_business_ethics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2188679245283019,
                "acc_stderr": 0.025447863825108614,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_global_facts": {
                "acc": 0.31,
                "acc_stderr": 0.046482319871173156,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370519,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_professional_accounting": {
                "acc": 0.26595744680851063,
                "acc_stderr": 0.026358065698880585,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_college_physics": {
                "acc": 0.18627450980392157,
                "acc_stderr": 0.038739587141493524,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2847682119205298,
                "acc_stderr": 0.03684881521389023,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_biology": {
                "acc": 0.19032258064516128,
                "acc_stderr": 0.02233170761182307,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_anatomy": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.035914440841969694,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_college_chemistry": {
                "acc": 0.2,
                "acc_stderr": 0.040201512610368466,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_computer_security": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_college_computer_science": {
                "acc": 0.2,
                "acc_stderr": 0.040201512610368445,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_astronomy": {
                "acc": 0.20394736842105263,
                "acc_stderr": 0.03279000406310051,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_college_mathematics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2978723404255319,
                "acc_stderr": 0.029896145682095455,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909283,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_machine_learning": {
                "acc": 0.2857142857142857,
                "acc_stderr": 0.04287858751340456,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2019704433497537,
                "acc_stderr": 0.02824735012218027,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2037037037037037,
                "acc_stderr": 0.027467401804057982,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.23544973544973544,
                "acc_stderr": 0.021851509822031722,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2896551724137931,
                "acc_stderr": 0.03780019230438015,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2074074074074074,
                "acc_stderr": 0.024720713193952155,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "arc_challenge": {
                "acc": 0.2696245733788396,
                "acc_stderr": 0.012968040686869166,
                "acc_norm": 0.295221843003413,
                "acc_norm_stderr": 0.013329750293382315,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "hellaswag": {
                "acc": 0.3579964150567616,
                "acc_stderr": 0.004784312972495382,
                "acc_norm": 0.4473212507468632,
                "acc_norm_stderr": 0.004962010338226348,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "truthfulqa_mc2": {
                "acc": 0.4061093580338541,
                "acc_stderr": 0.015136437773534737,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "truthfulqa_gen": {
                "bleu_max": 19.57158267860521,
                "bleu_max_stderr": 0.672039191352111,
                "bleu_acc": 0.3023255813953488,
                "bleu_acc_stderr": 0.016077509266133036,
                "bleu_diff": -5.301786631798468,
                "bleu_diff_stderr": 0.658779186406353,
                "rouge1_max": 43.18409379824872,
                "rouge1_max_stderr": 0.8482447875377391,
                "rouge1_acc": 0.2802937576499388,
                "rouge1_acc_stderr": 0.01572313952460875,
                "rouge1_diff": -7.480158262440118,
                "rouge1_diff_stderr": 0.8228031808206397,
                "rouge2_max": 26.21877195377042,
                "rouge2_max_stderr": 0.9378604257610167,
                "rouge2_acc": 0.20563035495716034,
                "rouge2_acc_stderr": 0.014148482219460978,
                "rouge2_diff": -8.570941233813052,
                "rouge2_diff_stderr": 0.9197989857896314,
                "rougeL_max": 40.08679096043078,
                "rougeL_max_stderr": 0.8489879245723853,
                "rougeL_acc": 0.2864137086903305,
                "rougeL_acc_stderr": 0.01582614243950234,
                "rougeL_diff": -7.518785257278961,
                "rougeL_diff_stderr": 0.8301172921913628,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "truthfulqa_mc1": {
                "acc": 0.24357405140758873,
                "acc_stderr": 0.01502635482491078,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "winogrande": {
                "acc": 0.5674822415153907,
                "acc_stderr": 0.013923911578623842,
                "timestamp": "2024-11-25T21-15-05.806497"
            },
            "gsm8k": {
                "exact_match": 0.026535253980288095,
                "exact_match_stderr": 0.004427045987265163,
                "timestamp": "2024-11-25T21-15-05.806497"
            }
        }
    }
}