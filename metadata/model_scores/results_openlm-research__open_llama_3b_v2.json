{
    "model_name": "openlm-research/open_llama_3b_v2",
    "last_updated": "2024-12-19 13:41:53.442327",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.009157509157509158,
                "exact_match_stderr": 0.004080306065048996,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.04362801377726751,
                "exact_match_stderr": 0.006925266930505693,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.012962962962962963,
                "exact_match_stderr": 0.004872192984581458,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.015503875968992248,
                "exact_match_stderr": 0.0041136172383604485,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_geometry": {
                "exact_match": 0.014613778705636743,
                "exact_match_stderr": 0.0054887134436863135,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.014767932489451477,
                "exact_match_stderr": 0.00554623858966847,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_algebra": {
                "exact_match": 0.02106149957877001,
                "exact_match_stderr": 0.004169461854206043,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_3da": {
                "acc": 0.284,
                "acc_stderr": 0.010085775202269382,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_3ds": {
                "acc": 0.2605,
                "acc_stderr": 0.009816723436540222,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_4da": {
                "acc": 0.0825,
                "acc_stderr": 0.0061535199604740055,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_2ds": {
                "acc": 0.4195,
                "acc_stderr": 0.011037245371590673,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_5ds": {
                "acc": 0.015,
                "acc_stderr": 0.0027186753387999575,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_5da": {
                "acc": 0.015,
                "acc_stderr": 0.002718675338799954,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_1dc": {
                "acc": 0.0955,
                "acc_stderr": 0.006573544001554193,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_4ds": {
                "acc": 0.101,
                "acc_stderr": 0.006739600218525675,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_2dm": {
                "acc": 0.1655,
                "acc_stderr": 0.00831200455424887,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_2da": {
                "acc": 0.4295,
                "acc_stderr": 0.01107141197325543,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "gsm8k_cot": {
                "exact_match": 0.04852160727824109,
                "exact_match_stderr": 0.005918468618921081,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "gsm8k": {
                "exact_match": 0.039423805913570885,
                "exact_match_stderr": 0.005360280030342438,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "anli_r2": {
                "brier_score": 0.707706480342601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "anli_r3": {
                "brier_score": 0.679183075689064,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "anli_r1": {
                "brier_score": 0.7235055238380341,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_eu": {
                "brier_score": 1.132908017291774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_vi": {
                "brier_score": 0.971548338532371,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_ru": {
                "brier_score": 0.7523375161853497,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_zh": {
                "brier_score": 0.9867010701190115,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_tr": {
                "brier_score": 0.8808176289678832,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_fr": {
                "brier_score": 0.7460738931677325,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_en": {
                "brier_score": 0.6474238073900174,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_ur": {
                "brier_score": 1.3250861980436117,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_ar": {
                "brier_score": 1.234167496082821,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_de": {
                "brier_score": 0.812354579101372,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_hi": {
                "brier_score": 1.1542572623687906,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_es": {
                "brier_score": 0.8910867470387117,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_bg": {
                "brier_score": 0.859452606954678,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_sw": {
                "brier_score": 1.0713476113363065,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_el": {
                "brier_score": 1.1450771586115809,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_th": {
                "brier_score": 0.9689833936635035,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "logiqa2": {
                "brier_score": 1.0179439999333861,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "mathqa": {
                "brier_score": 0.9356941842152771,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "lambada_standard": {
                "perplexity": 5.818821278870909,
                "perplexity_stderr": 0.13816721772720136,
                "acc": 0.5951872695517174,
                "acc_stderr": 0.006838580607651546,
                "timestamp": "2024-06-13T11-35-25.155228"
            },
            "lambada_openai": {
                "perplexity": 4.561675691644536,
                "perplexity_stderr": 0.10330641094995072,
                "acc": 0.6735882010479333,
                "acc_stderr": 0.006532692754359034,
                "timestamp": "2024-06-13T11-35-25.155228"
            },
            "mmlu_world_religions": {
                "acc": 0.2573099415204678,
                "acc_stderr": 0.03352799844161865,
                "brier_score": 0.7480009231732265,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.03670066451047182,
                "brier_score": 0.7615843108140404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_prehistory": {
                "acc": 0.2623456790123457,
                "acc_stderr": 0.02447722285613511,
                "brier_score": 0.7461830940453977,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2569832402234637,
                "acc_stderr": 0.01461446582196633,
                "brier_score": 0.7634191585528008,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.27848101265822783,
                "acc_stderr": 0.02917868230484255,
                "brier_score": 0.7547953289576513,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26878612716763006,
                "acc_stderr": 0.023868003262500125,
                "brier_score": 0.7489207949995742,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_professional_law": {
                "acc": 0.25097783572359844,
                "acc_stderr": 0.011073730299187215,
                "brier_score": 0.759512116632909,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.24539877300613497,
                "acc_stderr": 0.03380939813943354,
                "brier_score": 0.7544872650836869,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25,
                "acc_stderr": 0.03039153369274154,
                "brier_score": 0.7535941268466293,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_philosophy": {
                "acc": 0.27009646302250806,
                "acc_stderr": 0.025218040373410622,
                "brier_score": 0.7493121131400111,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.044143436668549335,
                "brier_score": 0.7561458409644394,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_international_law": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.04065578140908705,
                "brier_score": 0.7441216148127173,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21212121212121213,
                "acc_stderr": 0.031922715695482974,
                "brier_score": 0.7566455758380454,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.22797927461139897,
                "acc_stderr": 0.030276909945178267,
                "brier_score": 0.7633302138074438,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.026265024608275882,
                "brier_score": 0.7594553429265166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_geography": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.029620227874790486,
                "brier_score": 0.7576363270233148,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24036697247706423,
                "acc_stderr": 0.01832060732096407,
                "brier_score": 0.7565590314930034,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_public_relations": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.04265792110940588,
                "brier_score": 0.7469096593763332,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909284,
                "brier_score": 0.7470080039570334,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_sociology": {
                "acc": 0.2537313432835821,
                "acc_stderr": 0.030769444967296007,
                "brier_score": 0.7548193420044138,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.23333333333333334,
                "acc_stderr": 0.021444547301560472,
                "brier_score": 0.7595149623585433,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_security_studies": {
                "acc": 0.22040816326530613,
                "acc_stderr": 0.026537045312145298,
                "brier_score": 0.7512240644391838,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_professional_psychology": {
                "acc": 0.26143790849673204,
                "acc_stderr": 0.017776947157528044,
                "brier_score": 0.7506030256061083,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_human_sexuality": {
                "acc": 0.20610687022900764,
                "acc_stderr": 0.03547771004159463,
                "brier_score": 0.7542686799934393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_econometrics": {
                "acc": 0.21052631578947367,
                "acc_stderr": 0.038351539543994194,
                "brier_score": 0.7614829132464986,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_miscellaneous": {
                "acc": 0.29246487867177523,
                "acc_stderr": 0.016267000684598652,
                "brier_score": 0.7479227764357439,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_marketing": {
                "acc": 0.2948717948717949,
                "acc_stderr": 0.02987257770889117,
                "brier_score": 0.7485007588679611,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_management": {
                "acc": 0.23300970873786409,
                "acc_stderr": 0.041858325989283164,
                "brier_score": 0.76271521896551,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_nutrition": {
                "acc": 0.24836601307189543,
                "acc_stderr": 0.02473998135511359,
                "brier_score": 0.7542974540466784,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_medical_genetics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.735961193100805,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_human_aging": {
                "acc": 0.336322869955157,
                "acc_stderr": 0.031708824268455005,
                "brier_score": 0.7337681857323938,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_professional_medicine": {
                "acc": 0.21323529411764705,
                "acc_stderr": 0.02488097151229425,
                "brier_score": 0.779047316094294,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.03242414757483098,
                "brier_score": 0.7587275117336465,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_business_ethics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "brier_score": 0.7355173081904621,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2339622641509434,
                "acc_stderr": 0.02605529690115292,
                "brier_score": 0.7554649912770037,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_global_facts": {
                "acc": 0.35,
                "acc_stderr": 0.047937248544110196,
                "brier_score": 0.7279422265069059,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_virology": {
                "acc": 0.30120481927710846,
                "acc_stderr": 0.03571609230053481,
                "brier_score": 0.7404351214968269,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_professional_accounting": {
                "acc": 0.24822695035460993,
                "acc_stderr": 0.025770015644290406,
                "brier_score": 0.7517030850500305,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_college_physics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.04023382273617747,
                "brier_score": 0.7568115594569838,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_physics": {
                "acc": 0.31788079470198677,
                "acc_stderr": 0.03802039760107903,
                "brier_score": 0.7428996547009279,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_biology": {
                "acc": 0.25806451612903225,
                "acc_stderr": 0.02489246917246283,
                "brier_score": 0.7504824526624697,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_college_biology": {
                "acc": 0.20833333333333334,
                "acc_stderr": 0.033961162058453336,
                "brier_score": 0.7527306122742272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_anatomy": {
                "acc": 0.2740740740740741,
                "acc_stderr": 0.03853254836552003,
                "brier_score": 0.7443037185127753,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_college_chemistry": {
                "acc": 0.22,
                "acc_stderr": 0.0416333199893227,
                "brier_score": 0.760267604530027,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_computer_security": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7429189011446976,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_college_computer_science": {
                "acc": 0.26,
                "acc_stderr": 0.044084400227680794,
                "brier_score": 0.7496145861335097,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_astronomy": {
                "acc": 0.26973684210526316,
                "acc_stderr": 0.03611780560284898,
                "brier_score": 0.7488921848629451,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_college_mathematics": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421296,
                "brier_score": 0.7419170181348044,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2,
                "acc_stderr": 0.026148818018424506,
                "brier_score": 0.7562972788541444,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.7608485266619526,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621505,
                "brier_score": 0.7334407115286586,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_machine_learning": {
                "acc": 0.23214285714285715,
                "acc_stderr": 0.04007341809755805,
                "brier_score": 0.7489247129769812,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.27586206896551724,
                "acc_stderr": 0.03144712581678242,
                "brier_score": 0.748090631862592,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1712962962962963,
                "acc_stderr": 0.02569534164382468,
                "brier_score": 0.7684251083451906,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2619047619047619,
                "acc_stderr": 0.022644212615525218,
                "brier_score": 0.7539737149923048,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2896551724137931,
                "acc_stderr": 0.03780019230438015,
                "brier_score": 0.7435242337059804,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24814814814814815,
                "acc_stderr": 0.026335739404055803,
                "brier_score": 0.7540582461553792,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-17-55.916462"
            },
            "arc_challenge": {
                "acc": 0.36006825938566556,
                "acc_stderr": 0.014027516814585186,
                "acc_norm": 0.40784982935153585,
                "acc_norm_stderr": 0.014361097288449701,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "hellaswag": {
                "acc": 0.5309699263095001,
                "acc_stderr": 0.0049802004518516765,
                "acc_norm": 0.7155945030870344,
                "acc_norm_stderr": 0.00450208828747007,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "truthfulqa_mc2": {
                "acc": 0.34584348807121285,
                "acc_stderr": 0.013212582599603815,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "truthfulqa_gen": {
                "bleu_max": 25.09984177135623,
                "bleu_max_stderr": 0.7599305897286084,
                "bleu_acc": 0.3011015911872705,
                "bleu_acc_stderr": 0.01605899902610059,
                "bleu_diff": -7.793264166700625,
                "bleu_diff_stderr": 0.8253857817947617,
                "rouge1_max": 50.570610391811464,
                "rouge1_max_stderr": 0.859536655414322,
                "rouge1_acc": 0.2668298653610771,
                "rouge1_acc_stderr": 0.015483691939237258,
                "rouge1_diff": -10.097862717598343,
                "rouge1_diff_stderr": 0.9307007939492593,
                "rouge2_max": 34.0661565486233,
                "rouge2_max_stderr": 0.9907235368522948,
                "rouge2_acc": 0.23990208078335373,
                "rouge2_acc_stderr": 0.014948812679062142,
                "rouge2_diff": -11.881434651179656,
                "rouge2_diff_stderr": 1.0800608172022408,
                "rougeL_max": 47.56194685447575,
                "rougeL_max_stderr": 0.8711921370362652,
                "rougeL_acc": 0.2631578947368421,
                "rougeL_acc_stderr": 0.015415241740237002,
                "rougeL_diff": -10.243475244546104,
                "rougeL_diff_stderr": 0.9305534307619345,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "truthfulqa_mc1": {
                "acc": 0.2141982864137087,
                "acc_stderr": 0.014362148155690466,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "winogrande": {
                "acc": 0.6677190213101816,
                "acc_stderr": 0.013238316554236533,
                "timestamp": "2024-11-06T18-40-41.914183"
            }
        }
    }
}