{
    "model_name": "openlm-research/open_llama_3b_v2",
    "last_updated": "2024-12-04 11:25:40.016223",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.009157509157509158,
                "exact_match_stderr": 0.004080306065048996,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.04362801377726751,
                "exact_match_stderr": 0.006925266930505693,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.012962962962962963,
                "exact_match_stderr": 0.004872192984581458,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.015503875968992248,
                "exact_match_stderr": 0.0041136172383604485,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_geometry": {
                "exact_match": 0.014613778705636743,
                "exact_match_stderr": 0.0054887134436863135,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.014767932489451477,
                "exact_match_stderr": 0.00554623858966847,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "minerva_math_algebra": {
                "exact_match": 0.02106149957877001,
                "exact_match_stderr": 0.004169461854206043,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_3da": {
                "acc": 0.284,
                "acc_stderr": 0.010085775202269382,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_3ds": {
                "acc": 0.2605,
                "acc_stderr": 0.009816723436540222,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_4da": {
                "acc": 0.0825,
                "acc_stderr": 0.0061535199604740055,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_2ds": {
                "acc": 0.4195,
                "acc_stderr": 0.011037245371590673,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_5ds": {
                "acc": 0.015,
                "acc_stderr": 0.0027186753387999575,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_5da": {
                "acc": 0.015,
                "acc_stderr": 0.002718675338799954,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_1dc": {
                "acc": 0.0955,
                "acc_stderr": 0.006573544001554193,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_4ds": {
                "acc": 0.101,
                "acc_stderr": 0.006739600218525675,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_2dm": {
                "acc": 0.1655,
                "acc_stderr": 0.00831200455424887,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "arithmetic_2da": {
                "acc": 0.4295,
                "acc_stderr": 0.01107141197325543,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "gsm8k_cot": {
                "exact_match": 0.04852160727824109,
                "exact_match_stderr": 0.005918468618921081,
                "timestamp": "2024-06-13T11-26-55.018499"
            },
            "gsm8k": {
                "exact_match": 0.039423805913570885,
                "exact_match_stderr": 0.005360280030342438,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "anli_r2": {
                "brier_score": 0.707706480342601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "anli_r3": {
                "brier_score": 0.679183075689064,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "anli_r1": {
                "brier_score": 0.7235055238380341,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_eu": {
                "brier_score": 1.132908017291774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_vi": {
                "brier_score": 0.971548338532371,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_ru": {
                "brier_score": 0.7523375161853497,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_zh": {
                "brier_score": 0.9867010701190115,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_tr": {
                "brier_score": 0.8808176289678832,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_fr": {
                "brier_score": 0.7460738931677325,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_en": {
                "brier_score": 0.6474238073900174,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_ur": {
                "brier_score": 1.3250861980436117,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_ar": {
                "brier_score": 1.234167496082821,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_de": {
                "brier_score": 0.812354579101372,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_hi": {
                "brier_score": 1.1542572623687906,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_es": {
                "brier_score": 0.8910867470387117,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_bg": {
                "brier_score": 0.859452606954678,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_sw": {
                "brier_score": 1.0713476113363065,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_el": {
                "brier_score": 1.1450771586115809,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "xnli_th": {
                "brier_score": 0.9689833936635035,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "logiqa2": {
                "brier_score": 1.0179439999333861,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "mathqa": {
                "brier_score": 0.9356941842152771,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T11-34-17.917857"
            },
            "lambada_standard": {
                "perplexity": 5.818821278870909,
                "perplexity_stderr": 0.13816721772720136,
                "acc": 0.5951872695517174,
                "acc_stderr": 0.006838580607651546,
                "timestamp": "2024-06-13T11-35-25.155228"
            },
            "lambada_openai": {
                "perplexity": 4.561675691644536,
                "perplexity_stderr": 0.10330641094995072,
                "acc": 0.6735882010479333,
                "acc_stderr": 0.006532692754359034,
                "timestamp": "2024-06-13T11-35-25.155228"
            },
            "mmlu_world_religions": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.03565079670708312,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_formal_logic": {
                "acc": 0.23809523809523808,
                "acc_stderr": 0.03809523809523809,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_prehistory": {
                "acc": 0.2839506172839506,
                "acc_stderr": 0.02508947852376513,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2435754189944134,
                "acc_stderr": 0.01435591196476786,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2489451476793249,
                "acc_stderr": 0.028146970599422644,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26011560693641617,
                "acc_stderr": 0.023618678310069374,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_professional_law": {
                "acc": 0.2470664928292047,
                "acc_stderr": 0.01101575225527935,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2085889570552147,
                "acc_stderr": 0.031921934489347235,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.23039215686274508,
                "acc_stderr": 0.029554292605695066,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_philosophy": {
                "acc": 0.27009646302250806,
                "acc_stderr": 0.025218040373410622,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.04414343666854933,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_international_law": {
                "acc": 0.2809917355371901,
                "acc_stderr": 0.04103203830514511,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.24242424242424243,
                "acc_stderr": 0.033464098810559534,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.23316062176165803,
                "acc_stderr": 0.030516111371476008,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.28991596638655465,
                "acc_stderr": 0.029472485833136098,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_geography": {
                "acc": 0.21717171717171718,
                "acc_stderr": 0.02937661648494563,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24036697247706423,
                "acc_stderr": 0.01832060732096407,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_public_relations": {
                "acc": 0.3181818181818182,
                "acc_stderr": 0.04461272175910507,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695235,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_sociology": {
                "acc": 0.25870646766169153,
                "acc_stderr": 0.030965903123573033,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24871794871794872,
                "acc_stderr": 0.021916957709213803,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_security_studies": {
                "acc": 0.3224489795918367,
                "acc_stderr": 0.02992310056368391,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_professional_psychology": {
                "acc": 0.27124183006535946,
                "acc_stderr": 0.017986615304030316,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_human_sexuality": {
                "acc": 0.24427480916030533,
                "acc_stderr": 0.03768335959728742,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.04049339297748142,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2835249042145594,
                "acc_stderr": 0.016117318166832276,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_marketing": {
                "acc": 0.29914529914529914,
                "acc_stderr": 0.02999695185834948,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_management": {
                "acc": 0.2524271844660194,
                "acc_stderr": 0.04301250399690878,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_nutrition": {
                "acc": 0.26143790849673204,
                "acc_stderr": 0.025160998214292456,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_medical_genetics": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_human_aging": {
                "acc": 0.3811659192825112,
                "acc_stderr": 0.03259625118416827,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_professional_medicine": {
                "acc": 0.21691176470588236,
                "acc_stderr": 0.025035845227711274,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_college_medicine": {
                "acc": 0.2254335260115607,
                "acc_stderr": 0.03186209851641143,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_business_ethics": {
                "acc": 0.41,
                "acc_stderr": 0.049431107042371025,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.29056603773584905,
                "acc_stderr": 0.027943219989337145,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_global_facts": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_virology": {
                "acc": 0.3373493975903614,
                "acc_stderr": 0.0368078369072758,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_professional_accounting": {
                "acc": 0.30141843971631205,
                "acc_stderr": 0.02737412888263115,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.04092563958237654,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_physics": {
                "acc": 0.32450331125827814,
                "acc_stderr": 0.03822746937658753,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_biology": {
                "acc": 0.23548387096774193,
                "acc_stderr": 0.024137632429337717,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_college_biology": {
                "acc": 0.24305555555555555,
                "acc_stderr": 0.03586879280080341,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_anatomy": {
                "acc": 0.23703703703703705,
                "acc_stderr": 0.03673731683969506,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_college_chemistry": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909284,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_computer_security": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621505,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_college_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816507,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_astronomy": {
                "acc": 0.26973684210526316,
                "acc_stderr": 0.036117805602848975,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_college_mathematics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.3446808510638298,
                "acc_stderr": 0.031068985963122145,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768079,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_machine_learning": {
                "acc": 0.29464285714285715,
                "acc_stderr": 0.0432704093257873,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.030108330718011625,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.19444444444444445,
                "acc_stderr": 0.026991454502036726,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.29365079365079366,
                "acc_stderr": 0.02345603738398203,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.20689655172413793,
                "acc_stderr": 0.03375672449560553,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.025348097468097845,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "arc_challenge": {
                "acc": 0.36006825938566556,
                "acc_stderr": 0.014027516814585186,
                "acc_norm": 0.40784982935153585,
                "acc_norm_stderr": 0.014361097288449701,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "hellaswag": {
                "acc": 0.5309699263095001,
                "acc_stderr": 0.0049802004518516765,
                "acc_norm": 0.7155945030870344,
                "acc_norm_stderr": 0.00450208828747007,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "truthfulqa_mc2": {
                "acc": 0.34584348807121285,
                "acc_stderr": 0.013212582599603815,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "truthfulqa_gen": {
                "bleu_max": 25.09984177135623,
                "bleu_max_stderr": 0.7599305897286084,
                "bleu_acc": 0.3011015911872705,
                "bleu_acc_stderr": 0.01605899902610059,
                "bleu_diff": -7.793264166700625,
                "bleu_diff_stderr": 0.8253857817947617,
                "rouge1_max": 50.570610391811464,
                "rouge1_max_stderr": 0.859536655414322,
                "rouge1_acc": 0.2668298653610771,
                "rouge1_acc_stderr": 0.015483691939237258,
                "rouge1_diff": -10.097862717598343,
                "rouge1_diff_stderr": 0.9307007939492593,
                "rouge2_max": 34.0661565486233,
                "rouge2_max_stderr": 0.9907235368522948,
                "rouge2_acc": 0.23990208078335373,
                "rouge2_acc_stderr": 0.014948812679062142,
                "rouge2_diff": -11.881434651179656,
                "rouge2_diff_stderr": 1.0800608172022408,
                "rougeL_max": 47.56194685447575,
                "rougeL_max_stderr": 0.8711921370362652,
                "rougeL_acc": 0.2631578947368421,
                "rougeL_acc_stderr": 0.015415241740237002,
                "rougeL_diff": -10.243475244546104,
                "rougeL_diff_stderr": 0.9305534307619345,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "truthfulqa_mc1": {
                "acc": 0.2141982864137087,
                "acc_stderr": 0.014362148155690466,
                "timestamp": "2024-11-06T18-40-41.914183"
            },
            "winogrande": {
                "acc": 0.6677190213101816,
                "acc_stderr": 0.013238316554236533,
                "timestamp": "2024-11-06T18-40-41.914183"
            }
        }
    }
}