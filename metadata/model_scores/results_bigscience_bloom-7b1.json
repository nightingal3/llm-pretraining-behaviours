{
    "model_name": "bigscience/bloom-7b1",
    "last_updated": "2023-12-04",
    "results": {
        "harness": {
            "gsm8k": {
                "5-shot": {
                    "acc": 0.013646702047005308,
                    "acc_stderr": 0.0031957470754807806,
                    "timestamp": "2023-12-04T13-10-02.911977"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.0009437919463087249,
                    "acc_stderr": 0.00031446531194131934,
                    "f1": 0.04796455536912761,
                    "f1_stderr": 0.0011701293326885863,
                    "timestamp": "2023-10-19T04-28-54.166367"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.654301499605367,
                    "acc_stderr": 0.013366596951934375,
                    "timestamp": "2023-10-19T04-28-54.166367"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.3643344709897611,
                    "acc_stderr": 0.014063260279882412,
                    "acc_norm": 0.4112627986348123,
                    "acc_norm_stderr": 0.01437944106852208,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.4622585142401912,
                    "acc_stderr": 0.0049755460189506795,
                    "acc_norm": 0.6199960167297351,
                    "acc_norm_stderr": 0.004843954338451442,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768081,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768081,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.24444444444444444,
                    "acc_stderr": 0.03712537833614866,
                    "acc_norm": 0.24444444444444444,
                    "acc_norm_stderr": 0.03712537833614866,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.17763157894736842,
                    "acc_stderr": 0.03110318238312338,
                    "acc_norm": 0.17763157894736842,
                    "acc_norm_stderr": 0.03110318238312338,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768079,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768079,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.27547169811320754,
                    "acc_stderr": 0.027495663683724057,
                    "acc_norm": 0.27547169811320754,
                    "acc_norm_stderr": 0.027495663683724057,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.19444444444444445,
                    "acc_stderr": 0.03309615177059004,
                    "acc_norm": 0.19444444444444445,
                    "acc_norm_stderr": 0.03309615177059004,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.04093601807403326,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.04093601807403326,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621504,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621504,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.20809248554913296,
                    "acc_stderr": 0.0309528902177499,
                    "acc_norm": 0.20809248554913296,
                    "acc_norm_stderr": 0.0309528902177499,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.04023382273617749,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.04023382273617749,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816505,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816505,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.32340425531914896,
                    "acc_stderr": 0.030579442773610334,
                    "acc_norm": 0.32340425531914896,
                    "acc_norm_stderr": 0.030579442773610334,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.23684210526315788,
                    "acc_stderr": 0.039994238792813365,
                    "acc_norm": 0.23684210526315788,
                    "acc_norm_stderr": 0.039994238792813365,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2482758620689655,
                    "acc_stderr": 0.03600105692727771,
                    "acc_norm": 0.2482758620689655,
                    "acc_norm_stderr": 0.03600105692727771,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2751322751322751,
                    "acc_stderr": 0.023000086859068646,
                    "acc_norm": 0.2751322751322751,
                    "acc_norm_stderr": 0.023000086859068646,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.16666666666666666,
                    "acc_stderr": 0.033333333333333375,
                    "acc_norm": 0.16666666666666666,
                    "acc_norm_stderr": 0.033333333333333375,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252604,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.25483870967741934,
                    "acc_stderr": 0.024790118459332208,
                    "acc_norm": 0.25483870967741934,
                    "acc_norm_stderr": 0.024790118459332208,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.270935960591133,
                    "acc_stderr": 0.031270907132976984,
                    "acc_norm": 0.270935960591133,
                    "acc_norm_stderr": 0.031270907132976984,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.0479372485441102,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.0479372485441102,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.03453131801885415,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.03453131801885415,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.22727272727272727,
                    "acc_stderr": 0.0298575156733864,
                    "acc_norm": 0.22727272727272727,
                    "acc_norm_stderr": 0.0298575156733864,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.20725388601036268,
                    "acc_stderr": 0.02925282329180362,
                    "acc_norm": 0.20725388601036268,
                    "acc_norm_stderr": 0.02925282329180362,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.23076923076923078,
                    "acc_stderr": 0.02136202772522271,
                    "acc_norm": 0.23076923076923078,
                    "acc_norm_stderr": 0.02136202772522271,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.026962424325073852,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.026962424325073852,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.226890756302521,
                    "acc_stderr": 0.02720537153827948,
                    "acc_norm": 0.226890756302521,
                    "acc_norm_stderr": 0.02720537153827948,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.23841059602649006,
                    "acc_stderr": 0.034791855725996586,
                    "acc_norm": 0.23841059602649006,
                    "acc_norm_stderr": 0.034791855725996586,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.24770642201834864,
                    "acc_stderr": 0.018508143602547808,
                    "acc_norm": 0.24770642201834864,
                    "acc_norm_stderr": 0.018508143602547808,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.38425925925925924,
                    "acc_stderr": 0.03317354514310742,
                    "acc_norm": 0.38425925925925924,
                    "acc_norm_stderr": 0.03317354514310742,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.030964517926923403,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.030964517926923403,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.26582278481012656,
                    "acc_stderr": 0.028756799629658335,
                    "acc_norm": 0.26582278481012656,
                    "acc_norm_stderr": 0.028756799629658335,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.3632286995515695,
                    "acc_stderr": 0.03227790442850499,
                    "acc_norm": 0.3632286995515695,
                    "acc_norm_stderr": 0.03227790442850499,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.22900763358778625,
                    "acc_stderr": 0.036853466317118506,
                    "acc_norm": 0.22900763358778625,
                    "acc_norm_stderr": 0.036853466317118506,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.36363636363636365,
                    "acc_stderr": 0.043913262867240704,
                    "acc_norm": 0.36363636363636365,
                    "acc_norm_stderr": 0.043913262867240704,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.28703703703703703,
                    "acc_stderr": 0.043733130409147614,
                    "acc_norm": 0.28703703703703703,
                    "acc_norm_stderr": 0.043733130409147614,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.25766871165644173,
                    "acc_stderr": 0.03436150827846917,
                    "acc_norm": 0.25766871165644173,
                    "acc_norm_stderr": 0.03436150827846917,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.29464285714285715,
                    "acc_stderr": 0.0432704093257873,
                    "acc_norm": 0.29464285714285715,
                    "acc_norm_stderr": 0.0432704093257873,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2524271844660194,
                    "acc_stderr": 0.04301250399690877,
                    "acc_norm": 0.2524271844660194,
                    "acc_norm_stderr": 0.04301250399690877,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.25213675213675213,
                    "acc_stderr": 0.02844796547623101,
                    "acc_norm": 0.25213675213675213,
                    "acc_norm_stderr": 0.02844796547623101,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.044084400227680794,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.044084400227680794,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.2886334610472541,
                    "acc_stderr": 0.016203792703197804,
                    "acc_norm": 0.2886334610472541,
                    "acc_norm_stderr": 0.016203792703197804,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.24566473988439305,
                    "acc_stderr": 0.02317629820399201,
                    "acc_norm": 0.24566473988439305,
                    "acc_norm_stderr": 0.02317629820399201,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2424581005586592,
                    "acc_stderr": 0.014333522059217889,
                    "acc_norm": 0.2424581005586592,
                    "acc_norm_stderr": 0.014333522059217889,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.23202614379084968,
                    "acc_stderr": 0.02417084087934101,
                    "acc_norm": 0.23202614379084968,
                    "acc_norm_stderr": 0.02417084087934101,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.2733118971061093,
                    "acc_stderr": 0.02531176597542612,
                    "acc_norm": 0.2733118971061093,
                    "acc_norm_stderr": 0.02531176597542612,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2623456790123457,
                    "acc_stderr": 0.02447722285613511,
                    "acc_norm": 0.2623456790123457,
                    "acc_norm_stderr": 0.02447722285613511,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.26595744680851063,
                    "acc_stderr": 0.02635806569888059,
                    "acc_norm": 0.26595744680851063,
                    "acc_norm_stderr": 0.02635806569888059,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2529335071707953,
                    "acc_stderr": 0.011102268713839987,
                    "acc_norm": 0.2529335071707953,
                    "acc_norm_stderr": 0.011102268713839987,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.20955882352941177,
                    "acc_stderr": 0.024723110407677055,
                    "acc_norm": 0.20955882352941177,
                    "acc_norm_stderr": 0.024723110407677055,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.26143790849673204,
                    "acc_stderr": 0.017776947157528034,
                    "acc_norm": 0.26143790849673204,
                    "acc_norm_stderr": 0.017776947157528034,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.32727272727272727,
                    "acc_stderr": 0.0449429086625209,
                    "acc_norm": 0.32727272727272727,
                    "acc_norm_stderr": 0.0449429086625209,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.3020408163265306,
                    "acc_stderr": 0.02939360931987981,
                    "acc_norm": 0.3020408163265306,
                    "acc_norm_stderr": 0.02939360931987981,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.23880597014925373,
                    "acc_stderr": 0.030147775935409224,
                    "acc_norm": 0.23880597014925373,
                    "acc_norm_stderr": 0.030147775935409224,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3072289156626506,
                    "acc_stderr": 0.035915667978246635,
                    "acc_norm": 0.3072289156626506,
                    "acc_norm_stderr": 0.035915667978246635,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.29239766081871343,
                    "acc_stderr": 0.03488647713457922,
                    "acc_norm": 0.29239766081871343,
                    "acc_norm_stderr": 0.03488647713457922,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.22399020807833536,
                    "mc1_stderr": 0.014594964329474202,
                    "mc2": 0.38897842190357873,
                    "mc2_stderr": 0.014015753482036425,
                    "timestamp": "2023-08-11T17-31-45.771046"
                }
            }
        }
    }
}