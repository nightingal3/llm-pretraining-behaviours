{
    "model_name": "facebook/xglm-4.5B",
    "last_updated": "2024-12-19 13:38:27.656223",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0011074197120708748,
                "exact_match_stderr": 0.0011074197120708796,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "minerva_math_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_3da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000061,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_3ds": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000116,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_4da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_2ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_1dc": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339508,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_2dm": {
                "acc": 0.006,
                "acc_stderr": 0.0017272787111155192,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "arithmetic_2da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "gsm8k_cot": {
                "exact_match": 0.021986353297952996,
                "exact_match_stderr": 0.004039162758110061,
                "timestamp": "2024-06-09T15-39-47.755502"
            },
            "gsm8k": {
                "exact_match": 0.018953752843062926,
                "exact_match_stderr": 0.003756078341031474,
                "timestamp": "2024-11-10T14-16-29.255958"
            },
            "anli_r2": {
                "brier_score": 0.7423986272054851,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "anli_r3": {
                "brier_score": 0.753360865009507,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "anli_r1": {
                "brier_score": 0.7489020744444143,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_eu": {
                "brier_score": 0.8835776224752929,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_vi": {
                "brier_score": 0.734074056481032,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_ru": {
                "brier_score": 0.7752082188185605,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_zh": {
                "brier_score": 1.059743376070012,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_tr": {
                "brier_score": 0.7912014592670232,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_fr": {
                "brier_score": 0.7417811076459814,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_en": {
                "brier_score": 0.6323125293224192,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_ur": {
                "brier_score": 0.9364267567095893,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_ar": {
                "brier_score": 1.2526435424482893,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_de": {
                "brier_score": 0.8248883014366909,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_hi": {
                "brier_score": 0.7748000452700087,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_es": {
                "brier_score": 0.824750043726708,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_bg": {
                "brier_score": 0.7293483146313945,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_sw": {
                "brier_score": 0.7765272974570628,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_el": {
                "brier_score": 0.813931199828111,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "xnli_th": {
                "brier_score": 0.8560226403534275,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "logiqa2": {
                "brier_score": 1.1510780077217264,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "mathqa": {
                "brier_score": 0.9914166377643306,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T15-47-02.091070"
            },
            "lambada_standard": {
                "perplexity": 9.728468384838203,
                "perplexity_stderr": 0.27282567798166885,
                "acc": 0.5152338443625073,
                "acc_stderr": 0.006962743717451541,
                "timestamp": "2024-06-09T15-48-16.680733"
            },
            "lambada_openai": {
                "perplexity": 8.474112679424595,
                "perplexity_stderr": 0.23447686069986642,
                "acc": 0.526877547059965,
                "acc_stderr": 0.00695590589621272,
                "timestamp": "2024-06-09T15-48-16.680733"
            },
            "mmlu_world_religions": {
                "acc": 0.27485380116959063,
                "acc_stderr": 0.034240429246915824,
                "brier_score": 0.755274678758315,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_formal_logic": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.04006168083848876,
                "brier_score": 0.7663550040673333,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_prehistory": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.023788583551658533,
                "brier_score": 0.7652663553372573,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24581005586592178,
                "acc_stderr": 0.014400296429225622,
                "brier_score": 0.7672549005501607,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.270042194092827,
                "acc_stderr": 0.028900721906293426,
                "brier_score": 0.7571285708105355,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_moral_disputes": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.02289408248992599,
                "brier_score": 0.7647857763853563,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_professional_law": {
                "acc": 0.2607561929595828,
                "acc_stderr": 0.011213471559602338,
                "brier_score": 0.7557701064945394,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.26993865030674846,
                "acc_stderr": 0.034878251684978906,
                "brier_score": 0.7551927191767064,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.24019607843137256,
                "acc_stderr": 0.02998373305591362,
                "brier_score": 0.7678181155256781,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_philosophy": {
                "acc": 0.2057877813504823,
                "acc_stderr": 0.02296133990676424,
                "brier_score": 0.7916388053551179,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.044143436668549335,
                "brier_score": 0.7521606226757167,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_international_law": {
                "acc": 0.2644628099173554,
                "acc_stderr": 0.04026187527591207,
                "brier_score": 0.7654804359244274,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.20606060606060606,
                "acc_stderr": 0.03158415324047709,
                "brier_score": 0.7862340210921961,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.22797927461139897,
                "acc_stderr": 0.030276909945178267,
                "brier_score": 0.7665234321742033,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.226890756302521,
                "acc_stderr": 0.027205371538279476,
                "brier_score": 0.7561418553252524,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_geography": {
                "acc": 0.18686868686868688,
                "acc_stderr": 0.02777253333421899,
                "brier_score": 0.7781380168315729,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.21100917431192662,
                "acc_stderr": 0.017493922404112648,
                "brier_score": 0.7756897923681646,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_public_relations": {
                "acc": 0.3,
                "acc_stderr": 0.04389311454644287,
                "brier_score": 0.744357285811644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909282,
                "brier_score": 0.7615163811611504,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_sociology": {
                "acc": 0.21890547263681592,
                "acc_stderr": 0.029239174636647,
                "brier_score": 0.7675237295477422,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24102564102564103,
                "acc_stderr": 0.021685546665333195,
                "brier_score": 0.7628253979700401,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_security_studies": {
                "acc": 0.2897959183673469,
                "acc_stderr": 0.029043088683304324,
                "brier_score": 0.7478338865347484,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.017740899509177788,
                "brier_score": 0.7549298076413554,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_human_sexuality": {
                "acc": 0.22137404580152673,
                "acc_stderr": 0.03641297081313729,
                "brier_score": 0.7843585183451098,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.04049339297748141,
                "brier_score": 0.7519263840177439,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2247765006385696,
                "acc_stderr": 0.014927447101937164,
                "brier_score": 0.7688507326782054,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_marketing": {
                "acc": 0.23076923076923078,
                "acc_stderr": 0.027601921381417628,
                "brier_score": 0.759167155406607,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_management": {
                "acc": 0.21359223300970873,
                "acc_stderr": 0.04058042015646034,
                "brier_score": 0.7666997072926246,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_nutrition": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.025261691219729484,
                "brier_score": 0.7684122808652835,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_medical_genetics": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621505,
                "brier_score": 0.7475889299146562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_human_aging": {
                "acc": 0.336322869955157,
                "acc_stderr": 0.031708824268455005,
                "brier_score": 0.7285624927346905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_professional_medicine": {
                "acc": 0.1875,
                "acc_stderr": 0.023709788253811766,
                "brier_score": 0.7905421043154426,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_college_medicine": {
                "acc": 0.23121387283236994,
                "acc_stderr": 0.032147373020294696,
                "brier_score": 0.7691126269486598,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_business_ethics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "brier_score": 0.756343264035676,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.19622641509433963,
                "acc_stderr": 0.024442388131100824,
                "brier_score": 0.77492420767386,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_global_facts": {
                "acc": 0.19,
                "acc_stderr": 0.039427724440366234,
                "brier_score": 0.7720125546673212,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_virology": {
                "acc": 0.27710843373493976,
                "acc_stderr": 0.03484331592680588,
                "brier_score": 0.752677142414452,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2872340425531915,
                "acc_stderr": 0.026992199173064356,
                "brier_score": 0.7521154419726764,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_college_physics": {
                "acc": 0.3235294117647059,
                "acc_stderr": 0.046550104113196177,
                "brier_score": 0.7634458806639429,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2185430463576159,
                "acc_stderr": 0.03374235550425694,
                "brier_score": 0.7694405034154213,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2709677419354839,
                "acc_stderr": 0.025284416114900156,
                "brier_score": 0.7571428679121684,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_college_biology": {
                "acc": 0.2986111111111111,
                "acc_stderr": 0.03827052357950756,
                "brier_score": 0.7522643021022564,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_anatomy": {
                "acc": 0.2814814814814815,
                "acc_stderr": 0.03885004245800254,
                "brier_score": 0.7575248956377039,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_college_chemistry": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 0.784057389814469,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_computer_security": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7511719218716887,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_college_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.7568992421562741,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_astronomy": {
                "acc": 0.21052631578947367,
                "acc_stderr": 0.03317672787533157,
                "brier_score": 0.7923962788237392,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_college_mathematics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7616331652332431,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.20425531914893616,
                "acc_stderr": 0.026355158413349424,
                "brier_score": 0.7668170378959265,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7643398455539884,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 0.7451260614096664,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_machine_learning": {
                "acc": 0.2767857142857143,
                "acc_stderr": 0.042466243366976235,
                "brier_score": 0.7638119478953422,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.24630541871921183,
                "acc_stderr": 0.03031509928561773,
                "brier_score": 0.7686052199936642,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2916666666666667,
                "acc_stderr": 0.030998666304560524,
                "brier_score": 0.7430103334400534,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.021411684393694196,
                "brier_score": 0.7801099705145796,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.23448275862068965,
                "acc_stderr": 0.035306258743465914,
                "brier_score": 0.7613993454942953,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.22592592592592592,
                "acc_stderr": 0.02549753263960955,
                "brier_score": 0.7656972806536992,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-56-43.759015"
            },
            "arc_challenge": {
                "acc": 0.2977815699658703,
                "acc_stderr": 0.013363080107244485,
                "acc_norm": 0.31313993174061433,
                "acc_norm_stderr": 0.0135526715436235,
                "timestamp": "2024-11-10T14-16-29.255958"
            },
            "hellaswag": {
                "acc": 0.4347739494124676,
                "acc_stderr": 0.004947141797384129,
                "acc_norm": 0.5830511850229038,
                "acc_norm_stderr": 0.004920465936068606,
                "timestamp": "2024-11-10T14-16-29.255958"
            },
            "truthfulqa_mc2": {
                "acc": 0.3582664656425214,
                "acc_stderr": 0.013774296665210408,
                "timestamp": "2024-11-10T14-16-29.255958"
            },
            "truthfulqa_gen": {
                "bleu_max": 11.647008551534103,
                "bleu_max_stderr": 0.5837506738382987,
                "bleu_acc": 0.2533659730722154,
                "bleu_acc_stderr": 0.015225899340826847,
                "bleu_diff": -3.651974700389702,
                "bleu_diff_stderr": 0.47117969919233177,
                "rouge1_max": 29.74676225067295,
                "rouge1_max_stderr": 0.8122994874104966,
                "rouge1_acc": 0.2766217870257038,
                "rouge1_acc_stderr": 0.015659605755326933,
                "rouge1_diff": -5.319592251781975,
                "rouge1_diff_stderr": 0.5675727855582745,
                "rouge2_max": 17.451517261760348,
                "rouge2_max_stderr": 0.7677122688036538,
                "rouge2_acc": 0.18727050183598531,
                "rouge2_acc_stderr": 0.013657229868067026,
                "rouge2_diff": -6.283551976079485,
                "rouge2_diff_stderr": 0.6225195424994728,
                "rougeL_max": 27.33216522895946,
                "rougeL_max_stderr": 0.7816913352147911,
                "rougeL_acc": 0.2741738066095471,
                "rougeL_acc_stderr": 0.01561651849721937,
                "rougeL_diff": -5.342717008492413,
                "rougeL_diff_stderr": 0.5591802833523164,
                "timestamp": "2024-11-10T14-16-29.255958"
            },
            "truthfulqa_mc1": {
                "acc": 0.20807833537331702,
                "acc_stderr": 0.014210503473576613,
                "timestamp": "2024-11-10T14-16-29.255958"
            },
            "winogrande": {
                "acc": 0.5572217837411207,
                "acc_stderr": 0.013960157350784982,
                "timestamp": "2024-11-10T14-16-29.255958"
            }
        }
    }
}