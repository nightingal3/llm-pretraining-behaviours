{
    "model_name": "allenai/OLMo-1.7-7B-hf",
    "last_updated": "2024-12-04 11:22:32.850592",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.023809523809523808,
                "exact_match_stderr": 0.006530469219761487,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.10677382319173363,
                "exact_match_stderr": 0.01047016417181583,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.022222222222222223,
                "exact_match_stderr": 0.006349206349206315,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.029900332225913623,
                "exact_match_stderr": 0.005670781528776242,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "minerva_math_geometry": {
                "exact_match": 0.04175365344467641,
                "exact_match_stderr": 0.009148963161034336,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.05063291139240506,
                "exact_match_stderr": 0.010080984934213225,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "minerva_math_algebra": {
                "exact_match": 0.08508845829823083,
                "exact_match_stderr": 0.008101818991032753,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_3da": {
                "acc": 0.005,
                "acc_stderr": 0.0015775754727385088,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_3ds": {
                "acc": 0.012,
                "acc_stderr": 0.002435357362429852,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_4da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_2ds": {
                "acc": 0.054,
                "acc_stderr": 0.005055173329243417,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_1dc": {
                "acc": 0.0275,
                "acc_stderr": 0.0036576719757437657,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_4ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521547,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_2dm": {
                "acc": 0.0525,
                "acc_stderr": 0.004988418302285764,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "arithmetic_2da": {
                "acc": 0.0665,
                "acc_stderr": 0.0055726476832024295,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "gsm8k_cot": {
                "exact_match": 0.26156178923426837,
                "exact_match_stderr": 0.012105605733382442,
                "timestamp": "2024-06-11T10-13-43.093977"
            },
            "gsm8k": {
                "exact_match": 0.266868840030326,
                "exact_match_stderr": 0.012183780551887954,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "anli_r2": {
                "brier_score": 0.8391596666130678,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "anli_r3": {
                "brier_score": 0.7378078013226562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "anli_r1": {
                "brier_score": 0.8642801317460914,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_eu": {
                "brier_score": 0.9129364330761797,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_vi": {
                "brier_score": 0.967681110248311,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_ru": {
                "brier_score": 0.7775890640925086,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_zh": {
                "brier_score": 0.9694985754970613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_tr": {
                "brier_score": 0.8886549764770375,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_fr": {
                "brier_score": 0.819053260383407,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_en": {
                "brier_score": 0.634008528496921,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_ur": {
                "brier_score": 1.289747157627863,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_ar": {
                "brier_score": 1.1791061584321425,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_de": {
                "brier_score": 0.8058792999721985,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_hi": {
                "brier_score": 1.0788350632986858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_es": {
                "brier_score": 0.870575516923212,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_bg": {
                "brier_score": 0.8898152315502774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_sw": {
                "brier_score": 0.9207528263844112,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_el": {
                "brier_score": 1.1774979653735478,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "xnli_th": {
                "brier_score": 0.9694268391093578,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "logiqa2": {
                "brier_score": 1.0623218884422152,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "mathqa": {
                "brier_score": 0.8861806154897796,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T10-44-50.194312"
            },
            "lambada_standard": {
                "perplexity": 4.885471880639927,
                "perplexity_stderr": 0.10747308964809098,
                "acc": 0.6382689695323113,
                "acc_stderr": 0.006694325434645211,
                "timestamp": "2024-06-11T10-47-47.697491"
            },
            "lambada_openai": {
                "perplexity": 3.8600315637206424,
                "perplexity_stderr": 0.07946409721254376,
                "acc": 0.7075490005821852,
                "acc_stderr": 0.0063374841865443225,
                "timestamp": "2024-06-11T10-47-47.697491"
            },
            "mmlu_world_religions": {
                "acc": 0.7076023391812866,
                "acc_stderr": 0.034886477134579215,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_formal_logic": {
                "acc": 0.25396825396825395,
                "acc_stderr": 0.03893259610604674,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_prehistory": {
                "acc": 0.5740740740740741,
                "acc_stderr": 0.027513747284379424,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24022346368715083,
                "acc_stderr": 0.014288343803925319,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.6962025316455697,
                "acc_stderr": 0.029936696387138615,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_moral_disputes": {
                "acc": 0.5751445086705202,
                "acc_stderr": 0.026613350840261736,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_professional_law": {
                "acc": 0.38005215123859193,
                "acc_stderr": 0.012397328205137812,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.588957055214724,
                "acc_stderr": 0.038656978537853624,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.6225490196078431,
                "acc_stderr": 0.03402272044340704,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_philosophy": {
                "acc": 0.594855305466238,
                "acc_stderr": 0.027882383791325963,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_jurisprudence": {
                "acc": 0.5833333333333334,
                "acc_stderr": 0.04766075165356461,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_international_law": {
                "acc": 0.6776859504132231,
                "acc_stderr": 0.042664163633521664,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.6787878787878788,
                "acc_stderr": 0.03646204963253812,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.7357512953367875,
                "acc_stderr": 0.03182155050916644,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.5588235294117647,
                "acc_stderr": 0.0322529423239964,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_geography": {
                "acc": 0.7272727272727273,
                "acc_stderr": 0.03173071239071724,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.7394495412844037,
                "acc_stderr": 0.018819182034850068,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_public_relations": {
                "acc": 0.6,
                "acc_stderr": 0.0469237132203465,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.79,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_sociology": {
                "acc": 0.746268656716418,
                "acc_stderr": 0.030769444967296035,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.5282051282051282,
                "acc_stderr": 0.0253106392549339,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_security_studies": {
                "acc": 0.5836734693877551,
                "acc_stderr": 0.031557828165561644,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_professional_psychology": {
                "acc": 0.5081699346405228,
                "acc_stderr": 0.02022513434305727,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_human_sexuality": {
                "acc": 0.6030534351145038,
                "acc_stderr": 0.04291135671009224,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_econometrics": {
                "acc": 0.35964912280701755,
                "acc_stderr": 0.04514496132873633,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_miscellaneous": {
                "acc": 0.7113665389527458,
                "acc_stderr": 0.016203792703197797,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_marketing": {
                "acc": 0.8034188034188035,
                "acc_stderr": 0.02603538609895129,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_management": {
                "acc": 0.6893203883495146,
                "acc_stderr": 0.045821241601615506,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_nutrition": {
                "acc": 0.6111111111111112,
                "acc_stderr": 0.02791405551046801,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_medical_genetics": {
                "acc": 0.57,
                "acc_stderr": 0.049756985195624284,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_human_aging": {
                "acc": 0.6278026905829597,
                "acc_stderr": 0.03244305283008731,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4852941176470588,
                "acc_stderr": 0.03035969707904611,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_college_medicine": {
                "acc": 0.5086705202312138,
                "acc_stderr": 0.03811890988940412,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_business_ethics": {
                "acc": 0.59,
                "acc_stderr": 0.04943110704237102,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.5773584905660377,
                "acc_stderr": 0.03040233144576954,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_global_facts": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_virology": {
                "acc": 0.4457831325301205,
                "acc_stderr": 0.03869543323472101,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_professional_accounting": {
                "acc": 0.4326241134751773,
                "acc_stderr": 0.029555454236778838,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_college_physics": {
                "acc": 0.30392156862745096,
                "acc_stderr": 0.04576665403207764,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_physics": {
                "acc": 0.3509933774834437,
                "acc_stderr": 0.03896981964257375,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_biology": {
                "acc": 0.6548387096774193,
                "acc_stderr": 0.027045746573534334,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_college_biology": {
                "acc": 0.5347222222222222,
                "acc_stderr": 0.041711158581816184,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_anatomy": {
                "acc": 0.4888888888888889,
                "acc_stderr": 0.043182754919779756,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_college_chemistry": {
                "acc": 0.43,
                "acc_stderr": 0.049756985195624284,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_computer_security": {
                "acc": 0.62,
                "acc_stderr": 0.04878317312145634,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_college_computer_science": {
                "acc": 0.48,
                "acc_stderr": 0.050211673156867795,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_astronomy": {
                "acc": 0.5131578947368421,
                "acc_stderr": 0.04067533136309172,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_college_mathematics": {
                "acc": 0.34,
                "acc_stderr": 0.04760952285695235,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.425531914893617,
                "acc_stderr": 0.03232146916224469,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.36,
                "acc_stderr": 0.04824181513244218,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.57,
                "acc_stderr": 0.04975698519562428,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_machine_learning": {
                "acc": 0.3482142857142857,
                "acc_stderr": 0.04521829902833585,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.42857142857142855,
                "acc_stderr": 0.034819048444388045,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4444444444444444,
                "acc_stderr": 0.03388857118502325,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.30158730158730157,
                "acc_stderr": 0.023636975996101803,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.496551724137931,
                "acc_stderr": 0.0416656757710158,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.32592592592592595,
                "acc_stderr": 0.02857834836547308,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "arc_challenge": {
                "acc": 0.4598976109215017,
                "acc_stderr": 0.01456431885692485,
                "acc_norm": 0.4974402730375427,
                "acc_norm_stderr": 0.014611199329843784,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "hellaswag": {
                "acc": 0.5886277633937462,
                "acc_stderr": 0.004910767540867411,
                "acc_norm": 0.788886675960964,
                "acc_norm_stderr": 0.00407264587499226,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "truthfulqa_mc2": {
                "acc": 0.3590781045200021,
                "acc_stderr": 0.013472691004650292,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "truthfulqa_gen": {
                "bleu_max": 25.135570028092875,
                "bleu_max_stderr": 0.7937442609696079,
                "bleu_acc": 0.31211750305997554,
                "bleu_acc_stderr": 0.01622075676952092,
                "bleu_diff": -8.039782636719476,
                "bleu_diff_stderr": 0.8039064432980269,
                "rouge1_max": 50.26135196252015,
                "rouge1_max_stderr": 0.8621379304595767,
                "rouge1_acc": 0.29008567931456547,
                "rouge1_acc_stderr": 0.01588623687420952,
                "rouge1_diff": -11.903242447741343,
                "rouge1_diff_stderr": 0.9071414792248184,
                "rouge2_max": 33.597620557273906,
                "rouge2_max_stderr": 1.0023202672623095,
                "rouge2_acc": 0.24724602203182375,
                "rouge2_acc_stderr": 0.015102404797359649,
                "rouge2_diff": -12.949554590188171,
                "rouge2_diff_stderr": 1.025590144258018,
                "rougeL_max": 47.52109438620076,
                "rougeL_max_stderr": 0.8690433934810369,
                "rougeL_acc": 0.28151774785801714,
                "rougeL_acc_stderr": 0.01574402724825605,
                "rougeL_diff": -11.977711482592492,
                "rougeL_diff_stderr": 0.9105847605364588,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "truthfulqa_mc1": {
                "acc": 0.2386780905752754,
                "acc_stderr": 0.014922629695456416,
                "timestamp": "2024-11-22T01-22-23.457007"
            },
            "winogrande": {
                "acc": 0.7253354380426204,
                "acc_stderr": 0.012544516005117192,
                "timestamp": "2024-11-22T01-22-23.457007"
            }
        }
    }
}