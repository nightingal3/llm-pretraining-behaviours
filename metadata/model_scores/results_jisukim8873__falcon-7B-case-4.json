{
    "model_name": "jisukim8873/falcon-7B-case-4",
    "last_updated": "2024-12-04 11:25:12.377286",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.439419795221843,
                    "acc_stderr": 0.014503747823580127,
                    "acc_norm": 0.4761092150170648,
                    "acc_norm_stderr": 0.014594701798071654,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hellaswag": {
                "acc": 0.5990838478390759,
                "acc_stderr": 0.0048908247185303,
                "acc_norm": 0.7893845847440749,
                "acc_norm_stderr": 0.004069123905324906,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621504,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621504,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.28888888888888886,
                    "acc_stderr": 0.03915450630414251,
                    "acc_norm": 0.28888888888888886,
                    "acc_norm_stderr": 0.03915450630414251,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.23684210526315788,
                    "acc_stderr": 0.034597776068105365,
                    "acc_norm": 0.23684210526315788,
                    "acc_norm_stderr": 0.034597776068105365,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536934,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.038612291966536934,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.30566037735849055,
                    "acc_stderr": 0.028353298073322663,
                    "acc_norm": 0.30566037735849055,
                    "acc_norm_stderr": 0.028353298073322663,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.03621034121889507,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.03621034121889507,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.17,
                    "acc_stderr": 0.03775251680686371,
                    "acc_norm": 0.17,
                    "acc_norm_stderr": 0.03775251680686371,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.0479372485441102,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.0479372485441102,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768078,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768078,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.27167630057803466,
                    "acc_stderr": 0.033917503223216586,
                    "acc_norm": 0.27167630057803466,
                    "acc_norm_stderr": 0.033917503223216586,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.19607843137254902,
                    "acc_stderr": 0.03950581861179961,
                    "acc_norm": 0.19607843137254902,
                    "acc_norm_stderr": 0.03950581861179961,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.04902071300001974,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.04902071300001974,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3148936170212766,
                    "acc_stderr": 0.030363582197238174,
                    "acc_norm": 0.3148936170212766,
                    "acc_norm_stderr": 0.030363582197238174,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.30701754385964913,
                    "acc_stderr": 0.0433913832257986,
                    "acc_norm": 0.30701754385964913,
                    "acc_norm_stderr": 0.0433913832257986,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2482758620689655,
                    "acc_stderr": 0.03600105692727771,
                    "acc_norm": 0.2482758620689655,
                    "acc_norm_stderr": 0.03600105692727771,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2804232804232804,
                    "acc_stderr": 0.02313528797432563,
                    "acc_norm": 0.2804232804232804,
                    "acc_norm_stderr": 0.02313528797432563,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.037184890068181146,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.037184890068181146,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.04824181513244218,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.04824181513244218,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.3032258064516129,
                    "acc_stderr": 0.026148685930671753,
                    "acc_norm": 0.3032258064516129,
                    "acc_norm_stderr": 0.026148685930671753,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.32019704433497537,
                    "acc_stderr": 0.032826493853041504,
                    "acc_norm": 0.32019704433497537,
                    "acc_norm_stderr": 0.032826493853041504,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.047609522856952344,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.047609522856952344,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.2787878787878788,
                    "acc_stderr": 0.03501438706296781,
                    "acc_norm": 0.2787878787878788,
                    "acc_norm_stderr": 0.03501438706296781,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.26262626262626265,
                    "acc_stderr": 0.03135305009533086,
                    "acc_norm": 0.26262626262626265,
                    "acc_norm_stderr": 0.03135305009533086,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.2849740932642487,
                    "acc_stderr": 0.03257714077709662,
                    "acc_norm": 0.2849740932642487,
                    "acc_norm_stderr": 0.03257714077709662,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2717948717948718,
                    "acc_stderr": 0.022556551010132354,
                    "acc_norm": 0.2717948717948718,
                    "acc_norm_stderr": 0.022556551010132354,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.02671924078371218,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.02671924078371218,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2689075630252101,
                    "acc_stderr": 0.028801392193631273,
                    "acc_norm": 0.2689075630252101,
                    "acc_norm_stderr": 0.028801392193631273,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2847682119205298,
                    "acc_stderr": 0.03684881521389023,
                    "acc_norm": 0.2847682119205298,
                    "acc_norm_stderr": 0.03684881521389023,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.30091743119266057,
                    "acc_stderr": 0.019664751366802114,
                    "acc_norm": 0.30091743119266057,
                    "acc_norm_stderr": 0.019664751366802114,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.18981481481481483,
                    "acc_stderr": 0.026744714834691943,
                    "acc_norm": 0.18981481481481483,
                    "acc_norm_stderr": 0.026744714834691943,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.27450980392156865,
                    "acc_stderr": 0.031321798030832904,
                    "acc_norm": 0.27450980392156865,
                    "acc_norm_stderr": 0.031321798030832904,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.3037974683544304,
                    "acc_stderr": 0.0299366963871386,
                    "acc_norm": 0.3037974683544304,
                    "acc_norm_stderr": 0.0299366963871386,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.4125560538116592,
                    "acc_stderr": 0.03304062175449297,
                    "acc_norm": 0.4125560538116592,
                    "acc_norm_stderr": 0.03304062175449297,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.25190839694656486,
                    "acc_stderr": 0.03807387116306086,
                    "acc_norm": 0.25190839694656486,
                    "acc_norm_stderr": 0.03807387116306086,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.3305785123966942,
                    "acc_stderr": 0.04294340845212094,
                    "acc_norm": 0.3305785123966942,
                    "acc_norm_stderr": 0.04294340845212094,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.32407407407407407,
                    "acc_stderr": 0.04524596007030048,
                    "acc_norm": 0.32407407407407407,
                    "acc_norm_stderr": 0.04524596007030048,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.27607361963190186,
                    "acc_stderr": 0.03512385283705051,
                    "acc_norm": 0.27607361963190186,
                    "acc_norm_stderr": 0.03512385283705051,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.3482142857142857,
                    "acc_stderr": 0.04521829902833585,
                    "acc_norm": 0.3482142857142857,
                    "acc_norm_stderr": 0.04521829902833585,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.3106796116504854,
                    "acc_stderr": 0.04582124160161552,
                    "acc_norm": 0.3106796116504854,
                    "acc_norm_stderr": 0.04582124160161552,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.32905982905982906,
                    "acc_stderr": 0.030782321577688166,
                    "acc_norm": 0.32905982905982906,
                    "acc_norm_stderr": 0.030782321577688166,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.047937248544110196,
                    "acc_norm": 0.35,
                    "acc_norm_stderr": 0.047937248544110196,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.367816091954023,
                    "acc_stderr": 0.017243828891846273,
                    "acc_norm": 0.367816091954023,
                    "acc_norm_stderr": 0.017243828891846273,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.33236994219653176,
                    "acc_stderr": 0.025361168749688235,
                    "acc_norm": 0.33236994219653176,
                    "acc_norm_stderr": 0.025361168749688235,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.26256983240223464,
                    "acc_stderr": 0.014716824273017768,
                    "acc_norm": 0.26256983240223464,
                    "acc_norm_stderr": 0.014716824273017768,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.3366013071895425,
                    "acc_stderr": 0.027057974624494382,
                    "acc_norm": 0.3366013071895425,
                    "acc_norm_stderr": 0.027057974624494382,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.31511254019292606,
                    "acc_stderr": 0.02638527370346447,
                    "acc_norm": 0.31511254019292606,
                    "acc_norm_stderr": 0.02638527370346447,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.3055555555555556,
                    "acc_stderr": 0.025630824975621344,
                    "acc_norm": 0.3055555555555556,
                    "acc_norm_stderr": 0.025630824975621344,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.25886524822695034,
                    "acc_stderr": 0.026129572527180848,
                    "acc_norm": 0.25886524822695034,
                    "acc_norm_stderr": 0.026129572527180848,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2607561929595828,
                    "acc_stderr": 0.01121347155960232,
                    "acc_norm": 0.2607561929595828,
                    "acc_norm_stderr": 0.01121347155960232,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.1875,
                    "acc_stderr": 0.023709788253811766,
                    "acc_norm": 0.1875,
                    "acc_norm_stderr": 0.023709788253811766,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.25980392156862747,
                    "acc_stderr": 0.017740899509177795,
                    "acc_norm": 0.25980392156862747,
                    "acc_norm_stderr": 0.017740899509177795,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2818181818181818,
                    "acc_stderr": 0.04309118709946458,
                    "acc_norm": 0.2818181818181818,
                    "acc_norm_stderr": 0.04309118709946458,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.20408163265306123,
                    "acc_stderr": 0.02580128347509051,
                    "acc_norm": 0.20408163265306123,
                    "acc_norm_stderr": 0.02580128347509051,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.30845771144278605,
                    "acc_stderr": 0.03265819588512697,
                    "acc_norm": 0.30845771144278605,
                    "acc_norm_stderr": 0.03265819588512697,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.43,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.43,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3433734939759036,
                    "acc_stderr": 0.03696584317010601,
                    "acc_norm": 0.3433734939759036,
                    "acc_norm_stderr": 0.03696584317010601,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.3684210526315789,
                    "acc_stderr": 0.036996580176568775,
                    "acc_norm": 0.3684210526315789,
                    "acc_norm_stderr": 0.036996580176568775,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.25703794369645044,
                    "mc1_stderr": 0.01529807750948508,
                    "mc2": 0.3778933788894488,
                    "mc2_stderr": 0.014398186673209289,
                    "timestamp": "2024-02-21T02-28-16.670940"
                }
            },
            "winogrande": {
                "acc": 0.7016574585635359,
                "acc_stderr": 0.012858885010030425,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "gsm8k": {
                "exact_match": 0.07808946171341925,
                "exact_match_stderr": 0.007390654481108223,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "minerva_math_precalc": {
                "exact_match": 0.007326007326007326,
                "exact_match_stderr": 0.0036529080893830325,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.02640642939150402,
                "exact_match_stderr": 0.005436057762573987,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.012962962962962963,
                "exact_match_stderr": 0.004872192984581494,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.013289036544850499,
                "exact_match_stderr": 0.0038127511080199464,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "minerva_math_geometry": {
                "exact_match": 0.020876826722338204,
                "exact_match_stderr": 0.006539385795813939,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0189873417721519,
                "exact_match_stderr": 0.006275362513989589,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "minerva_math_algebra": {
                "exact_match": 0.018534119629317607,
                "exact_match_stderr": 0.0039163476763639645,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_3da": {
                "acc": 0.202,
                "acc_stderr": 0.008979884139540944,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_3ds": {
                "acc": 0.3245,
                "acc_stderr": 0.010471614123485492,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_4da": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_2ds": {
                "acc": 0.4725,
                "acc_stderr": 0.011166208716863541,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_5ds": {
                "acc": 0.035,
                "acc_stderr": 0.004110468096699785,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_5da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000087,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_1dc": {
                "acc": 0.091,
                "acc_stderr": 0.006432743590028116,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_4ds": {
                "acc": 0.084,
                "acc_stderr": 0.006204131335071231,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_2dm": {
                "acc": 0.2645,
                "acc_stderr": 0.00986501567495644,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "arithmetic_2da": {
                "acc": 0.7235,
                "acc_stderr": 0.010003694915178819,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "gsm8k_cot": {
                "exact_match": 0.09628506444275967,
                "exact_match_stderr": 0.00812526412821589,
                "timestamp": "2024-06-07T03-40-08.964049"
            },
            "anli_r2": {
                "brier_score": 0.9413381511880419,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "anli_r3": {
                "brier_score": 0.87237900881463,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "anli_r1": {
                "brier_score": 0.9857934321426033,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_eu": {
                "brier_score": 1.026246542896589,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_vi": {
                "brier_score": 1.0279853059907973,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_ru": {
                "brier_score": 0.8097107614311067,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_zh": {
                "brier_score": 1.0156490749806204,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_tr": {
                "brier_score": 0.9756836472864571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_fr": {
                "brier_score": 0.7603981109729621,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_en": {
                "brier_score": 0.662787162600886,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_ur": {
                "brier_score": 1.31954291668707,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_ar": {
                "brier_score": 1.2990240082694595,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_de": {
                "brier_score": 0.849940839182899,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_hi": {
                "brier_score": 1.1637487081246372,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_es": {
                "brier_score": 0.8194931547365195,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_bg": {
                "brier_score": 0.9860919344218411,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_sw": {
                "brier_score": 1.082079686536352,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_el": {
                "brier_score": 0.8625720197689004,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "xnli_th": {
                "brier_score": 0.9548472798777402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "logiqa2": {
                "brier_score": 1.05514711694456,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "mathqa": {
                "brier_score": 0.9309727513874027,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T03-51-22.620745"
            },
            "lambada_standard": {
                "perplexity": 4.1704788868142115,
                "perplexity_stderr": 0.09171832314219872,
                "acc": 0.6625266834853484,
                "acc_stderr": 0.006587694938528704,
                "timestamp": "2024-06-07T03-52-44.995548"
            },
            "lambada_openai": {
                "perplexity": 3.3182500746294745,
                "perplexity_stderr": 0.06880896651957533,
                "acc": 0.7343295167863381,
                "acc_stderr": 0.006153599372822545,
                "timestamp": "2024-06-07T03-52-44.995548"
            },
            "mmlu_world_religions": {
                "acc": 0.36257309941520466,
                "acc_stderr": 0.0368713061556206,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_formal_logic": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.03718489006818114,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_prehistory": {
                "acc": 0.30246913580246915,
                "acc_stderr": 0.025557653981868052,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.26033519553072626,
                "acc_stderr": 0.014676252009319471,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.3037974683544304,
                "acc_stderr": 0.02993669638713861,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_moral_disputes": {
                "acc": 0.32947976878612717,
                "acc_stderr": 0.025305258131879727,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_professional_law": {
                "acc": 0.2633637548891786,
                "acc_stderr": 0.011249506403605287,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.27607361963190186,
                "acc_stderr": 0.03512385283705051,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.27941176470588236,
                "acc_stderr": 0.031493281045079556,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_philosophy": {
                "acc": 0.3086816720257235,
                "acc_stderr": 0.02623696588115326,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_jurisprudence": {
                "acc": 0.35185185185185186,
                "acc_stderr": 0.04616631111801715,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_international_law": {
                "acc": 0.3305785123966942,
                "acc_stderr": 0.04294340845212094,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.2909090909090909,
                "acc_stderr": 0.03546563019624335,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.27979274611398963,
                "acc_stderr": 0.032396370467357015,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.028657491285071963,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_geography": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.03191178226713547,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.30642201834862387,
                "acc_stderr": 0.019765517220458523,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_public_relations": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.04265792110940588,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.42,
                "acc_stderr": 0.049604496374885836,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_sociology": {
                "acc": 0.3034825870646766,
                "acc_stderr": 0.032510068164586195,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2743589743589744,
                "acc_stderr": 0.022622765767493214,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_security_studies": {
                "acc": 0.20816326530612245,
                "acc_stderr": 0.025991117672813296,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2581699346405229,
                "acc_stderr": 0.017704531653250064,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_econometrics": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.04372748290278008,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_miscellaneous": {
                "acc": 0.3652618135376756,
                "acc_stderr": 0.01721853002883864,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_marketing": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.030882736974138656,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_management": {
                "acc": 0.33980582524271846,
                "acc_stderr": 0.04689765937278135,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_nutrition": {
                "acc": 0.3300653594771242,
                "acc_stderr": 0.026925654653615693,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_medical_genetics": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_human_aging": {
                "acc": 0.4170403587443946,
                "acc_stderr": 0.03309266936071721,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_professional_medicine": {
                "acc": 0.1948529411764706,
                "acc_stderr": 0.024060599423487414,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_college_medicine": {
                "acc": 0.27167630057803466,
                "acc_stderr": 0.03391750322321659,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_business_ethics": {
                "acc": 0.2,
                "acc_stderr": 0.040201512610368466,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.30566037735849055,
                "acc_stderr": 0.028353298073322666,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_global_facts": {
                "acc": 0.37,
                "acc_stderr": 0.04852365870939099,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_virology": {
                "acc": 0.3313253012048193,
                "acc_stderr": 0.03664314777288086,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_professional_accounting": {
                "acc": 0.25177304964539005,
                "acc_stderr": 0.025892151156709405,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_college_physics": {
                "acc": 0.19607843137254902,
                "acc_stderr": 0.03950581861179963,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2847682119205298,
                "acc_stderr": 0.03684881521389023,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_biology": {
                "acc": 0.3096774193548387,
                "acc_stderr": 0.026302774983517414,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_college_biology": {
                "acc": 0.2569444444444444,
                "acc_stderr": 0.03653946969442099,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_anatomy": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.03944624162501116,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_college_chemistry": {
                "acc": 0.17,
                "acc_stderr": 0.037752516806863715,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_computer_security": {
                "acc": 0.36,
                "acc_stderr": 0.048241815132442176,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_college_computer_science": {
                "acc": 0.35,
                "acc_stderr": 0.047937248544110196,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_astronomy": {
                "acc": 0.23026315789473684,
                "acc_stderr": 0.034260594244031654,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_college_mathematics": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.31063829787234043,
                "acc_stderr": 0.03025123757921317,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252604,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_machine_learning": {
                "acc": 0.3482142857142857,
                "acc_stderr": 0.045218299028335865,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.32019704433497537,
                "acc_stderr": 0.032826493853041504,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.18518518518518517,
                "acc_stderr": 0.026491914727355143,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.023068188848261107,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2620689655172414,
                "acc_stderr": 0.036646663372252565,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.027309140588230186,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "arc_challenge": {
                "acc": 0.4453924914675768,
                "acc_stderr": 0.01452398763834407,
                "acc_norm": 0.4735494880546075,
                "acc_norm_stderr": 0.014590931358120165,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "truthfulqa_mc2": {
                "acc": 0.37834578161036186,
                "acc_stderr": 0.01440719551886384,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "truthfulqa_gen": {
                "bleu_max": 25.491727663339983,
                "bleu_max_stderr": 0.7876825292141532,
                "bleu_acc": 0.2974296205630355,
                "bleu_acc_stderr": 0.016002651487360974,
                "bleu_diff": -7.301689206799327,
                "bleu_diff_stderr": 0.7522941543496632,
                "rouge1_max": 51.09004512811695,
                "rouge1_max_stderr": 0.8130980891432411,
                "rouge1_acc": 0.3011015911872705,
                "rouge1_acc_stderr": 0.01605899902610058,
                "rouge1_diff": -9.608532182458044,
                "rouge1_diff_stderr": 0.7813130202579182,
                "rouge2_max": 35.83529943497114,
                "rouge2_max_stderr": 0.9345260348229253,
                "rouge2_acc": 0.2692778457772338,
                "rouge2_acc_stderr": 0.015528566637087253,
                "rouge2_diff": -10.914652626830508,
                "rouge2_diff_stderr": 0.943625020821263,
                "rougeL_max": 47.82154692891093,
                "rougeL_max_stderr": 0.8290897847436749,
                "rougeL_acc": 0.3023255813953488,
                "rougeL_acc_stderr": 0.016077509266133033,
                "rougeL_diff": -9.662039631078054,
                "rougeL_diff_stderr": 0.7901057890971249,
                "timestamp": "2024-11-20T05-38-02.637168"
            },
            "truthfulqa_mc1": {
                "acc": 0.2594859241126071,
                "acc_stderr": 0.015345409485557996,
                "timestamp": "2024-11-20T05-38-02.637168"
            }
        }
    }
}