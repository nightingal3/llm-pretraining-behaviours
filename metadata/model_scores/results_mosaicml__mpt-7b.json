{
    "model_name": "mosaicml/mpt-7b",
    "last_updated": "2024-12-04 11:24:06.088694",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.029304029304029304,
                "exact_match_stderr": 0.007224487305459692,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.06659012629161883,
                "exact_match_stderr": 0.008452428160416104,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.040740740740740744,
                "exact_match_stderr": 0.008515067163720174,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.02547065337763012,
                "exact_match_stderr": 0.005245830272559335,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_geometry": {
                "exact_match": 0.033402922755741124,
                "exact_match_stderr": 0.008218660203335972,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.046413502109704644,
                "exact_match_stderr": 0.009673232932861578,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_algebra": {
                "exact_match": 0.03622577927548441,
                "exact_match_stderr": 0.005425680006601679,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_3da": {
                "acc": 0.0405,
                "acc_stderr": 0.004409035585862131,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_3ds": {
                "acc": 0.017,
                "acc_stderr": 0.0028913110935905586,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_4da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521504,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_2ds": {
                "acc": 0.217,
                "acc_stderr": 0.00921943593716572,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_5da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339509,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_1dc": {
                "acc": 0.017,
                "acc_stderr": 0.0028913110935905443,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.000500000000000013,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_2dm": {
                "acc": 0.0485,
                "acc_stderr": 0.004804728682127105,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_2da": {
                "acc": 0.199,
                "acc_stderr": 0.008929690346526223,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "gsm8k_cot": {
                "exact_match": 0.08567096285064443,
                "exact_match_stderr": 0.007709218855882752,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "gsm8k": {
                "exact_match": 0.060652009097801364,
                "exact_match_stderr": 0.006574733381405809,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "anli_r2": {
                "brier_score": 0.7871039264444815,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "anli_r3": {
                "brier_score": 0.7735706458763796,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "anli_r1": {
                "brier_score": 0.8064532070514647,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_eu": {
                "brier_score": 0.9836769306459834,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_vi": {
                "brier_score": 0.9706863969461264,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_ru": {
                "brier_score": 0.7263265977856224,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_zh": {
                "brier_score": 0.9844979242445991,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_tr": {
                "brier_score": 0.9073384888674126,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_fr": {
                "brier_score": 0.7455615780108278,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_en": {
                "brier_score": 0.6521561020686549,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_ur": {
                "brier_score": 1.3114352706806065,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_ar": {
                "brier_score": 1.1879643155024167,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_de": {
                "brier_score": 0.8140312095660267,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_hi": {
                "brier_score": 0.9094996182021011,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_es": {
                "brier_score": 0.9037442585372435,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_bg": {
                "brier_score": 0.825572748604095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_sw": {
                "brier_score": 1.1722319061632498,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_el": {
                "brier_score": 1.0056819587703962,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_th": {
                "brier_score": 0.8582353033497166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "logiqa2": {
                "brier_score": 0.9764895261294206,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "mathqa": {
                "brier_score": 0.9492149034082438,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "lambada_standard": {
                "perplexity": 4.924753971769574,
                "perplexity_stderr": 0.10796399230777208,
                "acc": 0.6194449835047545,
                "acc_stderr": 0.006764289222028883,
                "timestamp": "2024-06-09T16-58-32.555159"
            },
            "lambada_openai": {
                "perplexity": 3.8685392219566808,
                "perplexity_stderr": 0.08095253429483709,
                "acc": 0.685231903745391,
                "acc_stderr": 0.006470326766225591,
                "timestamp": "2024-06-09T16-58-32.555159"
            },
            "mmlu_world_religions": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.03615507630310937,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_formal_logic": {
                "acc": 0.19047619047619047,
                "acc_stderr": 0.03512207412302053,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_prehistory": {
                "acc": 0.3117283950617284,
                "acc_stderr": 0.02577311116963046,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.25139664804469275,
                "acc_stderr": 0.014508979453553991,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.270042194092827,
                "acc_stderr": 0.028900721906293426,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2861271676300578,
                "acc_stderr": 0.024332146779134128,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_professional_law": {
                "acc": 0.2633637548891786,
                "acc_stderr": 0.011249506403605284,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2331288343558282,
                "acc_stderr": 0.03322015795776741,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2696078431372549,
                "acc_stderr": 0.03114557065948678,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_philosophy": {
                "acc": 0.3086816720257235,
                "acc_stderr": 0.026236965881153266,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_jurisprudence": {
                "acc": 0.3425925925925926,
                "acc_stderr": 0.045879047413018105,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_international_law": {
                "acc": 0.23140495867768596,
                "acc_stderr": 0.03849856098794089,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.24242424242424243,
                "acc_stderr": 0.03346409881055953,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.31088082901554404,
                "acc_stderr": 0.03340361906276588,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.2689075630252101,
                "acc_stderr": 0.02880139219363127,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_geography": {
                "acc": 0.22727272727272727,
                "acc_stderr": 0.02985751567338641,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.26238532110091745,
                "acc_stderr": 0.018861885021534745,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_public_relations": {
                "acc": 0.3090909090909091,
                "acc_stderr": 0.044262946482000985,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.46,
                "acc_stderr": 0.05009082659620333,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_sociology": {
                "acc": 0.20398009950248755,
                "acc_stderr": 0.02849317624532607,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2743589743589744,
                "acc_stderr": 0.0226227657674932,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_security_studies": {
                "acc": 0.3224489795918367,
                "acc_stderr": 0.029923100563683906,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2679738562091503,
                "acc_stderr": 0.017917974069594726,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_human_sexuality": {
                "acc": 0.3053435114503817,
                "acc_stderr": 0.040393149787245605,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_econometrics": {
                "acc": 0.2894736842105263,
                "acc_stderr": 0.04266339443159394,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_miscellaneous": {
                "acc": 0.29757343550446996,
                "acc_stderr": 0.01634911191290942,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_marketing": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.030882736974138653,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_management": {
                "acc": 0.2621359223300971,
                "acc_stderr": 0.04354631077260595,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_nutrition": {
                "acc": 0.27124183006535946,
                "acc_stderr": 0.025457756696667878,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_medical_genetics": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_human_aging": {
                "acc": 0.3542600896860987,
                "acc_stderr": 0.032100621541349864,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_professional_medicine": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.024562204314142314,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_college_medicine": {
                "acc": 0.24277456647398843,
                "acc_stderr": 0.0326926380614177,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_business_ethics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.25660377358490566,
                "acc_stderr": 0.02688064788905199,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_global_facts": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_virology": {
                "acc": 0.3132530120481928,
                "acc_stderr": 0.036108050180310235,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_professional_accounting": {
                "acc": 0.25177304964539005,
                "acc_stderr": 0.0258921511567094,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_college_physics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.04023382273617749,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_physics": {
                "acc": 0.25165562913907286,
                "acc_stderr": 0.03543304234389985,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_biology": {
                "acc": 0.25806451612903225,
                "acc_stderr": 0.02489246917246283,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_college_biology": {
                "acc": 0.2986111111111111,
                "acc_stderr": 0.03827052357950756,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_anatomy": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.03785714465066654,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_college_chemistry": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_computer_security": {
                "acc": 0.27,
                "acc_stderr": 0.044619604333847394,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_college_computer_science": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_astronomy": {
                "acc": 0.26973684210526316,
                "acc_stderr": 0.036117805602848975,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_college_mathematics": {
                "acc": 0.35,
                "acc_stderr": 0.047937248544110196,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.32340425531914896,
                "acc_stderr": 0.03057944277361034,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.19,
                "acc_stderr": 0.039427724440366234,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165044,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_machine_learning": {
                "acc": 0.35714285714285715,
                "acc_stderr": 0.04547960999764376,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.16748768472906403,
                "acc_stderr": 0.026273086047535425,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.27314814814814814,
                "acc_stderr": 0.03038805130167812,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.24867724867724866,
                "acc_stderr": 0.022261817692400158,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.3448275862068966,
                "acc_stderr": 0.03960933549451208,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.29259259259259257,
                "acc_stderr": 0.027738969632176088,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "arc_challenge": {
                "acc": 0.4351535836177474,
                "acc_stderr": 0.014487986197186047,
                "acc_norm": 0.47013651877133106,
                "acc_norm_stderr": 0.014585305840007105,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "hellaswag": {
                "acc": 0.5730930093606851,
                "acc_stderr": 0.004936176784631944,
                "acc_norm": 0.7791276638119896,
                "acc_norm_stderr": 0.004139867975116252,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "truthfulqa_mc2": {
                "acc": 0.33431576358453946,
                "acc_stderr": 0.013089145381228533,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.602330933425307,
                "bleu_max_stderr": 0.7657159724776746,
                "bleu_acc": 0.3072215422276622,
                "bleu_acc_stderr": 0.016150201321323044,
                "bleu_diff": -9.827181119106774,
                "bleu_diff_stderr": 0.8506396518002634,
                "rouge1_max": 48.4307380348122,
                "rouge1_max_stderr": 0.9034301197408506,
                "rouge1_acc": 0.2729498164014688,
                "rouge1_acc_stderr": 0.01559475363200655,
                "rouge1_diff": -12.051963649063818,
                "rouge1_diff_stderr": 0.9400681010531302,
                "rouge2_max": 32.229261537796205,
                "rouge2_max_stderr": 1.018069487091937,
                "rouge2_acc": 0.20930232558139536,
                "rouge2_acc_stderr": 0.01424121943478583,
                "rouge2_diff": -14.473968970783242,
                "rouge2_diff_stderr": 1.0917907802458022,
                "rougeL_max": 45.768993552153226,
                "rougeL_max_stderr": 0.9070545874177821,
                "rougeL_acc": 0.26805385556915545,
                "rougeL_acc_stderr": 0.015506204722834555,
                "rougeL_diff": -12.622302003402247,
                "rougeL_diff_stderr": 0.9501788414558511,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "truthfulqa_mc1": {
                "acc": 0.204406364749082,
                "acc_stderr": 0.014117174337432616,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "winogrande": {
                "acc": 0.7253354380426204,
                "acc_stderr": 0.012544516005117193,
                "timestamp": "2024-11-19T21-35-21.939819"
            }
        }
    }
}