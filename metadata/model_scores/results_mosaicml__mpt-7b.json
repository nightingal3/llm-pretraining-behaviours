{
    "model_name": "mosaicml/mpt-7b",
    "last_updated": "2024-12-19 13:39:32.490198",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.029304029304029304,
                "exact_match_stderr": 0.007224487305459692,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.06659012629161883,
                "exact_match_stderr": 0.008452428160416104,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.040740740740740744,
                "exact_match_stderr": 0.008515067163720174,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.02547065337763012,
                "exact_match_stderr": 0.005245830272559335,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_geometry": {
                "exact_match": 0.033402922755741124,
                "exact_match_stderr": 0.008218660203335972,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.046413502109704644,
                "exact_match_stderr": 0.009673232932861578,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "minerva_math_algebra": {
                "exact_match": 0.03622577927548441,
                "exact_match_stderr": 0.005425680006601679,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_3da": {
                "acc": 0.0405,
                "acc_stderr": 0.004409035585862131,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_3ds": {
                "acc": 0.017,
                "acc_stderr": 0.0028913110935905586,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_4da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521504,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_2ds": {
                "acc": 0.217,
                "acc_stderr": 0.00921943593716572,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_5da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339509,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_1dc": {
                "acc": 0.017,
                "acc_stderr": 0.0028913110935905443,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_4ds": {
                "acc": 0.0005,
                "acc_stderr": 0.000500000000000013,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_2dm": {
                "acc": 0.0485,
                "acc_stderr": 0.004804728682127105,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "arithmetic_2da": {
                "acc": 0.199,
                "acc_stderr": 0.008929690346526223,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "gsm8k_cot": {
                "exact_match": 0.08567096285064443,
                "exact_match_stderr": 0.007709218855882752,
                "timestamp": "2024-06-09T16-47-21.073014"
            },
            "gsm8k": {
                "exact_match": 0.060652009097801364,
                "exact_match_stderr": 0.006574733381405809,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "anli_r2": {
                "brier_score": 0.7871039264444815,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "anli_r3": {
                "brier_score": 0.7735706458763796,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "anli_r1": {
                "brier_score": 0.8064532070514647,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_eu": {
                "brier_score": 0.9836769306459834,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_vi": {
                "brier_score": 0.9706863969461264,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_ru": {
                "brier_score": 0.7263265977856224,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_zh": {
                "brier_score": 0.9844979242445991,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_tr": {
                "brier_score": 0.9073384888674126,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_fr": {
                "brier_score": 0.7455615780108278,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_en": {
                "brier_score": 0.6521561020686549,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_ur": {
                "brier_score": 1.3114352706806065,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_ar": {
                "brier_score": 1.1879643155024167,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_de": {
                "brier_score": 0.8140312095660267,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_hi": {
                "brier_score": 0.9094996182021011,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_es": {
                "brier_score": 0.9037442585372435,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_bg": {
                "brier_score": 0.825572748604095,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_sw": {
                "brier_score": 1.1722319061632498,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_el": {
                "brier_score": 1.0056819587703962,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "xnli_th": {
                "brier_score": 0.8582353033497166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "logiqa2": {
                "brier_score": 0.9764895261294206,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "mathqa": {
                "brier_score": 0.9492149034082438,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T16-57-08.536020"
            },
            "lambada_standard": {
                "perplexity": 4.924753971769574,
                "perplexity_stderr": 0.10796399230777208,
                "acc": 0.6194449835047545,
                "acc_stderr": 0.006764289222028883,
                "timestamp": "2024-06-09T16-58-32.555159"
            },
            "lambada_openai": {
                "perplexity": 3.8685392219566808,
                "perplexity_stderr": 0.08095253429483709,
                "acc": 0.685231903745391,
                "acc_stderr": 0.006470326766225591,
                "timestamp": "2024-06-09T16-58-32.555159"
            },
            "mmlu_world_religions": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.03565079670708311,
                "brier_score": 0.728741684015239,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_formal_logic": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.04006168083848879,
                "brier_score": 0.7523749874783903,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_prehistory": {
                "acc": 0.3148148148148148,
                "acc_stderr": 0.025842248700902168,
                "brier_score": 0.7309064722060028,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23687150837988827,
                "acc_stderr": 0.014219570788103982,
                "brier_score": 0.7937717222558787,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.3080168776371308,
                "acc_stderr": 0.030052389335605702,
                "brier_score": 0.7382162992563585,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_moral_disputes": {
                "acc": 0.3092485549132948,
                "acc_stderr": 0.024883140570071755,
                "brier_score": 0.7411018298260529,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_professional_law": {
                "acc": 0.2666232073011734,
                "acc_stderr": 0.011293836031612143,
                "brier_score": 0.7487617393696027,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.26993865030674846,
                "acc_stderr": 0.03487825168497892,
                "brier_score": 0.7464647521267894,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.29411764705882354,
                "acc_stderr": 0.031980016601150706,
                "brier_score": 0.7394602455358426,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_philosophy": {
                "acc": 0.3086816720257235,
                "acc_stderr": 0.026236965881153266,
                "brier_score": 0.7320864499183289,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_jurisprudence": {
                "acc": 0.3611111111111111,
                "acc_stderr": 0.04643454608906275,
                "brier_score": 0.7192370502154816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_international_law": {
                "acc": 0.4462809917355372,
                "acc_stderr": 0.0453793517794788,
                "brier_score": 0.7043917930738771,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.3090909090909091,
                "acc_stderr": 0.03608541011573967,
                "brier_score": 0.7349772415005317,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.3160621761658031,
                "acc_stderr": 0.033553973696861736,
                "brier_score": 0.7290051141753543,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.3319327731092437,
                "acc_stderr": 0.030588697013783663,
                "brier_score": 0.7427459476388498,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_geography": {
                "acc": 0.2828282828282828,
                "acc_stderr": 0.03208779558786751,
                "brier_score": 0.7517428390896269,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.27339449541284405,
                "acc_stderr": 0.01910929984609828,
                "brier_score": 0.7492081397364514,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_public_relations": {
                "acc": 0.3181818181818182,
                "acc_stderr": 0.04461272175910507,
                "brier_score": 0.7322864162960925,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7209992823667533,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_sociology": {
                "acc": 0.3283582089552239,
                "acc_stderr": 0.033206858897443244,
                "brier_score": 0.7210230646478174,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24871794871794872,
                "acc_stderr": 0.0219169577092138,
                "brier_score": 0.7482215522041382,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_security_studies": {
                "acc": 0.2653061224489796,
                "acc_stderr": 0.028263889943784606,
                "brier_score": 0.7401749633097108,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_professional_psychology": {
                "acc": 0.3202614379084967,
                "acc_stderr": 0.018875682938069443,
                "brier_score": 0.7367828947482863,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_human_sexuality": {
                "acc": 0.31297709923664124,
                "acc_stderr": 0.04066962905677698,
                "brier_score": 0.7360726453126495,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_econometrics": {
                "acc": 0.2543859649122807,
                "acc_stderr": 0.040969851398436716,
                "brier_score": 0.7811233293785654,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_miscellaneous": {
                "acc": 0.37037037037037035,
                "acc_stderr": 0.017268607560005776,
                "brier_score": 0.7089064656141576,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_marketing": {
                "acc": 0.29914529914529914,
                "acc_stderr": 0.029996951858349483,
                "brier_score": 0.7226735305047545,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_management": {
                "acc": 0.32038834951456313,
                "acc_stderr": 0.04620284082280039,
                "brier_score": 0.7464823125999717,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_nutrition": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.02495418432487991,
                "brier_score": 0.748069712172246,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_medical_genetics": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "brier_score": 0.7375088711867108,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_human_aging": {
                "acc": 0.39461883408071746,
                "acc_stderr": 0.03280400504755291,
                "brier_score": 0.7173562027693366,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_professional_medicine": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.025767252010855956,
                "brier_score": 0.7525455665900059,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_college_medicine": {
                "acc": 0.27167630057803466,
                "acc_stderr": 0.03391750322321659,
                "brier_score": 0.7542974573066814,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_business_ethics": {
                "acc": 0.4,
                "acc_stderr": 0.04923659639173309,
                "brier_score": 0.7250757977146242,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.32452830188679244,
                "acc_stderr": 0.028815615713432115,
                "brier_score": 0.746111321222342,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_global_facts": {
                "acc": 0.39,
                "acc_stderr": 0.04902071300001974,
                "brier_score": 0.7102197513122844,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_virology": {
                "acc": 0.3192771084337349,
                "acc_stderr": 0.03629335329947859,
                "brier_score": 0.7342408644320478,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2907801418439716,
                "acc_stderr": 0.027090664368353178,
                "brier_score": 0.7492878539459001,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_college_physics": {
                "acc": 0.27450980392156865,
                "acc_stderr": 0.044405219061793254,
                "brier_score": 0.7787676832484953,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2052980132450331,
                "acc_stderr": 0.03297986648473835,
                "brier_score": 0.7554343222342074,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_biology": {
                "acc": 0.3,
                "acc_stderr": 0.026069362295335127,
                "brier_score": 0.7446064679583299,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_college_biology": {
                "acc": 0.2916666666666667,
                "acc_stderr": 0.038009680605548594,
                "brier_score": 0.7416430063536041,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_anatomy": {
                "acc": 0.28888888888888886,
                "acc_stderr": 0.03915450630414251,
                "brier_score": 0.7290725108490472,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_college_chemistry": {
                "acc": 0.21,
                "acc_stderr": 0.04093601807403326,
                "brier_score": 0.781694105501675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_computer_security": {
                "acc": 0.39,
                "acc_stderr": 0.04902071300001975,
                "brier_score": 0.7111190458395429,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_college_computer_science": {
                "acc": 0.15,
                "acc_stderr": 0.0358870281282637,
                "brier_score": 0.7767318512632042,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_astronomy": {
                "acc": 0.2565789473684211,
                "acc_stderr": 0.0355418036802569,
                "brier_score": 0.757864750388492,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_college_mathematics": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.7578289940819812,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.32340425531914896,
                "acc_stderr": 0.030579442773610344,
                "brier_score": 0.7403692614539054,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7513816282514579,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816506,
                "brier_score": 0.7392790076338863,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_machine_learning": {
                "acc": 0.29464285714285715,
                "acc_stderr": 0.0432704093257873,
                "brier_score": 0.74507395792578,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2512315270935961,
                "acc_stderr": 0.030516530732694436,
                "brier_score": 0.7485835684629456,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.21296296296296297,
                "acc_stderr": 0.027920963147993666,
                "brier_score": 0.7700312540477746,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2830687830687831,
                "acc_stderr": 0.023201392938194974,
                "brier_score": 0.7648506953710482,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2827586206896552,
                "acc_stderr": 0.03752833958003336,
                "brier_score": 0.7377471340632694,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.025348097468097852,
                "brier_score": 0.7762464169473738,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-49-04.168570"
            },
            "arc_challenge": {
                "acc": 0.4351535836177474,
                "acc_stderr": 0.014487986197186047,
                "acc_norm": 0.47013651877133106,
                "acc_norm_stderr": 0.014585305840007105,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "hellaswag": {
                "acc": 0.5730930093606851,
                "acc_stderr": 0.004936176784631944,
                "acc_norm": 0.7791276638119896,
                "acc_norm_stderr": 0.004139867975116252,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "truthfulqa_mc2": {
                "acc": 0.33431576358453946,
                "acc_stderr": 0.013089145381228533,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.602330933425307,
                "bleu_max_stderr": 0.7657159724776746,
                "bleu_acc": 0.3072215422276622,
                "bleu_acc_stderr": 0.016150201321323044,
                "bleu_diff": -9.827181119106774,
                "bleu_diff_stderr": 0.8506396518002634,
                "rouge1_max": 48.4307380348122,
                "rouge1_max_stderr": 0.9034301197408506,
                "rouge1_acc": 0.2729498164014688,
                "rouge1_acc_stderr": 0.01559475363200655,
                "rouge1_diff": -12.051963649063818,
                "rouge1_diff_stderr": 0.9400681010531302,
                "rouge2_max": 32.229261537796205,
                "rouge2_max_stderr": 1.018069487091937,
                "rouge2_acc": 0.20930232558139536,
                "rouge2_acc_stderr": 0.01424121943478583,
                "rouge2_diff": -14.473968970783242,
                "rouge2_diff_stderr": 1.0917907802458022,
                "rougeL_max": 45.768993552153226,
                "rougeL_max_stderr": 0.9070545874177821,
                "rougeL_acc": 0.26805385556915545,
                "rougeL_acc_stderr": 0.015506204722834555,
                "rougeL_diff": -12.622302003402247,
                "rougeL_diff_stderr": 0.9501788414558511,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "truthfulqa_mc1": {
                "acc": 0.204406364749082,
                "acc_stderr": 0.014117174337432616,
                "timestamp": "2024-11-19T21-35-21.939819"
            },
            "winogrande": {
                "acc": 0.7253354380426204,
                "acc_stderr": 0.012544516005117193,
                "timestamp": "2024-11-19T21-35-21.939819"
            }
        }
    }
}