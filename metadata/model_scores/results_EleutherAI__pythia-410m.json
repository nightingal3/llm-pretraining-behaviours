{
    "model_name": "EleutherAI/pythia-410m",
    "last_updated": "2023-11-13",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.23122866894197952,
                    "acc_stderr": 0.012320858834772283,
                    "acc_norm": 0.2619453924914676,
                    "acc_norm_stderr": 0.012849054826858115,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.33947420832503483,
                    "acc_stderr": 0.004725630911520322,
                    "acc_norm": 0.4084843656642103,
                    "acc_norm_stderr": 0.00490548949400508,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2814814814814815,
                    "acc_stderr": 0.03885004245800255,
                    "acc_norm": 0.2814814814814815,
                    "acc_norm_stderr": 0.03885004245800255,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.21710526315789475,
                    "acc_stderr": 0.03355045304882924,
                    "acc_norm": 0.21710526315789475,
                    "acc_norm_stderr": 0.03355045304882924,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2490566037735849,
                    "acc_stderr": 0.02661648298050171,
                    "acc_norm": 0.2490566037735849,
                    "acc_norm_stderr": 0.02661648298050171,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.03745554791462457,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.03745554791462457,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.42,
                    "acc_stderr": 0.049604496374885836,
                    "acc_norm": 0.42,
                    "acc_norm_stderr": 0.049604496374885836,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252604,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.047609522856952365,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.047609522856952365,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.20809248554913296,
                    "acc_stderr": 0.03095289021774988,
                    "acc_norm": 0.20809248554913296,
                    "acc_norm_stderr": 0.03095289021774988,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.24509803921568626,
                    "acc_stderr": 0.04280105837364395,
                    "acc_norm": 0.24509803921568626,
                    "acc_norm_stderr": 0.04280105837364395,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036843,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036843,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.28936170212765955,
                    "acc_stderr": 0.029644006577009618,
                    "acc_norm": 0.28936170212765955,
                    "acc_norm_stderr": 0.029644006577009618,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.22807017543859648,
                    "acc_stderr": 0.03947152782669415,
                    "acc_norm": 0.22807017543859648,
                    "acc_norm_stderr": 0.03947152782669415,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.22758620689655173,
                    "acc_stderr": 0.03493950380131183,
                    "acc_norm": 0.22758620689655173,
                    "acc_norm_stderr": 0.03493950380131183,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.23809523809523808,
                    "acc_stderr": 0.021935878081184756,
                    "acc_norm": 0.23809523809523808,
                    "acc_norm_stderr": 0.021935878081184756,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.29365079365079366,
                    "acc_stderr": 0.04073524322147125,
                    "acc_norm": 0.29365079365079366,
                    "acc_norm_stderr": 0.04073524322147125,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.17,
                    "acc_stderr": 0.03775251680686371,
                    "acc_norm": 0.17,
                    "acc_norm_stderr": 0.03775251680686371,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.29354838709677417,
                    "acc_stderr": 0.025906087021319288,
                    "acc_norm": 0.29354838709677417,
                    "acc_norm_stderr": 0.025906087021319288,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.26108374384236455,
                    "acc_stderr": 0.030903796952114475,
                    "acc_norm": 0.26108374384236455,
                    "acc_norm_stderr": 0.030903796952114475,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.23636363636363636,
                    "acc_stderr": 0.033175059300091805,
                    "acc_norm": 0.23636363636363636,
                    "acc_norm_stderr": 0.033175059300091805,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.30303030303030304,
                    "acc_stderr": 0.032742879140268674,
                    "acc_norm": 0.30303030303030304,
                    "acc_norm_stderr": 0.032742879140268674,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.23316062176165803,
                    "acc_stderr": 0.030516111371476008,
                    "acc_norm": 0.23316062176165803,
                    "acc_norm_stderr": 0.030516111371476008,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.358974358974359,
                    "acc_stderr": 0.024321738484602364,
                    "acc_norm": 0.358974358974359,
                    "acc_norm_stderr": 0.024321738484602364,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26296296296296295,
                    "acc_stderr": 0.026842057873833706,
                    "acc_norm": 0.26296296296296295,
                    "acc_norm_stderr": 0.026842057873833706,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.23949579831932774,
                    "acc_stderr": 0.027722065493361276,
                    "acc_norm": 0.23949579831932774,
                    "acc_norm_stderr": 0.027722065493361276,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2913907284768212,
                    "acc_stderr": 0.03710185726119995,
                    "acc_norm": 0.2913907284768212,
                    "acc_norm_stderr": 0.03710185726119995,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.326605504587156,
                    "acc_stderr": 0.020106990889937303,
                    "acc_norm": 0.326605504587156,
                    "acc_norm_stderr": 0.020106990889937303,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4722222222222222,
                    "acc_stderr": 0.0340470532865388,
                    "acc_norm": 0.4722222222222222,
                    "acc_norm_stderr": 0.0340470532865388,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.2696078431372549,
                    "acc_stderr": 0.031145570659486782,
                    "acc_norm": 0.2696078431372549,
                    "acc_norm_stderr": 0.031145570659486782,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.22362869198312235,
                    "acc_stderr": 0.027123298205229972,
                    "acc_norm": 0.22362869198312235,
                    "acc_norm_stderr": 0.027123298205229972,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.25112107623318386,
                    "acc_stderr": 0.029105220833224605,
                    "acc_norm": 0.25112107623318386,
                    "acc_norm_stderr": 0.029105220833224605,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.037276735755969195,
                    "acc_norm": 0.2366412213740458,
                    "acc_norm_stderr": 0.037276735755969195,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.4049586776859504,
                    "acc_stderr": 0.04481137755942469,
                    "acc_norm": 0.4049586776859504,
                    "acc_norm_stderr": 0.04481137755942469,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.21296296296296297,
                    "acc_stderr": 0.039578354719809805,
                    "acc_norm": 0.21296296296296297,
                    "acc_norm_stderr": 0.039578354719809805,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2822085889570552,
                    "acc_stderr": 0.03536117886664743,
                    "acc_norm": 0.2822085889570552,
                    "acc_norm_stderr": 0.03536117886664743,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.21428571428571427,
                    "acc_stderr": 0.03894641120044792,
                    "acc_norm": 0.21428571428571427,
                    "acc_norm_stderr": 0.03894641120044792,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2621359223300971,
                    "acc_stderr": 0.04354631077260594,
                    "acc_norm": 0.2621359223300971,
                    "acc_norm_stderr": 0.04354631077260594,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.20512820512820512,
                    "acc_stderr": 0.026453508054040335,
                    "acc_norm": 0.20512820512820512,
                    "acc_norm_stderr": 0.026453508054040335,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.25798212005108556,
                    "acc_stderr": 0.01564583018834895,
                    "acc_norm": 0.25798212005108556,
                    "acc_norm_stderr": 0.01564583018834895,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2543352601156069,
                    "acc_stderr": 0.02344582627654555,
                    "acc_norm": 0.2543352601156069,
                    "acc_norm_stderr": 0.02344582627654555,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2446927374301676,
                    "acc_stderr": 0.01437816988409845,
                    "acc_norm": 0.2446927374301676,
                    "acc_norm_stderr": 0.01437816988409845,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2549019607843137,
                    "acc_stderr": 0.024954184324879905,
                    "acc_norm": 0.2549019607843137,
                    "acc_norm_stderr": 0.024954184324879905,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.24437299035369775,
                    "acc_stderr": 0.024406162094668917,
                    "acc_norm": 0.24437299035369775,
                    "acc_norm_stderr": 0.024406162094668917,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.22530864197530864,
                    "acc_stderr": 0.023246202647819753,
                    "acc_norm": 0.22530864197530864,
                    "acc_norm_stderr": 0.023246202647819753,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.24468085106382978,
                    "acc_stderr": 0.025645553622266722,
                    "acc_norm": 0.24468085106382978,
                    "acc_norm_stderr": 0.025645553622266722,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.24119947848761408,
                    "acc_stderr": 0.010926496102034965,
                    "acc_norm": 0.24119947848761408,
                    "acc_norm_stderr": 0.010926496102034965,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.44485294117647056,
                    "acc_stderr": 0.03018753206032938,
                    "acc_norm": 0.44485294117647056,
                    "acc_norm_stderr": 0.03018753206032938,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.018120224251484594,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.018120224251484594,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.20909090909090908,
                    "acc_stderr": 0.03895091015724137,
                    "acc_norm": 0.20909090909090908,
                    "acc_norm_stderr": 0.03895091015724137,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.40408163265306124,
                    "acc_stderr": 0.031414708025865885,
                    "acc_norm": 0.40408163265306124,
                    "acc_norm_stderr": 0.031414708025865885,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.23383084577114427,
                    "acc_stderr": 0.029929415408348398,
                    "acc_norm": 0.23383084577114427,
                    "acc_norm_stderr": 0.029929415408348398,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.2710843373493976,
                    "acc_stderr": 0.03460579907553028,
                    "acc_norm": 0.2710843373493976,
                    "acc_norm_stderr": 0.03460579907553028,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.27485380116959063,
                    "acc_stderr": 0.034240429246915824,
                    "acc_norm": 0.27485380116959063,
                    "acc_norm_stderr": 0.034240429246915824,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.23745410036719705,
                    "mc1_stderr": 0.01489627744104184,
                    "mc2": 0.4121958367286861,
                    "mc2_stderr": 0.014564451157949564,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5311760063141279,
                    "acc_stderr": 0.014025142640639518,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.0018875838926174498,
                    "acc_stderr": 0.00044451099905590827,
                    "f1": 0.044600461409396115,
                    "f1_stderr": 0.0012188499729627125,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.006823351023502654,
                    "acc_stderr": 0.0022675371022545013,
                    "timestamp": "2023-11-13T14-11-57.049362"
                }
            }
        }
    }
}