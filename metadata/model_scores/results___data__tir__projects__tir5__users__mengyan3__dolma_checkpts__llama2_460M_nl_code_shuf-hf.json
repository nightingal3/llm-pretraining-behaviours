{
    "model_name": "__data__tir__projects__tir5__users__mengyan3__dolma_checkpts__llama2_460M_nl_code_shuf-hf",
    "last_updated": "2024-12-04 11:26:06.466017",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.22807017543859648,
                "acc_stderr": 0.03218093795602357,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_formal_logic": {
                "acc": 0.15873015873015872,
                "acc_stderr": 0.03268454013011745,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_prehistory": {
                "acc": 0.19753086419753085,
                "acc_stderr": 0.02215288992789896,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23575418994413408,
                "acc_stderr": 0.014196375686290804,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2616033755274262,
                "acc_stderr": 0.028609516716994934,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2254335260115607,
                "acc_stderr": 0.02249723019096755,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_professional_law": {
                "acc": 0.22816166883963493,
                "acc_stderr": 0.010717992192047884,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.3067484662576687,
                "acc_stderr": 0.03623089915724148,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.02886743144984932,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_philosophy": {
                "acc": 0.2379421221864952,
                "acc_stderr": 0.0241851506478187,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2037037037037037,
                "acc_stderr": 0.03893542518824847,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_international_law": {
                "acc": 0.2975206611570248,
                "acc_stderr": 0.04173349148083499,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.23636363636363636,
                "acc_stderr": 0.03317505930009179,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.29533678756476683,
                "acc_stderr": 0.03292296639155141,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.19747899159663865,
                "acc_stderr": 0.025859164122051456,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_geography": {
                "acc": 0.19696969696969696,
                "acc_stderr": 0.028335609732463355,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24036697247706423,
                "acc_stderr": 0.01832060732096407,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_public_relations": {
                "acc": 0.20909090909090908,
                "acc_stderr": 0.03895091015724137,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_sociology": {
                "acc": 0.21890547263681592,
                "acc_stderr": 0.029239174636647,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2358974358974359,
                "acc_stderr": 0.021525965407408726,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_security_studies": {
                "acc": 0.39591836734693875,
                "acc_stderr": 0.03130802899065686,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25163398692810457,
                "acc_stderr": 0.01755581809132227,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_human_sexuality": {
                "acc": 0.20610687022900764,
                "acc_stderr": 0.03547771004159463,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_econometrics": {
                "acc": 0.22807017543859648,
                "acc_stderr": 0.03947152782669415,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.015302380123542089,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_marketing": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.029343114798094476,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_management": {
                "acc": 0.1553398058252427,
                "acc_stderr": 0.03586594738573974,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_nutrition": {
                "acc": 0.27124183006535946,
                "acc_stderr": 0.025457756696667867,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_human_aging": {
                "acc": 0.2914798206278027,
                "acc_stderr": 0.030500283176545906,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_professional_medicine": {
                "acc": 0.43014705882352944,
                "acc_stderr": 0.030074971917302875,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_college_medicine": {
                "acc": 0.23121387283236994,
                "acc_stderr": 0.0321473730202947,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_business_ethics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.23773584905660378,
                "acc_stderr": 0.026199808807561915,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653694,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370519,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2127659574468085,
                "acc_stderr": 0.02441461297430771,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_college_physics": {
                "acc": 0.18627450980392157,
                "acc_stderr": 0.038739587141493524,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2185430463576159,
                "acc_stderr": 0.03374235550425694,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_biology": {
                "acc": 0.20967741935483872,
                "acc_stderr": 0.023157879349083522,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_anatomy": {
                "acc": 0.2814814814814815,
                "acc_stderr": 0.03885004245800254,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_college_chemistry": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_computer_security": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816506,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_college_computer_science": {
                "acc": 0.16,
                "acc_stderr": 0.03684529491774708,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.03110318238312338,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_college_mathematics": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036845,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.3021276595744681,
                "acc_stderr": 0.030017554471880554,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.041633319989322695,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_machine_learning": {
                "acc": 0.3392857142857143,
                "acc_stderr": 0.04493949068613539,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2660098522167488,
                "acc_stderr": 0.031089826002937523,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.3101851851851852,
                "acc_stderr": 0.03154696285656628,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.21164021164021163,
                "acc_stderr": 0.02103733150526289,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.20689655172413793,
                "acc_stderr": 0.03375672449560554,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.24444444444444444,
                "acc_stderr": 0.026202766534652148,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "arc_challenge": {
                "acc": 0.20477815699658702,
                "acc_stderr": 0.011792544338513417,
                "acc_norm": 0.25426621160409557,
                "acc_norm_stderr": 0.012724999945157748,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "hellaswag": {
                "acc": 0.31517625970922125,
                "acc_stderr": 0.004636365534819762,
                "acc_norm": 0.36427006572395937,
                "acc_norm_stderr": 0.004802413919932648,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "truthfulqa_mc2": {
                "acc": 0.41199518604872193,
                "acc_stderr": 0.014936381095236512,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "truthfulqa_gen": {
                "bleu_max": 15.95698975333046,
                "bleu_max_stderr": 0.6081263704679888,
                "bleu_acc": 0.3072215422276622,
                "bleu_acc_stderr": 0.016150201321323002,
                "bleu_diff": -3.7245464479483457,
                "bleu_diff_stderr": 0.5332523284321755,
                "rouge1_max": 38.49688357363435,
                "rouge1_max_stderr": 0.8065996999065227,
                "rouge1_acc": 0.2876376988984088,
                "rouge1_acc_stderr": 0.015846315101394812,
                "rouge1_diff": -5.987855138021341,
                "rouge1_diff_stderr": 0.6547523558599384,
                "rouge2_max": 21.231030167150415,
                "rouge2_max_stderr": 0.8532924387537144,
                "rouge2_acc": 0.19951040391676866,
                "rouge2_acc_stderr": 0.013989929967559649,
                "rouge2_diff": -6.083463853920763,
                "rouge2_diff_stderr": 0.7019618612886404,
                "rougeL_max": 35.43473176198696,
                "rougeL_max_stderr": 0.7987346666351347,
                "rougeL_acc": 0.2864137086903305,
                "rougeL_acc_stderr": 0.015826142439502342,
                "rougeL_diff": -6.12901789205229,
                "rougeL_diff_stderr": 0.6371927433731587,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "truthfulqa_mc1": {
                "acc": 0.24479804161566707,
                "acc_stderr": 0.015051869486714999,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "winogrande": {
                "acc": 0.505130228887135,
                "acc_stderr": 0.014051745961790516,
                "timestamp": "2024-11-22T00-15-58.591172"
            },
            "gsm8k": {
                "exact_match": 0.01819560272934041,
                "exact_match_stderr": 0.003681611894073874,
                "timestamp": "2024-11-22T00-15-58.591172"
            }
        }
    }
}