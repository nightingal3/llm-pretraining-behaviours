{
    "model_name": "Devio/test-3b",
    "last_updated": "2023-10-14",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.25597269624573377,
                    "acc_stderr": 0.012753013241244521,
                    "acc_norm": 0.2764505119453925,
                    "acc_norm_stderr": 0.013069662474252428,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.35988846843258315,
                    "acc_stderr": 0.004789865379084508,
                    "acc_norm": 0.4479187412865963,
                    "acc_norm_stderr": 0.004962638446396,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.042923469599092816,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.037498507091740206,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.037498507091740206,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.21710526315789475,
                    "acc_stderr": 0.03355045304882923,
                    "acc_norm": 0.21710526315789475,
                    "acc_norm_stderr": 0.03355045304882923,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322695,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.041633319989322695,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2339622641509434,
                    "acc_stderr": 0.026055296901152922,
                    "acc_norm": 0.2339622641509434,
                    "acc_norm_stderr": 0.026055296901152922,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2152777777777778,
                    "acc_stderr": 0.03437079344106135,
                    "acc_norm": 0.2152777777777778,
                    "acc_norm_stderr": 0.03437079344106135,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536955,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.038612291966536955,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.039427724440366234,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.039427724440366234,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.21965317919075145,
                    "acc_stderr": 0.031568093627031744,
                    "acc_norm": 0.21965317919075145,
                    "acc_norm_stderr": 0.031568093627031744,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.19607843137254902,
                    "acc_stderr": 0.03950581861179963,
                    "acc_norm": 0.19607843137254902,
                    "acc_norm_stderr": 0.03950581861179963,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.03861229196653694,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.03861229196653694,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3276595744680851,
                    "acc_stderr": 0.030683020843231,
                    "acc_norm": 0.3276595744680851,
                    "acc_norm_stderr": 0.030683020843231,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.21929824561403508,
                    "acc_stderr": 0.03892431106518754,
                    "acc_norm": 0.21929824561403508,
                    "acc_norm_stderr": 0.03892431106518754,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.25517241379310346,
                    "acc_stderr": 0.03632984052707842,
                    "acc_norm": 0.25517241379310346,
                    "acc_norm_stderr": 0.03632984052707842,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2566137566137566,
                    "acc_stderr": 0.022494510767503154,
                    "acc_norm": 0.2566137566137566,
                    "acc_norm_stderr": 0.022494510767503154,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.19047619047619047,
                    "acc_stderr": 0.03512207412302052,
                    "acc_norm": 0.19047619047619047,
                    "acc_norm_stderr": 0.03512207412302052,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536934,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.038612291966536934,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.29354838709677417,
                    "acc_stderr": 0.02590608702131929,
                    "acc_norm": 0.29354838709677417,
                    "acc_norm_stderr": 0.02590608702131929,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2315270935960591,
                    "acc_stderr": 0.029678333141444455,
                    "acc_norm": 0.2315270935960591,
                    "acc_norm_stderr": 0.029678333141444455,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.16,
                    "acc_stderr": 0.036845294917747094,
                    "acc_norm": 0.16,
                    "acc_norm_stderr": 0.036845294917747094,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.22424242424242424,
                    "acc_stderr": 0.032568666616811015,
                    "acc_norm": 0.22424242424242424,
                    "acc_norm_stderr": 0.032568666616811015,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.2474747474747475,
                    "acc_stderr": 0.030746300742124498,
                    "acc_norm": 0.2474747474747475,
                    "acc_norm_stderr": 0.030746300742124498,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.23316062176165803,
                    "acc_stderr": 0.030516111371476008,
                    "acc_norm": 0.23316062176165803,
                    "acc_norm_stderr": 0.030516111371476008,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.24871794871794872,
                    "acc_stderr": 0.0219169577092138,
                    "acc_norm": 0.24871794871794872,
                    "acc_norm_stderr": 0.0219169577092138,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.026067159222275805,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.026067159222275805,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.24369747899159663,
                    "acc_stderr": 0.02788682807838056,
                    "acc_norm": 0.24369747899159663,
                    "acc_norm_stderr": 0.02788682807838056,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2052980132450331,
                    "acc_stderr": 0.03297986648473836,
                    "acc_norm": 0.2052980132450331,
                    "acc_norm_stderr": 0.03297986648473836,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.22568807339449543,
                    "acc_stderr": 0.01792308766780305,
                    "acc_norm": 0.22568807339449543,
                    "acc_norm_stderr": 0.01792308766780305,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4351851851851852,
                    "acc_stderr": 0.033812000056435254,
                    "acc_norm": 0.4351851851851852,
                    "acc_norm_stderr": 0.033812000056435254,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.028867431449849303,
                    "acc_norm": 0.21568627450980393,
                    "acc_norm_stderr": 0.028867431449849303,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.26582278481012656,
                    "acc_stderr": 0.028756799629658342,
                    "acc_norm": 0.26582278481012656,
                    "acc_norm_stderr": 0.028756799629658342,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.18834080717488788,
                    "acc_stderr": 0.026241132996407252,
                    "acc_norm": 0.18834080717488788,
                    "acc_norm_stderr": 0.026241132996407252,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.22900763358778625,
                    "acc_stderr": 0.036853466317118506,
                    "acc_norm": 0.22900763358778625,
                    "acc_norm_stderr": 0.036853466317118506,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.256198347107438,
                    "acc_stderr": 0.03984979653302872,
                    "acc_norm": 0.256198347107438,
                    "acc_norm_stderr": 0.03984979653302872,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.040191074725573483,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.040191074725573483,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.294478527607362,
                    "acc_stderr": 0.03581165790474082,
                    "acc_norm": 0.294478527607362,
                    "acc_norm_stderr": 0.03581165790474082,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.24107142857142858,
                    "acc_stderr": 0.04059867246952687,
                    "acc_norm": 0.24107142857142858,
                    "acc_norm_stderr": 0.04059867246952687,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.20388349514563106,
                    "acc_stderr": 0.0398913985953177,
                    "acc_norm": 0.20388349514563106,
                    "acc_norm_stderr": 0.0398913985953177,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.20085470085470086,
                    "acc_stderr": 0.026246772946890477,
                    "acc_norm": 0.20085470085470086,
                    "acc_norm_stderr": 0.026246772946890477,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.28607918263090676,
                    "acc_stderr": 0.016160871405127543,
                    "acc_norm": 0.28607918263090676,
                    "acc_norm_stderr": 0.016160871405127543,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.1994219653179191,
                    "acc_stderr": 0.021511900654252524,
                    "acc_norm": 0.1994219653179191,
                    "acc_norm_stderr": 0.021511900654252524,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.23798882681564246,
                    "acc_stderr": 0.014242630070574875,
                    "acc_norm": 0.23798882681564246,
                    "acc_norm_stderr": 0.014242630070574875,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.24836601307189543,
                    "acc_stderr": 0.02473998135511359,
                    "acc_norm": 0.24836601307189543,
                    "acc_norm_stderr": 0.02473998135511359,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.19292604501607716,
                    "acc_stderr": 0.022411516780911366,
                    "acc_norm": 0.19292604501607716,
                    "acc_norm_stderr": 0.022411516780911366,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.26851851851851855,
                    "acc_stderr": 0.024659685185967277,
                    "acc_norm": 0.26851851851851855,
                    "acc_norm_stderr": 0.024659685185967277,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2553191489361702,
                    "acc_stderr": 0.026011992930902013,
                    "acc_norm": 0.2553191489361702,
                    "acc_norm_stderr": 0.026011992930902013,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.24902216427640156,
                    "acc_stderr": 0.01104489226404077,
                    "acc_norm": 0.24902216427640156,
                    "acc_norm_stderr": 0.01104489226404077,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.24632352941176472,
                    "acc_stderr": 0.02617343857052,
                    "acc_norm": 0.24632352941176472,
                    "acc_norm_stderr": 0.02617343857052,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.25326797385620914,
                    "acc_stderr": 0.017593486895366835,
                    "acc_norm": 0.25326797385620914,
                    "acc_norm_stderr": 0.017593486895366835,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.24545454545454545,
                    "acc_stderr": 0.04122066502878285,
                    "acc_norm": 0.24545454545454545,
                    "acc_norm_stderr": 0.04122066502878285,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.24081632653061225,
                    "acc_stderr": 0.027372942201788163,
                    "acc_norm": 0.24081632653061225,
                    "acc_norm_stderr": 0.027372942201788163,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.208955223880597,
                    "acc_stderr": 0.028748298931728655,
                    "acc_norm": 0.208955223880597,
                    "acc_norm_stderr": 0.028748298931728655,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.19879518072289157,
                    "acc_stderr": 0.031069390260789424,
                    "acc_norm": 0.19879518072289157,
                    "acc_norm_stderr": 0.031069390260789424,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.21052631578947367,
                    "acc_stderr": 0.0312678171466318,
                    "acc_norm": 0.21052631578947367,
                    "acc_norm_stderr": 0.0312678171466318,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.23011015911872704,
                    "mc1_stderr": 0.014734557959807765,
                    "mc2": 0.41415759101311883,
                    "mc2_stderr": 0.014688710447803573,
                    "timestamp": "2023-09-02T16-42-09.049307"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.006816275167785235,
                    "em_stderr": 0.000842612709585923,
                    "f1": 0.0460748741610739,
                    "f1_stderr": 0.001322491101848216,
                    "timestamp": "2023-10-14T16-45-10.101567"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.003032600454890068,
                    "acc_stderr": 0.0015145735612245457,
                    "timestamp": "2023-10-14T16-45-10.101567"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5548539857932123,
                    "acc_stderr": 0.013967662954355491,
                    "timestamp": "2023-10-14T16-45-10.101567"
                }
            }
        }
    }
}