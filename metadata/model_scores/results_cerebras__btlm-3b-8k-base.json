{
    "model_name": "cerebras/btlm-3b-8k-base",
    "last_updated": "2024-12-04 11:22:30.192230",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.014652014652014652,
                "exact_match_stderr": 0.005146894158982162,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.03444316877152698,
                "exact_match_stderr": 0.006182738010487318,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.005203704987512652,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0221483942414175,
                "exact_match_stderr": 0.004900093088615796,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_geometry": {
                "exact_match": 0.027139874739039668,
                "exact_match_stderr": 0.00743216209077084,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.027426160337552744,
                "exact_match_stderr": 0.0075095381303842195,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_algebra": {
                "exact_match": 0.019376579612468407,
                "exact_match_stderr": 0.004002647498105362,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_3da": {
                "acc": 0.0215,
                "acc_stderr": 0.003244092641792814,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_3ds": {
                "acc": 0.043,
                "acc_stderr": 0.004537156917767878,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_4da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521425,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_2ds": {
                "acc": 0.45,
                "acc_stderr": 0.011127079848413738,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_1dc": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000037,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_4ds": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339592,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_2dm": {
                "acc": 0.0335,
                "acc_stderr": 0.004024546370306097,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_2da": {
                "acc": 0.207,
                "acc_stderr": 0.009061818707033228,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "gsm8k_cot": {
                "exact_match": 0.04169825625473844,
                "exact_match_stderr": 0.005506205058175758,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "gsm8k": {
                "exact_match": 0.052312357846853674,
                "exact_match_stderr": 0.0061330577089592315,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "anli_r2": {
                "brier_score": 0.7462669818323581,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "anli_r3": {
                "brier_score": 0.7470389244949329,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "anli_r1": {
                "brier_score": 0.7739522354279014,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_eu": {
                "brier_score": 1.26760834525426,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_vi": {
                "brier_score": 1.0776254851454548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_ru": {
                "brier_score": 0.8572693761958452,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_zh": {
                "brier_score": 1.02506106359857,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_tr": {
                "brier_score": 0.9690088326153724,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_fr": {
                "brier_score": 0.7572273331665541,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_en": {
                "brier_score": 0.6442514069003313,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_ur": {
                "brier_score": 1.306062980613888,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_ar": {
                "brier_score": 0.9938749206769895,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_de": {
                "brier_score": 0.9222235658348613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_hi": {
                "brier_score": 0.894747482032871,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_es": {
                "brier_score": 0.925763882691409,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_bg": {
                "brier_score": 0.8952905602595471,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_sw": {
                "brier_score": 0.9874132082553371,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_el": {
                "brier_score": 1.0575280874979667,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_th": {
                "brier_score": 1.2107325943420113,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "logiqa2": {
                "brier_score": 1.0390121306040254,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "mathqa": {
                "brier_score": 0.9502679389813659,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "lambada_standard": {
                "perplexity": 5.9215811991829925,
                "perplexity_stderr": 0.14341353528409098,
                "acc": 0.6079953425189211,
                "acc_stderr": 0.006801548708056979,
                "timestamp": "2024-06-13T13-48-42.209852"
            },
            "lambada_openai": {
                "perplexity": 4.721463260829105,
                "perplexity_stderr": 0.11007675882818492,
                "acc": 0.6615563749272269,
                "acc_stderr": 0.0065923259327411495,
                "timestamp": "2024-06-13T13-48-42.209852"
            },
            "mmlu_world_religions": {
                "acc": 0.30409356725146197,
                "acc_stderr": 0.03528211258245231,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.03670066451047181,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_prehistory": {
                "acc": 0.2808641975308642,
                "acc_stderr": 0.025006469755799208,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24581005586592178,
                "acc_stderr": 0.014400296429225619,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.33755274261603374,
                "acc_stderr": 0.030781549102026223,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2861271676300578,
                "acc_stderr": 0.024332146779134128,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_professional_law": {
                "acc": 0.2529335071707953,
                "acc_stderr": 0.011102268713839987,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2147239263803681,
                "acc_stderr": 0.03226219377286774,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.030587591351604243,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_philosophy": {
                "acc": 0.2765273311897106,
                "acc_stderr": 0.025403832978179604,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_jurisprudence": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.04284467968052191,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_international_law": {
                "acc": 0.2644628099173554,
                "acc_stderr": 0.04026187527591205,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.296969696969697,
                "acc_stderr": 0.03567969772268047,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.27461139896373055,
                "acc_stderr": 0.03221024508041156,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.25630252100840334,
                "acc_stderr": 0.02835962087053395,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_geography": {
                "acc": 0.22727272727272727,
                "acc_stderr": 0.029857515673386414,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.26238532110091745,
                "acc_stderr": 0.018861885021534745,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_public_relations": {
                "acc": 0.2818181818181818,
                "acc_stderr": 0.043091187099464585,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_sociology": {
                "acc": 0.3034825870646766,
                "acc_stderr": 0.03251006816458619,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24102564102564103,
                "acc_stderr": 0.02168554666533319,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_security_studies": {
                "acc": 0.3510204081632653,
                "acc_stderr": 0.03055531675557364,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_professional_psychology": {
                "acc": 0.27124183006535946,
                "acc_stderr": 0.017986615304030316,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_human_sexuality": {
                "acc": 0.25190839694656486,
                "acc_stderr": 0.038073871163060866,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_econometrics": {
                "acc": 0.2543859649122807,
                "acc_stderr": 0.040969851398436716,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_miscellaneous": {
                "acc": 0.31928480204342274,
                "acc_stderr": 0.01667126174953872,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_marketing": {
                "acc": 0.28205128205128205,
                "acc_stderr": 0.02948036054954119,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_management": {
                "acc": 0.20388349514563106,
                "acc_stderr": 0.0398913985953177,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_nutrition": {
                "acc": 0.25163398692810457,
                "acc_stderr": 0.0248480182638752,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_human_aging": {
                "acc": 0.3901345291479821,
                "acc_stderr": 0.03273766725459157,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_professional_medicine": {
                "acc": 0.27205882352941174,
                "acc_stderr": 0.027033041151681456,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_college_medicine": {
                "acc": 0.24277456647398843,
                "acc_stderr": 0.0326926380614177,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_business_ethics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2339622641509434,
                "acc_stderr": 0.02605529690115292,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_global_facts": {
                "acc": 0.32,
                "acc_stderr": 0.04688261722621505,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_virology": {
                "acc": 0.3132530120481928,
                "acc_stderr": 0.036108050180310235,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_professional_accounting": {
                "acc": 0.24468085106382978,
                "acc_stderr": 0.02564555362226673,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_college_physics": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.04336432707993177,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_physics": {
                "acc": 0.23178807947019867,
                "acc_stderr": 0.034454062719870546,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_biology": {
                "acc": 0.23870967741935484,
                "acc_stderr": 0.024251071262208837,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_college_biology": {
                "acc": 0.3055555555555556,
                "acc_stderr": 0.03852084696008534,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_anatomy": {
                "acc": 0.2074074074074074,
                "acc_stderr": 0.03502553170678318,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_college_chemistry": {
                "acc": 0.27,
                "acc_stderr": 0.04461960433384741,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_computer_security": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768076,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_college_computer_science": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_astronomy": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.035834961763610645,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_college_mathematics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2978723404255319,
                "acc_stderr": 0.029896145682095462,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768079,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_machine_learning": {
                "acc": 0.25892857142857145,
                "acc_stderr": 0.04157751539865629,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.22660098522167488,
                "acc_stderr": 0.029454863835292968,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.25462962962962965,
                "acc_stderr": 0.029711275860005357,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2619047619047619,
                "acc_stderr": 0.022644212615525214,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.21379310344827587,
                "acc_stderr": 0.034165204477475494,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.25555555555555554,
                "acc_stderr": 0.026593939101844065,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "arc_challenge": {
                "acc": 0.38310580204778155,
                "acc_stderr": 0.01420647266167288,
                "acc_norm": 0.4112627986348123,
                "acc_norm_stderr": 0.014379441068522084,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "hellaswag": {
                "acc": 0.5226050587532364,
                "acc_stderr": 0.004984679359375625,
                "acc_norm": 0.7099183429595698,
                "acc_norm_stderr": 0.004528723951878226,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "truthfulqa_mc2": {
                "acc": 0.35964570198337426,
                "acc_stderr": 0.013582290589589971,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.382575479594944,
                "bleu_max_stderr": 0.7680990447091552,
                "bleu_acc": 0.3243574051407589,
                "bleu_acc_stderr": 0.01638797677964794,
                "bleu_diff": -7.56470475629629,
                "bleu_diff_stderr": 0.8511362909045714,
                "rouge1_max": 48.69810706635801,
                "rouge1_max_stderr": 0.8872210250095384,
                "rouge1_acc": 0.2778457772337821,
                "rouge1_acc_stderr": 0.015680929364024657,
                "rouge1_diff": -9.685772374361353,
                "rouge1_diff_stderr": 1.0120488242587284,
                "rouge2_max": 32.46140782430542,
                "rouge2_max_stderr": 1.0148035582417252,
                "rouge2_acc": 0.23378212974296206,
                "rouge2_acc_stderr": 0.014816195991931593,
                "rouge2_diff": -11.236829286123939,
                "rouge2_diff_stderr": 1.121260025022326,
                "rougeL_max": 46.12917282015782,
                "rougeL_max_stderr": 0.9023627784282651,
                "rougeL_acc": 0.26560587515299877,
                "rougeL_acc_stderr": 0.015461027627253586,
                "rougeL_diff": -9.989538296930288,
                "rougeL_diff_stderr": 1.0154544227366404,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "truthfulqa_mc1": {
                "acc": 0.22276621787025705,
                "acc_stderr": 0.01456650696139675,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "winogrande": {
                "acc": 0.6614048934490924,
                "acc_stderr": 0.013300169865842426,
                "timestamp": "2024-11-22T14-26-18.572446"
            }
        }
    }
}