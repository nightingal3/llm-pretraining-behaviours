{
    "model_name": "cerebras/btlm-3b-8k-base",
    "last_updated": "2024-12-19 13:37:47.471855",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.014652014652014652,
                "exact_match_stderr": 0.005146894158982162,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.03444316877152698,
                "exact_match_stderr": 0.006182738010487318,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.005203704987512652,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0221483942414175,
                "exact_match_stderr": 0.004900093088615796,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_geometry": {
                "exact_match": 0.027139874739039668,
                "exact_match_stderr": 0.00743216209077084,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.027426160337552744,
                "exact_match_stderr": 0.0075095381303842195,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "minerva_math_algebra": {
                "exact_match": 0.019376579612468407,
                "exact_match_stderr": 0.004002647498105362,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_3da": {
                "acc": 0.0215,
                "acc_stderr": 0.003244092641792814,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_3ds": {
                "acc": 0.043,
                "acc_stderr": 0.004537156917767878,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_4da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521425,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_2ds": {
                "acc": 0.45,
                "acc_stderr": 0.011127079848413738,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_1dc": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000037,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_4ds": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339592,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_2dm": {
                "acc": 0.0335,
                "acc_stderr": 0.004024546370306097,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "arithmetic_2da": {
                "acc": 0.207,
                "acc_stderr": 0.009061818707033228,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "gsm8k_cot": {
                "exact_match": 0.04169825625473844,
                "exact_match_stderr": 0.005506205058175758,
                "timestamp": "2024-06-13T13-40-07.732275"
            },
            "gsm8k": {
                "exact_match": 0.052312357846853674,
                "exact_match_stderr": 0.0061330577089592315,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "anli_r2": {
                "brier_score": 0.7462669818323581,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "anli_r3": {
                "brier_score": 0.7470389244949329,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "anli_r1": {
                "brier_score": 0.7739522354279014,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_eu": {
                "brier_score": 1.26760834525426,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_vi": {
                "brier_score": 1.0776254851454548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_ru": {
                "brier_score": 0.8572693761958452,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_zh": {
                "brier_score": 1.02506106359857,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_tr": {
                "brier_score": 0.9690088326153724,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_fr": {
                "brier_score": 0.7572273331665541,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_en": {
                "brier_score": 0.6442514069003313,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_ur": {
                "brier_score": 1.306062980613888,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_ar": {
                "brier_score": 0.9938749206769895,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_de": {
                "brier_score": 0.9222235658348613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_hi": {
                "brier_score": 0.894747482032871,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_es": {
                "brier_score": 0.925763882691409,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_bg": {
                "brier_score": 0.8952905602595471,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_sw": {
                "brier_score": 0.9874132082553371,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_el": {
                "brier_score": 1.0575280874979667,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "xnli_th": {
                "brier_score": 1.2107325943420113,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "logiqa2": {
                "brier_score": 1.0390121306040254,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "mathqa": {
                "brier_score": 0.9502679389813659,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T13-47-33.416752"
            },
            "lambada_standard": {
                "perplexity": 5.9215811991829925,
                "perplexity_stderr": 0.14341353528409098,
                "acc": 0.6079953425189211,
                "acc_stderr": 0.006801548708056979,
                "timestamp": "2024-06-13T13-48-42.209852"
            },
            "lambada_openai": {
                "perplexity": 4.721463260829105,
                "perplexity_stderr": 0.11007675882818492,
                "acc": 0.6615563749272269,
                "acc_stderr": 0.0065923259327411495,
                "timestamp": "2024-06-13T13-48-42.209852"
            },
            "mmlu_world_religions": {
                "acc": 0.29239766081871343,
                "acc_stderr": 0.034886477134579215,
                "brier_score": 0.765283458975345,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_formal_logic": {
                "acc": 0.19047619047619047,
                "acc_stderr": 0.03512207412302054,
                "brier_score": 0.7811181241262286,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_prehistory": {
                "acc": 0.29012345679012347,
                "acc_stderr": 0.025251173936495026,
                "brier_score": 0.7525978131388321,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.014422292204808838,
                "brier_score": 0.806961001008417,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.270042194092827,
                "acc_stderr": 0.028900721906293426,
                "brier_score": 0.7661484483752257,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2976878612716763,
                "acc_stderr": 0.024617055388677003,
                "brier_score": 0.7513136133716709,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_professional_law": {
                "acc": 0.27640156453715775,
                "acc_stderr": 0.01142215319455358,
                "brier_score": 0.7698465557943386,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.3312883435582822,
                "acc_stderr": 0.03697983910025588,
                "brier_score": 0.7507690419290711,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.030587591351604246,
                "brier_score": 0.7642473421486193,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_philosophy": {
                "acc": 0.31189710610932475,
                "acc_stderr": 0.02631185807185416,
                "brier_score": 0.7518637246830614,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_jurisprudence": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.04284467968052191,
                "brier_score": 0.7724755566562874,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_international_law": {
                "acc": 0.371900826446281,
                "acc_stderr": 0.044120158066245044,
                "brier_score": 0.7278607764002772,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.2787878787878788,
                "acc_stderr": 0.03501438706296782,
                "brier_score": 0.7578314247618458,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.23834196891191708,
                "acc_stderr": 0.03074890536390988,
                "brier_score": 0.7556016591155795,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23949579831932774,
                "acc_stderr": 0.02772206549336127,
                "brier_score": 0.7543189745804204,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_geography": {
                "acc": 0.25252525252525254,
                "acc_stderr": 0.030954055470365907,
                "brier_score": 0.7654693814686194,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24587155963302754,
                "acc_stderr": 0.018461940968708443,
                "brier_score": 0.760696893950433,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_public_relations": {
                "acc": 0.19090909090909092,
                "acc_stderr": 0.03764425585984927,
                "brier_score": 0.7899351526404036,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 0.7540244111010149,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_sociology": {
                "acc": 0.25870646766169153,
                "acc_stderr": 0.03096590312357303,
                "brier_score": 0.7590506093520147,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24358974358974358,
                "acc_stderr": 0.021763733684173933,
                "brier_score": 0.7591500438875595,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_security_studies": {
                "acc": 0.22857142857142856,
                "acc_stderr": 0.026882144922307748,
                "brier_score": 0.7515111354054198,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_professional_psychology": {
                "acc": 0.28594771241830064,
                "acc_stderr": 0.018280485072954687,
                "brier_score": 0.7591990945166102,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_human_sexuality": {
                "acc": 0.3053435114503817,
                "acc_stderr": 0.04039314978724561,
                "brier_score": 0.7547846920764133,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_econometrics": {
                "acc": 0.21052631578947367,
                "acc_stderr": 0.038351539543994194,
                "brier_score": 0.7843401099086258,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_miscellaneous": {
                "acc": 0.33588761174968074,
                "acc_stderr": 0.016889407235171683,
                "brier_score": 0.7393151258344807,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_marketing": {
                "acc": 0.2692307692307692,
                "acc_stderr": 0.029058588303748845,
                "brier_score": 0.7582287865784976,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_management": {
                "acc": 0.21359223300970873,
                "acc_stderr": 0.04058042015646035,
                "brier_score": 0.7677446200491856,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_nutrition": {
                "acc": 0.27450980392156865,
                "acc_stderr": 0.025553169991826517,
                "brier_score": 0.759851624640113,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_medical_genetics": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 0.7706140139510501,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_human_aging": {
                "acc": 0.22869955156950672,
                "acc_stderr": 0.028188240046929196,
                "brier_score": 0.7987091115125714,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_professional_medicine": {
                "acc": 0.15808823529411764,
                "acc_stderr": 0.02216146260806852,
                "brier_score": 0.7779534874194103,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.03242414757483098,
                "brier_score": 0.766554717412249,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_business_ethics": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816505,
                "brier_score": 0.7656375808021179,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.23773584905660378,
                "acc_stderr": 0.02619980880756193,
                "brier_score": 0.7631703136393346,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_global_facts": {
                "acc": 0.33,
                "acc_stderr": 0.047258156262526045,
                "brier_score": 0.7372744149477343,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_virology": {
                "acc": 0.21084337349397592,
                "acc_stderr": 0.03175554786629919,
                "brier_score": 0.7607599505428622,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.026684564340461,
                "brier_score": 0.7716867889954537,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.04092563958237656,
                "brier_score": 0.7879728501816907,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_physics": {
                "acc": 0.32450331125827814,
                "acc_stderr": 0.038227469376587525,
                "brier_score": 0.7381303399059673,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_biology": {
                "acc": 0.267741935483871,
                "acc_stderr": 0.025189006660212374,
                "brier_score": 0.7432912408175562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_college_biology": {
                "acc": 0.2916666666666667,
                "acc_stderr": 0.038009680605548574,
                "brier_score": 0.75721550669838,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_anatomy": {
                "acc": 0.34074074074074073,
                "acc_stderr": 0.040943762699967946,
                "brier_score": 0.7483348312450915,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_college_chemistry": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909283,
                "brier_score": 0.7596323870355468,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_computer_security": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7524614960670815,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_college_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7580127277762976,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_astronomy": {
                "acc": 0.3223684210526316,
                "acc_stderr": 0.03803510248351586,
                "brier_score": 0.7482452724551685,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_college_mathematics": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "brier_score": 0.7536100014431494,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2553191489361702,
                "acc_stderr": 0.028504856470514203,
                "brier_score": 0.7538476797353884,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7643901098719358,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.32,
                "acc_stderr": 0.046882617226215034,
                "brier_score": 0.7550698213030492,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_machine_learning": {
                "acc": 0.24107142857142858,
                "acc_stderr": 0.04059867246952687,
                "brier_score": 0.7783521858574265,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2955665024630542,
                "acc_stderr": 0.032104944337514575,
                "brier_score": 0.7512368230132239,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.23148148148148148,
                "acc_stderr": 0.028765111718046944,
                "brier_score": 0.7474000718882282,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2619047619047619,
                "acc_stderr": 0.022644212615525218,
                "brier_score": 0.7793545696195081,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.27586206896551724,
                "acc_stderr": 0.037245636197746325,
                "brier_score": 0.7511138343126749,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.026719240783712173,
                "brier_score": 0.7673250316212422,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-50-53.762839"
            },
            "arc_challenge": {
                "acc": 0.38310580204778155,
                "acc_stderr": 0.01420647266167288,
                "acc_norm": 0.4112627986348123,
                "acc_norm_stderr": 0.014379441068522084,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "hellaswag": {
                "acc": 0.5226050587532364,
                "acc_stderr": 0.004984679359375625,
                "acc_norm": 0.7099183429595698,
                "acc_norm_stderr": 0.004528723951878226,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "truthfulqa_mc2": {
                "acc": 0.35964570198337426,
                "acc_stderr": 0.013582290589589971,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "truthfulqa_gen": {
                "bleu_max": 24.382575479594944,
                "bleu_max_stderr": 0.7680990447091552,
                "bleu_acc": 0.3243574051407589,
                "bleu_acc_stderr": 0.01638797677964794,
                "bleu_diff": -7.56470475629629,
                "bleu_diff_stderr": 0.8511362909045714,
                "rouge1_max": 48.69810706635801,
                "rouge1_max_stderr": 0.8872210250095384,
                "rouge1_acc": 0.2778457772337821,
                "rouge1_acc_stderr": 0.015680929364024657,
                "rouge1_diff": -9.685772374361353,
                "rouge1_diff_stderr": 1.0120488242587284,
                "rouge2_max": 32.46140782430542,
                "rouge2_max_stderr": 1.0148035582417252,
                "rouge2_acc": 0.23378212974296206,
                "rouge2_acc_stderr": 0.014816195991931593,
                "rouge2_diff": -11.236829286123939,
                "rouge2_diff_stderr": 1.121260025022326,
                "rougeL_max": 46.12917282015782,
                "rougeL_max_stderr": 0.9023627784282651,
                "rougeL_acc": 0.26560587515299877,
                "rougeL_acc_stderr": 0.015461027627253586,
                "rougeL_diff": -9.989538296930288,
                "rougeL_diff_stderr": 1.0154544227366404,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "truthfulqa_mc1": {
                "acc": 0.22276621787025705,
                "acc_stderr": 0.01456650696139675,
                "timestamp": "2024-11-22T14-26-18.572446"
            },
            "winogrande": {
                "acc": 0.6614048934490924,
                "acc_stderr": 0.013300169865842426,
                "timestamp": "2024-11-22T14-26-18.572446"
            }
        }
    }
}