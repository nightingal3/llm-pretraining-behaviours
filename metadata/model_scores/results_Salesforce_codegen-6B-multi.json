{
    "model_name": "Salesforce/codegen-6B-multi",
    "last_updated": "2023-09-17",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.2380546075085324,
                    "acc_stderr": 0.012445770028026208,
                    "acc_norm": 0.2721843003412969,
                    "acc_norm_stderr": 0.013006600406423706,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.3350926110336586,
                    "acc_stderr": 0.004710581496639355,
                    "acc_norm": 0.41107349133638715,
                    "acc_norm_stderr": 0.004910229643262719,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2814814814814815,
                    "acc_stderr": 0.038850042458002526,
                    "acc_norm": 0.2814814814814815,
                    "acc_norm_stderr": 0.038850042458002526,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.20394736842105263,
                    "acc_stderr": 0.0327900040631005,
                    "acc_norm": 0.20394736842105263,
                    "acc_norm_stderr": 0.0327900040631005,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768079,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768079,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.27169811320754716,
                    "acc_stderr": 0.027377706624670713,
                    "acc_norm": 0.27169811320754716,
                    "acc_norm_stderr": 0.027377706624670713,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2916666666666667,
                    "acc_stderr": 0.03800968060554857,
                    "acc_norm": 0.2916666666666667,
                    "acc_norm_stderr": 0.03800968060554857,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.04020151261036845,
                    "acc_norm": 0.2,
                    "acc_norm_stderr": 0.04020151261036845,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.0440844002276808,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.0440844002276808,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816506,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816506,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2543352601156069,
                    "acc_stderr": 0.0332055644308557,
                    "acc_norm": 0.2543352601156069,
                    "acc_norm_stderr": 0.0332055644308557,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.19607843137254902,
                    "acc_stderr": 0.03950581861179963,
                    "acc_norm": 0.19607843137254902,
                    "acc_norm_stderr": 0.03950581861179963,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3446808510638298,
                    "acc_stderr": 0.03106898596312215,
                    "acc_norm": 0.3446808510638298,
                    "acc_norm_stderr": 0.03106898596312215,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.04142439719489361,
                    "acc_norm": 0.2631578947368421,
                    "acc_norm_stderr": 0.04142439719489361,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.30344827586206896,
                    "acc_stderr": 0.038312260488503336,
                    "acc_norm": 0.30344827586206896,
                    "acc_norm_stderr": 0.038312260488503336,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.24603174603174602,
                    "acc_stderr": 0.022182037202948368,
                    "acc_norm": 0.24603174603174602,
                    "acc_norm_stderr": 0.022182037202948368,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.03718489006818116,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.03718489006818116,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816508,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816508,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.23225806451612904,
                    "acc_stderr": 0.024022256130308235,
                    "acc_norm": 0.23225806451612904,
                    "acc_norm_stderr": 0.024022256130308235,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2512315270935961,
                    "acc_stderr": 0.030516530732694433,
                    "acc_norm": 0.2512315270935961,
                    "acc_norm_stderr": 0.030516530732694433,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252604,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.03453131801885416,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.03453131801885416,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.21717171717171718,
                    "acc_stderr": 0.029376616484945633,
                    "acc_norm": 0.21717171717171718,
                    "acc_norm_stderr": 0.029376616484945633,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.20725388601036268,
                    "acc_stderr": 0.029252823291803624,
                    "acc_norm": 0.20725388601036268,
                    "acc_norm_stderr": 0.029252823291803624,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2205128205128205,
                    "acc_stderr": 0.02102067268082791,
                    "acc_norm": 0.2205128205128205,
                    "acc_norm_stderr": 0.02102067268082791,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.24444444444444444,
                    "acc_stderr": 0.026202766534652148,
                    "acc_norm": 0.24444444444444444,
                    "acc_norm_stderr": 0.026202766534652148,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.25210084033613445,
                    "acc_stderr": 0.028205545033277723,
                    "acc_norm": 0.25210084033613445,
                    "acc_norm_stderr": 0.028205545033277723,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2847682119205298,
                    "acc_stderr": 0.03684881521389023,
                    "acc_norm": 0.2847682119205298,
                    "acc_norm_stderr": 0.03684881521389023,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.25504587155963304,
                    "acc_stderr": 0.01868850085653584,
                    "acc_norm": 0.25504587155963304,
                    "acc_norm_stderr": 0.01868850085653584,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.2638888888888889,
                    "acc_stderr": 0.03005820270430985,
                    "acc_norm": 0.2638888888888889,
                    "acc_norm_stderr": 0.03005820270430985,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.030964517926923403,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.030964517926923403,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.28270042194092826,
                    "acc_stderr": 0.02931281415395592,
                    "acc_norm": 0.28270042194092826,
                    "acc_norm_stderr": 0.02931281415395592,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.40358744394618834,
                    "acc_stderr": 0.032928028193303135,
                    "acc_norm": 0.40358744394618834,
                    "acc_norm_stderr": 0.032928028193303135,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.1984732824427481,
                    "acc_stderr": 0.03498149385462472,
                    "acc_norm": 0.1984732824427481,
                    "acc_norm_stderr": 0.03498149385462472,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.34710743801652894,
                    "acc_stderr": 0.04345724570292534,
                    "acc_norm": 0.34710743801652894,
                    "acc_norm_stderr": 0.04345724570292534,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.23148148148148148,
                    "acc_stderr": 0.04077494709252626,
                    "acc_norm": 0.23148148148148148,
                    "acc_norm_stderr": 0.04077494709252626,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2883435582822086,
                    "acc_stderr": 0.035590395316173425,
                    "acc_norm": 0.2883435582822086,
                    "acc_norm_stderr": 0.035590395316173425,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.2857142857142857,
                    "acc_stderr": 0.04287858751340456,
                    "acc_norm": 0.2857142857142857,
                    "acc_norm_stderr": 0.04287858751340456,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.17475728155339806,
                    "acc_stderr": 0.037601780060266224,
                    "acc_norm": 0.17475728155339806,
                    "acc_norm_stderr": 0.037601780060266224,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.027236013946196666,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.027236013946196666,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909282,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909282,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.2848020434227331,
                    "acc_stderr": 0.01613917409652258,
                    "acc_norm": 0.2848020434227331,
                    "acc_norm_stderr": 0.01613917409652258,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.29190751445086704,
                    "acc_stderr": 0.024476994076247337,
                    "acc_norm": 0.29190751445086704,
                    "acc_norm_stderr": 0.024476994076247337,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2435754189944134,
                    "acc_stderr": 0.01435591196476786,
                    "acc_norm": 0.2435754189944134,
                    "acc_norm_stderr": 0.01435591196476786,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.24836601307189543,
                    "acc_stderr": 0.024739981355113592,
                    "acc_norm": 0.24836601307189543,
                    "acc_norm_stderr": 0.024739981355113592,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.31189710610932475,
                    "acc_stderr": 0.02631185807185416,
                    "acc_norm": 0.31189710610932475,
                    "acc_norm_stderr": 0.02631185807185416,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.023788583551658526,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.023788583551658526,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.22340425531914893,
                    "acc_stderr": 0.024847921358063962,
                    "acc_norm": 0.22340425531914893,
                    "acc_norm_stderr": 0.024847921358063962,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.26597131681877445,
                    "acc_stderr": 0.011285033165551277,
                    "acc_norm": 0.26597131681877445,
                    "acc_norm_stderr": 0.011285033165551277,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.1875,
                    "acc_stderr": 0.023709788253811766,
                    "acc_norm": 0.1875,
                    "acc_norm_stderr": 0.023709788253811766,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.25326797385620914,
                    "acc_stderr": 0.01759348689536683,
                    "acc_norm": 0.25326797385620914,
                    "acc_norm_stderr": 0.01759348689536683,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2909090909090909,
                    "acc_stderr": 0.04350271442923243,
                    "acc_norm": 0.2909090909090909,
                    "acc_norm_stderr": 0.04350271442923243,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.17959183673469387,
                    "acc_stderr": 0.024573293589585637,
                    "acc_norm": 0.17959183673469387,
                    "acc_norm_stderr": 0.024573293589585637,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.23880597014925373,
                    "acc_stderr": 0.030147775935409224,
                    "acc_norm": 0.23880597014925373,
                    "acc_norm_stderr": 0.030147775935409224,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.03861229196653697,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.03861229196653697,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3072289156626506,
                    "acc_stderr": 0.03591566797824663,
                    "acc_norm": 0.3072289156626506,
                    "acc_norm_stderr": 0.03591566797824663,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.27485380116959063,
                    "acc_stderr": 0.034240429246915824,
                    "acc_norm": 0.27485380116959063,
                    "acc_norm_stderr": 0.034240429246915824,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.27050183598531213,
                    "mc1_stderr": 0.015550778332842897,
                    "mc2": 0.4565419940938498,
                    "mc2_stderr": 0.015175324004378129,
                    "timestamp": "2023-07-19T15-45-31.185140"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.0010486577181208054,
                    "em_stderr": 0.0003314581465219097,
                    "f1": 0.04059878355704704,
                    "f1_stderr": 0.0011641328961688674,
                    "timestamp": "2023-09-17T16-03-24.243188"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.009855951478392721,
                    "acc_stderr": 0.0027210765770416634,
                    "timestamp": "2023-09-17T16-03-24.243188"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5390686661404893,
                    "acc_stderr": 0.014009521680980306,
                    "timestamp": "2023-09-17T16-03-24.243188"
                }
            }
        }
    }
}