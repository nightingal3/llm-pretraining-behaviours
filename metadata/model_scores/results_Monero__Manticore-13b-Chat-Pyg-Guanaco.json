{
    "model_name": "Monero/Manticore-13b-Chat-Pyg-Guanaco",
    "last_updated": "2024-06-25 14:39:40.880629",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "em": 0.1636954697986577,
                    "em_stderr": 0.00378913611358371,
                    "f1": 0.25622378355704734,
                    "f1_stderr": 0.003909791858313052,
                    "timestamp": "2023-09-17T08-05-02.846180"
                }
            },
            "gsm8k": {
                "exact_match": 0.22896133434420016,
                "exact_match_stderr": 0.01157341289241821,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.739542225730071,
                    "acc_stderr": 0.012334833671998289,
                    "timestamp": "2023-09-17T08-05-02.846180"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.5290102389078498,
                    "acc_stderr": 0.014586776355294321,
                    "acc_norm": 0.568259385665529,
                    "acc_norm_stderr": 0.014474591427196199,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6149173471420036,
                    "acc_stderr": 0.004856203374715455,
                    "acc_norm": 0.823043218482374,
                    "acc_norm_stderr": 0.0038085217687699284,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.049236596391733084,
                    "acc_norm": 0.4,
                    "acc_norm_stderr": 0.049236596391733084,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.4740740740740741,
                    "acc_stderr": 0.04313531696750574,
                    "acc_norm": 0.4740740740740741,
                    "acc_norm_stderr": 0.04313531696750574,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.47368421052631576,
                    "acc_stderr": 0.04063302731486671,
                    "acc_norm": 0.47368421052631576,
                    "acc_norm_stderr": 0.04063302731486671,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.45,
                    "acc_stderr": 0.05,
                    "acc_norm": 0.45,
                    "acc_norm_stderr": 0.05,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.5056603773584906,
                    "acc_stderr": 0.030770900763851316,
                    "acc_norm": 0.5056603773584906,
                    "acc_norm_stderr": 0.030770900763851316,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.5069444444444444,
                    "acc_stderr": 0.04180806750294938,
                    "acc_norm": 0.5069444444444444,
                    "acc_norm_stderr": 0.04180806750294938,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.43,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.43,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621505,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621505,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.37572254335260113,
                    "acc_stderr": 0.036928207672648664,
                    "acc_norm": 0.37572254335260113,
                    "acc_norm_stderr": 0.036928207672648664,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.04220773659171452,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.04220773659171452,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.6,
                    "acc_stderr": 0.049236596391733084,
                    "acc_norm": 0.6,
                    "acc_norm_stderr": 0.049236596391733084,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.4127659574468085,
                    "acc_stderr": 0.03218471141400351,
                    "acc_norm": 0.4127659574468085,
                    "acc_norm_stderr": 0.03218471141400351,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.3508771929824561,
                    "acc_stderr": 0.044895393502707,
                    "acc_norm": 0.3508771929824561,
                    "acc_norm_stderr": 0.044895393502707,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.3793103448275862,
                    "acc_stderr": 0.040434618619167466,
                    "acc_norm": 0.3793103448275862,
                    "acc_norm_stderr": 0.040434618619167466,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.291005291005291,
                    "acc_stderr": 0.02339382650048487,
                    "acc_norm": 0.291005291005291,
                    "acc_norm_stderr": 0.02339382650048487,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.30158730158730157,
                    "acc_stderr": 0.04104947269903394,
                    "acc_norm": 0.30158730158730157,
                    "acc_norm_stderr": 0.04104947269903394,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.048241815132442176,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.048241815132442176,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.5129032258064516,
                    "acc_stderr": 0.028434533152681855,
                    "acc_norm": 0.5129032258064516,
                    "acc_norm_stderr": 0.028434533152681855,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.3448275862068966,
                    "acc_stderr": 0.03344283744280458,
                    "acc_norm": 0.3448275862068966,
                    "acc_norm_stderr": 0.03344283744280458,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.49,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.5515151515151515,
                    "acc_stderr": 0.03883565977956929,
                    "acc_norm": 0.5515151515151515,
                    "acc_norm_stderr": 0.03883565977956929,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.5808080808080808,
                    "acc_stderr": 0.03515520728670417,
                    "acc_norm": 0.5808080808080808,
                    "acc_norm_stderr": 0.03515520728670417,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.6839378238341969,
                    "acc_stderr": 0.03355397369686172,
                    "acc_norm": 0.6839378238341969,
                    "acc_norm_stderr": 0.03355397369686172,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.4128205128205128,
                    "acc_stderr": 0.024962683564331803,
                    "acc_norm": 0.4128205128205128,
                    "acc_norm_stderr": 0.024962683564331803,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2962962962962963,
                    "acc_stderr": 0.027840811495871937,
                    "acc_norm": 0.2962962962962963,
                    "acc_norm_stderr": 0.027840811495871937,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.48739495798319327,
                    "acc_stderr": 0.032468167657521745,
                    "acc_norm": 0.48739495798319327,
                    "acc_norm_stderr": 0.032468167657521745,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2582781456953642,
                    "acc_stderr": 0.035737053147634576,
                    "acc_norm": 0.2582781456953642,
                    "acc_norm_stderr": 0.035737053147634576,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.6293577981651376,
                    "acc_stderr": 0.02070745816435298,
                    "acc_norm": 0.6293577981651376,
                    "acc_norm_stderr": 0.02070745816435298,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.30092592592592593,
                    "acc_stderr": 0.03128039084329881,
                    "acc_norm": 0.30092592592592593,
                    "acc_norm_stderr": 0.03128039084329881,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.6323529411764706,
                    "acc_stderr": 0.03384132045674119,
                    "acc_norm": 0.6323529411764706,
                    "acc_norm_stderr": 0.03384132045674119,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.6371308016877637,
                    "acc_stderr": 0.031299208255302136,
                    "acc_norm": 0.6371308016877637,
                    "acc_norm_stderr": 0.031299208255302136,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.5919282511210763,
                    "acc_stderr": 0.03298574607842822,
                    "acc_norm": 0.5919282511210763,
                    "acc_norm_stderr": 0.03298574607842822,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.5267175572519084,
                    "acc_stderr": 0.04379024936553894,
                    "acc_norm": 0.5267175572519084,
                    "acc_norm_stderr": 0.04379024936553894,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.5702479338842975,
                    "acc_stderr": 0.04519082021319773,
                    "acc_norm": 0.5702479338842975,
                    "acc_norm_stderr": 0.04519082021319773,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.6018518518518519,
                    "acc_stderr": 0.04732332615978814,
                    "acc_norm": 0.6018518518518519,
                    "acc_norm_stderr": 0.04732332615978814,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.5214723926380368,
                    "acc_stderr": 0.03924746876751128,
                    "acc_norm": 0.5214723926380368,
                    "acc_norm_stderr": 0.03924746876751128,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.35714285714285715,
                    "acc_stderr": 0.04547960999764376,
                    "acc_norm": 0.35714285714285715,
                    "acc_norm_stderr": 0.04547960999764376,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.6407766990291263,
                    "acc_stderr": 0.04750458399041694,
                    "acc_norm": 0.6407766990291263,
                    "acc_norm_stderr": 0.04750458399041694,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.7222222222222222,
                    "acc_stderr": 0.029343114798094462,
                    "acc_norm": 0.7222222222222222,
                    "acc_norm_stderr": 0.029343114798094462,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.47,
                    "acc_stderr": 0.050161355804659205,
                    "acc_norm": 0.47,
                    "acc_norm_stderr": 0.050161355804659205,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.6551724137931034,
                    "acc_stderr": 0.01699712334611344,
                    "acc_norm": 0.6551724137931034,
                    "acc_norm_stderr": 0.01699712334611344,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.5317919075144508,
                    "acc_stderr": 0.02686462436675665,
                    "acc_norm": 0.5317919075144508,
                    "acc_norm_stderr": 0.02686462436675665,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.26256983240223464,
                    "acc_stderr": 0.014716824273017763,
                    "acc_norm": 0.26256983240223464,
                    "acc_norm_stderr": 0.014716824273017763,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.47058823529411764,
                    "acc_stderr": 0.028580341065138293,
                    "acc_norm": 0.47058823529411764,
                    "acc_norm_stderr": 0.028580341065138293,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.5080385852090032,
                    "acc_stderr": 0.02839442137098453,
                    "acc_norm": 0.5080385852090032,
                    "acc_norm_stderr": 0.02839442137098453,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.5648148148148148,
                    "acc_stderr": 0.027586006221607715,
                    "acc_norm": 0.5648148148148148,
                    "acc_norm_stderr": 0.027586006221607715,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.4078014184397163,
                    "acc_stderr": 0.029316011776343562,
                    "acc_norm": 0.4078014184397163,
                    "acc_norm_stderr": 0.029316011776343562,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.3878748370273794,
                    "acc_stderr": 0.01244499830967562,
                    "acc_norm": 0.3878748370273794,
                    "acc_norm_stderr": 0.01244499830967562,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.48161764705882354,
                    "acc_stderr": 0.03035230339535196,
                    "acc_norm": 0.48161764705882354,
                    "acc_norm_stderr": 0.03035230339535196,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.46405228758169936,
                    "acc_stderr": 0.02017548876548404,
                    "acc_norm": 0.46405228758169936,
                    "acc_norm_stderr": 0.02017548876548404,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.5636363636363636,
                    "acc_stderr": 0.04750185058907296,
                    "acc_norm": 0.5636363636363636,
                    "acc_norm_stderr": 0.04750185058907296,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.5224489795918368,
                    "acc_stderr": 0.03197694118713672,
                    "acc_norm": 0.5224489795918368,
                    "acc_norm_stderr": 0.03197694118713672,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.5671641791044776,
                    "acc_stderr": 0.03503490923673282,
                    "acc_norm": 0.5671641791044776,
                    "acc_norm_stderr": 0.03503490923673282,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.76,
                    "acc_stderr": 0.042923469599092816,
                    "acc_norm": 0.76,
                    "acc_norm_stderr": 0.042923469599092816,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.42168674698795183,
                    "acc_stderr": 0.03844453181770917,
                    "acc_norm": 0.42168674698795183,
                    "acc_norm_stderr": 0.03844453181770917,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.6842105263157895,
                    "acc_stderr": 0.03565079670708312,
                    "acc_norm": 0.6842105263157895,
                    "acc_norm_stderr": 0.03565079670708312,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.36474908200734396,
                    "mc1_stderr": 0.016850961061720113,
                    "mc2": 0.5228532383120019,
                    "mc2_stderr": 0.01587766921395582,
                    "timestamp": "2023-07-19T18-26-13.261313"
                }
            },
            "minerva_math_precalc": {
                "exact_match": 0.018315018315018316,
                "exact_match_stderr": 0.005743696731653661,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.06199770378874857,
                "exact_match_stderr": 0.00817579751206219,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.024074074074074074,
                "exact_match_stderr": 0.006602202509815387,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.021040974529346623,
                "exact_match_stderr": 0.004778723623319642,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "minerva_math_geometry": {
                "exact_match": 0.010438413361169102,
                "exact_match_stderr": 0.004648627117184662,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.04219409282700422,
                "exact_match_stderr": 0.009243448209077151,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "minerva_math_algebra": {
                "exact_match": 0.04296545914069082,
                "exact_match_stderr": 0.005888181524949664,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_3da": {
                "acc": 0.594,
                "acc_stderr": 0.010983729838291715,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_3ds": {
                "acc": 0.2335,
                "acc_stderr": 0.009462221822643413,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_4da": {
                "acc": 0.5165,
                "acc_stderr": 0.011177045144808303,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_2ds": {
                "acc": 0.21,
                "acc_stderr": 0.009109966835717524,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_5ds": {
                "acc": 0.0525,
                "acc_stderr": 0.004988418302285765,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_5da": {
                "acc": 0.2795,
                "acc_stderr": 0.010036944013122736,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_1dc": {
                "acc": 0.087,
                "acc_stderr": 0.0063035995814963615,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_4ds": {
                "acc": 0.192,
                "acc_stderr": 0.00880947236795147,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_2dm": {
                "acc": 0.0375,
                "acc_stderr": 0.004249223805764557,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "arithmetic_2da": {
                "acc": 0.652,
                "acc_stderr": 0.010653860914062438,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "gsm8k_cot": {
                "exact_match": 0.244882486732373,
                "exact_match_stderr": 0.011844819027863673,
                "timestamp": "2024-06-07T09-43-12.193200"
            },
            "anli_r2": {
                "brier_score": 0.8852013154077675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "anli_r3": {
                "brier_score": 0.7816093138893003,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "anli_r1": {
                "brier_score": 0.8665860062055024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_eu": {
                "brier_score": 1.295309492627305,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_vi": {
                "brier_score": 1.0284578721600939,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_ru": {
                "brier_score": 0.9424584802883822,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_zh": {
                "brier_score": 1.0732202227433019,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_tr": {
                "brier_score": 1.1494729899079483,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_fr": {
                "brier_score": 0.8836516075827534,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_en": {
                "brier_score": 0.8049487003810644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_ur": {
                "brier_score": 1.2526672854095398,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_ar": {
                "brier_score": 1.249788260932463,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_de": {
                "brier_score": 0.9308427131328507,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_hi": {
                "brier_score": 1.014295071171663,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_es": {
                "brier_score": 0.932183728871887,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_bg": {
                "brier_score": 0.9824751588952393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_sw": {
                "brier_score": 1.0143704119021475,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_el": {
                "brier_score": 1.021583604470639,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "xnli_th": {
                "brier_score": 1.1706481918320806,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "logiqa2": {
                "brier_score": 1.0546915532913335,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "mathqa": {
                "brier_score": 0.998681916345562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T09-59-09.061295"
            },
            "lambada_standard": {
                "perplexity": 4.071811300991231,
                "perplexity_stderr": 0.11908741129946793,
                "acc": 0.6567048321366195,
                "acc_stderr": 0.006615017904433671,
                "timestamp": "2024-06-07T10-01-08.919778"
            },
            "lambada_openai": {
                "perplexity": 3.103419189766064,
                "perplexity_stderr": 0.08087798156130009,
                "acc": 0.7155055307587813,
                "acc_stderr": 0.0062857265569445,
                "timestamp": "2024-06-07T10-01-08.919778"
            }
        }
    }
}