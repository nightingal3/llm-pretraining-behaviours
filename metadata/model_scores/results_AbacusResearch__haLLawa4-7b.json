{
    "model_name": "AbacusResearch/haLLawa4-7b",
    "last_updated": "2024-12-04 11:22:48.052429",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.08791208791208792,
                    "acc_stderr": 0.012129541821143602,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.40642939150401836,
                    "acc_stderr": 0.016652104255062996,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.15,
                    "acc_stderr": 0.01538015491211301,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.09302325581395349,
                    "acc_stderr": 0.009671427372356392,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.17745302713987474,
                    "acc_stderr": 0.01747463495280595,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.21729957805907174,
                    "acc_stderr": 0.01896254634032935,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.32940185341196293,
                    "acc_stderr": 0.013647460597468906,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.9845,
                    "acc_stderr": 0.002762913651550293,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.985,
                    "acc_stderr": 0.002718675338799954,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.948,
                    "acc_stderr": 0.004965916850399518,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.9955,
                    "acc_stderr": 0.0014969954902233232,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.901,
                    "acc_stderr": 0.006679955905951291,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.916,
                    "acc_stderr": 0.006204131335071236,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.757,
                    "acc_stderr": 0.009592784306726521,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.951,
                    "acc_stderr": 0.004828162753862956,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.6645,
                    "acc_stderr": 0.010560569957105102,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 1.0,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.7581501137225171,
                    "acc_stderr": 0.011794861371318702,
                    "timestamp": "2024-06-10T14-32-19.288753"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.7429871114480667,
                    "acc_stderr": 0.012036781757428677,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "anli_r2": {
                "0-shot": {
                    "brier_score": 0.7666504741725055,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "anli_r3": {
                "0-shot": {
                    "brier_score": 0.8663025386598154,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "anli_r1": {
                "0-shot": {
                    "brier_score": 0.6573805818915781,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_eu": {
                "0-shot": {
                    "brier_score": 1.1139982065153322,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_vi": {
                "0-shot": {
                    "brier_score": 0.9999180808359713,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_ru": {
                "0-shot": {
                    "brier_score": 0.9538905561506925,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_zh": {
                "0-shot": {
                    "brier_score": 1.0601931277652965,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_tr": {
                "0-shot": {
                    "brier_score": 1.0467850390267222,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_fr": {
                "0-shot": {
                    "brier_score": 0.9334503115142729,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_en": {
                "0-shot": {
                    "brier_score": 0.7398112895767275,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_ur": {
                "0-shot": {
                    "brier_score": 1.2069939556444398,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_ar": {
                "0-shot": {
                    "brier_score": 1.1995508224164844,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_de": {
                "0-shot": {
                    "brier_score": 0.9119484180329919,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_hi": {
                "0-shot": {
                    "brier_score": 0.9461357371948407,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_es": {
                "0-shot": {
                    "brier_score": 0.9585943286675973,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_bg": {
                "0-shot": {
                    "brier_score": 0.9511880273157712,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_sw": {
                "0-shot": {
                    "brier_score": 1.0399624826911418,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_el": {
                "0-shot": {
                    "brier_score": 0.9893964628647417,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "xnli_th": {
                "0-shot": {
                    "brier_score": 1.1061854373450353,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "brier_score": 0.9626222319450617,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "mathqa": {
                "5-shot": {
                    "brier_score": 0.9820782094042059,
                    "timestamp": "2024-06-10T14-43-21.034728"
                }
            },
            "lambada_standard": {
                "0-shot": {
                    "perplexity": 4.423249636812455,
                    "perplexity_stderr": 0.11448351786225823,
                    "acc": 0.6367164758393169,
                    "acc_stderr": 0.00670051166747681,
                    "timestamp": "2024-06-10T14-44-51.557853"
                }
            },
            "lambada_openai": {
                "0-shot": {
                    "perplexity": 3.629649594000675,
                    "perplexity_stderr": 0.08434195320266337,
                    "acc": 0.6959052978847273,
                    "acc_stderr": 0.006409019178962834,
                    "timestamp": "2024-06-10T14-44-51.557853"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.8304093567251462,
                    "acc_stderr": 0.028782108105401712,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.48412698412698413,
                    "acc_stderr": 0.04469881854072606,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.7253086419753086,
                    "acc_stderr": 0.024836057868294674,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.42905027932960893,
                    "acc_stderr": 0.01655328786311604,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.8059071729957806,
                    "acc_stderr": 0.025744902532290927,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.7312138728323699,
                    "acc_stderr": 0.023868003262500118,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.470013037809648,
                    "acc_stderr": 0.012747248967079055,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.7668711656441718,
                    "acc_stderr": 0.033220157957767414,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.8627450980392157,
                    "acc_stderr": 0.024152225962801584,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.7009646302250804,
                    "acc_stderr": 0.026003301117885142,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.7870370370370371,
                    "acc_stderr": 0.039578354719809784,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.768595041322314,
                    "acc_stderr": 0.038498560987940904,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.7636363636363637,
                    "acc_stderr": 0.03317505930009181,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.9015544041450777,
                    "acc_stderr": 0.021500249576033467,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.680672268907563,
                    "acc_stderr": 0.0302839955258844,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.7929292929292929,
                    "acc_stderr": 0.028869778460267073,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.8311926605504587,
                    "acc_stderr": 0.016060056268530364,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.6818181818181818,
                    "acc_stderr": 0.04461272175910508,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.03487350880197768,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.8407960199004975,
                    "acc_stderr": 0.02587064676616913,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.6692307692307692,
                    "acc_stderr": 0.02385479568097112,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.7387755102040816,
                    "acc_stderr": 0.028123429335142783,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.6748366013071896,
                    "acc_stderr": 0.01895088677080631,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.8091603053435115,
                    "acc_stderr": 0.03446513350752599,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.4824561403508772,
                    "acc_stderr": 0.04700708033551038,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.8314176245210728,
                    "acc_stderr": 0.013387895731543602,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.8760683760683761,
                    "acc_stderr": 0.021586494001281376,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.7766990291262136,
                    "acc_stderr": 0.04123553189891431,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.7320261437908496,
                    "acc_stderr": 0.025360603796242557,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.7,
                    "acc_stderr": 0.046056618647183814,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.6816143497757847,
                    "acc_stderr": 0.03126580522513713,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.6727941176470589,
                    "acc_stderr": 0.02850145286039657,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.6589595375722543,
                    "acc_stderr": 0.036146654241808254,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.63,
                    "acc_stderr": 0.048523658709390974,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.6830188679245283,
                    "acc_stderr": 0.028637235639800893,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.04824181513244218,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.5180722891566265,
                    "acc_stderr": 0.038899512528272166,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.4929078014184397,
                    "acc_stderr": 0.02982449855912901,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.45098039215686275,
                    "acc_stderr": 0.04951218252396262,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.37748344370860926,
                    "acc_stderr": 0.0395802723112157,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.7806451612903226,
                    "acc_stderr": 0.0235407993587233,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.03621034121889507,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.6148148148148148,
                    "acc_stderr": 0.042039210401562783,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.45,
                    "acc_stderr": 0.049999999999999996,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.04351941398892446,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.59,
                    "acc_stderr": 0.04943110704237101,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.7105263157894737,
                    "acc_stderr": 0.036906779861372814,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.5659574468085107,
                    "acc_stderr": 0.032400380867927465,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.35,
                    "acc_stderr": 0.0479372485441102,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.7,
                    "acc_stderr": 0.046056618647183814,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.4017857142857143,
                    "acc_stderr": 0.04653333146973646,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.5123152709359606,
                    "acc_stderr": 0.035169204442208966,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.5138888888888888,
                    "acc_stderr": 0.034086558679777494,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.3968253968253968,
                    "acc_stderr": 0.025197101074246487,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.5793103448275863,
                    "acc_stderr": 0.0411391498118926,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.31851851851851853,
                    "acc_stderr": 0.02840653309060846,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.7030716723549488,
                    "acc_stderr": 0.013352025976725223,
                    "acc_norm": 0.7235494880546075,
                    "acc_norm_stderr": 0.013069662474252423,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.7102170882294364,
                    "acc_stderr": 0.0045273436511307445,
                    "acc_norm": 0.8841864170483967,
                    "acc_norm_stderr": 0.003193472530282367,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.7478686028704679,
                    "acc_stderr": 0.014178044720511106,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 15.495195703574323,
                    "bleu_max_stderr": 0.5680370072696724,
                    "bleu_acc": 0.5263157894736842,
                    "bleu_acc_stderr": 0.01747924116197544,
                    "bleu_diff": 2.2914445163776342,
                    "bleu_diff_stderr": 0.44265660441282384,
                    "rouge1_max": 41.476176909090015,
                    "rouge1_max_stderr": 0.7163761236938211,
                    "rouge1_acc": 0.5703794369645043,
                    "rouge1_acc_stderr": 0.01732923458040911,
                    "rouge1_diff": 4.5715005574824925,
                    "rouge1_diff_stderr": 0.6975679934842908,
                    "rouge2_max": 26.293465437291108,
                    "rouge2_max_stderr": 0.7848741256847009,
                    "rouge2_acc": 0.4638922888616891,
                    "rouge2_acc_stderr": 0.01745780042226862,
                    "rouge2_diff": 3.935472582571117,
                    "rouge2_diff_stderr": 0.7546985023903435,
                    "rougeL_max": 37.83478014787854,
                    "rougeL_max_stderr": 0.7279950256366058,
                    "rougeL_acc": 0.5385556915544676,
                    "rougeL_acc_stderr": 0.017451384104637452,
                    "rougeL_diff": 4.12214109311066,
                    "rougeL_diff_stderr": 0.7002705028145758,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.5936352509179926,
                    "acc_stderr": 0.017193835812093907,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.8263614838200474,
                    "acc_stderr": 0.010646116480331005,
                    "timestamp": "2024-11-12T06-36-11.395928"
                }
            }
        }
    }
}