{
    "model_name": "mosaicml/mpt-7b-storywriter",
    "last_updated": "2024-12-19 13:38:31.178657",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "minerva_math_geometry": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "minerva_math_algebra": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_3da": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339458,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_2ds": {
                "acc": 0.0125,
                "acc_stderr": 0.00248494717876267,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_1dc": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_2dm": {
                "acc": 0.0265,
                "acc_stderr": 0.003592398594787676,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "arithmetic_2da": {
                "acc": 0.007,
                "acc_stderr": 0.0018647355360237512,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "gsm8k_cot": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-09T18-32-43.407050"
            },
            "gsm8k": {
                "exact_match": 0.05155420773313116,
                "exact_match_stderr": 0.00609088795526282,
                "timestamp": "2024-11-24T14-31-05.387078"
            },
            "anli_r2": {
                "brier_score": 0.8579585824630889,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "anli_r3": {
                "brier_score": 0.8532457528157426,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "anli_r1": {
                "brier_score": 0.8446244791915563,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_eu": {
                "brier_score": 1.2168588145018897,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_vi": {
                "brier_score": 1.2991601321455832,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_ru": {
                "brier_score": 1.2753960909711017,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_zh": {
                "brier_score": 1.0743836903664954,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_tr": {
                "brier_score": 1.1927998634673966,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_fr": {
                "brier_score": 1.290251935664798,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_en": {
                "brier_score": 0.8772257385108837,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_ur": {
                "brier_score": 1.2785277549537262,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_ar": {
                "brier_score": 0.9907536655140208,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_de": {
                "brier_score": 1.2274392301166228,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_hi": {
                "brier_score": 1.1088702072239738,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_es": {
                "brier_score": 1.2244126501923114,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_bg": {
                "brier_score": 1.0445857377325605,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_sw": {
                "brier_score": 1.084639887151233,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_el": {
                "brier_score": 1.1457153222036922,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "xnli_th": {
                "brier_score": 1.301506120005153,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "logiqa2": {
                "brier_score": 1.4689876462916187,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "mathqa": {
                "brier_score": 1.0079977099532107,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-09T18-42-36.450140"
            },
            "lambada_standard": {
                "perplexity": 202.229010197755,
                "perplexity_stderr": 9.46813064432611,
                "acc": 0.11449640985833495,
                "acc_stderr": 0.004436118825118391,
                "timestamp": "2024-06-09T18-43-59.272112"
            },
            "lambada_openai": {
                "perplexity": 176.61298095244155,
                "perplexity_stderr": 9.78176772928432,
                "acc": 0.17659615757810984,
                "acc_stderr": 0.005312624764825889,
                "timestamp": "2024-06-09T18-43-59.272112"
            },
            "mmlu_world_religions": {
                "acc": 0.26900584795321636,
                "acc_stderr": 0.03401052620104089,
                "brier_score": 0.753680202481827,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.03670066451047182,
                "brier_score": 0.8038510938196719,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_prehistory": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.024383665531035454,
                "brier_score": 0.7429447897247751,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2424581005586592,
                "acc_stderr": 0.014333522059217887,
                "brier_score": 0.7936312220620956,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.23628691983122363,
                "acc_stderr": 0.02765215314415925,
                "brier_score": 0.7641102367810089,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26878612716763006,
                "acc_stderr": 0.023868003262500114,
                "brier_score": 0.7481473957025903,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_professional_law": {
                "acc": 0.2757496740547588,
                "acc_stderr": 0.01141381360916099,
                "brier_score": 0.7599821212753395,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.26380368098159507,
                "acc_stderr": 0.034624199316156234,
                "brier_score": 0.7489141116387855,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25980392156862747,
                "acc_stderr": 0.03077855467869326,
                "brier_score": 0.758159885868019,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_philosophy": {
                "acc": 0.3183279742765273,
                "acc_stderr": 0.026457225067811032,
                "brier_score": 0.7371383091149315,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_jurisprudence": {
                "acc": 0.3148148148148148,
                "acc_stderr": 0.04489931073591311,
                "brier_score": 0.740167746960495,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_international_law": {
                "acc": 0.34710743801652894,
                "acc_stderr": 0.04345724570292535,
                "brier_score": 0.7215996248957431,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.30303030303030304,
                "acc_stderr": 0.03588624800091709,
                "brier_score": 0.7488523500000018,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.26424870466321243,
                "acc_stderr": 0.03182155050916649,
                "brier_score": 0.761883170110334,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.02665353159671549,
                "brier_score": 0.768040669440023,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_geography": {
                "acc": 0.20202020202020202,
                "acc_stderr": 0.028606204289229872,
                "brier_score": 0.776820768579571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.22935779816513763,
                "acc_stderr": 0.018025349724618684,
                "brier_score": 0.7668421972927677,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_public_relations": {
                "acc": 0.3,
                "acc_stderr": 0.04389311454644287,
                "brier_score": 0.7466252547843161,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7420782541244603,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_sociology": {
                "acc": 0.30845771144278605,
                "acc_stderr": 0.032658195885126966,
                "brier_score": 0.7544591020949951,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2358974358974359,
                "acc_stderr": 0.021525965407408726,
                "brier_score": 0.7728504872846409,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_security_studies": {
                "acc": 0.2163265306122449,
                "acc_stderr": 0.026358916334904035,
                "brier_score": 0.7685743597613134,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_professional_psychology": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.017630827375148383,
                "brier_score": 0.754200107568594,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_human_sexuality": {
                "acc": 0.22900763358778625,
                "acc_stderr": 0.036853466317118506,
                "brier_score": 0.7672084282956686,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.04049339297748142,
                "brier_score": 0.7884101694016207,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_miscellaneous": {
                "acc": 0.3243933588761175,
                "acc_stderr": 0.016740929047162702,
                "brier_score": 0.7347917320598347,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_marketing": {
                "acc": 0.2692307692307692,
                "acc_stderr": 0.02905858830374884,
                "brier_score": 0.7496901398504675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_management": {
                "acc": 0.2524271844660194,
                "acc_stderr": 0.04301250399690878,
                "brier_score": 0.7663697778928393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_nutrition": {
                "acc": 0.238562091503268,
                "acc_stderr": 0.024404394928087873,
                "brier_score": 0.772850127762166,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_medical_genetics": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.768684437705436,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_human_aging": {
                "acc": 0.3542600896860987,
                "acc_stderr": 0.032100621541349864,
                "brier_score": 0.7316352696315931,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_professional_medicine": {
                "acc": 0.19852941176470587,
                "acc_stderr": 0.024231013370541076,
                "brier_score": 0.7894777255106187,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_college_medicine": {
                "acc": 0.2832369942196532,
                "acc_stderr": 0.03435568056047874,
                "brier_score": 0.7649221885589982,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_business_ethics": {
                "acc": 0.33,
                "acc_stderr": 0.047258156262526045,
                "brier_score": 0.7479446150506702,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.26037735849056604,
                "acc_stderr": 0.0270087660907081,
                "brier_score": 0.7615723376439071,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_global_facts": {
                "acc": 0.39,
                "acc_stderr": 0.04902071300001974,
                "brier_score": 0.7120079089893042,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_virology": {
                "acc": 0.3313253012048193,
                "acc_stderr": 0.03664314777288086,
                "brier_score": 0.7541565314631925,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2907801418439716,
                "acc_stderr": 0.027090664368353178,
                "brier_score": 0.7545586416754995,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_college_physics": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.0438986995680878,
                "brier_score": 0.7868482264949791,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_physics": {
                "acc": 0.23841059602649006,
                "acc_stderr": 0.03479185572599661,
                "brier_score": 0.7700271734827929,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_biology": {
                "acc": 0.3032258064516129,
                "acc_stderr": 0.026148685930671742,
                "brier_score": 0.7505801621274838,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "brier_score": 0.7704275393486419,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_anatomy": {
                "acc": 0.28888888888888886,
                "acc_stderr": 0.0391545063041425,
                "brier_score": 0.7486436018611725,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_college_chemistry": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036623,
                "brier_score": 0.8148346665464601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_computer_security": {
                "acc": 0.33,
                "acc_stderr": 0.047258156262526045,
                "brier_score": 0.7270989693573726,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_college_computer_science": {
                "acc": 0.12,
                "acc_stderr": 0.03265986323710906,
                "brier_score": 0.8072218865613245,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_astronomy": {
                "acc": 0.26973684210526316,
                "acc_stderr": 0.03611780560284898,
                "brier_score": 0.7628450006384498,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_college_mathematics": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.7757384831926484,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.32340425531914896,
                "acc_stderr": 0.03057944277361035,
                "brier_score": 0.7454022711302398,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7519858148417691,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.27,
                "acc_stderr": 0.044619604333847394,
                "brier_score": 0.7488524750876776,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_machine_learning": {
                "acc": 0.2767857142857143,
                "acc_stderr": 0.04246624336697624,
                "brier_score": 0.7810329226775689,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2561576354679803,
                "acc_stderr": 0.0307127300709826,
                "brier_score": 0.7607518115873994,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1712962962962963,
                "acc_stderr": 0.025695341643824685,
                "brier_score": 0.800553431750079,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2619047619047619,
                "acc_stderr": 0.022644212615525218,
                "brier_score": 0.7751121777807123,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2896551724137931,
                "acc_stderr": 0.03780019230438014,
                "brier_score": 0.7521340858384736,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.25555555555555554,
                "acc_stderr": 0.02659393910184407,
                "brier_score": 0.775342769692641,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-10-46.853781"
            },
            "arc_challenge": {
                "acc": 0.4274744027303754,
                "acc_stderr": 0.014456862944650659,
                "acc_norm": 0.46075085324232085,
                "acc_norm_stderr": 0.014566303676636584,
                "timestamp": "2024-11-24T14-31-05.387078"
            },
            "hellaswag": {
                "acc": 0.5461063533160725,
                "acc_stderr": 0.004968521608065447,
                "acc_norm": 0.7425811591316471,
                "acc_norm_stderr": 0.004363185172047184,
                "timestamp": "2024-11-24T14-31-05.387078"
            },
            "truthfulqa_mc2": {
                "acc": 0.35969412975276555,
                "acc_stderr": 0.013513306511175506,
                "timestamp": "2024-11-24T14-31-05.387078"
            },
            "truthfulqa_gen": {
                "bleu_max": 17.977525977108503,
                "bleu_max_stderr": 0.7695541132431905,
                "bleu_acc": 0.2350061199510404,
                "bleu_acc_stderr": 0.014843061507731604,
                "bleu_diff": -5.0966333287389975,
                "bleu_diff_stderr": 0.721249793575583,
                "rouge1_max": 35.04312245719028,
                "rouge1_max_stderr": 1.0996506075148464,
                "rouge1_acc": 0.19216646266829865,
                "rouge1_acc_stderr": 0.013792870480628949,
                "rouge1_diff": -6.663656074389997,
                "rouge1_diff_stderr": 0.8025282580401039,
                "rouge2_max": 24.121071934633303,
                "rouge2_max_stderr": 1.0180145363654554,
                "rouge2_acc": 0.17135862913096694,
                "rouge2_acc_stderr": 0.013191409923739371,
                "rouge2_diff": -7.719271942054364,
                "rouge2_diff_stderr": 0.953796284275681,
                "rougeL_max": 33.244148594030946,
                "rougeL_max_stderr": 1.075159041310315,
                "rougeL_acc": 0.19216646266829865,
                "rougeL_acc_stderr": 0.01379287048062895,
                "rougeL_diff": -6.790992534457851,
                "rougeL_diff_stderr": 0.8170761181979462,
                "timestamp": "2024-11-24T14-31-05.387078"
            },
            "truthfulqa_mc1": {
                "acc": 0.21542227662178703,
                "acc_stderr": 0.014391902652427683,
                "timestamp": "2024-11-24T14-31-05.387078"
            },
            "winogrande": {
                "acc": 0.6827150749802684,
                "acc_stderr": 0.01308059841133212,
                "timestamp": "2024-11-24T14-31-05.387078"
            }
        }
    }
}