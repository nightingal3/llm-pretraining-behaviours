{
    "model_name": "cerebras/Cerebras-GPT-6.7B",
    "last_updated": "2024-12-19 13:42:07.428570",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.01098901098901099,
                "exact_match_stderr": 0.004465618427331413,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.021814006888633754,
                "exact_match_stderr": 0.0049524353688735236,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.022222222222222223,
                "exact_match_stderr": 0.006349206349206325,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.018826135105204873,
                "exact_match_stderr": 0.004525330498668476,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.0058614624258180245,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.012658227848101266,
                "exact_match_stderr": 0.005140313889578842,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_algebra": {
                "exact_match": 0.015164279696714406,
                "exact_match_stderr": 0.003548546043132535,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521528,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_2ds": {
                "acc": 0.016,
                "acc_stderr": 0.002806410156941534,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_1dc": {
                "acc": 0.033,
                "acc_stderr": 0.003995432609977371,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_2dm": {
                "acc": 0.0225,
                "acc_stderr": 0.003316982994845522,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_2da": {
                "acc": 0.0195,
                "acc_stderr": 0.003092678018912424,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "gsm8k_cot": {
                "exact_match": 0.032600454890068235,
                "exact_match_stderr": 0.00489166902193957,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "gsm8k": {
                "exact_match": 0.01819560272934041,
                "exact_match_stderr": 0.0036816118940738727,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "anli_r2": {
                "brier_score": 0.755675118118756,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "anli_r3": {
                "brier_score": 0.6999705904132336,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "anli_r1": {
                "brier_score": 0.7536494384689287,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_eu": {
                "brier_score": 1.089933422701538,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_vi": {
                "brier_score": 0.7966443228842566,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_ru": {
                "brier_score": 0.7989806201813834,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_zh": {
                "brier_score": 1.0334911487693128,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_tr": {
                "brier_score": 0.839021250171503,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_fr": {
                "brier_score": 0.8071513485603421,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_en": {
                "brier_score": 0.6656814608095787,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_ur": {
                "brier_score": 0.9428490629298696,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_ar": {
                "brier_score": 1.0302855031988798,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_de": {
                "brier_score": 0.8364057760607875,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_hi": {
                "brier_score": 0.7590215516967309,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_es": {
                "brier_score": 0.8316353906107824,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_bg": {
                "brier_score": 0.8758042326498179,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_sw": {
                "brier_score": 0.8831334480577933,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_el": {
                "brier_score": 0.8987268713940024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_th": {
                "brier_score": 0.8062685621961146,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "logiqa2": {
                "brier_score": 1.1489525363355495,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "mathqa": {
                "brier_score": 0.9586676691784485,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "lambada_standard": {
                "perplexity": 7.7343856575560315,
                "perplexity_stderr": 0.2042279549336044,
                "acc": 0.548418397050262,
                "acc_stderr": 0.006933239470474408,
                "timestamp": "2024-06-11T16-09-16.235478"
            },
            "lambada_openai": {
                "perplexity": 5.350437725308013,
                "perplexity_stderr": 0.13313966140293174,
                "acc": 0.6367164758393169,
                "acc_stderr": 0.0067005116674768135,
                "timestamp": "2024-06-11T16-09-16.235478"
            },
            "mmlu_world_religions": {
                "acc": 0.2046783625730994,
                "acc_stderr": 0.03094445977853321,
                "brier_score": 0.7957314835738306,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_formal_logic": {
                "acc": 0.25396825396825395,
                "acc_stderr": 0.03893259610604674,
                "brier_score": 0.777821546810293,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_prehistory": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.02465968518596728,
                "brier_score": 0.7605214100168148,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.27039106145251396,
                "acc_stderr": 0.014854993938010085,
                "brier_score": 0.7743030187987928,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2489451476793249,
                "acc_stderr": 0.028146970599422644,
                "brier_score": 0.7759565012926257,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2543352601156069,
                "acc_stderr": 0.02344582627654555,
                "brier_score": 0.7612048282171314,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_professional_law": {
                "acc": 0.2620599739243807,
                "acc_stderr": 0.011231552795890394,
                "brier_score": 0.7637414528130592,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.27607361963190186,
                "acc_stderr": 0.0351238528370505,
                "brier_score": 0.7592525580081232,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.23039215686274508,
                "acc_stderr": 0.029554292605695066,
                "brier_score": 0.7726198780482679,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_philosophy": {
                "acc": 0.21864951768488747,
                "acc_stderr": 0.023475581417861106,
                "brier_score": 0.7673678637362098,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_jurisprudence": {
                "acc": 0.3055555555555556,
                "acc_stderr": 0.044531975073749834,
                "brier_score": 0.7565128770198866,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_international_law": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.04065578140908705,
                "brier_score": 0.7638130866066336,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.24242424242424243,
                "acc_stderr": 0.03346409881055953,
                "brier_score": 0.7687477534580277,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21243523316062177,
                "acc_stderr": 0.02951928261681725,
                "brier_score": 0.7649253497346248,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23949579831932774,
                "acc_stderr": 0.027722065493361276,
                "brier_score": 0.7648537684862107,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_geography": {
                "acc": 0.25252525252525254,
                "acc_stderr": 0.030954055470365907,
                "brier_score": 0.7515992458403655,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.24770642201834864,
                "acc_stderr": 0.018508143602547815,
                "brier_score": 0.7572896033283785,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_public_relations": {
                "acc": 0.34545454545454546,
                "acc_stderr": 0.04554619617541054,
                "brier_score": 0.7489090104905242,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "brier_score": 0.7485337935026642,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_sociology": {
                "acc": 0.22885572139303484,
                "acc_stderr": 0.029705284056772443,
                "brier_score": 0.7896160133465063,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2358974358974359,
                "acc_stderr": 0.021525965407408726,
                "brier_score": 0.7592758243699075,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_security_studies": {
                "acc": 0.22040816326530613,
                "acc_stderr": 0.02653704531214531,
                "brier_score": 0.7651608116817085,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_professional_psychology": {
                "acc": 0.24836601307189543,
                "acc_stderr": 0.017479487001364764,
                "brier_score": 0.7626463494252116,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_human_sexuality": {
                "acc": 0.25190839694656486,
                "acc_stderr": 0.03807387116306086,
                "brier_score": 0.7803830593272657,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_econometrics": {
                "acc": 0.2719298245614035,
                "acc_stderr": 0.041857744240220575,
                "brier_score": 0.7678727754466477,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_miscellaneous": {
                "acc": 0.2771392081736909,
                "acc_stderr": 0.016005636294122418,
                "brier_score": 0.7670373181589404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_marketing": {
                "acc": 0.2606837606837607,
                "acc_stderr": 0.028760348956523414,
                "brier_score": 0.779345195412505,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_management": {
                "acc": 0.24271844660194175,
                "acc_stderr": 0.04245022486384495,
                "brier_score": 0.7577522389013621,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_nutrition": {
                "acc": 0.24183006535947713,
                "acc_stderr": 0.024518195641879334,
                "brier_score": 0.7696166584432055,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_medical_genetics": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "brier_score": 0.7349891487761616,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_human_aging": {
                "acc": 0.2914798206278027,
                "acc_stderr": 0.030500283176545896,
                "brier_score": 0.7480796284249333,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4411764705882353,
                "acc_stderr": 0.030161911930767102,
                "brier_score": 0.7181797736284031,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_college_medicine": {
                "acc": 0.2774566473988439,
                "acc_stderr": 0.03414014007044036,
                "brier_score": 0.7563416289746194,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_business_ethics": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7785417779353457,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2792452830188679,
                "acc_stderr": 0.027611163402399715,
                "brier_score": 0.7548416769314967,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_global_facts": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7468815557293845,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_virology": {
                "acc": 0.29518072289156627,
                "acc_stderr": 0.0355092018568963,
                "brier_score": 0.7496600717109347,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_professional_accounting": {
                "acc": 0.25177304964539005,
                "acc_stderr": 0.025892151156709405,
                "brier_score": 0.7673164867592129,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_college_physics": {
                "acc": 0.28431372549019607,
                "acc_stderr": 0.04488482852329017,
                "brier_score": 0.7516552649663509,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2582781456953642,
                "acc_stderr": 0.035737053147634576,
                "brier_score": 0.7498692234299653,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_biology": {
                "acc": 0.267741935483871,
                "acc_stderr": 0.025189006660212385,
                "brier_score": 0.7474022289330349,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_college_biology": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.03685651095897532,
                "brier_score": 0.7548767358146407,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_anatomy": {
                "acc": 0.24444444444444444,
                "acc_stderr": 0.037125378336148665,
                "brier_score": 0.7607710979577496,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_college_chemistry": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7695601415588362,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_computer_security": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 0.780396000142865,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_college_computer_science": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7623333845129798,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_astronomy": {
                "acc": 0.23026315789473684,
                "acc_stderr": 0.03426059424403165,
                "brier_score": 0.773281846138913,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_college_mathematics": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909283,
                "brier_score": 0.7632750997930634,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.28085106382978725,
                "acc_stderr": 0.029379170464124815,
                "brier_score": 0.7470199702981599,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 0.7659346829135254,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816505,
                "brier_score": 0.7693477530508929,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_machine_learning": {
                "acc": 0.20535714285714285,
                "acc_stderr": 0.03834241021419073,
                "brier_score": 0.7733834786097267,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.29064039408866993,
                "acc_stderr": 0.0319474007226554,
                "brier_score": 0.7443894806007804,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.25462962962962965,
                "acc_stderr": 0.029711275860005357,
                "brier_score": 0.7529905203543267,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.24603174603174602,
                "acc_stderr": 0.022182037202948368,
                "brier_score": 0.7755612883830363,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.3103448275862069,
                "acc_stderr": 0.038552896163789485,
                "brier_score": 0.7485238433258922,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2814814814814815,
                "acc_stderr": 0.027420019350945273,
                "brier_score": 0.7568005107125074,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-35-15.687187"
            },
            "arc_challenge": {
                "acc": 0.3242320819112628,
                "acc_stderr": 0.013678810399518813,
                "acc_norm": 0.35238907849829354,
                "acc_norm_stderr": 0.01396014260059869,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "hellaswag": {
                "acc": 0.4477195777733519,
                "acc_stderr": 0.004962429881904026,
                "acc_norm": 0.594901414060944,
                "acc_norm_stderr": 0.004899078300184236,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "truthfulqa_mc2": {
                "acc": 0.3803445214836313,
                "acc_stderr": 0.01392905684581036,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "truthfulqa_gen": {
                "bleu_max": 22.34064093494146,
                "bleu_max_stderr": 0.7183987511971597,
                "bleu_acc": 0.2741738066095471,
                "bleu_acc_stderr": 0.015616518497219367,
                "bleu_diff": -7.446457636161483,
                "bleu_diff_stderr": 0.7565638438274547,
                "rouge1_max": 48.16517696338372,
                "rouge1_max_stderr": 0.8420427437367002,
                "rouge1_acc": 0.2717258261933905,
                "rouge1_acc_stderr": 0.01557284045287583,
                "rouge1_diff": -8.43929405627723,
                "rouge1_diff_stderr": 0.823148680564784,
                "rouge2_max": 31.457102690393747,
                "rouge2_max_stderr": 0.9472192452069558,
                "rouge2_acc": 0.22888616891064872,
                "rouge2_acc_stderr": 0.014706994909055027,
                "rouge2_diff": -10.247194848412033,
                "rouge2_diff_stderr": 0.9595694112827735,
                "rougeL_max": 44.8539232504777,
                "rougeL_max_stderr": 0.8545145349902917,
                "rougeL_acc": 0.25703794369645044,
                "rougeL_acc_stderr": 0.015298077509485083,
                "rougeL_diff": -8.674604971448174,
                "rougeL_diff_stderr": 0.8352252105721707,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "truthfulqa_mc1": {
                "acc": 0.24357405140758873,
                "acc_stderr": 0.015026354824910782,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "winogrande": {
                "acc": 0.5966850828729282,
                "acc_stderr": 0.01378725728589624,
                "timestamp": "2024-11-21T21-50-03.007594"
            }
        }
    }
}