{
    "model_name": "cerebras/Cerebras-GPT-6.7B",
    "last_updated": "2024-12-04 11:25:50.673900",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.01098901098901099,
                "exact_match_stderr": 0.004465618427331413,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.021814006888633754,
                "exact_match_stderr": 0.0049524353688735236,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.022222222222222223,
                "exact_match_stderr": 0.006349206349206325,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.018826135105204873,
                "exact_match_stderr": 0.004525330498668476,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_geometry": {
                "exact_match": 0.016701461377870562,
                "exact_match_stderr": 0.0058614624258180245,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.012658227848101266,
                "exact_match_stderr": 0.005140313889578842,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "minerva_math_algebra": {
                "exact_match": 0.015164279696714406,
                "exact_match_stderr": 0.003548546043132535,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521528,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_3ds": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521539,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_2ds": {
                "acc": 0.016,
                "acc_stderr": 0.002806410156941534,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_1dc": {
                "acc": 0.033,
                "acc_stderr": 0.003995432609977371,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_2dm": {
                "acc": 0.0225,
                "acc_stderr": 0.003316982994845522,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "arithmetic_2da": {
                "acc": 0.0195,
                "acc_stderr": 0.003092678018912424,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "gsm8k_cot": {
                "exact_match": 0.032600454890068235,
                "exact_match_stderr": 0.00489166902193957,
                "timestamp": "2024-06-11T15-32-26.862833"
            },
            "gsm8k": {
                "exact_match": 0.01819560272934041,
                "exact_match_stderr": 0.0036816118940738727,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "anli_r2": {
                "brier_score": 0.755675118118756,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "anli_r3": {
                "brier_score": 0.6999705904132336,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "anli_r1": {
                "brier_score": 0.7536494384689287,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_eu": {
                "brier_score": 1.089933422701538,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_vi": {
                "brier_score": 0.7966443228842566,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_ru": {
                "brier_score": 0.7989806201813834,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_zh": {
                "brier_score": 1.0334911487693128,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_tr": {
                "brier_score": 0.839021250171503,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_fr": {
                "brier_score": 0.8071513485603421,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_en": {
                "brier_score": 0.6656814608095787,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_ur": {
                "brier_score": 0.9428490629298696,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_ar": {
                "brier_score": 1.0302855031988798,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_de": {
                "brier_score": 0.8364057760607875,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_hi": {
                "brier_score": 0.7590215516967309,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_es": {
                "brier_score": 0.8316353906107824,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_bg": {
                "brier_score": 0.8758042326498179,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_sw": {
                "brier_score": 0.8831334480577933,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_el": {
                "brier_score": 0.8987268713940024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "xnli_th": {
                "brier_score": 0.8062685621961146,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "logiqa2": {
                "brier_score": 1.1489525363355495,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "mathqa": {
                "brier_score": 0.9586676691784485,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-11T16-06-19.079815"
            },
            "lambada_standard": {
                "perplexity": 7.7343856575560315,
                "perplexity_stderr": 0.2042279549336044,
                "acc": 0.548418397050262,
                "acc_stderr": 0.006933239470474408,
                "timestamp": "2024-06-11T16-09-16.235478"
            },
            "lambada_openai": {
                "perplexity": 5.350437725308013,
                "perplexity_stderr": 0.13313966140293174,
                "acc": 0.6367164758393169,
                "acc_stderr": 0.0067005116674768135,
                "timestamp": "2024-06-11T16-09-16.235478"
            },
            "mmlu_world_religions": {
                "acc": 0.32748538011695905,
                "acc_stderr": 0.03599335771456027,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_formal_logic": {
                "acc": 0.23809523809523808,
                "acc_stderr": 0.03809523809523811,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_prehistory": {
                "acc": 0.24074074074074073,
                "acc_stderr": 0.02378858355165853,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.27262569832402234,
                "acc_stderr": 0.014893391735249608,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.22362869198312235,
                "acc_stderr": 0.027123298205229972,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_moral_disputes": {
                "acc": 0.28034682080924855,
                "acc_stderr": 0.02418242749657762,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_professional_law": {
                "acc": 0.26988265971316816,
                "acc_stderr": 0.01133738108425041,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.27607361963190186,
                "acc_stderr": 0.03512385283705051,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.029771775228145638,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_philosophy": {
                "acc": 0.2604501607717042,
                "acc_stderr": 0.02492672322484554,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_jurisprudence": {
                "acc": 0.23148148148148148,
                "acc_stderr": 0.04077494709252626,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_international_law": {
                "acc": 0.2644628099173554,
                "acc_stderr": 0.04026187527591207,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.03477691162163659,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21761658031088082,
                "acc_stderr": 0.02977866303775296,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.24789915966386555,
                "acc_stderr": 0.028047967224176892,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_geography": {
                "acc": 0.18686868686868688,
                "acc_stderr": 0.027772533334218977,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.29908256880733947,
                "acc_stderr": 0.01963041728541517,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_public_relations": {
                "acc": 0.24545454545454545,
                "acc_stderr": 0.04122066502878285,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.27,
                "acc_stderr": 0.04461960433384739,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_sociology": {
                "acc": 0.24875621890547264,
                "acc_stderr": 0.030567675938916714,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2564102564102564,
                "acc_stderr": 0.02213908110397153,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_security_studies": {
                "acc": 0.19591836734693877,
                "acc_stderr": 0.025409301953225678,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_professional_psychology": {
                "acc": 0.24673202614379086,
                "acc_stderr": 0.017440820367402493,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_human_sexuality": {
                "acc": 0.25190839694656486,
                "acc_stderr": 0.038073871163060866,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_econometrics": {
                "acc": 0.24561403508771928,
                "acc_stderr": 0.04049339297748142,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_miscellaneous": {
                "acc": 0.26436781609195403,
                "acc_stderr": 0.01576998484069052,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_marketing": {
                "acc": 0.23931623931623933,
                "acc_stderr": 0.027951826808924333,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_management": {
                "acc": 0.27184466019417475,
                "acc_stderr": 0.044052680241409216,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_nutrition": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.02428861946604612,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_medical_genetics": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768078,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_human_aging": {
                "acc": 0.29596412556053814,
                "acc_stderr": 0.030636591348699817,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_professional_medicine": {
                "acc": 0.41911764705882354,
                "acc_stderr": 0.029972807170464622,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_college_medicine": {
                "acc": 0.24277456647398843,
                "acc_stderr": 0.0326926380614177,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_business_ethics": {
                "acc": 0.14,
                "acc_stderr": 0.03487350880197773,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.23018867924528302,
                "acc_stderr": 0.025907897122408173,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653697,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_virology": {
                "acc": 0.26506024096385544,
                "acc_stderr": 0.03436024037944967,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2553191489361702,
                "acc_stderr": 0.026011992930902013,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_college_physics": {
                "acc": 0.24509803921568626,
                "acc_stderr": 0.042801058373643966,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_physics": {
                "acc": 0.271523178807947,
                "acc_stderr": 0.03631329803969653,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_biology": {
                "acc": 0.21935483870967742,
                "acc_stderr": 0.023540799358723295,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_college_biology": {
                "acc": 0.20833333333333334,
                "acc_stderr": 0.033961162058453336,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_anatomy": {
                "acc": 0.22962962962962963,
                "acc_stderr": 0.036333844140734636,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_college_chemistry": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_computer_security": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_college_computer_science": {
                "acc": 0.4,
                "acc_stderr": 0.049236596391733084,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_astronomy": {
                "acc": 0.2565789473684211,
                "acc_stderr": 0.0355418036802569,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_college_mathematics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.3191489361702128,
                "acc_stderr": 0.030472973363380056,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542128,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.37,
                "acc_stderr": 0.048523658709391,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_machine_learning": {
                "acc": 0.23214285714285715,
                "acc_stderr": 0.04007341809755807,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.20689655172413793,
                "acc_stderr": 0.02850137816789395,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.3425925925925926,
                "acc_stderr": 0.032365852526021574,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.23015873015873015,
                "acc_stderr": 0.021679219663693145,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.25517241379310346,
                "acc_stderr": 0.03632984052707842,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.26296296296296295,
                "acc_stderr": 0.026842057873833713,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "arc_challenge": {
                "acc": 0.3242320819112628,
                "acc_stderr": 0.013678810399518813,
                "acc_norm": 0.35238907849829354,
                "acc_norm_stderr": 0.01396014260059869,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "hellaswag": {
                "acc": 0.4477195777733519,
                "acc_stderr": 0.004962429881904026,
                "acc_norm": 0.594901414060944,
                "acc_norm_stderr": 0.004899078300184236,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "truthfulqa_mc2": {
                "acc": 0.3803445214836313,
                "acc_stderr": 0.01392905684581036,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "truthfulqa_gen": {
                "bleu_max": 22.34064093494146,
                "bleu_max_stderr": 0.7183987511971597,
                "bleu_acc": 0.2741738066095471,
                "bleu_acc_stderr": 0.015616518497219367,
                "bleu_diff": -7.446457636161483,
                "bleu_diff_stderr": 0.7565638438274547,
                "rouge1_max": 48.16517696338372,
                "rouge1_max_stderr": 0.8420427437367002,
                "rouge1_acc": 0.2717258261933905,
                "rouge1_acc_stderr": 0.01557284045287583,
                "rouge1_diff": -8.43929405627723,
                "rouge1_diff_stderr": 0.823148680564784,
                "rouge2_max": 31.457102690393747,
                "rouge2_max_stderr": 0.9472192452069558,
                "rouge2_acc": 0.22888616891064872,
                "rouge2_acc_stderr": 0.014706994909055027,
                "rouge2_diff": -10.247194848412033,
                "rouge2_diff_stderr": 0.9595694112827735,
                "rougeL_max": 44.8539232504777,
                "rougeL_max_stderr": 0.8545145349902917,
                "rougeL_acc": 0.25703794369645044,
                "rougeL_acc_stderr": 0.015298077509485083,
                "rougeL_diff": -8.674604971448174,
                "rougeL_diff_stderr": 0.8352252105721707,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "truthfulqa_mc1": {
                "acc": 0.24357405140758873,
                "acc_stderr": 0.015026354824910782,
                "timestamp": "2024-11-21T21-50-03.007594"
            },
            "winogrande": {
                "acc": 0.5966850828729282,
                "acc_stderr": 0.01378725728589624,
                "timestamp": "2024-11-21T21-50-03.007594"
            }
        }
    }
}