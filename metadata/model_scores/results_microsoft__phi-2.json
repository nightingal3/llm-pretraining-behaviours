{
    "model_name": "microsoft__phi-2",
    "last_updated": "2024-12-19 13:40:01.459180",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.695906432748538,
                "acc_stderr": 0.03528211258245231,
                "brier_score": 0.4124207429640924,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_formal_logic": {
                "acc": 0.3333333333333333,
                "acc_stderr": 0.04216370213557835,
                "brier_score": 0.7220382999831597,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_prehistory": {
                "acc": 0.6018518518518519,
                "acc_stderr": 0.02723741509459247,
                "brier_score": 0.5089921216509934,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2335195530726257,
                "acc_stderr": 0.01414957534897626,
                "brier_score": 0.8788760175524666,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.7552742616033755,
                "acc_stderr": 0.027985699387036416,
                "brier_score": 0.42015101380924585,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_moral_disputes": {
                "acc": 0.653179190751445,
                "acc_stderr": 0.025624723994030464,
                "brier_score": 0.4953851144132304,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_professional_law": {
                "acc": 0.39895697522816165,
                "acc_stderr": 0.01250675765529367,
                "brier_score": 0.6969188345540653,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.7607361963190185,
                "acc_stderr": 0.033519538795212696,
                "brier_score": 0.3697065462256061,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.6813725490196079,
                "acc_stderr": 0.0327028718148208,
                "brier_score": 0.4565368516589864,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_philosophy": {
                "acc": 0.5755627009646302,
                "acc_stderr": 0.028071928247946205,
                "brier_score": 0.5163780448328548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_jurisprudence": {
                "acc": 0.7037037037037037,
                "acc_stderr": 0.04414343666854933,
                "brier_score": 0.40901176900233016,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_international_law": {
                "acc": 0.7603305785123967,
                "acc_stderr": 0.038968789850704164,
                "brier_score": 0.35251137375069447,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.703030303030303,
                "acc_stderr": 0.03567969772268049,
                "brier_score": 0.4580662616057771,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.7668393782383419,
                "acc_stderr": 0.03051611137147602,
                "brier_score": 0.32228783313694453,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.5882352941176471,
                "acc_stderr": 0.03196876989195778,
                "brier_score": 0.5115888159559108,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_geography": {
                "acc": 0.7222222222222222,
                "acc_stderr": 0.03191178226713547,
                "brier_score": 0.38625008579175607,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.7651376146788991,
                "acc_stderr": 0.018175110510343578,
                "brier_score": 0.3279035759824191,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_public_relations": {
                "acc": 0.6454545454545455,
                "acc_stderr": 0.045820048415054174,
                "brier_score": 0.45567858680867424,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.72,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 0.32462247031549857,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_sociology": {
                "acc": 0.7810945273631841,
                "acc_stderr": 0.029239174636647,
                "brier_score": 0.30534825640524377,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.5769230769230769,
                "acc_stderr": 0.02504919787604235,
                "brier_score": 0.5302869478068375,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_security_studies": {
                "acc": 0.6571428571428571,
                "acc_stderr": 0.03038726291954773,
                "brier_score": 0.4391358426536197,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_professional_psychology": {
                "acc": 0.5392156862745098,
                "acc_stderr": 0.02016552331390791,
                "brier_score": 0.5711487339018314,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_human_sexuality": {
                "acc": 0.6564885496183206,
                "acc_stderr": 0.041649760719448786,
                "brier_score": 0.45412460592973525,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_econometrics": {
                "acc": 0.2982456140350877,
                "acc_stderr": 0.04303684033537314,
                "brier_score": 0.7754655884085445,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_miscellaneous": {
                "acc": 0.6871008939974457,
                "acc_stderr": 0.016580935940304048,
                "brier_score": 0.40109685014778523,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_marketing": {
                "acc": 0.7948717948717948,
                "acc_stderr": 0.02645350805404033,
                "brier_score": 0.27575313282111125,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_management": {
                "acc": 0.7378640776699029,
                "acc_stderr": 0.04354631077260595,
                "brier_score": 0.3819982905394013,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_nutrition": {
                "acc": 0.6111111111111112,
                "acc_stderr": 0.02791405551046801,
                "brier_score": 0.4926628596822712,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_medical_genetics": {
                "acc": 0.63,
                "acc_stderr": 0.048523658709391,
                "brier_score": 0.48573602332047644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_human_aging": {
                "acc": 0.6367713004484304,
                "acc_stderr": 0.032277904428505,
                "brier_score": 0.49588156699058694,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_professional_medicine": {
                "acc": 0.4485294117647059,
                "acc_stderr": 0.030211479609121593,
                "brier_score": 0.6606940817072791,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_college_medicine": {
                "acc": 0.5260115606936416,
                "acc_stderr": 0.03807301726504511,
                "brier_score": 0.5745741819956182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_business_ethics": {
                "acc": 0.58,
                "acc_stderr": 0.049604496374885836,
                "brier_score": 0.4980841866410591,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.6188679245283019,
                "acc_stderr": 0.029890609686286644,
                "brier_score": 0.4830551320075802,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_global_facts": {
                "acc": 0.36,
                "acc_stderr": 0.04824181513244218,
                "brier_score": 0.7370184276518171,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_virology": {
                "acc": 0.4819277108433735,
                "acc_stderr": 0.03889951252827216,
                "brier_score": 0.6755955415876875,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_professional_accounting": {
                "acc": 0.42907801418439717,
                "acc_stderr": 0.029525914302558555,
                "brier_score": 0.6734628820426131,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_college_physics": {
                "acc": 0.3137254901960784,
                "acc_stderr": 0.04617034827006718,
                "brier_score": 0.746275000406364,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_physics": {
                "acc": 0.3973509933774834,
                "acc_stderr": 0.0399552400768168,
                "brier_score": 0.7383930383603438,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_biology": {
                "acc": 0.6838709677419355,
                "acc_stderr": 0.026450874489042767,
                "brier_score": 0.43640200672949025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_college_biology": {
                "acc": 0.6111111111111112,
                "acc_stderr": 0.04076663253918567,
                "brier_score": 0.4879789821016725,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_anatomy": {
                "acc": 0.45185185185185184,
                "acc_stderr": 0.04299268905480864,
                "brier_score": 0.6026312492166612,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_college_chemistry": {
                "acc": 0.37,
                "acc_stderr": 0.04852365870939099,
                "brier_score": 0.6882345069402325,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_computer_security": {
                "acc": 0.64,
                "acc_stderr": 0.04824181513244218,
                "brier_score": 0.4518442546275091,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_college_computer_science": {
                "acc": 0.47,
                "acc_stderr": 0.05016135580465919,
                "brier_score": 0.6820303010165037,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_astronomy": {
                "acc": 0.5592105263157895,
                "acc_stderr": 0.04040311062490436,
                "brier_score": 0.5555860131240212,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_college_mathematics": {
                "acc": 0.44,
                "acc_stderr": 0.04988876515698589,
                "brier_score": 0.7020125283359032,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.4765957446808511,
                "acc_stderr": 0.03265019475033582,
                "brier_score": 0.632215822853458,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7463719598724317,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.71,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.43476423654292357,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_machine_learning": {
                "acc": 0.49107142857142855,
                "acc_stderr": 0.04745033255489123,
                "brier_score": 0.616295780702097,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.46798029556650245,
                "acc_stderr": 0.03510766597959217,
                "brier_score": 0.6137650704809741,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.4398148148148148,
                "acc_stderr": 0.033851779760448106,
                "brier_score": 0.6910018927397092,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.3862433862433862,
                "acc_stderr": 0.025075981767601688,
                "brier_score": 0.6964182032495722,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.5448275862068965,
                "acc_stderr": 0.04149886942192117,
                "brier_score": 0.608615043854258,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.29259259259259257,
                "acc_stderr": 0.027738969632176088,
                "brier_score": 0.7662039210743501,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-14-12.511021"
            },
            "arc_challenge": {
                "acc": 0.575938566552901,
                "acc_stderr": 0.014441889627464398,
                "acc_norm": 0.6083617747440273,
                "acc_norm_stderr": 0.014264122124938215,
                "timestamp": "2024-11-20T12-19-35.462797"
            },
            "hellaswag": {
                "acc": 0.5655247958573989,
                "acc_stderr": 0.004946748608271356,
                "acc_norm": 0.7542322246564429,
                "acc_norm_stderr": 0.004296615862786664,
                "timestamp": "2024-11-20T12-19-35.462797"
            },
            "truthfulqa_mc2": {
                "acc": 0.44503297968968214,
                "acc_stderr": 0.015126633399430035,
                "timestamp": "2024-11-20T12-19-35.462797"
            },
            "truthfulqa_gen": {
                "bleu_max": 30.636426608129604,
                "bleu_max_stderr": 0.8253318984766498,
                "bleu_acc": 0.3843329253365973,
                "bleu_acc_stderr": 0.01702870730124523,
                "bleu_diff": -1.6737273900429643,
                "bleu_diff_stderr": 0.9377681674822301,
                "rouge1_max": 56.49815681439475,
                "rouge1_max_stderr": 0.8385465502514413,
                "rouge1_acc": 0.3929008567931457,
                "rouge1_acc_stderr": 0.017097248285233065,
                "rouge1_diff": -1.2755753639684835,
                "rouge1_diff_stderr": 1.1278344789587786,
                "rouge2_max": 42.30652845433004,
                "rouge2_max_stderr": 1.0170277948205164,
                "rouge2_acc": 0.35495716034271724,
                "rouge2_acc_stderr": 0.01675086238137591,
                "rouge2_diff": -2.1576932295313727,
                "rouge2_diff_stderr": 1.2989464511092814,
                "rougeL_max": 53.81199070019868,
                "rougeL_max_stderr": 0.8662590848816515,
                "rougeL_acc": 0.3769889840881273,
                "rougeL_acc_stderr": 0.016965517578930358,
                "rougeL_diff": -1.3616416414143386,
                "rougeL_diff_stderr": 1.1477177478773908,
                "timestamp": "2024-11-20T12-19-35.462797"
            },
            "truthfulqa_mc1": {
                "acc": 0.30966952264381886,
                "acc_stderr": 0.016185744355144933,
                "timestamp": "2024-11-20T12-19-35.462797"
            },
            "winogrande": {
                "acc": 0.734017363851618,
                "acc_stderr": 0.012418323153051051,
                "timestamp": "2024-11-20T12-19-35.462797"
            },
            "gsm8k": {
                "exact_match": 0.576194086429113,
                "exact_match_stderr": 0.013611632008810359,
                "timestamp": "2024-11-20T12-19-35.462797"
            }
        }
    }
}