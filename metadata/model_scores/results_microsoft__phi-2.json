{
    "model_name": "microsoft__phi-2",
    "last_updated": "2024-12-04 11:24:13.297237",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.6900584795321637,
                    "acc_stderr": 0.03546976959393163,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.35714285714285715,
                    "acc_stderr": 0.04285714285714281,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.6234567901234568,
                    "acc_stderr": 0.026959344518747784,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.3139664804469274,
                    "acc_stderr": 0.015521923933523647,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.7383966244725738,
                    "acc_stderr": 0.028609516716994934,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.6676300578034682,
                    "acc_stderr": 0.025361168749688214,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.4198174706649283,
                    "acc_stderr": 0.01260496081608737,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.7484662576687117,
                    "acc_stderr": 0.034089978868575295,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.6617647058823529,
                    "acc_stderr": 0.03320574612945431,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.6270096463022508,
                    "acc_stderr": 0.027466610213140112,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.7129629629629629,
                    "acc_stderr": 0.043733130409147614,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.743801652892562,
                    "acc_stderr": 0.03984979653302872,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.6424242424242425,
                    "acc_stderr": 0.037425970438065864,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.8082901554404145,
                    "acc_stderr": 0.02840895362624526,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.6050420168067226,
                    "acc_stderr": 0.03175367846096626,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.7373737373737373,
                    "acc_stderr": 0.03135305009533085,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.01714985851425093,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.6727272727272727,
                    "acc_stderr": 0.0449429086625209,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.76,
                    "acc_stderr": 0.04292346959909283,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.8109452736318408,
                    "acc_stderr": 0.027686913588013028,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.5692307692307692,
                    "acc_stderr": 0.02510682066053975,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.7306122448979592,
                    "acc_stderr": 0.02840125202902294,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.5604575163398693,
                    "acc_stderr": 0.02007942040808792,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.7099236641221374,
                    "acc_stderr": 0.03980066246467765,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.38596491228070173,
                    "acc_stderr": 0.045796394220704334,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.6896551724137931,
                    "acc_stderr": 0.016543785026048318,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.8205128205128205,
                    "acc_stderr": 0.025140935950335442,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.7378640776699029,
                    "acc_stderr": 0.04354631077260595,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.6176470588235294,
                    "acc_stderr": 0.027826109307283683,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.63,
                    "acc_stderr": 0.04852365870939099,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.6502242152466368,
                    "acc_stderr": 0.03200736719484503,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.47794117647058826,
                    "acc_stderr": 0.03034326422421352,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.5953757225433526,
                    "acc_stderr": 0.03742461193887248,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.6,
                    "acc_stderr": 0.049236596391733084,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.6113207547169811,
                    "acc_stderr": 0.030000485448675986,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695235,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.4759036144578313,
                    "acc_stderr": 0.03887971849597264,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.4432624113475177,
                    "acc_stderr": 0.029634838473766006,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.3627450980392157,
                    "acc_stderr": 0.04784060704105654,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.37748344370860926,
                    "acc_stderr": 0.0395802723112157,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.7,
                    "acc_stderr": 0.02606936229533514,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.6666666666666666,
                    "acc_stderr": 0.039420826399272135,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.4444444444444444,
                    "acc_stderr": 0.04292596718256981,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.41,
                    "acc_stderr": 0.049431107042371025,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.74,
                    "acc_stderr": 0.044084400227680794,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.43,
                    "acc_stderr": 0.04975698519562428,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.5855263157894737,
                    "acc_stderr": 0.04008973785779206,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.04878317312145632,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.5063829787234042,
                    "acc_stderr": 0.032683358999363366,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.64,
                    "acc_stderr": 0.04824181513244218,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.49107142857142855,
                    "acc_stderr": 0.04745033255489123,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.4827586206896552,
                    "acc_stderr": 0.035158955511657,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.49537037037037035,
                    "acc_stderr": 0.03409825519163572,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.455026455026455,
                    "acc_stderr": 0.02564692836104939,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.5448275862068965,
                    "acc_stderr": 0.04149886942192117,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.3333333333333333,
                    "acc_stderr": 0.028742040903948485,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.575938566552901,
                    "acc_stderr": 0.014441889627464398,
                    "acc_norm": 0.6083617747440273,
                    "acc_norm_stderr": 0.014264122124938215,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5655247958573989,
                    "acc_stderr": 0.004946748608271356,
                    "acc_norm": 0.7542322246564429,
                    "acc_norm_stderr": 0.004296615862786664,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.44503297968968214,
                    "acc_stderr": 0.015126633399430035,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 30.636426608129604,
                    "bleu_max_stderr": 0.8253318984766498,
                    "bleu_acc": 0.3843329253365973,
                    "bleu_acc_stderr": 0.01702870730124523,
                    "bleu_diff": -1.6737273900429643,
                    "bleu_diff_stderr": 0.9377681674822301,
                    "rouge1_max": 56.49815681439475,
                    "rouge1_max_stderr": 0.8385465502514413,
                    "rouge1_acc": 0.3929008567931457,
                    "rouge1_acc_stderr": 0.017097248285233065,
                    "rouge1_diff": -1.2755753639684835,
                    "rouge1_diff_stderr": 1.1278344789587786,
                    "rouge2_max": 42.30652845433004,
                    "rouge2_max_stderr": 1.0170277948205164,
                    "rouge2_acc": 0.35495716034271724,
                    "rouge2_acc_stderr": 0.01675086238137591,
                    "rouge2_diff": -2.1576932295313727,
                    "rouge2_diff_stderr": 1.2989464511092814,
                    "rougeL_max": 53.81199070019868,
                    "rougeL_max_stderr": 0.8662590848816515,
                    "rougeL_acc": 0.3769889840881273,
                    "rougeL_acc_stderr": 0.016965517578930358,
                    "rougeL_diff": -1.3616416414143386,
                    "rougeL_diff_stderr": 1.1477177478773908,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.30966952264381886,
                    "acc_stderr": 0.016185744355144933,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.734017363851618,
                    "acc_stderr": 0.012418323153051051,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.576194086429113,
                    "acc_stderr": 0.013611632008810359,
                    "timestamp": "2024-11-20T12-19-35.462797"
                }
            }
        }
    }
}