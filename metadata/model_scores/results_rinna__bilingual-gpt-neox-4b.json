{
    "model_name": "rinna/bilingual-gpt-neox-4b",
    "last_updated": "2024-12-04 11:23:27.327504",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.007326007326007326,
                "exact_match_stderr": 0.0036529080893830334,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.010332950631458095,
                "exact_match_stderr": 0.0034284443646836484,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.018518518518518517,
                "exact_match_stderr": 0.005806972807912266,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.01107419712070875,
                "exact_match_stderr": 0.0034844537978317453,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "minerva_math_geometry": {
                "exact_match": 0.018789144050104383,
                "exact_match_stderr": 0.006210416427997402,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.006329113924050633,
                "exact_match_stderr": 0.003646382041065029,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "minerva_math_algebra": {
                "exact_match": 0.009267059814658803,
                "exact_match_stderr": 0.002782319118488801,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521547,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_3ds": {
                "acc": 0.0075,
                "acc_stderr": 0.0019296986470519833,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_2ds": {
                "acc": 0.0415,
                "acc_stderr": 0.0044608098381578925,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_1dc": {
                "acc": 0.066,
                "acc_stderr": 0.005553144938623081,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_2dm": {
                "acc": 0.033,
                "acc_stderr": 0.003995432609977368,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "arithmetic_2da": {
                "acc": 0.035,
                "acc_stderr": 0.004110468096699808,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "gsm8k_cot": {
                "exact_match": 0.021986353297952996,
                "exact_match_stderr": 0.0040391627581100676,
                "timestamp": "2024-06-13T15-18-28.417764"
            },
            "gsm8k": {
                "exact_match": 0.024260803639120546,
                "exact_match_stderr": 0.0042380079000014,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "anli_r2": {
                "brier_score": 0.7465044173321834,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "anli_r3": {
                "brier_score": 0.7382616042295201,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "anli_r1": {
                "brier_score": 0.7422539652276323,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_eu": {
                "brier_score": 0.9935567744254022,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_vi": {
                "brier_score": 0.8210592681442176,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_ru": {
                "brier_score": 0.8190550378326039,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_zh": {
                "brier_score": 0.9435664261389121,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_tr": {
                "brier_score": 0.8247652276169989,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_fr": {
                "brier_score": 0.7935452141545868,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_en": {
                "brier_score": 0.673510778129045,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_ur": {
                "brier_score": 0.9384835031798199,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_ar": {
                "brier_score": 1.080089095089373,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_de": {
                "brier_score": 0.8497396490738153,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_hi": {
                "brier_score": 0.7784793501566398,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_es": {
                "brier_score": 0.8344890347288778,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_bg": {
                "brier_score": 0.9131127340061094,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_sw": {
                "brier_score": 0.8375909307024677,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_el": {
                "brier_score": 0.920078994998459,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "xnli_th": {
                "brier_score": 0.7626190761994815,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "logiqa2": {
                "brier_score": 1.0595557248900274,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "mathqa": {
                "brier_score": 0.9543318277043356,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T15-27-31.122389"
            },
            "lambada_standard": {
                "perplexity": 9.40445963354471,
                "perplexity_stderr": 0.25202597356824585,
                "acc": 0.518338831748496,
                "acc_stderr": 0.006961290586136407,
                "timestamp": "2024-06-13T15-28-52.316557"
            },
            "lambada_openai": {
                "perplexity": 5.365362213035259,
                "perplexity_stderr": 0.12842366560448085,
                "acc": 0.6394333398020571,
                "acc_stderr": 0.006689636112543833,
                "timestamp": "2024-06-13T15-28-52.316557"
            },
            "mmlu_world_religions": {
                "acc": 0.25146198830409355,
                "acc_stderr": 0.033275044238468436,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_formal_logic": {
                "acc": 0.24603174603174602,
                "acc_stderr": 0.038522733649243156,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_prehistory": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.024922001168886338,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24692737430167597,
                "acc_stderr": 0.01442229220480886,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.23628691983122363,
                "acc_stderr": 0.027652153144159274,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_moral_disputes": {
                "acc": 0.1994219653179191,
                "acc_stderr": 0.021511900654252562,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_professional_law": {
                "acc": 0.2503259452411995,
                "acc_stderr": 0.01106415102716544,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.294478527607362,
                "acc_stderr": 0.03581165790474082,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.23039215686274508,
                "acc_stderr": 0.029554292605695066,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_philosophy": {
                "acc": 0.3054662379421222,
                "acc_stderr": 0.026160584450140485,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.04330043749650741,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_international_law": {
                "acc": 0.3140495867768595,
                "acc_stderr": 0.04236964753041018,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.24242424242424243,
                "acc_stderr": 0.03346409881055953,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.22797927461139897,
                "acc_stderr": 0.03027690994517825,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.2689075630252101,
                "acc_stderr": 0.028801392193631276,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_geography": {
                "acc": 0.23232323232323232,
                "acc_stderr": 0.030088629490217483,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.22018348623853212,
                "acc_stderr": 0.01776597865232757,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_public_relations": {
                "acc": 0.19090909090909092,
                "acc_stderr": 0.03764425585984926,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768077,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_sociology": {
                "acc": 0.25870646766169153,
                "acc_stderr": 0.030965903123573026,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24615384615384617,
                "acc_stderr": 0.021840866990423077,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_security_studies": {
                "acc": 0.17959183673469387,
                "acc_stderr": 0.024573293589585637,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25,
                "acc_stderr": 0.01751781884501444,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_human_sexuality": {
                "acc": 0.21374045801526717,
                "acc_stderr": 0.0359546161177469,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_econometrics": {
                "acc": 0.22807017543859648,
                "acc_stderr": 0.03947152782669415,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_miscellaneous": {
                "acc": 0.27330779054916987,
                "acc_stderr": 0.015936681062628556,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_marketing": {
                "acc": 0.23504273504273504,
                "acc_stderr": 0.027778835904935444,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_management": {
                "acc": 0.20388349514563106,
                "acc_stderr": 0.03989139859531769,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_nutrition": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.025261691219729487,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_medical_genetics": {
                "acc": 0.24,
                "acc_stderr": 0.04292346959909282,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_human_aging": {
                "acc": 0.30493273542600896,
                "acc_stderr": 0.030898610882477515,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_professional_medicine": {
                "acc": 0.1801470588235294,
                "acc_stderr": 0.02334516361654486,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_college_medicine": {
                "acc": 0.24277456647398843,
                "acc_stderr": 0.0326926380614177,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_business_ethics": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036843,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2490566037735849,
                "acc_stderr": 0.02661648298050171,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_global_facts": {
                "acc": 0.3,
                "acc_stderr": 0.04605661864718381,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_virology": {
                "acc": 0.30120481927710846,
                "acc_stderr": 0.0357160923005348,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2730496453900709,
                "acc_stderr": 0.02657786094330785,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_college_physics": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.043898699568087785,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_physics": {
                "acc": 0.25165562913907286,
                "acc_stderr": 0.035433042343899844,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2870967741935484,
                "acc_stderr": 0.025736542745594525,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_college_biology": {
                "acc": 0.22916666666666666,
                "acc_stderr": 0.03514697467862388,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_anatomy": {
                "acc": 0.28888888888888886,
                "acc_stderr": 0.0391545063041425,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_college_chemistry": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653694,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_computer_security": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036622,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_college_computer_science": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252604,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_astronomy": {
                "acc": 0.26973684210526316,
                "acc_stderr": 0.03611780560284898,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_college_mathematics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2425531914893617,
                "acc_stderr": 0.028020226271200217,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542128,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_machine_learning": {
                "acc": 0.24107142857142858,
                "acc_stderr": 0.04059867246952687,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2660098522167488,
                "acc_stderr": 0.031089826002937523,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.30092592592592593,
                "acc_stderr": 0.03128039084329882,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.24603174603174602,
                "acc_stderr": 0.022182037202948365,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2689655172413793,
                "acc_stderr": 0.036951833116502325,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.02671924078371216,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "arc_challenge": {
                "acc": 0.2901023890784983,
                "acc_stderr": 0.013261573677520776,
                "acc_norm": 0.3216723549488055,
                "acc_norm_stderr": 0.013650488084494162,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "hellaswag": {
                "acc": 0.4277036446922924,
                "acc_stderr": 0.004937345081868086,
                "acc_norm": 0.5650268870742879,
                "acc_norm_stderr": 0.004947402907996252,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "truthfulqa_mc2": {
                "acc": 0.3587180858240576,
                "acc_stderr": 0.013702244823217082,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "truthfulqa_gen": {
                "bleu_max": 20.298502784294392,
                "bleu_max_stderr": 0.6658217344882357,
                "bleu_acc": 0.29498164014687883,
                "bleu_acc_stderr": 0.01596440096558967,
                "bleu_diff": -6.505009210061705,
                "bleu_diff_stderr": 0.6268960954843193,
                "rouge1_max": 43.773621090740846,
                "rouge1_max_stderr": 0.8400411954477274,
                "rouge1_acc": 0.24112607099143207,
                "rouge1_acc_stderr": 0.014974827279752329,
                "rouge1_diff": -10.723279586053893,
                "rouge1_diff_stderr": 0.7370334795276966,
                "rouge2_max": 26.5835203998597,
                "rouge2_max_stderr": 0.8994004691527028,
                "rouge2_acc": 0.1799265605875153,
                "rouge2_acc_stderr": 0.013447109235537583,
                "rouge2_diff": -11.606543714790986,
                "rouge2_diff_stderr": 0.8349217988871113,
                "rougeL_max": 40.61834366861577,
                "rougeL_max_stderr": 0.833488462597628,
                "rougeL_acc": 0.23623011015911874,
                "rougeL_acc_stderr": 0.014869755015871124,
                "rougeL_diff": -10.553870158245887,
                "rougeL_diff_stderr": 0.7246002434925973,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "truthfulqa_mc1": {
                "acc": 0.2141982864137087,
                "acc_stderr": 0.014362148155690478,
                "timestamp": "2024-11-21T23-38-34.939143"
            },
            "winogrande": {
                "acc": 0.5872138910812944,
                "acc_stderr": 0.013837060648682096,
                "timestamp": "2024-11-21T23-38-34.939143"
            }
        }
    }
}