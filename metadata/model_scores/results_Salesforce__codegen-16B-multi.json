{
    "model_name": "Salesforce/codegen-16B-multi",
    "last_updated": "2024-12-19 13:40:12.946693",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.0347985347985348,
                "exact_match_stderr": 0.00785038966769786,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.03788748564867968,
                "exact_match_stderr": 0.006472934284600699,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.022222222222222223,
                "exact_match_stderr": 0.006349206349206319,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.024363233665559248,
                "exact_match_stderr": 0.005133437461128035,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "minerva_math_geometry": {
                "exact_match": 0.022964509394572025,
                "exact_match_stderr": 0.006851249878769252,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.016877637130801686,
                "exact_match_stderr": 0.0059228268948526815,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "minerva_math_algebra": {
                "exact_match": 0.02695871946082561,
                "exact_match_stderr": 0.00470297768200653,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_3da": {
                "acc": 0.022,
                "acc_stderr": 0.0032807593162018905,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_3ds": {
                "acc": 0.0145,
                "acc_stderr": 0.0026736583971427824,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_4da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521479,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_2ds": {
                "acc": 0.166,
                "acc_stderr": 0.008322056735817098,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_1dc": {
                "acc": 0.147,
                "acc_stderr": 0.007920029256998883,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_4ds": {
                "acc": 0.001,
                "acc_stderr": 0.0007069298939339487,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_2dm": {
                "acc": 0.0975,
                "acc_stderr": 0.006634672896399609,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "arithmetic_2da": {
                "acc": 0.1715,
                "acc_stderr": 0.008430860852092992,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "gsm8k_cot": {
                "exact_match": 0.03184230477634572,
                "exact_match_stderr": 0.004836348558260953,
                "timestamp": "2024-06-14T03-42-36.753618"
            },
            "gsm8k": {
                "exact_match": 0.03411675511751327,
                "exact_match_stderr": 0.0050002126007732545,
                "timestamp": "2024-11-20T10-20-44.927267"
            },
            "anli_r2": {
                "brier_score": 0.7344945652955857,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "anli_r3": {
                "brier_score": 0.7512937712393907,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "anli_r1": {
                "brier_score": 0.7409617387492144,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_eu": {
                "brier_score": 1.0499948811805855,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_vi": {
                "brier_score": 1.0494397425378459,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_ru": {
                "brier_score": 0.9528824583761204,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_zh": {
                "brier_score": 1.046794144815665,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_tr": {
                "brier_score": 0.9505776051805006,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_fr": {
                "brier_score": 0.8746123551516378,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_en": {
                "brier_score": 0.7168910989376758,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_ur": {
                "brier_score": 1.2504543392394984,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_ar": {
                "brier_score": 1.0299487744752638,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_de": {
                "brier_score": 0.9175188876319201,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_hi": {
                "brier_score": 1.0205470500057452,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_es": {
                "brier_score": 0.9198569977209587,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_bg": {
                "brier_score": 0.8427184705601842,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_sw": {
                "brier_score": 0.9401072122195173,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_el": {
                "brier_score": 0.9761407972947893,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "xnli_th": {
                "brier_score": 0.8572219481864177,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "logiqa2": {
                "brier_score": 1.1546592524731565,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "mathqa": {
                "brier_score": 0.953439313800392,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T04-03-38.652168"
            },
            "lambada_standard": {
                "perplexity": 25.851477547441906,
                "perplexity_stderr": 0.9461878766495387,
                "acc": 0.3512516980399767,
                "acc_stderr": 0.006650578225573468,
                "timestamp": "2024-06-14T04-05-35.450863"
            },
            "lambada_openai": {
                "perplexity": 15.404938907332895,
                "perplexity_stderr": 0.5077743811614407,
                "acc": 0.4255773335920823,
                "acc_stderr": 0.006888380073136036,
                "timestamp": "2024-06-14T04-05-35.450863"
            },
            "mmlu_world_religions": {
                "acc": 0.3216374269005848,
                "acc_stderr": 0.03582529442573122,
                "brier_score": 0.7627873242855722,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_formal_logic": {
                "acc": 0.25396825396825395,
                "acc_stderr": 0.03893259610604673,
                "brier_score": 0.7689830422591174,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_prehistory": {
                "acc": 0.25,
                "acc_stderr": 0.02409347123262133,
                "brier_score": 0.7807142929224355,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.24022346368715083,
                "acc_stderr": 0.014288343803925293,
                "brier_score": 0.7845479297986755,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2742616033755274,
                "acc_stderr": 0.029041333510598046,
                "brier_score": 0.7690532333296719,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_moral_disputes": {
                "acc": 0.30057803468208094,
                "acc_stderr": 0.024685316867257792,
                "brier_score": 0.7658024042802853,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_professional_law": {
                "acc": 0.2470664928292047,
                "acc_stderr": 0.011015752255279319,
                "brier_score": 0.7741547742598577,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.32515337423312884,
                "acc_stderr": 0.03680350371286461,
                "brier_score": 0.7559119563737517,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.28431372549019607,
                "acc_stderr": 0.03166009679399813,
                "brier_score": 0.7646895380414529,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_philosophy": {
                "acc": 0.2282958199356913,
                "acc_stderr": 0.023839303311398222,
                "brier_score": 0.7927169636500151,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_jurisprudence": {
                "acc": 0.19444444444444445,
                "acc_stderr": 0.03826076324884864,
                "brier_score": 0.8074905485544689,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_international_law": {
                "acc": 0.3305785123966942,
                "acc_stderr": 0.04294340845212094,
                "brier_score": 0.7338956031867301,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.24848484848484848,
                "acc_stderr": 0.03374402644139404,
                "brier_score": 0.7777113191258419,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.25906735751295334,
                "acc_stderr": 0.03161877917935411,
                "brier_score": 0.7777099996496186,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.2857142857142857,
                "acc_stderr": 0.029344572500634332,
                "brier_score": 0.771347524950132,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_geography": {
                "acc": 0.29292929292929293,
                "acc_stderr": 0.032424979581788166,
                "brier_score": 0.7628813204354998,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.25688073394495414,
                "acc_stderr": 0.018732492928342472,
                "brier_score": 0.7765931500840567,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_public_relations": {
                "acc": 0.24545454545454545,
                "acc_stderr": 0.04122066502878284,
                "brier_score": 0.8097039663378572,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252605,
                "brier_score": 0.7566402696284292,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_sociology": {
                "acc": 0.2537313432835821,
                "acc_stderr": 0.030769444967296024,
                "brier_score": 0.7699091524176438,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2205128205128205,
                "acc_stderr": 0.02102067268082791,
                "brier_score": 0.7887454152198857,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_security_studies": {
                "acc": 0.34285714285714286,
                "acc_stderr": 0.030387262919547735,
                "brier_score": 0.7482473159250772,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_professional_psychology": {
                "acc": 0.29411764705882354,
                "acc_stderr": 0.01843342764940189,
                "brier_score": 0.767820703187618,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_human_sexuality": {
                "acc": 0.25190839694656486,
                "acc_stderr": 0.03807387116306085,
                "brier_score": 0.7693791681119263,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_econometrics": {
                "acc": 0.16666666666666666,
                "acc_stderr": 0.03505859682597264,
                "brier_score": 0.8387501182260378,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_miscellaneous": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.01567100600933957,
                "brier_score": 0.7907263754130816,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_marketing": {
                "acc": 0.24786324786324787,
                "acc_stderr": 0.028286324075564407,
                "brier_score": 0.7746616650029644,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_management": {
                "acc": 0.1650485436893204,
                "acc_stderr": 0.03675668832233188,
                "brier_score": 0.8462996513443711,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_nutrition": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.02495418432487991,
                "brier_score": 0.7801372618222232,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_medical_genetics": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768079,
                "brier_score": 0.7627202267658829,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_human_aging": {
                "acc": 0.23766816143497757,
                "acc_stderr": 0.028568079464714277,
                "brier_score": 0.7865910674078115,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_professional_medicine": {
                "acc": 0.31985294117647056,
                "acc_stderr": 0.028332959514031218,
                "brier_score": 0.7558514526049404,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_college_medicine": {
                "acc": 0.19653179190751446,
                "acc_stderr": 0.030299574664788137,
                "brier_score": 0.794320488478051,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_business_ethics": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768078,
                "brier_score": 0.7641813854135013,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2490566037735849,
                "acc_stderr": 0.026616482980501708,
                "brier_score": 0.7981102158252912,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_global_facts": {
                "acc": 0.35,
                "acc_stderr": 0.0479372485441102,
                "brier_score": 0.7791554852331415,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_virology": {
                "acc": 0.23493975903614459,
                "acc_stderr": 0.03300533186128922,
                "brier_score": 0.7982656112392703,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_professional_accounting": {
                "acc": 0.22340425531914893,
                "acc_stderr": 0.024847921358063962,
                "brier_score": 0.8332174045787025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_college_physics": {
                "acc": 0.12745098039215685,
                "acc_stderr": 0.03318224921942076,
                "brier_score": 0.8445045042279045,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2781456953642384,
                "acc_stderr": 0.03658603262763743,
                "brier_score": 0.7806254894541519,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_biology": {
                "acc": 0.26129032258064516,
                "acc_stderr": 0.024993053397764815,
                "brier_score": 0.77598402053733,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_college_biology": {
                "acc": 0.3055555555555556,
                "acc_stderr": 0.03852084696008534,
                "brier_score": 0.7581486611163988,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_anatomy": {
                "acc": 0.31851851851851853,
                "acc_stderr": 0.0402477840197711,
                "brier_score": 0.7632834128768158,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_college_chemistry": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036623,
                "brier_score": 0.8187668700393952,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_computer_security": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7529534087015408,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_college_computer_science": {
                "acc": 0.27,
                "acc_stderr": 0.0446196043338474,
                "brier_score": 0.7637992365474011,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_astronomy": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.03583496176361063,
                "brier_score": 0.7669964212763819,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_college_mathematics": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.844011071165082,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.26382978723404255,
                "acc_stderr": 0.028809989854102973,
                "brier_score": 0.7723433608888436,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.9370967381574306,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.37,
                "acc_stderr": 0.04852365870939099,
                "brier_score": 0.7451369908253416,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_machine_learning": {
                "acc": 0.30357142857142855,
                "acc_stderr": 0.04364226155841044,
                "brier_score": 0.8142410358074835,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.21182266009852216,
                "acc_stderr": 0.028748983689941072,
                "brier_score": 0.8128684448501712,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1712962962962963,
                "acc_stderr": 0.025695341643824685,
                "brier_score": 0.8388018598400845,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2698412698412698,
                "acc_stderr": 0.022860838309232072,
                "brier_score": 0.8393886511286482,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.3448275862068966,
                "acc_stderr": 0.03960933549451207,
                "brier_score": 0.7445540897601858,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2518518518518518,
                "acc_stderr": 0.02646611753895991,
                "brier_score": 0.8483657210198746,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T19-42-25.772351"
            },
            "arc_challenge": {
                "acc": 0.28498293515358364,
                "acc_stderr": 0.013191348179838793,
                "acc_norm": 0.3370307167235495,
                "acc_norm_stderr": 0.01381347665290228,
                "timestamp": "2024-11-20T10-20-44.927267"
            },
            "hellaswag": {
                "acc": 0.39364668392750446,
                "acc_stderr": 0.004875595792850675,
                "acc_norm": 0.5127464648476399,
                "acc_norm_stderr": 0.004988159744742506,
                "timestamp": "2024-11-20T10-20-44.927267"
            },
            "truthfulqa_mc2": {
                "acc": 0.43278625644567936,
                "acc_stderr": 0.01474151113675408,
                "timestamp": "2024-11-20T10-20-44.927267"
            },
            "truthfulqa_gen": {
                "bleu_max": 17.563201921535452,
                "bleu_max_stderr": 0.6084253447461239,
                "bleu_acc": 0.3769889840881273,
                "bleu_acc_stderr": 0.016965517578930358,
                "bleu_diff": -2.9441042316118144,
                "bleu_diff_stderr": 0.6521697109915429,
                "rouge1_max": 40.112597617421514,
                "rouge1_max_stderr": 0.8426548513555856,
                "rouge1_acc": 0.3353733170134639,
                "rouge1_acc_stderr": 0.01652753403966899,
                "rouge1_diff": -5.161390138276828,
                "rouge1_diff_stderr": 0.9272668238645785,
                "rouge2_max": 21.567769834989484,
                "rouge2_max_stderr": 0.9243570922115246,
                "rouge2_acc": 0.1909424724602203,
                "rouge2_acc_stderr": 0.013759285842685714,
                "rouge2_diff": -7.056555663275975,
                "rouge2_diff_stderr": 0.9635864851509317,
                "rougeL_max": 37.40770287988807,
                "rougeL_max_stderr": 0.8348939987940525,
                "rougeL_acc": 0.32068543451652387,
                "rougeL_acc_stderr": 0.016339170373280917,
                "rougeL_diff": -5.184697052173686,
                "rougeL_diff_stderr": 0.9244121757045393,
                "timestamp": "2024-11-20T10-20-44.927267"
            },
            "truthfulqa_mc1": {
                "acc": 0.25703794369645044,
                "acc_stderr": 0.015298077509485083,
                "timestamp": "2024-11-20T10-20-44.927267"
            },
            "winogrande": {
                "acc": 0.55327545382794,
                "acc_stderr": 0.013972488371616687,
                "timestamp": "2024-11-20T10-20-44.927267"
            }
        }
    }
}