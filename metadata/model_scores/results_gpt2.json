{
    "model_name": "gpt2",
    "results": {
        "harness": {
            "anli": {
                "0-shot": {
                    "acc": 0.345,
                    "acc_stderr": 0.0147,
                    "timestamp": "2024-05-03 14:35:00.111304"
                }
            },
            "xnli": {
                "0-shot": {
                    "acc": 0.3506,
                    "acc_stderr": 0.0249,
                    "timestamp": "2024-05-03 14:35:01.257845"
                }
            },
            "mathqa": {
                "5-shot": {
                    "acc": 0.2111,
                    "acc_stderr": 0.0075,
                    "timestamp": "2024-05-03 14:35:01.842230"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "acc": 0.2392,
                    "acc_stderr": 0.0108,
                    "timestamp": "2024-05-03 14:35:02.374285"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.21052631578947367,
                    "acc_stderr": 0.031267817146631786,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.14285714285714285,
                    "acc_stderr": 0.031298431857438073,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.22530864197530864,
                    "acc_stderr": 0.023246202647819746,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.2424581005586592,
                    "acc_stderr": 0.014333522059217887,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.2489451476793249,
                    "acc_stderr": 0.028146970599422644,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.24277456647398843,
                    "acc_stderr": 0.023083658586984204,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.24641460234680573,
                    "acc_stderr": 0.011005971399927244,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.25766871165644173,
                    "acc_stderr": 0.03436150827846917,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.03039153369274154,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.2540192926045016,
                    "acc_stderr": 0.02472386150477169,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.21296296296296297,
                    "acc_stderr": 0.039578354719809784,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.32231404958677684,
                    "acc_stderr": 0.042664163633521685,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.21212121212121213,
                    "acc_stderr": 0.03192271569548299,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.36787564766839376,
                    "acc_stderr": 0.034801756684660366,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.28991596638655465,
                    "acc_stderr": 0.029472485833136098,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.35353535353535354,
                    "acc_stderr": 0.03406086723547153,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.3486238532110092,
                    "acc_stderr": 0.020431254090714328,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.21818181818181817,
                    "acc_stderr": 0.03955932861795833,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.22885572139303484,
                    "acc_stderr": 0.029705284056772453,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.2717948717948718,
                    "acc_stderr": 0.022556551010132368,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.031362502409358936,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.2630718954248366,
                    "acc_stderr": 0.01781267654232065,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.26717557251908397,
                    "acc_stderr": 0.03880848301082397,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.2543859649122807,
                    "acc_stderr": 0.0409698513984367,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.21839080459770116,
                    "acc_stderr": 0.01477435831993449,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.1794871794871795,
                    "acc_stderr": 0.025140935950335445,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.3300970873786408,
                    "acc_stderr": 0.046561471100123486,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.21895424836601307,
                    "acc_stderr": 0.02367908986180772,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.26905829596412556,
                    "acc_stderr": 0.029763779406874972,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.44485294117647056,
                    "acc_stderr": 0.03018753206032938,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.23699421965317918,
                    "acc_stderr": 0.03242414757483098,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536955,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.2339622641509434,
                    "acc_stderr": 0.02605529690115292,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.15,
                    "acc_stderr": 0.035887028128263714,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.1927710843373494,
                    "acc_stderr": 0.03070982405056527,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.26595744680851063,
                    "acc_stderr": 0.026358065698880592,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.2549019607843137,
                    "acc_stderr": 0.043364327079931785,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.2781456953642384,
                    "acc_stderr": 0.03658603262763743,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.3032258064516129,
                    "acc_stderr": 0.02614868593067175,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.03476590104304134,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.23703703703703705,
                    "acc_stderr": 0.03673731683969506,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.2,
                    "acc_stderr": 0.040201512610368445,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.16,
                    "acc_stderr": 0.03684529491774709,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.16447368421052633,
                    "acc_stderr": 0.03016753346863271,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542127,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.2680851063829787,
                    "acc_stderr": 0.02895734278834235,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768078,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.23214285714285715,
                    "acc_stderr": 0.04007341809755806,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.270935960591133,
                    "acc_stderr": 0.031270907132976984,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.4722222222222222,
                    "acc_stderr": 0.0340470532865388,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.25396825396825395,
                    "acc_stderr": 0.022418042891113946,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.2413793103448276,
                    "acc_stderr": 0.03565998174135302,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.26296296296296295,
                    "acc_stderr": 0.026842057873833713,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.19197952218430034,
                    "acc_stderr": 0.01150959890659812,
                    "acc_norm": 0.21843003412969283,
                    "acc_norm_stderr": 0.01207429160570097,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.291575383389763,
                    "acc_stderr": 0.004535589759202636,
                    "acc_norm": 0.31517625970922125,
                    "acc_norm_stderr": 0.004636365534819766,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.4069358340982317,
                    "acc_stderr": 0.01492194956899506,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 0.7189915567640092,
                    "bleu_max_stderr": 0.18475025971013068,
                    "bleu_acc": 0.009791921664626682,
                    "bleu_acc_stderr": 0.003447085998464435,
                    "bleu_diff": -0.16841589341392382,
                    "bleu_diff_stderr": 0.11447969225022704,
                    "rouge1_max": 1.4549180041272496,
                    "rouge1_max_stderr": 0.31578733165082745,
                    "rouge1_acc": 0.008567931456548347,
                    "rouge1_acc_stderr": 0.003226445945631001,
                    "rouge1_diff": -0.3752635907645988,
                    "rouge1_diff_stderr": 0.182185700437024,
                    "rouge2_max": 0.8607031116173555,
                    "rouge2_max_stderr": 0.23682006006327766,
                    "rouge2_acc": 0.0036719706242350062,
                    "rouge2_acc_stderr": 0.002117413579031932,
                    "rouge2_diff": -0.3291615551523294,
                    "rouge2_diff_stderr": 0.14899511750185152,
                    "rougeL_max": 1.3855912860137218,
                    "rougeL_max_stderr": 0.30611710377860496,
                    "rougeL_acc": 0.009791921664626682,
                    "rougeL_acc_stderr": 0.0034470859984644473,
                    "rougeL_diff": -0.34950587425767154,
                    "rougeL_diff_stderr": 0.16209746133838165,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.22766217870257038,
                    "acc_stderr": 0.014679255032111068,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5114443567482242,
                    "acc_stderr": 0.014048804199859322,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.006823351023502654,
                    "acc_stderr": 0.002267537102254489,
                    "timestamp": "2024-11-21T17-38-45.405469"
                }
            }
        }
    },
    "last_updated": "2024-12-04 11:24:51.479133"
}