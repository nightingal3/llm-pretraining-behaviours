{
    "model_name": "NousResearch/Nous-Hermes-2-Yi-34B",
    "last_updated": "2023-12-29",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.64419795221843,
                    "acc_stderr": 0.013990571137918762,
                    "acc_norm": 0.6689419795221843,
                    "acc_norm_stderr": 0.01375206241981783,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6577375024895439,
                    "acc_stderr": 0.004734972668299615,
                    "acc_norm": 0.8549093806014738,
                    "acc_norm_stderr": 0.0035147239847366095,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.05024183937956913,
                    "acc_norm": 0.49,
                    "acc_norm_stderr": 0.05024183937956913,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.7185185185185186,
                    "acc_stderr": 0.03885004245800253,
                    "acc_norm": 0.7185185185185186,
                    "acc_norm_stderr": 0.03885004245800253,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.8947368421052632,
                    "acc_stderr": 0.024974533450920707,
                    "acc_norm": 0.8947368421052632,
                    "acc_norm_stderr": 0.024974533450920707,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.78,
                    "acc_stderr": 0.04163331998932262,
                    "acc_norm": 0.78,
                    "acc_norm_stderr": 0.04163331998932262,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.8,
                    "acc_stderr": 0.02461829819586651,
                    "acc_norm": 0.8,
                    "acc_norm_stderr": 0.02461829819586651,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.9027777777777778,
                    "acc_stderr": 0.02477451625044017,
                    "acc_norm": 0.9027777777777778,
                    "acc_norm_stderr": 0.02477451625044017,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.51,
                    "acc_stderr": 0.05024183937956912,
                    "acc_norm": 0.51,
                    "acc_norm_stderr": 0.05024183937956912,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.65,
                    "acc_stderr": 0.04793724854411018,
                    "acc_norm": 0.65,
                    "acc_norm_stderr": 0.04793724854411018,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.49,
                    "acc_stderr": 0.05024183937956914,
                    "acc_norm": 0.49,
                    "acc_norm_stderr": 0.05024183937956914,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.6994219653179191,
                    "acc_stderr": 0.0349610148119118,
                    "acc_norm": 0.6994219653179191,
                    "acc_norm_stderr": 0.0349610148119118,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.5294117647058824,
                    "acc_stderr": 0.049665709039785295,
                    "acc_norm": 0.5294117647058824,
                    "acc_norm_stderr": 0.049665709039785295,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.83,
                    "acc_stderr": 0.03775251680686371,
                    "acc_norm": 0.83,
                    "acc_norm_stderr": 0.03775251680686371,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.7914893617021277,
                    "acc_stderr": 0.026556982117838728,
                    "acc_norm": 0.7914893617021277,
                    "acc_norm_stderr": 0.026556982117838728,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.5701754385964912,
                    "acc_stderr": 0.04657047260594963,
                    "acc_norm": 0.5701754385964912,
                    "acc_norm_stderr": 0.04657047260594963,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.7724137931034483,
                    "acc_stderr": 0.03493950380131184,
                    "acc_norm": 0.7724137931034483,
                    "acc_norm_stderr": 0.03493950380131184,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.6904761904761905,
                    "acc_stderr": 0.023809523809523864,
                    "acc_norm": 0.6904761904761905,
                    "acc_norm_stderr": 0.023809523809523864,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.5793650793650794,
                    "acc_stderr": 0.04415438226743745,
                    "acc_norm": 0.5793650793650794,
                    "acc_norm_stderr": 0.04415438226743745,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.51,
                    "acc_stderr": 0.05024183937956911,
                    "acc_norm": 0.51,
                    "acc_norm_stderr": 0.05024183937956911,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.896774193548387,
                    "acc_stderr": 0.01730838128103453,
                    "acc_norm": 0.896774193548387,
                    "acc_norm_stderr": 0.01730838128103453,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.6206896551724138,
                    "acc_stderr": 0.03413963805906235,
                    "acc_norm": 0.6206896551724138,
                    "acc_norm_stderr": 0.03413963805906235,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.84,
                    "acc_stderr": 0.03684529491774709,
                    "acc_norm": 0.84,
                    "acc_norm_stderr": 0.03684529491774709,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.8787878787878788,
                    "acc_stderr": 0.02548549837334323,
                    "acc_norm": 0.8787878787878788,
                    "acc_norm_stderr": 0.02548549837334323,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.898989898989899,
                    "acc_stderr": 0.021469735576055353,
                    "acc_norm": 0.898989898989899,
                    "acc_norm_stderr": 0.021469735576055353,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.9792746113989638,
                    "acc_stderr": 0.010281417011909039,
                    "acc_norm": 0.9792746113989638,
                    "acc_norm_stderr": 0.010281417011909039,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.8282051282051283,
                    "acc_stderr": 0.01912490360342356,
                    "acc_norm": 0.8282051282051283,
                    "acc_norm_stderr": 0.01912490360342356,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.4148148148148148,
                    "acc_stderr": 0.03003984245406929,
                    "acc_norm": 0.4148148148148148,
                    "acc_norm_stderr": 0.03003984245406929,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.8529411764705882,
                    "acc_stderr": 0.023005459446673936,
                    "acc_norm": 0.8529411764705882,
                    "acc_norm_stderr": 0.023005459446673936,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.5033112582781457,
                    "acc_stderr": 0.04082393379449654,
                    "acc_norm": 0.5033112582781457,
                    "acc_norm_stderr": 0.04082393379449654,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.9211009174311927,
                    "acc_stderr": 0.011558198113769569,
                    "acc_norm": 0.9211009174311927,
                    "acc_norm_stderr": 0.011558198113769569,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.6620370370370371,
                    "acc_stderr": 0.03225941352631295,
                    "acc_norm": 0.6620370370370371,
                    "acc_norm_stderr": 0.03225941352631295,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.9117647058823529,
                    "acc_stderr": 0.019907399791316952,
                    "acc_norm": 0.9117647058823529,
                    "acc_norm_stderr": 0.019907399791316952,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.9071729957805907,
                    "acc_stderr": 0.01888975055095671,
                    "acc_norm": 0.9071729957805907,
                    "acc_norm_stderr": 0.01888975055095671,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.7937219730941704,
                    "acc_stderr": 0.027157150479563824,
                    "acc_norm": 0.7937219730941704,
                    "acc_norm_stderr": 0.027157150479563824,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.8931297709923665,
                    "acc_stderr": 0.027096548624883733,
                    "acc_norm": 0.8931297709923665,
                    "acc_norm_stderr": 0.027096548624883733,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.9090909090909091,
                    "acc_stderr": 0.02624319405407388,
                    "acc_norm": 0.9090909090909091,
                    "acc_norm_stderr": 0.02624319405407388,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8981481481481481,
                    "acc_stderr": 0.02923927267563274,
                    "acc_norm": 0.8981481481481481,
                    "acc_norm_stderr": 0.02923927267563274,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.8711656441717791,
                    "acc_stderr": 0.026321383198783653,
                    "acc_norm": 0.8711656441717791,
                    "acc_norm_stderr": 0.026321383198783653,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.6071428571428571,
                    "acc_stderr": 0.04635550135609976,
                    "acc_norm": 0.6071428571428571,
                    "acc_norm_stderr": 0.04635550135609976,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.9223300970873787,
                    "acc_stderr": 0.026501440784762766,
                    "acc_norm": 0.9223300970873787,
                    "acc_norm_stderr": 0.026501440784762766,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.9188034188034188,
                    "acc_stderr": 0.017893784904018536,
                    "acc_norm": 0.9188034188034188,
                    "acc_norm_stderr": 0.017893784904018536,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.03487350880197771,
                    "acc_norm": 0.86,
                    "acc_norm_stderr": 0.03487350880197771,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.9106002554278416,
                    "acc_stderr": 0.010203017847688307,
                    "acc_norm": 0.9106002554278416,
                    "acc_norm_stderr": 0.010203017847688307,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.8352601156069365,
                    "acc_stderr": 0.019971040982442265,
                    "acc_norm": 0.8352601156069365,
                    "acc_norm_stderr": 0.019971040982442265,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.7106145251396648,
                    "acc_stderr": 0.015166544550490288,
                    "acc_norm": 0.7106145251396648,
                    "acc_norm_stderr": 0.015166544550490288,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.8431372549019608,
                    "acc_stderr": 0.02082375883758091,
                    "acc_norm": 0.8431372549019608,
                    "acc_norm_stderr": 0.02082375883758091,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.8135048231511254,
                    "acc_stderr": 0.022122439772480768,
                    "acc_norm": 0.8135048231511254,
                    "acc_norm_stderr": 0.022122439772480768,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.8888888888888888,
                    "acc_stderr": 0.017486432785880704,
                    "acc_norm": 0.8888888888888888,
                    "acc_norm_stderr": 0.017486432785880704,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.648936170212766,
                    "acc_stderr": 0.02847350127296376,
                    "acc_norm": 0.648936170212766,
                    "acc_norm_stderr": 0.02847350127296376,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.6166883963494133,
                    "acc_stderr": 0.012417603662901185,
                    "acc_norm": 0.6166883963494133,
                    "acc_norm_stderr": 0.012417603662901185,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.8308823529411765,
                    "acc_stderr": 0.02277086801011301,
                    "acc_norm": 0.8308823529411765,
                    "acc_norm_stderr": 0.02277086801011301,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.8235294117647058,
                    "acc_stderr": 0.015422512066262549,
                    "acc_norm": 0.8235294117647058,
                    "acc_norm_stderr": 0.015422512066262549,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.7181818181818181,
                    "acc_stderr": 0.043091187099464585,
                    "acc_norm": 0.7181818181818181,
                    "acc_norm_stderr": 0.043091187099464585,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.8408163265306122,
                    "acc_stderr": 0.023420972069166344,
                    "acc_norm": 0.8408163265306122,
                    "acc_norm_stderr": 0.023420972069166344,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8756218905472637,
                    "acc_stderr": 0.023335401790166323,
                    "acc_norm": 0.8756218905472637,
                    "acc_norm_stderr": 0.023335401790166323,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.92,
                    "acc_stderr": 0.0272659924344291,
                    "acc_norm": 0.92,
                    "acc_norm_stderr": 0.0272659924344291,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.572289156626506,
                    "acc_stderr": 0.038515976837185335,
                    "acc_norm": 0.572289156626506,
                    "acc_norm_stderr": 0.038515976837185335,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8713450292397661,
                    "acc_stderr": 0.025679342723276908,
                    "acc_norm": 0.8713450292397661,
                    "acc_norm_stderr": 0.025679342723276908,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.4357405140758874,
                    "mc1_stderr": 0.017358345398863127,
                    "mc2": 0.6037423421940498,
                    "mc2_stderr": 0.014892857583579324,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.829518547750592,
                    "acc_stderr": 0.01056902112282592,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.7005307050796058,
                    "acc_stderr": 0.012616300735519658,
                    "timestamp": "2023-12-29T16-55-23.292289"
                }
            }
        }
    }
}