{
    "model_name": "EleutherAI/gpt-j-6b",
    "last_updated": "2024-12-04 11:26:02.029746",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.018315018315018316,
                "exact_match_stderr": 0.005743696731653661,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.04477611940298507,
                "exact_match_stderr": 0.007011584710623335,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.022222222222222223,
                "exact_match_stderr": 0.0063492063492063145,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.023255813953488372,
                "exact_match_stderr": 0.0050182572516275815,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "minerva_math_geometry": {
                "exact_match": 0.022964509394572025,
                "exact_match_stderr": 0.006851249878769254,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.029535864978902954,
                "exact_match_stderr": 0.007784559126948299,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "minerva_math_algebra": {
                "exact_match": 0.02358887952822241,
                "exact_match_stderr": 0.004406843931023534,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_3da": {
                "acc": 0.087,
                "acc_stderr": 0.006303599581496389,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_3ds": {
                "acc": 0.0465,
                "acc_stderr": 0.004709561018023942,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_4da": {
                "acc": 0.007,
                "acc_stderr": 0.0018647355360237453,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_2ds": {
                "acc": 0.2175,
                "acc_stderr": 0.009227103810100017,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_5da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000127,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_1dc": {
                "acc": 0.089,
                "acc_stderr": 0.006368656050529475,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_4ds": {
                "acc": 0.0055,
                "acc_stderr": 0.0016541593398342208,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_2dm": {
                "acc": 0.1395,
                "acc_stderr": 0.007749187050909062,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "arithmetic_2da": {
                "acc": 0.24,
                "acc_stderr": 0.009552257472001268,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "gsm8k_cot": {
                "exact_match": 0.03335860500379075,
                "exact_match_stderr": 0.004946282649173773,
                "timestamp": "2024-06-13T16-47-03.193840"
            },
            "gsm8k": {
                "exact_match": 0.03639120545868082,
                "exact_match_stderr": 0.005158113489231192,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "anli_r2": {
                "brier_score": 0.7625067534704538,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "anli_r3": {
                "brier_score": 0.7268456375297242,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "anli_r1": {
                "brier_score": 0.7767404198801905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_eu": {
                "brier_score": 0.840514896839387,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_vi": {
                "brier_score": 0.758218907162874,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_ru": {
                "brier_score": 0.7743489919871677,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_zh": {
                "brier_score": 0.9538503257434168,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_tr": {
                "brier_score": 0.8753439505463679,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_fr": {
                "brier_score": 0.7600564801060831,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_en": {
                "brier_score": 0.631686417358844,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_ur": {
                "brier_score": 0.9619292774250547,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_ar": {
                "brier_score": 1.1116475345057533,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_de": {
                "brier_score": 0.8152549958871562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_hi": {
                "brier_score": 0.7460150666217235,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_es": {
                "brier_score": 0.7914512101723771,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_bg": {
                "brier_score": 0.7416509603093395,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_sw": {
                "brier_score": 0.8512933047395764,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_el": {
                "brier_score": 1.089738388519557,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "xnli_th": {
                "brier_score": 0.828907727940754,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "logiqa2": {
                "brier_score": 1.1310390010190252,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "mathqa": {
                "brier_score": 0.9512998262714331,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T17-23-39.734007"
            },
            "lambada_standard": {
                "perplexity": 5.680920517213677,
                "perplexity_stderr": 0.13320913025647432,
                "acc": 0.6136231321560256,
                "acc_stderr": 0.006783729046949487,
                "timestamp": "2024-06-13T17-26-24.151637"
            },
            "lambada_openai": {
                "perplexity": 4.1024027240281375,
                "perplexity_stderr": 0.08833542272706026,
                "acc": 0.6830972249175238,
                "acc_stderr": 0.00648210937056637,
                "timestamp": "2024-06-13T17-26-24.151637"
            },
            "mmlu_world_religions": {
                "acc": 0.34502923976608185,
                "acc_stderr": 0.036459813773888065,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_formal_logic": {
                "acc": 0.18253968253968253,
                "acc_stderr": 0.03455071019102151,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_prehistory": {
                "acc": 0.30864197530864196,
                "acc_stderr": 0.02570264026060376,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2424581005586592,
                "acc_stderr": 0.014333522059217892,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.2869198312236287,
                "acc_stderr": 0.029443773022594693,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26878612716763006,
                "acc_stderr": 0.02386800326250011,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_professional_law": {
                "acc": 0.288135593220339,
                "acc_stderr": 0.011567140661324568,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.24539877300613497,
                "acc_stderr": 0.03380939813943353,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.28431372549019607,
                "acc_stderr": 0.03166009679399813,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_philosophy": {
                "acc": 0.26688102893890675,
                "acc_stderr": 0.02512263760881664,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_jurisprudence": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.04284467968052191,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_international_law": {
                "acc": 0.24793388429752067,
                "acc_stderr": 0.03941897526516303,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.2787878787878788,
                "acc_stderr": 0.035014387062967806,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21761658031088082,
                "acc_stderr": 0.02977866303775296,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.02865749128507196,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_geography": {
                "acc": 0.23232323232323232,
                "acc_stderr": 0.030088629490217483,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.23302752293577983,
                "acc_stderr": 0.018125669180861507,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_public_relations": {
                "acc": 0.34545454545454546,
                "acc_stderr": 0.04554619617541054,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_sociology": {
                "acc": 0.2885572139303483,
                "acc_stderr": 0.03203841040213322,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2358974358974359,
                "acc_stderr": 0.021525965407408726,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_security_studies": {
                "acc": 0.35918367346938773,
                "acc_stderr": 0.03071356045510849,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_professional_psychology": {
                "acc": 0.27941176470588236,
                "acc_stderr": 0.018152871051538826,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_human_sexuality": {
                "acc": 0.22137404580152673,
                "acc_stderr": 0.03641297081313729,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_econometrics": {
                "acc": 0.2807017543859649,
                "acc_stderr": 0.042270544512322,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_miscellaneous": {
                "acc": 0.3128991060025543,
                "acc_stderr": 0.016580935940304055,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_marketing": {
                "acc": 0.27350427350427353,
                "acc_stderr": 0.02920254015343117,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_management": {
                "acc": 0.20388349514563106,
                "acc_stderr": 0.039891398595317706,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_nutrition": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.025261691219729494,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_medical_genetics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_human_aging": {
                "acc": 0.3273542600896861,
                "acc_stderr": 0.03149384670994131,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_professional_medicine": {
                "acc": 0.23897058823529413,
                "acc_stderr": 0.025905280644893006,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_college_medicine": {
                "acc": 0.27167630057803466,
                "acc_stderr": 0.03391750322321659,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_business_ethics": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.25660377358490566,
                "acc_stderr": 0.02688064788905199,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_global_facts": {
                "acc": 0.22,
                "acc_stderr": 0.0416333199893227,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_virology": {
                "acc": 0.3373493975903614,
                "acc_stderr": 0.03680783690727581,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2765957446808511,
                "acc_stderr": 0.026684564340461025,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.04092563958237654,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_physics": {
                "acc": 0.26490066225165565,
                "acc_stderr": 0.03603038545360384,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_biology": {
                "acc": 0.20967741935483872,
                "acc_stderr": 0.023157879349083515,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_anatomy": {
                "acc": 0.2740740740740741,
                "acc_stderr": 0.03853254836552003,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_college_chemistry": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653694,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_computer_security": {
                "acc": 0.41,
                "acc_stderr": 0.049431107042371025,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_college_computer_science": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165044,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_astronomy": {
                "acc": 0.2631578947368421,
                "acc_stderr": 0.03583496176361064,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_college_mathematics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.33617021276595743,
                "acc_stderr": 0.030881618520676942,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.15,
                "acc_stderr": 0.03588702812826373,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_machine_learning": {
                "acc": 0.38392857142857145,
                "acc_stderr": 0.04616143075028547,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.24630541871921183,
                "acc_stderr": 0.03031509928561773,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1574074074074074,
                "acc_stderr": 0.02483717351824239,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.23544973544973544,
                "acc_stderr": 0.021851509822031708,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2896551724137931,
                "acc_stderr": 0.037800192304380135,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2851851851851852,
                "acc_stderr": 0.027528599210340492,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "arc_challenge": {
                "acc": 0.36177474402730375,
                "acc_stderr": 0.014041957945038076,
                "acc_norm": 0.4087030716723549,
                "acc_norm_stderr": 0.014365750345427008,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "hellaswag": {
                "acc": 0.49611631149173474,
                "acc_stderr": 0.004989630887066199,
                "acc_norm": 0.6761601274646485,
                "acc_norm_stderr": 0.004669834130977053,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "truthfulqa_mc2": {
                "acc": 0.35957103209275876,
                "acc_stderr": 0.013461019906422903,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "truthfulqa_gen": {
                "bleu_max": 22.80807652676079,
                "bleu_max_stderr": 0.729394081143316,
                "bleu_acc": 0.2998776009791922,
                "bleu_acc_stderr": 0.01604035296671363,
                "bleu_diff": -7.044836167856834,
                "bleu_diff_stderr": 0.7649652886092126,
                "rouge1_max": 46.0394772961559,
                "rouge1_max_stderr": 0.8908991054603245,
                "rouge1_acc": 0.2558139534883721,
                "rouge1_acc_stderr": 0.01527417621928336,
                "rouge1_diff": -10.481586263318324,
                "rouge1_diff_stderr": 0.9003692764067813,
                "rouge2_max": 29.614709663039555,
                "rouge2_max_stderr": 0.9851662196235105,
                "rouge2_acc": 0.20318237454100369,
                "rouge2_acc_stderr": 0.014085666526340879,
                "rouge2_diff": -11.404292789473761,
                "rouge2_diff_stderr": 1.0230238475013214,
                "rougeL_max": 43.38528981216948,
                "rougeL_max_stderr": 0.8926391261722091,
                "rougeL_acc": 0.24357405140758873,
                "rougeL_acc_stderr": 0.01502635482491078,
                "rougeL_diff": -10.689938323819645,
                "rougeL_diff_stderr": 0.9132036865292477,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "truthfulqa_mc1": {
                "acc": 0.20195838433292534,
                "acc_stderr": 0.014053957441512346,
                "timestamp": "2024-11-22T19-43-59.503231"
            },
            "winogrande": {
                "acc": 0.6479873717442778,
                "acc_stderr": 0.013422874824929718,
                "timestamp": "2024-11-22T19-43-59.503231"
            }
        }
    }
}