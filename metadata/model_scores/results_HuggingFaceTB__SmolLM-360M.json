{
    "model_name": "HuggingFaceTB__SmolLM-360M",
    "last_updated": "2024-12-19 13:40:57.063141",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.03565079670708312,
                "brier_score": 0.7489952381826336,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_formal_logic": {
                "acc": 0.3253968253968254,
                "acc_stderr": 0.04190596438871137,
                "brier_score": 0.744205156755892,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_prehistory": {
                "acc": 0.22530864197530864,
                "acc_stderr": 0.023246202647819753,
                "brier_score": 0.7558066141669159,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.2424581005586592,
                "acc_stderr": 0.014333522059217892,
                "brier_score": 0.7580088742032026,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.3080168776371308,
                "acc_stderr": 0.0300523893356057,
                "brier_score": 0.7481386433912905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_moral_disputes": {
                "acc": 0.26011560693641617,
                "acc_stderr": 0.023618678310069363,
                "brier_score": 0.7537079980328932,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_professional_law": {
                "acc": 0.24185136897001303,
                "acc_stderr": 0.010936550813827075,
                "brier_score": 0.7553302357472506,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2147239263803681,
                "acc_stderr": 0.032262193772867744,
                "brier_score": 0.7586846766296248,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.22549019607843138,
                "acc_stderr": 0.02933116229425173,
                "brier_score": 0.7579910312618601,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_philosophy": {
                "acc": 0.20257234726688103,
                "acc_stderr": 0.022827317491059675,
                "brier_score": 0.7625719281567497,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.0401910747255735,
                "brier_score": 0.758042935757606,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_international_law": {
                "acc": 0.2727272727272727,
                "acc_stderr": 0.04065578140908705,
                "brier_score": 0.7402562726111097,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.28484848484848485,
                "acc_stderr": 0.035243908445117836,
                "brier_score": 0.7518476305982694,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.21761658031088082,
                "acc_stderr": 0.02977866303775297,
                "brier_score": 0.7561294622120225,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.027553614467863818,
                "brier_score": 0.7507375033633016,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_geography": {
                "acc": 0.23232323232323232,
                "acc_stderr": 0.030088629490217483,
                "brier_score": 0.7523217766742214,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.2018348623853211,
                "acc_stderr": 0.01720857935778759,
                "brier_score": 0.7599936909886118,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_public_relations": {
                "acc": 0.22727272727272727,
                "acc_stderr": 0.04013964554072775,
                "brier_score": 0.7501591730485774,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.7527221718432113,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_sociology": {
                "acc": 0.21890547263681592,
                "acc_stderr": 0.029239174636647,
                "brier_score": 0.7640614797494962,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.24358974358974358,
                "acc_stderr": 0.02176373368417391,
                "brier_score": 0.7528827648024928,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_security_studies": {
                "acc": 0.20408163265306123,
                "acc_stderr": 0.025801283475090513,
                "brier_score": 0.7677579726501517,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25326797385620914,
                "acc_stderr": 0.017593486895366835,
                "brier_score": 0.7584410341064913,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_human_sexuality": {
                "acc": 0.25190839694656486,
                "acc_stderr": 0.038073871163060866,
                "brier_score": 0.7628773720030533,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_econometrics": {
                "acc": 0.2719298245614035,
                "acc_stderr": 0.041857744240220575,
                "brier_score": 0.7629268418474607,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_miscellaneous": {
                "acc": 0.27586206896551724,
                "acc_stderr": 0.015982814774695625,
                "brier_score": 0.7496129073986078,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_marketing": {
                "acc": 0.27350427350427353,
                "acc_stderr": 0.029202540153431173,
                "brier_score": 0.7543714738187707,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_management": {
                "acc": 0.2815533980582524,
                "acc_stderr": 0.04453254836326467,
                "brier_score": 0.7475248395770401,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_nutrition": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.024288619466046116,
                "brier_score": 0.7556467124494557,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_medical_genetics": {
                "acc": 0.31,
                "acc_stderr": 0.04648231987117316,
                "brier_score": 0.7351355841978402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_human_aging": {
                "acc": 0.336322869955157,
                "acc_stderr": 0.031708824268455,
                "brier_score": 0.7448421549573002,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_professional_medicine": {
                "acc": 0.2977941176470588,
                "acc_stderr": 0.027778298701545443,
                "brier_score": 0.7454242717298569,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_college_medicine": {
                "acc": 0.20809248554913296,
                "acc_stderr": 0.030952890217749867,
                "brier_score": 0.7532253481406085,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_business_ethics": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "brier_score": 0.7491100760740861,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.25660377358490566,
                "acc_stderr": 0.026880647889051996,
                "brier_score": 0.7507402312612156,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_global_facts": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7467218803779668,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_virology": {
                "acc": 0.2891566265060241,
                "acc_stderr": 0.03529486801511115,
                "brier_score": 0.7489050763698124,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_professional_accounting": {
                "acc": 0.2695035460992908,
                "acc_stderr": 0.026469036818590634,
                "brier_score": 0.7560126391237888,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_college_physics": {
                "acc": 0.2647058823529412,
                "acc_stderr": 0.043898699568087785,
                "brier_score": 0.7657096561986334,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_physics": {
                "acc": 0.23178807947019867,
                "acc_stderr": 0.034454062719870546,
                "brier_score": 0.7666365700704936,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_biology": {
                "acc": 0.1967741935483871,
                "acc_stderr": 0.02261640942074203,
                "brier_score": 0.7657881483222735,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_college_biology": {
                "acc": 0.2152777777777778,
                "acc_stderr": 0.03437079344106135,
                "brier_score": 0.7582642369634256,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_anatomy": {
                "acc": 0.21481481481481482,
                "acc_stderr": 0.035478541985608236,
                "brier_score": 0.7615635040953493,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_college_chemistry": {
                "acc": 0.14,
                "acc_stderr": 0.034873508801977704,
                "brier_score": 0.7852103443073895,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_computer_security": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.7376635177692521,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_college_computer_science": {
                "acc": 0.21,
                "acc_stderr": 0.04093601807403326,
                "brier_score": 0.7581582435991463,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_astronomy": {
                "acc": 0.21710526315789475,
                "acc_stderr": 0.03355045304882921,
                "brier_score": 0.7593982714223672,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_college_mathematics": {
                "acc": 0.24,
                "acc_stderr": 0.042923469599092816,
                "brier_score": 0.7655389360560707,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2936170212765957,
                "acc_stderr": 0.02977164271249123,
                "brier_score": 0.74895512873291,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.23,
                "acc_stderr": 0.042295258468165065,
                "brier_score": 0.7620591635964997,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7487049269340801,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_machine_learning": {
                "acc": 0.22321428571428573,
                "acc_stderr": 0.03952301967702511,
                "brier_score": 0.7577644964672933,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.22660098522167488,
                "acc_stderr": 0.029454863835292965,
                "brier_score": 0.7634815544978084,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.2037037037037037,
                "acc_stderr": 0.027467401804057986,
                "brier_score": 0.7579563734578977,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.23544973544973544,
                "acc_stderr": 0.021851509822031715,
                "brier_score": 0.7556070477970247,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2482758620689655,
                "acc_stderr": 0.036001056927277696,
                "brier_score": 0.765478714407733,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.3,
                "acc_stderr": 0.02794045713622841,
                "brier_score": 0.7494715399118812,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-04-57.693669"
            },
            "arc_challenge": {
                "acc": 0.3651877133105802,
                "acc_stderr": 0.014070265519268802,
                "acc_norm": 0.386518771331058,
                "acc_norm_stderr": 0.014230084761910478,
                "timestamp": "2024-11-10T15-21-10.938137"
            },
            "hellaswag": {
                "acc": 0.41505676160127464,
                "acc_stderr": 0.004917248150601852,
                "acc_norm": 0.5423222465644294,
                "acc_norm_stderr": 0.004971874159777681,
                "timestamp": "2024-11-10T15-21-10.938137"
            },
            "truthfulqa_mc2": {
                "acc": 0.3793017405276516,
                "acc_stderr": 0.014255545371148842,
                "timestamp": "2024-11-10T15-21-10.938137"
            },
            "truthfulqa_gen": {
                "bleu_max": 23.51716504412742,
                "bleu_max_stderr": 0.7176975216279505,
                "bleu_acc": 0.2864137086903305,
                "bleu_acc_stderr": 0.01582614243950238,
                "bleu_diff": -7.083041541904997,
                "bleu_diff_stderr": 0.7077513549659533,
                "rouge1_max": 48.70952743752934,
                "rouge1_max_stderr": 0.8300761683331853,
                "rouge1_acc": 0.27539779681762544,
                "rouge1_acc_stderr": 0.01563813566777552,
                "rouge1_diff": -9.635957207171685,
                "rouge1_diff_stderr": 0.7485923569688212,
                "rouge2_max": 32.566501096217806,
                "rouge2_max_stderr": 0.9326341598087236,
                "rouge2_acc": 0.23623011015911874,
                "rouge2_acc_stderr": 0.014869755015871107,
                "rouge2_diff": -10.895866455590797,
                "rouge2_diff_stderr": 0.9226904079378292,
                "rougeL_max": 45.55323568193951,
                "rougeL_max_stderr": 0.8340494740388494,
                "rougeL_acc": 0.2607099143206854,
                "rougeL_acc_stderr": 0.015368841620766372,
                "rougeL_diff": -9.756463734915389,
                "rougeL_diff_stderr": 0.7580939692627898,
                "timestamp": "2024-11-10T15-21-10.938137"
            },
            "truthfulqa_mc1": {
                "acc": 0.25091799265605874,
                "acc_stderr": 0.015176985027707698,
                "timestamp": "2024-11-10T15-21-10.938137"
            },
            "winogrande": {
                "acc": 0.5572217837411207,
                "acc_stderr": 0.01396015735078497,
                "timestamp": "2024-11-10T15-21-10.938137"
            },
            "gsm8k": {
                "exact_match": 0.013646702047005308,
                "exact_match_stderr": 0.0031957470754808027,
                "timestamp": "2024-11-10T15-21-10.938137"
            }
        }
    }
}