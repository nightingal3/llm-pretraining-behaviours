{
    "model_name": "IEITYuan/Yuan2-2B-hf",
    "last_updated": "2024-12-19 13:41:57.237772",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.01282051282051282,
                "exact_match_stderr": 0.004818950982487626,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.06199770378874857,
                "exact_match_stderr": 0.008175797512062155,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.005203704987512651,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.004429678848283499,
                "exact_match_stderr": 0.0022111531423787936,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "minerva_math_geometry": {
                "exact_match": 0.029227557411273485,
                "exact_match_stderr": 0.007704439205471374,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.014767932489451477,
                "exact_match_stderr": 0.005546238589668469,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "minerva_math_algebra": {
                "exact_match": 0.03369839932603201,
                "exact_match_stderr": 0.0052398474232848045,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_3da": {
                "acc": 0.3295,
                "acc_stderr": 0.010512855704685485,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_3ds": {
                "acc": 0.124,
                "acc_stderr": 0.007371510671822562,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_4da": {
                "acc": 0.112,
                "acc_stderr": 0.007053571892184738,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_2ds": {
                "acc": 0.231,
                "acc_stderr": 0.00942676678219962,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_5ds": {
                "acc": 0.006,
                "acc_stderr": 0.0017272787111155172,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_5da": {
                "acc": 0.0495,
                "acc_stderr": 0.004851457855290592,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_1dc": {
                "acc": 0.303,
                "acc_stderr": 0.010278537063322019,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_4ds": {
                "acc": 0.039,
                "acc_stderr": 0.004329997048176563,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_2dm": {
                "acc": 0.0695,
                "acc_stderr": 0.005687798389997827,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "arithmetic_2da": {
                "acc": 0.5045,
                "acc_stderr": 0.011182683094883916,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "gsm8k_cot": {
                "exact_match": 0.09249431387414708,
                "exact_match_stderr": 0.007980396874560166,
                "timestamp": "2024-06-14T16-08-45.808697"
            },
            "gsm8k": {
                "exact_match": 0.09249431387414708,
                "exact_match_stderr": 0.007980396874560166,
                "timestamp": "2024-11-10T17-48-44.142080"
            },
            "anli_r2": {
                "brier_score": 0.9021877709220847,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "anli_r3": {
                "brier_score": 0.8960203323957571,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "anli_r1": {
                "brier_score": 0.8980224868789101,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_eu": {
                "brier_score": 1.1726851242898388,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_vi": {
                "brier_score": 1.0551222712344248,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_ru": {
                "brier_score": 1.0838363628503949,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_zh": {
                "brier_score": 1.0730192701838772,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_tr": {
                "brier_score": 1.1096395679638529,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_fr": {
                "brier_score": 1.0979677062130364,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_en": {
                "brier_score": 0.7874104438895939,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_ur": {
                "brier_score": 1.3108842358000463,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_ar": {
                "brier_score": 1.0708782622925244,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_de": {
                "brier_score": 0.9569650860704157,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_hi": {
                "brier_score": 1.2300842352985888,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_es": {
                "brier_score": 1.045692200792037,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_bg": {
                "brier_score": 1.1627810243907892,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_sw": {
                "brier_score": 1.088251408596032,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_el": {
                "brier_score": 1.1639414398316317,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "xnli_th": {
                "brier_score": 1.3070235732345081,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "logiqa2": {
                "brier_score": 1.2490949064547225,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "mathqa": {
                "brier_score": 0.990379580332815,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-14T16-16-22.006107"
            },
            "lambada_standard": {
                "perplexity": 523.2726641428518,
                "perplexity_stderr": 28.91907158439682,
                "acc": 0.1622355909179119,
                "acc_stderr": 0.005136249280239084,
                "timestamp": "2024-06-14T06-11-06.726003"
            },
            "lambada_openai": {
                "perplexity": 75.6433032154668,
                "perplexity_stderr": 3.790131948633793,
                "acc": 0.29283912284106345,
                "acc_stderr": 0.006339948563069486,
                "timestamp": "2024-06-14T06-11-06.726003"
            },
            "mmlu_world_religions": {
                "acc": 0.2807017543859649,
                "acc_stderr": 0.03446296217088426,
                "brier_score": 0.8096009779409294,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_formal_logic": {
                "acc": 0.21428571428571427,
                "acc_stderr": 0.03670066451047181,
                "brier_score": 0.8492333490932678,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_prehistory": {
                "acc": 0.22530864197530864,
                "acc_stderr": 0.023246202647819746,
                "brier_score": 0.8711652302179635,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.25027932960893856,
                "acc_stderr": 0.014487500852850407,
                "brier_score": 0.7841230310531501,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.26582278481012656,
                "acc_stderr": 0.028756799629658335,
                "brier_score": 0.8327831803880661,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2514450867052023,
                "acc_stderr": 0.023357365785874037,
                "brier_score": 0.856367471910325,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_professional_law": {
                "acc": 0.22946544980443284,
                "acc_stderr": 0.010739489382279505,
                "brier_score": 0.8877477808741326,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.2085889570552147,
                "acc_stderr": 0.031921934489347235,
                "brier_score": 0.8614073601523989,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25,
                "acc_stderr": 0.03039153369274154,
                "brier_score": 0.8723475480948988,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_philosophy": {
                "acc": 0.2282958199356913,
                "acc_stderr": 0.02383930331139821,
                "brier_score": 0.873744048004299,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_jurisprudence": {
                "acc": 0.26851851851851855,
                "acc_stderr": 0.04284467968052191,
                "brier_score": 0.8528512268750248,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_international_law": {
                "acc": 0.21487603305785125,
                "acc_stderr": 0.03749492448709699,
                "brier_score": 0.8545314075336393,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.23636363636363636,
                "acc_stderr": 0.033175059300091805,
                "brier_score": 0.8557986356963414,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.20725388601036268,
                "acc_stderr": 0.029252823291803638,
                "brier_score": 0.915030398053468,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.027553614467863807,
                "brier_score": 0.8578690440470526,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_geography": {
                "acc": 0.21717171717171718,
                "acc_stderr": 0.02937661648494563,
                "brier_score": 0.9057046642331704,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.22385321100917432,
                "acc_stderr": 0.01787121776779021,
                "brier_score": 0.8527920303028815,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_public_relations": {
                "acc": 0.2545454545454545,
                "acc_stderr": 0.04172343038705383,
                "brier_score": 0.8259331453627159,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.36,
                "acc_stderr": 0.04824181513244218,
                "brier_score": 0.7771535619076206,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_sociology": {
                "acc": 0.2537313432835821,
                "acc_stderr": 0.030769444967296035,
                "brier_score": 0.8309135703592723,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.2282051282051282,
                "acc_stderr": 0.02127839386358628,
                "brier_score": 0.8713082619742111,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_security_studies": {
                "acc": 0.22857142857142856,
                "acc_stderr": 0.026882144922307744,
                "brier_score": 0.8254330669999973,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25326797385620914,
                "acc_stderr": 0.017593486895366835,
                "brier_score": 0.8505259159401877,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_human_sexuality": {
                "acc": 0.3053435114503817,
                "acc_stderr": 0.04039314978724562,
                "brier_score": 0.8192683078241423,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_econometrics": {
                "acc": 0.20175438596491227,
                "acc_stderr": 0.037752050135836386,
                "brier_score": 0.8538722129937036,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_miscellaneous": {
                "acc": 0.23243933588761176,
                "acc_stderr": 0.015104550008905687,
                "brier_score": 0.8194701596707327,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_marketing": {
                "acc": 0.2863247863247863,
                "acc_stderr": 0.02961432369045665,
                "brier_score": 0.8463996248482538,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_management": {
                "acc": 0.22330097087378642,
                "acc_stderr": 0.04123553189891431,
                "brier_score": 0.8491363029476184,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_nutrition": {
                "acc": 0.23529411764705882,
                "acc_stderr": 0.024288619466046105,
                "brier_score": 0.857691953221437,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_medical_genetics": {
                "acc": 0.33,
                "acc_stderr": 0.04725815626252604,
                "brier_score": 0.7921112092000784,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_human_aging": {
                "acc": 0.29596412556053814,
                "acc_stderr": 0.03063659134869981,
                "brier_score": 0.7785455242696859,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_professional_medicine": {
                "acc": 0.19117647058823528,
                "acc_stderr": 0.02388688192244034,
                "brier_score": 0.9392371353631491,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_college_medicine": {
                "acc": 0.2543352601156069,
                "acc_stderr": 0.0332055644308557,
                "brier_score": 0.8467048802466525,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_business_ethics": {
                "acc": 0.18,
                "acc_stderr": 0.03861229196653697,
                "brier_score": 0.8523128729212764,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2490566037735849,
                "acc_stderr": 0.026616482980501704,
                "brier_score": 0.8344599422751379,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_global_facts": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036846,
                "brier_score": 0.8578052760213979,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_virology": {
                "acc": 0.2469879518072289,
                "acc_stderr": 0.03357351982064536,
                "brier_score": 0.8164458967902466,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_professional_accounting": {
                "acc": 0.21631205673758866,
                "acc_stderr": 0.02456172056056277,
                "brier_score": 0.8717352507015241,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_college_physics": {
                "acc": 0.24509803921568626,
                "acc_stderr": 0.04280105837364395,
                "brier_score": 0.8500361889495441,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2052980132450331,
                "acc_stderr": 0.03297986648473835,
                "brier_score": 0.9160716728274827,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2161290322580645,
                "acc_stderr": 0.023415293433568532,
                "brier_score": 0.9022114625100559,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_college_biology": {
                "acc": 0.2361111111111111,
                "acc_stderr": 0.03551446610810826,
                "brier_score": 0.8826057166424514,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_anatomy": {
                "acc": 0.22962962962962963,
                "acc_stderr": 0.03633384414073465,
                "brier_score": 0.8435118363521027,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_college_chemistry": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.8001169174103776,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_computer_security": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768078,
                "brier_score": 0.8297649101265755,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_college_computer_science": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 0.8745706761974686,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_astronomy": {
                "acc": 0.23026315789473684,
                "acc_stderr": 0.03426059424403165,
                "brier_score": 0.8320305956799038,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_college_mathematics": {
                "acc": 0.28,
                "acc_stderr": 0.04512608598542127,
                "brier_score": 0.7980341338123739,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.2723404255319149,
                "acc_stderr": 0.0291012906983867,
                "brier_score": 0.7987215599898191,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.23,
                "acc_stderr": 0.04229525846816506,
                "brier_score": 0.7936128840334643,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.2,
                "acc_stderr": 0.04020151261036845,
                "brier_score": 0.8855977049559729,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_machine_learning": {
                "acc": 0.25,
                "acc_stderr": 0.04109974682633932,
                "brier_score": 0.833709833107949,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.19704433497536947,
                "acc_stderr": 0.02798672466673622,
                "brier_score": 0.8658986710939709,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.22685185185185186,
                "acc_stderr": 0.02856165010242226,
                "brier_score": 0.8810555037720272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.21957671957671956,
                "acc_stderr": 0.021320018599770355,
                "brier_score": 0.8219060988248941,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2620689655172414,
                "acc_stderr": 0.036646663372252565,
                "brier_score": 0.831102037949681,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.02730914058823017,
                "brier_score": 0.8027807301116876,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T17-52-37.408897"
            },
            "arc_challenge": {
                "acc": 0.19880546075085323,
                "acc_stderr": 0.01166285019817553,
                "acc_norm": 0.23976109215017063,
                "acc_norm_stderr": 0.012476304127453947,
                "timestamp": "2024-11-10T17-48-44.142080"
            },
            "hellaswag": {
                "acc": 0.30541724756024696,
                "acc_stderr": 0.004596426220000867,
                "acc_norm": 0.3333001394144593,
                "acc_norm_stderr": 0.004704293898729931,
                "timestamp": "2024-11-10T17-48-44.142080"
            },
            "truthfulqa_mc2": {
                "acc": 0.4250838081980288,
                "acc_stderr": 0.01582787673797006,
                "timestamp": "2024-11-10T17-48-44.142080"
            },
            "truthfulqa_gen": {
                "bleu_max": 15.197501509988685,
                "bleu_max_stderr": 0.544225214068991,
                "bleu_acc": 0.3047735618115055,
                "bleu_acc_stderr": 0.016114124156882417,
                "bleu_diff": -2.903206350824163,
                "bleu_diff_stderr": 0.44799349927215737,
                "rouge1_max": 40.224440846013266,
                "rouge1_max_stderr": 0.7259309767822962,
                "rouge1_acc": 0.31211750305997554,
                "rouge1_acc_stderr": 0.01622075676952096,
                "rouge1_diff": -4.514331396454585,
                "rouge1_diff_stderr": 0.635813668954008,
                "rouge2_max": 23.95451031471122,
                "rouge2_max_stderr": 0.7969484153233573,
                "rouge2_acc": 0.24969400244798043,
                "rouge2_acc_stderr": 0.015152286907148125,
                "rouge2_diff": -5.21174195263994,
                "rouge2_diff_stderr": 0.7068689979410017,
                "rougeL_max": 36.89784769319415,
                "rougeL_max_stderr": 0.7270599149793284,
                "rougeL_acc": 0.30966952264381886,
                "rougeL_acc_stderr": 0.016185744355144936,
                "rougeL_diff": -4.171471272218275,
                "rougeL_diff_stderr": 0.6150306439924023,
                "timestamp": "2024-11-10T17-48-44.142080"
            },
            "truthfulqa_mc1": {
                "acc": 0.25458996328029376,
                "acc_stderr": 0.015250117079156507,
                "timestamp": "2024-11-10T17-48-44.142080"
            },
            "winogrande": {
                "acc": 0.5074980268350434,
                "acc_stderr": 0.01405090552122858,
                "timestamp": "2024-11-10T17-48-44.142080"
            }
        }
    }
}