{
    "model_name": "openlm-research/open_llama_3b_v2",
    "last_updated": "2023-10-15",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.35580204778157,
                    "acc_stderr": 0.013990571137918762,
                    "acc_norm": 0.40273037542662116,
                    "acc_norm_stderr": 0.014332236306790145,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5310695080661223,
                    "acc_stderr": 0.004980138679161043,
                    "acc_norm": 0.7159928301135232,
                    "acc_norm_stderr": 0.004500186424443805,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847415,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847415,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.23703703703703705,
                    "acc_stderr": 0.03673731683969506,
                    "acc_norm": 0.23703703703703705,
                    "acc_norm_stderr": 0.03673731683969506,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.27631578947368424,
                    "acc_stderr": 0.03639057569952924,
                    "acc_norm": 0.27631578947368424,
                    "acc_norm_stderr": 0.03639057569952924,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.04923659639173309,
                    "acc_norm": 0.4,
                    "acc_norm_stderr": 0.04923659639173309,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2679245283018868,
                    "acc_stderr": 0.027257260322494845,
                    "acc_norm": 0.2679245283018868,
                    "acc_norm_stderr": 0.027257260322494845,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.24305555555555555,
                    "acc_stderr": 0.0358687928008034,
                    "acc_norm": 0.24305555555555555,
                    "acc_norm_stderr": 0.0358687928008034,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909284,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909284,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.23699421965317918,
                    "acc_stderr": 0.03242414757483098,
                    "acc_norm": 0.23699421965317918,
                    "acc_norm_stderr": 0.03242414757483098,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.04023382273617749,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.04023382273617749,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621505,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.04688261722621505,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.32340425531914896,
                    "acc_stderr": 0.030579442773610337,
                    "acc_norm": 0.32340425531914896,
                    "acc_norm_stderr": 0.030579442773610337,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.24561403508771928,
                    "acc_stderr": 0.04049339297748143,
                    "acc_norm": 0.24561403508771928,
                    "acc_norm_stderr": 0.04049339297748143,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.20689655172413793,
                    "acc_stderr": 0.03375672449560554,
                    "acc_norm": 0.20689655172413793,
                    "acc_norm_stderr": 0.03375672449560554,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.291005291005291,
                    "acc_stderr": 0.023393826500484875,
                    "acc_norm": 0.291005291005291,
                    "acc_norm_stderr": 0.023393826500484875,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.24603174603174602,
                    "acc_stderr": 0.03852273364924318,
                    "acc_norm": 0.24603174603174602,
                    "acc_norm_stderr": 0.03852273364924318,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.23548387096774193,
                    "acc_stderr": 0.024137632429337707,
                    "acc_norm": 0.23548387096774193,
                    "acc_norm_stderr": 0.024137632429337707,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.21674876847290642,
                    "acc_stderr": 0.028990331252516235,
                    "acc_norm": 0.21674876847290642,
                    "acc_norm_stderr": 0.028990331252516235,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.042923469599092816,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.24848484848484848,
                    "acc_stderr": 0.03374402644139404,
                    "acc_norm": 0.24848484848484848,
                    "acc_norm_stderr": 0.03374402644139404,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.20202020202020202,
                    "acc_stderr": 0.02860620428922988,
                    "acc_norm": 0.20202020202020202,
                    "acc_norm_stderr": 0.02860620428922988,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.23834196891191708,
                    "acc_stderr": 0.03074890536390988,
                    "acc_norm": 0.23834196891191708,
                    "acc_norm_stderr": 0.03074890536390988,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2564102564102564,
                    "acc_stderr": 0.02213908110397155,
                    "acc_norm": 0.2564102564102564,
                    "acc_norm_stderr": 0.02213908110397155,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.23333333333333334,
                    "acc_stderr": 0.025787874220959316,
                    "acc_norm": 0.23333333333333334,
                    "acc_norm_stderr": 0.025787874220959316,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2773109243697479,
                    "acc_stderr": 0.02907937453948001,
                    "acc_norm": 0.2773109243697479,
                    "acc_norm_stderr": 0.02907937453948001,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.33112582781456956,
                    "acc_stderr": 0.038425817186598696,
                    "acc_norm": 0.33112582781456956,
                    "acc_norm_stderr": 0.038425817186598696,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.24770642201834864,
                    "acc_stderr": 0.018508143602547815,
                    "acc_norm": 0.24770642201834864,
                    "acc_norm_stderr": 0.018508143602547815,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.20833333333333334,
                    "acc_stderr": 0.02769691071309394,
                    "acc_norm": 0.20833333333333334,
                    "acc_norm_stderr": 0.02769691071309394,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.029771775228145638,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.029771775228145638,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.25316455696202533,
                    "acc_stderr": 0.028304657943035293,
                    "acc_norm": 0.25316455696202533,
                    "acc_norm_stderr": 0.028304657943035293,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.3991031390134529,
                    "acc_stderr": 0.03286745312567961,
                    "acc_norm": 0.3991031390134529,
                    "acc_norm_stderr": 0.03286745312567961,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.25190839694656486,
                    "acc_stderr": 0.03807387116306086,
                    "acc_norm": 0.25190839694656486,
                    "acc_norm_stderr": 0.03807387116306086,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.2975206611570248,
                    "acc_stderr": 0.04173349148083498,
                    "acc_norm": 0.2975206611570248,
                    "acc_norm_stderr": 0.04173349148083498,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2962962962962963,
                    "acc_stderr": 0.044143436668549335,
                    "acc_norm": 0.2962962962962963,
                    "acc_norm_stderr": 0.044143436668549335,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2147239263803681,
                    "acc_stderr": 0.03226219377286774,
                    "acc_norm": 0.2147239263803681,
                    "acc_norm_stderr": 0.03226219377286774,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.29464285714285715,
                    "acc_stderr": 0.043270409325787296,
                    "acc_norm": 0.29464285714285715,
                    "acc_norm_stderr": 0.043270409325787296,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.27184466019417475,
                    "acc_stderr": 0.044052680241409216,
                    "acc_norm": 0.27184466019417475,
                    "acc_norm_stderr": 0.044052680241409216,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.2863247863247863,
                    "acc_stderr": 0.029614323690456648,
                    "acc_norm": 0.2863247863247863,
                    "acc_norm_stderr": 0.029614323690456648,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.042295258468165044,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.042295258468165044,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.280970625798212,
                    "acc_stderr": 0.016073127851221246,
                    "acc_norm": 0.280970625798212,
                    "acc_norm_stderr": 0.016073127851221246,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.25722543352601157,
                    "acc_stderr": 0.023532925431044287,
                    "acc_norm": 0.25722543352601157,
                    "acc_norm_stderr": 0.023532925431044287,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.24022346368715083,
                    "acc_stderr": 0.014288343803925312,
                    "acc_norm": 0.24022346368715083,
                    "acc_norm_stderr": 0.014288343803925312,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.025261691219729494,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.025261691219729494,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.2765273311897106,
                    "acc_stderr": 0.025403832978179625,
                    "acc_norm": 0.2765273311897106,
                    "acc_norm_stderr": 0.025403832978179625,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2993827160493827,
                    "acc_stderr": 0.025483115601195462,
                    "acc_norm": 0.2993827160493827,
                    "acc_norm_stderr": 0.025483115601195462,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.3120567375886525,
                    "acc_stderr": 0.02764012054516993,
                    "acc_norm": 0.3120567375886525,
                    "acc_norm_stderr": 0.02764012054516993,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2457627118644068,
                    "acc_stderr": 0.01099615663514269,
                    "acc_norm": 0.2457627118644068,
                    "acc_norm_stderr": 0.01099615663514269,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.21323529411764705,
                    "acc_stderr": 0.024880971512294275,
                    "acc_norm": 0.21323529411764705,
                    "acc_norm_stderr": 0.024880971512294275,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.017848089574913226,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.017848089574913226,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.32727272727272727,
                    "acc_stderr": 0.04494290866252088,
                    "acc_norm": 0.32727272727272727,
                    "acc_norm_stderr": 0.04494290866252088,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.3346938775510204,
                    "acc_stderr": 0.030209235226242307,
                    "acc_norm": 0.3346938775510204,
                    "acc_norm_stderr": 0.030209235226242307,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.27860696517412936,
                    "acc_stderr": 0.031700561834973086,
                    "acc_norm": 0.27860696517412936,
                    "acc_norm_stderr": 0.031700561834973086,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.36,
                    "acc_stderr": 0.048241815132442176,
                    "acc_norm": 0.36,
                    "acc_norm_stderr": 0.048241815132442176,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3373493975903614,
                    "acc_stderr": 0.03680783690727581,
                    "acc_norm": 0.3373493975903614,
                    "acc_norm_stderr": 0.03680783690727581,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.2982456140350877,
                    "acc_stderr": 0.03508771929824563,
                    "acc_norm": 0.2982456140350877,
                    "acc_norm_stderr": 0.03508771929824563,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2178702570379437,
                    "mc1_stderr": 0.014450846714123897,
                    "mc2": 0.3477745324693538,
                    "mc2_stderr": 0.013261749316970146,
                    "timestamp": "2023-07-24T10-28-09.665576"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.001153523489932886,
                    "acc_stderr": 0.0003476179896857095,
                    "f1": 0.05134962248322172,
                    "f1_stderr": 0.0012730168443049574,
                    "timestamp": "2023-10-15T11-22-56.677003"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.009097801364670205,
                    "acc_stderr": 0.002615326510775673,
                    "timestamp": "2023-10-15T11-22-56.677003"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.67008681925809,
                    "acc_stderr": 0.013214432542517527,
                    "timestamp": "2023-10-15T11-22-56.677003"
                }
            }
        }
    }
}