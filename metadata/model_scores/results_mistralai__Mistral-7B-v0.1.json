{
    "model_name": "mistralai/Mistral-7B-v0.1",
    "last_updated": "2024-06-25 14:40:25.718332",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "acc": 0.001572986577181208,
                    "acc_stderr": 0.00040584511324177333,
                    "f1": 0.06143666107382555,
                    "f1_stderr": 0.0013713061256604275,
                    "timestamp": "2023-10-26T01-29-53.089924"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.378316906747536,
                    "acc_stderr": 0.013358407831777112,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7861089187056038,
                    "acc_stderr": 0.011524466954090254,
                    "timestamp": "2023-10-26T01-29-53.089924"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.568259385665529,
                    "acc_stderr": 0.014474591427196202,
                    "acc_norm": 0.5998293515358362,
                    "acc_norm_stderr": 0.014317197787809172,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6294562836088429,
                    "acc_stderr": 0.00481963366883254,
                    "acc_norm": 0.8331009759012149,
                    "acc_norm_stderr": 0.0037212361965025162,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6296296296296297,
                    "acc_stderr": 0.041716541613545426,
                    "acc_norm": 0.6296296296296297,
                    "acc_norm_stderr": 0.041716541613545426,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.6578947368421053,
                    "acc_stderr": 0.03860731599316091,
                    "acc_norm": 0.6578947368421053,
                    "acc_norm_stderr": 0.03860731599316091,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.57,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.57,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.6943396226415094,
                    "acc_stderr": 0.028353298073322663,
                    "acc_norm": 0.6943396226415094,
                    "acc_norm_stderr": 0.028353298073322663,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.7291666666666666,
                    "acc_stderr": 0.03716177437566017,
                    "acc_norm": 0.7291666666666666,
                    "acc_norm_stderr": 0.03716177437566017,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.04923659639173309,
                    "acc_norm": 0.4,
                    "acc_norm_stderr": 0.04923659639173309,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.6647398843930635,
                    "acc_stderr": 0.03599586301247077,
                    "acc_norm": 0.6647398843930635,
                    "acc_norm_stderr": 0.03599586301247077,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.39215686274509803,
                    "acc_stderr": 0.04858083574266346,
                    "acc_norm": 0.39215686274509803,
                    "acc_norm_stderr": 0.04858083574266346,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.77,
                    "acc_stderr": 0.042295258468165065,
                    "acc_norm": 0.77,
                    "acc_norm_stderr": 0.042295258468165065,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.574468085106383,
                    "acc_stderr": 0.03232146916224468,
                    "acc_norm": 0.574468085106383,
                    "acc_norm_stderr": 0.03232146916224468,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.047036043419179864,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.047036043419179864,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5724137931034483,
                    "acc_stderr": 0.04122737111370332,
                    "acc_norm": 0.5724137931034483,
                    "acc_norm_stderr": 0.04122737111370332,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.373015873015873,
                    "acc_stderr": 0.02490699045899257,
                    "acc_norm": 0.373015873015873,
                    "acc_norm_stderr": 0.02490699045899257,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.4126984126984127,
                    "acc_stderr": 0.04403438954768177,
                    "acc_norm": 0.4126984126984127,
                    "acc_norm_stderr": 0.04403438954768177,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7709677419354839,
                    "acc_stderr": 0.023904914311782648,
                    "acc_norm": 0.7709677419354839,
                    "acc_norm_stderr": 0.023904914311782648,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.5270935960591133,
                    "acc_stderr": 0.03512819077876106,
                    "acc_norm": 0.5270935960591133,
                    "acc_norm_stderr": 0.03512819077876106,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.68,
                    "acc_stderr": 0.04688261722621504,
                    "acc_norm": 0.68,
                    "acc_norm_stderr": 0.04688261722621504,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.7818181818181819,
                    "acc_stderr": 0.032250781083062896,
                    "acc_norm": 0.7818181818181819,
                    "acc_norm_stderr": 0.032250781083062896,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.7727272727272727,
                    "acc_stderr": 0.029857515673386417,
                    "acc_norm": 0.7727272727272727,
                    "acc_norm_stderr": 0.029857515673386417,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.8652849740932642,
                    "acc_stderr": 0.02463978909770944,
                    "acc_norm": 0.8652849740932642,
                    "acc_norm_stderr": 0.02463978909770944,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.6666666666666666,
                    "acc_stderr": 0.023901157979402534,
                    "acc_norm": 0.6666666666666666,
                    "acc_norm_stderr": 0.023901157979402534,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.337037037037037,
                    "acc_stderr": 0.028820884666253255,
                    "acc_norm": 0.337037037037037,
                    "acc_norm_stderr": 0.028820884666253255,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6596638655462185,
                    "acc_stderr": 0.030778057422931673,
                    "acc_norm": 0.6596638655462185,
                    "acc_norm_stderr": 0.030778057422931673,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.32450331125827814,
                    "acc_stderr": 0.038227469376587525,
                    "acc_norm": 0.32450331125827814,
                    "acc_norm_stderr": 0.038227469376587525,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8238532110091743,
                    "acc_stderr": 0.016332882393431385,
                    "acc_norm": 0.8238532110091743,
                    "acc_norm_stderr": 0.016332882393431385,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.5740740740740741,
                    "acc_stderr": 0.03372343271653062,
                    "acc_norm": 0.5740740740740741,
                    "acc_norm_stderr": 0.03372343271653062,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.7990196078431373,
                    "acc_stderr": 0.028125972265654373,
                    "acc_norm": 0.7990196078431373,
                    "acc_norm_stderr": 0.028125972265654373,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.7721518987341772,
                    "acc_stderr": 0.027303484599069436,
                    "acc_norm": 0.7721518987341772,
                    "acc_norm_stderr": 0.027303484599069436,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.7040358744394619,
                    "acc_stderr": 0.030636591348699803,
                    "acc_norm": 0.7040358744394619,
                    "acc_norm_stderr": 0.030636591348699803,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.7938931297709924,
                    "acc_stderr": 0.03547771004159463,
                    "acc_norm": 0.7938931297709924,
                    "acc_norm_stderr": 0.03547771004159463,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.7768595041322314,
                    "acc_stderr": 0.03800754475228732,
                    "acc_norm": 0.7768595041322314,
                    "acc_norm_stderr": 0.03800754475228732,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.7777777777777778,
                    "acc_stderr": 0.040191074725573483,
                    "acc_norm": 0.7777777777777778,
                    "acc_norm_stderr": 0.040191074725573483,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.7914110429447853,
                    "acc_stderr": 0.031921934489347235,
                    "acc_norm": 0.7914110429447853,
                    "acc_norm_stderr": 0.031921934489347235,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.48214285714285715,
                    "acc_stderr": 0.047427623612430116,
                    "acc_norm": 0.48214285714285715,
                    "acc_norm_stderr": 0.047427623612430116,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.8155339805825242,
                    "acc_stderr": 0.03840423627288276,
                    "acc_norm": 0.8155339805825242,
                    "acc_norm_stderr": 0.03840423627288276,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8717948717948718,
                    "acc_stderr": 0.02190190511507333,
                    "acc_norm": 0.8717948717948718,
                    "acc_norm_stderr": 0.02190190511507333,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.74,
                    "acc_stderr": 0.04408440022768078,
                    "acc_norm": 0.74,
                    "acc_norm_stderr": 0.04408440022768078,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.8173690932311622,
                    "acc_stderr": 0.013816335389973136,
                    "acc_norm": 0.8173690932311622,
                    "acc_norm_stderr": 0.013816335389973136,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7109826589595376,
                    "acc_stderr": 0.02440517393578323,
                    "acc_norm": 0.7109826589595376,
                    "acc_norm_stderr": 0.02440517393578323,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.32513966480446926,
                    "acc_stderr": 0.01566654278505355,
                    "acc_norm": 0.32513966480446926,
                    "acc_norm_stderr": 0.01566654278505355,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7581699346405228,
                    "acc_stderr": 0.024518195641879334,
                    "acc_norm": 0.7581699346405228,
                    "acc_norm_stderr": 0.024518195641879334,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.6977491961414791,
                    "acc_stderr": 0.026082700695399665,
                    "acc_norm": 0.6977491961414791,
                    "acc_norm_stderr": 0.026082700695399665,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.7345679012345679,
                    "acc_stderr": 0.024569223600460845,
                    "acc_norm": 0.7345679012345679,
                    "acc_norm_stderr": 0.024569223600460845,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.4858156028368794,
                    "acc_stderr": 0.02981549448368206,
                    "acc_norm": 0.4858156028368794,
                    "acc_norm_stderr": 0.02981549448368206,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.44784876140808344,
                    "acc_stderr": 0.01270058240476822,
                    "acc_norm": 0.44784876140808344,
                    "acc_norm_stderr": 0.01270058240476822,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.6911764705882353,
                    "acc_stderr": 0.02806499816704009,
                    "acc_norm": 0.6911764705882353,
                    "acc_norm_stderr": 0.02806499816704009,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.6813725490196079,
                    "acc_stderr": 0.01885008469646872,
                    "acc_norm": 0.6813725490196079,
                    "acc_norm_stderr": 0.01885008469646872,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6727272727272727,
                    "acc_stderr": 0.0449429086625209,
                    "acc_norm": 0.6727272727272727,
                    "acc_norm_stderr": 0.0449429086625209,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.726530612244898,
                    "acc_stderr": 0.028535560337128448,
                    "acc_norm": 0.726530612244898,
                    "acc_norm_stderr": 0.028535560337128448,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8308457711442786,
                    "acc_stderr": 0.026508590656233264,
                    "acc_norm": 0.8308457711442786,
                    "acc_norm_stderr": 0.026508590656233264,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.034873508801977704,
                    "acc_norm": 0.86,
                    "acc_norm_stderr": 0.034873508801977704,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5542168674698795,
                    "acc_stderr": 0.03869543323472101,
                    "acc_norm": 0.5542168674698795,
                    "acc_norm_stderr": 0.03869543323472101,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8304093567251462,
                    "acc_stderr": 0.02878210810540171,
                    "acc_norm": 0.8304093567251462,
                    "acc_norm_stderr": 0.02878210810540171,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2802937576499388,
                    "mc1_stderr": 0.015723139524608763,
                    "mc2": 0.4215317106968115,
                    "mc2_stderr": 0.014138129483133954,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.04212454212454213,
                    "acc_stderr": 0.008604464925374512,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.23765786452353616,
                    "acc_stderr": 0.014430834004369359,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.08888888888888889,
                    "acc_stderr": 0.012257870465567287,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.05426356589147287,
                    "acc_stderr": 0.007542858423834505,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.11064718162839249,
                    "acc_stderr": 0.014348062924241815,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.12025316455696203,
                    "acc_stderr": 0.014955348691492934,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.18281381634372368,
                    "acc_stderr": 0.011223354238019886,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.982,
                    "acc_stderr": 0.0029736208922129417,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.9885,
                    "acc_stderr": 0.0023846841214675823,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.954,
                    "acc_stderr": 0.004685400355171836,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.998,
                    "acc_stderr": 0.00099924934306949,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.8715,
                    "acc_stderr": 0.007484776946774901,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.909,
                    "acc_stderr": 0.0064327435900281065,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.6455,
                    "acc_stderr": 0.010699164035359294,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.945,
                    "acc_stderr": 0.005099068566917326,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.7115,
                    "acc_stderr": 0.010133371482818627,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 0.9985,
                    "acc_stderr": 0.0008655920660521451,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.41925701288855194,
                    "acc_stderr": 0.013591720959042111,
                    "timestamp": "2024-06-14T11-32-48.523139"
                }
            },
            "anli_r2": {
                "0-shot": {
                    "brier_score": 0.7950624544675337,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "anli_r3": {
                "0-shot": {
                    "brier_score": 0.7365319523049788,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "anli_r1": {
                "0-shot": {
                    "brier_score": 0.824029011811061,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_eu": {
                "0-shot": {
                    "brier_score": 0.9652658697204612,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_vi": {
                "0-shot": {
                    "brier_score": 0.8766059818111679,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_ru": {
                "0-shot": {
                    "brier_score": 0.7413504110189894,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_zh": {
                "0-shot": {
                    "brier_score": 0.9609280684971038,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_tr": {
                "0-shot": {
                    "brier_score": 0.8729771059898165,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_fr": {
                "0-shot": {
                    "brier_score": 0.7650067503932926,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_en": {
                "0-shot": {
                    "brier_score": 0.6632403264489223,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_ur": {
                "0-shot": {
                    "brier_score": 1.2217226967863828,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_ar": {
                "0-shot": {
                    "brier_score": 1.2723443593223103,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_de": {
                "0-shot": {
                    "brier_score": 0.8115761663377433,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_hi": {
                "0-shot": {
                    "brier_score": 0.8465004556210892,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_es": {
                "0-shot": {
                    "brier_score": 0.8695701092578179,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_bg": {
                "0-shot": {
                    "brier_score": 0.84232405643235,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_sw": {
                "0-shot": {
                    "brier_score": 0.8955590100538611,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_el": {
                "0-shot": {
                    "brier_score": 0.7705875419124961,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "xnli_th": {
                "0-shot": {
                    "brier_score": 0.9058259849840192,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "brier_score": 0.8994132704589166,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "mathqa": {
                "5-shot": {
                    "brier_score": 0.8214663265573888,
                    "timestamp": "2024-06-14T11-43-05.046285"
                }
            },
            "lambada_standard": {
                "0-shot": {
                    "perplexity": 3.776806210204293,
                    "perplexity_stderr": 0.0726222723228476,
                    "acc": 0.694352804191733,
                    "acc_stderr": 0.006418187162765859,
                    "timestamp": "2024-06-14T11-44-28.222087"
                }
            },
            "lambada_openai": {
                "0-shot": {
                    "perplexity": 3.1806434293677626,
                    "perplexity_stderr": 0.058187731736352985,
                    "acc": 0.7568406753347564,
                    "acc_stderr": 0.0059766767751295215,
                    "timestamp": "2024-06-14T11-44-28.222087"
                }
            }
        }
    }
}