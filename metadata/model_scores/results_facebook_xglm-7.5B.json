{
    "model_name": "facebook/xglm-7.5B",
    "last_updated": "2023-07-19",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "em": 0.13905201342281878,
                    "em_stderr": 0.0035433720039612262,
                    "f1": 0.18580851510067117,
                    "f1_stderr": 0.0037071149655913006,
                    "timestamp": "2023-10-18T03-17-20.065422"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.002274450341167551,
                    "acc_stderr": 0.001312157814867432,
                    "timestamp": "2023-10-18T03-17-20.065422"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5872138910812944,
                    "acc_stderr": 0.013837060648682094,
                    "timestamp": "2023-10-18T03-17-20.065422"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.30716723549488056,
                    "acc_stderr": 0.013481034054980945,
                    "acc_norm": 0.3412969283276451,
                    "acc_norm_stderr": 0.013855831287497723,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.44961163114917346,
                    "acc_stderr": 0.004964378762425247,
                    "acc_norm": 0.6077474606652061,
                    "acc_norm_stderr": 0.004872546302641851,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847415,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847415,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.03749850709174022,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.03749850709174022,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.2236842105263158,
                    "acc_stderr": 0.03391160934343602,
                    "acc_norm": 0.2236842105263158,
                    "acc_norm_stderr": 0.03391160934343602,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768078,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768078,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.3018867924528302,
                    "acc_stderr": 0.028254200344438662,
                    "acc_norm": 0.3018867924528302,
                    "acc_norm_stderr": 0.028254200344438662,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2986111111111111,
                    "acc_stderr": 0.03827052357950756,
                    "acc_norm": 0.2986111111111111,
                    "acc_norm_stderr": 0.03827052357950756,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.04461960433384741,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.04461960433384741,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542128,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542128,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.25,
                    "acc_stderr": 0.04351941398892446,
                    "acc_norm": 0.25,
                    "acc_norm_stderr": 0.04351941398892446,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2543352601156069,
                    "acc_stderr": 0.0332055644308557,
                    "acc_norm": 0.2543352601156069,
                    "acc_norm_stderr": 0.0332055644308557,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.19607843137254902,
                    "acc_stderr": 0.03950581861179961,
                    "acc_norm": 0.19607843137254902,
                    "acc_norm_stderr": 0.03950581861179961,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542128,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542128,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.28936170212765955,
                    "acc_stderr": 0.029644006577009618,
                    "acc_norm": 0.28936170212765955,
                    "acc_norm_stderr": 0.029644006577009618,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2543859649122807,
                    "acc_stderr": 0.040969851398436695,
                    "acc_norm": 0.2543859649122807,
                    "acc_norm_stderr": 0.040969851398436695,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.20689655172413793,
                    "acc_stderr": 0.03375672449560553,
                    "acc_norm": 0.20689655172413793,
                    "acc_norm_stderr": 0.03375672449560553,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2671957671957672,
                    "acc_stderr": 0.02278967314577656,
                    "acc_norm": 0.2671957671957672,
                    "acc_norm_stderr": 0.02278967314577656,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.31746031746031744,
                    "acc_stderr": 0.04163453031302859,
                    "acc_norm": 0.31746031746031744,
                    "acc_norm_stderr": 0.04163453031302859,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.04560480215720684,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.04560480215720684,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.33225806451612905,
                    "acc_stderr": 0.026795560848122797,
                    "acc_norm": 0.33225806451612905,
                    "acc_norm_stderr": 0.026795560848122797,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.3103448275862069,
                    "acc_stderr": 0.032550867699701024,
                    "acc_norm": 0.3103448275862069,
                    "acc_norm_stderr": 0.032550867699701024,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.045126085985421296,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.045126085985421296,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.2787878787878788,
                    "acc_stderr": 0.0350143870629678,
                    "acc_norm": 0.2787878787878788,
                    "acc_norm_stderr": 0.0350143870629678,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.35858585858585856,
                    "acc_stderr": 0.03416903640391521,
                    "acc_norm": 0.35858585858585856,
                    "acc_norm_stderr": 0.03416903640391521,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.3626943005181347,
                    "acc_stderr": 0.03469713791704371,
                    "acc_norm": 0.3626943005181347,
                    "acc_norm_stderr": 0.03469713791704371,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.3564102564102564,
                    "acc_stderr": 0.024283140529467295,
                    "acc_norm": 0.3564102564102564,
                    "acc_norm_stderr": 0.024283140529467295,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.24814814814814815,
                    "acc_stderr": 0.0263357394040558,
                    "acc_norm": 0.24814814814814815,
                    "acc_norm_stderr": 0.0263357394040558,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.3319327731092437,
                    "acc_stderr": 0.030588697013783663,
                    "acc_norm": 0.3319327731092437,
                    "acc_norm_stderr": 0.030588697013783663,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2980132450331126,
                    "acc_stderr": 0.037345356767871984,
                    "acc_norm": 0.2980132450331126,
                    "acc_norm_stderr": 0.037345356767871984,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.27706422018348625,
                    "acc_stderr": 0.019188482590169538,
                    "acc_norm": 0.27706422018348625,
                    "acc_norm_stderr": 0.019188482590169538,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4212962962962963,
                    "acc_stderr": 0.03367462138896078,
                    "acc_norm": 0.4212962962962963,
                    "acc_norm_stderr": 0.03367462138896078,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.02977177522814563,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.02977177522814563,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.25738396624472576,
                    "acc_stderr": 0.028458820991460295,
                    "acc_norm": 0.25738396624472576,
                    "acc_norm_stderr": 0.028458820991460295,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.3452914798206278,
                    "acc_stderr": 0.03191100192835794,
                    "acc_norm": 0.3452914798206278,
                    "acc_norm_stderr": 0.03191100192835794,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.0372767357559692,
                    "acc_norm": 0.2366412213740458,
                    "acc_norm_stderr": 0.0372767357559692,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.36363636363636365,
                    "acc_stderr": 0.043913262867240704,
                    "acc_norm": 0.36363636363636365,
                    "acc_norm_stderr": 0.043913262867240704,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.21296296296296297,
                    "acc_stderr": 0.0395783547198098,
                    "acc_norm": 0.21296296296296297,
                    "acc_norm_stderr": 0.0395783547198098,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.25766871165644173,
                    "acc_stderr": 0.03436150827846917,
                    "acc_norm": 0.25766871165644173,
                    "acc_norm_stderr": 0.03436150827846917,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.22321428571428573,
                    "acc_stderr": 0.039523019677025116,
                    "acc_norm": 0.22321428571428573,
                    "acc_norm_stderr": 0.039523019677025116,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2621359223300971,
                    "acc_stderr": 0.043546310772605956,
                    "acc_norm": 0.2621359223300971,
                    "acc_norm_stderr": 0.043546310772605956,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.1794871794871795,
                    "acc_stderr": 0.025140935950335428,
                    "acc_norm": 0.1794871794871795,
                    "acc_norm_stderr": 0.025140935950335428,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.2554278416347382,
                    "acc_stderr": 0.015594955384455777,
                    "acc_norm": 0.2554278416347382,
                    "acc_norm_stderr": 0.015594955384455777,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.23121387283236994,
                    "acc_stderr": 0.022698657167855716,
                    "acc_norm": 0.23121387283236994,
                    "acc_norm_stderr": 0.022698657167855716,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2424581005586592,
                    "acc_stderr": 0.014333522059217889,
                    "acc_norm": 0.2424581005586592,
                    "acc_norm_stderr": 0.014333522059217889,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.025646863097137897,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.025646863097137897,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.24437299035369775,
                    "acc_stderr": 0.024406162094668872,
                    "acc_norm": 0.24437299035369775,
                    "acc_norm_stderr": 0.024406162094668872,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.27469135802469136,
                    "acc_stderr": 0.02483605786829468,
                    "acc_norm": 0.27469135802469136,
                    "acc_norm_stderr": 0.02483605786829468,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2624113475177305,
                    "acc_stderr": 0.026244920349843014,
                    "acc_norm": 0.2624113475177305,
                    "acc_norm_stderr": 0.026244920349843014,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.24445893089960888,
                    "acc_stderr": 0.010976425013113906,
                    "acc_norm": 0.24445893089960888,
                    "acc_norm_stderr": 0.010976425013113906,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4522058823529412,
                    "acc_stderr": 0.030233758551596452,
                    "acc_norm": 0.4522058823529412,
                    "acc_norm_stderr": 0.030233758551596452,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2696078431372549,
                    "acc_stderr": 0.017952449196987862,
                    "acc_norm": 0.2696078431372549,
                    "acc_norm_stderr": 0.017952449196987862,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.24545454545454545,
                    "acc_stderr": 0.041220665028782834,
                    "acc_norm": 0.24545454545454545,
                    "acc_norm_stderr": 0.041220665028782834,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.2897959183673469,
                    "acc_stderr": 0.02904308868330435,
                    "acc_norm": 0.2897959183673469,
                    "acc_norm_stderr": 0.02904308868330435,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.23880597014925373,
                    "acc_stderr": 0.030147775935409224,
                    "acc_norm": 0.23880597014925373,
                    "acc_norm_stderr": 0.030147775935409224,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.042923469599092816,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.2891566265060241,
                    "acc_stderr": 0.03529486801511115,
                    "acc_norm": 0.2891566265060241,
                    "acc_norm_stderr": 0.03529486801511115,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.2807017543859649,
                    "acc_stderr": 0.034462962170884265,
                    "acc_norm": 0.2807017543859649,
                    "acc_norm_stderr": 0.034462962170884265,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.20930232558139536,
                    "mc1_stderr": 0.014241219434785828,
                    "mc2": 0.36661523882354585,
                    "mc2_stderr": 0.013666875267077829,
                    "timestamp": "2023-07-19T15-31-59.100861"
                }
            }
        }
    }
}