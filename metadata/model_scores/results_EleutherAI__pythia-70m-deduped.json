{
    "model_name": "EleutherAI/pythia-70m-deduped",
    "last_updated": "2024-12-19 13:41:39.605279",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "em": 0.0012583892617449664,
                    "em_stderr": 0.0003630560893119184,
                    "f1": 0.023000209731543642,
                    "f1_stderr": 0.0009427318515971101,
                    "timestamp": "2023-10-19T00-18-19.073831"
                }
            },
            "gsm8k": {
                "exact_match": 0.009097801364670205,
                "exact_match_stderr": 0.002615326510775671,
                "timestamp": "2024-11-20T02-41-14.007364"
            },
            "winogrande": {
                "acc": 0.4925019731649566,
                "acc_stderr": 0.014050905521228573,
                "timestamp": "2024-11-20T02-41-14.007364"
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.18344709897610922,
                    "acc_stderr": 0.011310170179554536,
                    "acc_norm": 0.21075085324232082,
                    "acc_norm_stderr": 0.011918271754852197,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hellaswag": {
                "acc": 0.26857199761003786,
                "acc_stderr": 0.004423109313298973,
                "acc_norm": 0.27504481179047996,
                "acc_norm_stderr": 0.004456242601950572,
                "timestamp": "2024-11-20T02-41-14.007364"
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.042923469599092816,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.35555555555555557,
                    "acc_stderr": 0.04135176749720385,
                    "acc_norm": 0.35555555555555557,
                    "acc_norm_stderr": 0.04135176749720385,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.18421052631578946,
                    "acc_stderr": 0.0315469804508223,
                    "acc_norm": 0.18421052631578946,
                    "acc_norm_stderr": 0.0315469804508223,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816505,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816505,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2339622641509434,
                    "acc_stderr": 0.026055296901152922,
                    "acc_norm": 0.2339622641509434,
                    "acc_norm_stderr": 0.026055296901152922,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.22916666666666666,
                    "acc_stderr": 0.03514697467862388,
                    "acc_norm": 0.22916666666666666,
                    "acc_norm_stderr": 0.03514697467862388,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909282,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909282,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.045126085985421276,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.045126085985421276,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2023121387283237,
                    "acc_stderr": 0.03063114553919882,
                    "acc_norm": 0.2023121387283237,
                    "acc_norm_stderr": 0.03063114553919882,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.041583075330832865,
                    "acc_norm": 0.22549019607843138,
                    "acc_norm_stderr": 0.041583075330832865,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.28936170212765955,
                    "acc_stderr": 0.02964400657700962,
                    "acc_norm": 0.28936170212765955,
                    "acc_norm_stderr": 0.02964400657700962,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2543859649122807,
                    "acc_stderr": 0.0409698513984367,
                    "acc_norm": 0.2543859649122807,
                    "acc_norm_stderr": 0.0409698513984367,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.20689655172413793,
                    "acc_stderr": 0.03375672449560554,
                    "acc_norm": 0.20689655172413793,
                    "acc_norm_stderr": 0.03375672449560554,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.25396825396825395,
                    "acc_stderr": 0.022418042891113942,
                    "acc_norm": 0.25396825396825395,
                    "acc_norm_stderr": 0.022418042891113942,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.20634920634920634,
                    "acc_stderr": 0.036196045241242515,
                    "acc_norm": 0.20634920634920634,
                    "acc_norm_stderr": 0.036196045241242515,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.17,
                    "acc_stderr": 0.0377525168068637,
                    "acc_norm": 0.17,
                    "acc_norm_stderr": 0.0377525168068637,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.3258064516129032,
                    "acc_stderr": 0.0266620105785671,
                    "acc_norm": 0.3258064516129032,
                    "acc_norm_stderr": 0.0266620105785671,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.28078817733990147,
                    "acc_stderr": 0.0316185633535861,
                    "acc_norm": 0.28078817733990147,
                    "acc_norm_stderr": 0.0316185633535861,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.24242424242424243,
                    "acc_stderr": 0.03346409881055953,
                    "acc_norm": 0.24242424242424243,
                    "acc_norm_stderr": 0.03346409881055953,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.02962022787479048,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.02962022787479048,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.35233160621761656,
                    "acc_stderr": 0.03447478286414359,
                    "acc_norm": 0.35233160621761656,
                    "acc_norm_stderr": 0.03447478286414359,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2846153846153846,
                    "acc_stderr": 0.022878322799706283,
                    "acc_norm": 0.2846153846153846,
                    "acc_norm_stderr": 0.022878322799706283,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.026466117538959916,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.026466117538959916,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.23109243697478993,
                    "acc_stderr": 0.027381406927868963,
                    "acc_norm": 0.23109243697478993,
                    "acc_norm_stderr": 0.027381406927868963,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.2052980132450331,
                    "acc_stderr": 0.03297986648473835,
                    "acc_norm": 0.2052980132450331,
                    "acc_norm_stderr": 0.03297986648473835,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.24036697247706423,
                    "acc_stderr": 0.01832060732096407,
                    "acc_norm": 0.24036697247706423,
                    "acc_norm_stderr": 0.01832060732096407,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4583333333333333,
                    "acc_stderr": 0.033981108902946366,
                    "acc_norm": 0.4583333333333333,
                    "acc_norm_stderr": 0.033981108902946366,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.25980392156862747,
                    "acc_stderr": 0.03077855467869326,
                    "acc_norm": 0.25980392156862747,
                    "acc_norm_stderr": 0.03077855467869326,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.28270042194092826,
                    "acc_stderr": 0.029312814153955934,
                    "acc_norm": 0.28270042194092826,
                    "acc_norm_stderr": 0.029312814153955934,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.2556053811659193,
                    "acc_stderr": 0.029275891003969927,
                    "acc_norm": 0.2556053811659193,
                    "acc_norm_stderr": 0.029275891003969927,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2900763358778626,
                    "acc_stderr": 0.03980066246467765,
                    "acc_norm": 0.2900763358778626,
                    "acc_norm_stderr": 0.03980066246467765,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.19834710743801653,
                    "acc_stderr": 0.036401182719909456,
                    "acc_norm": 0.19834710743801653,
                    "acc_norm_stderr": 0.036401182719909456,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2037037037037037,
                    "acc_stderr": 0.038935425188248475,
                    "acc_norm": 0.2037037037037037,
                    "acc_norm_stderr": 0.038935425188248475,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.25153374233128833,
                    "acc_stderr": 0.03408997886857529,
                    "acc_norm": 0.25153374233128833,
                    "acc_norm_stderr": 0.03408997886857529,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.17857142857142858,
                    "acc_stderr": 0.03635209121577806,
                    "acc_norm": 0.17857142857142858,
                    "acc_norm_stderr": 0.03635209121577806,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.18446601941747573,
                    "acc_stderr": 0.03840423627288276,
                    "acc_norm": 0.18446601941747573,
                    "acc_norm_stderr": 0.03840423627288276,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.20085470085470086,
                    "acc_stderr": 0.02624677294689048,
                    "acc_norm": 0.20085470085470086,
                    "acc_norm_stderr": 0.02624677294689048,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695235,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.04760952285695235,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.23754789272030652,
                    "acc_stderr": 0.01521873304615019,
                    "acc_norm": 0.23754789272030652,
                    "acc_norm_stderr": 0.01521873304615019,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.2658959537572254,
                    "acc_stderr": 0.023786203255508283,
                    "acc_norm": 0.2658959537572254,
                    "acc_norm_stderr": 0.023786203255508283,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2670391061452514,
                    "acc_stderr": 0.014796502622562555,
                    "acc_norm": 0.2670391061452514,
                    "acc_norm_stderr": 0.014796502622562555,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.025261691219729484,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.025261691219729484,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.19614147909967847,
                    "acc_stderr": 0.022552447780478043,
                    "acc_norm": 0.19614147909967847,
                    "acc_norm_stderr": 0.022552447780478043,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.2345679012345679,
                    "acc_stderr": 0.023576881744005716,
                    "acc_norm": 0.2345679012345679,
                    "acc_norm_stderr": 0.023576881744005716,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.25886524822695034,
                    "acc_stderr": 0.026129572527180848,
                    "acc_norm": 0.25886524822695034,
                    "acc_norm_stderr": 0.026129572527180848,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2392438070404172,
                    "acc_stderr": 0.010896123652676655,
                    "acc_norm": 0.2392438070404172,
                    "acc_norm_stderr": 0.010896123652676655,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.41544117647058826,
                    "acc_stderr": 0.02993534270787775,
                    "acc_norm": 0.41544117647058826,
                    "acc_norm_stderr": 0.02993534270787775,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.24836601307189543,
                    "acc_stderr": 0.017479487001364764,
                    "acc_norm": 0.24836601307189543,
                    "acc_norm_stderr": 0.017479487001364764,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.21818181818181817,
                    "acc_stderr": 0.03955932861795833,
                    "acc_norm": 0.21818181818181817,
                    "acc_norm_stderr": 0.03955932861795833,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.22857142857142856,
                    "acc_stderr": 0.026882144922307748,
                    "acc_norm": 0.22857142857142856,
                    "acc_norm_stderr": 0.026882144922307748,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.24875621890547264,
                    "acc_stderr": 0.030567675938916707,
                    "acc_norm": 0.24875621890547264,
                    "acc_norm_stderr": 0.030567675938916707,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.046882617226215034,
                    "acc_norm": 0.32,
                    "acc_norm_stderr": 0.046882617226215034,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.1927710843373494,
                    "acc_stderr": 0.030709824050565274,
                    "acc_norm": 0.1927710843373494,
                    "acc_norm_stderr": 0.030709824050565274,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.30994152046783624,
                    "acc_stderr": 0.03546976959393163,
                    "acc_norm": 0.30994152046783624,
                    "acc_norm_stderr": 0.03546976959393163,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2521419828641371,
                    "mc1_stderr": 0.015201522246299962,
                    "mc2": 0.47514385475605914,
                    "mc2_stderr": 0.01572213315211734,
                    "timestamp": "2023-07-19T13-42-51.890470"
                }
            },
            "mmlu_world_religions": {
                "acc": 0.3157894736842105,
                "acc_stderr": 0.03565079670708313,
                "brier_score": 0.9458017124200467,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_formal_logic": {
                "acc": 0.2698412698412698,
                "acc_stderr": 0.03970158273235172,
                "brier_score": 0.9944529998737204,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_prehistory": {
                "acc": 0.21296296296296297,
                "acc_stderr": 0.022779719088733396,
                "brier_score": 1.1104097102004094,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574885,
                "brier_score": 1.0107834268490483,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.270042194092827,
                "acc_stderr": 0.028900721906293426,
                "brier_score": 1.0541749946967076,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_moral_disputes": {
                "acc": 0.24855491329479767,
                "acc_stderr": 0.023267528432100174,
                "brier_score": 1.1503169332598566,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_professional_law": {
                "acc": 0.2470664928292047,
                "acc_stderr": 0.011015752255279329,
                "brier_score": 1.0777841379995983,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.22085889570552147,
                "acc_stderr": 0.032591773927421776,
                "brier_score": 1.1873647876473623,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25,
                "acc_stderr": 0.03039153369274154,
                "brier_score": 1.0797949488812022,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_philosophy": {
                "acc": 0.1864951768488746,
                "acc_stderr": 0.02212243977248077,
                "brier_score": 1.285858430936262,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_jurisprudence": {
                "acc": 0.25925925925925924,
                "acc_stderr": 0.04236511258094634,
                "brier_score": 1.113612046428483,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_international_law": {
                "acc": 0.2396694214876033,
                "acc_stderr": 0.03896878985070417,
                "brier_score": 1.1609394278395657,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03225078108306289,
                "brier_score": 1.0797849216735733,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.19689119170984457,
                "acc_stderr": 0.02869787397186069,
                "brier_score": 1.1874656538908335,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.21008403361344538,
                "acc_stderr": 0.026461398717471874,
                "brier_score": 1.1303851879741371,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_geography": {
                "acc": 0.17676767676767677,
                "acc_stderr": 0.027178752639044915,
                "brier_score": 1.145581139934715,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.1944954128440367,
                "acc_stderr": 0.01697028909045803,
                "brier_score": 1.172174012955203,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "brier_score": 1.1841312871839917,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 1.0080093333768496,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_sociology": {
                "acc": 0.24378109452736318,
                "acc_stderr": 0.030360490154014652,
                "brier_score": 1.1267198627231036,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.20256410256410257,
                "acc_stderr": 0.020377660970371397,
                "brier_score": 1.1203296493874824,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_security_studies": {
                "acc": 0.18775510204081633,
                "acc_stderr": 0.02500025603954622,
                "brier_score": 1.2394159309129527,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25,
                "acc_stderr": 0.01751781884501444,
                "brier_score": 1.0924564010576683,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.03844876139785271,
                "brier_score": 1.007720693073942,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.039994238792813386,
                "brier_score": 1.0545954545627128,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_miscellaneous": {
                "acc": 0.23754789272030652,
                "acc_stderr": 0.015218733046150195,
                "brier_score": 1.0303813244930453,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_marketing": {
                "acc": 0.2905982905982906,
                "acc_stderr": 0.029745048572674057,
                "brier_score": 1.0758487519425721,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "brier_score": 1.1930902498012064,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_nutrition": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.023805186524888142,
                "brier_score": 1.0089845272533258,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.9776851574403502,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_human_aging": {
                "acc": 0.31390134529147984,
                "acc_stderr": 0.03114679648297246,
                "brier_score": 0.9962092883560694,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_professional_medicine": {
                "acc": 0.1875,
                "acc_stderr": 0.023709788253811766,
                "brier_score": 1.0150739563833298,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_college_medicine": {
                "acc": 0.2138728323699422,
                "acc_stderr": 0.03126511206173043,
                "brier_score": 1.0742749717473168,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_business_ethics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.9929656337361632,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.21132075471698114,
                "acc_stderr": 0.025125766484827856,
                "brier_score": 1.0657689854635248,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.038612291966536955,
                "brier_score": 1.053537895714484,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370518,
                "brier_score": 1.0126987671317704,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_professional_accounting": {
                "acc": 0.23404255319148937,
                "acc_stderr": 0.025257861359432407,
                "brier_score": 1.0677264210796846,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.040925639582376556,
                "brier_score": 1.0123830470834905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_physics": {
                "acc": 0.19205298013245034,
                "acc_stderr": 0.032162984205936135,
                "brier_score": 1.0638867246138666,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_biology": {
                "acc": 0.1774193548387097,
                "acc_stderr": 0.021732540689329265,
                "brier_score": 1.1388412832167263,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_college_biology": {
                "acc": 0.2638888888888889,
                "acc_stderr": 0.03685651095897532,
                "brier_score": 1.0202379818652771,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_anatomy": {
                "acc": 0.1925925925925926,
                "acc_stderr": 0.03406542058502653,
                "brier_score": 1.0443440972173736,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_college_chemistry": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036623,
                "brier_score": 0.9933008024295017,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_computer_security": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.9582996285251297,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_college_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 1.018394924584262,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123398,
                "brier_score": 1.1303538216548912,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_college_mathematics": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 1.0666907621711257,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.26382978723404255,
                "acc_stderr": 0.02880998985410298,
                "brier_score": 1.0360568049709666,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 1.15021593285239,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768079,
                "brier_score": 0.9914560507266944,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_machine_learning": {
                "acc": 0.33035714285714285,
                "acc_stderr": 0.04464285714285713,
                "brier_score": 0.9066774590600205,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.16748768472906403,
                "acc_stderr": 0.02627308604753542,
                "brier_score": 1.011348400920999,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1527777777777778,
                "acc_stderr": 0.02453632602613422,
                "brier_score": 1.1822905019744807,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.20899470899470898,
                "acc_stderr": 0.020940481565334835,
                "brier_score": 1.0424135555350613,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.03565998174135302,
                "brier_score": 1.0297063015300025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2111111111111111,
                "acc_stderr": 0.02488211685765508,
                "brier_score": 0.9946655374318092,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-22-21.895675"
            },
            "arc_challenge": {
                "acc": 0.1697952218430034,
                "acc_stderr": 0.010971775157784204,
                "acc_norm": 0.21331058020477817,
                "acc_norm_stderr": 0.011970971742326334,
                "timestamp": "2024-11-20T02-41-14.007364"
            },
            "truthfulqa_mc2": {
                "acc": 0.4767651653188276,
                "acc_stderr": 0.015673835636167466,
                "timestamp": "2024-11-20T02-41-14.007364"
            },
            "truthfulqa_gen": {
                "bleu_max": 46.031535492951335,
                "bleu_max_stderr": 0.8673187818828474,
                "bleu_acc": 0.795593635250918,
                "bleu_acc_stderr": 0.014117174337432615,
                "bleu_diff": 39.793171656866846,
                "bleu_diff_stderr": 0.9094412465216882,
                "rouge1_max": 69.81726690246458,
                "rouge1_max_stderr": 1.1081734621228823,
                "rouge1_acc": 0.799265605875153,
                "rouge1_acc_stderr": 0.01402204571748216,
                "rouge1_diff": 56.95442784511964,
                "rouge1_diff_stderr": 1.3527087958065618,
                "rouge2_max": 63.72306529658852,
                "rouge2_max_stderr": 1.2425333117054245,
                "rouge2_acc": 0.7552019583843329,
                "rouge2_acc_stderr": 0.015051869486714985,
                "rouge2_diff": 60.61075595094692,
                "rouge2_diff_stderr": 1.3860618275238243,
                "rougeL_max": 69.31439415404608,
                "rougeL_max_stderr": 1.1276933183097786,
                "rougeL_acc": 0.8102815177478581,
                "rougeL_acc_stderr": 0.013725485265185087,
                "rougeL_diff": 57.784969619564976,
                "rougeL_diff_stderr": 1.3395186035921138,
                "timestamp": "2024-11-20T02-41-14.007364"
            },
            "truthfulqa_mc1": {
                "acc": 0.2582619339045288,
                "acc_stderr": 0.015321821688476183,
                "timestamp": "2024-11-20T02-41-14.007364"
            }
        }
    }
}