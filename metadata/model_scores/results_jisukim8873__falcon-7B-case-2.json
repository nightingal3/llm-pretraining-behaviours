{
    "model_name": "jisukim8873/falcon-7B-case-2",
    "last_updated": "2024-06-25 14:40:31.888387",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.4334470989761092,
                    "acc_stderr": 0.014481376224558896,
                    "acc_norm": 0.4718430034129693,
                    "acc_norm_stderr": 0.0145882041051022,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5975901214897431,
                    "acc_stderr": 0.00489381489020832,
                    "acc_norm": 0.7847042421828321,
                    "acc_norm_stderr": 0.004101873407354699,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.04461960433384739,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.04461960433384739,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.3037037037037037,
                    "acc_stderr": 0.03972552884785136,
                    "acc_norm": 0.3037037037037037,
                    "acc_norm_stderr": 0.03972552884785136,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.20394736842105263,
                    "acc_stderr": 0.0327900040631005,
                    "acc_norm": 0.20394736842105263,
                    "acc_norm_stderr": 0.0327900040631005,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.0440844002276808,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.0440844002276808,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.32452830188679244,
                    "acc_stderr": 0.028815615713432115,
                    "acc_norm": 0.32452830188679244,
                    "acc_norm_stderr": 0.028815615713432115,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2708333333333333,
                    "acc_stderr": 0.03716177437566016,
                    "acc_norm": 0.2708333333333333,
                    "acc_norm_stderr": 0.03716177437566016,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.17,
                    "acc_stderr": 0.03775251680686371,
                    "acc_norm": 0.17,
                    "acc_norm_stderr": 0.03775251680686371,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909283,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909283,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.2658959537572254,
                    "acc_stderr": 0.033687629322594316,
                    "acc_norm": 0.2658959537572254,
                    "acc_norm_stderr": 0.033687629322594316,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.2549019607843137,
                    "acc_stderr": 0.043364327079931785,
                    "acc_norm": 0.2549019607843137,
                    "acc_norm_stderr": 0.043364327079931785,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.38,
                    "acc_stderr": 0.04878317312145633,
                    "acc_norm": 0.38,
                    "acc_norm_stderr": 0.04878317312145633,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3191489361702128,
                    "acc_stderr": 0.03047297336338005,
                    "acc_norm": 0.3191489361702128,
                    "acc_norm_stderr": 0.03047297336338005,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.30701754385964913,
                    "acc_stderr": 0.04339138322579861,
                    "acc_norm": 0.30701754385964913,
                    "acc_norm_stderr": 0.04339138322579861,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.3103448275862069,
                    "acc_stderr": 0.038552896163789485,
                    "acc_norm": 0.3103448275862069,
                    "acc_norm_stderr": 0.038552896163789485,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.26455026455026454,
                    "acc_stderr": 0.022717467897708628,
                    "acc_norm": 0.26455026455026454,
                    "acc_norm_stderr": 0.022717467897708628,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.1746031746031746,
                    "acc_stderr": 0.03395490020856109,
                    "acc_norm": 0.1746031746031746,
                    "acc_norm_stderr": 0.03395490020856109,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.039427724440366234,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.039427724440366234,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.3258064516129032,
                    "acc_stderr": 0.026662010578567104,
                    "acc_norm": 0.3258064516129032,
                    "acc_norm_stderr": 0.026662010578567104,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.3103448275862069,
                    "acc_stderr": 0.03255086769970103,
                    "acc_norm": 0.3103448275862069,
                    "acc_norm_stderr": 0.03255086769970103,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.28,
                    "acc_stderr": 0.04512608598542128,
                    "acc_norm": 0.28,
                    "acc_norm_stderr": 0.04512608598542128,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.3393939393939394,
                    "acc_stderr": 0.03697442205031596,
                    "acc_norm": 0.3393939393939394,
                    "acc_norm_stderr": 0.03697442205031596,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.30303030303030304,
                    "acc_stderr": 0.03274287914026868,
                    "acc_norm": 0.30303030303030304,
                    "acc_norm_stderr": 0.03274287914026868,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.24352331606217617,
                    "acc_stderr": 0.03097543638684543,
                    "acc_norm": 0.24352331606217617,
                    "acc_norm_stderr": 0.03097543638684543,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.2564102564102564,
                    "acc_stderr": 0.022139081103971545,
                    "acc_norm": 0.2564102564102564,
                    "acc_norm_stderr": 0.022139081103971545,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.29259259259259257,
                    "acc_stderr": 0.02773896963217609,
                    "acc_norm": 0.29259259259259257,
                    "acc_norm_stderr": 0.02773896963217609,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.2773109243697479,
                    "acc_stderr": 0.02907937453948001,
                    "acc_norm": 0.2773109243697479,
                    "acc_norm_stderr": 0.02907937453948001,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.24503311258278146,
                    "acc_stderr": 0.035118075718047245,
                    "acc_norm": 0.24503311258278146,
                    "acc_norm_stderr": 0.035118075718047245,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.28440366972477066,
                    "acc_stderr": 0.019342036587702584,
                    "acc_norm": 0.28440366972477066,
                    "acc_norm_stderr": 0.019342036587702584,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.19907407407407407,
                    "acc_stderr": 0.02723229846269021,
                    "acc_norm": 0.19907407407407407,
                    "acc_norm_stderr": 0.02723229846269021,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.28921568627450983,
                    "acc_stderr": 0.03182231867647553,
                    "acc_norm": 0.28921568627450983,
                    "acc_norm_stderr": 0.03182231867647553,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.3080168776371308,
                    "acc_stderr": 0.030052389335605702,
                    "acc_norm": 0.3080168776371308,
                    "acc_norm_stderr": 0.030052389335605702,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.3991031390134529,
                    "acc_stderr": 0.032867453125679603,
                    "acc_norm": 0.3991031390134529,
                    "acc_norm_stderr": 0.032867453125679603,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2366412213740458,
                    "acc_stderr": 0.037276735755969195,
                    "acc_norm": 0.2366412213740458,
                    "acc_norm_stderr": 0.037276735755969195,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.34710743801652894,
                    "acc_stderr": 0.04345724570292535,
                    "acc_norm": 0.34710743801652894,
                    "acc_norm_stderr": 0.04345724570292535,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.28703703703703703,
                    "acc_stderr": 0.04373313040914761,
                    "acc_norm": 0.28703703703703703,
                    "acc_norm_stderr": 0.04373313040914761,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.294478527607362,
                    "acc_stderr": 0.03581165790474082,
                    "acc_norm": 0.294478527607362,
                    "acc_norm_stderr": 0.03581165790474082,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.35714285714285715,
                    "acc_stderr": 0.04547960999764376,
                    "acc_norm": 0.35714285714285715,
                    "acc_norm_stderr": 0.04547960999764376,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.2524271844660194,
                    "acc_stderr": 0.043012503996908764,
                    "acc_norm": 0.2524271844660194,
                    "acc_norm_stderr": 0.043012503996908764,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.36752136752136755,
                    "acc_stderr": 0.03158539157745637,
                    "acc_norm": 0.36752136752136755,
                    "acc_norm_stderr": 0.03158539157745637,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252604,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.36015325670498083,
                    "acc_stderr": 0.017166362471369295,
                    "acc_norm": 0.36015325670498083,
                    "acc_norm_stderr": 0.017166362471369295,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.32947976878612717,
                    "acc_stderr": 0.025305258131879702,
                    "acc_norm": 0.32947976878612717,
                    "acc_norm_stderr": 0.025305258131879702,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2871508379888268,
                    "acc_stderr": 0.015131608849963759,
                    "acc_norm": 0.2871508379888268,
                    "acc_norm_stderr": 0.015131608849963759,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.32679738562091504,
                    "acc_stderr": 0.02685729466328142,
                    "acc_norm": 0.32679738562091504,
                    "acc_norm_stderr": 0.02685729466328142,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.31189710610932475,
                    "acc_stderr": 0.02631185807185416,
                    "acc_norm": 0.31189710610932475,
                    "acc_norm_stderr": 0.02631185807185416,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.24691358024691357,
                    "acc_stderr": 0.02399350170904211,
                    "acc_norm": 0.24691358024691357,
                    "acc_norm_stderr": 0.02399350170904211,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2801418439716312,
                    "acc_stderr": 0.026789172351140245,
                    "acc_norm": 0.2801418439716312,
                    "acc_norm_stderr": 0.026789172351140245,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.25749674054758803,
                    "acc_stderr": 0.011167706014904156,
                    "acc_norm": 0.25749674054758803,
                    "acc_norm_stderr": 0.011167706014904156,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.20588235294117646,
                    "acc_stderr": 0.024562204314142314,
                    "acc_norm": 0.20588235294117646,
                    "acc_norm_stderr": 0.024562204314142314,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.01784808957491322,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.01784808957491322,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.32727272727272727,
                    "acc_stderr": 0.04494290866252089,
                    "acc_norm": 0.32727272727272727,
                    "acc_norm_stderr": 0.04494290866252089,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.2163265306122449,
                    "acc_stderr": 0.026358916334904038,
                    "acc_norm": 0.2163265306122449,
                    "acc_norm_stderr": 0.026358916334904038,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.2835820895522388,
                    "acc_stderr": 0.031871875379197986,
                    "acc_norm": 0.2835820895522388,
                    "acc_norm_stderr": 0.031871875379197986,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.41,
                    "acc_stderr": 0.04943110704237102,
                    "acc_norm": 0.41,
                    "acc_norm_stderr": 0.04943110704237102,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3795180722891566,
                    "acc_stderr": 0.03777798822748017,
                    "acc_norm": 0.3795180722891566,
                    "acc_norm_stderr": 0.03777798822748017,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.3391812865497076,
                    "acc_stderr": 0.036310534964889056,
                    "acc_norm": 0.3391812865497076,
                    "acc_norm_stderr": 0.036310534964889056,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.26193390452876375,
                    "mc1_stderr": 0.01539211880501503,
                    "mc2": 0.3862844409155128,
                    "mc2_stderr": 0.014439073256995538,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7040252565114443,
                    "acc_stderr": 0.012829348226339014,
                    "timestamp": "2024-03-04T05-03-52.331388"
                }
            },
            "gsm8k": {
                "exact_match": 0.06974981046247157,
                "exact_match_stderr": 0.007016389571013849,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "minerva_math_precalc": {
                "exact_match": 0.009157509157509158,
                "exact_match_stderr": 0.004080306065049012,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.02525832376578645,
                "exact_match_stderr": 0.005319703220303023,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.009259259259259259,
                "exact_match_stderr": 0.00412547301549024,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.012181616832779624,
                "exact_match_stderr": 0.0036524791938863832,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "minerva_math_geometry": {
                "exact_match": 0.022964509394572025,
                "exact_match_stderr": 0.006851249878769243,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.008438818565400843,
                "exact_match_stderr": 0.0042060072077130545,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "minerva_math_algebra": {
                "exact_match": 0.016006739679865205,
                "exact_match_stderr": 0.0036442247924417305,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_3da": {
                "acc": 0.033,
                "acc_stderr": 0.003995432609977368,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_3ds": {
                "acc": 0.0555,
                "acc_stderr": 0.005120838456077828,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_4da": {
                "acc": 0.002,
                "acc_stderr": 0.000999249343069495,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_2ds": {
                "acc": 0.3135,
                "acc_stderr": 0.010376064107029178,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_5ds": {
                "acc": 0.0025,
                "acc_stderr": 0.0011169148353275362,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_5da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000087,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_1dc": {
                "acc": 0.092,
                "acc_stderr": 0.006464433033702537,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_4ds": {
                "acc": 0.004,
                "acc_stderr": 0.0014117352790977043,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_2dm": {
                "acc": 0.1035,
                "acc_stderr": 0.006813008406113383,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "arithmetic_2da": {
                "acc": 0.147,
                "acc_stderr": 0.007920029256998863,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "gsm8k_cot": {
                "exact_match": 0.10462471569370735,
                "exact_match_stderr": 0.008430668082029294,
                "timestamp": "2024-06-07T20-31-55.858419"
            },
            "anli_r2": {
                "brier_score": 0.9517210089818693,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "anli_r3": {
                "brier_score": 0.9091491237138922,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "anli_r1": {
                "brier_score": 0.9740803307547983,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_eu": {
                "brier_score": 1.039626455615514,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_vi": {
                "brier_score": 0.9944426502362462,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_ru": {
                "brier_score": 0.8204429465406277,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_zh": {
                "brier_score": 1.0121312596962848,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_tr": {
                "brier_score": 0.9511392449037739,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_fr": {
                "brier_score": 0.7485518412292533,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_en": {
                "brier_score": 0.6574942705526986,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_ur": {
                "brier_score": 1.3224866657294507,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_ar": {
                "brier_score": 1.2550232606263443,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_de": {
                "brier_score": 0.8394366588628737,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_hi": {
                "brier_score": 1.1395961000527088,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_es": {
                "brier_score": 0.8186718184732203,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_bg": {
                "brier_score": 0.9250478368056083,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_sw": {
                "brier_score": 1.106399353671628,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_el": {
                "brier_score": 0.9901456477406901,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "xnli_th": {
                "brier_score": 0.9661701648978412,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "logiqa2": {
                "brier_score": 1.0772771409734918,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "mathqa": {
                "brier_score": 0.9470815194657676,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-07T20-43-19.317762"
            },
            "lambada_standard": {
                "perplexity": 4.151489270057361,
                "perplexity_stderr": 0.0898434559924617,
                "acc": 0.6600038812342325,
                "acc_stderr": 0.006599671169668108,
                "timestamp": "2024-06-07T20-44-40.271983"
            },
            "lambada_openai": {
                "perplexity": 3.3084712739580997,
                "perplexity_stderr": 0.06869330646899366,
                "acc": 0.7345235784979623,
                "acc_stderr": 0.006152164239586487,
                "timestamp": "2024-06-07T20-44-40.271983"
            }
        }
    }
}