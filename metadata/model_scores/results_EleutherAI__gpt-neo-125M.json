{
    "model_name": "EleutherAI/gpt-neo-125M",
    "last_updated": "2024-12-04 11:24:53.916102",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.003663003663003663,
                    "acc_stderr": 0.002587757368193456,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.002296211251435132,
                    "acc_stderr": 0.00162273313693462,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.0022148394241417496,
                    "acc_stderr": 0.0015652595934070484,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.0041753653444676405,
                    "acc_stderr": 0.002949339217075629,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.002109704641350211,
                    "acc_stderr": 0.002109704641350211,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.003369839932603201,
                    "acc_stderr": 0.001682787605228352,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.0005,
                    "acc_stderr": 0.0005000000000000061,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.001,
                    "acc_stderr": 0.0007069298939339515,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.001,
                    "acc_stderr": 0.000706929893933945,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.0025,
                    "acc_stderr": 0.0011169148353275264,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.0285,
                    "acc_stderr": 0.0037216663472429377,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 0.001,
                    "acc_stderr": 0.0007069298939339552,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.015163002274450341,
                    "acc_stderr": 0.003366022949726344,
                    "timestamp": "2024-06-05T04-52-37.443352"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.017437452615617893,
                    "acc_stderr": 0.003605486867998238,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "anli_r2": {
                "0-shot": {
                    "brier_score": 0.9548365401800128,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "anli_r3": {
                "0-shot": {
                    "brier_score": 0.8891583885006976,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "anli_r1": {
                "0-shot": {
                    "brier_score": 0.9622490181407919,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_eu": {
                "0-shot": {
                    "brier_score": 1.1177099043871799,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_vi": {
                "0-shot": {
                    "brier_score": 0.849920028552591,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_ru": {
                "0-shot": {
                    "brier_score": 0.9576388869814577,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_zh": {
                "0-shot": {
                    "brier_score": 0.9929665020833541,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_tr": {
                "0-shot": {
                    "brier_score": 0.9095949998078355,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_fr": {
                "0-shot": {
                    "brier_score": 0.9206902731267517,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_en": {
                "0-shot": {
                    "brier_score": 0.7190039933226706,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_ur": {
                "0-shot": {
                    "brier_score": 1.3284008366257785,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_ar": {
                "0-shot": {
                    "brier_score": 0.8957307885455548,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_de": {
                "0-shot": {
                    "brier_score": 0.9295157814794096,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_hi": {
                "0-shot": {
                    "brier_score": 0.789158069353759,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_es": {
                "0-shot": {
                    "brier_score": 1.042790212974061,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_bg": {
                "0-shot": {
                    "brier_score": 1.2387012114897136,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_sw": {
                "0-shot": {
                    "brier_score": 1.281388698211314,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_el": {
                "0-shot": {
                    "brier_score": 1.2797796563123813,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "xnli_th": {
                "0-shot": {
                    "brier_score": 1.0078771683411278,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "brier_score": 1.1460935685457103,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "mathqa": {
                "5-shot": {
                    "brier_score": 1.0224773820452193,
                    "timestamp": "2024-06-04T14-52-34.656011"
                }
            },
            "lambada_standard": {
                "0-shot": {
                    "perplexity": 111.82437441637022,
                    "perplexity_stderr": 5.021215660258462,
                    "acc": 0.26062487871143025,
                    "acc_stderr": 0.006115788029333536,
                    "timestamp": "2024-06-04T12-32-54.620336"
                }
            },
            "lambada_openai": {
                "0-shot": {
                    "perplexity": 30.265471083236193,
                    "perplexity_stderr": 1.1271375050920782,
                    "acc": 0.3735687948767708,
                    "acc_stderr": 0.006739599048608376,
                    "timestamp": "2024-06-04T12-32-54.620336"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.2046783625730994,
                    "acc_stderr": 0.030944459778533204,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.24603174603174602,
                    "acc_stderr": 0.03852273364924316,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.21604938271604937,
                    "acc_stderr": 0.022899162918445796,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.2245810055865922,
                    "acc_stderr": 0.01395680366654464,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.22362869198312235,
                    "acc_stderr": 0.027123298205229972,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.24855491329479767,
                    "acc_stderr": 0.023267528432100174,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.2470664928292047,
                    "acc_stderr": 0.011015752255279327,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.24539877300613497,
                    "acc_stderr": 0.03380939813943354,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.29901960784313725,
                    "acc_stderr": 0.03213325717373615,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.1864951768488746,
                    "acc_stderr": 0.02212243977248077,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.23148148148148148,
                    "acc_stderr": 0.04077494709252627,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.2231404958677686,
                    "acc_stderr": 0.03800754475228733,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.24848484848484848,
                    "acc_stderr": 0.03374402644139404,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.35751295336787564,
                    "acc_stderr": 0.034588160421810066,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.23109243697478993,
                    "acc_stderr": 0.027381406927868952,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.36363636363636365,
                    "acc_stderr": 0.03427308652999935,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.27889908256880735,
                    "acc_stderr": 0.019227468876463514,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.041723430387053825,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.24378109452736318,
                    "acc_stderr": 0.030360490154014652,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.2512820512820513,
                    "acc_stderr": 0.021992016662370533,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.031362502409358936,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.27124183006535946,
                    "acc_stderr": 0.017986615304030295,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.2595419847328244,
                    "acc_stderr": 0.03844876139785271,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.041424397194893624,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.24010217113665389,
                    "acc_stderr": 0.015274685213734191,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.2692307692307692,
                    "acc_stderr": 0.029058588303748845,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.2524271844660194,
                    "acc_stderr": 0.04301250399690878,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.27124183006535946,
                    "acc_stderr": 0.02545775669666787,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.27802690582959644,
                    "acc_stderr": 0.030069584874494043,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.4485294117647059,
                    "acc_stderr": 0.030211479609121593,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.1907514450867052,
                    "acc_stderr": 0.029957851329869334,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.042295258468165044,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.25660377358490566,
                    "acc_stderr": 0.02688064788905199,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536955,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.19879518072289157,
                    "acc_stderr": 0.03106939026078942,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.24113475177304963,
                    "acc_stderr": 0.025518731049537766,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.04158307533083286,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.31788079470198677,
                    "acc_stderr": 0.038020397601079024,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.25161290322580643,
                    "acc_stderr": 0.02468597928623996,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.2708333333333333,
                    "acc_stderr": 0.03716177437566018,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.2740740740740741,
                    "acc_stderr": 0.03853254836552003,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.03942772444036624,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.17763157894736842,
                    "acc_stderr": 0.031103182383123398,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.2851063829787234,
                    "acc_stderr": 0.029513196625539355,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.26785714285714285,
                    "acc_stderr": 0.04203277291467763,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.27586206896551724,
                    "acc_stderr": 0.03144712581678242,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.4583333333333333,
                    "acc_stderr": 0.03398110890294636,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.022019080012217897,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.23448275862068965,
                    "acc_stderr": 0.035306258743465914,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.026466117538959912,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.20136518771331058,
                    "acc_stderr": 0.011718927477444267,
                    "acc_norm": 0.23890784982935154,
                    "acc_norm_stderr": 0.01246107137631662,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.2849034056960765,
                    "acc_stderr": 0.004504459553909797,
                    "acc_norm": 0.3020314678350926,
                    "acc_norm_stderr": 0.004582004744713358,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.45579370727747115,
                    "acc_stderr": 0.015399303339136132,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 13.82766815700744,
                    "bleu_max_stderr": 0.5154784183208384,
                    "bleu_acc": 0.39167686658506734,
                    "bleu_acc_stderr": 0.017087795881769643,
                    "bleu_diff": -0.3071316802783103,
                    "bleu_diff_stderr": 0.48061175573860104,
                    "rouge1_max": 34.23389894145755,
                    "rouge1_max_stderr": 0.7612633125881414,
                    "rouge1_acc": 0.35862913096695226,
                    "rouge1_acc_stderr": 0.016789289499502022,
                    "rouge1_diff": -1.3810176678838904,
                    "rouge1_diff_stderr": 0.7542591368961628,
                    "rouge2_max": 16.07909085959659,
                    "rouge2_max_stderr": 0.7862657983849908,
                    "rouge2_acc": 0.1909424724602203,
                    "rouge2_acc_stderr": 0.013759285842685716,
                    "rouge2_diff": -2.05928101776518,
                    "rouge2_diff_stderr": 0.7203850974001921,
                    "rougeL_max": 31.690463611915124,
                    "rougeL_max_stderr": 0.7444436261611048,
                    "rougeL_acc": 0.3659730722154223,
                    "rougeL_acc_stderr": 0.016862941684088383,
                    "rougeL_diff": -0.9361203963395062,
                    "rougeL_diff_stderr": 0.7449460193846943,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.2582619339045288,
                    "acc_stderr": 0.015321821688476187,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.510655090765588,
                    "acc_stderr": 0.014049294536290403,
                    "timestamp": "2024-11-21T17-28-44.517154"
                }
            }
        }
    }
}