{
    "model_name": "EleutherAI/gpt-neo-125m",
    "last_updated": "2023-07-19",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.19112627986348124,
                    "acc_stderr": 0.01149005529277859,
                    "acc_norm": 0.2295221843003413,
                    "acc_norm_stderr": 0.012288926760890785,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.28340967934674366,
                    "acc_stderr": 0.0044973255339596264,
                    "acc_norm": 0.30262895837482573,
                    "acc_norm_stderr": 0.004584571102598107,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.04163331998932269,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.04163331998932269,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.03820169914517905,
                    "acc_norm": 0.26666666666666666,
                    "acc_norm_stderr": 0.03820169914517905,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.17763157894736842,
                    "acc_stderr": 0.031103182383123398,
                    "acc_norm": 0.17763157894736842,
                    "acc_norm_stderr": 0.031103182383123398,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.042295258468165044,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.042295258468165044,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.25660377358490566,
                    "acc_stderr": 0.026880647889051975,
                    "acc_norm": 0.25660377358490566,
                    "acc_norm_stderr": 0.026880647889051975,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.2777777777777778,
                    "acc_stderr": 0.037455547914624576,
                    "acc_norm": 0.2777777777777778,
                    "acc_norm_stderr": 0.037455547914624576,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.04292346959909282,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.04292346959909282,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.34,
                    "acc_stderr": 0.04760952285695235,
                    "acc_norm": 0.34,
                    "acc_norm_stderr": 0.04760952285695235,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.04461960433384741,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.04461960433384741,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.1907514450867052,
                    "acc_stderr": 0.029957851329869327,
                    "acc_norm": 0.1907514450867052,
                    "acc_norm_stderr": 0.029957851329869327,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.22549019607843138,
                    "acc_stderr": 0.04158307533083286,
                    "acc_norm": 0.22549019607843138,
                    "acc_norm_stderr": 0.04158307533083286,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.19,
                    "acc_stderr": 0.03942772444036622,
                    "acc_norm": 0.19,
                    "acc_norm_stderr": 0.03942772444036622,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.2851063829787234,
                    "acc_stderr": 0.029513196625539355,
                    "acc_norm": 0.2851063829787234,
                    "acc_norm_stderr": 0.029513196625539355,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2543859649122807,
                    "acc_stderr": 0.0409698513984367,
                    "acc_norm": 0.2543859649122807,
                    "acc_norm_stderr": 0.0409698513984367,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.23448275862068965,
                    "acc_stderr": 0.035306258743465914,
                    "acc_norm": 0.23448275862068965,
                    "acc_norm_stderr": 0.035306258743465914,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.24074074074074073,
                    "acc_stderr": 0.022019080012217893,
                    "acc_norm": 0.24074074074074073,
                    "acc_norm_stderr": 0.022019080012217893,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.24603174603174602,
                    "acc_stderr": 0.03852273364924315,
                    "acc_norm": 0.24603174603174602,
                    "acc_norm_stderr": 0.03852273364924315,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.18,
                    "acc_stderr": 0.038612291966536934,
                    "acc_norm": 0.18,
                    "acc_norm_stderr": 0.038612291966536934,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.24838709677419354,
                    "acc_stderr": 0.024580028921481,
                    "acc_norm": 0.24838709677419354,
                    "acc_norm_stderr": 0.024580028921481,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.28078817733990147,
                    "acc_stderr": 0.03161856335358609,
                    "acc_norm": 0.28078817733990147,
                    "acc_norm_stderr": 0.03161856335358609,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.040936018074033256,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.24848484848484848,
                    "acc_stderr": 0.03374402644139404,
                    "acc_norm": 0.24848484848484848,
                    "acc_norm_stderr": 0.03374402644139404,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.35858585858585856,
                    "acc_stderr": 0.034169036403915214,
                    "acc_norm": 0.35858585858585856,
                    "acc_norm_stderr": 0.034169036403915214,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.35751295336787564,
                    "acc_stderr": 0.034588160421810045,
                    "acc_norm": 0.35751295336787564,
                    "acc_norm_stderr": 0.034588160421810045,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.25384615384615383,
                    "acc_stderr": 0.022066054378726257,
                    "acc_norm": 0.25384615384615383,
                    "acc_norm_stderr": 0.022066054378726257,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.2518518518518518,
                    "acc_stderr": 0.02646611753895991,
                    "acc_norm": 0.2518518518518518,
                    "acc_norm_stderr": 0.02646611753895991,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.23109243697478993,
                    "acc_stderr": 0.027381406927868963,
                    "acc_norm": 0.23109243697478993,
                    "acc_norm_stderr": 0.027381406927868963,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.31125827814569534,
                    "acc_stderr": 0.03780445850526733,
                    "acc_norm": 0.31125827814569534,
                    "acc_norm_stderr": 0.03780445850526733,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.27889908256880735,
                    "acc_stderr": 0.01922746887646352,
                    "acc_norm": 0.27889908256880735,
                    "acc_norm_stderr": 0.01922746887646352,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.4583333333333333,
                    "acc_stderr": 0.033981108902946366,
                    "acc_norm": 0.4583333333333333,
                    "acc_norm_stderr": 0.033981108902946366,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.29901960784313725,
                    "acc_stderr": 0.03213325717373617,
                    "acc_norm": 0.29901960784313725,
                    "acc_norm_stderr": 0.03213325717373617,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.21940928270042195,
                    "acc_stderr": 0.026939106581553945,
                    "acc_norm": 0.21940928270042195,
                    "acc_norm_stderr": 0.026939106581553945,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.27802690582959644,
                    "acc_stderr": 0.030069584874494047,
                    "acc_norm": 0.27802690582959644,
                    "acc_norm_stderr": 0.030069584874494047,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.2595419847328244,
                    "acc_stderr": 0.03844876139785271,
                    "acc_norm": 0.2595419847328244,
                    "acc_norm_stderr": 0.03844876139785271,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.2231404958677686,
                    "acc_stderr": 0.03800754475228733,
                    "acc_norm": 0.2231404958677686,
                    "acc_norm_stderr": 0.03800754475228733,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.0401910747255735,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.0401910747255735,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.24539877300613497,
                    "acc_stderr": 0.03380939813943354,
                    "acc_norm": 0.24539877300613497,
                    "acc_norm_stderr": 0.03380939813943354,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.26785714285714285,
                    "acc_stderr": 0.04203277291467762,
                    "acc_norm": 0.26785714285714285,
                    "acc_norm_stderr": 0.04203277291467762,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.24271844660194175,
                    "acc_stderr": 0.04245022486384495,
                    "acc_norm": 0.24271844660194175,
                    "acc_norm_stderr": 0.04245022486384495,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.2692307692307692,
                    "acc_stderr": 0.02905858830374884,
                    "acc_norm": 0.2692307692307692,
                    "acc_norm_stderr": 0.02905858830374884,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.24010217113665389,
                    "acc_stderr": 0.015274685213734195,
                    "acc_norm": 0.24010217113665389,
                    "acc_norm_stderr": 0.015274685213734195,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.24566473988439305,
                    "acc_stderr": 0.02317629820399201,
                    "acc_norm": 0.24566473988439305,
                    "acc_norm_stderr": 0.02317629820399201,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.22681564245810057,
                    "acc_stderr": 0.014005843570897882,
                    "acc_norm": 0.22681564245810057,
                    "acc_norm_stderr": 0.014005843570897882,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.2647058823529412,
                    "acc_stderr": 0.025261691219729484,
                    "acc_norm": 0.2647058823529412,
                    "acc_norm_stderr": 0.025261691219729484,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.1832797427652733,
                    "acc_stderr": 0.021974198848265805,
                    "acc_norm": 0.1832797427652733,
                    "acc_norm_stderr": 0.021974198848265805,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.21604938271604937,
                    "acc_stderr": 0.022899162918445803,
                    "acc_norm": 0.21604938271604937,
                    "acc_norm_stderr": 0.022899162918445803,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.24822695035460993,
                    "acc_stderr": 0.02577001564429039,
                    "acc_norm": 0.24822695035460993,
                    "acc_norm_stderr": 0.02577001564429039,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.2470664928292047,
                    "acc_stderr": 0.011015752255279338,
                    "acc_norm": 0.2470664928292047,
                    "acc_norm_stderr": 0.011015752255279338,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4485294117647059,
                    "acc_stderr": 0.030211479609121593,
                    "acc_norm": 0.4485294117647059,
                    "acc_norm_stderr": 0.030211479609121593,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2696078431372549,
                    "acc_stderr": 0.017952449196987862,
                    "acc_norm": 0.2696078431372549,
                    "acc_norm_stderr": 0.017952449196987862,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.041723430387053825,
                    "acc_norm": 0.2545454545454545,
                    "acc_norm_stderr": 0.041723430387053825,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.031362502409358936,
                    "acc_norm": 0.4,
                    "acc_norm_stderr": 0.031362502409358936,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.24378109452736318,
                    "acc_stderr": 0.03036049015401465,
                    "acc_norm": 0.24378109452736318,
                    "acc_norm_stderr": 0.03036049015401465,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.27,
                    "acc_stderr": 0.044619604333847394,
                    "acc_norm": 0.27,
                    "acc_norm_stderr": 0.044619604333847394,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.19879518072289157,
                    "acc_stderr": 0.03106939026078942,
                    "acc_norm": 0.19879518072289157,
                    "acc_norm_stderr": 0.03106939026078942,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.2046783625730994,
                    "acc_stderr": 0.030944459778533193,
                    "acc_norm": 0.2046783625730994,
                    "acc_norm_stderr": 0.030944459778533193,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2582619339045288,
                    "mc1_stderr": 0.01532182168847619,
                    "mc2": 0.455761630633801,
                    "mc2_stderr": 0.015400185849714155,
                    "timestamp": "2023-08-12T09-36-50.642447"
                }
            },
            "drop": {
                "3-shot": {
                    "em": 0.0016778523489932886,
                    "em_stderr": 0.0004191330178826801,
                    "f1": 0.03690436241610747,
                    "f1_stderr": 0.0011592977848577672,
                    "timestamp": "2023-10-18T09-42-25.890470"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.003032600454890068,
                    "acc_stderr": 0.0015145735612245494,
                    "timestamp": "2023-10-18T09-42-25.890470"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5177584846093133,
                    "acc_stderr": 0.014043619596174959,
                    "timestamp": "2023-10-18T09-42-25.890470"
                }
            }
        }
    }
}