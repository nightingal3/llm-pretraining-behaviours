{
    "model_name": "NinedayWang__PolyCoder-2.7B",
    "last_updated": "2024-12-19 13:37:53.735592",
    "results": {
        "harness": {
            "mmlu_world_religions": {
                "acc": 0.30409356725146197,
                "acc_stderr": 0.0352821125824523,
                "brier_score": 0.8938431035787754,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_formal_logic": {
                "acc": 0.2857142857142857,
                "acc_stderr": 0.04040610178208841,
                "brier_score": 1.0150480465735368,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_prehistory": {
                "acc": 0.2191358024691358,
                "acc_stderr": 0.023016705640262203,
                "brier_score": 1.0688599538686459,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574885,
                "brier_score": 0.8886120543076361,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.26582278481012656,
                "acc_stderr": 0.028756799629658335,
                "brier_score": 0.9960394039300178,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_moral_disputes": {
                "acc": 0.2514450867052023,
                "acc_stderr": 0.023357365785874037,
                "brier_score": 1.05020278137478,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_professional_law": {
                "acc": 0.2457627118644068,
                "acc_stderr": 0.01099615663514269,
                "brier_score": 1.1471446337073656,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.22085889570552147,
                "acc_stderr": 0.032591773927421776,
                "brier_score": 1.1352476522307973,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.25,
                "acc_stderr": 0.03039153369274154,
                "brier_score": 1.0396970803983272,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_philosophy": {
                "acc": 0.1832797427652733,
                "acc_stderr": 0.0219741988482658,
                "brier_score": 1.1221353849032953,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_jurisprudence": {
                "acc": 0.25,
                "acc_stderr": 0.04186091791394607,
                "brier_score": 1.088399168511025,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_international_law": {
                "acc": 0.2396694214876033,
                "acc_stderr": 0.03896878985070417,
                "brier_score": 1.126887944187717,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03225078108306289,
                "brier_score": 0.9985889584374511,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.20207253886010362,
                "acc_stderr": 0.02897908979429673,
                "brier_score": 1.161175617365508,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.20588235294117646,
                "acc_stderr": 0.026265024608275886,
                "brier_score": 1.136253757095844,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_geography": {
                "acc": 0.17676767676767677,
                "acc_stderr": 0.027178752639044915,
                "brier_score": 1.1626575476275403,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.1908256880733945,
                "acc_stderr": 0.01684767640009109,
                "brier_score": 1.1522750595179523,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_public_relations": {
                "acc": 0.21818181818181817,
                "acc_stderr": 0.03955932861795833,
                "brier_score": 1.1047895125095324,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.9752901097938361,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_sociology": {
                "acc": 0.23880597014925373,
                "acc_stderr": 0.030147775935409227,
                "brier_score": 1.106775743949372,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.20256410256410257,
                "acc_stderr": 0.020377660970371397,
                "brier_score": 1.106854573740813,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_security_studies": {
                "acc": 0.1836734693877551,
                "acc_stderr": 0.024789071332007643,
                "brier_score": 1.2265616632194247,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_professional_psychology": {
                "acc": 0.25163398692810457,
                "acc_stderr": 0.01755581809132225,
                "brier_score": 1.1237729121517326,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_human_sexuality": {
                "acc": 0.2595419847328244,
                "acc_stderr": 0.0384487613978527,
                "brier_score": 1.0527196659699807,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_econometrics": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.039994238792813386,
                "brier_score": 1.0186789892109231,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_miscellaneous": {
                "acc": 0.24010217113665389,
                "acc_stderr": 0.015274685213734188,
                "brier_score": 0.9769991448277808,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_marketing": {
                "acc": 0.2905982905982906,
                "acc_stderr": 0.029745048572674057,
                "brier_score": 1.102306778717253,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_management": {
                "acc": 0.17475728155339806,
                "acc_stderr": 0.03760178006026621,
                "brier_score": 1.190507413382486,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_nutrition": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.02355083135199509,
                "brier_score": 1.0773290847012829,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_medical_genetics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.9521524003092359,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_human_aging": {
                "acc": 0.31390134529147984,
                "acc_stderr": 0.03114679648297246,
                "brier_score": 0.947392555723536,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_professional_medicine": {
                "acc": 0.18382352941176472,
                "acc_stderr": 0.02352924218519311,
                "brier_score": 1.1710175839535208,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_college_medicine": {
                "acc": 0.2138728323699422,
                "acc_stderr": 0.03126511206173043,
                "brier_score": 1.1487105628511547,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_business_ethics": {
                "acc": 0.3,
                "acc_stderr": 0.046056618647183814,
                "brier_score": 0.9736715265758287,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.21132075471698114,
                "acc_stderr": 0.025125766484827856,
                "brier_score": 1.101333058881401,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_global_facts": {
                "acc": 0.18,
                "acc_stderr": 0.038612291966536955,
                "brier_score": 1.0434812793332202,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_virology": {
                "acc": 0.28313253012048195,
                "acc_stderr": 0.03507295431370518,
                "brier_score": 1.0022878477677757,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_professional_accounting": {
                "acc": 0.22695035460992907,
                "acc_stderr": 0.024987106365642976,
                "brier_score": 1.1000058041634257,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_college_physics": {
                "acc": 0.21568627450980393,
                "acc_stderr": 0.040925639582376556,
                "brier_score": 1.0905135942657984,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_physics": {
                "acc": 0.2052980132450331,
                "acc_stderr": 0.032979866484738336,
                "brier_score": 1.1401240382655313,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_biology": {
                "acc": 0.19032258064516128,
                "acc_stderr": 0.022331707611823085,
                "brier_score": 1.119124794185339,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_college_biology": {
                "acc": 0.25,
                "acc_stderr": 0.03621034121889507,
                "brier_score": 1.0291517128501562,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_anatomy": {
                "acc": 0.18518518518518517,
                "acc_stderr": 0.03355677216313142,
                "brier_score": 1.1034100401415832,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_college_chemistry": {
                "acc": 0.19,
                "acc_stderr": 0.03942772444036623,
                "brier_score": 1.0940351459391089,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_computer_security": {
                "acc": 0.28,
                "acc_stderr": 0.045126085985421276,
                "brier_score": 1.0316655764682736,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_college_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 1.1385161111574733,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_astronomy": {
                "acc": 0.17763157894736842,
                "acc_stderr": 0.031103182383123398,
                "brier_score": 1.15340863761547,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_college_mathematics": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 1.0943448594442058,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.26382978723404255,
                "acc_stderr": 0.02880998985410298,
                "brier_score": 0.9734414312468421,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.22,
                "acc_stderr": 0.04163331998932269,
                "brier_score": 1.0547517500487764,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 1.096824166644259,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_machine_learning": {
                "acc": 0.32142857142857145,
                "acc_stderr": 0.04432804055291519,
                "brier_score": 0.955715696475034,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.17733990147783252,
                "acc_stderr": 0.02687433727680835,
                "brier_score": 1.084687793483445,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1527777777777778,
                "acc_stderr": 0.02453632602613422,
                "brier_score": 1.2395721614530157,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.20899470899470898,
                "acc_stderr": 0.020940481565334835,
                "brier_score": 1.0251953695947718,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.2413793103448276,
                "acc_stderr": 0.03565998174135302,
                "brier_score": 1.061490829841905,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.2074074074074074,
                "acc_stderr": 0.024720713193952165,
                "brier_score": 1.0396806234134943,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T18-53-22.386081"
            },
            "arc_challenge": {
                "acc": 0.181740614334471,
                "acc_stderr": 0.011269198948880236,
                "acc_norm": 0.21928327645051193,
                "acc_norm_stderr": 0.012091245787615713,
                "timestamp": "2024-11-22T00-21-02.759683"
            },
            "hellaswag": {
                "acc": 0.2773351921927903,
                "acc_stderr": 0.004467684132772418,
                "acc_norm": 0.2975502887870942,
                "acc_norm_stderr": 0.004562462665505278,
                "timestamp": "2024-11-22T00-21-02.759683"
            },
            "truthfulqa_mc2": {
                "acc": 0.4900198282743296,
                "acc_stderr": 0.015924776108438145,
                "timestamp": "2024-11-22T00-21-02.759683"
            },
            "truthfulqa_gen": {
                "bleu_max": 19.109929609778494,
                "bleu_max_stderr": 0.6347291486261849,
                "bleu_acc": 0.2913096695226438,
                "bleu_acc_stderr": 0.015905987048184824,
                "bleu_diff": -2.73808807451127,
                "bleu_diff_stderr": 0.4927649447592839,
                "rouge1_max": 43.64910481680449,
                "rouge1_max_stderr": 0.7663193889003078,
                "rouge1_acc": 0.2974296205630355,
                "rouge1_acc_stderr": 0.01600265148736098,
                "rouge1_diff": -5.224813168517498,
                "rouge1_diff_stderr": 0.5828636449526232,
                "rouge2_max": 27.07899530604482,
                "rouge2_max_stderr": 0.8474168667870206,
                "rouge2_acc": 0.24724602203182375,
                "rouge2_acc_stderr": 0.015102404797359652,
                "rouge2_diff": -4.815471980544821,
                "rouge2_diff_stderr": 0.6322835005066595,
                "rougeL_max": 40.60861307441461,
                "rougeL_max_stderr": 0.765726782060013,
                "rougeL_acc": 0.2802937576499388,
                "rougeL_acc_stderr": 0.015723139524608777,
                "rougeL_diff": -4.909635032740114,
                "rougeL_diff_stderr": 0.5671337300796637,
                "timestamp": "2024-11-22T00-21-02.759683"
            },
            "truthfulqa_mc1": {
                "acc": 0.2876376988984088,
                "acc_stderr": 0.015846315101394805,
                "timestamp": "2024-11-22T00-21-02.759683"
            },
            "winogrande": {
                "acc": 0.5209155485398579,
                "acc_stderr": 0.014040185494212942,
                "timestamp": "2024-11-22T00-21-02.759683"
            },
            "gsm8k": {
                "exact_match": 0.02122820318423048,
                "exact_match_stderr": 0.003970449129848635,
                "timestamp": "2024-11-22T00-21-02.759683"
            }
        }
    }
}