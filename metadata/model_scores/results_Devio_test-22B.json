{
    "model_name": "Devio/test-22B",
    "last_updated": "2023-09-02",
    "results": {
        "harness": {
            "drop": {
                "3-shot": {
                    "em": 0.002936241610738255,
                    "em_stderr": 0.0005541113054709917,
                    "f1": 0.03323510906040272,
                    "f1_stderr": 0.0011026689087019657,
                    "timestamp": "2023-10-16T03-23-54.397499"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.0037907505686125853,
                    "acc_stderr": 0.0016927007401501832,
                    "timestamp": "2023-10-16T03-23-54.397499"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5769534333070244,
                    "acc_stderr": 0.013885055359056472,
                    "timestamp": "2023-10-16T03-23-54.397499"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.3438566552901024,
                    "acc_stderr": 0.01388064457015621,
                    "acc_norm": 0.39419795221843,
                    "acc_norm_stderr": 0.014280522667467325,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.4690300736904999,
                    "acc_stderr": 0.004980200451851677,
                    "acc_norm": 0.6450906193985262,
                    "acc_norm_stderr": 0.004775079636567092,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768081,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768081,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.22962962962962963,
                    "acc_stderr": 0.03633384414073462,
                    "acc_norm": 0.22962962962962963,
                    "acc_norm_stderr": 0.03633384414073462,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.32894736842105265,
                    "acc_stderr": 0.03823428969926604,
                    "acc_norm": 0.32894736842105265,
                    "acc_norm_stderr": 0.03823428969926604,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.17,
                    "acc_stderr": 0.0377525168068637,
                    "acc_norm": 0.17,
                    "acc_norm_stderr": 0.0377525168068637,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.2981132075471698,
                    "acc_stderr": 0.028152837942493854,
                    "acc_norm": 0.2981132075471698,
                    "acc_norm_stderr": 0.028152837942493854,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.24305555555555555,
                    "acc_stderr": 0.03586879280080341,
                    "acc_norm": 0.24305555555555555,
                    "acc_norm_stderr": 0.03586879280080341,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.24,
                    "acc_stderr": 0.042923469599092816,
                    "acc_norm": 0.24,
                    "acc_norm_stderr": 0.042923469599092816,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.3063583815028902,
                    "acc_stderr": 0.03514942551267438,
                    "acc_norm": 0.3063583815028902,
                    "acc_norm_stderr": 0.03514942551267438,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.29411764705882354,
                    "acc_stderr": 0.045338381959297736,
                    "acc_norm": 0.29411764705882354,
                    "acc_norm_stderr": 0.045338381959297736,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.04093601807403326,
                    "acc_norm": 0.21,
                    "acc_norm_stderr": 0.04093601807403326,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.28936170212765955,
                    "acc_stderr": 0.02964400657700962,
                    "acc_norm": 0.28936170212765955,
                    "acc_norm_stderr": 0.02964400657700962,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.23684210526315788,
                    "acc_stderr": 0.039994238792813344,
                    "acc_norm": 0.23684210526315788,
                    "acc_norm_stderr": 0.039994238792813344,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.2689655172413793,
                    "acc_stderr": 0.036951833116502325,
                    "acc_norm": 0.2689655172413793,
                    "acc_norm_stderr": 0.036951833116502325,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2698412698412698,
                    "acc_stderr": 0.022860838309232072,
                    "acc_norm": 0.2698412698412698,
                    "acc_norm_stderr": 0.022860838309232072,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.23015873015873015,
                    "acc_stderr": 0.03764950879790606,
                    "acc_norm": 0.23015873015873015,
                    "acc_norm_stderr": 0.03764950879790606,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.047258156262526045,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.047258156262526045,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.33225806451612905,
                    "acc_stderr": 0.026795560848122797,
                    "acc_norm": 0.33225806451612905,
                    "acc_norm_stderr": 0.026795560848122797,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.2955665024630542,
                    "acc_stderr": 0.032104944337514575,
                    "acc_norm": 0.2955665024630542,
                    "acc_norm_stderr": 0.032104944337514575,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.22,
                    "acc_stderr": 0.041633319989322695,
                    "acc_norm": 0.22,
                    "acc_norm_stderr": 0.041633319989322695,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.24848484848484848,
                    "acc_stderr": 0.03374402644139404,
                    "acc_norm": 0.24848484848484848,
                    "acc_norm_stderr": 0.03374402644139404,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.35353535353535354,
                    "acc_stderr": 0.03406086723547153,
                    "acc_norm": 0.35353535353535354,
                    "acc_norm_stderr": 0.03406086723547153,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.35233160621761656,
                    "acc_stderr": 0.03447478286414359,
                    "acc_norm": 0.35233160621761656,
                    "acc_norm_stderr": 0.03447478286414359,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.3564102564102564,
                    "acc_stderr": 0.024283140529467295,
                    "acc_norm": 0.3564102564102564,
                    "acc_norm_stderr": 0.024283140529467295,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.26296296296296295,
                    "acc_stderr": 0.02684205787383371,
                    "acc_norm": 0.26296296296296295,
                    "acc_norm_stderr": 0.02684205787383371,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.35294117647058826,
                    "acc_stderr": 0.031041941304059288,
                    "acc_norm": 0.35294117647058826,
                    "acc_norm_stderr": 0.031041941304059288,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.36423841059602646,
                    "acc_stderr": 0.03929111781242741,
                    "acc_norm": 0.36423841059602646,
                    "acc_norm_stderr": 0.03929111781242741,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.3486238532110092,
                    "acc_stderr": 0.020431254090714324,
                    "acc_norm": 0.3486238532110092,
                    "acc_norm_stderr": 0.020431254090714324,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.3888888888888889,
                    "acc_stderr": 0.03324708911809117,
                    "acc_norm": 0.3888888888888889,
                    "acc_norm_stderr": 0.03324708911809117,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.25980392156862747,
                    "acc_stderr": 0.03077855467869326,
                    "acc_norm": 0.25980392156862747,
                    "acc_norm_stderr": 0.03077855467869326,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.20253164556962025,
                    "acc_stderr": 0.026160568246601457,
                    "acc_norm": 0.20253164556962025,
                    "acc_norm_stderr": 0.026160568246601457,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.1210762331838565,
                    "acc_stderr": 0.021894174113185737,
                    "acc_norm": 0.1210762331838565,
                    "acc_norm_stderr": 0.021894174113185737,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.32061068702290074,
                    "acc_stderr": 0.04093329229834277,
                    "acc_norm": 0.32061068702290074,
                    "acc_norm_stderr": 0.04093329229834277,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.17355371900826447,
                    "acc_stderr": 0.03457272836917671,
                    "acc_norm": 0.17355371900826447,
                    "acc_norm_stderr": 0.03457272836917671,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.2222222222222222,
                    "acc_stderr": 0.040191074725573483,
                    "acc_norm": 0.2222222222222222,
                    "acc_norm_stderr": 0.040191074725573483,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.2331288343558282,
                    "acc_stderr": 0.033220157957767414,
                    "acc_norm": 0.2331288343558282,
                    "acc_norm_stderr": 0.033220157957767414,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.16964285714285715,
                    "acc_stderr": 0.03562367850095391,
                    "acc_norm": 0.16964285714285715,
                    "acc_norm_stderr": 0.03562367850095391,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.3786407766990291,
                    "acc_stderr": 0.04802694698258972,
                    "acc_norm": 0.3786407766990291,
                    "acc_norm_stderr": 0.04802694698258972,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.19658119658119658,
                    "acc_stderr": 0.02603538609895129,
                    "acc_norm": 0.19658119658119658,
                    "acc_norm_stderr": 0.02603538609895129,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816506,
                    "acc_norm": 0.23,
                    "acc_norm_stderr": 0.04229525846816506,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.21839080459770116,
                    "acc_stderr": 0.0147743583199345,
                    "acc_norm": 0.21839080459770116,
                    "acc_norm_stderr": 0.0147743583199345,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.21676300578034682,
                    "acc_stderr": 0.02218347766841286,
                    "acc_norm": 0.21676300578034682,
                    "acc_norm_stderr": 0.02218347766841286,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.24134078212290502,
                    "acc_stderr": 0.014310999547961459,
                    "acc_norm": 0.24134078212290502,
                    "acc_norm_stderr": 0.014310999547961459,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.29411764705882354,
                    "acc_stderr": 0.026090162504279053,
                    "acc_norm": 0.29411764705882354,
                    "acc_norm_stderr": 0.026090162504279053,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.2604501607717042,
                    "acc_stderr": 0.02492672322484554,
                    "acc_norm": 0.2604501607717042,
                    "acc_norm_stderr": 0.02492672322484554,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.22839506172839505,
                    "acc_stderr": 0.023358211840626267,
                    "acc_norm": 0.22839506172839505,
                    "acc_norm_stderr": 0.023358211840626267,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2695035460992908,
                    "acc_stderr": 0.026469036818590624,
                    "acc_norm": 0.2695035460992908,
                    "acc_norm_stderr": 0.026469036818590624,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.24511082138200782,
                    "acc_stderr": 0.010986307870045517,
                    "acc_norm": 0.24511082138200782,
                    "acc_norm_stderr": 0.010986307870045517,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4485294117647059,
                    "acc_stderr": 0.030211479609121593,
                    "acc_norm": 0.4485294117647059,
                    "acc_norm_stderr": 0.030211479609121593,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.2173202614379085,
                    "acc_stderr": 0.01668482092914859,
                    "acc_norm": 0.2173202614379085,
                    "acc_norm_stderr": 0.01668482092914859,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.2545454545454545,
                    "acc_stderr": 0.04172343038705383,
                    "acc_norm": 0.2545454545454545,
                    "acc_norm_stderr": 0.04172343038705383,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.031362502409358936,
                    "acc_norm": 0.4,
                    "acc_norm_stderr": 0.031362502409358936,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.29850746268656714,
                    "acc_stderr": 0.032357437893550424,
                    "acc_norm": 0.29850746268656714,
                    "acc_norm_stderr": 0.032357437893550424,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768078,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768078,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.1927710843373494,
                    "acc_stderr": 0.030709824050565274,
                    "acc_norm": 0.1927710843373494,
                    "acc_norm_stderr": 0.030709824050565274,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.1695906432748538,
                    "acc_stderr": 0.028782108105401712,
                    "acc_norm": 0.1695906432748538,
                    "acc_norm_stderr": 0.028782108105401712,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.22888616891064872,
                    "mc1_stderr": 0.014706994909055027,
                    "mc2": 0.3713156999785537,
                    "mc2_stderr": 0.014209414703476026,
                    "timestamp": "2023-09-02T01-38-52.675251"
                }
            }
        }
    }
}