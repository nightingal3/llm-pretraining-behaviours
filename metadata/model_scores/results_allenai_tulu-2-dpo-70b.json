{
    "model_name": "allenai/tulu-2-dpo-70b",
    "last_updated": "2024-02-02",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.6825938566552902,
                    "acc_stderr": 0.013602239088038167,
                    "acc_norm": 0.7209897610921502,
                    "acc_norm_stderr": 0.01310678488360134,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.7082254530969926,
                    "acc_stderr": 0.004536500714147989,
                    "acc_norm": 0.8898625771758614,
                    "acc_norm_stderr": 0.0031242116171988606,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6148148148148148,
                    "acc_stderr": 0.04203921040156279,
                    "acc_norm": 0.6148148148148148,
                    "acc_norm_stderr": 0.04203921040156279,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.7828947368421053,
                    "acc_stderr": 0.03355045304882924,
                    "acc_norm": 0.7828947368421053,
                    "acc_norm_stderr": 0.03355045304882924,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.73,
                    "acc_stderr": 0.0446196043338474,
                    "acc_norm": 0.73,
                    "acc_norm_stderr": 0.0446196043338474,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.7509433962264151,
                    "acc_stderr": 0.02661648298050171,
                    "acc_norm": 0.7509433962264151,
                    "acc_norm_stderr": 0.02661648298050171,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.8263888888888888,
                    "acc_stderr": 0.03167473383795718,
                    "acc_norm": 0.8263888888888888,
                    "acc_norm_stderr": 0.03167473383795718,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.51,
                    "acc_stderr": 0.05024183937956911,
                    "acc_norm": 0.51,
                    "acc_norm_stderr": 0.05024183937956911,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.59,
                    "acc_stderr": 0.04943110704237102,
                    "acc_norm": 0.59,
                    "acc_norm_stderr": 0.04943110704237102,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.39,
                    "acc_stderr": 0.049020713000019756,
                    "acc_norm": 0.39,
                    "acc_norm_stderr": 0.049020713000019756,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.7225433526011561,
                    "acc_stderr": 0.03414014007044037,
                    "acc_norm": 0.7225433526011561,
                    "acc_norm_stderr": 0.03414014007044037,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.43137254901960786,
                    "acc_stderr": 0.04928099597287534,
                    "acc_norm": 0.43137254901960786,
                    "acc_norm_stderr": 0.04928099597287534,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.72,
                    "acc_stderr": 0.045126085985421276,
                    "acc_norm": 0.72,
                    "acc_norm_stderr": 0.045126085985421276,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.6978723404255319,
                    "acc_stderr": 0.03001755447188056,
                    "acc_norm": 0.6978723404255319,
                    "acc_norm_stderr": 0.03001755447188056,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.4473684210526316,
                    "acc_stderr": 0.04677473004491199,
                    "acc_norm": 0.4473684210526316,
                    "acc_norm_stderr": 0.04677473004491199,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5793103448275863,
                    "acc_stderr": 0.04113914981189261,
                    "acc_norm": 0.5793103448275863,
                    "acc_norm_stderr": 0.04113914981189261,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.455026455026455,
                    "acc_stderr": 0.025646928361049398,
                    "acc_norm": 0.455026455026455,
                    "acc_norm_stderr": 0.025646928361049398,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.48412698412698413,
                    "acc_stderr": 0.04469881854072606,
                    "acc_norm": 0.48412698412698413,
                    "acc_norm_stderr": 0.04469881854072606,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7935483870967742,
                    "acc_stderr": 0.023025899617188716,
                    "acc_norm": 0.7935483870967742,
                    "acc_norm_stderr": 0.023025899617188716,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.5270935960591133,
                    "acc_stderr": 0.03512819077876106,
                    "acc_norm": 0.5270935960591133,
                    "acc_norm_stderr": 0.03512819077876106,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.78,
                    "acc_stderr": 0.04163331998932261,
                    "acc_norm": 0.78,
                    "acc_norm_stderr": 0.04163331998932261,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.8363636363636363,
                    "acc_stderr": 0.02888787239548795,
                    "acc_norm": 0.8363636363636363,
                    "acc_norm_stderr": 0.02888787239548795,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.8888888888888888,
                    "acc_stderr": 0.02239078763821676,
                    "acc_norm": 0.8888888888888888,
                    "acc_norm_stderr": 0.02239078763821676,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.9378238341968912,
                    "acc_stderr": 0.01742697415424052,
                    "acc_norm": 0.9378238341968912,
                    "acc_norm_stderr": 0.01742697415424052,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.7333333333333333,
                    "acc_stderr": 0.022421273612923707,
                    "acc_norm": 0.7333333333333333,
                    "acc_norm_stderr": 0.022421273612923707,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.3592592592592593,
                    "acc_stderr": 0.029252905927251976,
                    "acc_norm": 0.3592592592592593,
                    "acc_norm_stderr": 0.029252905927251976,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.8025210084033614,
                    "acc_stderr": 0.025859164122051453,
                    "acc_norm": 0.8025210084033614,
                    "acc_norm_stderr": 0.025859164122051453,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.47019867549668876,
                    "acc_stderr": 0.040752249922169775,
                    "acc_norm": 0.47019867549668876,
                    "acc_norm_stderr": 0.040752249922169775,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8935779816513761,
                    "acc_stderr": 0.013221554674594372,
                    "acc_norm": 0.8935779816513761,
                    "acc_norm_stderr": 0.013221554674594372,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.6064814814814815,
                    "acc_stderr": 0.03331747876370312,
                    "acc_norm": 0.6064814814814815,
                    "acc_norm_stderr": 0.03331747876370312,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.9166666666666666,
                    "acc_stderr": 0.019398452135813905,
                    "acc_norm": 0.9166666666666666,
                    "acc_norm_stderr": 0.019398452135813905,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.8607594936708861,
                    "acc_stderr": 0.022535526352692705,
                    "acc_norm": 0.8607594936708861,
                    "acc_norm_stderr": 0.022535526352692705,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.7668161434977578,
                    "acc_stderr": 0.028380391147094713,
                    "acc_norm": 0.7668161434977578,
                    "acc_norm_stderr": 0.028380391147094713,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.8320610687022901,
                    "acc_stderr": 0.03278548537343138,
                    "acc_norm": 0.8320610687022901,
                    "acc_norm_stderr": 0.03278548537343138,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.8677685950413223,
                    "acc_stderr": 0.0309227883204458,
                    "acc_norm": 0.8677685950413223,
                    "acc_norm_stderr": 0.0309227883204458,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.8425925925925926,
                    "acc_stderr": 0.035207039905179635,
                    "acc_norm": 0.8425925925925926,
                    "acc_norm_stderr": 0.035207039905179635,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.8282208588957055,
                    "acc_stderr": 0.029634717272371037,
                    "acc_norm": 0.8282208588957055,
                    "acc_norm_stderr": 0.029634717272371037,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.04745789978762494,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.04745789978762494,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.8155339805825242,
                    "acc_stderr": 0.03840423627288276,
                    "acc_norm": 0.8155339805825242,
                    "acc_norm_stderr": 0.03840423627288276,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8974358974358975,
                    "acc_stderr": 0.019875655027867457,
                    "acc_norm": 0.8974358974358975,
                    "acc_norm_stderr": 0.019875655027867457,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.69,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.69,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.8531289910600255,
                    "acc_stderr": 0.012658201736147288,
                    "acc_norm": 0.8531289910600255,
                    "acc_norm_stderr": 0.012658201736147288,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7658959537572254,
                    "acc_stderr": 0.022797110278071124,
                    "acc_norm": 0.7658959537572254,
                    "acc_norm_stderr": 0.022797110278071124,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.511731843575419,
                    "acc_stderr": 0.016717897676932162,
                    "acc_norm": 0.511731843575419,
                    "acc_norm_stderr": 0.016717897676932162,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7843137254901961,
                    "acc_stderr": 0.02355083135199509,
                    "acc_norm": 0.7843137254901961,
                    "acc_norm_stderr": 0.02355083135199509,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.7620578778135049,
                    "acc_stderr": 0.02418515064781871,
                    "acc_norm": 0.7620578778135049,
                    "acc_norm_stderr": 0.02418515064781871,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.8209876543209876,
                    "acc_stderr": 0.02133086876212706,
                    "acc_norm": 0.8209876543209876,
                    "acc_norm_stderr": 0.02133086876212706,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.574468085106383,
                    "acc_stderr": 0.02949482760014436,
                    "acc_norm": 0.574468085106383,
                    "acc_norm_stderr": 0.02949482760014436,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.546284224250326,
                    "acc_stderr": 0.012715404841277752,
                    "acc_norm": 0.546284224250326,
                    "acc_norm_stderr": 0.012715404841277752,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.75,
                    "acc_stderr": 0.026303648393696036,
                    "acc_norm": 0.75,
                    "acc_norm_stderr": 0.026303648393696036,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.7598039215686274,
                    "acc_stderr": 0.01728276069516741,
                    "acc_norm": 0.7598039215686274,
                    "acc_norm_stderr": 0.01728276069516741,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.7727272727272727,
                    "acc_stderr": 0.04013964554072775,
                    "acc_norm": 0.7727272727272727,
                    "acc_norm_stderr": 0.04013964554072775,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.7673469387755102,
                    "acc_stderr": 0.027049257915896175,
                    "acc_norm": 0.7673469387755102,
                    "acc_norm_stderr": 0.027049257915896175,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8756218905472637,
                    "acc_stderr": 0.023335401790166327,
                    "acc_norm": 0.8756218905472637,
                    "acc_norm_stderr": 0.023335401790166327,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.88,
                    "acc_stderr": 0.03265986323710906,
                    "acc_norm": 0.88,
                    "acc_norm_stderr": 0.03265986323710906,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5120481927710844,
                    "acc_stderr": 0.03891364495835817,
                    "acc_norm": 0.5120481927710844,
                    "acc_norm_stderr": 0.03891364495835817,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8713450292397661,
                    "acc_stderr": 0.02567934272327691,
                    "acc_norm": 0.8713450292397661,
                    "acc_norm_stderr": 0.02567934272327691,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.4675642594859241,
                    "mc1_stderr": 0.017466632149577613,
                    "mc2": 0.6577655722264159,
                    "mc2_stderr": 0.014903281756393213,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.8326756116811366,
                    "acc_stderr": 0.010490608806828079,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.6262319939347991,
                    "acc_stderr": 0.013326342860737007,
                    "timestamp": "2024-02-02T06-48-43.589029"
                }
            }
        }
    }
}