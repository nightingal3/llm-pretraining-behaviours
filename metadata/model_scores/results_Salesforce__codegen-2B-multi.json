{
    "model_name": "Salesforce/codegen-2B-multi",
    "last_updated": "2024-12-19 13:40:42.041483",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "exact_match": 0.016483516483516484,
                "exact_match_stderr": 0.005454029764766761,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "minerva_math_prealgebra": {
                "exact_match": 0.009184845005740528,
                "exact_match_stderr": 0.003234242725762958,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "minerva_math_num_theory": {
                "exact_match": 0.014814814814814815,
                "exact_match_stderr": 0.005203704987512652,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "minerva_math_intermediate_algebra": {
                "exact_match": 0.018826135105204873,
                "exact_match_stderr": 0.004525330498668474,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "minerva_math_geometry": {
                "exact_match": 0.010438413361169102,
                "exact_match_stderr": 0.004648627117184649,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "minerva_math_counting_and_prob": {
                "exact_match": 0.006329113924050633,
                "exact_match_stderr": 0.003646382041065055,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "minerva_math_algebra": {
                "exact_match": 0.016849199663016005,
                "exact_match_stderr": 0.0037372948497597118,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "fld_default": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "fld_star": {
                "exact_match": 0.0,
                "exact_match_stderr": 0.0,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_3da": {
                "acc": 0.0015,
                "acc_stderr": 0.0008655920660521547,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_3ds": {
                "acc": 0.003,
                "acc_stderr": 0.0012232122154646984,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_4da": {
                "acc": 0.0005,
                "acc_stderr": 0.0005000000000000151,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_2ds": {
                "acc": 0.0145,
                "acc_stderr": 0.002673658397142754,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_5ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_5da": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_1dc": {
                "acc": 0.021,
                "acc_stderr": 0.0032069677767574555,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_4ds": {
                "acc": 0.0,
                "acc_stderr": 0.0,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_2dm": {
                "acc": 0.017,
                "acc_stderr": 0.0028913110935905456,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "arithmetic_2da": {
                "acc": 0.0105,
                "acc_stderr": 0.002279796863070998,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "gsm8k_cot": {
                "exact_match": 0.024260803639120546,
                "exact_match_stderr": 0.004238007900001408,
                "timestamp": "2024-06-13T20-25-41.224859"
            },
            "gsm8k": {
                "exact_match": 0.025018953752843062,
                "exact_match_stderr": 0.00430204504656429,
                "timestamp": "2024-11-11T08-57-26.989242"
            },
            "anli_r2": {
                "brier_score": 0.9748658390993673,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "anli_r3": {
                "brier_score": 0.9159103324247587,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "anli_r1": {
                "brier_score": 1.0092709408874003,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_eu": {
                "brier_score": 1.1666516618470189,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_vi": {
                "brier_score": 1.2843093361039912,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_ru": {
                "brier_score": 0.8090664913187244,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_zh": {
                "brier_score": 0.904579300460954,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_tr": {
                "brier_score": 0.9205007916072985,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_fr": {
                "brier_score": 0.9220325735050247,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_en": {
                "brier_score": 0.7458236186385612,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_ur": {
                "brier_score": 1.3227919367050145,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_ar": {
                "brier_score": 0.908786661893419,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_de": {
                "brier_score": 0.9684063717218064,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_hi": {
                "brier_score": 1.071163113090109,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_es": {
                "brier_score": 1.0508250865994877,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_bg": {
                "brier_score": 1.0973174685369405,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_sw": {
                "brier_score": 1.0296151222717675,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_el": {
                "brier_score": 0.9165722128481477,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "xnli_th": {
                "brier_score": 1.0251103514516042,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "logiqa2": {
                "brier_score": 1.1763968660378155,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "mathqa": {
                "brier_score": 0.9772584388033301,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-06-13T20-34-28.631005"
            },
            "lambada_standard": {
                "perplexity": 81.4029030188419,
                "perplexity_stderr": 3.4255201511790587,
                "acc": 0.263729866097419,
                "acc_stderr": 0.006139179363569852,
                "timestamp": "2024-06-13T20-35-34.571703"
            },
            "lambada_openai": {
                "perplexity": 71.22943112687902,
                "perplexity_stderr": 3.023615035564398,
                "acc": 0.27440326023675526,
                "acc_stderr": 0.006216620663857009,
                "timestamp": "2024-06-13T20-35-34.571703"
            },
            "mmlu_world_religions": {
                "acc": 0.3216374269005848,
                "acc_stderr": 0.03582529442573122,
                "brier_score": 0.7546803865019402,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_formal_logic": {
                "acc": 0.23809523809523808,
                "acc_stderr": 0.03809523809523812,
                "brier_score": 0.7861060931204427,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_prehistory": {
                "acc": 0.23148148148148148,
                "acc_stderr": 0.023468429832451166,
                "brier_score": 0.7956158233024767,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_moral_scenarios": {
                "acc": 0.23798882681564246,
                "acc_stderr": 0.014242630070574885,
                "brier_score": 0.7758538855796485,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_world_history": {
                "acc": 0.23628691983122363,
                "acc_stderr": 0.027652153144159267,
                "brier_score": 0.7881333404455585,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_moral_disputes": {
                "acc": 0.27167630057803466,
                "acc_stderr": 0.02394851290546835,
                "brier_score": 0.770457570332024,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_professional_law": {
                "acc": 0.24119947848761408,
                "acc_stderr": 0.010926496102034952,
                "brier_score": 0.7875492624430832,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_logical_fallacies": {
                "acc": 0.27607361963190186,
                "acc_stderr": 0.0351238528370505,
                "brier_score": 0.7756305991459267,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_us_history": {
                "acc": 0.2549019607843137,
                "acc_stderr": 0.030587591351604233,
                "brier_score": 0.782571487009046,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_philosophy": {
                "acc": 0.21543408360128619,
                "acc_stderr": 0.02335022547547143,
                "brier_score": 0.7930833367764941,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_jurisprudence": {
                "acc": 0.2222222222222222,
                "acc_stderr": 0.040191074725573483,
                "brier_score": 0.7829344122884471,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_international_law": {
                "acc": 0.30578512396694213,
                "acc_stderr": 0.04205953933884122,
                "brier_score": 0.7516569535072102,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_european_history": {
                "acc": 0.20606060606060606,
                "acc_stderr": 0.03158415324047709,
                "brier_score": 0.8014641560807866,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_government_and_politics": {
                "acc": 0.23834196891191708,
                "acc_stderr": 0.03074890536390988,
                "brier_score": 0.8255799819870075,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_microeconomics": {
                "acc": 0.18487394957983194,
                "acc_stderr": 0.025215992877954202,
                "brier_score": 0.8322855849349483,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_geography": {
                "acc": 0.22727272727272727,
                "acc_stderr": 0.02985751567338641,
                "brier_score": 0.8086400814182112,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_psychology": {
                "acc": 0.22201834862385322,
                "acc_stderr": 0.01781884956479663,
                "brier_score": 0.8065734496672069,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_public_relations": {
                "acc": 0.2545454545454545,
                "acc_stderr": 0.04172343038705383,
                "brier_score": 0.7773867180018731,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_us_foreign_policy": {
                "acc": 0.21,
                "acc_stderr": 0.040936018074033256,
                "brier_score": 0.7970292919788169,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_sociology": {
                "acc": 0.24378109452736318,
                "acc_stderr": 0.030360490154014652,
                "brier_score": 0.8042877580813117,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_macroeconomics": {
                "acc": 0.19743589743589743,
                "acc_stderr": 0.020182646968674844,
                "brier_score": 0.8294006145780994,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_security_studies": {
                "acc": 0.17142857142857143,
                "acc_stderr": 0.024127463462650153,
                "brier_score": 0.8660242939475324,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_professional_psychology": {
                "acc": 0.272875816993464,
                "acc_stderr": 0.01802047414839358,
                "brier_score": 0.785573099759002,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_human_sexuality": {
                "acc": 0.1984732824427481,
                "acc_stderr": 0.03498149385462471,
                "brier_score": 0.8018306923112876,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_econometrics": {
                "acc": 0.19298245614035087,
                "acc_stderr": 0.037124548537213684,
                "brier_score": 0.80948299531357,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_miscellaneous": {
                "acc": 0.29118773946360155,
                "acc_stderr": 0.016246087069701404,
                "brier_score": 0.7720002992526842,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_marketing": {
                "acc": 0.2777777777777778,
                "acc_stderr": 0.029343114798094462,
                "brier_score": 0.7700657137319908,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_management": {
                "acc": 0.24271844660194175,
                "acc_stderr": 0.04245022486384495,
                "brier_score": 0.8236806321437848,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_nutrition": {
                "acc": 0.27450980392156865,
                "acc_stderr": 0.025553169991826528,
                "brier_score": 0.819694635782886,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_medical_genetics": {
                "acc": 0.26,
                "acc_stderr": 0.04408440022768076,
                "brier_score": 0.7874899117454538,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_human_aging": {
                "acc": 0.29596412556053814,
                "acc_stderr": 0.0306365913486998,
                "brier_score": 0.7483159620278041,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_professional_medicine": {
                "acc": 0.20220588235294118,
                "acc_stderr": 0.024398192986654924,
                "brier_score": 0.8288349039405379,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_college_medicine": {
                "acc": 0.23699421965317918,
                "acc_stderr": 0.03242414757483098,
                "brier_score": 0.8334233429462069,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_business_ethics": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7715835120318814,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_clinical_knowledge": {
                "acc": 0.2528301886792453,
                "acc_stderr": 0.02674989977124124,
                "brier_score": 0.8192907779010937,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_global_facts": {
                "acc": 0.25,
                "acc_stderr": 0.04351941398892446,
                "brier_score": 0.7677745198062343,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_virology": {
                "acc": 0.23493975903614459,
                "acc_stderr": 0.03300533186128922,
                "brier_score": 0.7896623984697541,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_professional_accounting": {
                "acc": 0.23404255319148937,
                "acc_stderr": 0.025257861359432403,
                "brier_score": 0.8035852632894487,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_college_physics": {
                "acc": 0.11764705882352941,
                "acc_stderr": 0.03205907733144527,
                "brier_score": 0.7963115355798652,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_physics": {
                "acc": 0.26490066225165565,
                "acc_stderr": 0.036030385453603826,
                "brier_score": 0.7843686953032997,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_biology": {
                "acc": 0.2645161290322581,
                "acc_stderr": 0.025091892378859275,
                "brier_score": 0.7824577489394099,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_college_biology": {
                "acc": 0.2361111111111111,
                "acc_stderr": 0.03551446610810826,
                "brier_score": 0.7743808593997604,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_anatomy": {
                "acc": 0.2962962962962963,
                "acc_stderr": 0.03944624162501116,
                "brier_score": 0.7409161817727408,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_college_chemistry": {
                "acc": 0.18,
                "acc_stderr": 0.038612291966536955,
                "brier_score": 0.8176881871196044,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_computer_security": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7562892819901279,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_college_computer_science": {
                "acc": 0.29,
                "acc_stderr": 0.045604802157206845,
                "brier_score": 0.7794711889789567,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_astronomy": {
                "acc": 0.23684210526315788,
                "acc_stderr": 0.03459777606810537,
                "brier_score": 0.7763864267043885,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_college_mathematics": {
                "acc": 0.29,
                "acc_stderr": 0.04560480215720684,
                "brier_score": 0.7686642085054471,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_conceptual_physics": {
                "acc": 0.23404255319148937,
                "acc_stderr": 0.0276784525782124,
                "brier_score": 0.7660169929396504,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_abstract_algebra": {
                "acc": 0.26,
                "acc_stderr": 0.0440844002276808,
                "brier_score": 0.7865734465162898,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_computer_science": {
                "acc": 0.38,
                "acc_stderr": 0.048783173121456316,
                "brier_score": 0.7439680983789182,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_machine_learning": {
                "acc": 0.3392857142857143,
                "acc_stderr": 0.04493949068613539,
                "brier_score": 0.7405766796738416,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_chemistry": {
                "acc": 0.2019704433497537,
                "acc_stderr": 0.02824735012218027,
                "brier_score": 0.7965945037147942,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_statistics": {
                "acc": 0.1712962962962963,
                "acc_stderr": 0.025695341643824688,
                "brier_score": 0.8460945729720145,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_elementary_mathematics": {
                "acc": 0.2671957671957672,
                "acc_stderr": 0.02278967314577656,
                "brier_score": 0.8122475508499548,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_electrical_engineering": {
                "acc": 0.3103448275862069,
                "acc_stderr": 0.03855289616378949,
                "brier_score": 0.7545684510084733,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "mmlu_high_school_mathematics": {
                "acc": 0.25555555555555554,
                "acc_stderr": 0.02659393910184408,
                "brier_score": 0.8056289075921087,
                "brier_score_stderr": "N/A",
                "timestamp": "2024-12-15T20-11-53.465958"
            },
            "arc_challenge": {
                "acc": 0.22184300341296928,
                "acc_stderr": 0.012141659068147884,
                "acc_norm": 0.257679180887372,
                "acc_norm_stderr": 0.012780770562768393,
                "timestamp": "2024-11-11T08-57-26.989242"
            },
            "hellaswag": {
                "acc": 0.31428002389962156,
                "acc_stderr": 0.004632797375289776,
                "acc_norm": 0.36666002788289187,
                "acc_norm_stderr": 0.004809077205343483,
                "timestamp": "2024-11-11T08-57-26.989242"
            },
            "truthfulqa_mc2": {
                "acc": 0.4459179118660402,
                "acc_stderr": 0.01522157614487195,
                "timestamp": "2024-11-11T08-57-26.989242"
            },
            "truthfulqa_gen": {
                "bleu_max": 12.757546154780197,
                "bleu_max_stderr": 0.4498040933988642,
                "bleu_acc": 0.379436964504284,
                "bleu_acc_stderr": 0.016987039266142954,
                "bleu_diff": -1.8337630237130322,
                "bleu_diff_stderr": 0.4401683091426479,
                "rouge1_max": 30.34079217924218,
                "rouge1_max_stderr": 0.759556986026027,
                "rouge1_acc": 0.2766217870257038,
                "rouge1_acc_stderr": 0.015659605755326936,
                "rouge1_diff": -7.075553576988576,
                "rouge1_diff_stderr": 0.7258364783365392,
                "rouge2_max": 13.032258561545998,
                "rouge2_max_stderr": 0.7083547799321905,
                "rouge2_acc": 0.1346389228886169,
                "rouge2_acc_stderr": 0.011949202293705486,
                "rouge2_diff": -6.167131616599388,
                "rouge2_diff_stderr": 0.6346861400524839,
                "rougeL_max": 27.669505162919336,
                "rougeL_max_stderr": 0.7249567476362944,
                "rougeL_acc": 0.27906976744186046,
                "rougeL_acc_stderr": 0.015702107090627915,
                "rougeL_diff": -6.591009170287162,
                "rougeL_diff_stderr": 0.7117399236362356,
                "timestamp": "2024-11-11T08-57-26.989242"
            },
            "truthfulqa_mc1": {
                "acc": 0.25458996328029376,
                "acc_stderr": 0.015250117079156496,
                "timestamp": "2024-11-11T08-57-26.989242"
            },
            "winogrande": {
                "acc": 0.5501183898973955,
                "acc_stderr": 0.013981711904049735,
                "timestamp": "2024-11-11T08-57-26.989242"
            }
        }
    }
}