{
    "model_name": "Salesforce/codegen-2B-multi",
    "last_updated": "2024-12-04 11:24:46.619250",
    "results": {
        "harness": {
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.016483516483516484,
                    "acc_stderr": 0.005454029764766761,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.009184845005740528,
                    "acc_stderr": 0.003234242725762958,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.014814814814814815,
                    "acc_stderr": 0.005203704987512652,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.018826135105204873,
                    "acc_stderr": 0.004525330498668474,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.010438413361169102,
                    "acc_stderr": 0.004648627117184649,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.006329113924050633,
                    "acc_stderr": 0.003646382041065055,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.016849199663016005,
                    "acc_stderr": 0.0037372948497597118,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.0015,
                    "acc_stderr": 0.0008655920660521547,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.003,
                    "acc_stderr": 0.0012232122154646984,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.0005,
                    "acc_stderr": 0.0005000000000000151,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.0145,
                    "acc_stderr": 0.002673658397142754,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.021,
                    "acc_stderr": 0.0032069677767574555,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.017,
                    "acc_stderr": 0.0028913110935905456,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 0.0105,
                    "acc_stderr": 0.002279796863070998,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.024260803639120546,
                    "acc_stderr": 0.004238007900001408,
                    "timestamp": "2024-06-13T20-25-41.224859"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.025018953752843062,
                    "acc_stderr": 0.00430204504656429,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "anli_r2": {
                "0-shot": {
                    "brier_score": 0.9748658390993673,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "anli_r3": {
                "0-shot": {
                    "brier_score": 0.9159103324247587,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "anli_r1": {
                "0-shot": {
                    "brier_score": 1.0092709408874003,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_eu": {
                "0-shot": {
                    "brier_score": 1.1666516618470189,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_vi": {
                "0-shot": {
                    "brier_score": 1.2843093361039912,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_ru": {
                "0-shot": {
                    "brier_score": 0.8090664913187244,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_zh": {
                "0-shot": {
                    "brier_score": 0.904579300460954,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_tr": {
                "0-shot": {
                    "brier_score": 0.9205007916072985,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_fr": {
                "0-shot": {
                    "brier_score": 0.9220325735050247,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_en": {
                "0-shot": {
                    "brier_score": 0.7458236186385612,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_ur": {
                "0-shot": {
                    "brier_score": 1.3227919367050145,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_ar": {
                "0-shot": {
                    "brier_score": 0.908786661893419,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_de": {
                "0-shot": {
                    "brier_score": 0.9684063717218064,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_hi": {
                "0-shot": {
                    "brier_score": 1.071163113090109,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_es": {
                "0-shot": {
                    "brier_score": 1.0508250865994877,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_bg": {
                "0-shot": {
                    "brier_score": 1.0973174685369405,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_sw": {
                "0-shot": {
                    "brier_score": 1.0296151222717675,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_el": {
                "0-shot": {
                    "brier_score": 0.9165722128481477,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "xnli_th": {
                "0-shot": {
                    "brier_score": 1.0251103514516042,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "brier_score": 1.1763968660378155,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "mathqa": {
                "5-shot": {
                    "brier_score": 0.9772584388033301,
                    "timestamp": "2024-06-13T20-34-28.631005"
                }
            },
            "lambada_standard": {
                "0-shot": {
                    "perplexity": 81.4029030188419,
                    "perplexity_stderr": 3.4255201511790587,
                    "acc": 0.263729866097419,
                    "acc_stderr": 0.006139179363569852,
                    "timestamp": "2024-06-13T20-35-34.571703"
                }
            },
            "lambada_openai": {
                "0-shot": {
                    "perplexity": 71.22943112687902,
                    "perplexity_stderr": 3.023615035564398,
                    "acc": 0.27440326023675526,
                    "acc_stderr": 0.006216620663857009,
                    "timestamp": "2024-06-13T20-35-34.571703"
                }
            },
            "mmlu_world_religions": {
                "0-shot": {
                    "acc": 0.23391812865497075,
                    "acc_stderr": 0.03246721765117826,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_formal_logic": {
                "0-shot": {
                    "acc": 0.12698412698412698,
                    "acc_stderr": 0.029780417522688434,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_prehistory": {
                "0-shot": {
                    "acc": 0.23148148148148148,
                    "acc_stderr": 0.023468429832451166,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_moral_scenarios": {
                "0-shot": {
                    "acc": 0.2424581005586592,
                    "acc_stderr": 0.014333522059217887,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_world_history": {
                "0-shot": {
                    "acc": 0.2109704641350211,
                    "acc_stderr": 0.02655837250266192,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_moral_disputes": {
                "0-shot": {
                    "acc": 0.2543352601156069,
                    "acc_stderr": 0.02344582627654555,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_professional_law": {
                "0-shot": {
                    "acc": 0.24902216427640156,
                    "acc_stderr": 0.01104489226404077,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_logical_fallacies": {
                "0-shot": {
                    "acc": 0.26993865030674846,
                    "acc_stderr": 0.03487825168497892,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_us_history": {
                "0-shot": {
                    "acc": 0.24509803921568626,
                    "acc_stderr": 0.030190282453501954,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_philosophy": {
                "0-shot": {
                    "acc": 0.3022508038585209,
                    "acc_stderr": 0.02608270069539966,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_jurisprudence": {
                "0-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.04236511258094635,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_international_law": {
                "0-shot": {
                    "acc": 0.35537190082644626,
                    "acc_stderr": 0.0436923632657398,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_european_history": {
                "0-shot": {
                    "acc": 0.23030303030303031,
                    "acc_stderr": 0.032876667586034886,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_government_and_politics": {
                "0-shot": {
                    "acc": 0.34196891191709844,
                    "acc_stderr": 0.03423465100104284,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_microeconomics": {
                "0-shot": {
                    "acc": 0.21008403361344538,
                    "acc_stderr": 0.026461398717471874,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_geography": {
                "0-shot": {
                    "acc": 0.23232323232323232,
                    "acc_stderr": 0.030088629490217487,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_psychology": {
                "0-shot": {
                    "acc": 0.21467889908256882,
                    "acc_stderr": 0.01760430414925649,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_public_relations": {
                "0-shot": {
                    "acc": 0.22727272727272727,
                    "acc_stderr": 0.04013964554072774,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_us_foreign_policy": {
                "0-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_sociology": {
                "0-shot": {
                    "acc": 0.22885572139303484,
                    "acc_stderr": 0.02970528405677245,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_macroeconomics": {
                "0-shot": {
                    "acc": 0.26666666666666666,
                    "acc_stderr": 0.022421273612923707,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_security_studies": {
                "0-shot": {
                    "acc": 0.24081632653061225,
                    "acc_stderr": 0.027372942201788163,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_professional_psychology": {
                "0-shot": {
                    "acc": 0.26143790849673204,
                    "acc_stderr": 0.017776947157528037,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_human_sexuality": {
                "0-shot": {
                    "acc": 0.20610687022900764,
                    "acc_stderr": 0.03547771004159463,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_econometrics": {
                "0-shot": {
                    "acc": 0.2543859649122807,
                    "acc_stderr": 0.04096985139843671,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_miscellaneous": {
                "0-shot": {
                    "acc": 0.27330779054916987,
                    "acc_stderr": 0.015936681062628556,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_marketing": {
                "0-shot": {
                    "acc": 0.31196581196581197,
                    "acc_stderr": 0.030351527323344944,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_management": {
                "0-shot": {
                    "acc": 0.1262135922330097,
                    "acc_stderr": 0.03288180278808628,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_nutrition": {
                "0-shot": {
                    "acc": 0.238562091503268,
                    "acc_stderr": 0.02440439492808787,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_medical_genetics": {
                "0-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816505,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_human_aging": {
                "0-shot": {
                    "acc": 0.32286995515695066,
                    "acc_stderr": 0.03138147637575498,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_professional_medicine": {
                "0-shot": {
                    "acc": 0.35661764705882354,
                    "acc_stderr": 0.029097209568411955,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_college_medicine": {
                "0-shot": {
                    "acc": 0.23699421965317918,
                    "acc_stderr": 0.03242414757483098,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_business_ethics": {
                "0-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_clinical_knowledge": {
                "0-shot": {
                    "acc": 0.25660377358490566,
                    "acc_stderr": 0.026880647889051982,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_global_facts": {
                "0-shot": {
                    "acc": 0.32,
                    "acc_stderr": 0.04688261722621505,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_virology": {
                "0-shot": {
                    "acc": 0.29518072289156627,
                    "acc_stderr": 0.035509201856896294,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_professional_accounting": {
                "0-shot": {
                    "acc": 0.2765957446808511,
                    "acc_stderr": 0.02668456434046098,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_college_physics": {
                "0-shot": {
                    "acc": 0.21568627450980393,
                    "acc_stderr": 0.040925639582376556,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_physics": {
                "0-shot": {
                    "acc": 0.26490066225165565,
                    "acc_stderr": 0.036030385453603826,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_biology": {
                "0-shot": {
                    "acc": 0.3258064516129032,
                    "acc_stderr": 0.026662010578567104,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_college_biology": {
                "0-shot": {
                    "acc": 0.2638888888888889,
                    "acc_stderr": 0.03685651095897532,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_anatomy": {
                "0-shot": {
                    "acc": 0.3037037037037037,
                    "acc_stderr": 0.039725528847851375,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_college_chemistry": {
                "0-shot": {
                    "acc": 0.13,
                    "acc_stderr": 0.03379976689896309,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_computer_security": {
                "0-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816507,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_college_computer_science": {
                "0-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.048523658709391,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_astronomy": {
                "0-shot": {
                    "acc": 0.19078947368421054,
                    "acc_stderr": 0.031975658210325,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_college_mathematics": {
                "0-shot": {
                    "acc": 0.23,
                    "acc_stderr": 0.04229525846816506,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_conceptual_physics": {
                "0-shot": {
                    "acc": 0.2553191489361702,
                    "acc_stderr": 0.028504856470514203,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_abstract_algebra": {
                "0-shot": {
                    "acc": 0.21,
                    "acc_stderr": 0.040936018074033256,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_computer_science": {
                "0-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252605,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_machine_learning": {
                "0-shot": {
                    "acc": 0.22321428571428573,
                    "acc_stderr": 0.039523019677025116,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_chemistry": {
                "0-shot": {
                    "acc": 0.22167487684729065,
                    "acc_stderr": 0.029225575892489614,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_statistics": {
                "0-shot": {
                    "acc": 0.44907407407407407,
                    "acc_stderr": 0.03392238405321616,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_elementary_mathematics": {
                "0-shot": {
                    "acc": 0.2619047619047619,
                    "acc_stderr": 0.022644212615525218,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_electrical_engineering": {
                "0-shot": {
                    "acc": 0.2827586206896552,
                    "acc_stderr": 0.03752833958003336,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "mmlu_high_school_mathematics": {
                "0-shot": {
                    "acc": 0.27037037037037037,
                    "acc_stderr": 0.027080372815145665,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "arc_challenge": {
                "25-shot": {
                    "acc": 0.22184300341296928,
                    "acc_stderr": 0.012141659068147884,
                    "acc_norm": 0.257679180887372,
                    "acc_norm_stderr": 0.012780770562768393,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.31428002389962156,
                    "acc_stderr": 0.004632797375289776,
                    "acc_norm": 0.36666002788289187,
                    "acc_norm_stderr": 0.004809077205343483,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "truthfulqa_mc2": {
                "0-shot": {
                    "acc": 0.4459179118660402,
                    "acc_stderr": 0.01522157614487195,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "truthfulqa_gen": {
                "0-shot": {
                    "bleu_max": 12.757546154780197,
                    "bleu_max_stderr": 0.4498040933988642,
                    "bleu_acc": 0.379436964504284,
                    "bleu_acc_stderr": 0.016987039266142954,
                    "bleu_diff": -1.8337630237130322,
                    "bleu_diff_stderr": 0.4401683091426479,
                    "rouge1_max": 30.34079217924218,
                    "rouge1_max_stderr": 0.759556986026027,
                    "rouge1_acc": 0.2766217870257038,
                    "rouge1_acc_stderr": 0.015659605755326936,
                    "rouge1_diff": -7.075553576988576,
                    "rouge1_diff_stderr": 0.7258364783365392,
                    "rouge2_max": 13.032258561545998,
                    "rouge2_max_stderr": 0.7083547799321905,
                    "rouge2_acc": 0.1346389228886169,
                    "rouge2_acc_stderr": 0.011949202293705486,
                    "rouge2_diff": -6.167131616599388,
                    "rouge2_diff_stderr": 0.6346861400524839,
                    "rougeL_max": 27.669505162919336,
                    "rougeL_max_stderr": 0.7249567476362944,
                    "rougeL_acc": 0.27906976744186046,
                    "rougeL_acc_stderr": 0.015702107090627915,
                    "rougeL_diff": -6.591009170287162,
                    "rougeL_diff_stderr": 0.7117399236362356,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "truthfulqa_mc1": {
                "0-shot": {
                    "acc": 0.25458996328029376,
                    "acc_stderr": 0.015250117079156496,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.5501183898973955,
                    "acc_stderr": 0.013981711904049735,
                    "timestamp": "2024-11-11T08-57-26.989242"
                }
            }
        }
    }
}