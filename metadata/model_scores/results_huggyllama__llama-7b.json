{
    "model_name": "huggyllama/llama-7b",
    "last_updated": "2024-06-25 14:40:01.205588",
    "results": {
        "harness": {
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.47696245733788395,
                    "acc_stderr": 0.014595873205358267,
                    "acc_norm": 0.5093856655290102,
                    "acc_norm_stderr": 0.014608816322065,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.5753833897629954,
                    "acc_stderr": 0.004932745013072713,
                    "acc_norm": 0.7781318462457678,
                    "acc_norm_stderr": 0.004146537488135709,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.26,
                    "acc_stderr": 0.04408440022768081,
                    "acc_norm": 0.26,
                    "acc_norm_stderr": 0.04408440022768081,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.3851851851851852,
                    "acc_stderr": 0.042039210401562783,
                    "acc_norm": 0.3851851851851852,
                    "acc_norm_stderr": 0.042039210401562783,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.34210526315789475,
                    "acc_stderr": 0.03860731599316092,
                    "acc_norm": 0.34210526315789475,
                    "acc_norm_stderr": 0.03860731599316092,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.41,
                    "acc_stderr": 0.049431107042371025,
                    "acc_norm": 0.41,
                    "acc_norm_stderr": 0.049431107042371025,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.35471698113207545,
                    "acc_stderr": 0.02944517532819959,
                    "acc_norm": 0.35471698113207545,
                    "acc_norm_stderr": 0.02944517532819959,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.375,
                    "acc_stderr": 0.04048439222695598,
                    "acc_norm": 0.375,
                    "acc_norm_stderr": 0.04048439222695598,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.3,
                    "acc_stderr": 0.046056618647183814,
                    "acc_norm": 0.3,
                    "acc_norm_stderr": 0.046056618647183814,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252604,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252604,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.3352601156069364,
                    "acc_stderr": 0.03599586301247078,
                    "acc_norm": 0.3352601156069364,
                    "acc_norm_stderr": 0.03599586301247078,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.23529411764705882,
                    "acc_stderr": 0.04220773659171451,
                    "acc_norm": 0.23529411764705882,
                    "acc_norm_stderr": 0.04220773659171451,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.45,
                    "acc_stderr": 0.05,
                    "acc_norm": 0.45,
                    "acc_norm_stderr": 0.05,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.3702127659574468,
                    "acc_stderr": 0.03156564682236785,
                    "acc_norm": 0.3702127659574468,
                    "acc_norm_stderr": 0.03156564682236785,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.2631578947368421,
                    "acc_stderr": 0.04142439719489362,
                    "acc_norm": 0.2631578947368421,
                    "acc_norm_stderr": 0.04142439719489362,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.22758620689655173,
                    "acc_stderr": 0.03493950380131184,
                    "acc_norm": 0.22758620689655173,
                    "acc_norm_stderr": 0.03493950380131184,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.2619047619047619,
                    "acc_stderr": 0.022644212615525214,
                    "acc_norm": 0.2619047619047619,
                    "acc_norm_stderr": 0.022644212615525214,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.2619047619047619,
                    "acc_stderr": 0.03932537680392871,
                    "acc_norm": 0.2619047619047619,
                    "acc_norm_stderr": 0.03932537680392871,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.31,
                    "acc_stderr": 0.04648231987117316,
                    "acc_norm": 0.31,
                    "acc_norm_stderr": 0.04648231987117316,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.33225806451612905,
                    "acc_stderr": 0.0267955608481228,
                    "acc_norm": 0.33225806451612905,
                    "acc_norm_stderr": 0.0267955608481228,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.30049261083743845,
                    "acc_stderr": 0.03225799476233485,
                    "acc_norm": 0.30049261083743845,
                    "acc_norm_stderr": 0.03225799476233485,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.33,
                    "acc_stderr": 0.04725815626252605,
                    "acc_norm": 0.33,
                    "acc_norm_stderr": 0.04725815626252605,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.43636363636363634,
                    "acc_stderr": 0.03872592983524754,
                    "acc_norm": 0.43636363636363634,
                    "acc_norm_stderr": 0.03872592983524754,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.3333333333333333,
                    "acc_stderr": 0.03358618145732522,
                    "acc_norm": 0.3333333333333333,
                    "acc_norm_stderr": 0.03358618145732522,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.44559585492227977,
                    "acc_stderr": 0.0358701498607566,
                    "acc_norm": 0.44559585492227977,
                    "acc_norm_stderr": 0.0358701498607566,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.3435897435897436,
                    "acc_stderr": 0.024078696580635477,
                    "acc_norm": 0.3435897435897436,
                    "acc_norm_stderr": 0.024078696580635477,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.25925925925925924,
                    "acc_stderr": 0.026719240783712173,
                    "acc_norm": 0.25925925925925924,
                    "acc_norm_stderr": 0.026719240783712173,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.33613445378151263,
                    "acc_stderr": 0.030684737115135363,
                    "acc_norm": 0.33613445378151263,
                    "acc_norm_stderr": 0.030684737115135363,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.26490066225165565,
                    "acc_stderr": 0.036030385453603854,
                    "acc_norm": 0.26490066225165565,
                    "acc_norm_stderr": 0.036030385453603854,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.47339449541284406,
                    "acc_stderr": 0.02140695268815159,
                    "acc_norm": 0.47339449541284406,
                    "acc_norm_stderr": 0.02140695268815159,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.3055555555555556,
                    "acc_stderr": 0.031415546294025445,
                    "acc_norm": 0.3055555555555556,
                    "acc_norm_stderr": 0.031415546294025445,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.35784313725490197,
                    "acc_stderr": 0.03364487286088299,
                    "acc_norm": 0.35784313725490197,
                    "acc_norm_stderr": 0.03364487286088299,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.43037974683544306,
                    "acc_stderr": 0.03223017195937598,
                    "acc_norm": 0.43037974683544306,
                    "acc_norm_stderr": 0.03223017195937598,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.39461883408071746,
                    "acc_stderr": 0.03280400504755291,
                    "acc_norm": 0.39461883408071746,
                    "acc_norm_stderr": 0.03280400504755291,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.3511450381679389,
                    "acc_stderr": 0.0418644516301375,
                    "acc_norm": 0.3511450381679389,
                    "acc_norm_stderr": 0.0418644516301375,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.5206611570247934,
                    "acc_stderr": 0.04560456086387235,
                    "acc_norm": 0.5206611570247934,
                    "acc_norm_stderr": 0.04560456086387235,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.4166666666666667,
                    "acc_stderr": 0.04766075165356461,
                    "acc_norm": 0.4166666666666667,
                    "acc_norm_stderr": 0.04766075165356461,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.4294478527607362,
                    "acc_stderr": 0.038890666191127216,
                    "acc_norm": 0.4294478527607362,
                    "acc_norm_stderr": 0.038890666191127216,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.2767857142857143,
                    "acc_stderr": 0.042466243366976256,
                    "acc_norm": 0.2767857142857143,
                    "acc_norm_stderr": 0.042466243366976256,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.3300970873786408,
                    "acc_stderr": 0.046561471100123514,
                    "acc_norm": 0.3300970873786408,
                    "acc_norm_stderr": 0.046561471100123514,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.47863247863247865,
                    "acc_stderr": 0.03272616447634954,
                    "acc_norm": 0.47863247863247865,
                    "acc_norm_stderr": 0.03272616447634954,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.048523658709391,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.048523658709391,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.42528735632183906,
                    "acc_stderr": 0.017679225489431447,
                    "acc_norm": 0.42528735632183906,
                    "acc_norm_stderr": 0.017679225489431447,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.3959537572254335,
                    "acc_stderr": 0.02632981334194625,
                    "acc_norm": 0.3959537572254335,
                    "acc_norm_stderr": 0.02632981334194625,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.2424581005586592,
                    "acc_stderr": 0.014333522059217889,
                    "acc_norm": 0.2424581005586592,
                    "acc_norm_stderr": 0.014333522059217889,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.3888888888888889,
                    "acc_stderr": 0.02791405551046801,
                    "acc_norm": 0.3888888888888889,
                    "acc_norm_stderr": 0.02791405551046801,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.3954983922829582,
                    "acc_stderr": 0.027770918531427834,
                    "acc_norm": 0.3954983922829582,
                    "acc_norm_stderr": 0.027770918531427834,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.345679012345679,
                    "acc_stderr": 0.026462487777001883,
                    "acc_norm": 0.345679012345679,
                    "acc_norm_stderr": 0.026462487777001883,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.2695035460992908,
                    "acc_stderr": 0.026469036818590624,
                    "acc_norm": 0.2695035460992908,
                    "acc_norm_stderr": 0.026469036818590624,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.29595827900912647,
                    "acc_stderr": 0.011658518525277054,
                    "acc_norm": 0.29595827900912647,
                    "acc_norm_stderr": 0.011658518525277054,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.4411764705882353,
                    "acc_stderr": 0.030161911930767102,
                    "acc_norm": 0.4411764705882353,
                    "acc_norm_stderr": 0.030161911930767102,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.35784313725490197,
                    "acc_stderr": 0.019393058402355442,
                    "acc_norm": 0.35784313725490197,
                    "acc_norm_stderr": 0.019393058402355442,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.41818181818181815,
                    "acc_stderr": 0.0472457740573157,
                    "acc_norm": 0.41818181818181815,
                    "acc_norm_stderr": 0.0472457740573157,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.34285714285714286,
                    "acc_stderr": 0.030387262919547728,
                    "acc_norm": 0.34285714285714286,
                    "acc_norm_stderr": 0.030387262919547728,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.46766169154228854,
                    "acc_stderr": 0.035281314729336065,
                    "acc_norm": 0.46766169154228854,
                    "acc_norm_stderr": 0.035281314729336065,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.43,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.43,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.3433734939759036,
                    "acc_stderr": 0.03696584317010601,
                    "acc_norm": 0.3433734939759036,
                    "acc_norm_stderr": 0.03696584317010601,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.4853801169590643,
                    "acc_stderr": 0.038331852752130205,
                    "acc_norm": 0.4853801169590643,
                    "acc_norm_stderr": 0.038331852752130205,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2215422276621787,
                    "mc1_stderr": 0.014537867601301137,
                    "mc2": 0.3432793294414406,
                    "mc2_stderr": 0.01318846106276968,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7142857142857143,
                    "acc_stderr": 0.012696531870038616,
                    "timestamp": "2023-12-04T17-40-08.047341"
                }
            },
            "gsm8k": {
                "5-shot": {
                    "acc": 0.08718726307808947,
                    "acc_stderr": 0.007770691416783579,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "minerva_math_precalc": {
                "5-shot": {
                    "acc": 0.023809523809523808,
                    "acc_stderr": 0.006530469219761487,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "minerva_math_prealgebra": {
                "5-shot": {
                    "acc": 0.05855338691159587,
                    "acc_stderr": 0.007960021356233738,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "minerva_math_num_theory": {
                "5-shot": {
                    "acc": 0.016666666666666666,
                    "acc_stderr": 0.005514172815089617,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "minerva_math_intermediate_algebra": {
                "5-shot": {
                    "acc": 0.02768549280177187,
                    "acc_stderr": 0.005462937642242674,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "minerva_math_geometry": {
                "5-shot": {
                    "acc": 0.03549060542797495,
                    "acc_stderr": 0.008462447487439958,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "minerva_math_counting_and_prob": {
                "5-shot": {
                    "acc": 0.03375527426160337,
                    "acc_stderr": 0.008303932653126971,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "minerva_math_algebra": {
                "5-shot": {
                    "acc": 0.02948609941027801,
                    "acc_stderr": 0.0049120999853740025,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "fld_default": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "fld_star": {
                "0-shot": {
                    "acc": 0.0,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_3da": {
                "5-shot": {
                    "acc": 0.671,
                    "acc_stderr": 0.010508792383460743,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_3ds": {
                "5-shot": {
                    "acc": 0.4205,
                    "acc_stderr": 0.011040870681821411,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_4da": {
                "5-shot": {
                    "acc": 0.3535,
                    "acc_stderr": 0.010692335480100337,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_2ds": {
                "5-shot": {
                    "acc": 0.4915,
                    "acc_stderr": 0.011181519941139164,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_5ds": {
                "5-shot": {
                    "acc": 0.1135,
                    "acc_stderr": 0.007094648829999192,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_5da": {
                "5-shot": {
                    "acc": 0.105,
                    "acc_stderr": 0.0068564572122015,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_1dc": {
                "5-shot": {
                    "acc": 0.17,
                    "acc_stderr": 0.008401505379771036,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_4ds": {
                "5-shot": {
                    "acc": 0.3045,
                    "acc_stderr": 0.01029285414368695,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_2dm": {
                "5-shot": {
                    "acc": 0.1865,
                    "acc_stderr": 0.008711878294128388,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "arithmetic_2da": {
                "5-shot": {
                    "acc": 0.7645,
                    "acc_stderr": 0.009490253287121995,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "gsm8k_cot": {
                "5-shot": {
                    "acc": 0.10993176648976498,
                    "acc_stderr": 0.008616195587865418,
                    "timestamp": "2024-06-14T09-56-19.523695"
                }
            },
            "anli_r2": {
                "0-shot": {
                    "brier_score": 0.7348861281471742,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "anli_r3": {
                "0-shot": {
                    "brier_score": 0.7095518208768032,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "anli_r1": {
                "0-shot": {
                    "brier_score": 0.7461478989427204,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_eu": {
                "0-shot": {
                    "brier_score": 1.0296807787278208,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_vi": {
                "0-shot": {
                    "brier_score": 0.9782764908278533,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_ru": {
                "0-shot": {
                    "brier_score": 0.8046437633798106,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_zh": {
                "0-shot": {
                    "brier_score": 0.9663395791429601,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_tr": {
                "0-shot": {
                    "brier_score": 0.8179342178015766,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_fr": {
                "0-shot": {
                    "brier_score": 0.7545951761984023,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_en": {
                "0-shot": {
                    "brier_score": 0.6399015782286974,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_ur": {
                "0-shot": {
                    "brier_score": 1.18515651443942,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_ar": {
                "0-shot": {
                    "brier_score": 1.2101674335547863,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_de": {
                "0-shot": {
                    "brier_score": 0.7906547361490429,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_hi": {
                "0-shot": {
                    "brier_score": 0.8598989915709084,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_es": {
                "0-shot": {
                    "brier_score": 0.8477390395566538,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_bg": {
                "0-shot": {
                    "brier_score": 0.8810810022273962,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_sw": {
                "0-shot": {
                    "brier_score": 1.0933436482571928,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_el": {
                "0-shot": {
                    "brier_score": 0.8910593589743593,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "xnli_th": {
                "0-shot": {
                    "brier_score": 1.0236076918742028,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "logiqa2": {
                "0-shot": {
                    "brier_score": 0.9998570403139664,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "mathqa": {
                "5-shot": {
                    "brier_score": 0.9133227108274022,
                    "timestamp": "2024-06-07T04-05-04.754622"
                }
            },
            "lambada_standard": {
                "0-shot": {
                    "perplexity": 4.405370129844651,
                    "perplexity_stderr": 0.08983307496604127,
                    "acc": 0.6780516204152921,
                    "acc_stderr": 0.0065093342627460075,
                    "timestamp": "2024-06-14T10-08-00.794605"
                }
            },
            "lambada_openai": {
                "0-shot": {
                    "perplexity": 3.488115332748822,
                    "perplexity_stderr": 0.06835244562396407,
                    "acc": 0.7351057636328352,
                    "acc_stderr": 0.006147849695828234,
                    "timestamp": "2024-06-14T10-08-00.794605"
                }
            }
        }
    }
}