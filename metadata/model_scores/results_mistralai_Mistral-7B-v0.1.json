{
    "model_name": "mistralai/Mistral-7B-v0.1",
    "last_updated": "2023-10-26",
    "results": {
        "harness": {
            "gsm8k": {
                "5-shot": {
                    "acc": 0.3707354056103108,
                    "acc_stderr": 0.013304267705458433,
                    "timestamp": "2023-12-02T13-02-14.153054"
                }
            },
            "drop": {
                "3-shot": {
                    "acc": 0.001572986577181208,
                    "acc_stderr": 0.00040584511324177333,
                    "f1": 0.06143666107382555,
                    "f1_stderr": 0.0013713061256604275,
                    "timestamp": "2023-10-26T01-29-53.089924"
                }
            },
            "winogrande": {
                "5-shot": {
                    "acc": 0.7861089187056038,
                    "acc_stderr": 0.011524466954090254,
                    "timestamp": "2023-10-26T01-29-53.089924"
                }
            },
            "arc:challenge": {
                "25-shot": {
                    "acc": 0.568259385665529,
                    "acc_stderr": 0.014474591427196202,
                    "acc_norm": 0.5998293515358362,
                    "acc_norm_stderr": 0.014317197787809172,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hellaswag": {
                "10-shot": {
                    "acc": 0.6294562836088429,
                    "acc_stderr": 0.00481963366883254,
                    "acc_norm": 0.8331009759012149,
                    "acc_norm_stderr": 0.0037212361965025162,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-abstract_algebra": {
                "5-shot": {
                    "acc": 0.29,
                    "acc_stderr": 0.045604802157206845,
                    "acc_norm": 0.29,
                    "acc_norm_stderr": 0.045604802157206845,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-anatomy": {
                "5-shot": {
                    "acc": 0.6296296296296297,
                    "acc_stderr": 0.041716541613545426,
                    "acc_norm": 0.6296296296296297,
                    "acc_norm_stderr": 0.041716541613545426,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-astronomy": {
                "5-shot": {
                    "acc": 0.6578947368421053,
                    "acc_stderr": 0.03860731599316091,
                    "acc_norm": 0.6578947368421053,
                    "acc_norm_stderr": 0.03860731599316091,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-business_ethics": {
                "5-shot": {
                    "acc": 0.57,
                    "acc_stderr": 0.049756985195624284,
                    "acc_norm": 0.57,
                    "acc_norm_stderr": 0.049756985195624284,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-clinical_knowledge": {
                "5-shot": {
                    "acc": 0.6943396226415094,
                    "acc_stderr": 0.028353298073322663,
                    "acc_norm": 0.6943396226415094,
                    "acc_norm_stderr": 0.028353298073322663,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_biology": {
                "5-shot": {
                    "acc": 0.7291666666666666,
                    "acc_stderr": 0.03716177437566017,
                    "acc_norm": 0.7291666666666666,
                    "acc_norm_stderr": 0.03716177437566017,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_chemistry": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_computer_science": {
                "5-shot": {
                    "acc": 0.52,
                    "acc_stderr": 0.050211673156867795,
                    "acc_norm": 0.52,
                    "acc_norm_stderr": 0.050211673156867795,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_mathematics": {
                "5-shot": {
                    "acc": 0.4,
                    "acc_stderr": 0.04923659639173309,
                    "acc_norm": 0.4,
                    "acc_norm_stderr": 0.04923659639173309,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_medicine": {
                "5-shot": {
                    "acc": 0.6647398843930635,
                    "acc_stderr": 0.03599586301247077,
                    "acc_norm": 0.6647398843930635,
                    "acc_norm_stderr": 0.03599586301247077,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-college_physics": {
                "5-shot": {
                    "acc": 0.39215686274509803,
                    "acc_stderr": 0.04858083574266346,
                    "acc_norm": 0.39215686274509803,
                    "acc_norm_stderr": 0.04858083574266346,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-computer_security": {
                "5-shot": {
                    "acc": 0.77,
                    "acc_stderr": 0.042295258468165065,
                    "acc_norm": 0.77,
                    "acc_norm_stderr": 0.042295258468165065,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-conceptual_physics": {
                "5-shot": {
                    "acc": 0.574468085106383,
                    "acc_stderr": 0.03232146916224468,
                    "acc_norm": 0.574468085106383,
                    "acc_norm_stderr": 0.03232146916224468,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-econometrics": {
                "5-shot": {
                    "acc": 0.5,
                    "acc_stderr": 0.047036043419179864,
                    "acc_norm": 0.5,
                    "acc_norm_stderr": 0.047036043419179864,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-electrical_engineering": {
                "5-shot": {
                    "acc": 0.5724137931034483,
                    "acc_stderr": 0.04122737111370332,
                    "acc_norm": 0.5724137931034483,
                    "acc_norm_stderr": 0.04122737111370332,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-elementary_mathematics": {
                "5-shot": {
                    "acc": 0.373015873015873,
                    "acc_stderr": 0.02490699045899257,
                    "acc_norm": 0.373015873015873,
                    "acc_norm_stderr": 0.02490699045899257,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-formal_logic": {
                "5-shot": {
                    "acc": 0.4126984126984127,
                    "acc_stderr": 0.04403438954768177,
                    "acc_norm": 0.4126984126984127,
                    "acc_norm_stderr": 0.04403438954768177,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-global_facts": {
                "5-shot": {
                    "acc": 0.37,
                    "acc_stderr": 0.04852365870939099,
                    "acc_norm": 0.37,
                    "acc_norm_stderr": 0.04852365870939099,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_biology": {
                "5-shot": {
                    "acc": 0.7709677419354839,
                    "acc_stderr": 0.023904914311782648,
                    "acc_norm": 0.7709677419354839,
                    "acc_norm_stderr": 0.023904914311782648,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_chemistry": {
                "5-shot": {
                    "acc": 0.5270935960591133,
                    "acc_stderr": 0.03512819077876106,
                    "acc_norm": 0.5270935960591133,
                    "acc_norm_stderr": 0.03512819077876106,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_computer_science": {
                "5-shot": {
                    "acc": 0.68,
                    "acc_stderr": 0.04688261722621504,
                    "acc_norm": 0.68,
                    "acc_norm_stderr": 0.04688261722621504,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_european_history": {
                "5-shot": {
                    "acc": 0.7818181818181819,
                    "acc_stderr": 0.032250781083062896,
                    "acc_norm": 0.7818181818181819,
                    "acc_norm_stderr": 0.032250781083062896,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_geography": {
                "5-shot": {
                    "acc": 0.7727272727272727,
                    "acc_stderr": 0.029857515673386417,
                    "acc_norm": 0.7727272727272727,
                    "acc_norm_stderr": 0.029857515673386417,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_government_and_politics": {
                "5-shot": {
                    "acc": 0.8652849740932642,
                    "acc_stderr": 0.02463978909770944,
                    "acc_norm": 0.8652849740932642,
                    "acc_norm_stderr": 0.02463978909770944,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_macroeconomics": {
                "5-shot": {
                    "acc": 0.6666666666666666,
                    "acc_stderr": 0.023901157979402534,
                    "acc_norm": 0.6666666666666666,
                    "acc_norm_stderr": 0.023901157979402534,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_mathematics": {
                "5-shot": {
                    "acc": 0.337037037037037,
                    "acc_stderr": 0.028820884666253255,
                    "acc_norm": 0.337037037037037,
                    "acc_norm_stderr": 0.028820884666253255,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_microeconomics": {
                "5-shot": {
                    "acc": 0.6596638655462185,
                    "acc_stderr": 0.030778057422931673,
                    "acc_norm": 0.6596638655462185,
                    "acc_norm_stderr": 0.030778057422931673,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_physics": {
                "5-shot": {
                    "acc": 0.32450331125827814,
                    "acc_stderr": 0.038227469376587525,
                    "acc_norm": 0.32450331125827814,
                    "acc_norm_stderr": 0.038227469376587525,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_psychology": {
                "5-shot": {
                    "acc": 0.8238532110091743,
                    "acc_stderr": 0.016332882393431385,
                    "acc_norm": 0.8238532110091743,
                    "acc_norm_stderr": 0.016332882393431385,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_statistics": {
                "5-shot": {
                    "acc": 0.5740740740740741,
                    "acc_stderr": 0.03372343271653062,
                    "acc_norm": 0.5740740740740741,
                    "acc_norm_stderr": 0.03372343271653062,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_us_history": {
                "5-shot": {
                    "acc": 0.7990196078431373,
                    "acc_stderr": 0.028125972265654373,
                    "acc_norm": 0.7990196078431373,
                    "acc_norm_stderr": 0.028125972265654373,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-high_school_world_history": {
                "5-shot": {
                    "acc": 0.7721518987341772,
                    "acc_stderr": 0.027303484599069436,
                    "acc_norm": 0.7721518987341772,
                    "acc_norm_stderr": 0.027303484599069436,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-human_aging": {
                "5-shot": {
                    "acc": 0.7040358744394619,
                    "acc_stderr": 0.030636591348699803,
                    "acc_norm": 0.7040358744394619,
                    "acc_norm_stderr": 0.030636591348699803,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-human_sexuality": {
                "5-shot": {
                    "acc": 0.7938931297709924,
                    "acc_stderr": 0.03547771004159463,
                    "acc_norm": 0.7938931297709924,
                    "acc_norm_stderr": 0.03547771004159463,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-international_law": {
                "5-shot": {
                    "acc": 0.7768595041322314,
                    "acc_stderr": 0.03800754475228732,
                    "acc_norm": 0.7768595041322314,
                    "acc_norm_stderr": 0.03800754475228732,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-jurisprudence": {
                "5-shot": {
                    "acc": 0.7777777777777778,
                    "acc_stderr": 0.040191074725573483,
                    "acc_norm": 0.7777777777777778,
                    "acc_norm_stderr": 0.040191074725573483,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-logical_fallacies": {
                "5-shot": {
                    "acc": 0.7914110429447853,
                    "acc_stderr": 0.031921934489347235,
                    "acc_norm": 0.7914110429447853,
                    "acc_norm_stderr": 0.031921934489347235,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-machine_learning": {
                "5-shot": {
                    "acc": 0.48214285714285715,
                    "acc_stderr": 0.047427623612430116,
                    "acc_norm": 0.48214285714285715,
                    "acc_norm_stderr": 0.047427623612430116,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-management": {
                "5-shot": {
                    "acc": 0.8155339805825242,
                    "acc_stderr": 0.03840423627288276,
                    "acc_norm": 0.8155339805825242,
                    "acc_norm_stderr": 0.03840423627288276,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-marketing": {
                "5-shot": {
                    "acc": 0.8717948717948718,
                    "acc_stderr": 0.02190190511507333,
                    "acc_norm": 0.8717948717948718,
                    "acc_norm_stderr": 0.02190190511507333,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-medical_genetics": {
                "5-shot": {
                    "acc": 0.74,
                    "acc_stderr": 0.04408440022768078,
                    "acc_norm": 0.74,
                    "acc_norm_stderr": 0.04408440022768078,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-miscellaneous": {
                "5-shot": {
                    "acc": 0.8173690932311622,
                    "acc_stderr": 0.013816335389973136,
                    "acc_norm": 0.8173690932311622,
                    "acc_norm_stderr": 0.013816335389973136,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-moral_disputes": {
                "5-shot": {
                    "acc": 0.7109826589595376,
                    "acc_stderr": 0.02440517393578323,
                    "acc_norm": 0.7109826589595376,
                    "acc_norm_stderr": 0.02440517393578323,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-moral_scenarios": {
                "5-shot": {
                    "acc": 0.32513966480446926,
                    "acc_stderr": 0.01566654278505355,
                    "acc_norm": 0.32513966480446926,
                    "acc_norm_stderr": 0.01566654278505355,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-nutrition": {
                "5-shot": {
                    "acc": 0.7581699346405228,
                    "acc_stderr": 0.024518195641879334,
                    "acc_norm": 0.7581699346405228,
                    "acc_norm_stderr": 0.024518195641879334,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-philosophy": {
                "5-shot": {
                    "acc": 0.6977491961414791,
                    "acc_stderr": 0.026082700695399665,
                    "acc_norm": 0.6977491961414791,
                    "acc_norm_stderr": 0.026082700695399665,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-prehistory": {
                "5-shot": {
                    "acc": 0.7345679012345679,
                    "acc_stderr": 0.024569223600460845,
                    "acc_norm": 0.7345679012345679,
                    "acc_norm_stderr": 0.024569223600460845,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_accounting": {
                "5-shot": {
                    "acc": 0.4858156028368794,
                    "acc_stderr": 0.02981549448368206,
                    "acc_norm": 0.4858156028368794,
                    "acc_norm_stderr": 0.02981549448368206,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_law": {
                "5-shot": {
                    "acc": 0.44784876140808344,
                    "acc_stderr": 0.01270058240476822,
                    "acc_norm": 0.44784876140808344,
                    "acc_norm_stderr": 0.01270058240476822,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_medicine": {
                "5-shot": {
                    "acc": 0.6911764705882353,
                    "acc_stderr": 0.02806499816704009,
                    "acc_norm": 0.6911764705882353,
                    "acc_norm_stderr": 0.02806499816704009,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-professional_psychology": {
                "5-shot": {
                    "acc": 0.6813725490196079,
                    "acc_stderr": 0.01885008469646872,
                    "acc_norm": 0.6813725490196079,
                    "acc_norm_stderr": 0.01885008469646872,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-public_relations": {
                "5-shot": {
                    "acc": 0.6727272727272727,
                    "acc_stderr": 0.0449429086625209,
                    "acc_norm": 0.6727272727272727,
                    "acc_norm_stderr": 0.0449429086625209,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-security_studies": {
                "5-shot": {
                    "acc": 0.726530612244898,
                    "acc_stderr": 0.028535560337128448,
                    "acc_norm": 0.726530612244898,
                    "acc_norm_stderr": 0.028535560337128448,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-sociology": {
                "5-shot": {
                    "acc": 0.8308457711442786,
                    "acc_stderr": 0.026508590656233264,
                    "acc_norm": 0.8308457711442786,
                    "acc_norm_stderr": 0.026508590656233264,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-us_foreign_policy": {
                "5-shot": {
                    "acc": 0.86,
                    "acc_stderr": 0.034873508801977704,
                    "acc_norm": 0.86,
                    "acc_norm_stderr": 0.034873508801977704,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-virology": {
                "5-shot": {
                    "acc": 0.5542168674698795,
                    "acc_stderr": 0.03869543323472101,
                    "acc_norm": 0.5542168674698795,
                    "acc_norm_stderr": 0.03869543323472101,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "hendrycksTest-world_religions": {
                "5-shot": {
                    "acc": 0.8304093567251462,
                    "acc_stderr": 0.02878210810540171,
                    "acc_norm": 0.8304093567251462,
                    "acc_norm_stderr": 0.02878210810540171,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            },
            "truthfulqa:mc": {
                "0-shot": {
                    "mc1": 0.2802937576499388,
                    "mc1_stderr": 0.015723139524608763,
                    "mc2": 0.4215317106968115,
                    "mc2_stderr": 0.014138129483133954,
                    "timestamp": "2023-09-27T15-30-59.039834"
                }
            }
        }
    }
}