id,benchmark,setting,metric,metric_value,metric_stderr
Deci/DeciCoder-1b,drop,3-shot,accuracy,0.0006291946308724,0.0002568002749723
Deci/DeciCoder-1b,drop,3-shot,f1,0.0297881711409396,0.0009513874747103
Deci/DeciCoder-1b,gsm8k,5-shot,accuracy,0.0174374526156178,0.0036054868679982
Deci/DeciCoder-1b,winogrande,5-shot,accuracy,0.5082872928176796,0.0140505553228241
Deci/DeciCoder-1b,arc:challenge,25-shot,accuracy,0.1604095563139931,0.0107243360591109
Deci/DeciCoder-1b,arc:challenge,25-shot,acc_norm,0.2116040955631399,0.0119359163586328
Deci/DeciCoder-1b,hellaswag,10-shot,accuracy,0.2826130252937662,0.0044934958720001
Deci/DeciCoder-1b,hellaswag,10-shot,acc_norm,0.3108942441744672,0.0046191364973598
Deci/DeciCoder-1b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.29,0.0456048021572068
Deci/DeciCoder-1b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.29,0.0456048021572068
Deci/DeciCoder-1b,hendrycksTest-anatomy,5-shot,accuracy,0.2592592592592592,0.0378571446506665
Deci/DeciCoder-1b,hendrycksTest-anatomy,5-shot,acc_norm,0.2592592592592592,0.0378571446506665
Deci/DeciCoder-1b,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.0358349617636106
Deci/DeciCoder-1b,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.0358349617636106
Deci/DeciCoder-1b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.047258156262526
Deci/DeciCoder-1b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.047258156262526
Deci/DeciCoder-1b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2,0.0246182981958665
Deci/DeciCoder-1b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2,0.0246182981958665
Deci/DeciCoder-1b,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
Deci/DeciCoder-1b,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.0368565109589753
Deci/DeciCoder-1b,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.0440844002276807
Deci/DeciCoder-1b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.0440844002276807
Deci/DeciCoder-1b,hendrycksTest-college_computer_science,5-shot,accuracy,0.19,0.0394277244403662
Deci/DeciCoder-1b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.19,0.0394277244403662
Deci/DeciCoder-1b,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.0416333199893226
Deci/DeciCoder-1b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.0416333199893226
Deci/DeciCoder-1b,hendrycksTest-college_medicine,5-shot,accuracy,0.2254335260115607,0.0318620985164114
Deci/DeciCoder-1b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2254335260115607,0.0318620985164114
Deci/DeciCoder-1b,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.0433643270799317
Deci/DeciCoder-1b,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.0433643270799317
Deci/DeciCoder-1b,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
Deci/DeciCoder-1b,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
Deci/DeciCoder-1b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2808510638297872,0.0293791704641248
Deci/DeciCoder-1b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2808510638297872,0.0293791704641248
Deci/DeciCoder-1b,hendrycksTest-econometrics,5-shot,accuracy,0.2017543859649122,0.0377520501358363
Deci/DeciCoder-1b,hendrycksTest-econometrics,5-shot,acc_norm,0.2017543859649122,0.0377520501358363
Deci/DeciCoder-1b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2275862068965517,0.0349395038013118
Deci/DeciCoder-1b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2275862068965517,0.0349395038013118
Deci/DeciCoder-1b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2328042328042328,0.0217659616721545
Deci/DeciCoder-1b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2328042328042328,0.0217659616721545
Deci/DeciCoder-1b,hendrycksTest-formal_logic,5-shot,accuracy,0.2698412698412698,0.0397015827323517
Deci/DeciCoder-1b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2698412698412698,0.0397015827323517
Deci/DeciCoder-1b,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.0460566186471838
Deci/DeciCoder-1b,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.0460566186471838
Deci/DeciCoder-1b,hendrycksTest-high_school_biology,5-shot,accuracy,0.1741935483870967,0.0215762481845145
Deci/DeciCoder-1b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.1741935483870967,0.0215762481845145
Deci/DeciCoder-1b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.1871921182266009,0.0274449249668826
Deci/DeciCoder-1b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.1871921182266009,0.0274449249668826
Deci/DeciCoder-1b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.3,0.0460566186471838
Deci/DeciCoder-1b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.3,0.0460566186471838
Deci/DeciCoder-1b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2303030303030303,0.0328766675860348
Deci/DeciCoder-1b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2303030303030303,0.0328766675860348
Deci/DeciCoder-1b,hendrycksTest-high_school_geography,5-shot,accuracy,0.207070707070707,0.028869778460267
Deci/DeciCoder-1b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.207070707070707,0.028869778460267
Deci/DeciCoder-1b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2072538860103626,0.0292528232918036
Deci/DeciCoder-1b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2072538860103626,0.0292528232918036
Deci/DeciCoder-1b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.1948717948717948,0.0200831675951813
Deci/DeciCoder-1b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.1948717948717948,0.0200831675951813
Deci/DeciCoder-1b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2444444444444444,0.0262027665346521
Deci/DeciCoder-1b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2444444444444444,0.0262027665346521
Deci/DeciCoder-1b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.1932773109243697,0.0256494702658891
Deci/DeciCoder-1b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.1932773109243697,0.0256494702658891
Deci/DeciCoder-1b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
Deci/DeciCoder-1b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
Deci/DeciCoder-1b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1908256880733945,0.0168476764000911
Deci/DeciCoder-1b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1908256880733945,0.0168476764000911
Deci/DeciCoder-1b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1712962962962963,0.0256953416438246
Deci/DeciCoder-1b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1712962962962963,0.0256953416438246
Deci/DeciCoder-1b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
Deci/DeciCoder-1b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2598039215686274,0.0307785546786932
Deci/DeciCoder-1b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.0289007219062934
Deci/DeciCoder-1b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.0289007219062934
Deci/DeciCoder-1b,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.0310244117405722
Deci/DeciCoder-1b,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.0310244117405722
Deci/DeciCoder-1b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2824427480916031,0.0394840612576836
Deci/DeciCoder-1b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2824427480916031,0.0394840612576836
Deci/DeciCoder-1b,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.0398497965330287
Deci/DeciCoder-1b,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.0398497965330287
Deci/DeciCoder-1b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
Deci/DeciCoder-1b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2314814814814814,0.0407749470925262
Deci/DeciCoder-1b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
Deci/DeciCoder-1b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.263803680981595,0.0346241993161562
Deci/DeciCoder-1b,hendrycksTest-machine_learning,5-shot,accuracy,0.2589285714285714,0.0415775153986562
Deci/DeciCoder-1b,hendrycksTest-machine_learning,5-shot,acc_norm,0.2589285714285714,0.0415775153986562
Deci/DeciCoder-1b,hendrycksTest-management,5-shot,accuracy,0.1844660194174757,0.0384042362728827
Deci/DeciCoder-1b,hendrycksTest-management,5-shot,acc_norm,0.1844660194174757,0.0384042362728827
Deci/DeciCoder-1b,hendrycksTest-marketing,5-shot,accuracy,0.282051282051282,0.0294803605495411
Deci/DeciCoder-1b,hendrycksTest-marketing,5-shot,acc_norm,0.282051282051282,0.0294803605495411
Deci/DeciCoder-1b,hendrycksTest-medical_genetics,5-shot,accuracy,0.2,0.0402015126103684
Deci/DeciCoder-1b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.2,0.0402015126103684
Deci/DeciCoder-1b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2796934865900383,0.0160507921480365
Deci/DeciCoder-1b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2796934865900383,0.0160507921480365
Deci/DeciCoder-1b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2398843930635838,0.0229895925431235
Deci/DeciCoder-1b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2398843930635838,0.0229895925431235
Deci/DeciCoder-1b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
Deci/DeciCoder-1b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2469273743016759,0.0144222922048088
Deci/DeciCoder-1b,hendrycksTest-nutrition,5-shot,accuracy,0.218954248366013,0.0236790898618077
Deci/DeciCoder-1b,hendrycksTest-nutrition,5-shot,acc_norm,0.218954248366013,0.0236790898618077
Deci/DeciCoder-1b,hendrycksTest-philosophy,5-shot,accuracy,0.2829581993569132,0.0255830624899848
Deci/DeciCoder-1b,hendrycksTest-philosophy,5-shot,acc_norm,0.2829581993569132,0.0255830624899848
Deci/DeciCoder-1b,hendrycksTest-prehistory,5-shot,accuracy,0.2438271604938271,0.0238918795419596
Deci/DeciCoder-1b,hendrycksTest-prehistory,5-shot,acc_norm,0.2438271604938271,0.0238918795419596
Deci/DeciCoder-1b,hendrycksTest-professional_accounting,5-shot,accuracy,0.25177304964539,0.0258921511567094
Deci/DeciCoder-1b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25177304964539,0.0258921511567094
Deci/DeciCoder-1b,hendrycksTest-professional_law,5-shot,accuracy,0.241851368970013,0.010936550813827
Deci/DeciCoder-1b,hendrycksTest-professional_law,5-shot,acc_norm,0.241851368970013,0.010936550813827
Deci/DeciCoder-1b,hendrycksTest-professional_medicine,5-shot,accuracy,0.1727941176470588,0.0229660675855817
Deci/DeciCoder-1b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1727941176470588,0.0229660675855817
Deci/DeciCoder-1b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2826797385620915,0.0182172695520534
Deci/DeciCoder-1b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2826797385620915,0.0182172695520534
Deci/DeciCoder-1b,hendrycksTest-public_relations,5-shot,accuracy,0.2454545454545454,0.0412206650287828
Deci/DeciCoder-1b,hendrycksTest-public_relations,5-shot,acc_norm,0.2454545454545454,0.0412206650287828
Deci/DeciCoder-1b,hendrycksTest-security_studies,5-shot,accuracy,0.1959183673469387,0.0254093019532256
Deci/DeciCoder-1b,hendrycksTest-security_studies,5-shot,acc_norm,0.1959183673469387,0.0254093019532256
Deci/DeciCoder-1b,hendrycksTest-sociology,5-shot,accuracy,0.2587064676616915,0.030965903123573
Deci/DeciCoder-1b,hendrycksTest-sociology,5-shot,acc_norm,0.2587064676616915,0.030965903123573
Deci/DeciCoder-1b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276808
Deci/DeciCoder-1b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.0440844002276808
Deci/DeciCoder-1b,hendrycksTest-virology,5-shot,accuracy,0.253012048192771,0.0338442915523313
Deci/DeciCoder-1b,hendrycksTest-virology,5-shot,acc_norm,0.253012048192771,0.0338442915523313
Deci/DeciCoder-1b,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.0350877192982456
Deci/DeciCoder-1b,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.0350877192982456
Deci/DeciCoder-1b,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.0153218216884761
Deci/DeciCoder-1b,truthfulqa:mc,0-shot,mc2,0.4705381335286149,0.0154910129799629
rinna/bilingual-gpt-neox-4b,minerva_math_precalc,5-shot,accuracy,0.0073260073260073,0.003652908089383
rinna/bilingual-gpt-neox-4b,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458,0.0034284443646836
rinna/bilingual-gpt-neox-4b,minerva_math_num_theory,5-shot,accuracy,0.0185185185185185,0.0058069728079122
rinna/bilingual-gpt-neox-4b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0110741971207087,0.0034844537978317
rinna/bilingual-gpt-neox-4b,minerva_math_geometry,5-shot,accuracy,0.0187891440501043,0.0062104164279974
rinna/bilingual-gpt-neox-4b,minerva_math_counting_and_prob,5-shot,accuracy,0.0063291139240506,0.003646382041065
rinna/bilingual-gpt-neox-4b,minerva_math_algebra,5-shot,accuracy,0.0092670598146588,0.0027823191184888
rinna/bilingual-gpt-neox-4b,fld_default,0-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,fld_star,0-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521
rinna/bilingual-gpt-neox-4b,arithmetic_3ds,5-shot,accuracy,0.0075,0.0019296986470519
rinna/bilingual-gpt-neox-4b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
rinna/bilingual-gpt-neox-4b,arithmetic_2ds,5-shot,accuracy,0.0415,0.0044608098381578
rinna/bilingual-gpt-neox-4b,arithmetic_5ds,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_5da,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_1dc,5-shot,accuracy,0.066,0.005553144938623
rinna/bilingual-gpt-neox-4b,arithmetic_4ds,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_2dm,5-shot,accuracy,0.033,0.0039954326099773
rinna/bilingual-gpt-neox-4b,arithmetic_2da,5-shot,accuracy,0.035,0.0041104680966998
rinna/bilingual-gpt-neox-4b,gsm8k_cot,5-shot,accuracy,0.0219863532979529,0.00403916275811
rinna/bilingual-gpt-neox-4b,gsm8k,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,anli_r2,0-shot,brier_score,0.7465044173321834,
rinna/bilingual-gpt-neox-4b,anli_r3,0-shot,brier_score,0.7382616042295201,
rinna/bilingual-gpt-neox-4b,anli_r1,0-shot,brier_score,0.7422539652276323,
rinna/bilingual-gpt-neox-4b,xnli_eu,0-shot,brier_score,0.9935567744254022,
rinna/bilingual-gpt-neox-4b,xnli_vi,0-shot,brier_score,0.8210592681442176,
rinna/bilingual-gpt-neox-4b,xnli_ru,0-shot,brier_score,0.8190550378326039,
rinna/bilingual-gpt-neox-4b,xnli_zh,0-shot,brier_score,0.943566426138912,
rinna/bilingual-gpt-neox-4b,xnli_tr,0-shot,brier_score,0.8247652276169989,
rinna/bilingual-gpt-neox-4b,xnli_fr,0-shot,brier_score,0.7935452141545868,
rinna/bilingual-gpt-neox-4b,xnli_en,0-shot,brier_score,0.673510778129045,
rinna/bilingual-gpt-neox-4b,xnli_ur,0-shot,brier_score,0.93848350317982,
rinna/bilingual-gpt-neox-4b,xnli_ar,0-shot,brier_score,1.080089095089373,
rinna/bilingual-gpt-neox-4b,xnli_de,0-shot,brier_score,0.8497396490738153,
rinna/bilingual-gpt-neox-4b,xnli_hi,0-shot,brier_score,0.7784793501566398,
rinna/bilingual-gpt-neox-4b,xnli_es,0-shot,brier_score,0.8344890347288778,
rinna/bilingual-gpt-neox-4b,xnli_bg,0-shot,brier_score,0.9131127340061094,
rinna/bilingual-gpt-neox-4b,xnli_sw,0-shot,brier_score,0.8375909307024677,
rinna/bilingual-gpt-neox-4b,xnli_el,0-shot,brier_score,0.920078994998459,
rinna/bilingual-gpt-neox-4b,xnli_th,0-shot,brier_score,0.7626190761994815,
rinna/bilingual-gpt-neox-4b,logiqa2,0-shot,brier_score,1.0595557248900274,
rinna/bilingual-gpt-neox-4b,mathqa,0-shot,brier_score,0.9543318277043356,
rinna/bilingual-gpt-neox-4b,lambada_standard,0-shot,perplexity,9.40445963354471,0.2520259735682458
rinna/bilingual-gpt-neox-4b,lambada_standard,0-shot,accuracy,0.518338831748496,0.0069612905861364
rinna/bilingual-gpt-neox-4b,lambada_openai,0-shot,perplexity,5.365362213035259,0.1284236656044808
rinna/bilingual-gpt-neox-4b,lambada_openai,0-shot,accuracy,0.6394333398020571,0.0066896361125438
rinna/bilingual-gpt-neox-4b,mmlu_world_religions,0-shot,accuracy,0.2514619883040935,0.0332750442384684
rinna/bilingual-gpt-neox-4b,mmlu_formal_logic,0-shot,accuracy,0.246031746031746,0.0385227336492431
rinna/bilingual-gpt-neox-4b,mmlu_prehistory,0-shot,accuracy,0.2777777777777778,0.0249220011688863
rinna/bilingual-gpt-neox-4b,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
rinna/bilingual-gpt-neox-4b,mmlu_high_school_world_history,0-shot,accuracy,0.2362869198312236,0.0276521531441592
rinna/bilingual-gpt-neox-4b,mmlu_moral_disputes,0-shot,accuracy,0.1994219653179191,0.0215119006542525
rinna/bilingual-gpt-neox-4b,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.0110641510271654
rinna/bilingual-gpt-neox-4b,mmlu_logical_fallacies,0-shot,accuracy,0.294478527607362,0.0358116579047408
rinna/bilingual-gpt-neox-4b,mmlu_high_school_us_history,0-shot,accuracy,0.230392156862745,0.029554292605695
rinna/bilingual-gpt-neox-4b,mmlu_philosophy,0-shot,accuracy,0.3054662379421222,0.0261605844501404
rinna/bilingual-gpt-neox-4b,mmlu_jurisprudence,0-shot,accuracy,0.2777777777777778,0.0433004374965074
rinna/bilingual-gpt-neox-4b,mmlu_international_law,0-shot,accuracy,0.3140495867768595,0.0423696475304101
rinna/bilingual-gpt-neox-4b,mmlu_high_school_european_history,0-shot,accuracy,0.2424242424242424,0.0334640988105595
rinna/bilingual-gpt-neox-4b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2279792746113989,0.0302769099451782
rinna/bilingual-gpt-neox-4b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2689075630252101,0.0288013921936312
rinna/bilingual-gpt-neox-4b,mmlu_high_school_geography,0-shot,accuracy,0.2323232323232323,0.0300886294902174
rinna/bilingual-gpt-neox-4b,mmlu_high_school_psychology,0-shot,accuracy,0.2201834862385321,0.0177659786523275
rinna/bilingual-gpt-neox-4b,mmlu_public_relations,0-shot,accuracy,0.1909090909090909,0.0376442558598492
rinna/bilingual-gpt-neox-4b,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276807
rinna/bilingual-gpt-neox-4b,mmlu_sociology,0-shot,accuracy,0.2587064676616915,0.030965903123573
rinna/bilingual-gpt-neox-4b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2461538461538461,0.021840866990423
rinna/bilingual-gpt-neox-4b,mmlu_security_studies,0-shot,accuracy,0.1795918367346938,0.0245732935895856
rinna/bilingual-gpt-neox-4b,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
rinna/bilingual-gpt-neox-4b,mmlu_human_sexuality,0-shot,accuracy,0.2137404580152671,0.0359546161177469
rinna/bilingual-gpt-neox-4b,mmlu_econometrics,0-shot,accuracy,0.2280701754385964,0.0394715278266941
rinna/bilingual-gpt-neox-4b,mmlu_miscellaneous,0-shot,accuracy,0.2733077905491698,0.0159366810626285
rinna/bilingual-gpt-neox-4b,mmlu_marketing,0-shot,accuracy,0.235042735042735,0.0277788359049354
rinna/bilingual-gpt-neox-4b,mmlu_management,0-shot,accuracy,0.203883495145631,0.0398913985953176
rinna/bilingual-gpt-neox-4b,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.0252616912197294
rinna/bilingual-gpt-neox-4b,mmlu_medical_genetics,0-shot,accuracy,0.24,0.0429234695990928
rinna/bilingual-gpt-neox-4b,mmlu_human_aging,0-shot,accuracy,0.3049327354260089,0.0308986108824775
rinna/bilingual-gpt-neox-4b,mmlu_professional_medicine,0-shot,accuracy,0.1801470588235294,0.0233451636165448
rinna/bilingual-gpt-neox-4b,mmlu_college_medicine,0-shot,accuracy,0.2427745664739884,0.0326926380614177
rinna/bilingual-gpt-neox-4b,mmlu_business_ethics,0-shot,accuracy,0.2,0.0402015126103684
rinna/bilingual-gpt-neox-4b,mmlu_clinical_knowledge,0-shot,accuracy,0.2490566037735849,0.0266164829805017
rinna/bilingual-gpt-neox-4b,mmlu_global_facts,0-shot,accuracy,0.3,0.0460566186471838
rinna/bilingual-gpt-neox-4b,mmlu_virology,0-shot,accuracy,0.3012048192771084,0.0357160923005348
rinna/bilingual-gpt-neox-4b,mmlu_professional_accounting,0-shot,accuracy,0.2730496453900709,0.0265778609433078
rinna/bilingual-gpt-neox-4b,mmlu_college_physics,0-shot,accuracy,0.2647058823529412,0.0438986995680877
rinna/bilingual-gpt-neox-4b,mmlu_high_school_physics,0-shot,accuracy,0.2516556291390728,0.0354330423438998
rinna/bilingual-gpt-neox-4b,mmlu_high_school_biology,0-shot,accuracy,0.2870967741935484,0.0257365427455945
rinna/bilingual-gpt-neox-4b,mmlu_college_biology,0-shot,accuracy,0.2291666666666666,0.0351469746786238
rinna/bilingual-gpt-neox-4b,mmlu_anatomy,0-shot,accuracy,0.2888888888888888,0.0391545063041425
rinna/bilingual-gpt-neox-4b,mmlu_college_chemistry,0-shot,accuracy,0.18,0.0386122919665369
rinna/bilingual-gpt-neox-4b,mmlu_computer_security,0-shot,accuracy,0.19,0.0394277244403662
rinna/bilingual-gpt-neox-4b,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
rinna/bilingual-gpt-neox-4b,mmlu_astronomy,0-shot,accuracy,0.2697368421052631,0.0361178056028489
rinna/bilingual-gpt-neox-4b,mmlu_college_mathematics,0-shot,accuracy,0.28,0.0451260859854212
rinna/bilingual-gpt-neox-4b,mmlu_conceptual_physics,0-shot,accuracy,0.2425531914893617,0.0280202262712002
rinna/bilingual-gpt-neox-4b,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.0451260859854212
rinna/bilingual-gpt-neox-4b,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.0460566186471838
rinna/bilingual-gpt-neox-4b,mmlu_machine_learning,0-shot,accuracy,0.2410714285714285,0.0405986724695268
rinna/bilingual-gpt-neox-4b,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.0310898260029375
rinna/bilingual-gpt-neox-4b,mmlu_high_school_statistics,0-shot,accuracy,0.3009259259259259,0.0312803908432988
rinna/bilingual-gpt-neox-4b,mmlu_elementary_mathematics,0-shot,accuracy,0.246031746031746,0.0221820372029483
rinna/bilingual-gpt-neox-4b,mmlu_electrical_engineering,0-shot,accuracy,0.2689655172413793,0.0369518331165023
rinna/bilingual-gpt-neox-4b,mmlu_high_school_mathematics,0-shot,accuracy,0.2592592592592592,0.0267192407837121
rinna/bilingual-gpt-neox-4b,arc_challenge,25-shot,accuracy,0.2901023890784983,0.0132615736775207
rinna/bilingual-gpt-neox-4b,arc_challenge,25-shot,acc_norm,0.3216723549488055,0.0136504880844941
rinna/bilingual-gpt-neox-4b,hellaswag,10-shot,accuracy,0.3646683927504481,0.0048035333333642
rinna/bilingual-gpt-neox-4b,hellaswag,10-shot,acc_norm,0.4372634933280223,0.0049503473337018
rinna/bilingual-gpt-neox-4b,truthfulqa_mc2,0-shot,accuracy,0.3587180858240576,0.013702244823217
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,bleu_max,20.298502784294392,0.6658217344882357
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,bleu_acc,0.2949816401468788,0.0159644009655896
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,bleu_diff,-6.505009210061705,0.6268960954843193
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge1_max,43.773621090740846,0.8400411954477274
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge1_acc,0.241126070991432,0.0149748272797523
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge1_diff,-10.723279586053891,0.7370334795276966
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge2_max,26.5835203998597,0.8994004691527028
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge2_acc,0.1799265605875153,0.0134471092355375
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge2_diff,-11.606543714790986,0.8349217988871113
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rougeL_max,40.61834366861577,0.833488462597628
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rougeL_acc,0.2362301101591187,0.0148697550158711
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rougeL_diff,-10.553870158245887,0.7246002434925973
rinna/bilingual-gpt-neox-4b,truthfulqa_mc1,0-shot,accuracy,0.2141982864137087,0.0143621481556904
rinna/bilingual-gpt-neox-4b,winogrande,5-shot,accuracy,0.5185477505919495,0.0140428137088883
kevin009/babyllama-v0.6,arc:challenge,25-shot,accuracy,0.3523890784982935,0.0139601426005986
kevin009/babyllama-v0.6,arc:challenge,25-shot,acc_norm,0.3609215017064846,0.0140347613861754
kevin009/babyllama-v0.6,hellaswag,10-shot,accuracy,0.4633539135630352,0.0049763614543413
kevin009/babyllama-v0.6,hellaswag,10-shot,acc_norm,0.6159131647082254,0.0048538457503921
kevin009/babyllama-v0.6,hendrycksTest-abstract_algebra,5-shot,accuracy,0.24,0.0429234695990928
kevin009/babyllama-v0.6,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.24,0.0429234695990928
kevin009/babyllama-v0.6,hendrycksTest-anatomy,5-shot,accuracy,0.1777777777777777,0.0330278985990171
kevin009/babyllama-v0.6,hendrycksTest-anatomy,5-shot,acc_norm,0.1777777777777777,0.0330278985990171
kevin009/babyllama-v0.6,hendrycksTest-astronomy,5-shot,accuracy,0.1710526315789473,0.030643607071677
kevin009/babyllama-v0.6,hendrycksTest-astronomy,5-shot,acc_norm,0.1710526315789473,0.030643607071677
kevin009/babyllama-v0.6,hendrycksTest-business_ethics,5-shot,accuracy,0.24,0.0429234695990928
kevin009/babyllama-v0.6,hendrycksTest-business_ethics,5-shot,acc_norm,0.24,0.0429234695990928
kevin009/babyllama-v0.6,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2830188679245283,0.0277242364927009
kevin009/babyllama-v0.6,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2830188679245283,0.0277242364927009
kevin009/babyllama-v0.6,hendrycksTest-college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008034
kevin009/babyllama-v0.6,hendrycksTest-college_biology,5-shot,acc_norm,0.2430555555555555,0.0358687928008034
kevin009/babyllama-v0.6,hendrycksTest-college_chemistry,5-shot,accuracy,0.25,0.0435194139889244
kevin009/babyllama-v0.6,hendrycksTest-college_chemistry,5-shot,acc_norm,0.25,0.0435194139889244
kevin009/babyllama-v0.6,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.0446196043338474
kevin009/babyllama-v0.6,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.0446196043338474
kevin009/babyllama-v0.6,hendrycksTest-college_mathematics,5-shot,accuracy,0.35,0.0479372485441102
kevin009/babyllama-v0.6,hendrycksTest-college_mathematics,5-shot,acc_norm,0.35,0.0479372485441102
kevin009/babyllama-v0.6,hendrycksTest-college_medicine,5-shot,accuracy,0.1965317919075144,0.0302995746647881
kevin009/babyllama-v0.6,hendrycksTest-college_medicine,5-shot,acc_norm,0.1965317919075144,0.0302995746647881
kevin009/babyllama-v0.6,hendrycksTest-college_physics,5-shot,accuracy,0.196078431372549,0.0395058186117996
kevin009/babyllama-v0.6,hendrycksTest-college_physics,5-shot,acc_norm,0.196078431372549,0.0395058186117996
kevin009/babyllama-v0.6,hendrycksTest-computer_security,5-shot,accuracy,0.31,0.0464823198711731
kevin009/babyllama-v0.6,hendrycksTest-computer_security,5-shot,acc_norm,0.31,0.0464823198711731
kevin009/babyllama-v0.6,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2808510638297872,0.0293791704641248
kevin009/babyllama-v0.6,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2808510638297872,0.0293791704641248
kevin009/babyllama-v0.6,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
kevin009/babyllama-v0.6,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
kevin009/babyllama-v0.6,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2344827586206896,0.0353062587434659
kevin009/babyllama-v0.6,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2344827586206896,0.0353062587434659
kevin009/babyllama-v0.6,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
kevin009/babyllama-v0.6,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
kevin009/babyllama-v0.6,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.0356701667527686
kevin009/babyllama-v0.6,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.0356701667527686
kevin009/babyllama-v0.6,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
kevin009/babyllama-v0.6,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
kevin009/babyllama-v0.6,hendrycksTest-high_school_biology,5-shot,accuracy,0.2645161290322581,0.0250918923788592
kevin009/babyllama-v0.6,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2645161290322581,0.0250918923788592
kevin009/babyllama-v0.6,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2413793103448276,0.0301083307180116
kevin009/babyllama-v0.6,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2413793103448276,0.0301083307180116
kevin009/babyllama-v0.6,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.22,0.0416333199893226
kevin009/babyllama-v0.6,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.22,0.0416333199893226
kevin009/babyllama-v0.6,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2909090909090909,0.0354656301962433
kevin009/babyllama-v0.6,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2909090909090909,0.0354656301962433
kevin009/babyllama-v0.6,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.0296202278747904
kevin009/babyllama-v0.6,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.0296202278747904
kevin009/babyllama-v0.6,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2227979274611398,0.0300311479776415
kevin009/babyllama-v0.6,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2227979274611398,0.0300311479776415
kevin009/babyllama-v0.6,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2461538461538461,0.021840866990423
kevin009/babyllama-v0.6,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2461538461538461,0.021840866990423
kevin009/babyllama-v0.6,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2666666666666666,0.0269624243250738
kevin009/babyllama-v0.6,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2666666666666666,0.0269624243250738
kevin009/babyllama-v0.6,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2352941176470588,0.0275536144678638
kevin009/babyllama-v0.6,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2352941176470588,0.0275536144678638
kevin009/babyllama-v0.6,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.0329798664847383
kevin009/babyllama-v0.6,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.0329798664847383
kevin009/babyllama-v0.6,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2403669724770642,0.018320607320964
kevin009/babyllama-v0.6,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2403669724770642,0.018320607320964
kevin009/babyllama-v0.6,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3842592592592592,0.0331735451431074
kevin009/babyllama-v0.6,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3842592592592592,0.0331735451431074
kevin009/babyllama-v0.6,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2401960784313725,0.0299837330559136
kevin009/babyllama-v0.6,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2401960784313725,0.0299837330559136
kevin009/babyllama-v0.6,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2531645569620253,0.0283046579430353
kevin009/babyllama-v0.6,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2531645569620253,0.0283046579430353
kevin009/babyllama-v0.6,hendrycksTest-human_aging,5-shot,accuracy,0.3632286995515695,0.0322779044285049
kevin009/babyllama-v0.6,hendrycksTest-human_aging,5-shot,acc_norm,0.3632286995515695,0.0322779044285049
kevin009/babyllama-v0.6,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
kevin009/babyllama-v0.6,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
kevin009/babyllama-v0.6,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.0398497965330287
kevin009/babyllama-v0.6,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.0398497965330287
kevin009/babyllama-v0.6,hendrycksTest-jurisprudence,5-shot,accuracy,0.2592592592592592,0.0423651125809463
kevin009/babyllama-v0.6,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2592592592592592,0.0423651125809463
kevin009/babyllama-v0.6,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2208588957055214,0.0325917739274217
kevin009/babyllama-v0.6,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2208588957055214,0.0325917739274217
kevin009/babyllama-v0.6,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257872
kevin009/babyllama-v0.6,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257872
kevin009/babyllama-v0.6,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.0435463107726059
kevin009/babyllama-v0.6,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.0435463107726059
kevin009/babyllama-v0.6,hendrycksTest-marketing,5-shot,accuracy,0.282051282051282,0.0294803605495411
kevin009/babyllama-v0.6,hendrycksTest-marketing,5-shot,acc_norm,0.282051282051282,0.0294803605495411
kevin009/babyllama-v0.6,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.0446196043338474
kevin009/babyllama-v0.6,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.0446196043338474
kevin009/babyllama-v0.6,hendrycksTest-miscellaneous,5-shot,accuracy,0.280970625798212,0.0160731278512212
kevin009/babyllama-v0.6,hendrycksTest-miscellaneous,5-shot,acc_norm,0.280970625798212,0.0160731278512212
kevin009/babyllama-v0.6,hendrycksTest-moral_disputes,5-shot,accuracy,0.2254335260115607,0.0224972301909675
kevin009/babyllama-v0.6,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2254335260115607,0.0224972301909675
kevin009/babyllama-v0.6,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2268156424581005,0.0140058435708978
kevin009/babyllama-v0.6,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2268156424581005,0.0140058435708978
kevin009/babyllama-v0.6,hendrycksTest-nutrition,5-shot,accuracy,0.2418300653594771,0.0245181956418793
kevin009/babyllama-v0.6,hendrycksTest-nutrition,5-shot,acc_norm,0.2418300653594771,0.0245181956418793
kevin009/babyllama-v0.6,hendrycksTest-philosophy,5-shot,accuracy,0.2668810289389067,0.0251226376088166
kevin009/babyllama-v0.6,hendrycksTest-philosophy,5-shot,acc_norm,0.2668810289389067,0.0251226376088166
kevin009/babyllama-v0.6,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.0245692236004608
kevin009/babyllama-v0.6,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.0245692236004608
kevin009/babyllama-v0.6,hendrycksTest-professional_accounting,5-shot,accuracy,0.2588652482269503,0.0261295725271808
kevin009/babyllama-v0.6,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2588652482269503,0.0261295725271808
kevin009/babyllama-v0.6,hendrycksTest-professional_law,5-shot,accuracy,0.2385919165580182,0.0108859297420022
kevin009/babyllama-v0.6,hendrycksTest-professional_law,5-shot,acc_norm,0.2385919165580182,0.0108859297420022
kevin009/babyllama-v0.6,hendrycksTest-professional_medicine,5-shot,accuracy,0.2022058823529411,0.0243981929866549
kevin009/babyllama-v0.6,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2022058823529411,0.0243981929866549
kevin009/babyllama-v0.6,hendrycksTest-professional_psychology,5-shot,accuracy,0.2663398692810457,0.0178831881346671
kevin009/babyllama-v0.6,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2663398692810457,0.0178831881346671
kevin009/babyllama-v0.6,hendrycksTest-public_relations,5-shot,accuracy,0.3181818181818182,0.044612721759105
kevin009/babyllama-v0.6,hendrycksTest-public_relations,5-shot,acc_norm,0.3181818181818182,0.044612721759105
kevin009/babyllama-v0.6,hendrycksTest-security_studies,5-shot,accuracy,0.1714285714285714,0.0241274634626501
kevin009/babyllama-v0.6,hendrycksTest-security_studies,5-shot,acc_norm,0.1714285714285714,0.0241274634626501
kevin009/babyllama-v0.6,hendrycksTest-sociology,5-shot,accuracy,0.2338308457711442,0.0299294154083483
kevin009/babyllama-v0.6,hendrycksTest-sociology,5-shot,acc_norm,0.2338308457711442,0.0299294154083483
kevin009/babyllama-v0.6,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.0409360180740332
kevin009/babyllama-v0.6,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.0409360180740332
kevin009/babyllama-v0.6,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.0362933532994786
kevin009/babyllama-v0.6,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.0362933532994786
kevin009/babyllama-v0.6,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.0344629621708842
kevin009/babyllama-v0.6,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.0344629621708842
kevin009/babyllama-v0.6,truthfulqa:mc,0-shot,mc1,0.2203182374541003,0.0145090451714872
kevin009/babyllama-v0.6,truthfulqa:mc,0-shot,mc2,0.3584100057903431,0.0137763148921701
kevin009/babyllama-v0.6,winogrande,5-shot,accuracy,0.6101026045777427,0.0137075473170084
kevin009/babyllama-v0.6,gsm8k,5-shot,accuracy,0.0227445034116755,0.0041066206377496
kevin009/babyllama-v0.6,minerva_math_precalc,5-shot,accuracy,0.0091575091575091,0.0040803060650489
kevin009/babyllama-v0.6,minerva_math_prealgebra,5-shot,accuracy,0.0229621125143513,0.0050781087418956
kevin009/babyllama-v0.6,minerva_math_num_theory,5-shot,accuracy,0.024074074074074,0.0066022025098153
kevin009/babyllama-v0.6,minerva_math_intermediate_algebra,5-shot,accuracy,0.0177187153931339,0.0043926922934929
kevin009/babyllama-v0.6,minerva_math_geometry,5-shot,accuracy,0.0250521920668058,0.0071482478380138
kevin009/babyllama-v0.6,minerva_math_counting_and_prob,5-shot,accuracy,0.0210970464135021,0.0066076963656265
kevin009/babyllama-v0.6,minerva_math_algebra,5-shot,accuracy,0.0202190395956192,0.0040869790805184
kevin009/babyllama-v0.6,fld_default,0-shot,accuracy,0.0,
kevin009/babyllama-v0.6,fld_star,0-shot,accuracy,0.0,
kevin009/babyllama-v0.6,arithmetic_3da,5-shot,accuracy,0.3475,0.0106502858785409
kevin009/babyllama-v0.6,arithmetic_3ds,5-shot,accuracy,0.244,0.0096061511059077
kevin009/babyllama-v0.6,arithmetic_4da,5-shot,accuracy,0.096,0.0065889078649976
kevin009/babyllama-v0.6,arithmetic_2ds,5-shot,accuracy,0.4245,0.0110549075297011
kevin009/babyllama-v0.6,arithmetic_5ds,5-shot,accuracy,0.024,0.0034231358327511
kevin009/babyllama-v0.6,arithmetic_5da,5-shot,accuracy,0.0195,0.0030926780189124
kevin009/babyllama-v0.6,arithmetic_1dc,5-shot,accuracy,0.0375,0.0042492238057645
kevin009/babyllama-v0.6,arithmetic_4ds,5-shot,accuracy,0.097,0.0066194719354608
kevin009/babyllama-v0.6,arithmetic_2dm,5-shot,accuracy,0.0985,0.0066649145187895
kevin009/babyllama-v0.6,arithmetic_2da,5-shot,accuracy,0.5085,0.0111815199411391
kevin009/babyllama-v0.6,gsm8k_cot,5-shot,accuracy,0.026535253980288,0.0044270459872651
kevin009/babyllama-v0.6,anli_r2,0-shot,brier_score,0.7480318885162998,
kevin009/babyllama-v0.6,anli_r3,0-shot,brier_score,0.7193364105546545,
kevin009/babyllama-v0.6,anli_r1,0-shot,brier_score,0.767149399796389,
kevin009/babyllama-v0.6,xnli_eu,0-shot,brier_score,1.04324101721511,
kevin009/babyllama-v0.6,xnli_vi,0-shot,brier_score,0.9579775898410936,
kevin009/babyllama-v0.6,xnli_ru,0-shot,brier_score,0.8504576037078885,
kevin009/babyllama-v0.6,xnli_zh,0-shot,brier_score,1.122019714170886,
kevin009/babyllama-v0.6,xnli_tr,0-shot,brier_score,0.9338172675485504,
kevin009/babyllama-v0.6,xnli_fr,0-shot,brier_score,0.8695514801542201,
kevin009/babyllama-v0.6,xnli_en,0-shot,brier_score,0.6284457155142247,
kevin009/babyllama-v0.6,xnli_ur,0-shot,brier_score,1.331657016160129,
kevin009/babyllama-v0.6,xnli_ar,0-shot,brier_score,1.2875160416993416,
kevin009/babyllama-v0.6,xnli_de,0-shot,brier_score,0.8691284199259482,
kevin009/babyllama-v0.6,xnli_hi,0-shot,brier_score,0.9802004371134696,
kevin009/babyllama-v0.6,xnli_es,0-shot,brier_score,0.9155068659792368,
kevin009/babyllama-v0.6,xnli_bg,0-shot,brier_score,0.8526807535621683,
kevin009/babyllama-v0.6,xnli_sw,0-shot,brier_score,0.9587101927586476,
kevin009/babyllama-v0.6,xnli_el,0-shot,brier_score,0.8194813150819024,
kevin009/babyllama-v0.6,xnli_th,0-shot,brier_score,1.026181179887515,
kevin009/babyllama-v0.6,logiqa2,0-shot,brier_score,1.1193557858325285,
kevin009/babyllama-v0.6,mathqa,0-shot,brier_score,0.9959932566490026,
kevin009/babyllama-v0.6,lambada_standard,0-shot,perplexity,10.607375165530296,0.3083784957379466
kevin009/babyllama-v0.6,lambada_standard,0-shot,accuracy,0.4995148457209392,0.0069659743779615
kevin009/babyllama-v0.6,lambada_openai,0-shot,perplexity,6.514190794191786,0.1763731003336171
kevin009/babyllama-v0.6,lambada_openai,0-shot,accuracy,0.5899476033378614,0.0068523331681548
EleutherAI/gpt-neo-1.3B,arc:challenge,25-shot,accuracy,0.2713310580204778,0.0129938077275457
EleutherAI/gpt-neo-1.3B,arc:challenge,25-shot,acc_norm,0.3122866894197952,0.013542598541688
EleutherAI/gpt-neo-1.3B,hellaswag,10-shot,accuracy,0.3878709420434176,0.0048626905948157
EleutherAI/gpt-neo-1.3B,hellaswag,10-shot,acc_norm,0.4863572993427604,0.0049879236366285
EleutherAI/gpt-neo-1.3B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,hendrycksTest-anatomy,5-shot,accuracy,0.1481481481481481,0.0306886476103526
EleutherAI/gpt-neo-1.3B,hendrycksTest-anatomy,5-shot,acc_norm,0.1481481481481481,0.0306886476103526
EleutherAI/gpt-neo-1.3B,hendrycksTest-astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/gpt-neo-1.3B,hendrycksTest-astronomy,5-shot,acc_norm,0.1776315789473684,0.0311031823831233
EleutherAI/gpt-neo-1.3B,hendrycksTest-business_ethics,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/gpt-neo-1.3B,hendrycksTest-business_ethics,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/gpt-neo-1.3B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2716981132075471,0.0273777066246707
EleutherAI/gpt-neo-1.3B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2716981132075471,0.0273777066246707
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.0365394696944209
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.0365394696944209
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_chemistry,5-shot,accuracy,0.21,0.0409360180740332
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.21,0.0409360180740332
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177498
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2080924855491329,0.0309528902177498
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
EleutherAI/gpt-neo-1.3B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2851063829787234,0.0295131966255393
EleutherAI/gpt-neo-1.3B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2851063829787234,0.0295131966255393
EleutherAI/gpt-neo-1.3B,hendrycksTest-econometrics,5-shot,accuracy,0.2280701754385964,0.0394715278266941
EleutherAI/gpt-neo-1.3B,hendrycksTest-econometrics,5-shot,acc_norm,0.2280701754385964,0.0394715278266941
EleutherAI/gpt-neo-1.3B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2827586206896552,0.0375283395800333
EleutherAI/gpt-neo-1.3B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2827586206896552,0.0375283395800333
EleutherAI/gpt-neo-1.3B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2539682539682539,0.0224180428911139
EleutherAI/gpt-neo-1.3B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2539682539682539,0.0224180428911139
EleutherAI/gpt-neo-1.3B,hendrycksTest-formal_logic,5-shot,accuracy,0.2539682539682539,0.0389325961060467
EleutherAI/gpt-neo-1.3B,hendrycksTest-formal_logic,5-shot,acc_norm,0.2539682539682539,0.0389325961060467
EleutherAI/gpt-neo-1.3B,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.0394277244403662
EleutherAI/gpt-neo-1.3B,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.0394277244403662
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2,0.0227552049595429
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2,0.0227552049595429
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.1970443349753694,0.0279867246667362
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.1970443349753694,0.0279867246667362
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.27,0.0446196043338473
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.27,0.0446196043338473
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2181818181818181,0.0322507810830628
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_geography,5-shot,accuracy,0.1868686868686868,0.0277725333342189
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1868686868686868,0.0277725333342189
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2072538860103626,0.0292528232918036
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2072538860103626,0.0292528232918036
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3128205128205128,0.0235075790206453
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3128205128205128,0.0235075790206453
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.0264661175389599
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.0264661175389599
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2310924369747899,0.0273814069278689
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2310924369747899,0.0273814069278689
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1981651376146789,0.0170905738042178
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1981651376146789,0.0170905738042178
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4074074074074074,0.0335099160469604
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4074074074074074,0.0335099160469604
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.230392156862745,0.029554292605695
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.230392156862745,0.029554292605695
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2827004219409282,0.0293128141539559
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2827004219409282,0.0293128141539559
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.0310244117405722
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.0310244117405722
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
EleutherAI/gpt-neo-1.3B,hendrycksTest-international_law,5-shot,accuracy,0.2892561983471074,0.0413911272763546
EleutherAI/gpt-neo-1.3B,hendrycksTest-international_law,5-shot,acc_norm,0.2892561983471074,0.0413911272763546
EleutherAI/gpt-neo-1.3B,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.041860917913946
EleutherAI/gpt-neo-1.3B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.041860917913946
EleutherAI/gpt-neo-1.3B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2392638036809816,0.0335195387952126
EleutherAI/gpt-neo-1.3B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2392638036809816,0.0335195387952126
EleutherAI/gpt-neo-1.3B,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257872
EleutherAI/gpt-neo-1.3B,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257872
EleutherAI/gpt-neo-1.3B,hendrycksTest-management,5-shot,accuracy,0.174757281553398,0.0376017800602662
EleutherAI/gpt-neo-1.3B,hendrycksTest-management,5-shot,acc_norm,0.174757281553398,0.0376017800602662
EleutherAI/gpt-neo-1.3B,hendrycksTest-marketing,5-shot,accuracy,0.2948717948717949,0.0298725777088911
EleutherAI/gpt-neo-1.3B,hendrycksTest-marketing,5-shot,acc_norm,0.2948717948717949,0.0298725777088911
EleutherAI/gpt-neo-1.3B,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/gpt-neo-1.3B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/gpt-neo-1.3B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2222222222222222,0.0148668216647095
EleutherAI/gpt-neo-1.3B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2222222222222222,0.0148668216647095
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2485549132947976,0.0232675284321001
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2485549132947976,0.0232675284321001
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2402234636871508,0.0142883438039252
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2402234636871508,0.0142883438039252
EleutherAI/gpt-neo-1.3B,hendrycksTest-nutrition,5-shot,accuracy,0.2287581699346405,0.0240510297399122
EleutherAI/gpt-neo-1.3B,hendrycksTest-nutrition,5-shot,acc_norm,0.2287581699346405,0.0240510297399122
EleutherAI/gpt-neo-1.3B,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/gpt-neo-1.3B,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.0221224397724807
EleutherAI/gpt-neo-1.3B,hendrycksTest-prehistory,5-shot,accuracy,0.2314814814814814,0.0234684298324511
EleutherAI/gpt-neo-1.3B,hendrycksTest-prehistory,5-shot,acc_norm,0.2314814814814814,0.0234684298324511
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2375886524822695,0.0253895125527299
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2375886524822695,0.0253895125527299
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_law,5-shot,accuracy,0.2496740547588005,0.0110545383778323
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_law,5-shot,acc_norm,0.2496740547588005,0.0110545383778323
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_medicine,5-shot,accuracy,0.1654411764705882,0.0225717710254947
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1654411764705882,0.0225717710254947
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2483660130718954,0.0174794870013647
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2483660130718954,0.0174794870013647
EleutherAI/gpt-neo-1.3B,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/gpt-neo-1.3B,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
EleutherAI/gpt-neo-1.3B,hendrycksTest-security_studies,5-shot,accuracy,0.2163265306122449,0.026358916334904
EleutherAI/gpt-neo-1.3B,hendrycksTest-security_studies,5-shot,acc_norm,0.2163265306122449,0.026358916334904
EleutherAI/gpt-neo-1.3B,hendrycksTest-sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/gpt-neo-1.3B,hendrycksTest-sociology,5-shot,acc_norm,0.2437810945273631,0.0303604901540146
EleutherAI/gpt-neo-1.3B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-neo-1.3B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/gpt-neo-1.3B,hendrycksTest-virology,5-shot,accuracy,0.2771084337349397,0.0348433159268058
EleutherAI/gpt-neo-1.3B,hendrycksTest-virology,5-shot,acc_norm,0.2771084337349397,0.0348433159268058
EleutherAI/gpt-neo-1.3B,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.0358252944257312
EleutherAI/gpt-neo-1.3B,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.0358252944257312
EleutherAI/gpt-neo-1.3B,truthfulqa:mc,0-shot,mc1,0.2313341493268053,0.0147619451748626
EleutherAI/gpt-neo-1.3B,truthfulqa:mc,0-shot,mc2,0.3962930604436832,0.0142695842532758
EleutherAI/gpt-neo-1.3B,drop,3-shot,accuracy,0.0018875838926174,0.0004445109990559
EleutherAI/gpt-neo-1.3B,drop,3-shot,f1,0.0460465604026847,0.0011927407325477
EleutherAI/gpt-neo-1.3B,gsm8k,5-shot,accuracy,0.0174374526156178,0.0036054868679982
EleutherAI/gpt-neo-1.3B,winogrande,5-shot,accuracy,0.5564325177584846,0.0139626949076204
EleutherAI/pythia-6.9b,minerva_math_precalc,5-shot,accuracy,0.0036630036630036,0.0025877573681934
EleutherAI/pythia-6.9b,minerva_math_prealgebra,5-shot,accuracy,0.0298507462686567,0.0057694876379482
EleutherAI/pythia-6.9b,minerva_math_num_theory,5-shot,accuracy,0.0055555555555555,0.0032015451273209
EleutherAI/pythia-6.9b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0188261351052048,0.0045253304986684
EleutherAI/pythia-6.9b,minerva_math_geometry,5-shot,accuracy,0.0187891440501043,0.0062104164279974
EleutherAI/pythia-6.9b,minerva_math_counting_and_prob,5-shot,accuracy,0.0147679324894514,0.0055462385896684
EleutherAI/pythia-6.9b,minerva_math_algebra,5-shot,accuracy,0.0160067396798652,0.0036442247924417
EleutherAI/pythia-6.9b,fld_default,0-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,fld_star,0-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,arithmetic_3da,5-shot,accuracy,0.007,0.0018647355360237
EleutherAI/pythia-6.9b,arithmetic_3ds,5-shot,accuracy,0.0085,0.0020532859010609
EleutherAI/pythia-6.9b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
EleutherAI/pythia-6.9b,arithmetic_2ds,5-shot,accuracy,0.093,0.0064958908780204
EleutherAI/pythia-6.9b,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,arithmetic_1dc,5-shot,accuracy,0.0565,0.0051640302675624
EleutherAI/pythia-6.9b,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005
EleutherAI/pythia-6.9b,arithmetic_2dm,5-shot,accuracy,0.054,0.0050551733292434
EleutherAI/pythia-6.9b,arithmetic_2da,5-shot,accuracy,0.053,0.0050107937521926
EleutherAI/pythia-6.9b,gsm8k_cot,5-shot,accuracy,0.0295678544351781,0.0046658931342207
EleutherAI/pythia-6.9b,gsm8k,5-shot,accuracy,0.0288097043214556,0.0046074842837674
EleutherAI/pythia-6.9b,anli_r2,0-shot,brier_score,0.8480871011410138,
EleutherAI/pythia-6.9b,anli_r3,0-shot,brier_score,0.8179776021045503,
EleutherAI/pythia-6.9b,anli_r1,0-shot,brier_score,0.8722694615179778,
EleutherAI/pythia-6.9b,xnli_eu,0-shot,brier_score,0.8999839016489077,
EleutherAI/pythia-6.9b,xnli_vi,0-shot,brier_score,0.8204605652395003,
EleutherAI/pythia-6.9b,xnli_ru,0-shot,brier_score,0.8106878224944725,
EleutherAI/pythia-6.9b,xnli_zh,0-shot,brier_score,1.09036580135667,
EleutherAI/pythia-6.9b,xnli_tr,0-shot,brier_score,0.8878553632357027,
EleutherAI/pythia-6.9b,xnli_fr,0-shot,brier_score,0.7961302677354256,
EleutherAI/pythia-6.9b,xnli_en,0-shot,brier_score,0.653954455047553,
EleutherAI/pythia-6.9b,xnli_ur,0-shot,brier_score,1.191915875742246,
EleutherAI/pythia-6.9b,xnli_ar,0-shot,brier_score,1.2272435798439718,
EleutherAI/pythia-6.9b,xnli_de,0-shot,brier_score,0.828333282541149,
EleutherAI/pythia-6.9b,xnli_hi,0-shot,brier_score,0.801853810502381,
EleutherAI/pythia-6.9b,xnli_es,0-shot,brier_score,0.8599668349851329,
EleutherAI/pythia-6.9b,xnli_bg,0-shot,brier_score,0.7686484176938864,
EleutherAI/pythia-6.9b,xnli_sw,0-shot,brier_score,0.8502894613384985,
EleutherAI/pythia-6.9b,xnli_el,0-shot,brier_score,0.8562714843114025,
EleutherAI/pythia-6.9b,xnli_th,0-shot,brier_score,0.7751103774504331,
EleutherAI/pythia-6.9b,logiqa2,0-shot,brier_score,1.165272169859693,
EleutherAI/pythia-6.9b,mathqa,0-shot,brier_score,0.95247880187882,
EleutherAI/pythia-6.9b,lambada_standard,0-shot,perplexity,6.937810377810337,0.1757119002721059
EleutherAI/pythia-6.9b,lambada_standard,0-shot,accuracy,0.5587036677663497,0.0069177998562182
EleutherAI/pythia-6.9b,lambada_openai,0-shot,perplexity,4.459300840447834,0.10000526645129
EleutherAI/pythia-6.9b,lambada_openai,0-shot,accuracy,0.6710653987968174,0.0065455971958505
EleutherAI/pythia-6.9b,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.0356507967070831
EleutherAI/pythia-6.9b,mmlu_formal_logic,0-shot,accuracy,0.2539682539682539,0.0389325961060467
EleutherAI/pythia-6.9b,mmlu_prehistory,0-shot,accuracy,0.2839506172839506,0.0250894785237651
EleutherAI/pythia-6.9b,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
EleutherAI/pythia-6.9b,mmlu_high_school_world_history,0-shot,accuracy,0.2573839662447257,0.0284588209914603
EleutherAI/pythia-6.9b,mmlu_moral_disputes,0-shot,accuracy,0.2774566473988439,0.0241057126077543
EleutherAI/pythia-6.9b,mmlu_professional_law,0-shot,accuracy,0.2483702737940026,0.0110352125980345
EleutherAI/pythia-6.9b,mmlu_logical_fallacies,0-shot,accuracy,0.294478527607362,0.0358116579047408
EleutherAI/pythia-6.9b,mmlu_high_school_us_history,0-shot,accuracy,0.2941176470588235,0.0319800166011507
EleutherAI/pythia-6.9b,mmlu_philosophy,0-shot,accuracy,0.3344051446945337,0.0267954223278939
EleutherAI/pythia-6.9b,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.0441434366685493
EleutherAI/pythia-6.9b,mmlu_international_law,0-shot,accuracy,0.3471074380165289,0.0434572457029253
EleutherAI/pythia-6.9b,mmlu_high_school_european_history,0-shot,accuracy,0.2424242424242424,0.0334640988105595
EleutherAI/pythia-6.9b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2590673575129533,0.0316187791793541
EleutherAI/pythia-6.9b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2184873949579832,0.0268415143229589
EleutherAI/pythia-6.9b,mmlu_high_school_geography,0-shot,accuracy,0.2575757575757575,0.0311562695196468
EleutherAI/pythia-6.9b,mmlu_high_school_psychology,0-shot,accuracy,0.2385321100917431,0.0182725758102318
EleutherAI/pythia-6.9b,mmlu_public_relations,0-shot,accuracy,0.2272727272727272,0.0401396455407277
EleutherAI/pythia-6.9b,mmlu_us_foreign_policy,0-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-6.9b,mmlu_sociology,0-shot,accuracy,0.2686567164179104,0.0313432835820895
EleutherAI/pythia-6.9b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2564102564102564,0.0221390811039715
EleutherAI/pythia-6.9b,mmlu_security_studies,0-shot,accuracy,0.2122448979591836,0.0261769671978667
EleutherAI/pythia-6.9b,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612379
EleutherAI/pythia-6.9b,mmlu_human_sexuality,0-shot,accuracy,0.2290076335877862,0.0368534663171185
EleutherAI/pythia-6.9b,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.0409698513984367
EleutherAI/pythia-6.9b,mmlu_miscellaneous,0-shot,accuracy,0.2669220945083014,0.0158184508947775
EleutherAI/pythia-6.9b,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.0298725777088911
EleutherAI/pythia-6.9b,mmlu_management,0-shot,accuracy,0.2524271844660194,0.0430125039969087
EleutherAI/pythia-6.9b,mmlu_nutrition,0-shot,accuracy,0.2483660130718954,0.0247399813551135
EleutherAI/pythia-6.9b,mmlu_medical_genetics,0-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-6.9b,mmlu_human_aging,0-shot,accuracy,0.1928251121076233,0.0264782409604893
EleutherAI/pythia-6.9b,mmlu_professional_medicine,0-shot,accuracy,0.2095588235294117,0.024723110407677
EleutherAI/pythia-6.9b,mmlu_college_medicine,0-shot,accuracy,0.2312138728323699,0.0321473730202947
EleutherAI/pythia-6.9b,mmlu_business_ethics,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-6.9b,mmlu_clinical_knowledge,0-shot,accuracy,0.2037735849056604,0.0247907845017754
EleutherAI/pythia-6.9b,mmlu_global_facts,0-shot,accuracy,0.33,0.047258156262526
EleutherAI/pythia-6.9b,mmlu_virology,0-shot,accuracy,0.2469879518072289,0.0335735198206453
EleutherAI/pythia-6.9b,mmlu_professional_accounting,0-shot,accuracy,0.226950354609929,0.0249871063656429
EleutherAI/pythia-6.9b,mmlu_college_physics,0-shot,accuracy,0.2352941176470588,0.0422077365917145
EleutherAI/pythia-6.9b,mmlu_high_school_physics,0-shot,accuracy,0.271523178807947,0.0363132980396965
EleutherAI/pythia-6.9b,mmlu_high_school_biology,0-shot,accuracy,0.2419354838709677,0.0243625996930311
EleutherAI/pythia-6.9b,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.0368565109589753
EleutherAI/pythia-6.9b,mmlu_anatomy,0-shot,accuracy,0.3407407407407407,0.0409437626999679
EleutherAI/pythia-6.9b,mmlu_college_chemistry,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-6.9b,mmlu_computer_security,0-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-6.9b,mmlu_college_computer_science,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-6.9b,mmlu_astronomy,0-shot,accuracy,0.3092105263157895,0.0376107086986748
EleutherAI/pythia-6.9b,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-6.9b,mmlu_conceptual_physics,0-shot,accuracy,0.2042553191489361,0.0263551584133494
EleutherAI/pythia-6.9b,mmlu_abstract_algebra,0-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-6.9b,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-6.9b,mmlu_machine_learning,0-shot,accuracy,0.2053571428571428,0.0383424102141907
EleutherAI/pythia-6.9b,mmlu_high_school_chemistry,0-shot,accuracy,0.3004926108374384,0.0322579947623348
EleutherAI/pythia-6.9b,mmlu_high_school_statistics,0-shot,accuracy,0.2916666666666667,0.0309986663045605
EleutherAI/pythia-6.9b,mmlu_elementary_mathematics,0-shot,accuracy,0.2619047619047619,0.0226442126155252
EleutherAI/pythia-6.9b,mmlu_electrical_engineering,0-shot,accuracy,0.3241379310344827,0.0390043206918555
EleutherAI/pythia-6.9b,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
EleutherAI/pythia-6.9b,arc_challenge,25-shot,accuracy,0.3575085324232082,0.0140054942759165
EleutherAI/pythia-6.9b,arc_challenge,25-shot,acc_norm,0.3959044368600682,0.0142912283935365
EleutherAI/pythia-6.9b,hellaswag,10-shot,accuracy,0.4816769567815176,0.0049864298081467
EleutherAI/pythia-6.9b,hellaswag,10-shot,acc_norm,0.6514638518223461,0.0047553292439766
EleutherAI/pythia-6.9b,truthfulqa_mc2,0-shot,accuracy,0.3515928441032545,0.0136221219244483
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,bleu_max,24.82204925180925,0.7441915100142679
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,bleu_acc,0.2876376988984088,0.0158463151013947
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,bleu_diff,-8.72973777939928,0.799560880574795
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge1_max,50.64779573586556,0.8314230941524794
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge1_acc,0.2692778457772338,0.0155285666370872
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge1_diff,-11.027700507612522,0.8410289087382377
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge2_max,33.75326270225714,0.9630024578384584
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge2_acc,0.226438188494492,0.0146513373246025
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge2_diff,-13.190984697589348,1.019946221960148
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rougeL_max,47.78257310566027,0.8446585069903312
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rougeL_acc,0.2594859241126071,0.015345409485558
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rougeL_diff,-11.191544484143854,0.859662499239064
EleutherAI/pythia-6.9b,truthfulqa_mc1,0-shot,accuracy,0.2178702570379437,0.0144508467141238
EleutherAI/pythia-6.9b,winogrande,5-shot,accuracy,0.6400947119179163,0.0134896095902667
EleutherAI/pile-t5-large,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.0358252944257312
EleutherAI/pile-t5-large,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.0404061017820884
EleutherAI/pile-t5-large,mmlu_prehistory,0-shot,accuracy,0.2160493827160493,0.0228991629184458
EleutherAI/pile-t5-large,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705748
EleutherAI/pile-t5-large,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
EleutherAI/pile-t5-large,mmlu_moral_disputes,0-shot,accuracy,0.2485549132947976,0.0232675284321001
EleutherAI/pile-t5-large,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.0109961566351426
EleutherAI/pile-t5-large,mmlu_logical_fallacies,0-shot,accuracy,0.2208588957055214,0.0325917739274217
EleutherAI/pile-t5-large,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.0303915336927415
EleutherAI/pile-t5-large,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/pile-t5-large,mmlu_jurisprudence,0-shot,accuracy,0.2592592592592592,0.0423651125809463
EleutherAI/pile-t5-large,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.0389687898507041
EleutherAI/pile-t5-large,mmlu_high_school_european_history,0-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/pile-t5-large,mmlu_high_school_government_and_politics,0-shot,accuracy,0.1968911917098445,0.0286978739718606
EleutherAI/pile-t5-large,mmlu_high_school_microeconomics,0-shot,accuracy,0.2100840336134453,0.0264613987174718
EleutherAI/pile-t5-large,mmlu_high_school_geography,0-shot,accuracy,0.1767676767676767,0.0271787526390449
EleutherAI/pile-t5-large,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936
EleutherAI/pile-t5-large,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pile-t5-large,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pile-t5-large,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/pile-t5-large,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2025641025641025,0.0203776609703713
EleutherAI/pile-t5-large,mmlu_security_studies,0-shot,accuracy,0.1877551020408163,0.0250002560395462
EleutherAI/pile-t5-large,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
EleutherAI/pile-t5-large,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/pile-t5-large,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
EleutherAI/pile-t5-large,mmlu_miscellaneous,0-shot,accuracy,0.2375478927203065,0.0152187330461501
EleutherAI/pile-t5-large,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674
EleutherAI/pile-t5-large,mmlu_management,0-shot,accuracy,0.174757281553398,0.0376017800602662
EleutherAI/pile-t5-large,mmlu_nutrition,0-shot,accuracy,0.2254901960784313,0.0239291555173512
EleutherAI/pile-t5-large,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pile-t5-large,mmlu_human_aging,0-shot,accuracy,0.3139013452914798,0.0311467964829724
EleutherAI/pile-t5-large,mmlu_professional_medicine,0-shot,accuracy,0.1838235294117647,0.0235292421851931
EleutherAI/pile-t5-large,mmlu_college_medicine,0-shot,accuracy,0.2080924855491329,0.0309528902177498
EleutherAI/pile-t5-large,mmlu_business_ethics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pile-t5-large,mmlu_clinical_knowledge,0-shot,accuracy,0.2150943396226415,0.0252883945028913
EleutherAI/pile-t5-large,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pile-t5-large,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
EleutherAI/pile-t5-large,mmlu_professional_accounting,0-shot,accuracy,0.2340425531914893,0.0252578613594324
EleutherAI/pile-t5-large,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/pile-t5-large,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.0325784738443677
EleutherAI/pile-t5-large,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.0217325406893292
EleutherAI/pile-t5-large,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
EleutherAI/pile-t5-large,mmlu_anatomy,0-shot,accuracy,0.1851851851851851,0.0335567721631314
EleutherAI/pile-t5-large,mmlu_college_chemistry,0-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pile-t5-large,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pile-t5-large,mmlu_college_computer_science,0-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pile-t5-large,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/pile-t5-large,mmlu_college_mathematics,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pile-t5-large,mmlu_conceptual_physics,0-shot,accuracy,0.2638297872340425,0.0288099898541029
EleutherAI/pile-t5-large,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pile-t5-large,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pile-t5-large,mmlu_machine_learning,0-shot,accuracy,0.3125,0.0439946505757152
EleutherAI/pile-t5-large,mmlu_high_school_chemistry,0-shot,accuracy,0.1527093596059113,0.0253089045393806
EleutherAI/pile-t5-large,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.0245363260261342
EleutherAI/pile-t5-large,mmlu_elementary_mathematics,0-shot,accuracy,0.2089947089947089,0.0209404815653348
EleutherAI/pile-t5-large,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.035659981741353
EleutherAI/pile-t5-large,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.024882116857655
EleutherAI/pile-t5-large,arc_challenge,25-shot,accuracy,0.2158703071672355,0.0120229753600306
EleutherAI/pile-t5-large,arc_challenge,25-shot,acc_norm,0.2755972696245733,0.0130571696557618
EleutherAI/pile-t5-large,hellaswag,10-shot,accuracy,0.2823142800238996,0.004492055279407
EleutherAI/pile-t5-large,hellaswag,10-shot,acc_norm,0.3055168293168691,0.0045968459363566
EleutherAI/pile-t5-large,truthfulqa_mc2,0-shot,accuracy,0.4872333237366009,0.0163067633521161
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,bleu_max,1.0499198159270122,0.0487618788928744
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,bleu_acc,0.2619339045287637,0.015392118805015
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,bleu_diff,0.1359949224956114,0.0377919815991862
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge1_max,6.835958235823023,0.2206705505521531
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge1_acc,0.3329253365973072,0.016497402382012
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge1_diff,-0.2033235213298894,0.173658373604581
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge2_max,1.9870130473864616,0.1643772249675612
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge2_acc,0.1199510403916768,0.0113739246583194
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge2_diff,0.225823886283835,0.1327658805426097
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rougeL_max,6.194140588727995,0.2042698877532696
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rougeL_acc,0.3219094247246022,0.0163555676119604
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rougeL_diff,-0.0526910210370656,0.1606242072864941
EleutherAI/pile-t5-large,truthfulqa_mc1,0-shot,accuracy,0.2643818849449204,0.0154382111195225
EleutherAI/pile-t5-large,winogrande,5-shot,accuracy,0.5224940805051302,0.0140382578240598
EleutherAI/pile-t5-large,gsm8k,5-shot,accuracy,0.0106141015921152,0.0028227133223877
NousResearch/Nous-Hermes-2-Yi-34B,arc:challenge,25-shot,accuracy,0.64419795221843,0.0139905711379187
NousResearch/Nous-Hermes-2-Yi-34B,arc:challenge,25-shot,acc_norm,0.6689419795221843,0.0137520624198178
NousResearch/Nous-Hermes-2-Yi-34B,hellaswag,10-shot,accuracy,0.6577375024895439,0.0047349726682996
NousResearch/Nous-Hermes-2-Yi-34B,hellaswag,10-shot,acc_norm,0.8549093806014738,0.0035147239847366
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.49,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.49,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-anatomy,5-shot,accuracy,0.7185185185185186,0.0388500424580025
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-anatomy,5-shot,acc_norm,0.7185185185185186,0.0388500424580025
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-astronomy,5-shot,accuracy,0.8947368421052632,0.0249745334509207
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-astronomy,5-shot,acc_norm,0.8947368421052632,0.0249745334509207
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-business_ethics,5-shot,accuracy,0.78,0.0416333199893226
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-business_ethics,5-shot,acc_norm,0.78,0.0416333199893226
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8,0.0246182981958665
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8,0.0246182981958665
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_biology,5-shot,accuracy,0.9027777777777778,0.0247745162504401
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_biology,5-shot,acc_norm,0.9027777777777778,0.0247745162504401
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_chemistry,5-shot,accuracy,0.51,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.51,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_computer_science,5-shot,accuracy,0.65,0.0479372485441101
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.65,0.0479372485441101
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_mathematics,5-shot,accuracy,0.49,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.49,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_medicine,5-shot,accuracy,0.6994219653179191,0.0349610148119118
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_medicine,5-shot,acc_norm,0.6994219653179191,0.0349610148119118
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_physics,5-shot,accuracy,0.5294117647058824,0.0496657090397852
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_physics,5-shot,acc_norm,0.5294117647058824,0.0496657090397852
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-computer_security,5-shot,accuracy,0.83,0.0377525168068637
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-computer_security,5-shot,acc_norm,0.83,0.0377525168068637
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7914893617021277,0.0265569821178387
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7914893617021277,0.0265569821178387
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-econometrics,5-shot,accuracy,0.5701754385964912,0.0465704726059496
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-econometrics,5-shot,acc_norm,0.5701754385964912,0.0465704726059496
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7724137931034483,0.0349395038013118
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7724137931034483,0.0349395038013118
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.6904761904761905,0.0238095238095238
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.6904761904761905,0.0238095238095238
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-formal_logic,5-shot,accuracy,0.5793650793650794,0.0441543822674374
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5793650793650794,0.0441543822674374
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-global_facts,5-shot,accuracy,0.51,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-global_facts,5-shot,acc_norm,0.51,0.0502418393795691
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_biology,5-shot,accuracy,0.896774193548387,0.0173083812810345
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.896774193548387,0.0173083812810345
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6206896551724138,0.0341396380590623
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6206896551724138,0.0341396380590623
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.84,0.036845294917747
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.84,0.036845294917747
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8787878787878788,0.0254854983733432
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8787878787878788,0.0254854983733432
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_geography,5-shot,accuracy,0.898989898989899,0.0214697355760553
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.898989898989899,0.0214697355760553
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9792746113989638,0.010281417011909
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9792746113989638,0.010281417011909
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8282051282051283,0.0191249036034235
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8282051282051283,0.0191249036034235
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4148148148148148,0.0300398424540692
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4148148148148148,0.0300398424540692
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8529411764705882,0.0230054594466739
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8529411764705882,0.0230054594466739
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_physics,5-shot,accuracy,0.5033112582781457,0.0408239337944965
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5033112582781457,0.0408239337944965
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9211009174311928,0.0115581981137695
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9211009174311928,0.0115581981137695
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6620370370370371,0.0322594135263129
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6620370370370371,0.0322594135263129
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9117647058823528,0.0199073997913169
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9117647058823528,0.0199073997913169
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9071729957805909,0.0188897505509567
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9071729957805909,0.0188897505509567
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_aging,5-shot,accuracy,0.7937219730941704,0.0271571504795638
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_aging,5-shot,acc_norm,0.7937219730941704,0.0271571504795638
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8931297709923665,0.0270965486248837
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8931297709923665,0.0270965486248837
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-international_law,5-shot,accuracy,0.9090909090909092,0.0262431940540738
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-international_law,5-shot,acc_norm,0.9090909090909092,0.0262431940540738
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8981481481481481,0.0292392726756327
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8981481481481481,0.0292392726756327
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8711656441717791,0.0263213831987836
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8711656441717791,0.0263213831987836
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-machine_learning,5-shot,accuracy,0.6071428571428571,0.0463555013560997
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-machine_learning,5-shot,acc_norm,0.6071428571428571,0.0463555013560997
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-management,5-shot,accuracy,0.9223300970873788,0.0265014407847627
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-management,5-shot,acc_norm,0.9223300970873788,0.0265014407847627
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-marketing,5-shot,accuracy,0.9188034188034188,0.0178937849040185
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-marketing,5-shot,acc_norm,0.9188034188034188,0.0178937849040185
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-medical_genetics,5-shot,accuracy,0.86,0.0348735088019777
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.86,0.0348735088019777
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9106002554278416,0.0102030178476883
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9106002554278416,0.0102030178476883
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_disputes,5-shot,accuracy,0.8352601156069365,0.0199710409824422
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8352601156069365,0.0199710409824422
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.7106145251396648,0.0151665445504902
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.7106145251396648,0.0151665445504902
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-nutrition,5-shot,accuracy,0.8431372549019608,0.0208237588375809
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-nutrition,5-shot,acc_norm,0.8431372549019608,0.0208237588375809
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-philosophy,5-shot,accuracy,0.8135048231511254,0.0221224397724807
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-philosophy,5-shot,acc_norm,0.8135048231511254,0.0221224397724807
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-prehistory,5-shot,accuracy,0.8888888888888888,0.0174864327858807
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-prehistory,5-shot,acc_norm,0.8888888888888888,0.0174864327858807
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_accounting,5-shot,accuracy,0.648936170212766,0.0284735012729637
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.648936170212766,0.0284735012729637
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_law,5-shot,accuracy,0.6166883963494133,0.0124176036629011
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_law,5-shot,acc_norm,0.6166883963494133,0.0124176036629011
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8308823529411765,0.022770868010113
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8308823529411765,0.022770868010113
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8235294117647058,0.0154225120662625
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8235294117647058,0.0154225120662625
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-public_relations,5-shot,accuracy,0.7181818181818181,0.0430911870994645
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-public_relations,5-shot,acc_norm,0.7181818181818181,0.0430911870994645
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-security_studies,5-shot,accuracy,0.8408163265306122,0.0234209720691663
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-security_studies,5-shot,acc_norm,0.8408163265306122,0.0234209720691663
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-sociology,5-shot,accuracy,0.8756218905472637,0.0233354017901663
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-sociology,5-shot,acc_norm,0.8756218905472637,0.0233354017901663
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.0272659924344291
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-virology,5-shot,accuracy,0.572289156626506,0.0385159768371853
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-virology,5-shot,acc_norm,0.572289156626506,0.0385159768371853
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-world_religions,5-shot,accuracy,0.8713450292397661,0.0256793427232769
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-world_religions,5-shot,acc_norm,0.8713450292397661,0.0256793427232769
NousResearch/Nous-Hermes-2-Yi-34B,truthfulqa:mc,0-shot,mc1,0.4357405140758874,0.0173583453988631
NousResearch/Nous-Hermes-2-Yi-34B,truthfulqa:mc,0-shot,mc2,0.6037423421940498,0.0148928575835793
NousResearch/Nous-Hermes-2-Yi-34B,winogrande,5-shot,accuracy,0.829518547750592,0.0105690211228259
NousResearch/Nous-Hermes-2-Yi-34B,gsm8k,5-shot,accuracy,0.7005307050796058,0.0126163007355196
cerebras/Cerebras-GPT-2.7B,minerva_math_precalc,5-shot,accuracy,0.0219780219780219,0.0062801549282525
cerebras/Cerebras-GPT-2.7B,minerva_math_prealgebra,5-shot,accuracy,0.0252583237657864,0.005319703220303
cerebras/Cerebras-GPT-2.7B,minerva_math_num_theory,5-shot,accuracy,0.0111111111111111,0.0045150037076946
cerebras/Cerebras-GPT-2.7B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0143964562569213,0.0039662095909102
cerebras/Cerebras-GPT-2.7B,minerva_math_geometry,5-shot,accuracy,0.0167014613778705,0.005861462425818
cerebras/Cerebras-GPT-2.7B,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.0062753625139895
cerebras/Cerebras-GPT-2.7B,minerva_math_algebra,5-shot,accuracy,0.0092670598146588,0.0027823191184888
cerebras/Cerebras-GPT-2.7B,fld_default,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,fld_star,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
cerebras/Cerebras-GPT-2.7B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
cerebras/Cerebras-GPT-2.7B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
cerebras/Cerebras-GPT-2.7B,arithmetic_2ds,5-shot,accuracy,0.0125,0.0024849471787626
cerebras/Cerebras-GPT-2.7B,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_1dc,5-shot,accuracy,0.0135,0.0025811249685072
cerebras/Cerebras-GPT-2.7B,arithmetic_4ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_2dm,5-shot,accuracy,0.022,0.0032807593162018
cerebras/Cerebras-GPT-2.7B,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
cerebras/Cerebras-GPT-2.7B,gsm8k_cot,5-shot,accuracy,0.0272934040940106,0.0044880953802097
cerebras/Cerebras-GPT-2.7B,gsm8k,5-shot,accuracy,0.0045489006823351,0.0018535550440036
cerebras/Cerebras-GPT-2.7B,anli_r2,0-shot,brier_score,0.7441656976970576,
cerebras/Cerebras-GPT-2.7B,anli_r3,0-shot,brier_score,0.7347729274486875,
cerebras/Cerebras-GPT-2.7B,anli_r1,0-shot,brier_score,0.7559520884886023,
cerebras/Cerebras-GPT-2.7B,xnli_eu,0-shot,brier_score,1.1749589886887095,
cerebras/Cerebras-GPT-2.7B,xnli_vi,0-shot,brier_score,0.7928664423474424,
cerebras/Cerebras-GPT-2.7B,xnli_ru,0-shot,brier_score,0.7994578741034748,
cerebras/Cerebras-GPT-2.7B,xnli_zh,0-shot,brier_score,0.940473825271897,
cerebras/Cerebras-GPT-2.7B,xnli_tr,0-shot,brier_score,0.8423086576456662,
cerebras/Cerebras-GPT-2.7B,xnli_fr,0-shot,brier_score,0.8332420930715818,
cerebras/Cerebras-GPT-2.7B,xnli_en,0-shot,brier_score,0.6809222796174058,
cerebras/Cerebras-GPT-2.7B,xnli_ur,0-shot,brier_score,1.016116798585533,
cerebras/Cerebras-GPT-2.7B,xnli_ar,0-shot,brier_score,1.0259751211437746,
cerebras/Cerebras-GPT-2.7B,xnli_de,0-shot,brier_score,0.9495039241411009,
cerebras/Cerebras-GPT-2.7B,xnli_hi,0-shot,brier_score,0.7931095115018697,
cerebras/Cerebras-GPT-2.7B,xnli_es,0-shot,brier_score,0.8553823700128562,
cerebras/Cerebras-GPT-2.7B,xnli_bg,0-shot,brier_score,0.7603246399463625,
cerebras/Cerebras-GPT-2.7B,xnli_sw,0-shot,brier_score,0.8740183453743544,
cerebras/Cerebras-GPT-2.7B,xnli_el,0-shot,brier_score,1.1103931638852953,
cerebras/Cerebras-GPT-2.7B,xnli_th,0-shot,brier_score,0.8312683414736515,
cerebras/Cerebras-GPT-2.7B,logiqa2,0-shot,brier_score,1.152721216889245,
cerebras/Cerebras-GPT-2.7B,mathqa,0-shot,brier_score,0.9819685448386692,
cerebras/Cerebras-GPT-2.7B,lambada_standard,0-shot,perplexity,12.37585271202106,0.3686403362745451
cerebras/Cerebras-GPT-2.7B,lambada_standard,0-shot,accuracy,0.4680768484378032,0.0069517652757561
cerebras/Cerebras-GPT-2.7B,lambada_openai,0-shot,perplexity,7.735706433404285,0.2096390884531938
cerebras/Cerebras-GPT-2.7B,lambada_openai,0-shot,accuracy,0.5651077042499515,0.0069066674236192
cerebras/Cerebras-GPT-2.7B,mmlu_world_religions,0-shot,accuracy,0.2690058479532163,0.0340105262010409
cerebras/Cerebras-GPT-2.7B,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.0371848900681811
cerebras/Cerebras-GPT-2.7B,mmlu_prehistory,0-shot,accuracy,0.2376543209876543,0.0236835918370085
cerebras/Cerebras-GPT-2.7B,mmlu_moral_scenarios,0-shot,accuracy,0.2715083798882681,0.0148742521680952
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_world_history,0-shot,accuracy,0.2573839662447257,0.0284588209914602
cerebras/Cerebras-GPT-2.7B,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763,0.0238680032625001
cerebras/Cerebras-GPT-2.7B,mmlu_professional_law,0-shot,accuracy,0.2633637548891786,0.0112495064036052
cerebras/Cerebras-GPT-2.7B,mmlu_logical_fallacies,0-shot,accuracy,0.3190184049079754,0.0366199755107383
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_us_history,0-shot,accuracy,0.2696078431372549,0.0311455706594867
cerebras/Cerebras-GPT-2.7B,mmlu_philosophy,0-shot,accuracy,0.270096463022508,0.0252180403734106
cerebras/Cerebras-GPT-2.7B,mmlu_jurisprudence,0-shot,accuracy,0.1851851851851851,0.0375526586503718
cerebras/Cerebras-GPT-2.7B,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.0389687898507041
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_european_history,0-shot,accuracy,0.2242424242424242,0.032568666616811
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2176165803108808,0.0297786630377529
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678638
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_geography,0-shot,accuracy,0.2272727272727272,0.0298575156733864
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_psychology,0-shot,accuracy,0.2330275229357798,0.0181256691808615
cerebras/Cerebras-GPT-2.7B,mmlu_public_relations,0-shot,accuracy,0.1636363636363636,0.0354343305429867
cerebras/Cerebras-GPT-2.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.0456048021572068
cerebras/Cerebras-GPT-2.7B,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.241025641025641,0.0216855466653331
cerebras/Cerebras-GPT-2.7B,mmlu_security_studies,0-shot,accuracy,0.3265306122448979,0.0300210562384403
cerebras/Cerebras-GPT-2.7B,mmlu_professional_psychology,0-shot,accuracy,0.2581699346405229,0.01770453165325
cerebras/Cerebras-GPT-2.7B,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.0372767357559691
cerebras/Cerebras-GPT-2.7B,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
cerebras/Cerebras-GPT-2.7B,mmlu_miscellaneous,0-shot,accuracy,0.2618135376756066,0.0157208386784452
cerebras/Cerebras-GPT-2.7B,mmlu_marketing,0-shot,accuracy,0.2564102564102564,0.0286059537020042
cerebras/Cerebras-GPT-2.7B,mmlu_management,0-shot,accuracy,0.2233009708737864,0.0412355318989143
cerebras/Cerebras-GPT-2.7B,mmlu_nutrition,0-shot,accuracy,0.2679738562091503,0.0253606037962425
cerebras/Cerebras-GPT-2.7B,mmlu_medical_genetics,0-shot,accuracy,0.21,0.0409360180740332
cerebras/Cerebras-GPT-2.7B,mmlu_human_aging,0-shot,accuracy,0.242152466367713,0.0287513923986947
cerebras/Cerebras-GPT-2.7B,mmlu_professional_medicine,0-shot,accuracy,0.25,0.026303648393696
cerebras/Cerebras-GPT-2.7B,mmlu_college_medicine,0-shot,accuracy,0.2312138728323699,0.0321473730202946
cerebras/Cerebras-GPT-2.7B,mmlu_business_ethics,0-shot,accuracy,0.22,0.0416333199893226
cerebras/Cerebras-GPT-2.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.2150943396226415,0.0252883945028913
cerebras/Cerebras-GPT-2.7B,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
cerebras/Cerebras-GPT-2.7B,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
cerebras/Cerebras-GPT-2.7B,mmlu_professional_accounting,0-shot,accuracy,0.2695035460992908,0.0264690368185906
cerebras/Cerebras-GPT-2.7B,mmlu_college_physics,0-shot,accuracy,0.1764705882352941,0.0379328118530781
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.0365860326276374
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_biology,0-shot,accuracy,0.2483870967741935,0.024580028921481
cerebras/Cerebras-GPT-2.7B,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.0371617743756601
cerebras/Cerebras-GPT-2.7B,mmlu_anatomy,0-shot,accuracy,0.2814814814814815,0.0388500424580025
cerebras/Cerebras-GPT-2.7B,mmlu_college_chemistry,0-shot,accuracy,0.19,0.0394277244403662
cerebras/Cerebras-GPT-2.7B,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276808
cerebras/Cerebras-GPT-2.7B,mmlu_college_computer_science,0-shot,accuracy,0.37,0.048523658709391
cerebras/Cerebras-GPT-2.7B,mmlu_astronomy,0-shot,accuracy,0.1973684210526315,0.0323898160169939
cerebras/Cerebras-GPT-2.7B,mmlu_college_mathematics,0-shot,accuracy,0.25,0.0435194139889244
cerebras/Cerebras-GPT-2.7B,mmlu_conceptual_physics,0-shot,accuracy,0.2595744680851063,0.0286591793742923
cerebras/Cerebras-GPT-2.7B,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.042295258468165
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.39,0.0490207130000197
cerebras/Cerebras-GPT-2.7B,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.0449394906861353
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.0310898260029375
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_statistics,0-shot,accuracy,0.2824074074074074,0.0307013721115109
cerebras/Cerebras-GPT-2.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.2037037037037037,0.0207427405601226
cerebras/Cerebras-GPT-2.7B,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.0360010569272777
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.2666666666666666,0.0269624243250738
cerebras/Cerebras-GPT-2.7B,arc_challenge,25-shot,accuracy,0.2610921501706484,0.0128355239094738
cerebras/Cerebras-GPT-2.7B,arc_challenge,25-shot,acc_norm,0.2883959044368601,0.0132383944224281
cerebras/Cerebras-GPT-2.7B,hellaswag,10-shot,accuracy,0.3854809798844851,0.0048571404107767
cerebras/Cerebras-GPT-2.7B,hellaswag,10-shot,acc_norm,0.4929296952798247,0.0049892825160553
cerebras/Cerebras-GPT-2.7B,truthfulqa_mc2,0-shot,accuracy,0.413716384386627,0.0144404989112949
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,bleu_max,23.035243335291693,0.7624195895417043
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,bleu_acc,0.4467564259485924,0.0174039775225571
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,bleu_diff,2.6100154453645934,0.9107727769465376
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge1_max,45.78174774695968,0.9626102741710352
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge1_acc,0.3855569155446756,0.0170388390105916
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge1_diff,3.3839378780303964,1.307720244891136
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge2_max,29.129612331742987,1.1201208139741414
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge2_acc,0.2839657282741738,0.0157853708583967
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge2_diff,2.646752250018767,1.3612360250946929
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rougeL_max,43.71531484324765,0.971141942560561
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rougeL_acc,0.390452876376989,0.0170782307434314
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rougeL_diff,3.717886576479185,1.3006816230973743
cerebras/Cerebras-GPT-2.7B,truthfulqa_mc1,0-shot,accuracy,0.2460220318237454,0.0150772192006625
cerebras/Cerebras-GPT-2.7B,winogrande,5-shot,accuracy,0.5414364640883977,0.0140041468537919
cerebras/Cerebras-GPT-6.7B,minerva_math_precalc,5-shot,accuracy,0.0109890109890109,0.0044656184273314
cerebras/Cerebras-GPT-6.7B,minerva_math_prealgebra,5-shot,accuracy,0.0218140068886337,0.0049524353688735
cerebras/Cerebras-GPT-6.7B,minerva_math_num_theory,5-shot,accuracy,0.0222222222222222,0.0063492063492063
cerebras/Cerebras-GPT-6.7B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0188261351052048,0.0045253304986684
cerebras/Cerebras-GPT-6.7B,minerva_math_geometry,5-shot,accuracy,0.0167014613778705,0.005861462425818
cerebras/Cerebras-GPT-6.7B,minerva_math_counting_and_prob,5-shot,accuracy,0.0126582278481012,0.0051403138895788
cerebras/Cerebras-GPT-6.7B,minerva_math_algebra,5-shot,accuracy,0.0151642796967144,0.0035485460431325
cerebras/Cerebras-GPT-6.7B,fld_default,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,fld_star,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521
cerebras/Cerebras-GPT-6.7B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
cerebras/Cerebras-GPT-6.7B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
cerebras/Cerebras-GPT-6.7B,arithmetic_2ds,5-shot,accuracy,0.016,0.0028064101569415
cerebras/Cerebras-GPT-6.7B,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_1dc,5-shot,accuracy,0.033,0.0039954326099773
cerebras/Cerebras-GPT-6.7B,arithmetic_4ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_2dm,5-shot,accuracy,0.0225,0.0033169829948455
cerebras/Cerebras-GPT-6.7B,arithmetic_2da,5-shot,accuracy,0.0195,0.0030926780189124
cerebras/Cerebras-GPT-6.7B,gsm8k_cot,5-shot,accuracy,0.0326004548900682,0.0048916690219395
cerebras/Cerebras-GPT-6.7B,gsm8k,5-shot,accuracy,0.0053070507960576,0.002001305720948
cerebras/Cerebras-GPT-6.7B,anli_r2,0-shot,brier_score,0.755675118118756,
cerebras/Cerebras-GPT-6.7B,anli_r3,0-shot,brier_score,0.6999705904132336,
cerebras/Cerebras-GPT-6.7B,anli_r1,0-shot,brier_score,0.7536494384689287,
cerebras/Cerebras-GPT-6.7B,xnli_eu,0-shot,brier_score,1.089933422701538,
cerebras/Cerebras-GPT-6.7B,xnli_vi,0-shot,brier_score,0.7966443228842566,
cerebras/Cerebras-GPT-6.7B,xnli_ru,0-shot,brier_score,0.7989806201813834,
cerebras/Cerebras-GPT-6.7B,xnli_zh,0-shot,brier_score,1.0334911487693128,
cerebras/Cerebras-GPT-6.7B,xnli_tr,0-shot,brier_score,0.839021250171503,
cerebras/Cerebras-GPT-6.7B,xnli_fr,0-shot,brier_score,0.8071513485603421,
cerebras/Cerebras-GPT-6.7B,xnli_en,0-shot,brier_score,0.6656814608095787,
cerebras/Cerebras-GPT-6.7B,xnli_ur,0-shot,brier_score,0.9428490629298696,
cerebras/Cerebras-GPT-6.7B,xnli_ar,0-shot,brier_score,1.0302855031988798,
cerebras/Cerebras-GPT-6.7B,xnli_de,0-shot,brier_score,0.8364057760607875,
cerebras/Cerebras-GPT-6.7B,xnli_hi,0-shot,brier_score,0.7590215516967309,
cerebras/Cerebras-GPT-6.7B,xnli_es,0-shot,brier_score,0.8316353906107824,
cerebras/Cerebras-GPT-6.7B,xnli_bg,0-shot,brier_score,0.8758042326498179,
cerebras/Cerebras-GPT-6.7B,xnli_sw,0-shot,brier_score,0.8831334480577933,
cerebras/Cerebras-GPT-6.7B,xnli_el,0-shot,brier_score,0.8987268713940024,
cerebras/Cerebras-GPT-6.7B,xnli_th,0-shot,brier_score,0.8062685621961146,
cerebras/Cerebras-GPT-6.7B,logiqa2,0-shot,brier_score,1.1489525363355495,
cerebras/Cerebras-GPT-6.7B,mathqa,0-shot,brier_score,0.9586676691784484,
cerebras/Cerebras-GPT-6.7B,lambada_standard,0-shot,perplexity,7.734385657556031,0.2042279549336044
cerebras/Cerebras-GPT-6.7B,lambada_standard,0-shot,accuracy,0.548418397050262,0.0069332394704744
cerebras/Cerebras-GPT-6.7B,lambada_openai,0-shot,perplexity,5.350437725308013,0.1331396614029317
cerebras/Cerebras-GPT-6.7B,lambada_openai,0-shot,accuracy,0.6367164758393169,0.0067005116674768
cerebras/Cerebras-GPT-6.7B,mmlu_world_religions,0-shot,accuracy,0.327485380116959,0.0359933577145602
cerebras/Cerebras-GPT-6.7B,mmlu_formal_logic,0-shot,accuracy,0.238095238095238,0.0380952380952381
cerebras/Cerebras-GPT-6.7B,mmlu_prehistory,0-shot,accuracy,0.2407407407407407,0.0237885835516585
cerebras/Cerebras-GPT-6.7B,mmlu_moral_scenarios,0-shot,accuracy,0.2726256983240223,0.0148933917352496
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_world_history,0-shot,accuracy,0.2236286919831223,0.0271232982052299
cerebras/Cerebras-GPT-6.7B,mmlu_moral_disputes,0-shot,accuracy,0.2803468208092485,0.0241824274965776
cerebras/Cerebras-GPT-6.7B,mmlu_professional_law,0-shot,accuracy,0.2698826597131681,0.0113373810842504
cerebras/Cerebras-GPT-6.7B,mmlu_logical_fallacies,0-shot,accuracy,0.2760736196319018,0.0351238528370505
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_us_history,0-shot,accuracy,0.2352941176470588,0.0297717752281456
cerebras/Cerebras-GPT-6.7B,mmlu_philosophy,0-shot,accuracy,0.2604501607717042,0.0249267232248455
cerebras/Cerebras-GPT-6.7B,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
cerebras/Cerebras-GPT-6.7B,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.040261875275912
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_european_history,0-shot,accuracy,0.2727272727272727,0.0347769116216365
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2176165803108808,0.0297786630377529
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2478991596638655,0.0280479672241768
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_geography,0-shot,accuracy,0.1868686868686868,0.0277725333342189
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_psychology,0-shot,accuracy,0.2990825688073394,0.0196304172854151
cerebras/Cerebras-GPT-6.7B,mmlu_public_relations,0-shot,accuracy,0.2454545454545454,0.0412206650287828
cerebras/Cerebras-GPT-6.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.0446196043338473
cerebras/Cerebras-GPT-6.7B,mmlu_sociology,0-shot,accuracy,0.2487562189054726,0.0305676759389167
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2564102564102564,0.0221390811039715
cerebras/Cerebras-GPT-6.7B,mmlu_security_studies,0-shot,accuracy,0.1959183673469387,0.0254093019532256
cerebras/Cerebras-GPT-6.7B,mmlu_professional_psychology,0-shot,accuracy,0.2467320261437908,0.0174408203674024
cerebras/Cerebras-GPT-6.7B,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
cerebras/Cerebras-GPT-6.7B,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
cerebras/Cerebras-GPT-6.7B,mmlu_miscellaneous,0-shot,accuracy,0.264367816091954,0.0157699848406905
cerebras/Cerebras-GPT-6.7B,mmlu_marketing,0-shot,accuracy,0.2393162393162393,0.0279518268089243
cerebras/Cerebras-GPT-6.7B,mmlu_management,0-shot,accuracy,0.2718446601941747,0.0440526802414092
cerebras/Cerebras-GPT-6.7B,mmlu_nutrition,0-shot,accuracy,0.2352941176470588,0.0242886194660461
cerebras/Cerebras-GPT-6.7B,mmlu_medical_genetics,0-shot,accuracy,0.26,0.0440844002276807
cerebras/Cerebras-GPT-6.7B,mmlu_human_aging,0-shot,accuracy,0.2959641255605381,0.0306365913486998
cerebras/Cerebras-GPT-6.7B,mmlu_professional_medicine,0-shot,accuracy,0.4191176470588235,0.0299728071704646
cerebras/Cerebras-GPT-6.7B,mmlu_college_medicine,0-shot,accuracy,0.2427745664739884,0.0326926380614177
cerebras/Cerebras-GPT-6.7B,mmlu_business_ethics,0-shot,accuracy,0.14,0.0348735088019777
cerebras/Cerebras-GPT-6.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.230188679245283,0.0259078971224081
cerebras/Cerebras-GPT-6.7B,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
cerebras/Cerebras-GPT-6.7B,mmlu_virology,0-shot,accuracy,0.2650602409638554,0.0343602403794496
cerebras/Cerebras-GPT-6.7B,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902
cerebras/Cerebras-GPT-6.7B,mmlu_college_physics,0-shot,accuracy,0.2450980392156862,0.0428010583736439
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_physics,0-shot,accuracy,0.271523178807947,0.0363132980396965
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_biology,0-shot,accuracy,0.2193548387096774,0.0235407993587232
cerebras/Cerebras-GPT-6.7B,mmlu_college_biology,0-shot,accuracy,0.2083333333333333,0.0339611620584533
cerebras/Cerebras-GPT-6.7B,mmlu_anatomy,0-shot,accuracy,0.2296296296296296,0.0363338441407346
cerebras/Cerebras-GPT-6.7B,mmlu_college_chemistry,0-shot,accuracy,0.31,0.0464823198711731
cerebras/Cerebras-GPT-6.7B,mmlu_computer_security,0-shot,accuracy,0.21,0.0409360180740332
cerebras/Cerebras-GPT-6.7B,mmlu_college_computer_science,0-shot,accuracy,0.4,0.049236596391733
cerebras/Cerebras-GPT-6.7B,mmlu_astronomy,0-shot,accuracy,0.2565789473684211,0.0355418036802569
cerebras/Cerebras-GPT-6.7B,mmlu_college_mathematics,0-shot,accuracy,0.3,0.0460566186471838
cerebras/Cerebras-GPT-6.7B,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.03047297336338
cerebras/Cerebras-GPT-6.7B,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.0451260859854212
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.37,0.048523658709391
cerebras/Cerebras-GPT-6.7B,mmlu_machine_learning,0-shot,accuracy,0.2321428571428571,0.040073418097558
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.2068965517241379,0.0285013781678939
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_statistics,0-shot,accuracy,0.3425925925925926,0.0323658525260215
cerebras/Cerebras-GPT-6.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.2301587301587301,0.0216792196636931
cerebras/Cerebras-GPT-6.7B,mmlu_electrical_engineering,0-shot,accuracy,0.2551724137931034,0.0363298405270784
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
cerebras/Cerebras-GPT-6.7B,arc_challenge,25-shot,accuracy,0.3242320819112628,0.0136788103995188
cerebras/Cerebras-GPT-6.7B,arc_challenge,25-shot,acc_norm,0.3523890784982935,0.0139601426005986
cerebras/Cerebras-GPT-6.7B,hellaswag,10-shot,accuracy,0.4451304521011751,0.0049596452633902
cerebras/Cerebras-GPT-6.7B,hellaswag,10-shot,acc_norm,0.5936068512248556,0.0049015581323355
cerebras/Cerebras-GPT-6.7B,truthfulqa_mc2,0-shot,accuracy,0.3803445214836313,0.0139290568458103
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,bleu_max,22.34064093494146,0.7183987511971597
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,bleu_acc,0.2741738066095471,0.0156165184972193
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,bleu_diff,-7.446457636161483,0.7565638438274547
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge1_max,48.16517696338372,0.8420427437367002
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge1_acc,0.2717258261933905,0.0155728404528758
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge1_diff,-8.43929405627723,0.823148680564784
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge2_max,31.457102690393747,0.9472192452069558
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge2_acc,0.2288861689106487,0.014706994909055
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge2_diff,-10.247194848412033,0.9595694112827736
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rougeL_max,44.8539232504777,0.8545145349902917
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rougeL_acc,0.2570379436964504,0.015298077509485
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rougeL_diff,-8.674604971448174,0.8352252105721707
cerebras/Cerebras-GPT-6.7B,truthfulqa_mc1,0-shot,accuracy,0.2435740514075887,0.0150263548249107
cerebras/Cerebras-GPT-6.7B,winogrande,5-shot,accuracy,0.5872138910812944,0.0138370606486821
AbacusResearch/jaLLAbi2-7b,minerva_math_precalc,5-shot,accuracy,0.0787545787545787,0.0115379147687345
AbacusResearch/jaLLAbi2-7b,minerva_math_prealgebra,5-shot,accuracy,0.4247990815154994,0.0167587623970092
AbacusResearch/jaLLAbi2-7b,minerva_math_num_theory,5-shot,accuracy,0.15,0.015380154912113
AbacusResearch/jaLLAbi2-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0786267995570321,0.0089618943216255
AbacusResearch/jaLLAbi2-7b,minerva_math_geometry,5-shot,accuracy,0.1920668058455114,0.0180177241850113
AbacusResearch/jaLLAbi2-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.2172995780590717,0.0189625463403293
AbacusResearch/jaLLAbi2-7b,minerva_math_algebra,5-shot,accuracy,0.3201347935973041,0.0135467620421289
AbacusResearch/jaLLAbi2-7b,fld_default,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi2-7b,fld_star,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi2-7b,arithmetic_3da,5-shot,accuracy,0.988,0.0024353573624298
AbacusResearch/jaLLAbi2-7b,arithmetic_3ds,5-shot,accuracy,0.985,0.0027186753387999
AbacusResearch/jaLLAbi2-7b,arithmetic_4da,5-shot,accuracy,0.9445,0.0051208384560778
AbacusResearch/jaLLAbi2-7b,arithmetic_2ds,5-shot,accuracy,0.995,0.0015775754727385
AbacusResearch/jaLLAbi2-7b,arithmetic_5ds,5-shot,accuracy,0.9005,0.0066949448200168
AbacusResearch/jaLLAbi2-7b,arithmetic_5da,5-shot,accuracy,0.9245,0.005909089072508
AbacusResearch/jaLLAbi2-7b,arithmetic_1dc,5-shot,accuracy,0.769,0.0094267667821996
AbacusResearch/jaLLAbi2-7b,arithmetic_4ds,5-shot,accuracy,0.9545,0.0046610876272534
AbacusResearch/jaLLAbi2-7b,arithmetic_2dm,5-shot,accuracy,0.674,0.0104841288850927
AbacusResearch/jaLLAbi2-7b,arithmetic_2da,5-shot,accuracy,1.0,
AbacusResearch/jaLLAbi2-7b,gsm8k_cot,5-shot,accuracy,0.759666413949962,0.0117695807038369
AbacusResearch/jaLLAbi2-7b,gsm8k,5-shot,accuracy,0.7354056103108415,0.0121505540015632
AbacusResearch/jaLLAbi2-7b,anli_r2,0-shot,brier_score,0.7735189327159094,
AbacusResearch/jaLLAbi2-7b,anli_r3,0-shot,brier_score,0.8359426771055016,
AbacusResearch/jaLLAbi2-7b,anli_r1,0-shot,brier_score,0.6783474989688411,
AbacusResearch/jaLLAbi2-7b,xnli_eu,0-shot,brier_score,1.0950678701767658,
AbacusResearch/jaLLAbi2-7b,xnli_vi,0-shot,brier_score,0.985332158114376,
AbacusResearch/jaLLAbi2-7b,xnli_ru,0-shot,brier_score,0.9458423472607614,
AbacusResearch/jaLLAbi2-7b,xnli_zh,0-shot,brier_score,1.0653229084833014,
AbacusResearch/jaLLAbi2-7b,xnli_tr,0-shot,brier_score,1.0516964598791096,
AbacusResearch/jaLLAbi2-7b,xnli_fr,0-shot,brier_score,0.9213564341895442,
AbacusResearch/jaLLAbi2-7b,xnli_en,0-shot,brier_score,0.7308615429117169,
AbacusResearch/jaLLAbi2-7b,xnli_ur,0-shot,brier_score,1.1916179316711006,
AbacusResearch/jaLLAbi2-7b,xnli_ar,0-shot,brier_score,1.2229906458740496,
AbacusResearch/jaLLAbi2-7b,xnli_de,0-shot,brier_score,0.8943690517747402,
AbacusResearch/jaLLAbi2-7b,xnli_hi,0-shot,brier_score,0.9606897711791604,
AbacusResearch/jaLLAbi2-7b,xnli_es,0-shot,brier_score,0.970678317354639,
AbacusResearch/jaLLAbi2-7b,xnli_bg,0-shot,brier_score,0.9716211440198456,
AbacusResearch/jaLLAbi2-7b,xnli_sw,0-shot,brier_score,1.04639506388785,
AbacusResearch/jaLLAbi2-7b,xnli_el,0-shot,brier_score,0.9811536445855956,
AbacusResearch/jaLLAbi2-7b,xnli_th,0-shot,brier_score,1.060485339632959,
AbacusResearch/jaLLAbi2-7b,logiqa2,0-shot,brier_score,0.9724763634284794,
AbacusResearch/jaLLAbi2-7b,mathqa,0-shot,brier_score,0.9560746958174166,
AbacusResearch/jaLLAbi2-7b,lambada_standard,0-shot,perplexity,4.06627682379339,0.1036816188994386
AbacusResearch/jaLLAbi2-7b,lambada_standard,0-shot,accuracy,0.6598098195226082,0.0066005837660637
AbacusResearch/jaLLAbi2-7b,lambada_openai,0-shot,perplexity,3.3960452191861408,0.0779873212935821
AbacusResearch/jaLLAbi2-7b,lambada_openai,0-shot,accuracy,0.7135649136425384,0.0062985694739873
AbacusResearch/jaLLAbi2-7b,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.0283809195961458
AbacusResearch/jaLLAbi2-7b,mmlu_formal_logic,0-shot,accuracy,0.4603174603174603,0.0445802912547097
AbacusResearch/jaLLAbi2-7b,mmlu_prehistory,0-shot,accuracy,0.7283950617283951,0.0247486244905373
AbacusResearch/jaLLAbi2-7b,mmlu_moral_scenarios,0-shot,accuracy,0.4581005586592179,0.0166636832950205
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_world_history,0-shot,accuracy,0.8016877637130801,0.0259550208416211
AbacusResearch/jaLLAbi2-7b,mmlu_moral_disputes,0-shot,accuracy,0.7514450867052023,0.0232675284321001
AbacusResearch/jaLLAbi2-7b,mmlu_professional_law,0-shot,accuracy,0.4680573663624511,0.0127441497048696
AbacusResearch/jaLLAbi2-7b,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_us_history,0-shot,accuracy,0.8529411764705882,0.0248574780802504
AbacusResearch/jaLLAbi2-7b,mmlu_philosophy,0-shot,accuracy,0.7266881028938906,0.0253117659754261
AbacusResearch/jaLLAbi2-7b,mmlu_jurisprudence,0-shot,accuracy,0.7962962962962963,0.0389354251882484
AbacusResearch/jaLLAbi2-7b,mmlu_international_law,0-shot,accuracy,0.7851239669421488,0.0374949244870969
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_european_history,0-shot,accuracy,0.7757575757575758,0.032568666616811
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8860103626943006,0.0229351440539194
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.6722689075630253,0.0304899114176732
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_geography,0-shot,accuracy,0.8232323232323232,0.0271787526390449
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_psychology,0-shot,accuracy,0.8403669724770643,0.0157034983484617
AbacusResearch/jaLLAbi2-7b,mmlu_public_relations,0-shot,accuracy,0.6727272727272727,0.0449429086625209
AbacusResearch/jaLLAbi2-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.86,0.0348735088019776
AbacusResearch/jaLLAbi2-7b,mmlu_sociology,0-shot,accuracy,0.835820895522388,0.0261939235444541
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.658974358974359,0.024035489676335
AbacusResearch/jaLLAbi2-7b,mmlu_security_studies,0-shot,accuracy,0.746938775510204,0.0278330238713997
AbacusResearch/jaLLAbi2-7b,mmlu_professional_psychology,0-shot,accuracy,0.6699346405228758,0.0190237261607245
AbacusResearch/jaLLAbi2-7b,mmlu_human_sexuality,0-shot,accuracy,0.7938931297709924,0.0354777100415946
AbacusResearch/jaLLAbi2-7b,mmlu_econometrics,0-shot,accuracy,0.4912280701754385,0.0470288043204961
AbacusResearch/jaLLAbi2-7b,mmlu_miscellaneous,0-shot,accuracy,0.8250319284802043,0.0135866192199033
AbacusResearch/jaLLAbi2-7b,mmlu_marketing,0-shot,accuracy,0.8803418803418803,0.0212627194004069
AbacusResearch/jaLLAbi2-7b,mmlu_management,0-shot,accuracy,0.7766990291262136,0.0412355318989143
AbacusResearch/jaLLAbi2-7b,mmlu_nutrition,0-shot,accuracy,0.7222222222222222,0.0256468630971379
AbacusResearch/jaLLAbi2-7b,mmlu_medical_genetics,0-shot,accuracy,0.72,0.0451260859854212
AbacusResearch/jaLLAbi2-7b,mmlu_human_aging,0-shot,accuracy,0.6995515695067265,0.0307693520082291
AbacusResearch/jaLLAbi2-7b,mmlu_professional_medicine,0-shot,accuracy,0.6801470588235294,0.0283329595140312
AbacusResearch/jaLLAbi2-7b,mmlu_college_medicine,0-shot,accuracy,0.6647398843930635,0.0359958630124707
AbacusResearch/jaLLAbi2-7b,mmlu_business_ethics,0-shot,accuracy,0.65,0.0479372485441101
AbacusResearch/jaLLAbi2-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.7132075471698113,0.027834912527544
AbacusResearch/jaLLAbi2-7b,mmlu_global_facts,0-shot,accuracy,0.34,0.0476095228569523
AbacusResearch/jaLLAbi2-7b,mmlu_virology,0-shot,accuracy,0.5301204819277109,0.0388542542086676
AbacusResearch/jaLLAbi2-7b,mmlu_professional_accounting,0-shot,accuracy,0.5070921985815603,0.029824498559129
AbacusResearch/jaLLAbi2-7b,mmlu_college_physics,0-shot,accuracy,0.392156862745098,0.0485808357426634
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_physics,0-shot,accuracy,0.3774834437086092,0.0395802723112157
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_biology,0-shot,accuracy,0.7935483870967742,0.0230258996171887
AbacusResearch/jaLLAbi2-7b,mmlu_college_biology,0-shot,accuracy,0.7708333333333334,0.0351469746786238
AbacusResearch/jaLLAbi2-7b,mmlu_anatomy,0-shot,accuracy,0.6518518518518519,0.0411532461033695
AbacusResearch/jaLLAbi2-7b,mmlu_college_chemistry,0-shot,accuracy,0.47,0.0501613558046591
AbacusResearch/jaLLAbi2-7b,mmlu_computer_security,0-shot,accuracy,0.77,0.042295258468165
AbacusResearch/jaLLAbi2-7b,mmlu_college_computer_science,0-shot,accuracy,0.52,0.0502116731568677
AbacusResearch/jaLLAbi2-7b,mmlu_astronomy,0-shot,accuracy,0.7039473684210527,0.037150621549989
AbacusResearch/jaLLAbi2-7b,mmlu_college_mathematics,0-shot,accuracy,0.29,0.0456048021572068
AbacusResearch/jaLLAbi2-7b,mmlu_conceptual_physics,0-shot,accuracy,0.5787234042553191,0.0322783451014626
AbacusResearch/jaLLAbi2-7b,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.0482418151324421
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.7,0.0460566186471838
AbacusResearch/jaLLAbi2-7b,mmlu_machine_learning,0-shot,accuracy,0.4375,0.0470856752188052
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.5123152709359606,0.0351692044422089
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_statistics,0-shot,accuracy,0.4953703703703703,0.0340982551916357
AbacusResearch/jaLLAbi2-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.4047619047619047,0.0252798503974049
AbacusResearch/jaLLAbi2-7b,mmlu_electrical_engineering,0-shot,accuracy,0.5586206896551724,0.0413793103448275
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.3518518518518518,0.029116617606083
AbacusResearch/jaLLAbi2-7b,arc_challenge,25-shot,accuracy,0.6919795221843004,0.013491429517292
AbacusResearch/jaLLAbi2-7b,arc_challenge,25-shot,acc_norm,0.7226962457337884,0.0130820958390593
AbacusResearch/jaLLAbi2-7b,hellaswag,10-shot,accuracy,0.7108145787691695,0.0045245758929529
AbacusResearch/jaLLAbi2-7b,hellaswag,10-shot,acc_norm,0.8833897629954193,0.003202993346991
AbacusResearch/jaLLAbi2-7b,truthfulqa_mc2,0-shot,accuracy,0.7074488603697261,0.0146983116973299
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,bleu_max,16.994584405212713,0.5978346623164609
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,bleu_acc,0.5214198286413708,0.0174874321447116
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,bleu_diff,2.5161099258317203,0.4812096593307796
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge1_max,43.149301424681205,0.7451779721995008
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge1_acc,0.5495716034271726,0.0174172643719676
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge1_diff,4.598718031567308,0.7629498840038131
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge2_max,28.509782535112222,0.8264201912092511
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge2_acc,0.4724602203182375,0.0174769301907121
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge2_diff,4.175284034929332,0.8296229945452628
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rougeL_max,39.81725800184248,0.7558047743596359
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rougeL_acc,0.5483476132190942,0.0174214803002776
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rougeL_diff,4.509539289026502,0.764155345941694
AbacusResearch/jaLLAbi2-7b,truthfulqa_mc1,0-shot,accuracy,0.5520195838433293,0.0174085130634229
AbacusResearch/jaLLAbi2-7b,winogrande,5-shot,accuracy,0.835043409629045,0.0104309174682374
EleutherAI/pythia-70m,drop,3-shot,accuracy,0.0044043624161073,0.0006781451620479
EleutherAI/pythia-70m,drop,3-shot,f1,0.0332938338926174,0.0012298223132837
EleutherAI/pythia-70m,gsm8k,5-shot,accuracy,0.00303260045489,0.0015145735612245
EleutherAI/pythia-70m,winogrande,5-shot,accuracy,0.5146014206787688,0.0140464923832758
EleutherAI/pythia-70m,arc:challenge,25-shot,accuracy,0.1808873720136518,0.011248574467407
EleutherAI/pythia-70m,arc:challenge,25-shot,acc_norm,0.2158703071672355,0.0120229753600306
EleutherAI/pythia-70m,hellaswag,10-shot,accuracy,0.2658832901812388,0.0044089948686501
EleutherAI/pythia-70m,hellaswag,10-shot,acc_norm,0.2728540131447918,0.0044451609976183
EleutherAI/pythia-70m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-70m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/pythia-70m,hendrycksTest-anatomy,5-shot,accuracy,0.2740740740740741,0.03853254836552
EleutherAI/pythia-70m,hendrycksTest-anatomy,5-shot,acc_norm,0.2740740740740741,0.03853254836552
EleutherAI/pythia-70m,hendrycksTest-astronomy,5-shot,accuracy,0.2368421052631578,0.0345977760681053
EleutherAI/pythia-70m,hendrycksTest-astronomy,5-shot,acc_norm,0.2368421052631578,0.0345977760681053
EleutherAI/pythia-70m,hendrycksTest-business_ethics,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-70m,hendrycksTest-business_ethics,5-shot,acc_norm,0.25,0.0435194139889244
EleutherAI/pythia-70m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2415094339622641,0.0263414803711183
EleutherAI/pythia-70m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2415094339622641,0.0263414803711183
EleutherAI/pythia-70m,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.0347659010430413
EleutherAI/pythia-70m,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.0347659010430413
EleutherAI/pythia-70m,hendrycksTest-college_chemistry,5-shot,accuracy,0.4,0.049236596391733
EleutherAI/pythia-70m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.4,0.049236596391733
EleutherAI/pythia-70m,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-70m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.0440844002276807
EleutherAI/pythia-70m,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-70m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-70m,hendrycksTest-college_medicine,5-shot,accuracy,0.2947976878612717,0.0347659960751647
EleutherAI/pythia-70m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2947976878612717,0.0347659960751647
EleutherAI/pythia-70m,hendrycksTest-college_physics,5-shot,accuracy,0.284313725490196,0.0448848285232901
EleutherAI/pythia-70m,hendrycksTest-college_physics,5-shot,acc_norm,0.284313725490196,0.0448848285232901
EleutherAI/pythia-70m,hendrycksTest-computer_security,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-70m,hendrycksTest-computer_security,5-shot,acc_norm,0.25,0.0435194139889244
EleutherAI/pythia-70m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.0289573427883423
EleutherAI/pythia-70m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.0289573427883423
EleutherAI/pythia-70m,hendrycksTest-econometrics,5-shot,accuracy,0.2017543859649122,0.0377520501358363
EleutherAI/pythia-70m,hendrycksTest-econometrics,5-shot,acc_norm,0.2017543859649122,0.0377520501358363
EleutherAI/pythia-70m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2689655172413793,0.0369518331165023
EleutherAI/pythia-70m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2689655172413793,0.0369518331165023
EleutherAI/pythia-70m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.0224945107675031
EleutherAI/pythia-70m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.0224945107675031
EleutherAI/pythia-70m,hendrycksTest-formal_logic,5-shot,accuracy,0.1666666666666666,0.0333333333333333
EleutherAI/pythia-70m,hendrycksTest-formal_logic,5-shot,acc_norm,0.1666666666666666,0.0333333333333333
EleutherAI/pythia-70m,hendrycksTest-global_facts,5-shot,accuracy,0.14,0.0348735088019777
EleutherAI/pythia-70m,hendrycksTest-global_facts,5-shot,acc_norm,0.14,0.0348735088019777
EleutherAI/pythia-70m,hendrycksTest-high_school_biology,5-shot,accuracy,0.267741935483871,0.0251890066602123
EleutherAI/pythia-70m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.267741935483871,0.0251890066602123
EleutherAI/pythia-70m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2807881773399014,0.0316185633535861
EleutherAI/pythia-70m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2807881773399014,0.0316185633535861
EleutherAI/pythia-70m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-70m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/pythia-70m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2666666666666666,0.0345313180188541
EleutherAI/pythia-70m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2666666666666666,0.0345313180188541
EleutherAI/pythia-70m,hendrycksTest-high_school_geography,5-shot,accuracy,0.3131313131313131,0.0330420508781365
EleutherAI/pythia-70m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3131313131313131,0.0330420508781365
EleutherAI/pythia-70m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3471502590673575,0.0343569616836135
EleutherAI/pythia-70m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3471502590673575,0.0343569616836135
EleutherAI/pythia-70m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3307692307692307,0.0238547956809711
EleutherAI/pythia-70m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3307692307692307,0.0238547956809711
EleutherAI/pythia-70m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2851851851851852,0.0275285992103404
EleutherAI/pythia-70m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2851851851851852,0.0275285992103404
EleutherAI/pythia-70m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.0288013921936312
EleutherAI/pythia-70m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.0288013921936312
EleutherAI/pythia-70m,hendrycksTest-high_school_physics,5-shot,accuracy,0.2913907284768212,0.0371018572611999
EleutherAI/pythia-70m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2913907284768212,0.0371018572611999
EleutherAI/pythia-70m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2587155963302752,0.0187760523196196
EleutherAI/pythia-70m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2587155963302752,0.0187760523196196
EleutherAI/pythia-70m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4675925925925926,0.0340280158135896
EleutherAI/pythia-70m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4675925925925926,0.0340280158135896
EleutherAI/pythia-70m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2352941176470588,0.0297717752281456
EleutherAI/pythia-70m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2352941176470588,0.0297717752281456
EleutherAI/pythia-70m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2573839662447257,0.0284588209914602
EleutherAI/pythia-70m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2573839662447257,0.0284588209914602
EleutherAI/pythia-70m,hendrycksTest-human_aging,5-shot,accuracy,0.273542600896861,0.0299185867077988
EleutherAI/pythia-70m,hendrycksTest-human_aging,5-shot,acc_norm,0.273542600896861,0.0299185867077988
EleutherAI/pythia-70m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2748091603053435,0.0391534540884783
EleutherAI/pythia-70m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2748091603053435,0.0391534540884783
EleutherAI/pythia-70m,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.0389687898507041
EleutherAI/pythia-70m,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.0389687898507041
EleutherAI/pythia-70m,hendrycksTest-jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
EleutherAI/pythia-70m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2314814814814814,0.0407749470925262
EleutherAI/pythia-70m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2699386503067484,0.0348782516849789
EleutherAI/pythia-70m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2699386503067484,0.0348782516849789
EleutherAI/pythia-70m,hendrycksTest-machine_learning,5-shot,accuracy,0.2142857142857142,0.0389464112004479
EleutherAI/pythia-70m,hendrycksTest-machine_learning,5-shot,acc_norm,0.2142857142857142,0.0389464112004479
EleutherAI/pythia-70m,hendrycksTest-management,5-shot,accuracy,0.1844660194174757,0.0384042362728827
EleutherAI/pythia-70m,hendrycksTest-management,5-shot,acc_norm,0.1844660194174757,0.0384042362728827
EleutherAI/pythia-70m,hendrycksTest-marketing,5-shot,accuracy,0.1965811965811965,0.0260353860989512
EleutherAI/pythia-70m,hendrycksTest-marketing,5-shot,acc_norm,0.1965811965811965,0.0260353860989512
EleutherAI/pythia-70m,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-70m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/pythia-70m,hendrycksTest-miscellaneous,5-shot,accuracy,0.2171136653895274,0.0147431253948232
EleutherAI/pythia-70m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2171136653895274,0.0147431253948232
EleutherAI/pythia-70m,hendrycksTest-moral_disputes,5-shot,accuracy,0.245664739884393,0.023176298203992
EleutherAI/pythia-70m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.245664739884393,0.023176298203992
EleutherAI/pythia-70m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2324022346368715,0.0141259687546734
EleutherAI/pythia-70m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2324022346368715,0.0141259687546734
EleutherAI/pythia-70m,hendrycksTest-nutrition,5-shot,accuracy,0.2483660130718954,0.0247399813551135
EleutherAI/pythia-70m,hendrycksTest-nutrition,5-shot,acc_norm,0.2483660130718954,0.0247399813551135
EleutherAI/pythia-70m,hendrycksTest-philosophy,5-shot,accuracy,0.2186495176848874,0.0234755814178611
EleutherAI/pythia-70m,hendrycksTest-philosophy,5-shot,acc_norm,0.2186495176848874,0.0234755814178611
EleutherAI/pythia-70m,hendrycksTest-prehistory,5-shot,accuracy,0.2407407407407407,0.0237885835516585
EleutherAI/pythia-70m,hendrycksTest-prehistory,5-shot,acc_norm,0.2407407407407407,0.0237885835516585
EleutherAI/pythia-70m,hendrycksTest-professional_accounting,5-shot,accuracy,0.2163120567375886,0.0245617205605627
EleutherAI/pythia-70m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2163120567375886,0.0245617205605627
EleutherAI/pythia-70m,hendrycksTest-professional_law,5-shot,accuracy,0.2346805736636245,0.0108240268724493
EleutherAI/pythia-70m,hendrycksTest-professional_law,5-shot,acc_norm,0.2346805736636245,0.0108240268724493
EleutherAI/pythia-70m,hendrycksTest-professional_medicine,5-shot,accuracy,0.3419117647058823,0.0288147224222541
EleutherAI/pythia-70m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3419117647058823,0.0288147224222541
EleutherAI/pythia-70m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2173202614379085,0.0166848209291486
EleutherAI/pythia-70m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2173202614379085,0.0166848209291486
EleutherAI/pythia-70m,hendrycksTest-public_relations,5-shot,accuracy,0.2636363636363636,0.0422022469297198
EleutherAI/pythia-70m,hendrycksTest-public_relations,5-shot,acc_norm,0.2636363636363636,0.0422022469297198
EleutherAI/pythia-70m,hendrycksTest-security_studies,5-shot,accuracy,0.273469387755102,0.0285355603371284
EleutherAI/pythia-70m,hendrycksTest-security_studies,5-shot,acc_norm,0.273469387755102,0.0285355603371284
EleutherAI/pythia-70m,hendrycksTest-sociology,5-shot,accuracy,0.2786069651741293,0.031700561834973
EleutherAI/pythia-70m,hendrycksTest-sociology,5-shot,acc_norm,0.2786069651741293,0.031700561834973
EleutherAI/pythia-70m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-70m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/pythia-70m,hendrycksTest-virology,5-shot,accuracy,0.2409638554216867,0.0332939411907352
EleutherAI/pythia-70m,hendrycksTest-virology,5-shot,acc_norm,0.2409638554216867,0.0332939411907352
EleutherAI/pythia-70m,hendrycksTest-world_religions,5-shot,accuracy,0.2222222222222222,0.0318857801768639
EleutherAI/pythia-70m,hendrycksTest-world_religions,5-shot,acc_norm,0.2222222222222222,0.0318857801768639
EleutherAI/pythia-70m,truthfulqa:mc,0-shot,mc1,0.237454100367197,0.0148962774410418
EleutherAI/pythia-70m,truthfulqa:mc,0-shot,mc2,0.4706464468379622,0.0155037547033522
llama2_220M_nl_only_shuf-hf,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.03565079670708313
llama2_220M_nl_only_shuf-hf,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.03970158273235172
llama2_220M_nl_only_shuf-hf,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
llama2_220M_nl_only_shuf-hf,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
llama2_220M_nl_only_shuf-hf,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
llama2_220M_nl_only_shuf-hf,mmlu_moral_disputes,0-shot,accuracy,0.23699421965317918,0.02289408248992599
llama2_220M_nl_only_shuf-hf,mmlu_professional_law,0-shot,accuracy,0.24511082138200782,0.010986307870045505
llama2_220M_nl_only_shuf-hf,mmlu_logical_fallacies,0-shot,accuracy,0.22699386503067484,0.03291099578615768
llama2_220M_nl_only_shuf-hf,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
llama2_220M_nl_only_shuf-hf,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
llama2_220M_nl_only_shuf-hf,mmlu_jurisprudence,0-shot,accuracy,0.26851851851851855,0.04284467968052192
llama2_220M_nl_only_shuf-hf,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794088
llama2_220M_nl_only_shuf-hf,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
llama2_220M_nl_only_shuf-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
llama2_220M_nl_only_shuf-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
llama2_220M_nl_only_shuf-hf,mmlu_high_school_geography,0-shot,accuracy,0.18181818181818182,0.027479603010538808
llama2_220M_nl_only_shuf-hf,mmlu_high_school_psychology,0-shot,accuracy,0.1889908256880734,0.016785481159203624
llama2_220M_nl_only_shuf-hf,mmlu_public_relations,0-shot,accuracy,0.20909090909090908,0.03895091015724137
llama2_220M_nl_only_shuf-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
llama2_220M_nl_only_shuf-hf,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
llama2_220M_nl_only_shuf-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20512820512820512,0.020473233173551972
llama2_220M_nl_only_shuf-hf,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
llama2_220M_nl_only_shuf-hf,mmlu_professional_psychology,0-shot,accuracy,0.24836601307189543,0.017479487001364764
llama2_220M_nl_only_shuf-hf,mmlu_human_sexuality,0-shot,accuracy,0.26717557251908397,0.038808483010823965
llama2_220M_nl_only_shuf-hf,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
llama2_220M_nl_only_shuf-hf,mmlu_miscellaneous,0-shot,accuracy,0.23243933588761176,0.015104550008905709
llama2_220M_nl_only_shuf-hf,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
llama2_220M_nl_only_shuf-hf,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
llama2_220M_nl_only_shuf-hf,mmlu_nutrition,0-shot,accuracy,0.21568627450980393,0.02355083135199509
llama2_220M_nl_only_shuf-hf,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_only_shuf-hf,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.031024411740572206
llama2_220M_nl_only_shuf-hf,mmlu_professional_medicine,0-shot,accuracy,0.1801470588235294,0.023345163616544866
llama2_220M_nl_only_shuf-hf,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.03126511206173043
llama2_220M_nl_only_shuf-hf,mmlu_business_ethics,0-shot,accuracy,0.28,0.04512608598542127
llama2_220M_nl_only_shuf-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.2188679245283019,0.025447863825108614
llama2_220M_nl_only_shuf-hf,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
llama2_220M_nl_only_shuf-hf,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
llama2_220M_nl_only_shuf-hf,mmlu_professional_accounting,0-shot,accuracy,0.23049645390070922,0.025123739226872402
llama2_220M_nl_only_shuf-hf,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
llama2_220M_nl_only_shuf-hf,mmlu_high_school_physics,0-shot,accuracy,0.19205298013245034,0.032162984205936135
llama2_220M_nl_only_shuf-hf,mmlu_high_school_biology,0-shot,accuracy,0.1870967741935484,0.022185710092252262
llama2_220M_nl_only_shuf-hf,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
llama2_220M_nl_only_shuf-hf,mmlu_anatomy,0-shot,accuracy,0.2,0.03455473702325437
llama2_220M_nl_only_shuf-hf,mmlu_college_chemistry,0-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_only_shuf-hf,mmlu_computer_security,0-shot,accuracy,0.29,0.045604802157206845
llama2_220M_nl_only_shuf-hf,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
llama2_220M_nl_only_shuf-hf,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_only_shuf-hf,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_only_shuf-hf,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
llama2_220M_nl_only_shuf-hf,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_only_shuf-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
llama2_220M_nl_only_shuf-hf,mmlu_machine_learning,0-shot,accuracy,0.32142857142857145,0.04432804055291518
llama2_220M_nl_only_shuf-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.15763546798029557,0.0256390141311724
llama2_220M_nl_only_shuf-hf,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
llama2_220M_nl_only_shuf-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
llama2_220M_nl_only_shuf-hf,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
llama2_220M_nl_only_shuf-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
llama2_220M_nl_only_shuf-hf,arc_challenge,25-shot,accuracy,0.18344709897610922,0.011310170179554536
llama2_220M_nl_only_shuf-hf,arc_challenge,25-shot,acc_norm,0.22440273037542663,0.012191404938603843
llama2_220M_nl_only_shuf-hf,hellaswag,10-shot,accuracy,0.3064130651264688,0.004600612000422661
llama2_220M_nl_only_shuf-hf,hellaswag,10-shot,acc_norm,0.3488348934475204,0.004756275875018261
llama2_220M_nl_only_shuf-hf,truthfulqa_mc2,0-shot,accuracy,0.42516004967159166,0.014980155972848238
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,bleu_max,13.245193057292553,0.5392567154704236
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,bleu_acc,0.31211750305997554,0.01622075676952096
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,bleu_diff,-2.1038506421564227,0.4272897488054712
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rouge1_max,32.83986416573393,0.797202455988802
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rouge1_acc,0.2876376988984088,0.015846315101394788
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rouge1_diff,-5.300496494078777,0.5971929544496779
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rouge2_max,16.52423367986746,0.7783960296759083
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rouge2_acc,0.18849449204406366,0.013691467148835375
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rouge2_diff,-4.270342180737802,0.6210440741887262
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rougeL_max,30.09753561970282,0.7730685592985539
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rougeL_acc,0.2913096695226438,0.01590598704818482
llama2_220M_nl_only_shuf-hf,truthfulqa_gen,0-shot,rougeL_diff,-5.181069664548507,0.5868656105739014
llama2_220M_nl_only_shuf-hf,truthfulqa_mc1,0-shot,accuracy,0.23990208078335373,0.014948812679062133
llama2_220M_nl_only_shuf-hf,winogrande,5-shot,accuracy,0.5295974743488555,0.014027843827840083
meta-llama/Llama-2-70b-chat-hf,drop,3-shot,accuracy,0.040373322147651,0.0020157564185176
meta-llama/Llama-2-70b-chat-hf,drop,3-shot,f1,0.1050272651006715,0.0023756238577676
meta-llama/Llama-2-70b-chat-hf,gsm8k,5-shot,accuracy,0.266868840030326,0.0121837805518879
meta-llama/Llama-2-70b-chat-hf,winogrande,5-shot,accuracy,0.8050513022888713,0.0111340994159382
huggyllama/llama-7b,arc:challenge,25-shot,accuracy,0.4769624573378839,0.0145958732053582
huggyllama/llama-7b,arc:challenge,25-shot,acc_norm,0.5093856655290102,0.014608816322065
huggyllama/llama-7b,hellaswag,10-shot,accuracy,0.5753833897629954,0.0049327450130727
huggyllama/llama-7b,hellaswag,10-shot,acc_norm,0.7781318462457678,0.0041465374881357
huggyllama/llama-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.26,0.0440844002276808
huggyllama/llama-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.26,0.0440844002276808
huggyllama/llama-7b,hendrycksTest-anatomy,5-shot,accuracy,0.3851851851851852,0.0420392104015627
huggyllama/llama-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.3851851851851852,0.0420392104015627
huggyllama/llama-7b,hendrycksTest-astronomy,5-shot,accuracy,0.3421052631578947,0.0386073159931609
huggyllama/llama-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.3421052631578947,0.0386073159931609
huggyllama/llama-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.41,0.049431107042371
huggyllama/llama-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.41,0.049431107042371
huggyllama/llama-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3547169811320754,0.0294451753281995
huggyllama/llama-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3547169811320754,0.0294451753281995
huggyllama/llama-7b,hendrycksTest-college_biology,5-shot,accuracy,0.375,0.0404843922269559
huggyllama/llama-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.375,0.0404843922269559
huggyllama/llama-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.3,0.0460566186471838
huggyllama/llama-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.3,0.0460566186471838
huggyllama/llama-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.31,0.0464823198711731
huggyllama/llama-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.31,0.0464823198711731
huggyllama/llama-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.33,0.047258156262526
huggyllama/llama-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.33,0.047258156262526
huggyllama/llama-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.3352601156069364,0.0359958630124707
huggyllama/llama-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.3352601156069364,0.0359958630124707
huggyllama/llama-7b,hendrycksTest-college_physics,5-shot,accuracy,0.2352941176470588,0.0422077365917145
huggyllama/llama-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.2352941176470588,0.0422077365917145
huggyllama/llama-7b,hendrycksTest-computer_security,5-shot,accuracy,0.45,0.05
huggyllama/llama-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.45,0.05
huggyllama/llama-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3702127659574468,0.0315656468223678
huggyllama/llama-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3702127659574468,0.0315656468223678
huggyllama/llama-7b,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
huggyllama/llama-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
huggyllama/llama-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2275862068965517,0.0349395038013118
huggyllama/llama-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2275862068965517,0.0349395038013118
huggyllama/llama-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2619047619047619,0.0226442126155252
huggyllama/llama-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2619047619047619,0.0226442126155252
huggyllama/llama-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.2619047619047619,0.0393253768039287
huggyllama/llama-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2619047619047619,0.0393253768039287
huggyllama/llama-7b,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
huggyllama/llama-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
huggyllama/llama-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.332258064516129,0.0267955608481228
huggyllama/llama-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.332258064516129,0.0267955608481228
huggyllama/llama-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3004926108374384,0.0322579947623348
huggyllama/llama-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3004926108374384,0.0322579947623348
huggyllama/llama-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
huggyllama/llama-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526
huggyllama/llama-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.4363636363636363,0.0387259298352475
huggyllama/llama-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.4363636363636363,0.0387259298352475
huggyllama/llama-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.3333333333333333,0.0335861814573252
huggyllama/llama-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3333333333333333,0.0335861814573252
huggyllama/llama-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.4455958549222797,0.0358701498607566
huggyllama/llama-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.4455958549222797,0.0358701498607566
huggyllama/llama-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3435897435897436,0.0240786965806354
huggyllama/llama-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3435897435897436,0.0240786965806354
huggyllama/llama-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
huggyllama/llama-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2592592592592592,0.0267192407837121
huggyllama/llama-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3361344537815126,0.0306847371151353
huggyllama/llama-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3361344537815126,0.0306847371151353
huggyllama/llama-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
huggyllama/llama-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
huggyllama/llama-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.473394495412844,0.0214069526881515
huggyllama/llama-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.473394495412844,0.0214069526881515
huggyllama/llama-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3055555555555556,0.0314155462940254
huggyllama/llama-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3055555555555556,0.0314155462940254
huggyllama/llama-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3578431372549019,0.0336448728608829
huggyllama/llama-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3578431372549019,0.0336448728608829
huggyllama/llama-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.430379746835443,0.0322301719593759
huggyllama/llama-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.430379746835443,0.0322301719593759
huggyllama/llama-7b,hendrycksTest-human_aging,5-shot,accuracy,0.3946188340807174,0.0328040050475529
huggyllama/llama-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.3946188340807174,0.0328040050475529
huggyllama/llama-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.3511450381679389,0.0418644516301375
huggyllama/llama-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3511450381679389,0.0418644516301375
huggyllama/llama-7b,hendrycksTest-international_law,5-shot,accuracy,0.5206611570247934,0.0456045608638723
huggyllama/llama-7b,hendrycksTest-international_law,5-shot,acc_norm,0.5206611570247934,0.0456045608638723
huggyllama/llama-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.4166666666666667,0.0476607516535646
huggyllama/llama-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.4166666666666667,0.0476607516535646
huggyllama/llama-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.4294478527607362,0.0388906661911272
huggyllama/llama-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.4294478527607362,0.0388906661911272
huggyllama/llama-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.2767857142857143,0.0424662433669762
huggyllama/llama-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.2767857142857143,0.0424662433669762
huggyllama/llama-7b,hendrycksTest-management,5-shot,accuracy,0.3300970873786408,0.0465614711001235
huggyllama/llama-7b,hendrycksTest-management,5-shot,acc_norm,0.3300970873786408,0.0465614711001235
huggyllama/llama-7b,hendrycksTest-marketing,5-shot,accuracy,0.4786324786324786,0.0327261644763495
huggyllama/llama-7b,hendrycksTest-marketing,5-shot,acc_norm,0.4786324786324786,0.0327261644763495
huggyllama/llama-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.37,0.048523658709391
huggyllama/llama-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.37,0.048523658709391
huggyllama/llama-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.425287356321839,0.0176792254894314
huggyllama/llama-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.425287356321839,0.0176792254894314
huggyllama/llama-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.3959537572254335,0.0263298133419462
huggyllama/llama-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3959537572254335,0.0263298133419462
huggyllama/llama-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
huggyllama/llama-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
huggyllama/llama-7b,hendrycksTest-nutrition,5-shot,accuracy,0.3888888888888889,0.027914055510468
huggyllama/llama-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.3888888888888889,0.027914055510468
huggyllama/llama-7b,hendrycksTest-philosophy,5-shot,accuracy,0.3954983922829582,0.0277709185314278
huggyllama/llama-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.3954983922829582,0.0277709185314278
huggyllama/llama-7b,hendrycksTest-prehistory,5-shot,accuracy,0.345679012345679,0.0264624877770018
huggyllama/llama-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.345679012345679,0.0264624877770018
huggyllama/llama-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2695035460992908,0.0264690368185906
huggyllama/llama-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2695035460992908,0.0264690368185906
huggyllama/llama-7b,hendrycksTest-professional_law,5-shot,accuracy,0.2959582790091264,0.011658518525277
huggyllama/llama-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2959582790091264,0.011658518525277
huggyllama/llama-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.4411764705882353,0.0301619119307671
huggyllama/llama-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4411764705882353,0.0301619119307671
huggyllama/llama-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.3578431372549019,0.0193930584023554
huggyllama/llama-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.3578431372549019,0.0193930584023554
huggyllama/llama-7b,hendrycksTest-public_relations,5-shot,accuracy,0.4181818181818181,0.0472457740573157
huggyllama/llama-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.4181818181818181,0.0472457740573157
huggyllama/llama-7b,hendrycksTest-security_studies,5-shot,accuracy,0.3428571428571428,0.0303872629195477
huggyllama/llama-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.3428571428571428,0.0303872629195477
huggyllama/llama-7b,hendrycksTest-sociology,5-shot,accuracy,0.4676616915422885,0.035281314729336
huggyllama/llama-7b,hendrycksTest-sociology,5-shot,acc_norm,0.4676616915422885,0.035281314729336
huggyllama/llama-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.43,0.0497569851956242
huggyllama/llama-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.43,0.0497569851956242
huggyllama/llama-7b,hendrycksTest-virology,5-shot,accuracy,0.3433734939759036,0.036965843170106
huggyllama/llama-7b,hendrycksTest-virology,5-shot,acc_norm,0.3433734939759036,0.036965843170106
huggyllama/llama-7b,hendrycksTest-world_religions,5-shot,accuracy,0.4853801169590643,0.0383318527521302
huggyllama/llama-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.4853801169590643,0.0383318527521302
huggyllama/llama-7b,truthfulqa:mc,0-shot,mc1,0.2215422276621787,0.0145378676013011
huggyllama/llama-7b,truthfulqa:mc,0-shot,mc2,0.3432793294414406,0.0131884610627696
huggyllama/llama-7b,winogrande,5-shot,accuracy,0.7142857142857143,0.0126965318700386
huggyllama/llama-7b,gsm8k,5-shot,accuracy,0.0871872630780894,0.0077706914167835
huggyllama/llama-7b,minerva_math_precalc,5-shot,accuracy,0.0238095238095238,0.0065304692197614
huggyllama/llama-7b,minerva_math_prealgebra,5-shot,accuracy,0.0585533869115958,0.0079600213562337
huggyllama/llama-7b,minerva_math_num_theory,5-shot,accuracy,0.0166666666666666,0.0055141728150896
huggyllama/llama-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0276854928017718,0.0054629376422426
huggyllama/llama-7b,minerva_math_geometry,5-shot,accuracy,0.0354906054279749,0.0084624474874399
huggyllama/llama-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.0337552742616033,0.0083039326531269
huggyllama/llama-7b,minerva_math_algebra,5-shot,accuracy,0.029486099410278,0.004912099985374
huggyllama/llama-7b,fld_default,0-shot,accuracy,0.0,
huggyllama/llama-7b,fld_star,0-shot,accuracy,0.0,
huggyllama/llama-7b,arithmetic_3da,5-shot,accuracy,0.671,0.0105087923834607
huggyllama/llama-7b,arithmetic_3ds,5-shot,accuracy,0.4205,0.0110408706818214
huggyllama/llama-7b,arithmetic_4da,5-shot,accuracy,0.3535,0.0106923354801003
huggyllama/llama-7b,arithmetic_2ds,5-shot,accuracy,0.4915,0.0111815199411391
huggyllama/llama-7b,arithmetic_5ds,5-shot,accuracy,0.1135,0.0070946488299991
huggyllama/llama-7b,arithmetic_5da,5-shot,accuracy,0.105,0.0068564572122015
huggyllama/llama-7b,arithmetic_1dc,5-shot,accuracy,0.17,0.008401505379771
huggyllama/llama-7b,arithmetic_4ds,5-shot,accuracy,0.3045,0.0102928541436869
huggyllama/llama-7b,arithmetic_2dm,5-shot,accuracy,0.1865,0.0087118782941283
huggyllama/llama-7b,arithmetic_2da,5-shot,accuracy,0.7645,0.0094902532871219
huggyllama/llama-7b,gsm8k_cot,5-shot,accuracy,0.1099317664897649,0.0086161955878654
huggyllama/llama-7b,anli_r2,0-shot,brier_score,0.7348861281471742,
huggyllama/llama-7b,anli_r3,0-shot,brier_score,0.7095518208768032,
huggyllama/llama-7b,anli_r1,0-shot,brier_score,0.7461478989427204,
huggyllama/llama-7b,xnli_eu,0-shot,brier_score,1.0296807787278208,
huggyllama/llama-7b,xnli_vi,0-shot,brier_score,0.9782764908278532,
huggyllama/llama-7b,xnli_ru,0-shot,brier_score,0.8046437633798106,
huggyllama/llama-7b,xnli_zh,0-shot,brier_score,0.96633957914296,
huggyllama/llama-7b,xnli_tr,0-shot,brier_score,0.8179342178015766,
huggyllama/llama-7b,xnli_fr,0-shot,brier_score,0.7545951761984023,
huggyllama/llama-7b,xnli_en,0-shot,brier_score,0.6399015782286974,
huggyllama/llama-7b,xnli_ur,0-shot,brier_score,1.18515651443942,
huggyllama/llama-7b,xnli_ar,0-shot,brier_score,1.2101674335547865,
huggyllama/llama-7b,xnli_de,0-shot,brier_score,0.7906547361490429,
huggyllama/llama-7b,xnli_hi,0-shot,brier_score,0.8598989915709084,
huggyllama/llama-7b,xnli_es,0-shot,brier_score,0.8477390395566538,
huggyllama/llama-7b,xnli_bg,0-shot,brier_score,0.8810810022273962,
huggyllama/llama-7b,xnli_sw,0-shot,brier_score,1.0933436482571928,
huggyllama/llama-7b,xnli_el,0-shot,brier_score,0.8910593589743593,
huggyllama/llama-7b,xnli_th,0-shot,brier_score,1.0236076918742028,
huggyllama/llama-7b,logiqa2,0-shot,brier_score,0.9998570403139664,
huggyllama/llama-7b,mathqa,0-shot,brier_score,0.9133227108274022,
huggyllama/llama-7b,lambada_standard,0-shot,perplexity,4.405370129844651,0.0898330749660412
huggyllama/llama-7b,lambada_standard,0-shot,accuracy,0.6780516204152921,0.006509334262746
huggyllama/llama-7b,lambada_openai,0-shot,perplexity,3.488115332748822,0.068352445623964
huggyllama/llama-7b,lambada_openai,0-shot,accuracy,0.7351057636328352,0.0061478496958282
froggeric/WestLake-10.7B-v2,minerva_math_precalc,5-shot,accuracy,0.0384615384615384,0.0082375564784253
froggeric/WestLake-10.7B-v2,minerva_math_prealgebra,5-shot,accuracy,0.2789896670493685,0.0152056565672971
froggeric/WestLake-10.7B-v2,minerva_math_num_theory,5-shot,accuracy,0.0814814814814814,0.0117836282811216
froggeric/WestLake-10.7B-v2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0376522702104097,0.0063380889177306
froggeric/WestLake-10.7B-v2,minerva_math_geometry,5-shot,accuracy,0.081419624217119,0.0125086136856882
froggeric/WestLake-10.7B-v2,minerva_math_counting_and_prob,5-shot,accuracy,0.0970464135021097,0.0136110581153728
froggeric/WestLake-10.7B-v2,minerva_math_algebra,5-shot,accuracy,0.1979780960404381,0.0115706922289987
froggeric/WestLake-10.7B-v2,fld_default,0-shot,accuracy,0.0002,0.0001999999999999
froggeric/WestLake-10.7B-v2,fld_star,0-shot,accuracy,0.0002,0.0001999999999999
froggeric/WestLake-10.7B-v2,arithmetic_3da,5-shot,accuracy,0.954,0.0046854003551718
froggeric/WestLake-10.7B-v2,arithmetic_3ds,5-shot,accuracy,0.975,0.0034919331033682
froggeric/WestLake-10.7B-v2,arithmetic_4da,5-shot,accuracy,0.8725,0.0074598726430096
froggeric/WestLake-10.7B-v2,arithmetic_2ds,5-shot,accuracy,0.99,0.0022254159696827
froggeric/WestLake-10.7B-v2,arithmetic_5ds,5-shot,accuracy,0.8575,0.0078184038472926
froggeric/WestLake-10.7B-v2,arithmetic_5da,5-shot,accuracy,0.81,0.0087743087617841
froggeric/WestLake-10.7B-v2,arithmetic_1dc,5-shot,accuracy,0.6025,0.0109456282774996
froggeric/WestLake-10.7B-v2,arithmetic_4ds,5-shot,accuracy,0.92,0.0060678174992828
froggeric/WestLake-10.7B-v2,arithmetic_2dm,5-shot,accuracy,0.5035,0.0111828620308756
froggeric/WestLake-10.7B-v2,arithmetic_2da,5-shot,accuracy,0.982,0.0029736208922129
froggeric/WestLake-10.7B-v2,gsm8k_cot,5-shot,accuracy,0.6550416982562547,0.0130936301336662
froggeric/WestLake-10.7B-v2,gsm8k,5-shot,accuracy,0.6209249431387415,0.0133636302950883
froggeric/WestLake-10.7B-v2,anli_r2,0-shot,brier_score,0.8472615638756087,
froggeric/WestLake-10.7B-v2,anli_r3,0-shot,brier_score,0.7853073888110543,
froggeric/WestLake-10.7B-v2,anli_r1,0-shot,brier_score,0.6792771671674599,
froggeric/WestLake-10.7B-v2,xnli_eu,0-shot,brier_score,1.065526750088975,
froggeric/WestLake-10.7B-v2,xnli_vi,0-shot,brier_score,1.0439012064803812,
froggeric/WestLake-10.7B-v2,xnli_ru,0-shot,brier_score,0.9928432729414078,
froggeric/WestLake-10.7B-v2,xnli_zh,0-shot,brier_score,1.1708548798400409,
froggeric/WestLake-10.7B-v2,xnli_tr,0-shot,brier_score,1.0415841664185104,
froggeric/WestLake-10.7B-v2,xnli_fr,0-shot,brier_score,1.0902927110654848,
froggeric/WestLake-10.7B-v2,xnli_en,0-shot,brier_score,0.7999870468682789,
froggeric/WestLake-10.7B-v2,xnli_ur,0-shot,brier_score,1.203193103469732,
froggeric/WestLake-10.7B-v2,xnli_ar,0-shot,brier_score,1.218269698711459,
froggeric/WestLake-10.7B-v2,xnli_de,0-shot,brier_score,0.9566540615741678,
froggeric/WestLake-10.7B-v2,xnli_hi,0-shot,brier_score,0.9931017430900576,
froggeric/WestLake-10.7B-v2,xnli_es,0-shot,brier_score,1.046201440430368,
froggeric/WestLake-10.7B-v2,xnli_bg,0-shot,brier_score,0.9533770617239662,
froggeric/WestLake-10.7B-v2,xnli_sw,0-shot,brier_score,1.039675312596033,
froggeric/WestLake-10.7B-v2,xnli_el,0-shot,brier_score,0.9991586293399104,
froggeric/WestLake-10.7B-v2,xnli_th,0-shot,brier_score,1.2343337415593545,
froggeric/WestLake-10.7B-v2,logiqa2,0-shot,brier_score,0.91354784900568,
froggeric/WestLake-10.7B-v2,mathqa,0-shot,brier_score,1.0613930870257773,
froggeric/WestLake-10.7B-v2,lambada_standard,0-shot,perplexity,7.47557563678433,0.2480883655464811
froggeric/WestLake-10.7B-v2,lambada_standard,0-shot,accuracy,0.5486124587618862,0.0069329758883686
froggeric/WestLake-10.7B-v2,lambada_openai,0-shot,perplexity,5.457072232577165,0.1617344927365642
froggeric/WestLake-10.7B-v2,lambada_openai,0-shot,accuracy,0.6037259848631865,0.0068144342382628
froggeric/WestLake-10.7B-v2,mmlu_world_religions,0-shot,accuracy,0.8070175438596491,0.0302674575548984
froggeric/WestLake-10.7B-v2,mmlu_formal_logic,0-shot,accuracy,0.4523809523809524,0.0445180795905532
froggeric/WestLake-10.7B-v2,mmlu_prehistory,0-shot,accuracy,0.7376543209876543,0.0244772228561351
froggeric/WestLake-10.7B-v2,mmlu_moral_scenarios,0-shot,accuracy,0.3351955307262569,0.0157880071901858
froggeric/WestLake-10.7B-v2,mmlu_high_school_world_history,0-shot,accuracy,0.810126582278481,0.0255301004602334
froggeric/WestLake-10.7B-v2,mmlu_moral_disputes,0-shot,accuracy,0.7052023121387283,0.0245476177948038
froggeric/WestLake-10.7B-v2,mmlu_professional_law,0-shot,accuracy,0.4797913950456323,0.0127598014277675
froggeric/WestLake-10.7B-v2,mmlu_logical_fallacies,0-shot,accuracy,0.7484662576687117,0.0340899788685752
froggeric/WestLake-10.7B-v2,mmlu_high_school_us_history,0-shot,accuracy,0.7941176470588235,0.0283794494515886
froggeric/WestLake-10.7B-v2,mmlu_philosophy,0-shot,accuracy,0.684887459807074,0.0263852737034644
froggeric/WestLake-10.7B-v2,mmlu_jurisprudence,0-shot,accuracy,0.8055555555555556,0.0382607632488486
froggeric/WestLake-10.7B-v2,mmlu_international_law,0-shot,accuracy,0.7768595041322314,0.0380075447522873
froggeric/WestLake-10.7B-v2,mmlu_high_school_european_history,0-shot,accuracy,0.7575757575757576,0.0334640988105595
froggeric/WestLake-10.7B-v2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8911917098445595,0.0224732533327687
froggeric/WestLake-10.7B-v2,mmlu_high_school_microeconomics,0-shot,accuracy,0.6554621848739496,0.0308686826041216
froggeric/WestLake-10.7B-v2,mmlu_high_school_geography,0-shot,accuracy,0.7575757575757576,0.030532892233932
froggeric/WestLake-10.7B-v2,mmlu_high_school_psychology,0-shot,accuracy,0.8348623853211009,0.015919557829976
froggeric/WestLake-10.7B-v2,mmlu_public_relations,0-shot,accuracy,0.6636363636363637,0.045253935963025
froggeric/WestLake-10.7B-v2,mmlu_us_foreign_policy,0-shot,accuracy,0.86,0.0348735088019776
froggeric/WestLake-10.7B-v2,mmlu_sociology,0-shot,accuracy,0.8407960199004975,0.0258706467661691
froggeric/WestLake-10.7B-v2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6384615384615384,0.0243595814653969
froggeric/WestLake-10.7B-v2,mmlu_security_studies,0-shot,accuracy,0.710204081632653,0.0290430886833043
froggeric/WestLake-10.7B-v2,mmlu_professional_psychology,0-shot,accuracy,0.6928104575163399,0.0186633596714636
froggeric/WestLake-10.7B-v2,mmlu_human_sexuality,0-shot,accuracy,0.732824427480916,0.0388084830108239
froggeric/WestLake-10.7B-v2,mmlu_econometrics,0-shot,accuracy,0.5087719298245614,0.0470288043204961
froggeric/WestLake-10.7B-v2,mmlu_miscellaneous,0-shot,accuracy,0.8186462324393359,0.0137786937784641
froggeric/WestLake-10.7B-v2,mmlu_marketing,0-shot,accuracy,0.8717948717948718,0.0219019051150733
froggeric/WestLake-10.7B-v2,mmlu_management,0-shot,accuracy,0.7572815533980582,0.0424502248638449
froggeric/WestLake-10.7B-v2,mmlu_nutrition,0-shot,accuracy,0.6830065359477124,0.0266432784745087
froggeric/WestLake-10.7B-v2,mmlu_medical_genetics,0-shot,accuracy,0.77,0.042295258468165
froggeric/WestLake-10.7B-v2,mmlu_human_aging,0-shot,accuracy,0.7309417040358744,0.0297637794068749
froggeric/WestLake-10.7B-v2,mmlu_professional_medicine,0-shot,accuracy,0.6580882352941176,0.0288147224222541
froggeric/WestLake-10.7B-v2,mmlu_college_medicine,0-shot,accuracy,0.6416184971098265,0.0365634365335316
froggeric/WestLake-10.7B-v2,mmlu_business_ethics,0-shot,accuracy,0.67,0.047258156262526
froggeric/WestLake-10.7B-v2,mmlu_clinical_knowledge,0-shot,accuracy,0.6641509433962264,0.0290672201466448
froggeric/WestLake-10.7B-v2,mmlu_global_facts,0-shot,accuracy,0.4,0.049236596391733
froggeric/WestLake-10.7B-v2,mmlu_virology,0-shot,accuracy,0.5301204819277109,0.0388542542086676
froggeric/WestLake-10.7B-v2,mmlu_professional_accounting,0-shot,accuracy,0.475177304964539,0.0297907192438297
froggeric/WestLake-10.7B-v2,mmlu_college_physics,0-shot,accuracy,0.3725490196078431,0.0481084014808263
froggeric/WestLake-10.7B-v2,mmlu_high_school_physics,0-shot,accuracy,0.4039735099337748,0.0400648568536534
froggeric/WestLake-10.7B-v2,mmlu_high_school_biology,0-shot,accuracy,0.7677419354838709,0.0240222561303082
froggeric/WestLake-10.7B-v2,mmlu_college_biology,0-shot,accuracy,0.7222222222222222,0.0374555479146245
froggeric/WestLake-10.7B-v2,mmlu_anatomy,0-shot,accuracy,0.562962962962963,0.042849586397534
froggeric/WestLake-10.7B-v2,mmlu_college_chemistry,0-shot,accuracy,0.43,0.0497569851956242
froggeric/WestLake-10.7B-v2,mmlu_computer_security,0-shot,accuracy,0.73,0.0446196043338473
froggeric/WestLake-10.7B-v2,mmlu_college_computer_science,0-shot,accuracy,0.57,0.0497569851956242
froggeric/WestLake-10.7B-v2,mmlu_astronomy,0-shot,accuracy,0.7039473684210527,0.037150621549989
froggeric/WestLake-10.7B-v2,mmlu_college_mathematics,0-shot,accuracy,0.28,0.0451260859854212
froggeric/WestLake-10.7B-v2,mmlu_conceptual_physics,0-shot,accuracy,0.574468085106383,0.0323214691622446
froggeric/WestLake-10.7B-v2,mmlu_abstract_algebra,0-shot,accuracy,0.34,0.0476095228569523
froggeric/WestLake-10.7B-v2,mmlu_high_school_computer_science,0-shot,accuracy,0.69,0.0464823198711731
froggeric/WestLake-10.7B-v2,mmlu_machine_learning,0-shot,accuracy,0.5,0.0474578997876249
froggeric/WestLake-10.7B-v2,mmlu_high_school_chemistry,0-shot,accuracy,0.5073891625615764,0.0351760354036101
froggeric/WestLake-10.7B-v2,mmlu_high_school_statistics,0-shot,accuracy,0.5185185185185185,0.0340763209385405
froggeric/WestLake-10.7B-v2,mmlu_elementary_mathematics,0-shot,accuracy,0.4312169312169312,0.0255064816981382
froggeric/WestLake-10.7B-v2,mmlu_electrical_engineering,0-shot,accuracy,0.5655172413793104,0.0413074087955549
froggeric/WestLake-10.7B-v2,mmlu_high_school_mathematics,0-shot,accuracy,0.3333333333333333,0.0287420409039484
froggeric/WestLake-10.7B-v2,arc_challenge,25-shot,accuracy,0.6672354948805461,0.0137698630461923
froggeric/WestLake-10.7B-v2,arc_challenge,25-shot,acc_norm,0.6953924914675768,0.0134495221099324
froggeric/WestLake-10.7B-v2,hellaswag,10-shot,accuracy,0.7130053774148576,0.0045143455477803
froggeric/WestLake-10.7B-v2,hellaswag,10-shot,acc_norm,0.8808006373232424,0.00323360742389
froggeric/WestLake-10.7B-v2,truthfulqa_mc2,0-shot,accuracy,0.6502221311834233,0.0157288668802904
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,bleu_max,10.118772746100689,0.4033244046848448
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,bleu_acc,0.4602203182374541,0.0174480172239608
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,bleu_diff,0.2367800730027972,0.2543545770766141
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge1_max,32.93967882301825,0.6250936418056136
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge1_acc,0.4883720930232558,0.01749876717574
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge1_diff,0.3499431820705031,0.4362739793916173
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge2_max,18.88579023032469,0.621447224079988
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge2_acc,0.3831089351285189,0.0170184616793898
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge2_diff,-0.4377782614406173,0.4605252038625926
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rougeL_max,28.92110479821841,0.6163965826709437
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rougeL_acc,0.4626682986536107,0.0174546451509705
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rougeL_diff,-0.2106018967291621,0.4195858986205983
froggeric/WestLake-10.7B-v2,truthfulqa_mc1,0-shot,accuracy,0.5116279069767442,0.01749876717574
froggeric/WestLake-10.7B-v2,winogrande,5-shot,accuracy,0.8634569850039463,0.0096502429002916
IEITYuan/Yuan2-2B-hf,minerva_math_precalc,5-shot,accuracy,0.0128205128205128,0.0048189509824876
IEITYuan/Yuan2-2B-hf,minerva_math_prealgebra,5-shot,accuracy,0.0619977037887485,0.0081757975120621
IEITYuan/Yuan2-2B-hf,minerva_math_num_theory,5-shot,accuracy,0.0148148148148148,0.0052037049875126
IEITYuan/Yuan2-2B-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.0044296788482834,0.0022111531423787
IEITYuan/Yuan2-2B-hf,minerva_math_geometry,5-shot,accuracy,0.0292275574112734,0.0077044392054713
IEITYuan/Yuan2-2B-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.0147679324894514,0.0055462385896684
IEITYuan/Yuan2-2B-hf,minerva_math_algebra,5-shot,accuracy,0.033698399326032,0.0052398474232848
IEITYuan/Yuan2-2B-hf,fld_default,0-shot,accuracy,0.0,
IEITYuan/Yuan2-2B-hf,fld_star,0-shot,accuracy,0.0,
IEITYuan/Yuan2-2B-hf,arithmetic_3da,5-shot,accuracy,0.3295,0.0105128557046854
IEITYuan/Yuan2-2B-hf,arithmetic_3ds,5-shot,accuracy,0.124,0.0073715106718225
IEITYuan/Yuan2-2B-hf,arithmetic_4da,5-shot,accuracy,0.112,0.0070535718921847
IEITYuan/Yuan2-2B-hf,arithmetic_2ds,5-shot,accuracy,0.231,0.0094267667821996
IEITYuan/Yuan2-2B-hf,arithmetic_5ds,5-shot,accuracy,0.006,0.0017272787111155
IEITYuan/Yuan2-2B-hf,arithmetic_5da,5-shot,accuracy,0.0495,0.0048514578552905
IEITYuan/Yuan2-2B-hf,arithmetic_1dc,5-shot,accuracy,0.303,0.010278537063322
IEITYuan/Yuan2-2B-hf,arithmetic_4ds,5-shot,accuracy,0.039,0.0043299970481765
IEITYuan/Yuan2-2B-hf,arithmetic_2dm,5-shot,accuracy,0.0695,0.0056877983899978
IEITYuan/Yuan2-2B-hf,arithmetic_2da,5-shot,accuracy,0.5045,0.0111826830948839
IEITYuan/Yuan2-2B-hf,gsm8k_cot,5-shot,accuracy,0.092494313874147,0.0079803968745601
IEITYuan/Yuan2-2B-hf,gsm8k,5-shot,accuracy,0.092494313874147,0.0079803968745601
IEITYuan/Yuan2-2B-hf,anli_r2,0-shot,brier_score,0.9021877709220848,
IEITYuan/Yuan2-2B-hf,anli_r3,0-shot,brier_score,0.8960203323957571,
IEITYuan/Yuan2-2B-hf,anli_r1,0-shot,brier_score,0.8980224868789101,
IEITYuan/Yuan2-2B-hf,xnli_eu,0-shot,brier_score,1.1726851242898388,
IEITYuan/Yuan2-2B-hf,xnli_vi,0-shot,brier_score,1.0551222712344248,
IEITYuan/Yuan2-2B-hf,xnli_ru,0-shot,brier_score,1.0838363628503949,
IEITYuan/Yuan2-2B-hf,xnli_zh,0-shot,brier_score,1.0730192701838772,
IEITYuan/Yuan2-2B-hf,xnli_tr,0-shot,brier_score,1.1096395679638529,
IEITYuan/Yuan2-2B-hf,xnli_fr,0-shot,brier_score,1.0979677062130364,
IEITYuan/Yuan2-2B-hf,xnli_en,0-shot,brier_score,0.7874104438895939,
IEITYuan/Yuan2-2B-hf,xnli_ur,0-shot,brier_score,1.3108842358000463,
IEITYuan/Yuan2-2B-hf,xnli_ar,0-shot,brier_score,1.0708782622925244,
IEITYuan/Yuan2-2B-hf,xnli_de,0-shot,brier_score,0.9569650860704156,
IEITYuan/Yuan2-2B-hf,xnli_hi,0-shot,brier_score,1.2300842352985888,
IEITYuan/Yuan2-2B-hf,xnli_es,0-shot,brier_score,1.045692200792037,
IEITYuan/Yuan2-2B-hf,xnli_bg,0-shot,brier_score,1.1627810243907892,
IEITYuan/Yuan2-2B-hf,xnli_sw,0-shot,brier_score,1.088251408596032,
IEITYuan/Yuan2-2B-hf,xnli_el,0-shot,brier_score,1.1639414398316317,
IEITYuan/Yuan2-2B-hf,xnli_th,0-shot,brier_score,1.307023573234508,
IEITYuan/Yuan2-2B-hf,logiqa2,0-shot,brier_score,1.2490949064547223,
IEITYuan/Yuan2-2B-hf,mathqa,0-shot,brier_score,0.990379580332815,
IEITYuan/Yuan2-2B-hf,lambada_standard,0-shot,perplexity,523.2726641428518,28.91907158439682
IEITYuan/Yuan2-2B-hf,lambada_standard,0-shot,accuracy,0.1622355909179119,0.005136249280239
IEITYuan/Yuan2-2B-hf,lambada_openai,0-shot,perplexity,75.6433032154668,3.790131948633793
IEITYuan/Yuan2-2B-hf,lambada_openai,0-shot,accuracy,0.2928391228410634,0.0063399485630694
IEITYuan/Yuan2-2B-hf,mmlu_world_religions,0-shot,accuracy,0.2105263157894736,0.0312678171466317
IEITYuan/Yuan2-2B-hf,mmlu_formal_logic,0-shot,accuracy,0.2063492063492063,0.0361960452412425
IEITYuan/Yuan2-2B-hf,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.0247486244905373
IEITYuan/Yuan2-2B-hf,mmlu_moral_scenarios,0-shot,accuracy,0.2659217877094972,0.0147767650664388
IEITYuan/Yuan2-2B-hf,mmlu_high_school_world_history,0-shot,accuracy,0.2658227848101265,0.0287567996296583
IEITYuan/Yuan2-2B-hf,mmlu_moral_disputes,0-shot,accuracy,0.2716763005780346,0.0239485129054683
IEITYuan/Yuan2-2B-hf,mmlu_professional_law,0-shot,accuracy,0.2666232073011734,0.0112938360316121
IEITYuan/Yuan2-2B-hf,mmlu_logical_fallacies,0-shot,accuracy,0.2147239263803681,0.0322621937728677
IEITYuan/Yuan2-2B-hf,mmlu_high_school_us_history,0-shot,accuracy,0.2156862745098039,0.0288674314498493
IEITYuan/Yuan2-2B-hf,mmlu_philosophy,0-shot,accuracy,0.2540192926045016,0.0247238615047716
IEITYuan/Yuan2-2B-hf,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
IEITYuan/Yuan2-2B-hf,mmlu_international_law,0-shot,accuracy,0.2479338842975206,0.039418975265163
IEITYuan/Yuan2-2B-hf,mmlu_high_school_european_history,0-shot,accuracy,0.2484848484848484,0.033744026441394
IEITYuan/Yuan2-2B-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2849740932642487,0.0325771407770966
IEITYuan/Yuan2-2B-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.2184873949579832,0.0268415143229589
IEITYuan/Yuan2-2B-hf,mmlu_high_school_geography,0-shot,accuracy,0.2272727272727272,0.0298575156733863
IEITYuan/Yuan2-2B-hf,mmlu_high_school_psychology,0-shot,accuracy,0.2165137614678899,0.0176587105944431
IEITYuan/Yuan2-2B-hf,mmlu_public_relations,0-shot,accuracy,0.2454545454545454,0.0412206650287828
IEITYuan/Yuan2-2B-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.32,0.046882617226215
IEITYuan/Yuan2-2B-hf,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296
IEITYuan/Yuan2-2B-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2230769230769231,0.021107730127244
IEITYuan/Yuan2-2B-hf,mmlu_security_studies,0-shot,accuracy,0.2204081632653061,0.0265370453121452
IEITYuan/Yuan2-2B-hf,mmlu_professional_psychology,0-shot,accuracy,0.2663398692810457,0.0178831881346672
IEITYuan/Yuan2-2B-hf,mmlu_human_sexuality,0-shot,accuracy,0.2290076335877862,0.0368534663171185
IEITYuan/Yuan2-2B-hf,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.0414243971948936
IEITYuan/Yuan2-2B-hf,mmlu_miscellaneous,0-shot,accuracy,0.2796934865900383,0.0160507921480365
IEITYuan/Yuan2-2B-hf,mmlu_marketing,0-shot,accuracy,0.235042735042735,0.0277788359049354
IEITYuan/Yuan2-2B-hf,mmlu_management,0-shot,accuracy,0.2621359223300971,0.0435463107726059
IEITYuan/Yuan2-2B-hf,mmlu_nutrition,0-shot,accuracy,0.238562091503268,0.0244043949280878
IEITYuan/Yuan2-2B-hf,mmlu_medical_genetics,0-shot,accuracy,0.23,0.042295258468165
IEITYuan/Yuan2-2B-hf,mmlu_human_aging,0-shot,accuracy,0.2914798206278027,0.0305002831765459
IEITYuan/Yuan2-2B-hf,mmlu_professional_medicine,0-shot,accuracy,0.1838235294117647,0.0235292421851931
IEITYuan/Yuan2-2B-hf,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
IEITYuan/Yuan2-2B-hf,mmlu_business_ethics,0-shot,accuracy,0.25,0.0435194139889244
IEITYuan/Yuan2-2B-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.290566037735849,0.0279432199893371
IEITYuan/Yuan2-2B-hf,mmlu_global_facts,0-shot,accuracy,0.33,0.047258156262526
IEITYuan/Yuan2-2B-hf,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.0361080501803102
IEITYuan/Yuan2-2B-hf,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539,0.0258921511567094
IEITYuan/Yuan2-2B-hf,mmlu_college_physics,0-shot,accuracy,0.2254901960784313,0.0415830753308328
IEITYuan/Yuan2-2B-hf,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.0329798664847383
IEITYuan/Yuan2-2B-hf,mmlu_high_school_biology,0-shot,accuracy,0.2,0.0227552049595429
IEITYuan/Yuan2-2B-hf,mmlu_college_biology,0-shot,accuracy,0.1944444444444444,0.03309615177059
IEITYuan/Yuan2-2B-hf,mmlu_anatomy,0-shot,accuracy,0.237037037037037,0.036737316839695
IEITYuan/Yuan2-2B-hf,mmlu_college_chemistry,0-shot,accuracy,0.22,0.0416333199893226
IEITYuan/Yuan2-2B-hf,mmlu_computer_security,0-shot,accuracy,0.29,0.0456048021572068
IEITYuan/Yuan2-2B-hf,mmlu_college_computer_science,0-shot,accuracy,0.15,0.0358870281282637
IEITYuan/Yuan2-2B-hf,mmlu_astronomy,0-shot,accuracy,0.1842105263157894,0.0315469804508223
IEITYuan/Yuan2-2B-hf,mmlu_college_mathematics,0-shot,accuracy,0.25,0.0435194139889244
IEITYuan/Yuan2-2B-hf,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.03047297336338
IEITYuan/Yuan2-2B-hf,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
IEITYuan/Yuan2-2B-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.0435194139889244
IEITYuan/Yuan2-2B-hf,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.0428785875134045
IEITYuan/Yuan2-2B-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.2413793103448276,0.0301083307180116
IEITYuan/Yuan2-2B-hf,mmlu_high_school_statistics,0-shot,accuracy,0.3379629629629629,0.0322594135263129
IEITYuan/Yuan2-2B-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.2539682539682539,0.0224180428911139
IEITYuan/Yuan2-2B-hf,mmlu_electrical_engineering,0-shot,accuracy,0.3034482758620689,0.0383122604885033
IEITYuan/Yuan2-2B-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.2407407407407407,0.0260671592222757
IEITYuan/Yuan2-2B-hf,arc_challenge,25-shot,accuracy,0.1988054607508532,0.0116628501981755
IEITYuan/Yuan2-2B-hf,arc_challenge,25-shot,acc_norm,0.2397610921501706,0.0124763041274539
IEITYuan/Yuan2-2B-hf,hellaswag,10-shot,accuracy,0.3054172475602469,0.0045964262200008
IEITYuan/Yuan2-2B-hf,hellaswag,10-shot,acc_norm,0.3333001394144593,0.0047042938987299
IEITYuan/Yuan2-2B-hf,truthfulqa_mc2,0-shot,accuracy,0.4250838081980288,0.01582787673797
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,bleu_max,15.197501509988683,0.544225214068991
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,bleu_acc,0.3047735618115055,0.0161141241568824
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,bleu_diff,-2.903206350824163,0.4479934992721573
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge1_max,40.22444084601327,0.7259309767822962
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge1_acc,0.3121175030599755,0.0162207567695209
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge1_diff,-4.514331396454585,0.635813668954008
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge2_max,23.95451031471122,0.7969484153233573
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge2_acc,0.2496940024479804,0.0151522869071481
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge2_diff,-5.21174195263994,0.7068689979410017
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rougeL_max,36.89784769319415,0.7270599149793284
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rougeL_acc,0.3096695226438188,0.0161857443551449
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rougeL_diff,-4.171471272218275,0.6150306439924023
IEITYuan/Yuan2-2B-hf,truthfulqa_mc1,0-shot,accuracy,0.2545899632802937,0.0152501170791565
IEITYuan/Yuan2-2B-hf,winogrande,5-shot,accuracy,0.5074980268350434,0.0140509055212285
openlm-research/open_llama_3b_v2,arc:challenge,25-shot,accuracy,0.35580204778157,0.0139905711379187
openlm-research/open_llama_3b_v2,arc:challenge,25-shot,acc_norm,0.4027303754266211,0.0143322363067901
openlm-research/open_llama_3b_v2,hellaswag,10-shot,accuracy,0.5309699263095001,0.0049802004518516
openlm-research/open_llama_3b_v2,hellaswag,10-shot,acc_norm,0.7155945030870344,0.00450208828747
openlm-research/open_llama_3b_v2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
openlm-research/open_llama_3b_v2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
openlm-research/open_llama_3b_v2,hendrycksTest-anatomy,5-shot,accuracy,0.237037037037037,0.036737316839695
openlm-research/open_llama_3b_v2,hendrycksTest-anatomy,5-shot,acc_norm,0.237037037037037,0.036737316839695
openlm-research/open_llama_3b_v2,hendrycksTest-astronomy,5-shot,accuracy,0.2763157894736842,0.0363905756995292
openlm-research/open_llama_3b_v2,hendrycksTest-astronomy,5-shot,acc_norm,0.2763157894736842,0.0363905756995292
openlm-research/open_llama_3b_v2,hendrycksTest-business_ethics,5-shot,accuracy,0.4,0.049236596391733
openlm-research/open_llama_3b_v2,hendrycksTest-business_ethics,5-shot,acc_norm,0.4,0.049236596391733
openlm-research/open_llama_3b_v2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.0272572603224948
openlm-research/open_llama_3b_v2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.0272572603224948
openlm-research/open_llama_3b_v2,hendrycksTest-college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008034
openlm-research/open_llama_3b_v2,hendrycksTest-college_biology,5-shot,acc_norm,0.2430555555555555,0.0358687928008034
openlm-research/open_llama_3b_v2,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
openlm-research/open_llama_3b_v2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
openlm-research/open_llama_3b_v2,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.0446196043338474
openlm-research/open_llama_3b_v2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.0446196043338474
openlm-research/open_llama_3b_v2,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
openlm-research/open_llama_3b_v2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
openlm-research/open_llama_3b_v2,hendrycksTest-college_medicine,5-shot,accuracy,0.2369942196531791,0.0324241475748309
openlm-research/open_llama_3b_v2,hendrycksTest-college_medicine,5-shot,acc_norm,0.2369942196531791,0.0324241475748309
openlm-research/open_llama_3b_v2,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
openlm-research/open_llama_3b_v2,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
openlm-research/open_llama_3b_v2,hendrycksTest-computer_security,5-shot,accuracy,0.32,0.046882617226215
openlm-research/open_llama_3b_v2,hendrycksTest-computer_security,5-shot,acc_norm,0.32,0.046882617226215
openlm-research/open_llama_3b_v2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3234042553191489,0.0305794427736103
openlm-research/open_llama_3b_v2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3234042553191489,0.0305794427736103
openlm-research/open_llama_3b_v2,hendrycksTest-econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
openlm-research/open_llama_3b_v2,hendrycksTest-econometrics,5-shot,acc_norm,0.2456140350877192,0.0404933929774814
openlm-research/open_llama_3b_v2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2068965517241379,0.0337567244956055
openlm-research/open_llama_3b_v2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2068965517241379,0.0337567244956055
openlm-research/open_llama_3b_v2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.291005291005291,0.0233938265004848
openlm-research/open_llama_3b_v2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.291005291005291,0.0233938265004848
openlm-research/open_llama_3b_v2,hendrycksTest-formal_logic,5-shot,accuracy,0.246031746031746,0.0385227336492431
openlm-research/open_llama_3b_v2,hendrycksTest-formal_logic,5-shot,acc_norm,0.246031746031746,0.0385227336492431
openlm-research/open_llama_3b_v2,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
openlm-research/open_llama_3b_v2,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_biology,5-shot,accuracy,0.2354838709677419,0.0241376324293377
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2354838709677419,0.0241376324293377
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2167487684729064,0.0289903312525162
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2167487684729064,0.0289903312525162
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.0429234695990928
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.0429234695990928
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2484848484848484,0.033744026441394
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2484848484848484,0.033744026441394
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_geography,5-shot,accuracy,0.202020202020202,0.0286062042892298
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.202020202020202,0.0286062042892298
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.238341968911917,0.0307489053639098
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.238341968911917,0.0307489053639098
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2564102564102564,0.0221390811039715
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2564102564102564,0.0221390811039715
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2333333333333333,0.0257878742209593
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2333333333333333,0.0257878742209593
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2773109243697479,0.02907937453948
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2773109243697479,0.02907937453948
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_physics,5-shot,accuracy,0.3311258278145695,0.0384258171865986
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3311258278145695,0.0384258171865986
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2477064220183486,0.0185081436025478
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2477064220183486,0.0185081436025478
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2083333333333333,0.0276969107130939
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2083333333333333,0.0276969107130939
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2352941176470588,0.0297717752281456
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2352941176470588,0.0297717752281456
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2531645569620253,0.0283046579430352
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2531645569620253,0.0283046579430352
openlm-research/open_llama_3b_v2,hendrycksTest-human_aging,5-shot,accuracy,0.3991031390134529,0.0328674531256796
openlm-research/open_llama_3b_v2,hendrycksTest-human_aging,5-shot,acc_norm,0.3991031390134529,0.0328674531256796
openlm-research/open_llama_3b_v2,hendrycksTest-human_sexuality,5-shot,accuracy,0.2519083969465648,0.0380738711630608
openlm-research/open_llama_3b_v2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2519083969465648,0.0380738711630608
openlm-research/open_llama_3b_v2,hendrycksTest-international_law,5-shot,accuracy,0.2975206611570248,0.0417334914808349
openlm-research/open_llama_3b_v2,hendrycksTest-international_law,5-shot,acc_norm,0.2975206611570248,0.0417334914808349
openlm-research/open_llama_3b_v2,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.0441434366685493
openlm-research/open_llama_3b_v2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.0441434366685493
openlm-research/open_llama_3b_v2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2147239263803681,0.0322621937728677
openlm-research/open_llama_3b_v2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2147239263803681,0.0322621937728677
openlm-research/open_llama_3b_v2,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257872
openlm-research/open_llama_3b_v2,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257872
openlm-research/open_llama_3b_v2,hendrycksTest-management,5-shot,accuracy,0.2718446601941747,0.0440526802414092
openlm-research/open_llama_3b_v2,hendrycksTest-management,5-shot,acc_norm,0.2718446601941747,0.0440526802414092
openlm-research/open_llama_3b_v2,hendrycksTest-marketing,5-shot,accuracy,0.2863247863247863,0.0296143236904566
openlm-research/open_llama_3b_v2,hendrycksTest-marketing,5-shot,acc_norm,0.2863247863247863,0.0296143236904566
openlm-research/open_llama_3b_v2,hendrycksTest-medical_genetics,5-shot,accuracy,0.23,0.042295258468165
openlm-research/open_llama_3b_v2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.23,0.042295258468165
openlm-research/open_llama_3b_v2,hendrycksTest-miscellaneous,5-shot,accuracy,0.280970625798212,0.0160731278512212
openlm-research/open_llama_3b_v2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.280970625798212,0.0160731278512212
openlm-research/open_llama_3b_v2,hendrycksTest-moral_disputes,5-shot,accuracy,0.2572254335260115,0.0235329254310442
openlm-research/open_llama_3b_v2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2572254335260115,0.0235329254310442
openlm-research/open_llama_3b_v2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2402234636871508,0.0142883438039253
openlm-research/open_llama_3b_v2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2402234636871508,0.0142883438039253
openlm-research/open_llama_3b_v2,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.0252616912197294
openlm-research/open_llama_3b_v2,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.0252616912197294
openlm-research/open_llama_3b_v2,hendrycksTest-philosophy,5-shot,accuracy,0.2765273311897106,0.0254038329781796
openlm-research/open_llama_3b_v2,hendrycksTest-philosophy,5-shot,acc_norm,0.2765273311897106,0.0254038329781796
openlm-research/open_llama_3b_v2,hendrycksTest-prehistory,5-shot,accuracy,0.2993827160493827,0.0254831156011954
openlm-research/open_llama_3b_v2,hendrycksTest-prehistory,5-shot,acc_norm,0.2993827160493827,0.0254831156011954
openlm-research/open_llama_3b_v2,hendrycksTest-professional_accounting,5-shot,accuracy,0.3120567375886525,0.0276401205451699
openlm-research/open_llama_3b_v2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3120567375886525,0.0276401205451699
openlm-research/open_llama_3b_v2,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.0109961566351426
openlm-research/open_llama_3b_v2,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.0109961566351426
openlm-research/open_llama_3b_v2,hendrycksTest-professional_medicine,5-shot,accuracy,0.213235294117647,0.0248809715122942
openlm-research/open_llama_3b_v2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.213235294117647,0.0248809715122942
openlm-research/open_llama_3b_v2,hendrycksTest-professional_psychology,5-shot,accuracy,0.2647058823529412,0.0178480895749132
openlm-research/open_llama_3b_v2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2647058823529412,0.0178480895749132
openlm-research/open_llama_3b_v2,hendrycksTest-public_relations,5-shot,accuracy,0.3272727272727272,0.0449429086625208
openlm-research/open_llama_3b_v2,hendrycksTest-public_relations,5-shot,acc_norm,0.3272727272727272,0.0449429086625208
openlm-research/open_llama_3b_v2,hendrycksTest-security_studies,5-shot,accuracy,0.3346938775510204,0.0302092352262423
openlm-research/open_llama_3b_v2,hendrycksTest-security_studies,5-shot,acc_norm,0.3346938775510204,0.0302092352262423
openlm-research/open_llama_3b_v2,hendrycksTest-sociology,5-shot,accuracy,0.2786069651741293,0.031700561834973
openlm-research/open_llama_3b_v2,hendrycksTest-sociology,5-shot,acc_norm,0.2786069651741293,0.031700561834973
openlm-research/open_llama_3b_v2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.36,0.0482418151324421
openlm-research/open_llama_3b_v2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.36,0.0482418151324421
openlm-research/open_llama_3b_v2,hendrycksTest-virology,5-shot,accuracy,0.3373493975903614,0.0368078369072758
openlm-research/open_llama_3b_v2,hendrycksTest-virology,5-shot,acc_norm,0.3373493975903614,0.0368078369072758
openlm-research/open_llama_3b_v2,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.0350877192982456
openlm-research/open_llama_3b_v2,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.0350877192982456
openlm-research/open_llama_3b_v2,truthfulqa:mc,0-shot,mc1,0.2178702570379437,0.0144508467141238
openlm-research/open_llama_3b_v2,truthfulqa:mc,0-shot,mc2,0.3477745324693538,0.0132617493169701
openlm-research/open_llama_3b_v2,drop,3-shot,accuracy,0.0011535234899328,0.0003476179896857
openlm-research/open_llama_3b_v2,drop,3-shot,f1,0.0513496224832217,0.0012730168443049
openlm-research/open_llama_3b_v2,gsm8k,5-shot,accuracy,0.0394238059135708,0.0053602800303424
openlm-research/open_llama_3b_v2,winogrande,5-shot,accuracy,0.6677190213101816,0.0132383165542365
EleutherAI/pythia-2.8b-deduped,drop,3-shot,accuracy,0.0012583892617449,0.0003630560893119
EleutherAI/pythia-2.8b-deduped,drop,3-shot,f1,0.0446549916107384,0.0011620582208289
EleutherAI/pythia-2.8b-deduped,gsm8k,5-shot,accuracy,0.0083396512509476,0.0025049422268605
EleutherAI/pythia-2.8b-deduped,winogrande,5-shot,accuracy,0.6022099447513812,0.013755743513749
EleutherAI/pythia-2.8b-deduped,arc:challenge,25-shot,accuracy,0.325938566552901,0.0136974324666932
EleutherAI/pythia-2.8b-deduped,arc:challenge,25-shot,acc_norm,0.3626279863481229,0.014049106564955
EleutherAI/pythia-2.8b-deduped,hellaswag,10-shot,accuracy,0.4516032662816172,0.0049663518350282
EleutherAI/pythia-2.8b-deduped,hellaswag,10-shot,acc_norm,0.6065524795857399,0.0048751626991216
EleutherAI/pythia-2.8b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-2.8b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-2.8b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.2962962962962963,0.0394462416250111
EleutherAI/pythia-2.8b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.2962962962962963,0.0394462416250111
EleutherAI/pythia-2.8b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.2039473684210526,0.0327900040631005
EleutherAI/pythia-2.8b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.2039473684210526,0.0327900040631005
EleutherAI/pythia-2.8b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-2.8b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/pythia-2.8b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.0272572603224948
EleutherAI/pythia-2.8b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.0272572603224948
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2361111111111111,0.0355144661081082
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2361111111111111,0.0355144661081082
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2369942196531791,0.0324241475748309
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2369942196531791,0.0324241475748309
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.1470588235294117,0.0352406895156744
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.1470588235294117,0.0352406895156744
EleutherAI/pythia-2.8b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-2.8b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-2.8b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2808510638297872,0.0293791704641248
EleutherAI/pythia-2.8b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2808510638297872,0.0293791704641248
EleutherAI/pythia-2.8b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.219298245614035,0.0389243110651875
EleutherAI/pythia-2.8b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.219298245614035,0.0389243110651875
EleutherAI/pythia-2.8b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2758620689655172,0.0372456361977463
EleutherAI/pythia-2.8b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2758620689655172,0.0372456361977463
EleutherAI/pythia-2.8b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2645502645502645,0.0227174678977086
EleutherAI/pythia-2.8b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2645502645502645,0.0227174678977086
EleutherAI/pythia-2.8b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.0371848900681811
EleutherAI/pythia-2.8b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.0371848900681811
EleutherAI/pythia-2.8b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526
EleutherAI/pythia-2.8b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.2483870967741935,0.024580028921481
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2483870967741935,0.024580028921481
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2315270935960591,0.0296783331414444
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2315270935960591,0.0296783331414444
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2848484848484848,0.0352439084451178
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2848484848484848,0.0352439084451178
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2777777777777778,0.0319117822671354
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2777777777777778,0.0319117822671354
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2694300518134715,0.0320186712287779
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2694300518134715,0.0320186712287779
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3205128205128205,0.0236612963939642
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3205128205128205,0.0236612963939642
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2478991596638655,0.0280479672241768
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2478991596638655,0.0280479672241768
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.3178807947019867,0.038020397601079
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3178807947019867,0.038020397601079
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2330275229357798,0.0181256691808615
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2330275229357798,0.0181256691808615
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.449074074074074,0.0339223840532161
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.449074074074074,0.0339223840532161
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2156862745098039,0.0288674314498493
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2156862745098039,0.0288674314498493
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2320675105485232,0.0274797445508085
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2320675105485232,0.0274797445508085
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.3497757847533632,0.032007367194845
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.3497757847533632,0.032007367194845
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2290076335877862,0.0368534663171185
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2290076335877862,0.0368534663171185
EleutherAI/pythia-2.8b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.3471074380165289,0.0434572457029253
EleutherAI/pythia-2.8b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.3471074380165289,0.0434572457029253
EleutherAI/pythia-2.8b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
EleutherAI/pythia-2.8b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2314814814814814,0.0407749470925262
EleutherAI/pythia-2.8b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2699386503067484,0.0348782516849789
EleutherAI/pythia-2.8b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2699386503067484,0.0348782516849789
EleutherAI/pythia-2.8b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.2678571428571428,0.0420327729146776
EleutherAI/pythia-2.8b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.2678571428571428,0.0420327729146776
EleutherAI/pythia-2.8b-deduped,hendrycksTest-management,5-shot,accuracy,0.233009708737864,0.0418583259892831
EleutherAI/pythia-2.8b-deduped,hendrycksTest-management,5-shot,acc_norm,0.233009708737864,0.0418583259892831
EleutherAI/pythia-2.8b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2435897435897435,0.0281209665039143
EleutherAI/pythia-2.8b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2435897435897435,0.0281209665039143
EleutherAI/pythia-2.8b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-2.8b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-2.8b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.280970625798212,0.0160731278512212
EleutherAI/pythia-2.8b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.280970625798212,0.0160731278512212
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2861271676300578,0.0243321467791341
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2861271676300578,0.0243321467791341
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2256983240223463,0.013981395058455
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2256983240223463,0.013981395058455
EleutherAI/pythia-2.8b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.0252616912197294
EleutherAI/pythia-2.8b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.0252616912197294
EleutherAI/pythia-2.8b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2765273311897106,0.0254038329781796
EleutherAI/pythia-2.8b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2765273311897106,0.0254038329781796
EleutherAI/pythia-2.8b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2777777777777778,0.0249220011688863
EleutherAI/pythia-2.8b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2777777777777778,0.0249220011688863
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.258148631029987,0.0111769237193133
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.258148631029987,0.0111769237193133
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.4338235294117647,0.0301056365700166
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4338235294117647,0.0301056365700166
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2745098039215686,0.0180540274588151
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2745098039215686,0.0180540274588151
EleutherAI/pythia-2.8b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.3545454545454545,0.0458200484150541
EleutherAI/pythia-2.8b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.3545454545454545,0.0458200484150541
EleutherAI/pythia-2.8b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.1959183673469387,0.0254093019532256
EleutherAI/pythia-2.8b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.1959183673469387,0.0254093019532256
EleutherAI/pythia-2.8b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.030769444967296
EleutherAI/pythia-2.8b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.030769444967296
EleutherAI/pythia-2.8b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-2.8b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-2.8b-deduped,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.0359156679782466
EleutherAI/pythia-2.8b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.0359156679782466
EleutherAI/pythia-2.8b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.0350877192982456
EleutherAI/pythia-2.8b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.0350877192982456
EleutherAI/pythia-2.8b-deduped,truthfulqa:mc,0-shot,mc1,0.2141982864137087,0.0143621481556904
EleutherAI/pythia-2.8b-deduped,truthfulqa:mc,0-shot,mc2,0.3555978234501585,0.0135949056133465
allenai/tulu-2-dpo-70b,arc:challenge,25-shot,accuracy,0.6825938566552902,0.0136022390880381
allenai/tulu-2-dpo-70b,arc:challenge,25-shot,acc_norm,0.7209897610921502,0.0131067848836013
allenai/tulu-2-dpo-70b,hellaswag,10-shot,accuracy,0.7082254530969926,0.0045365007141479
allenai/tulu-2-dpo-70b,hellaswag,10-shot,acc_norm,0.8898625771758614,0.0031242116171988
allenai/tulu-2-dpo-70b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.0460566186471838
allenai/tulu-2-dpo-70b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.0460566186471838
allenai/tulu-2-dpo-70b,hendrycksTest-anatomy,5-shot,accuracy,0.6148148148148148,0.0420392104015627
allenai/tulu-2-dpo-70b,hendrycksTest-anatomy,5-shot,acc_norm,0.6148148148148148,0.0420392104015627
allenai/tulu-2-dpo-70b,hendrycksTest-astronomy,5-shot,accuracy,0.7828947368421053,0.0335504530488292
allenai/tulu-2-dpo-70b,hendrycksTest-astronomy,5-shot,acc_norm,0.7828947368421053,0.0335504530488292
allenai/tulu-2-dpo-70b,hendrycksTest-business_ethics,5-shot,accuracy,0.73,0.0446196043338474
allenai/tulu-2-dpo-70b,hendrycksTest-business_ethics,5-shot,acc_norm,0.73,0.0446196043338474
allenai/tulu-2-dpo-70b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7509433962264151,0.0266164829805017
allenai/tulu-2-dpo-70b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7509433962264151,0.0266164829805017
allenai/tulu-2-dpo-70b,hendrycksTest-college_biology,5-shot,accuracy,0.8263888888888888,0.0316747338379571
allenai/tulu-2-dpo-70b,hendrycksTest-college_biology,5-shot,acc_norm,0.8263888888888888,0.0316747338379571
allenai/tulu-2-dpo-70b,hendrycksTest-college_chemistry,5-shot,accuracy,0.51,0.0502418393795691
allenai/tulu-2-dpo-70b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.51,0.0502418393795691
allenai/tulu-2-dpo-70b,hendrycksTest-college_computer_science,5-shot,accuracy,0.59,0.049431107042371
allenai/tulu-2-dpo-70b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.59,0.049431107042371
allenai/tulu-2-dpo-70b,hendrycksTest-college_mathematics,5-shot,accuracy,0.39,0.0490207130000197
allenai/tulu-2-dpo-70b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.39,0.0490207130000197
allenai/tulu-2-dpo-70b,hendrycksTest-college_medicine,5-shot,accuracy,0.7225433526011561,0.0341401400704403
allenai/tulu-2-dpo-70b,hendrycksTest-college_medicine,5-shot,acc_norm,0.7225433526011561,0.0341401400704403
allenai/tulu-2-dpo-70b,hendrycksTest-college_physics,5-shot,accuracy,0.4313725490196078,0.0492809959728753
allenai/tulu-2-dpo-70b,hendrycksTest-college_physics,5-shot,acc_norm,0.4313725490196078,0.0492809959728753
allenai/tulu-2-dpo-70b,hendrycksTest-computer_security,5-shot,accuracy,0.72,0.0451260859854212
allenai/tulu-2-dpo-70b,hendrycksTest-computer_security,5-shot,acc_norm,0.72,0.0451260859854212
allenai/tulu-2-dpo-70b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6978723404255319,0.0300175544718805
allenai/tulu-2-dpo-70b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6978723404255319,0.0300175544718805
allenai/tulu-2-dpo-70b,hendrycksTest-econometrics,5-shot,accuracy,0.4473684210526316,0.0467747300449119
allenai/tulu-2-dpo-70b,hendrycksTest-econometrics,5-shot,acc_norm,0.4473684210526316,0.0467747300449119
allenai/tulu-2-dpo-70b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5793103448275863,0.0411391498118926
allenai/tulu-2-dpo-70b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5793103448275863,0.0411391498118926
allenai/tulu-2-dpo-70b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.455026455026455,0.0256469283610493
allenai/tulu-2-dpo-70b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.455026455026455,0.0256469283610493
allenai/tulu-2-dpo-70b,hendrycksTest-formal_logic,5-shot,accuracy,0.4841269841269841,0.044698818540726
allenai/tulu-2-dpo-70b,hendrycksTest-formal_logic,5-shot,acc_norm,0.4841269841269841,0.044698818540726
allenai/tulu-2-dpo-70b,hendrycksTest-global_facts,5-shot,accuracy,0.52,0.0502116731568677
allenai/tulu-2-dpo-70b,hendrycksTest-global_facts,5-shot,acc_norm,0.52,0.0502116731568677
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_biology,5-shot,accuracy,0.7935483870967742,0.0230258996171887
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7935483870967742,0.0230258996171887
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.035128190778761
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.035128190778761
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.78,0.0416333199893226
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.78,0.0416333199893226
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8363636363636363,0.0288878723954879
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8363636363636363,0.0288878723954879
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_geography,5-shot,accuracy,0.8888888888888888,0.0223907876382167
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8888888888888888,0.0223907876382167
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9378238341968912,0.0174269741542405
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9378238341968912,0.0174269741542405
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7333333333333333,0.0224212736129237
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.7333333333333333,0.0224212736129237
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3592592592592593,0.0292529059272519
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3592592592592593,0.0292529059272519
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8025210084033614,0.0258591641220514
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8025210084033614,0.0258591641220514
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_physics,5-shot,accuracy,0.4701986754966887,0.0407522499221697
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4701986754966887,0.0407522499221697
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8935779816513761,0.0132215546745943
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8935779816513761,0.0132215546745943
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6064814814814815,0.0333174787637031
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6064814814814815,0.0333174787637031
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9166666666666666,0.0193984521358139
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9166666666666666,0.0193984521358139
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8607594936708861,0.0225355263526927
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8607594936708861,0.0225355263526927
allenai/tulu-2-dpo-70b,hendrycksTest-human_aging,5-shot,accuracy,0.7668161434977578,0.0283803911470947
allenai/tulu-2-dpo-70b,hendrycksTest-human_aging,5-shot,acc_norm,0.7668161434977578,0.0283803911470947
allenai/tulu-2-dpo-70b,hendrycksTest-human_sexuality,5-shot,accuracy,0.8320610687022901,0.0327854853734313
allenai/tulu-2-dpo-70b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8320610687022901,0.0327854853734313
allenai/tulu-2-dpo-70b,hendrycksTest-international_law,5-shot,accuracy,0.8677685950413223,0.0309227883204458
allenai/tulu-2-dpo-70b,hendrycksTest-international_law,5-shot,acc_norm,0.8677685950413223,0.0309227883204458
allenai/tulu-2-dpo-70b,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.0352070399051796
allenai/tulu-2-dpo-70b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.0352070399051796
allenai/tulu-2-dpo-70b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8282208588957055,0.029634717272371
allenai/tulu-2-dpo-70b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8282208588957055,0.029634717272371
allenai/tulu-2-dpo-70b,hendrycksTest-machine_learning,5-shot,accuracy,0.5,0.0474578997876249
allenai/tulu-2-dpo-70b,hendrycksTest-machine_learning,5-shot,acc_norm,0.5,0.0474578997876249
allenai/tulu-2-dpo-70b,hendrycksTest-management,5-shot,accuracy,0.8155339805825242,0.0384042362728827
allenai/tulu-2-dpo-70b,hendrycksTest-management,5-shot,acc_norm,0.8155339805825242,0.0384042362728827
allenai/tulu-2-dpo-70b,hendrycksTest-marketing,5-shot,accuracy,0.8974358974358975,0.0198756550278674
allenai/tulu-2-dpo-70b,hendrycksTest-marketing,5-shot,acc_norm,0.8974358974358975,0.0198756550278674
allenai/tulu-2-dpo-70b,hendrycksTest-medical_genetics,5-shot,accuracy,0.69,0.0464823198711731
allenai/tulu-2-dpo-70b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.69,0.0464823198711731
allenai/tulu-2-dpo-70b,hendrycksTest-miscellaneous,5-shot,accuracy,0.8531289910600255,0.0126582017361472
allenai/tulu-2-dpo-70b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8531289910600255,0.0126582017361472
allenai/tulu-2-dpo-70b,hendrycksTest-moral_disputes,5-shot,accuracy,0.7658959537572254,0.0227971102780711
allenai/tulu-2-dpo-70b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7658959537572254,0.0227971102780711
allenai/tulu-2-dpo-70b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.511731843575419,0.0167178976769321
allenai/tulu-2-dpo-70b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.511731843575419,0.0167178976769321
allenai/tulu-2-dpo-70b,hendrycksTest-nutrition,5-shot,accuracy,0.7843137254901961,0.023550831351995
allenai/tulu-2-dpo-70b,hendrycksTest-nutrition,5-shot,acc_norm,0.7843137254901961,0.023550831351995
allenai/tulu-2-dpo-70b,hendrycksTest-philosophy,5-shot,accuracy,0.7620578778135049,0.0241851506478187
allenai/tulu-2-dpo-70b,hendrycksTest-philosophy,5-shot,acc_norm,0.7620578778135049,0.0241851506478187
allenai/tulu-2-dpo-70b,hendrycksTest-prehistory,5-shot,accuracy,0.8209876543209876,0.021330868762127
allenai/tulu-2-dpo-70b,hendrycksTest-prehistory,5-shot,acc_norm,0.8209876543209876,0.021330868762127
allenai/tulu-2-dpo-70b,hendrycksTest-professional_accounting,5-shot,accuracy,0.574468085106383,0.0294948276001443
allenai/tulu-2-dpo-70b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.574468085106383,0.0294948276001443
allenai/tulu-2-dpo-70b,hendrycksTest-professional_law,5-shot,accuracy,0.546284224250326,0.0127154048412777
allenai/tulu-2-dpo-70b,hendrycksTest-professional_law,5-shot,acc_norm,0.546284224250326,0.0127154048412777
allenai/tulu-2-dpo-70b,hendrycksTest-professional_medicine,5-shot,accuracy,0.75,0.026303648393696
allenai/tulu-2-dpo-70b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.75,0.026303648393696
allenai/tulu-2-dpo-70b,hendrycksTest-professional_psychology,5-shot,accuracy,0.7598039215686274,0.0172827606951674
allenai/tulu-2-dpo-70b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7598039215686274,0.0172827606951674
allenai/tulu-2-dpo-70b,hendrycksTest-public_relations,5-shot,accuracy,0.7727272727272727,0.0401396455407277
allenai/tulu-2-dpo-70b,hendrycksTest-public_relations,5-shot,acc_norm,0.7727272727272727,0.0401396455407277
allenai/tulu-2-dpo-70b,hendrycksTest-security_studies,5-shot,accuracy,0.7673469387755102,0.0270492579158961
allenai/tulu-2-dpo-70b,hendrycksTest-security_studies,5-shot,acc_norm,0.7673469387755102,0.0270492579158961
allenai/tulu-2-dpo-70b,hendrycksTest-sociology,5-shot,accuracy,0.8756218905472637,0.0233354017901663
allenai/tulu-2-dpo-70b,hendrycksTest-sociology,5-shot,acc_norm,0.8756218905472637,0.0233354017901663
allenai/tulu-2-dpo-70b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.88,0.032659863237109
allenai/tulu-2-dpo-70b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.88,0.032659863237109
allenai/tulu-2-dpo-70b,hendrycksTest-virology,5-shot,accuracy,0.5120481927710844,0.0389136449583581
allenai/tulu-2-dpo-70b,hendrycksTest-virology,5-shot,acc_norm,0.5120481927710844,0.0389136449583581
allenai/tulu-2-dpo-70b,hendrycksTest-world_religions,5-shot,accuracy,0.8713450292397661,0.0256793427232769
allenai/tulu-2-dpo-70b,hendrycksTest-world_religions,5-shot,acc_norm,0.8713450292397661,0.0256793427232769
allenai/tulu-2-dpo-70b,truthfulqa:mc,0-shot,mc1,0.4675642594859241,0.0174666321495776
allenai/tulu-2-dpo-70b,truthfulqa:mc,0-shot,mc2,0.6577655722264159,0.0149032817563932
allenai/tulu-2-dpo-70b,winogrande,5-shot,accuracy,0.8326756116811366,0.010490608806828
allenai/tulu-2-dpo-70b,gsm8k,5-shot,accuracy,0.6262319939347991,0.013326342860737
Salesforce/codegen-350M-mono,minerva_math_precalc,5-shot,accuracy,0.0018315018315018,0.0018315018315018
Salesforce/codegen-350M-mono,minerva_math_prealgebra,5-shot,accuracy,0.0045924225028702,0.0022922488477037
Salesforce/codegen-350M-mono,minerva_math_num_theory,5-shot,accuracy,0.0018518518518518,0.0018518518518518
Salesforce/codegen-350M-mono,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417,0.001565259593407
Salesforce/codegen-350M-mono,minerva_math_geometry,5-shot,accuracy,0.0020876826722338,0.0020876826722338
Salesforce/codegen-350M-mono,minerva_math_counting_and_prob,5-shot,accuracy,0.0063291139240506,0.003646382041065
Salesforce/codegen-350M-mono,minerva_math_algebra,5-shot,accuracy,0.0067396798652064,0.0023757942810498
Salesforce/codegen-350M-mono,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
Salesforce/codegen-350M-mono,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
Salesforce/codegen-350M-mono,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
Salesforce/codegen-350M-mono,arithmetic_2ds,5-shot,accuracy,0.0055,0.0016541593398342
Salesforce/codegen-350M-mono,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,arithmetic_1dc,5-shot,accuracy,0.0525,0.0049884183022857
Salesforce/codegen-350M-mono,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005
Salesforce/codegen-350M-mono,arithmetic_2dm,5-shot,accuracy,0.0205,0.0031693686198869
Salesforce/codegen-350M-mono,arithmetic_2da,5-shot,accuracy,0.013,0.0025335171905233
Salesforce/codegen-350M-mono,gsm8k_cot,5-shot,accuracy,0.0197119029567854,0.0038289829787357
Salesforce/codegen-350M-mono,gsm8k,5-shot,accuracy,0.0174374526156178,0.0036054868679982
Salesforce/codegen-350M-mono,anli_r2,0-shot,brier_score,0.8673139893251594,
Salesforce/codegen-350M-mono,anli_r3,0-shot,brier_score,0.8478090822960272,
Salesforce/codegen-350M-mono,anli_r1,0-shot,brier_score,0.8706127110280617,
Salesforce/codegen-350M-mono,xnli_eu,0-shot,brier_score,1.2682920496708858,
Salesforce/codegen-350M-mono,xnli_vi,0-shot,brier_score,1.2752175230857816,
Salesforce/codegen-350M-mono,xnli_ru,0-shot,brier_score,0.8045751050884926,
Salesforce/codegen-350M-mono,xnli_zh,0-shot,brier_score,1.2459315046432844,
Salesforce/codegen-350M-mono,xnli_tr,0-shot,brier_score,1.1658554315468868,
Salesforce/codegen-350M-mono,xnli_fr,0-shot,brier_score,1.1841733511077182,
Salesforce/codegen-350M-mono,xnli_en,0-shot,brier_score,0.7473727928562213,
Salesforce/codegen-350M-mono,xnli_ur,0-shot,brier_score,1.2271500572114633,
Salesforce/codegen-350M-mono,xnli_ar,0-shot,brier_score,1.0042596344795447,
Salesforce/codegen-350M-mono,xnli_de,0-shot,brier_score,1.014094216947774,
Salesforce/codegen-350M-mono,xnli_hi,0-shot,brier_score,1.1401151042767916,
Salesforce/codegen-350M-mono,xnli_es,0-shot,brier_score,1.0787926125248246,
Salesforce/codegen-350M-mono,xnli_bg,0-shot,brier_score,1.1483978030694448,
Salesforce/codegen-350M-mono,xnli_sw,0-shot,brier_score,0.9825310566958356,
Salesforce/codegen-350M-mono,xnli_el,0-shot,brier_score,1.0833010621729644,
Salesforce/codegen-350M-mono,xnli_th,0-shot,brier_score,1.0701844940260412,
Salesforce/codegen-350M-mono,logiqa2,0-shot,brier_score,1.2332295009469143,
Salesforce/codegen-350M-mono,mathqa,0-shot,brier_score,0.9914231048948398,
Salesforce/codegen-350M-mono,lambada_standard,0-shot,perplexity,647.5605869289026,29.442408532236637
Salesforce/codegen-350M-mono,lambada_standard,0-shot,accuracy,0.1340966427323889,0.004747399033321
Salesforce/codegen-350M-mono,lambada_openai,0-shot,perplexity,727.0942907672784,37.34894050370883
Salesforce/codegen-350M-mono,lambada_openai,0-shot,accuracy,0.1414709877741121,0.0048553803197848
Salesforce/codegen-350M-mono,mmlu_world_religions,0-shot,accuracy,0.2923976608187134,0.0348864771345792
Salesforce/codegen-350M-mono,mmlu_formal_logic,0-shot,accuracy,0.2777777777777778,0.0400616808384887
Salesforce/codegen-350M-mono,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.0230167056402622
Salesforce/codegen-350M-mono,mmlu_moral_scenarios,0-shot,accuracy,0.2491620111731843,0.0144658938298599
Salesforce/codegen-350M-mono,mmlu_high_school_world_history,0-shot,accuracy,0.2658227848101265,0.0287567996296583
Salesforce/codegen-350M-mono,mmlu_moral_disputes,0-shot,accuracy,0.2630057803468208,0.0237030995252581
Salesforce/codegen-350M-mono,mmlu_professional_law,0-shot,accuracy,0.2451108213820078,0.0109863078700455
Salesforce/codegen-350M-mono,mmlu_logical_fallacies,0-shot,accuracy,0.2269938650306748,0.0329109957861576
Salesforce/codegen-350M-mono,mmlu_high_school_us_history,0-shot,accuracy,0.2401960784313725,0.0299837330559136
Salesforce/codegen-350M-mono,mmlu_philosophy,0-shot,accuracy,0.1929260450160771,0.0224115167809113
Salesforce/codegen-350M-mono,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.0441434366685493
Salesforce/codegen-350M-mono,mmlu_international_law,0-shot,accuracy,0.2314049586776859,0.0384985609879408
Salesforce/codegen-350M-mono,mmlu_high_school_european_history,0-shot,accuracy,0.2363636363636363,0.0331750593000918
Salesforce/codegen-350M-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2020725388601036,0.0289790897942967
Salesforce/codegen-350M-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.2100840336134453,0.0264613987174718
Salesforce/codegen-350M-mono,mmlu_high_school_geography,0-shot,accuracy,0.1818181818181818,0.0274796030105388
Salesforce/codegen-350M-mono,mmlu_high_school_psychology,0-shot,accuracy,0.2,0.0171498585142509
Salesforce/codegen-350M-mono,mmlu_public_relations,0-shot,accuracy,0.209090909090909,0.0389509101572413
Salesforce/codegen-350M-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-mono,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
Salesforce/codegen-350M-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2230769230769231,0.0211077301272439
Salesforce/codegen-350M-mono,mmlu_security_studies,0-shot,accuracy,0.1755102040816326,0.02435280072297
Salesforce/codegen-350M-mono,mmlu_professional_psychology,0-shot,accuracy,0.272875816993464,0.0180204741483935
Salesforce/codegen-350M-mono,mmlu_human_sexuality,0-shot,accuracy,0.2290076335877862,0.0368534663171185
Salesforce/codegen-350M-mono,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
Salesforce/codegen-350M-mono,mmlu_miscellaneous,0-shot,accuracy,0.2503192848020434,0.0154910889514945
Salesforce/codegen-350M-mono,mmlu_marketing,0-shot,accuracy,0.282051282051282,0.0294803605495411
Salesforce/codegen-350M-mono,mmlu_management,0-shot,accuracy,0.174757281553398,0.0376017800602662
Salesforce/codegen-350M-mono,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.0252616912197294
Salesforce/codegen-350M-mono,mmlu_medical_genetics,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-mono,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.0314938467099413
Salesforce/codegen-350M-mono,mmlu_professional_medicine,0-shot,accuracy,0.2022058823529411,0.0243981929866549
Salesforce/codegen-350M-mono,mmlu_college_medicine,0-shot,accuracy,0.2023121387283237,0.0306311455391988
Salesforce/codegen-350M-mono,mmlu_business_ethics,0-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-350M-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.1962264150943396,0.0244423881311008
Salesforce/codegen-350M-mono,mmlu_global_facts,0-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-350M-mono,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
Salesforce/codegen-350M-mono,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.0266845643404609
Salesforce/codegen-350M-mono,mmlu_college_physics,0-shot,accuracy,0.2549019607843137,0.0433643270799318
Salesforce/codegen-350M-mono,mmlu_high_school_physics,0-shot,accuracy,0.2649006622516556,0.0360303854536038
Salesforce/codegen-350M-mono,mmlu_high_school_biology,0-shot,accuracy,0.2096774193548387,0.0231578793490835
Salesforce/codegen-350M-mono,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.0368565109589753
Salesforce/codegen-350M-mono,mmlu_anatomy,0-shot,accuracy,0.2,0.0345547370232543
Salesforce/codegen-350M-mono,mmlu_college_chemistry,0-shot,accuracy,0.19,0.0394277244403662
Salesforce/codegen-350M-mono,mmlu_computer_security,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-mono,mmlu_college_computer_science,0-shot,accuracy,0.22,0.0416333199893227
Salesforce/codegen-350M-mono,mmlu_astronomy,0-shot,accuracy,0.1842105263157894,0.0315469804508223
Salesforce/codegen-350M-mono,mmlu_college_mathematics,0-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-350M-mono,mmlu_conceptual_physics,0-shot,accuracy,0.2468085106382978,0.0281854413012341
Salesforce/codegen-350M-mono,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
Salesforce/codegen-350M-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-mono,mmlu_machine_learning,0-shot,accuracy,0.2678571428571428,0.0420327729146776
Salesforce/codegen-350M-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.2068965517241379,0.0285013781678939
Salesforce/codegen-350M-mono,mmlu_high_school_statistics,0-shot,accuracy,0.199074074074074,0.0272322984626902
Salesforce/codegen-350M-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.2486772486772486,0.0222618176924001
Salesforce/codegen-350M-mono,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.0360010569272777
Salesforce/codegen-350M-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.237037037037037,0.0259288761327661
Salesforce/codegen-350M-mono,arc_challenge,25-shot,accuracy,0.174914675767918,0.0111015625018282
Salesforce/codegen-350M-mono,arc_challenge,25-shot,acc_norm,0.2090443686006826,0.0118827469874064
Salesforce/codegen-350M-mono,hellaswag,10-shot,accuracy,0.27185819557857,0.004440079173277
Salesforce/codegen-350M-mono,hellaswag,10-shot,acc_norm,0.2898824935271858,0.0045278040162537
Salesforce/codegen-350M-mono,truthfulqa_mc2,0-shot,accuracy,0.4636136886011197,0.0156246372104503
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,bleu_max,17.866027572769028,0.6178956466724292
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,bleu_acc,0.3341493268053855,0.0165125306771505
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,bleu_diff,-2.1708615982970616,0.4770014144933768
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge1_max,40.82248996851368,0.78495594086117
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge1_acc,0.3047735618115055,0.0161141241568824
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge1_diff,-4.957623701292929,0.6248240530678496
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge2_max,24.3758310549308,0.8575180753868562
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge2_acc,0.230110159118727,0.0147345579598077
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge2_diff,-4.351761806675208,0.6492003146837445
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rougeL_max,37.79851538845665,0.7858244663149364
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rougeL_acc,0.3023255813953488,0.016077509266133
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rougeL_diff,-4.666501260483323,0.6089020246878603
Salesforce/codegen-350M-mono,truthfulqa_mc1,0-shot,accuracy,0.2521419828641371,0.0152015222462999
Salesforce/codegen-350M-mono,winogrande,5-shot,accuracy,0.5193370165745856,0.0140419727337129
EleutherAI/gpt-neo-2.7B,arc:challenge,25-shot,accuracy,0.310580204778157,0.013522292098053
EleutherAI/gpt-neo-2.7B,arc:challenge,25-shot,acc_norm,0.333617747440273,0.0137786870541765
EleutherAI/gpt-neo-2.7B,hellaswag,10-shot,accuracy,0.4266082453694483,0.0049357353003488
EleutherAI/gpt-neo-2.7B,hellaswag,10-shot,acc_norm,0.561840270862378,0.0049514703019958
EleutherAI/gpt-neo-2.7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-neo-2.7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/gpt-neo-2.7B,hendrycksTest-anatomy,5-shot,accuracy,0.2074074074074074,0.0350255317067831
EleutherAI/gpt-neo-2.7B,hendrycksTest-anatomy,5-shot,acc_norm,0.2074074074074074,0.0350255317067831
EleutherAI/gpt-neo-2.7B,hendrycksTest-astronomy,5-shot,accuracy,0.1907894736842105,0.031975658210325
EleutherAI/gpt-neo-2.7B,hendrycksTest-astronomy,5-shot,acc_norm,0.1907894736842105,0.031975658210325
EleutherAI/gpt-neo-2.7B,hendrycksTest-business_ethics,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-neo-2.7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/gpt-neo-2.7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2641509433962264,0.0271342916287417
EleutherAI/gpt-neo-2.7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2641509433962264,0.0271342916287417
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.0368565109589753
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.0440844002276807
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.0440844002276807
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_physics,5-shot,acc_norm,0.2254901960784313,0.0415830753308328
EleutherAI/gpt-neo-2.7B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.0292418838696288
EleutherAI/gpt-neo-2.7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.0292418838696288
EleutherAI/gpt-neo-2.7B,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neo-2.7B,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neo-2.7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2551724137931034,0.0363298405270784
EleutherAI/gpt-neo-2.7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2551724137931034,0.0363298405270784
EleutherAI/gpt-neo-2.7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.291005291005291,0.0233938265004848
EleutherAI/gpt-neo-2.7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.291005291005291,0.0233938265004848
EleutherAI/gpt-neo-2.7B,hendrycksTest-formal_logic,5-shot,accuracy,0.1746031746031746,0.0339549002085611
EleutherAI/gpt-neo-2.7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.1746031746031746,0.0339549002085611
EleutherAI/gpt-neo-2.7B,hendrycksTest-global_facts,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/gpt-neo-2.7B,hendrycksTest-global_facts,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2354838709677419,0.0241376324293377
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2354838709677419,0.0241376324293377
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2463054187192118,0.0303150992856177
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2463054187192118,0.0303150992856177
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.35,0.0479372485441101
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.35,0.0479372485441101
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2424242424242424,0.0334640988105595
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2424242424242424,0.0334640988105595
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.3333333333333333,0.0335861814573252
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3333333333333333,0.0335861814573252
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2590673575129533,0.031618779179354
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2590673575129533,0.031618779179354
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3435897435897436,0.0240786965806354
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3435897435897436,0.0240786965806354
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.237037037037037,0.0259288761327661
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.237037037037037,0.0259288761327661
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2563025210084033,0.0283596208705339
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2563025210084033,0.0283596208705339
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2317880794701986,0.0344540627198705
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2317880794701986,0.0344540627198705
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3100917431192661,0.0198308496844397
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3100917431192661,0.0198308496844397
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3935185185185185,0.0333174787637031
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3935185185185185,0.0333174787637031
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.196078431372549,0.0278659422866393
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.196078431372549,0.0278659422866393
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2236286919831223,0.0271232982052299
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2236286919831223,0.0271232982052299
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_aging,5-shot,accuracy,0.1838565022421524,0.0259983790923565
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_aging,5-shot,acc_norm,0.1838565022421524,0.0259983790923565
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2824427480916031,0.0394840612576836
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2824427480916031,0.0394840612576836
EleutherAI/gpt-neo-2.7B,hendrycksTest-international_law,5-shot,accuracy,0.2314049586776859,0.0384985609879409
EleutherAI/gpt-neo-2.7B,hendrycksTest-international_law,5-shot,acc_norm,0.2314049586776859,0.0384985609879409
EleutherAI/gpt-neo-2.7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
EleutherAI/gpt-neo-2.7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
EleutherAI/gpt-neo-2.7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2576687116564417,0.0343615082784691
EleutherAI/gpt-neo-2.7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2576687116564417,0.0343615082784691
EleutherAI/gpt-neo-2.7B,hendrycksTest-machine_learning,5-shot,accuracy,0.1785714285714285,0.036352091215778
EleutherAI/gpt-neo-2.7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.1785714285714285,0.036352091215778
EleutherAI/gpt-neo-2.7B,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.0435463107726059
EleutherAI/gpt-neo-2.7B,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.0435463107726059
EleutherAI/gpt-neo-2.7B,hendrycksTest-marketing,5-shot,accuracy,0.2692307692307692,0.0290585883037488
EleutherAI/gpt-neo-2.7B,hendrycksTest-marketing,5-shot,acc_norm,0.2692307692307692,0.0290585883037488
EleutherAI/gpt-neo-2.7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2388250319284802,0.0152468031973986
EleutherAI/gpt-neo-2.7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2388250319284802,0.0152468031973986
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2485549132947976,0.0232675284321001
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2485549132947976,0.0232675284321001
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.264804469273743,0.0147569064832606
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.264804469273743,0.0147569064832606
EleutherAI/gpt-neo-2.7B,hendrycksTest-nutrition,5-shot,accuracy,0.3169934640522875,0.0266432784745087
EleutherAI/gpt-neo-2.7B,hendrycksTest-nutrition,5-shot,acc_norm,0.3169934640522875,0.0266432784745087
EleutherAI/gpt-neo-2.7B,hendrycksTest-philosophy,5-shot,accuracy,0.315112540192926,0.0263852737034644
EleutherAI/gpt-neo-2.7B,hendrycksTest-philosophy,5-shot,acc_norm,0.315112540192926,0.0263852737034644
EleutherAI/gpt-neo-2.7B,hendrycksTest-prehistory,5-shot,accuracy,0.3086419753086419,0.0257026402606037
EleutherAI/gpt-neo-2.7B,hendrycksTest-prehistory,5-shot,acc_norm,0.3086419753086419,0.0257026402606037
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_law,5-shot,accuracy,0.2438070404172099,0.0109665079721784
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_law,5-shot,acc_norm,0.2438070404172099,0.0109665079721784
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.4301470588235294,0.0300749719173028
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4301470588235294,0.0300749719173028
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2745098039215686,0.0180540274588152
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2745098039215686,0.0180540274588152
EleutherAI/gpt-neo-2.7B,hendrycksTest-public_relations,5-shot,accuracy,0.1909090909090909,0.0376442558598492
EleutherAI/gpt-neo-2.7B,hendrycksTest-public_relations,5-shot,acc_norm,0.1909090909090909,0.0376442558598492
EleutherAI/gpt-neo-2.7B,hendrycksTest-security_studies,5-shot,accuracy,0.2857142857142857,0.0289205832206755
EleutherAI/gpt-neo-2.7B,hendrycksTest-security_studies,5-shot,acc_norm,0.2857142857142857,0.0289205832206755
EleutherAI/gpt-neo-2.7B,hendrycksTest-sociology,5-shot,accuracy,0.2238805970149253,0.0294752502360171
EleutherAI/gpt-neo-2.7B,hendrycksTest-sociology,5-shot,acc_norm,0.2238805970149253,0.0294752502360171
EleutherAI/gpt-neo-2.7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/gpt-neo-2.7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/gpt-neo-2.7B,hendrycksTest-virology,5-shot,accuracy,0.3132530120481928,0.0361080501803102
EleutherAI/gpt-neo-2.7B,hendrycksTest-virology,5-shot,acc_norm,0.3132530120481928,0.0361080501803102
EleutherAI/gpt-neo-2.7B,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.0344629621708842
EleutherAI/gpt-neo-2.7B,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.0344629621708842
EleutherAI/gpt-neo-2.7B,truthfulqa:mc,0-shot,mc1,0.2399020807833537,0.0149488126790621
EleutherAI/gpt-neo-2.7B,truthfulqa:mc,0-shot,mc2,0.3977795990878021,0.0140388956656635
EleutherAI/gpt-neo-2.7B,drop,3-shot,accuracy,0.001363255033557,0.000377860919646
EleutherAI/gpt-neo-2.7B,drop,3-shot,f1,0.0477485318791948,0.0012502430800989
EleutherAI/gpt-neo-2.7B,gsm8k,5-shot,accuracy,0.0204700530705079,0.0039004133859157
EleutherAI/gpt-neo-2.7B,winogrande,5-shot,accuracy,0.5777426992896606,0.0138815820306585
CohereForAI/c4ai-command-r-plus,arc:challenge,25-shot,accuracy,0.659556313993174,0.0138474605188929
CohereForAI/c4ai-command-r-plus,arc:challenge,25-shot,acc_norm,0.7039249146757679,0.0133409160852462
CohereForAI/c4ai-command-r-plus,hellaswag,10-shot,accuracy,0.6927902808205537,0.0046039424398615
CohereForAI/c4ai-command-r-plus,hellaswag,10-shot,acc_norm,0.8796056562437762,0.0032475703304569
CohereForAI/c4ai-command-r-plus,hendrycksTest-abstract_algebra,5-shot,accuracy,0.44,0.0498887651569858
CohereForAI/c4ai-command-r-plus,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.44,0.0498887651569858
CohereForAI/c4ai-command-r-plus,hendrycksTest-anatomy,5-shot,accuracy,0.7481481481481481,0.0374985070917402
CohereForAI/c4ai-command-r-plus,hendrycksTest-anatomy,5-shot,acc_norm,0.7481481481481481,0.0374985070917402
CohereForAI/c4ai-command-r-plus,hendrycksTest-astronomy,5-shot,accuracy,0.8486842105263158,0.0291626315968439
CohereForAI/c4ai-command-r-plus,hendrycksTest-astronomy,5-shot,acc_norm,0.8486842105263158,0.0291626315968439
CohereForAI/c4ai-command-r-plus,hendrycksTest-business_ethics,5-shot,accuracy,0.79,0.0409360180740332
CohereForAI/c4ai-command-r-plus,hendrycksTest-business_ethics,5-shot,acc_norm,0.79,0.0409360180740332
CohereForAI/c4ai-command-r-plus,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.769811320754717,0.0259078971224081
CohereForAI/c4ai-command-r-plus,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.769811320754717,0.0259078971224081
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_biology,5-shot,accuracy,0.8611111111111112,0.0289198029561349
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_biology,5-shot,acc_norm,0.8611111111111112,0.0289198029561349
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_chemistry,5-shot,accuracy,0.49,0.0502418393795691
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_chemistry,5-shot,acc_norm,0.49,0.0502418393795691
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_computer_science,5-shot,accuracy,0.61,0.0490207130000197
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_computer_science,5-shot,acc_norm,0.61,0.0490207130000197
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_mathematics,5-shot,accuracy,0.39,0.0490207130000197
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_mathematics,5-shot,acc_norm,0.39,0.0490207130000197
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_medicine,5-shot,accuracy,0.7398843930635838,0.0334503691678899
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_medicine,5-shot,acc_norm,0.7398843930635838,0.0334503691678899
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_physics,5-shot,accuracy,0.5098039215686274,0.0497422946042281
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_physics,5-shot,acc_norm,0.5098039215686274,0.0497422946042281
CohereForAI/c4ai-command-r-plus,hendrycksTest-computer_security,5-shot,accuracy,0.84,0.036845294917747
CohereForAI/c4ai-command-r-plus,hendrycksTest-computer_security,5-shot,acc_norm,0.84,0.036845294917747
CohereForAI/c4ai-command-r-plus,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7191489361702128,0.0293791704641248
CohereForAI/c4ai-command-r-plus,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7191489361702128,0.0293791704641248
CohereForAI/c4ai-command-r-plus,hendrycksTest-econometrics,5-shot,accuracy,0.6052631578947368,0.0459818805781654
CohereForAI/c4ai-command-r-plus,hendrycksTest-econometrics,5-shot,acc_norm,0.6052631578947368,0.0459818805781654
CohereForAI/c4ai-command-r-plus,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7241379310344828,0.0372456361977463
CohereForAI/c4ai-command-r-plus,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7241379310344828,0.0372456361977463
CohereForAI/c4ai-command-r-plus,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.5661375661375662,0.0255250343824748
CohereForAI/c4ai-command-r-plus,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.5661375661375662,0.0255250343824748
CohereForAI/c4ai-command-r-plus,hendrycksTest-formal_logic,5-shot,accuracy,0.5634920634920635,0.0443593289285146
CohereForAI/c4ai-command-r-plus,hendrycksTest-formal_logic,5-shot,acc_norm,0.5634920634920635,0.0443593289285146
CohereForAI/c4ai-command-r-plus,hendrycksTest-global_facts,5-shot,accuracy,0.6,0.049236596391733
CohereForAI/c4ai-command-r-plus,hendrycksTest-global_facts,5-shot,acc_norm,0.6,0.049236596391733
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_biology,5-shot,accuracy,0.8258064516129032,0.0215762481845145
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8258064516129032,0.0215762481845145
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6403940886699507,0.0337645824650956
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6403940886699507,0.0337645824650956
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.81,0.0394277244403662
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.81,0.0394277244403662
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8606060606060606,0.0270459488258653
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8606060606060606,0.0270459488258653
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_geography,5-shot,accuracy,0.9090909090909092,0.0204820867754242
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9090909090909092,0.0204820867754242
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9378238341968912,0.0174269741542405
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9378238341968912,0.0174269741542405
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7128205128205128,0.0229399254185306
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.7128205128205128,0.0229399254185306
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3888888888888889,0.0297232789614766
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3888888888888889,0.0297232789614766
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7983193277310925,0.0260643134063045
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7983193277310925,0.0260643134063045
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_physics,5-shot,accuracy,0.5099337748344371,0.0408167710724843
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5099337748344371,0.0408167710724843
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_psychology,5-shot,accuracy,0.906422018348624,0.0124868418246019
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.906422018348624,0.0124868418246019
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_statistics,5-shot,accuracy,0.625,0.0330169089872108
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.625,0.0330169089872108
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8970588235294118,0.0213283375708043
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8970588235294118,0.0213283375708043
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9071729957805909,0.0188897505509567
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9071729957805909,0.0188897505509567
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_aging,5-shot,accuracy,0.820627802690583,0.0257498195691928
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_aging,5-shot,acc_norm,0.820627802690583,0.0257498195691928
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_sexuality,5-shot,accuracy,0.8473282442748091,0.0315452167200547
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8473282442748091,0.0315452167200547
CohereForAI/c4ai-command-r-plus,hendrycksTest-international_law,5-shot,accuracy,0.9008264462809916,0.0272852463127589
CohereForAI/c4ai-command-r-plus,hendrycksTest-international_law,5-shot,acc_norm,0.9008264462809916,0.0272852463127589
CohereForAI/c4ai-command-r-plus,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.0352070399051796
CohereForAI/c4ai-command-r-plus,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.0352070399051796
CohereForAI/c4ai-command-r-plus,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8404907975460123,0.0287674817259838
CohereForAI/c4ai-command-r-plus,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8404907975460123,0.0287674817259838
CohereForAI/c4ai-command-r-plus,hendrycksTest-machine_learning,5-shot,accuracy,0.5178571428571429,0.0474276236124301
CohereForAI/c4ai-command-r-plus,hendrycksTest-machine_learning,5-shot,acc_norm,0.5178571428571429,0.0474276236124301
CohereForAI/c4ai-command-r-plus,hendrycksTest-management,5-shot,accuracy,0.8543689320388349,0.0349260647662379
CohereForAI/c4ai-command-r-plus,hendrycksTest-management,5-shot,acc_norm,0.8543689320388349,0.0349260647662379
CohereForAI/c4ai-command-r-plus,hendrycksTest-marketing,5-shot,accuracy,0.9017094017094016,0.0195034449007575
CohereForAI/c4ai-command-r-plus,hendrycksTest-marketing,5-shot,acc_norm,0.9017094017094016,0.0195034449007575
CohereForAI/c4ai-command-r-plus,hendrycksTest-medical_genetics,5-shot,accuracy,0.83,0.0377525168068637
CohereForAI/c4ai-command-r-plus,hendrycksTest-medical_genetics,5-shot,acc_norm,0.83,0.0377525168068637
CohereForAI/c4ai-command-r-plus,hendrycksTest-miscellaneous,5-shot,accuracy,0.879948914431673,0.0116227366920412
CohereForAI/c4ai-command-r-plus,hendrycksTest-miscellaneous,5-shot,acc_norm,0.879948914431673,0.0116227366920412
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_disputes,5-shot,accuracy,0.7745664739884393,0.0224972301909675
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7745664739884393,0.0224972301909675
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6312849162011173,0.0161357590150301
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6312849162011173,0.0161357590150301
CohereForAI/c4ai-command-r-plus,hendrycksTest-nutrition,5-shot,accuracy,0.7973856209150327,0.0230154468779856
CohereForAI/c4ai-command-r-plus,hendrycksTest-nutrition,5-shot,acc_norm,0.7973856209150327,0.0230154468779856
CohereForAI/c4ai-command-r-plus,hendrycksTest-philosophy,5-shot,accuracy,0.77491961414791,0.023720088516179
CohereForAI/c4ai-command-r-plus,hendrycksTest-philosophy,5-shot,acc_norm,0.77491961414791,0.023720088516179
CohereForAI/c4ai-command-r-plus,hendrycksTest-prehistory,5-shot,accuracy,0.8611111111111112,0.0192425262265445
CohereForAI/c4ai-command-r-plus,hendrycksTest-prehistory,5-shot,acc_norm,0.8611111111111112,0.0192425262265445
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_accounting,5-shot,accuracy,0.5780141843971631,0.0294621892333705
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5780141843971631,0.0294621892333705
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_law,5-shot,accuracy,0.590612777053455,0.0125587808955707
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_law,5-shot,acc_norm,0.590612777053455,0.0125587808955707
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_medicine,5-shot,accuracy,0.75,0.026303648393696
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_medicine,5-shot,acc_norm,0.75,0.026303648393696
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_psychology,5-shot,accuracy,0.7973856209150327,0.0162610552837461
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7973856209150327,0.0162610552837461
CohereForAI/c4ai-command-r-plus,hendrycksTest-public_relations,5-shot,accuracy,0.7909090909090909,0.0389509101572413
CohereForAI/c4ai-command-r-plus,hendrycksTest-public_relations,5-shot,acc_norm,0.7909090909090909,0.0389509101572413
CohereForAI/c4ai-command-r-plus,hendrycksTest-security_studies,5-shot,accuracy,0.8244897959183674,0.02435280072297
CohereForAI/c4ai-command-r-plus,hendrycksTest-security_studies,5-shot,acc_norm,0.8244897959183674,0.02435280072297
CohereForAI/c4ai-command-r-plus,hendrycksTest-sociology,5-shot,accuracy,0.8855721393034826,0.0225093453251017
CohereForAI/c4ai-command-r-plus,hendrycksTest-sociology,5-shot,acc_norm,0.8855721393034826,0.0225093453251017
CohereForAI/c4ai-command-r-plus,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.027265992434429
CohereForAI/c4ai-command-r-plus,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.027265992434429
CohereForAI/c4ai-command-r-plus,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721
CohereForAI/c4ai-command-r-plus,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721
CohereForAI/c4ai-command-r-plus,hendrycksTest-world_religions,5-shot,accuracy,0.8830409356725146,0.0246480689613661
CohereForAI/c4ai-command-r-plus,hendrycksTest-world_religions,5-shot,acc_norm,0.8830409356725146,0.0246480689613661
CohereForAI/c4ai-command-r-plus,truthfulqa:mc,0-shot,mc1,0.3965728274173806,0.0171249309420235
CohereForAI/c4ai-command-r-plus,truthfulqa:mc,0-shot,mc2,0.5695167541939289,0.015126847126703
CohereForAI/c4ai-command-r-plus,winogrande,5-shot,accuracy,0.8382004735595896,0.0103501280102924
CohereForAI/c4ai-command-r-plus,gsm8k,5-shot,accuracy,0.4730856709628506,0.0137525171897174
mistralai/Mistral-7B-v0.1,drop,3-shot,accuracy,0.0015729865771812,0.0004058451132417
mistralai/Mistral-7B-v0.1,drop,3-shot,f1,0.0614366610738255,0.0013713061256604
mistralai/Mistral-7B-v0.1,gsm8k,5-shot,accuracy,0.3707354056103108,0.0133042677054584
mistralai/Mistral-7B-v0.1,winogrande,5-shot,accuracy,0.7861089187056038,0.0115244669540902
mistralai/Mistral-7B-v0.1,arc:challenge,25-shot,accuracy,0.568259385665529,0.0144745914271962
mistralai/Mistral-7B-v0.1,arc:challenge,25-shot,acc_norm,0.5998293515358362,0.0143171977878091
mistralai/Mistral-7B-v0.1,hellaswag,10-shot,accuracy,0.6294562836088429,0.0048196336688325
mistralai/Mistral-7B-v0.1,hellaswag,10-shot,acc_norm,0.8331009759012149,0.0037212361965025
mistralai/Mistral-7B-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.29,0.0456048021572068
mistralai/Mistral-7B-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.29,0.0456048021572068
mistralai/Mistral-7B-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.6296296296296297,0.0417165416135454
mistralai/Mistral-7B-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.6296296296296297,0.0417165416135454
mistralai/Mistral-7B-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.6578947368421053,0.0386073159931609
mistralai/Mistral-7B-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.6578947368421053,0.0386073159931609
mistralai/Mistral-7B-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.57,0.0497569851956242
mistralai/Mistral-7B-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.57,0.0497569851956242
mistralai/Mistral-7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.6943396226415094,0.0283532980733226
mistralai/Mistral-7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.6943396226415094,0.0283532980733226
mistralai/Mistral-7B-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.7291666666666666,0.0371617743756601
mistralai/Mistral-7B-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.7291666666666666,0.0371617743756601
mistralai/Mistral-7B-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.52,0.0502116731568677
mistralai/Mistral-7B-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.52,0.0502116731568677
mistralai/Mistral-7B-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.52,0.0502116731568677
mistralai/Mistral-7B-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.52,0.0502116731568677
mistralai/Mistral-7B-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.4,0.049236596391733
mistralai/Mistral-7B-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.4,0.049236596391733
mistralai/Mistral-7B-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.6647398843930635,0.0359958630124707
mistralai/Mistral-7B-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.6647398843930635,0.0359958630124707
mistralai/Mistral-7B-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.392156862745098,0.0485808357426634
mistralai/Mistral-7B-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.392156862745098,0.0485808357426634
mistralai/Mistral-7B-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.77,0.042295258468165
mistralai/Mistral-7B-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.77,0.042295258468165
mistralai/Mistral-7B-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.574468085106383,0.0323214691622446
mistralai/Mistral-7B-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.574468085106383,0.0323214691622446
mistralai/Mistral-7B-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.5,0.0470360434191798
mistralai/Mistral-7B-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.5,0.0470360434191798
mistralai/Mistral-7B-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5724137931034483,0.0412273711137033
mistralai/Mistral-7B-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5724137931034483,0.0412273711137033
mistralai/Mistral-7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.373015873015873,0.0249069904589925
mistralai/Mistral-7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.373015873015873,0.0249069904589925
mistralai/Mistral-7B-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.4126984126984127,0.0440343895476817
mistralai/Mistral-7B-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.4126984126984127,0.0440343895476817
mistralai/Mistral-7B-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.37,0.0485236587093909
mistralai/Mistral-7B-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.37,0.0485236587093909
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.7709677419354839,0.0239049143117826
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7709677419354839,0.0239049143117826
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.035128190778761
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.035128190778761
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.68,0.046882617226215
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.68,0.046882617226215
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7818181818181819,0.0322507810830628
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7818181818181819,0.0322507810830628
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.7727272727272727,0.0298575156733864
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.7727272727272727,0.0298575156733864
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.8652849740932642,0.0246397890977094
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.8652849740932642,0.0246397890977094
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6666666666666666,0.0239011579794025
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6666666666666666,0.0239011579794025
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.337037037037037,0.0288208846662532
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.337037037037037,0.0288208846662532
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.6596638655462185,0.0307780574229316
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.6596638655462185,0.0307780574229316
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.3245033112582781,0.0382274693765875
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3245033112582781,0.0382274693765875
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8238532110091743,0.0163328823934313
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8238532110091743,0.0163328823934313
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5740740740740741,0.0337234327165306
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5740740740740741,0.0337234327165306
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.7990196078431373,0.0281259722656543
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.7990196078431373,0.0281259722656543
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.7721518987341772,0.0273034845990694
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.7721518987341772,0.0273034845990694
mistralai/Mistral-7B-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.7040358744394619,0.0306365913486998
mistralai/Mistral-7B-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.7040358744394619,0.0306365913486998
mistralai/Mistral-7B-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.7938931297709924,0.0354777100415946
mistralai/Mistral-7B-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7938931297709924,0.0354777100415946
mistralai/Mistral-7B-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.7768595041322314,0.0380075447522873
mistralai/Mistral-7B-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.7768595041322314,0.0380075447522873
mistralai/Mistral-7B-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.7777777777777778,0.0401910747255734
mistralai/Mistral-7B-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7777777777777778,0.0401910747255734
mistralai/Mistral-7B-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7914110429447853,0.0319219344893472
mistralai/Mistral-7B-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7914110429447853,0.0319219344893472
mistralai/Mistral-7B-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.4821428571428571,0.0474276236124301
mistralai/Mistral-7B-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.4821428571428571,0.0474276236124301
mistralai/Mistral-7B-v0.1,hendrycksTest-management,5-shot,accuracy,0.8155339805825242,0.0384042362728827
mistralai/Mistral-7B-v0.1,hendrycksTest-management,5-shot,acc_norm,0.8155339805825242,0.0384042362728827
mistralai/Mistral-7B-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.8717948717948718,0.0219019051150733
mistralai/Mistral-7B-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.8717948717948718,0.0219019051150733
mistralai/Mistral-7B-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.74,0.0440844002276807
mistralai/Mistral-7B-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.74,0.0440844002276807
mistralai/Mistral-7B-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.8173690932311622,0.0138163353899731
mistralai/Mistral-7B-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8173690932311622,0.0138163353899731
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.7109826589595376,0.0244051739357832
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7109826589595376,0.0244051739357832
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.3251396648044692,0.0156665427850535
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.3251396648044692,0.0156665427850535
mistralai/Mistral-7B-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.7581699346405228,0.0245181956418793
mistralai/Mistral-7B-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.7581699346405228,0.0245181956418793
mistralai/Mistral-7B-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.6977491961414791,0.0260827006953996
mistralai/Mistral-7B-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.6977491961414791,0.0260827006953996
mistralai/Mistral-7B-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.7345679012345679,0.0245692236004608
mistralai/Mistral-7B-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.7345679012345679,0.0245692236004608
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.4858156028368794,0.029815494483682
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4858156028368794,0.029815494483682
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.4478487614080834,0.0127005824047682
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.4478487614080834,0.0127005824047682
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.6911764705882353,0.02806499816704
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.6911764705882353,0.02806499816704
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.6813725490196079,0.0188500846964687
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.6813725490196079,0.0188500846964687
mistralai/Mistral-7B-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.6727272727272727,0.0449429086625209
mistralai/Mistral-7B-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.6727272727272727,0.0449429086625209
mistralai/Mistral-7B-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.726530612244898,0.0285355603371284
mistralai/Mistral-7B-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.726530612244898,0.0285355603371284
mistralai/Mistral-7B-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.8308457711442786,0.0265085906562332
mistralai/Mistral-7B-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.8308457711442786,0.0265085906562332
mistralai/Mistral-7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.0348735088019777
mistralai/Mistral-7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.0348735088019777
mistralai/Mistral-7B-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721
mistralai/Mistral-7B-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721
mistralai/Mistral-7B-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8304093567251462,0.0287821081054017
mistralai/Mistral-7B-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8304093567251462,0.0287821081054017
mistralai/Mistral-7B-v0.1,truthfulqa:mc,0-shot,mc1,0.2802937576499388,0.0157231395246087
mistralai/Mistral-7B-v0.1,truthfulqa:mc,0-shot,mc2,0.4215317106968115,0.0141381294831339
mistralai/Mistral-7B-v0.1,minerva_math_precalc,5-shot,accuracy,0.0421245421245421,0.0086044649253745
mistralai/Mistral-7B-v0.1,minerva_math_prealgebra,5-shot,accuracy,0.2376578645235361,0.0144308340043693
mistralai/Mistral-7B-v0.1,minerva_math_num_theory,5-shot,accuracy,0.0888888888888888,0.0122578704655672
mistralai/Mistral-7B-v0.1,minerva_math_intermediate_algebra,5-shot,accuracy,0.0542635658914728,0.0075428584238345
mistralai/Mistral-7B-v0.1,minerva_math_geometry,5-shot,accuracy,0.1106471816283924,0.0143480629242418
mistralai/Mistral-7B-v0.1,minerva_math_counting_and_prob,5-shot,accuracy,0.120253164556962,0.0149553486914929
mistralai/Mistral-7B-v0.1,minerva_math_algebra,5-shot,accuracy,0.1828138163437236,0.0112233542380198
mistralai/Mistral-7B-v0.1,fld_default,0-shot,accuracy,0.0,
mistralai/Mistral-7B-v0.1,fld_star,0-shot,accuracy,0.0,
mistralai/Mistral-7B-v0.1,arithmetic_3da,5-shot,accuracy,0.982,0.0029736208922129
mistralai/Mistral-7B-v0.1,arithmetic_3ds,5-shot,accuracy,0.9885,0.0023846841214675
mistralai/Mistral-7B-v0.1,arithmetic_4da,5-shot,accuracy,0.954,0.0046854003551718
mistralai/Mistral-7B-v0.1,arithmetic_2ds,5-shot,accuracy,0.998,0.0009992493430694
mistralai/Mistral-7B-v0.1,arithmetic_5ds,5-shot,accuracy,0.8715,0.0074847769467749
mistralai/Mistral-7B-v0.1,arithmetic_5da,5-shot,accuracy,0.909,0.0064327435900281
mistralai/Mistral-7B-v0.1,arithmetic_1dc,5-shot,accuracy,0.6455,0.0106991640353592
mistralai/Mistral-7B-v0.1,arithmetic_4ds,5-shot,accuracy,0.945,0.0050990685669173
mistralai/Mistral-7B-v0.1,arithmetic_2dm,5-shot,accuracy,0.7115,0.0101333714828186
mistralai/Mistral-7B-v0.1,arithmetic_2da,5-shot,accuracy,0.9985,0.0008655920660521
mistralai/Mistral-7B-v0.1,gsm8k_cot,5-shot,accuracy,0.4192570128885519,0.0135917209590421
mistralai/Mistral-7B-v0.1,anli_r2,0-shot,brier_score,0.7950624544675337,
mistralai/Mistral-7B-v0.1,anli_r3,0-shot,brier_score,0.7365319523049788,
mistralai/Mistral-7B-v0.1,anli_r1,0-shot,brier_score,0.824029011811061,
mistralai/Mistral-7B-v0.1,xnli_eu,0-shot,brier_score,0.9652658697204612,
mistralai/Mistral-7B-v0.1,xnli_vi,0-shot,brier_score,0.8766059818111679,
mistralai/Mistral-7B-v0.1,xnli_ru,0-shot,brier_score,0.7413504110189894,
mistralai/Mistral-7B-v0.1,xnli_zh,0-shot,brier_score,0.9609280684971038,
mistralai/Mistral-7B-v0.1,xnli_tr,0-shot,brier_score,0.8729771059898165,
mistralai/Mistral-7B-v0.1,xnli_fr,0-shot,brier_score,0.7650067503932926,
mistralai/Mistral-7B-v0.1,xnli_en,0-shot,brier_score,0.6632403264489223,
mistralai/Mistral-7B-v0.1,xnli_ur,0-shot,brier_score,1.2217226967863828,
mistralai/Mistral-7B-v0.1,xnli_ar,0-shot,brier_score,1.2723443593223105,
mistralai/Mistral-7B-v0.1,xnli_de,0-shot,brier_score,0.8115761663377433,
mistralai/Mistral-7B-v0.1,xnli_hi,0-shot,brier_score,0.8465004556210892,
mistralai/Mistral-7B-v0.1,xnli_es,0-shot,brier_score,0.8695701092578179,
mistralai/Mistral-7B-v0.1,xnli_bg,0-shot,brier_score,0.84232405643235,
mistralai/Mistral-7B-v0.1,xnli_sw,0-shot,brier_score,0.8955590100538611,
mistralai/Mistral-7B-v0.1,xnli_el,0-shot,brier_score,0.7705875419124961,
mistralai/Mistral-7B-v0.1,xnli_th,0-shot,brier_score,0.9058259849840192,
mistralai/Mistral-7B-v0.1,logiqa2,0-shot,brier_score,0.8994132704589166,
mistralai/Mistral-7B-v0.1,mathqa,0-shot,brier_score,0.8214663265573888,
mistralai/Mistral-7B-v0.1,lambada_standard,0-shot,perplexity,3.776806210204293,0.0726222723228476
mistralai/Mistral-7B-v0.1,lambada_standard,0-shot,accuracy,0.694352804191733,0.0064181871627658
mistralai/Mistral-7B-v0.1,lambada_openai,0-shot,perplexity,3.1806434293677626,0.0581877317363529
mistralai/Mistral-7B-v0.1,lambada_openai,0-shot,accuracy,0.7568406753347564,0.0059766767751295
jisukim8873/falcon-7B-case-5,arc:challenge,25-shot,accuracy,0.4462457337883959,0.0145267055485399
jisukim8873/falcon-7B-case-5,arc:challenge,25-shot,acc_norm,0.4837883959044368,0.0146037085674149
jisukim8873/falcon-7B-case-5,hellaswag,10-shot,accuracy,0.5970922127066322,0.0048948011198986
jisukim8873/falcon-7B-case-5,hellaswag,10-shot,acc_norm,0.7851025692093209,0.0040991171222808
jisukim8873/falcon-7B-case-5,hendrycksTest-abstract_algebra,5-shot,accuracy,0.34,0.0476095228569523
jisukim8873/falcon-7B-case-5,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.34,0.0476095228569523
jisukim8873/falcon-7B-case-5,hendrycksTest-anatomy,5-shot,accuracy,0.3185185185185185,0.040247784019771
jisukim8873/falcon-7B-case-5,hendrycksTest-anatomy,5-shot,acc_norm,0.3185185185185185,0.040247784019771
jisukim8873/falcon-7B-case-5,hendrycksTest-astronomy,5-shot,accuracy,0.2368421052631578,0.0345977760681053
jisukim8873/falcon-7B-case-5,hendrycksTest-astronomy,5-shot,acc_norm,0.2368421052631578,0.0345977760681053
jisukim8873/falcon-7B-case-5,hendrycksTest-business_ethics,5-shot,accuracy,0.19,0.0394277244403662
jisukim8873/falcon-7B-case-5,hendrycksTest-business_ethics,5-shot,acc_norm,0.19,0.0394277244403662
jisukim8873/falcon-7B-case-5,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3169811320754717,0.0286372356398009
jisukim8873/falcon-7B-case-5,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3169811320754717,0.0286372356398009
jisukim8873/falcon-7B-case-5,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.0371617743756601
jisukim8873/falcon-7B-case-5,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.0371617743756601
jisukim8873/falcon-7B-case-5,hendrycksTest-college_chemistry,5-shot,accuracy,0.16,0.036845294917747
jisukim8873/falcon-7B-case-5,hendrycksTest-college_chemistry,5-shot,acc_norm,0.16,0.036845294917747
jisukim8873/falcon-7B-case-5,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-5,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-5,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.0440844002276807
jisukim8873/falcon-7B-case-5,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.0440844002276807
jisukim8873/falcon-7B-case-5,hendrycksTest-college_medicine,5-shot,accuracy,0.2832369942196532,0.0343556805604787
jisukim8873/falcon-7B-case-5,hendrycksTest-college_medicine,5-shot,acc_norm,0.2832369942196532,0.0343556805604787
jisukim8873/falcon-7B-case-5,hendrycksTest-college_physics,5-shot,accuracy,0.1666666666666666,0.0370828466241654
jisukim8873/falcon-7B-case-5,hendrycksTest-college_physics,5-shot,acc_norm,0.1666666666666666,0.0370828466241654
jisukim8873/falcon-7B-case-5,hendrycksTest-computer_security,5-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-5,hendrycksTest-computer_security,5-shot,acc_norm,0.33,0.047258156262526
jisukim8873/falcon-7B-case-5,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3489361702127659,0.0311585221313577
jisukim8873/falcon-7B-case-5,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3489361702127659,0.0311585221313577
jisukim8873/falcon-7B-case-5,hendrycksTest-econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
jisukim8873/falcon-7B-case-5,hendrycksTest-econometrics,5-shot,acc_norm,0.2456140350877192,0.0404933929774814
jisukim8873/falcon-7B-case-5,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2689655172413793,0.0369518331165023
jisukim8873/falcon-7B-case-5,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2689655172413793,0.0369518331165023
jisukim8873/falcon-7B-case-5,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2671957671957672,0.0227896731457765
jisukim8873/falcon-7B-case-5,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2671957671957672,0.0227896731457765
jisukim8873/falcon-7B-case-5,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.0356701667527686
jisukim8873/falcon-7B-case-5,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.0356701667527686
jisukim8873/falcon-7B-case-5,hendrycksTest-global_facts,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-5,hendrycksTest-global_facts,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_biology,5-shot,accuracy,0.3419354838709677,0.0269852895765527
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3419354838709677,0.0269852895765527
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3152709359605911,0.0326908087197018
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3152709359605911,0.0326908087197018
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.39,0.0490207130000197
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.39,0.0490207130000197
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3454545454545454,0.0371315806748191
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3454545454545454,0.0371315806748191
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_geography,5-shot,accuracy,0.2878787878787879,0.0322588351230099
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2878787878787879,0.0322588351230099
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.0325771407770966
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.0325771407770966
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2615384615384615,0.0222821412042044
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2615384615384615,0.0222821412042044
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2555555555555555,0.026593939101844
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2555555555555555,0.026593939101844
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2857142857142857,0.0293445725006343
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2857142857142857,0.0293445725006343
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.0363132980396965
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3009174311926605,0.0196647513668021
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3009174311926605,0.0196647513668021
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2037037037037037,0.0274674018040579
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2037037037037037,0.0274674018040579
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2941176470588235,0.0319800166011507
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2941176470588235,0.0319800166011507
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3333333333333333,0.0306858205966108
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3333333333333333,0.0306858205966108
jisukim8873/falcon-7B-case-5,hendrycksTest-human_aging,5-shot,accuracy,0.4260089686098655,0.0331883328621728
jisukim8873/falcon-7B-case-5,hendrycksTest-human_aging,5-shot,acc_norm,0.4260089686098655,0.0331883328621728
jisukim8873/falcon-7B-case-5,hendrycksTest-human_sexuality,5-shot,accuracy,0.2824427480916031,0.0394840612576836
jisukim8873/falcon-7B-case-5,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2824427480916031,0.0394840612576836
jisukim8873/falcon-7B-case-5,hendrycksTest-international_law,5-shot,accuracy,0.3636363636363636,0.0439132628672407
jisukim8873/falcon-7B-case-5,hendrycksTest-international_law,5-shot,acc_norm,0.3636363636363636,0.0439132628672407
jisukim8873/falcon-7B-case-5,hendrycksTest-jurisprudence,5-shot,accuracy,0.3055555555555556,0.0445319750737498
jisukim8873/falcon-7B-case-5,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3055555555555556,0.0445319750737498
jisukim8873/falcon-7B-case-5,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2515337423312883,0.0340899788685752
jisukim8873/falcon-7B-case-5,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2515337423312883,0.0340899788685752
jisukim8873/falcon-7B-case-5,hendrycksTest-machine_learning,5-shot,accuracy,0.3303571428571428,0.0446428571428571
jisukim8873/falcon-7B-case-5,hendrycksTest-machine_learning,5-shot,acc_norm,0.3303571428571428,0.0446428571428571
jisukim8873/falcon-7B-case-5,hendrycksTest-management,5-shot,accuracy,0.3009708737864077,0.0454160944650394
jisukim8873/falcon-7B-case-5,hendrycksTest-management,5-shot,acc_norm,0.3009708737864077,0.0454160944650394
jisukim8873/falcon-7B-case-5,hendrycksTest-marketing,5-shot,accuracy,0.3675213675213675,0.0315853915774563
jisukim8873/falcon-7B-case-5,hendrycksTest-marketing,5-shot,acc_norm,0.3675213675213675,0.0315853915774563
jisukim8873/falcon-7B-case-5,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-5,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-5,hendrycksTest-miscellaneous,5-shot,accuracy,0.3614303959131545,0.0171796013289007
jisukim8873/falcon-7B-case-5,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3614303959131545,0.0171796013289007
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_disputes,5-shot,accuracy,0.3699421965317919,0.0259924720293063
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3699421965317919,0.0259924720293063
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2491620111731843,0.0144658938298599
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2491620111731843,0.0144658938298599
jisukim8873/falcon-7B-case-5,hendrycksTest-nutrition,5-shot,accuracy,0.3104575163398693,0.0264930332251458
jisukim8873/falcon-7B-case-5,hendrycksTest-nutrition,5-shot,acc_norm,0.3104575163398693,0.0264930332251458
jisukim8873/falcon-7B-case-5,hendrycksTest-philosophy,5-shot,accuracy,0.337620578778135,0.0268588258794885
jisukim8873/falcon-7B-case-5,hendrycksTest-philosophy,5-shot,acc_norm,0.337620578778135,0.0268588258794885
jisukim8873/falcon-7B-case-5,hendrycksTest-prehistory,5-shot,accuracy,0.3055555555555556,0.0256308249756213
jisukim8873/falcon-7B-case-5,hendrycksTest-prehistory,5-shot,acc_norm,0.3055555555555556,0.0256308249756213
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2482269503546099,0.0257700156442903
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_law,5-shot,accuracy,0.2907431551499348,0.0115980623728519
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_law,5-shot,acc_norm,0.2907431551499348,0.0115980623728519
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_medicine,5-shot,accuracy,0.1764705882352941,0.0231574683085593
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1764705882352941,0.0231574683085593
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_psychology,5-shot,accuracy,0.2892156862745098,0.0183425298452759
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2892156862745098,0.0183425298452759
jisukim8873/falcon-7B-case-5,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
jisukim8873/falcon-7B-case-5,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.0417234303870538
jisukim8873/falcon-7B-case-5,hendrycksTest-security_studies,5-shot,accuracy,0.2653061224489796,0.0282638899437845
jisukim8873/falcon-7B-case-5,hendrycksTest-security_studies,5-shot,acc_norm,0.2653061224489796,0.0282638899437845
jisukim8873/falcon-7B-case-5,hendrycksTest-sociology,5-shot,accuracy,0.3134328358208955,0.0328018820534864
jisukim8873/falcon-7B-case-5,hendrycksTest-sociology,5-shot,acc_norm,0.3134328358208955,0.0328018820534864
jisukim8873/falcon-7B-case-5,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.42,0.0496044963748858
jisukim8873/falcon-7B-case-5,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.42,0.0496044963748858
jisukim8873/falcon-7B-case-5,hendrycksTest-virology,5-shot,accuracy,0.3734939759036144,0.0376584511716886
jisukim8873/falcon-7B-case-5,hendrycksTest-virology,5-shot,acc_norm,0.3734939759036144,0.0376584511716886
jisukim8873/falcon-7B-case-5,hendrycksTest-world_religions,5-shot,accuracy,0.3567251461988304,0.0367401300286095
jisukim8873/falcon-7B-case-5,hendrycksTest-world_religions,5-shot,acc_norm,0.3567251461988304,0.0367401300286095
jisukim8873/falcon-7B-case-5,truthfulqa:mc,0-shot,mc1,0.2484700122399021,0.0151274270965206
jisukim8873/falcon-7B-case-5,truthfulqa:mc,0-shot,mc2,0.3603496537547557,0.0141745924467294
jisukim8873/falcon-7B-case-5,winogrande,5-shot,accuracy,0.7182320441988951,0.0126433260118529
jisukim8873/falcon-7B-case-5,gsm8k,5-shot,accuracy,0.0796057619408642,0.0074559243386762
jisukim8873/falcon-7B-case-5,minerva_math_precalc,5-shot,accuracy,0.0073260073260073,0.003652908089383
jisukim8873/falcon-7B-case-5,minerva_math_prealgebra,5-shot,accuracy,0.0355912743972445,0.0062812012527095
jisukim8873/falcon-7B-case-5,minerva_math_num_theory,5-shot,accuracy,0.0185185185185185,0.0058069728079122
jisukim8873/falcon-7B-case-5,minerva_math_intermediate_algebra,5-shot,accuracy,0.0110741971207087,0.0034844537978317
jisukim8873/falcon-7B-case-5,minerva_math_geometry,5-shot,accuracy,0.0062630480167014,0.0036083997328878
jisukim8873/falcon-7B-case-5,minerva_math_counting_and_prob,5-shot,accuracy,0.0084388185654008,0.004206007207713
jisukim8873/falcon-7B-case-5,minerva_math_algebra,5-shot,accuracy,0.0117944397641112,0.003134873065301
jisukim8873/falcon-7B-case-5,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
jisukim8873/falcon-7B-case-5,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339
jisukim8873/falcon-7B-case-5,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
jisukim8873/falcon-7B-case-5,arithmetic_2ds,5-shot,accuracy,0.0125,0.0024849471787626
jisukim8873/falcon-7B-case-5,arithmetic_5ds,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_1dc,5-shot,accuracy,0.0565,0.0051640302675624
jisukim8873/falcon-7B-case-5,arithmetic_4ds,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_2dm,5-shot,accuracy,0.041,0.004435012363831
jisukim8873/falcon-7B-case-5,arithmetic_2da,5-shot,accuracy,0.0095,0.00216961485391
jisukim8873/falcon-7B-case-5,gsm8k_cot,5-shot,accuracy,0.1061410159211523,0.0084843469484345
jisukim8873/falcon-7B-case-5,anli_r2,0-shot,brier_score,0.9513281547700652,
jisukim8873/falcon-7B-case-5,anli_r3,0-shot,brier_score,0.8632624385858723,
jisukim8873/falcon-7B-case-5,anli_r1,0-shot,brier_score,0.9747999915331574,
jisukim8873/falcon-7B-case-5,xnli_eu,0-shot,brier_score,1.0516363750130937,
jisukim8873/falcon-7B-case-5,xnli_vi,0-shot,brier_score,0.9989937908225948,
jisukim8873/falcon-7B-case-5,xnli_ru,0-shot,brier_score,0.8313811181787544,
jisukim8873/falcon-7B-case-5,xnli_zh,0-shot,brier_score,0.9903379657251044,
jisukim8873/falcon-7B-case-5,xnli_tr,0-shot,brier_score,0.9686053353297328,
jisukim8873/falcon-7B-case-5,xnli_fr,0-shot,brier_score,0.7336352662626028,
jisukim8873/falcon-7B-case-5,xnli_en,0-shot,brier_score,0.6661677143555013,
jisukim8873/falcon-7B-case-5,xnli_ur,0-shot,brier_score,1.2696545502356362,
jisukim8873/falcon-7B-case-5,xnli_ar,0-shot,brier_score,1.2884254322323176,
jisukim8873/falcon-7B-case-5,xnli_de,0-shot,brier_score,0.8278491965645761,
jisukim8873/falcon-7B-case-5,xnli_hi,0-shot,brier_score,1.04730207869374,
jisukim8873/falcon-7B-case-5,xnli_es,0-shot,brier_score,0.8100639118042829,
jisukim8873/falcon-7B-case-5,xnli_bg,0-shot,brier_score,0.9677419215546682,
jisukim8873/falcon-7B-case-5,xnli_sw,0-shot,brier_score,1.1267158887404765,
jisukim8873/falcon-7B-case-5,xnli_el,0-shot,brier_score,0.9376379182808556,
jisukim8873/falcon-7B-case-5,xnli_th,0-shot,brier_score,0.9937338487237533,
jisukim8873/falcon-7B-case-5,logiqa2,0-shot,brier_score,1.07732917197947,
jisukim8873/falcon-7B-case-5,mathqa,0-shot,brier_score,0.9433681905807896,
jisukim8873/falcon-7B-case-5,lambada_standard,0-shot,perplexity,4.154513384117848,0.0911278377997474
jisukim8873/falcon-7B-case-5,lambada_standard,0-shot,accuracy,0.6679604114108286,0.0065611862807911
jisukim8873/falcon-7B-case-5,lambada_openai,0-shot,perplexity,3.231212255722818,0.0669021157431911
jisukim8873/falcon-7B-case-5,lambada_openai,0-shot,accuracy,0.7393751212885697,0.0061157880293335
facebook/xglm-4.5B,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_prealgebra,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0011074197120708,0.0011074197120708
facebook/xglm-4.5B,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_algebra,5-shot,accuracy,0.0,
facebook/xglm-4.5B,fld_default,0-shot,accuracy,0.0,
facebook/xglm-4.5B,fld_star,0-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_3da,5-shot,accuracy,0.0005,0.0005
facebook/xglm-4.5B,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005
facebook/xglm-4.5B,arithmetic_4da,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_2ds,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339
facebook/xglm-4.5B,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_2dm,5-shot,accuracy,0.006,0.0017272787111155
facebook/xglm-4.5B,arithmetic_2da,5-shot,accuracy,0.0,
facebook/xglm-4.5B,gsm8k_cot,5-shot,accuracy,0.0219863532979529,0.00403916275811
facebook/xglm-4.5B,gsm8k,5-shot,accuracy,0.0022744503411675,0.0013121578148674
facebook/xglm-4.5B,anli_r2,0-shot,brier_score,0.7423986272054851,
facebook/xglm-4.5B,anli_r3,0-shot,brier_score,0.753360865009507,
facebook/xglm-4.5B,anli_r1,0-shot,brier_score,0.7489020744444143,
facebook/xglm-4.5B,xnli_eu,0-shot,brier_score,0.8835776224752929,
facebook/xglm-4.5B,xnli_vi,0-shot,brier_score,0.734074056481032,
facebook/xglm-4.5B,xnli_ru,0-shot,brier_score,0.7752082188185605,
facebook/xglm-4.5B,xnli_zh,0-shot,brier_score,1.059743376070012,
facebook/xglm-4.5B,xnli_tr,0-shot,brier_score,0.7912014592670232,
facebook/xglm-4.5B,xnli_fr,0-shot,brier_score,0.7417811076459814,
facebook/xglm-4.5B,xnli_en,0-shot,brier_score,0.6323125293224192,
facebook/xglm-4.5B,xnli_ur,0-shot,brier_score,0.9364267567095892,
facebook/xglm-4.5B,xnli_ar,0-shot,brier_score,1.2526435424482891,
facebook/xglm-4.5B,xnli_de,0-shot,brier_score,0.8248883014366909,
facebook/xglm-4.5B,xnli_hi,0-shot,brier_score,0.7748000452700087,
facebook/xglm-4.5B,xnli_es,0-shot,brier_score,0.824750043726708,
facebook/xglm-4.5B,xnli_bg,0-shot,brier_score,0.7293483146313945,
facebook/xglm-4.5B,xnli_sw,0-shot,brier_score,0.7765272974570628,
facebook/xglm-4.5B,xnli_el,0-shot,brier_score,0.813931199828111,
facebook/xglm-4.5B,xnli_th,0-shot,brier_score,0.8560226403534275,
facebook/xglm-4.5B,logiqa2,0-shot,brier_score,1.1510780077217264,
facebook/xglm-4.5B,mathqa,0-shot,brier_score,0.9914166377643306,
facebook/xglm-4.5B,lambada_standard,0-shot,perplexity,9.728468384838203,0.2728256779816688
facebook/xglm-4.5B,lambada_standard,0-shot,accuracy,0.5152338443625073,0.0069627437174515
facebook/xglm-4.5B,lambada_openai,0-shot,perplexity,8.474112679424595,0.2344768606998664
facebook/xglm-4.5B,lambada_openai,0-shot,accuracy,0.526877547059965,0.0069559058962127
facebook/xglm-4.5B,mmlu_world_religions,0-shot,accuracy,0.2105263157894736,0.0312678171466317
facebook/xglm-4.5B,mmlu_formal_logic,0-shot,accuracy,0.1746031746031746,0.0339549002085611
facebook/xglm-4.5B,mmlu_prehistory,0-shot,accuracy,0.2654320987654321,0.0245692236004608
facebook/xglm-4.5B,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705748
facebook/xglm-4.5B,mmlu_high_school_world_history,0-shot,accuracy,0.2320675105485232,0.0274797445508085
facebook/xglm-4.5B,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763,0.0238680032625001
facebook/xglm-4.5B,mmlu_professional_law,0-shot,accuracy,0.2698826597131681,0.0113373810842504
facebook/xglm-4.5B,mmlu_logical_fallacies,0-shot,accuracy,0.3006134969325153,0.0360251131880677
facebook/xglm-4.5B,mmlu_high_school_us_history,0-shot,accuracy,0.2107843137254902,0.0286265479124373
facebook/xglm-4.5B,mmlu_philosophy,0-shot,accuracy,0.3022508038585209,0.0260827006953996
facebook/xglm-4.5B,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
facebook/xglm-4.5B,mmlu_international_law,0-shot,accuracy,0.3471074380165289,0.0434572457029253
facebook/xglm-4.5B,mmlu_high_school_european_history,0-shot,accuracy,0.2121212121212121,0.0319227156954829
facebook/xglm-4.5B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2538860103626943,0.0314102478056531
facebook/xglm-4.5B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678637
facebook/xglm-4.5B,mmlu_high_school_geography,0-shot,accuracy,0.2676767676767677,0.0315444988827028
facebook/xglm-4.5B,mmlu_high_school_psychology,0-shot,accuracy,0.255045871559633,0.0186885008565358
facebook/xglm-4.5B,mmlu_public_relations,0-shot,accuracy,0.2363636363636363,0.0406930631972137
facebook/xglm-4.5B,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-4.5B,mmlu_sociology,0-shot,accuracy,0.2786069651741293,0.0317005618349731
facebook/xglm-4.5B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2435897435897435,0.0217637336841739
facebook/xglm-4.5B,mmlu_security_studies,0-shot,accuracy,0.236734693877551,0.0272128358840731
facebook/xglm-4.5B,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612379
facebook/xglm-4.5B,mmlu_human_sexuality,0-shot,accuracy,0.2671755725190839,0.0388084830108239
facebook/xglm-4.5B,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.0418577442402205
facebook/xglm-4.5B,mmlu_miscellaneous,0-shot,accuracy,0.2068965517241379,0.0144856560416691
facebook/xglm-4.5B,mmlu_marketing,0-shot,accuracy,0.2393162393162393,0.0279518268089243
facebook/xglm-4.5B,mmlu_management,0-shot,accuracy,0.2912621359223301,0.0449867632057292
facebook/xglm-4.5B,mmlu_nutrition,0-shot,accuracy,0.2581699346405229,0.0250585033169581
facebook/xglm-4.5B,mmlu_medical_genetics,0-shot,accuracy,0.26,0.0440844002276807
facebook/xglm-4.5B,mmlu_human_aging,0-shot,accuracy,0.2152466367713004,0.0275840666022082
facebook/xglm-4.5B,mmlu_professional_medicine,0-shot,accuracy,0.3272058823529412,0.0285014528603965
facebook/xglm-4.5B,mmlu_college_medicine,0-shot,accuracy,0.2716763005780346,0.0339175032232165
facebook/xglm-4.5B,mmlu_business_ethics,0-shot,accuracy,0.23,0.042295258468165
facebook/xglm-4.5B,mmlu_clinical_knowledge,0-shot,accuracy,0.2264150943396226,0.0257575598931067
facebook/xglm-4.5B,mmlu_global_facts,0-shot,accuracy,0.34,0.0476095228569523
facebook/xglm-4.5B,mmlu_virology,0-shot,accuracy,0.1927710843373494,0.0307098240505652
facebook/xglm-4.5B,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.0266845643404609
facebook/xglm-4.5B,mmlu_college_physics,0-shot,accuracy,0.196078431372549,0.0395058186117996
facebook/xglm-4.5B,mmlu_high_school_physics,0-shot,accuracy,0.2649006622516556,0.0360303854536038
facebook/xglm-4.5B,mmlu_high_school_biology,0-shot,accuracy,0.2645161290322581,0.0250918923788592
facebook/xglm-4.5B,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.0371617743756601
facebook/xglm-4.5B,mmlu_anatomy,0-shot,accuracy,0.3259259259259259,0.040491220417025
facebook/xglm-4.5B,mmlu_college_chemistry,0-shot,accuracy,0.24,0.0429234695990928
facebook/xglm-4.5B,mmlu_computer_security,0-shot,accuracy,0.29,0.0456048021572068
facebook/xglm-4.5B,mmlu_college_computer_science,0-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-4.5B,mmlu_astronomy,0-shot,accuracy,0.2236842105263158,0.033911609343436
facebook/xglm-4.5B,mmlu_college_mathematics,0-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-4.5B,mmlu_conceptual_physics,0-shot,accuracy,0.2425531914893617,0.0280202262712002
facebook/xglm-4.5B,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.042295258468165
facebook/xglm-4.5B,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.0456048021572068
facebook/xglm-4.5B,mmlu_machine_learning,0-shot,accuracy,0.25,0.0410997468263393
facebook/xglm-4.5B,mmlu_high_school_chemistry,0-shot,accuracy,0.2463054187192118,0.0303150992856177
facebook/xglm-4.5B,mmlu_high_school_statistics,0-shot,accuracy,0.2777777777777778,0.0305467452649531
facebook/xglm-4.5B,mmlu_elementary_mathematics,0-shot,accuracy,0.2539682539682539,0.0224180428911139
facebook/xglm-4.5B,mmlu_electrical_engineering,0-shot,accuracy,0.2344827586206896,0.0353062587434659
facebook/xglm-4.5B,mmlu_high_school_mathematics,0-shot,accuracy,0.2481481481481481,0.0263357394040558
facebook/xglm-4.5B,arc_challenge,25-shot,accuracy,0.2977815699658703,0.0133630801072444
facebook/xglm-4.5B,arc_challenge,25-shot,acc_norm,0.3131399317406143,0.0135526715436235
facebook/xglm-4.5B,hellaswag,10-shot,accuracy,0.4334793865763792,0.0049454247716115
facebook/xglm-4.5B,hellaswag,10-shot,acc_norm,0.5794662417845051,0.0049263585644945
facebook/xglm-4.5B,truthfulqa_mc2,0-shot,accuracy,0.3582664656425214,0.0137742966652104
facebook/xglm-4.5B,truthfulqa_gen,0-shot,bleu_max,11.647008551534103,0.5837506738382987
facebook/xglm-4.5B,truthfulqa_gen,0-shot,bleu_acc,0.2533659730722154,0.0152258993408268
facebook/xglm-4.5B,truthfulqa_gen,0-shot,bleu_diff,-3.651974700389702,0.4711796991923317
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge1_max,29.74676225067295,0.8122994874104966
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge1_acc,0.2766217870257038,0.0156596057553269
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge1_diff,-5.319592251781975,0.5675727855582745
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge2_max,17.451517261760348,0.7677122688036538
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge2_acc,0.1872705018359853,0.013657229868067
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge2_diff,-6.283551976079485,0.6225195424994728
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rougeL_max,27.33216522895946,0.7816913352147911
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rougeL_acc,0.2741738066095471,0.0156165184972193
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rougeL_diff,-5.342717008492413,0.5591802833523164
facebook/xglm-4.5B,truthfulqa_mc1,0-shot,accuracy,0.208078335373317,0.0142105034735766
facebook/xglm-4.5B,winogrande,5-shot,accuracy,0.5493291239147593,0.0139839288690402
meta-llama/Llama-2-7b-hf,minerva_math_precalc,5-shot,accuracy,0.0183150183150183,0.0057436967316536
meta-llama/Llama-2-7b-hf,minerva_math_prealgebra,5-shot,accuracy,0.0711825487944891,0.0087175073906075
meta-llama/Llama-2-7b-hf,minerva_math_num_theory,5-shot,accuracy,0.0185185185185185,0.0058069728079122
meta-llama/Llama-2-7b-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.0210409745293466,0.0047787236233196
meta-llama/Llama-2-7b-hf,minerva_math_geometry,5-shot,accuracy,0.0375782881002087,0.0086983575090329
meta-llama/Llama-2-7b-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.0253164556962025,0.0072227519260437
meta-llama/Llama-2-7b-hf,minerva_math_algebra,5-shot,accuracy,0.0320134793597304,0.005111622218276
meta-llama/Llama-2-7b-hf,fld_default,0-shot,accuracy,0.0,
meta-llama/Llama-2-7b-hf,fld_star,0-shot,accuracy,0.0,
meta-llama/Llama-2-7b-hf,arithmetic_3da,5-shot,accuracy,0.8525,0.0079311617473944
meta-llama/Llama-2-7b-hf,arithmetic_3ds,5-shot,accuracy,0.439,0.0110995991166473
meta-llama/Llama-2-7b-hf,arithmetic_4da,5-shot,accuracy,0.6725,0.0104965214943684
meta-llama/Llama-2-7b-hf,arithmetic_2ds,5-shot,accuracy,0.5025,0.0111829962309907
meta-llama/Llama-2-7b-hf,arithmetic_5ds,5-shot,accuracy,0.2475,0.0096523810134916
meta-llama/Llama-2-7b-hf,arithmetic_5da,5-shot,accuracy,0.405,0.0109794250253344
meta-llama/Llama-2-7b-hf,arithmetic_1dc,5-shot,accuracy,0.1845,0.0086756849155773
meta-llama/Llama-2-7b-hf,arithmetic_4ds,5-shot,accuracy,0.363,0.0107551539583744
meta-llama/Llama-2-7b-hf,arithmetic_2dm,5-shot,accuracy,0.1545,0.0080837830731894
meta-llama/Llama-2-7b-hf,arithmetic_2da,5-shot,accuracy,0.8885,0.0070397907871727
meta-llama/Llama-2-7b-hf,gsm8k_cot,5-shot,accuracy,0.1440485216072782,0.0096721109730652
meta-llama/Llama-2-7b-hf,gsm8k,5-shot,accuracy,0.1402577710386656,0.0095651082814286
meta-llama/Llama-2-7b-hf,anli_r2,0-shot,brier_score,0.6728312849024087,
meta-llama/Llama-2-7b-hf,anli_r3,0-shot,brier_score,0.675924163095268,
meta-llama/Llama-2-7b-hf,anli_r1,0-shot,brier_score,0.680913669643727,
meta-llama/Llama-2-7b-hf,xnli_eu,0-shot,brier_score,1.0405008687131228,
meta-llama/Llama-2-7b-hf,xnli_vi,0-shot,brier_score,0.9640582331183832,
meta-llama/Llama-2-7b-hf,xnli_ru,0-shot,brier_score,0.8785393767177124,
meta-llama/Llama-2-7b-hf,xnli_zh,0-shot,brier_score,0.9451672393270156,
meta-llama/Llama-2-7b-hf,xnli_tr,0-shot,brier_score,0.8861324289870143,
meta-llama/Llama-2-7b-hf,xnli_fr,0-shot,brier_score,0.7371020830089063,
meta-llama/Llama-2-7b-hf,xnli_en,0-shot,brier_score,0.6309192441010487,
meta-llama/Llama-2-7b-hf,xnli_ur,0-shot,brier_score,1.2513457477843288,
meta-llama/Llama-2-7b-hf,xnli_ar,0-shot,brier_score,1.0275160054419723,
meta-llama/Llama-2-7b-hf,xnli_de,0-shot,brier_score,0.7952906725023694,
meta-llama/Llama-2-7b-hf,xnli_hi,0-shot,brier_score,0.9207229324542658,
meta-llama/Llama-2-7b-hf,xnli_es,0-shot,brier_score,0.9614303254112364,
meta-llama/Llama-2-7b-hf,xnli_bg,0-shot,brier_score,0.8810276928274537,
meta-llama/Llama-2-7b-hf,xnli_sw,0-shot,brier_score,0.8940776492925108,
meta-llama/Llama-2-7b-hf,xnli_el,0-shot,brier_score,0.8013356806216894,
meta-llama/Llama-2-7b-hf,xnli_th,0-shot,brier_score,0.9641360198328498,
meta-llama/Llama-2-7b-hf,logiqa2,0-shot,brier_score,0.9310549072453368,
meta-llama/Llama-2-7b-hf,mathqa,0-shot,brier_score,0.8984744744150647,
meta-llama/Llama-2-7b-hf,lambada_standard,0-shot,perplexity,4.129570848882444,0.0822446133353049
meta-llama/Llama-2-7b-hf,lambada_standard,0-shot,accuracy,0.6821269163594023,0.0064874129551929
meta-llama/Llama-2-7b-hf,lambada_openai,0-shot,perplexity,3.395129491760032,0.0667431018007937
meta-llama/Llama-2-7b-hf,lambada_openai,0-shot,accuracy,0.7391810595769455,0.0061172615702386
meta-llama/Llama-2-7b-hf,mmlu_world_religions,0-shot,accuracy,0.695906432748538,0.0352821125824523
meta-llama/Llama-2-7b-hf,mmlu_formal_logic,0-shot,accuracy,0.2936507936507936,0.0407352432214712
meta-llama/Llama-2-7b-hf,mmlu_prehistory,0-shot,accuracy,0.5030864197530864,0.0278202141585943
meta-llama/Llama-2-7b-hf,mmlu_moral_scenarios,0-shot,accuracy,0.2402234636871508,0.0142883438039253
meta-llama/Llama-2-7b-hf,mmlu_high_school_world_history,0-shot,accuracy,0.6371308016877637,0.0312992082553021
meta-llama/Llama-2-7b-hf,mmlu_moral_disputes,0-shot,accuracy,0.5057803468208093,0.0269172961791491
meta-llama/Llama-2-7b-hf,mmlu_professional_law,0-shot,accuracy,0.3670143415906128,0.0123102642448421
meta-llama/Llama-2-7b-hf,mmlu_logical_fallacies,0-shot,accuracy,0.5153374233128835,0.0392652237870884
meta-llama/Llama-2-7b-hf,mmlu_high_school_us_history,0-shot,accuracy,0.5441176470588235,0.0349562452201547
meta-llama/Llama-2-7b-hf,mmlu_philosophy,0-shot,accuracy,0.5916398713826366,0.0279170507484846
meta-llama/Llama-2-7b-hf,mmlu_jurisprudence,0-shot,accuracy,0.5277777777777778,0.0482621729413989
meta-llama/Llama-2-7b-hf,mmlu_international_law,0-shot,accuracy,0.6528925619834711,0.0434572457029253
meta-llama/Llama-2-7b-hf,mmlu_high_school_european_history,0-shot,accuracy,0.6,0.0382546027838002
meta-llama/Llama-2-7b-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6787564766839378,0.0336995086854906
meta-llama/Llama-2-7b-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.4327731092436975,0.0321835810774261
meta-llama/Llama-2-7b-hf,mmlu_high_school_geography,0-shot,accuracy,0.494949494949495,0.035621707606254
meta-llama/Llama-2-7b-hf,mmlu_high_school_psychology,0-shot,accuracy,0.6238532110091743,0.020769231968205
meta-llama/Llama-2-7b-hf,mmlu_public_relations,0-shot,accuracy,0.5636363636363636,0.0475018505890729
meta-llama/Llama-2-7b-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.62,0.0487831731214563
meta-llama/Llama-2-7b-hf,mmlu_sociology,0-shot,accuracy,0.6517412935323383,0.0336878746611545
meta-llama/Llama-2-7b-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4512820512820513,0.0252303812389348
meta-llama/Llama-2-7b-hf,mmlu_security_studies,0-shot,accuracy,0.473469387755102,0.0319641273452327
meta-llama/Llama-2-7b-hf,mmlu_professional_psychology,0-shot,accuracy,0.4444444444444444,0.0201025838958871
meta-llama/Llama-2-7b-hf,mmlu_human_sexuality,0-shot,accuracy,0.549618320610687,0.0436364369852477
meta-llama/Llama-2-7b-hf,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.0418577442402205
meta-llama/Llama-2-7b-hf,mmlu_miscellaneous,0-shot,accuracy,0.6436781609195402,0.0171258537627558
meta-llama/Llama-2-7b-hf,mmlu_marketing,0-shot,accuracy,0.6965811965811965,0.0301182101069426
meta-llama/Llama-2-7b-hf,mmlu_management,0-shot,accuracy,0.5436893203883495,0.0493180199422041
meta-llama/Llama-2-7b-hf,mmlu_nutrition,0-shot,accuracy,0.4967320261437908,0.0286293051940035
meta-llama/Llama-2-7b-hf,mmlu_medical_genetics,0-shot,accuracy,0.51,0.0502418393795691
meta-llama/Llama-2-7b-hf,mmlu_human_aging,0-shot,accuracy,0.5650224215246636,0.0332728337027134
meta-llama/Llama-2-7b-hf,mmlu_professional_medicine,0-shot,accuracy,0.5183823529411765,0.0303523033953519
meta-llama/Llama-2-7b-hf,mmlu_college_medicine,0-shot,accuracy,0.4104046242774566,0.0375075704489553
meta-llama/Llama-2-7b-hf,mmlu_business_ethics,0-shot,accuracy,0.53,0.0501613558046591
meta-llama/Llama-2-7b-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.460377358490566,0.0306760965993891
meta-llama/Llama-2-7b-hf,mmlu_global_facts,0-shot,accuracy,0.32,0.046882617226215
meta-llama/Llama-2-7b-hf,mmlu_virology,0-shot,accuracy,0.4216867469879518,0.0384445318177091
meta-llama/Llama-2-7b-hf,mmlu_professional_accounting,0-shot,accuracy,0.3475177304964539,0.0284066278095909
meta-llama/Llama-2-7b-hf,mmlu_college_physics,0-shot,accuracy,0.2450980392156862,0.0428010583736439
meta-llama/Llama-2-7b-hf,mmlu_high_school_physics,0-shot,accuracy,0.3178807947019867,0.038020397601079
meta-llama/Llama-2-7b-hf,mmlu_high_school_biology,0-shot,accuracy,0.4967741935483871,0.0284434142264383
meta-llama/Llama-2-7b-hf,mmlu_college_biology,0-shot,accuracy,0.4722222222222222,0.0417475257892318
meta-llama/Llama-2-7b-hf,mmlu_anatomy,0-shot,accuracy,0.4666666666666667,0.0430973290103635
meta-llama/Llama-2-7b-hf,mmlu_college_chemistry,0-shot,accuracy,0.34,0.0476095228569523
meta-llama/Llama-2-7b-hf,mmlu_computer_security,0-shot,accuracy,0.61,0.0490207130000197
meta-llama/Llama-2-7b-hf,mmlu_college_computer_science,0-shot,accuracy,0.34,0.0476095228569523
meta-llama/Llama-2-7b-hf,mmlu_astronomy,0-shot,accuracy,0.3881578947368421,0.0396584209751274
meta-llama/Llama-2-7b-hf,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215
meta-llama/Llama-2-7b-hf,mmlu_conceptual_physics,0-shot,accuracy,0.4127659574468085,0.0321847114140035
meta-llama/Llama-2-7b-hf,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.0456048021572068
meta-llama/Llama-2-7b-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.39,0.0490207130000197
meta-llama/Llama-2-7b-hf,mmlu_machine_learning,0-shot,accuracy,0.375,0.0459509138808629
meta-llama/Llama-2-7b-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.3743842364532019,0.0340515538056195
meta-llama/Llama-2-7b-hf,mmlu_high_school_statistics,0-shot,accuracy,0.2638888888888889,0.0300582027043098
meta-llama/Llama-2-7b-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.2671957671957672,0.0227896731457765
meta-llama/Llama-2-7b-hf,mmlu_electrical_engineering,0-shot,accuracy,0.4758620689655172,0.0416180850350152
meta-llama/Llama-2-7b-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.3,0.0279404571362284
meta-llama/Llama-2-7b-hf,arc_challenge,25-shot,accuracy,0.4854948805460751,0.01460524108137
meta-llama/Llama-2-7b-hf,arc_challenge,25-shot,acc_norm,0.5290102389078498,0.0145867763552943
meta-llama/Llama-2-7b-hf,hellaswag,10-shot,accuracy,0.5868352917745469,0.0049139557050801
meta-llama/Llama-2-7b-hf,hellaswag,10-shot,acc_norm,0.7866958773152758,0.0040880347451954
meta-llama/Llama-2-7b-hf,truthfulqa_mc2,0-shot,accuracy,0.3896540994541171,0.0135773498383092
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,bleu_max,30.794481532169428,0.8287919046619883
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,bleu_acc,0.3439412484700122,0.0166290875142768
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,bleu_diff,-5.993291829792142,0.9641520859323808
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge1_max,56.46359723412253,0.8547674057001485
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge1_acc,0.3219094247246022,0.0163555676119604
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge1_diff,-7.067120288688261,1.0624470832704904
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge2_max,42.08844049121037,1.020295723228426
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge2_acc,0.3059975520195838,0.016132229728155
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge2_diff,-8.293387411017884,1.2550691006847554
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rougeL_max,53.62297062326941,0.8821248084779468
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rougeL_acc,0.3243574051407589,0.0163879767796479
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rougeL_diff,-7.257582732253812,1.0716212651200012
meta-llama/Llama-2-7b-hf,truthfulqa_mc1,0-shot,accuracy,0.2521419828641371,0.0152015222462999
meta-llama/Llama-2-7b-hf,winogrande,5-shot,accuracy,0.7458563535911602,0.0122363072197082
cerebras/Cerebras-GPT-1.3B,minerva_math_precalc,5-shot,accuracy,0.0091575091575091,0.0040803060650489
cerebras/Cerebras-GPT-1.3B,minerva_math_prealgebra,5-shot,accuracy,0.0126291618828932,0.003785888218263
cerebras/Cerebras-GPT-1.3B,minerva_math_num_theory,5-shot,accuracy,0.0129629629629629,0.0048721929845815
cerebras/Cerebras-GPT-1.3B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0155038759689922,0.0041136172383604
cerebras/Cerebras-GPT-1.3B,minerva_math_geometry,5-shot,accuracy,0.0062630480167014,0.0036083997328878
cerebras/Cerebras-GPT-1.3B,minerva_math_counting_and_prob,5-shot,accuracy,0.010548523206751,0.0046974537353761
cerebras/Cerebras-GPT-1.3B,minerva_math_algebra,5-shot,accuracy,0.0092670598146588,0.0027823191184887
cerebras/Cerebras-GPT-1.3B,fld_default,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,fld_star,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
cerebras/Cerebras-GPT-1.3B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
cerebras/Cerebras-GPT-1.3B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
cerebras/Cerebras-GPT-1.3B,arithmetic_2ds,5-shot,accuracy,0.013,0.0025335171905233
cerebras/Cerebras-GPT-1.3B,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_1dc,5-shot,accuracy,0.0275,0.0036576719757437
cerebras/Cerebras-GPT-1.3B,arithmetic_4ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_2dm,5-shot,accuracy,0.022,0.0032807593162018
cerebras/Cerebras-GPT-1.3B,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
cerebras/Cerebras-GPT-1.3B,gsm8k_cot,5-shot,accuracy,0.0144048521607278,0.0032820559171369
cerebras/Cerebras-GPT-1.3B,gsm8k,5-shot,accuracy,0.0022744503411675,0.0013121578148673
cerebras/Cerebras-GPT-1.3B,anli_r2,0-shot,brier_score,0.8160206698968121,
cerebras/Cerebras-GPT-1.3B,anli_r3,0-shot,brier_score,0.7564486854420894,
cerebras/Cerebras-GPT-1.3B,anli_r1,0-shot,brier_score,0.8453036236123399,
cerebras/Cerebras-GPT-1.3B,xnli_eu,0-shot,brier_score,1.203122941977538,
cerebras/Cerebras-GPT-1.3B,xnli_vi,0-shot,brier_score,0.9581888474081792,
cerebras/Cerebras-GPT-1.3B,xnli_ru,0-shot,brier_score,0.8555274102354892,
cerebras/Cerebras-GPT-1.3B,xnli_zh,0-shot,brier_score,0.9525134852831152,
cerebras/Cerebras-GPT-1.3B,xnli_tr,0-shot,brier_score,0.8488424294763287,
cerebras/Cerebras-GPT-1.3B,xnli_fr,0-shot,brier_score,0.9004072134991292,
cerebras/Cerebras-GPT-1.3B,xnli_en,0-shot,brier_score,0.676735865099788,
cerebras/Cerebras-GPT-1.3B,xnli_ur,0-shot,brier_score,1.119118044439167,
cerebras/Cerebras-GPT-1.3B,xnli_ar,0-shot,brier_score,1.0334678129216437,
cerebras/Cerebras-GPT-1.3B,xnli_de,0-shot,brier_score,0.8840107844662859,
cerebras/Cerebras-GPT-1.3B,xnli_hi,0-shot,brier_score,0.9489724257727796,
cerebras/Cerebras-GPT-1.3B,xnli_es,0-shot,brier_score,0.9940572286363858,
cerebras/Cerebras-GPT-1.3B,xnli_bg,0-shot,brier_score,0.7972760255131902,
cerebras/Cerebras-GPT-1.3B,xnli_sw,0-shot,brier_score,0.9917977124323484,
cerebras/Cerebras-GPT-1.3B,xnli_el,0-shot,brier_score,1.2932784189714346,
cerebras/Cerebras-GPT-1.3B,xnli_th,0-shot,brier_score,0.7809459010580413,
cerebras/Cerebras-GPT-1.3B,logiqa2,0-shot,brier_score,1.2019264190394812,
cerebras/Cerebras-GPT-1.3B,mathqa,0-shot,brier_score,0.9930432217011608,
cerebras/Cerebras-GPT-1.3B,lambada_standard,0-shot,perplexity,30.33127376461433,1.0890794817806768
cerebras/Cerebras-GPT-1.3B,lambada_standard,0-shot,accuracy,0.3584319813700757,0.0066809281736803
cerebras/Cerebras-GPT-1.3B,lambada_openai,0-shot,perplexity,14.11714896411139,0.4545475556587265
cerebras/Cerebras-GPT-1.3B,lambada_openai,0-shot,accuracy,0.4614787502425771,0.0069452734458058
cerebras/Cerebras-GPT-1.3B,mmlu_world_religions,0-shot,accuracy,0.3040935672514619,0.0352821125824523
cerebras/Cerebras-GPT-1.3B,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.0393253768039287
cerebras/Cerebras-GPT-1.3B,mmlu_prehistory,0-shot,accuracy,0.2623456790123457,0.0244772228561351
cerebras/Cerebras-GPT-1.3B,mmlu_moral_scenarios,0-shot,accuracy,0.2368715083798882,0.0142195707881039
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.029041333510598
cerebras/Cerebras-GPT-1.3B,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763,0.0238680032625001
cerebras/Cerebras-GPT-1.3B,mmlu_professional_law,0-shot,accuracy,0.2379400260756193,0.0108757007876942
cerebras/Cerebras-GPT-1.3B,mmlu_logical_fallacies,0-shot,accuracy,0.3006134969325153,0.0360251131880677
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_us_history,0-shot,accuracy,0.2254901960784313,0.0293311622942517
cerebras/Cerebras-GPT-1.3B,mmlu_philosophy,0-shot,accuracy,0.2668810289389067,0.0251226376088166
cerebras/Cerebras-GPT-1.3B,mmlu_jurisprudence,0-shot,accuracy,0.2407407407407407,0.0413311944024383
cerebras/Cerebras-GPT-1.3B,mmlu_international_law,0-shot,accuracy,0.2727272727272727,0.040655781409087
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_european_history,0-shot,accuracy,0.2666666666666666,0.0345313180188541
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3419689119170984,0.0342346510010428
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2226890756302521,0.0270254334988823
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_geography,0-shot,accuracy,0.3131313131313131,0.0330420508781365
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_psychology,0-shot,accuracy,0.3431192660550459,0.020354777736086
cerebras/Cerebras-GPT-1.3B,mmlu_public_relations,0-shot,accuracy,0.1636363636363636,0.0354343305429867
cerebras/Cerebras-GPT-1.3B,mmlu_us_foreign_policy,0-shot,accuracy,0.2,0.0402015126103684
cerebras/Cerebras-GPT-1.3B,mmlu_sociology,0-shot,accuracy,0.2288557213930348,0.0297052840567724
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3230769230769231,0.0237108885019705
cerebras/Cerebras-GPT-1.3B,mmlu_security_studies,0-shot,accuracy,0.2326530612244897,0.0270492579158961
cerebras/Cerebras-GPT-1.3B,mmlu_professional_psychology,0-shot,accuracy,0.272875816993464,0.0180204741483935
cerebras/Cerebras-GPT-1.3B,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.0372767357559691
cerebras/Cerebras-GPT-1.3B,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
cerebras/Cerebras-GPT-1.3B,mmlu_miscellaneous,0-shot,accuracy,0.247765006385696,0.0154380830805689
cerebras/Cerebras-GPT-1.3B,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674
cerebras/Cerebras-GPT-1.3B,mmlu_management,0-shot,accuracy,0.203883495145631,0.0398913985953177
cerebras/Cerebras-GPT-1.3B,mmlu_nutrition,0-shot,accuracy,0.2973856209150327,0.0261739085067185
cerebras/Cerebras-GPT-1.3B,mmlu_medical_genetics,0-shot,accuracy,0.28,0.0451260859854212
cerebras/Cerebras-GPT-1.3B,mmlu_human_aging,0-shot,accuracy,0.242152466367713,0.0287513923986947
cerebras/Cerebras-GPT-1.3B,mmlu_professional_medicine,0-shot,accuracy,0.4191176470588235,0.0299728071704646
cerebras/Cerebras-GPT-1.3B,mmlu_college_medicine,0-shot,accuracy,0.2774566473988439,0.0341401400704403
cerebras/Cerebras-GPT-1.3B,mmlu_business_ethics,0-shot,accuracy,0.22,0.0416333199893227
cerebras/Cerebras-GPT-1.3B,mmlu_clinical_knowledge,0-shot,accuracy,0.2716981132075471,0.0273777066246707
cerebras/Cerebras-GPT-1.3B,mmlu_global_facts,0-shot,accuracy,0.31,0.0464823198711731
cerebras/Cerebras-GPT-1.3B,mmlu_virology,0-shot,accuracy,0.3313253012048193,0.0366431477728808
cerebras/Cerebras-GPT-1.3B,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.026684564340461
cerebras/Cerebras-GPT-1.3B,mmlu_college_physics,0-shot,accuracy,0.2254901960784313,0.0415830753308328
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649,0.0347918557259966
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_biology,0-shot,accuracy,0.2032258064516129,0.0228916879845549
cerebras/Cerebras-GPT-1.3B,mmlu_college_biology,0-shot,accuracy,0.2361111111111111,0.0355144661081082
cerebras/Cerebras-GPT-1.3B,mmlu_anatomy,0-shot,accuracy,0.2074074074074074,0.0350255317067831
cerebras/Cerebras-GPT-1.3B,mmlu_college_chemistry,0-shot,accuracy,0.24,0.0429234695990928
cerebras/Cerebras-GPT-1.3B,mmlu_computer_security,0-shot,accuracy,0.21,0.0409360180740332
cerebras/Cerebras-GPT-1.3B,mmlu_college_computer_science,0-shot,accuracy,0.34,0.0476095228569523
cerebras/Cerebras-GPT-1.3B,mmlu_astronomy,0-shot,accuracy,0.2105263157894736,0.0331767278753315
cerebras/Cerebras-GPT-1.3B,mmlu_college_mathematics,0-shot,accuracy,0.31,0.0464823198711731
cerebras/Cerebras-GPT-1.3B,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.0295131966255393
cerebras/Cerebras-GPT-1.3B,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.0435194139889244
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.0456048021572068
cerebras/Cerebras-GPT-1.3B,mmlu_machine_learning,0-shot,accuracy,0.2946428571428571,0.0432704093257872
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_chemistry,0-shot,accuracy,0.2364532019704433,0.0298961142917335
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
cerebras/Cerebras-GPT-1.3B,mmlu_elementary_mathematics,0-shot,accuracy,0.2407407407407407,0.0220190800122179
cerebras/Cerebras-GPT-1.3B,mmlu_electrical_engineering,0-shot,accuracy,0.2344827586206896,0.0353062587434659
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
cerebras/Cerebras-GPT-1.3B,arc_challenge,25-shot,accuracy,0.2474402730375426,0.0126103526632926
cerebras/Cerebras-GPT-1.3B,arc_challenge,25-shot,acc_norm,0.2738907849829352,0.0130320049729895
cerebras/Cerebras-GPT-1.3B,hellaswag,10-shot,accuracy,0.3290181238797052,0.0046889631757581
cerebras/Cerebras-GPT-1.3B,hellaswag,10-shot,acc_norm,0.385381398127863,0.0048569064737193
cerebras/Cerebras-GPT-1.3B,truthfulqa_mc2,0-shot,accuracy,0.4270182875007654,0.0148964438724039
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,bleu_max,15.75896364576763,0.5364357683647447
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,bleu_acc,0.3598531211750306,0.0168018604666771
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,bleu_diff,-3.344586321984534,0.5222101463879798
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge1_max,36.903769503650345,0.7998635375855746
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge1_acc,0.2913096695226438,0.0159059870481848
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge1_diff,-6.460703210665295,0.7328034463145772
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge2_max,19.326584406586324,0.8077506474590068
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge2_acc,0.1823745410036719,0.0135180556361872
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge2_diff,-6.646991226024485,0.7429900455011789
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rougeL_max,33.790056148847405,0.78334334845043
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rougeL_acc,0.2851897184822521,0.0158058278744548
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rougeL_diff,-6.286118787315412,0.7141317626988594
cerebras/Cerebras-GPT-1.3B,truthfulqa_mc1,0-shot,accuracy,0.244798041615667,0.0150518694867149
cerebras/Cerebras-GPT-1.3B,winogrande,5-shot,accuracy,0.5343330702446725,0.0140193175315425
mistralai/Mixtral-8x7B-Instruct-v0.1,arc:challenge,25-shot,accuracy,0.6655290102389079,0.0137874603224413
mistralai/Mixtral-8x7B-Instruct-v0.1,arc:challenge,25-shot,acc_norm,0.7013651877133106,0.0133740786150687
mistralai/Mixtral-8x7B-Instruct-v0.1,hellaswag,10-shot,accuracy,0.6858195578570006,0.0046323996774908
mistralai/Mixtral-8x7B-Instruct-v0.1,hellaswag,10-shot,acc_norm,0.8755228042222665,0.0032945048075552
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.42,0.0496044963748858
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.42,0.0496044963748858
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.6666666666666666,0.0407231481187683
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.6666666666666666,0.0407231481187683
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.7894736842105263,0.0331767278753315
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.7894736842105263,0.0331767278753315
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.73,0.0446196043338473
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.73,0.0446196043338473
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7773584905660378,0.025604233470899
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7773584905660378,0.025604233470899
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.8263888888888888,0.0316747338379571
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.8263888888888888,0.0316747338379571
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.5,0.0502518907629606
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.5,0.0502518907629606
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.66,0.0476095228569523
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.66,0.0476095228569523
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.46,0.0500908265962033
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.46,0.0500908265962033
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.7572254335260116,0.0326926380614177
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.7572254335260116,0.0326926380614177
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.4313725490196078,0.0492809959728753
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.4313725490196078,0.0492809959728753
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.81,0.0394277244403662
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.81,0.0394277244403662
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6680851063829787,0.0307837367577456
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6680851063829787,0.0307837367577456
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.6140350877192983,0.0457963942207043
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.6140350877192983,0.0457963942207043
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6482758620689655,0.0397923663749741
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6482758620689655,0.0397923663749741
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4788359788359788,0.0257282309521307
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4788359788359788,0.0257282309521307
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.5238095238095238,0.0446706262840327
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.5238095238095238,0.0446706262840327
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.42,0.0496044963748858
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.42,0.0496044963748858
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.8516129032258064,0.0202227375543303
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8516129032258064,0.0202227375543303
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6206896551724138,0.0341396380590623
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6206896551724138,0.0341396380590623
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.78,0.0416333199893226
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.78,0.0416333199893226
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8,0.0312347523777211
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8,0.0312347523777211
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.8686868686868687,0.0240631564168225
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8686868686868687,0.0240631564168225
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9585492227979274,0.0143854328574764
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9585492227979274,0.0143854328574764
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6974358974358974,0.0232908880537727
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6974358974358974,0.0232908880537727
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3888888888888889,0.0297232789614766
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3888888888888889,0.0297232789614766
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8025210084033614,0.0258591641220514
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8025210084033614,0.0258591641220514
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.4701986754966887,0.0407522499221697
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4701986754966887,0.0407522499221697
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8844036697247707,0.0137087495341726
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8844036697247707,0.0137087495341726
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5972222222222222,0.0334488738299786
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5972222222222222,0.0334488738299786
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8529411764705882,0.0248574780802504
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8529411764705882,0.0248574780802504
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8481012658227848,0.0233638780966324
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8481012658227848,0.0233638780966324
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.757847533632287,0.0287513923986947
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.757847533632287,0.0287513923986947
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.8091603053435115,0.0344651335075259
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8091603053435115,0.0344651335075259
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.8760330578512396,0.0300830987160352
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.8760330578512396,0.0300830987160352
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.0352070399051796
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.0352070399051796
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8159509202453987,0.0304467776879717
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8159509202453987,0.0304467776879717
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.5714285714285714,0.0469711392301021
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.5714285714285714,0.0469711392301021
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-management,5-shot,accuracy,0.8446601941747572,0.0358659473857397
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-management,5-shot,acc_norm,0.8446601941747572,0.0358659473857397
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.9230769230769232,0.0174569878724361
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.9230769230769232,0.0174569878724361
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.77,0.042295258468165
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.77,0.042295258468165
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.879948914431673,0.0116227366920412
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.879948914431673,0.0116227366920412
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.7803468208092486,0.0222896388526178
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7803468208092486,0.0222896388526178
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4603351955307262,0.016669799592112
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4603351955307262,0.016669799592112
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.8202614379084967,0.0219860321820641
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.8202614379084967,0.0219860321820641
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.797427652733119,0.0228273174910596
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.797427652733119,0.0228273174910596
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.8333333333333334,0.02073635840806
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.8333333333333334,0.02073635840806
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.5531914893617021,0.0296582350976669
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5531914893617021,0.0296582350976669
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.5443285528031291,0.0127199495430322
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.5443285528031291,0.0127199495430322
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.7941176470588235,0.0245622043141423
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.7941176470588235,0.0245622043141423
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.7647058823529411,0.0171605872350463
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7647058823529411,0.0171605872350463
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.7090909090909091,0.0435027144292324
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.7090909090909091,0.0435027144292324
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.7714285714285715,0.0268821449223077
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.7714285714285715,0.0268821449223077
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.0220763261018246
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.0220763261018246
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.9,0.0301511344577763
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.9,0.0301511344577763
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5060240963855421,0.0389221219533304
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5060240963855421,0.0389221219533304
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.0251729843501557
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.0251729843501557
mistralai/Mixtral-8x7B-Instruct-v0.1,truthfulqa:mc,0-shot,mc1,0.5006119951040392,0.0175034879388925
mistralai/Mixtral-8x7B-Instruct-v0.1,truthfulqa:mc,0-shot,mc2,0.649788114114722,0.0151192607040758
mistralai/Mixtral-8x7B-Instruct-v0.1,winogrande,5-shot,accuracy,0.8105761641673244,0.0110127904329892
mistralai/Mixtral-8x7B-Instruct-v0.1,gsm8k,5-shot,accuracy,0.6110689916603488,0.0134283824812742
jb723/cross_lingual_epoch2,arc:challenge,25-shot,accuracy,0.3293515358361775,0.0137340576526354
jb723/cross_lingual_epoch2,arc:challenge,25-shot,acc_norm,0.3924914675767918,0.0142696346356707
jb723/cross_lingual_epoch2,hellaswag,10-shot,accuracy,0.3392750448117905,0.0047249566658799
jb723/cross_lingual_epoch2,hellaswag,10-shot,acc_norm,0.4791874128659629,0.004985456752161
jb723/cross_lingual_epoch2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.0451260859854212
jb723/cross_lingual_epoch2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.0451260859854212
jb723/cross_lingual_epoch2,hendrycksTest-anatomy,5-shot,accuracy,0.4148148148148148,0.042561937679014
jb723/cross_lingual_epoch2,hendrycksTest-anatomy,5-shot,acc_norm,0.4148148148148148,0.042561937679014
jb723/cross_lingual_epoch2,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.0373852067611966
jb723/cross_lingual_epoch2,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.0373852067611966
jb723/cross_lingual_epoch2,hendrycksTest-business_ethics,5-shot,accuracy,0.4,0.049236596391733
jb723/cross_lingual_epoch2,hendrycksTest-business_ethics,5-shot,acc_norm,0.4,0.049236596391733
jb723/cross_lingual_epoch2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4339622641509434,0.0305032920133425
jb723/cross_lingual_epoch2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4339622641509434,0.0305032920133425
jb723/cross_lingual_epoch2,hendrycksTest-college_biology,5-shot,accuracy,0.3125,0.0387608545591276
jb723/cross_lingual_epoch2,hendrycksTest-college_biology,5-shot,acc_norm,0.3125,0.0387608545591276
jb723/cross_lingual_epoch2,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.0386122919665369
jb723/cross_lingual_epoch2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.0386122919665369
jb723/cross_lingual_epoch2,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
jb723/cross_lingual_epoch2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
jb723/cross_lingual_epoch2,hendrycksTest-college_mathematics,5-shot,accuracy,0.29,0.0456048021572068
jb723/cross_lingual_epoch2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.29,0.0456048021572068
jb723/cross_lingual_epoch2,hendrycksTest-college_medicine,5-shot,accuracy,0.3583815028901734,0.0365634365335315
jb723/cross_lingual_epoch2,hendrycksTest-college_medicine,5-shot,acc_norm,0.3583815028901734,0.0365634365335315
jb723/cross_lingual_epoch2,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
jb723/cross_lingual_epoch2,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
jb723/cross_lingual_epoch2,hendrycksTest-computer_security,5-shot,accuracy,0.48,0.0502116731568678
jb723/cross_lingual_epoch2,hendrycksTest-computer_security,5-shot,acc_norm,0.48,0.0502116731568678
jb723/cross_lingual_epoch2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.0303635821972381
jb723/cross_lingual_epoch2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.0303635821972381
jb723/cross_lingual_epoch2,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984367
jb723/cross_lingual_epoch2,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.0409698513984367
jb723/cross_lingual_epoch2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3862068965517241,0.0405732473441903
jb723/cross_lingual_epoch2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3862068965517241,0.0405732473441903
jb723/cross_lingual_epoch2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.0224945107675031
jb723/cross_lingual_epoch2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.0224945107675031
jb723/cross_lingual_epoch2,hendrycksTest-formal_logic,5-shot,accuracy,0.3015873015873015,0.0410494726990339
jb723/cross_lingual_epoch2,hendrycksTest-formal_logic,5-shot,acc_norm,0.3015873015873015,0.0410494726990339
jb723/cross_lingual_epoch2,hendrycksTest-global_facts,5-shot,accuracy,0.29,0.0456048021572068
jb723/cross_lingual_epoch2,hendrycksTest-global_facts,5-shot,acc_norm,0.29,0.0456048021572068
jb723/cross_lingual_epoch2,hendrycksTest-high_school_biology,5-shot,accuracy,0.4032258064516129,0.0279061508260411
jb723/cross_lingual_epoch2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4032258064516129,0.0279061508260411
jb723/cross_lingual_epoch2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2463054187192118,0.0303150992856177
jb723/cross_lingual_epoch2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2463054187192118,0.0303150992856177
jb723/cross_lingual_epoch2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.35,0.0479372485441102
jb723/cross_lingual_epoch2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
jb723/cross_lingual_epoch2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3696969696969697,0.0376943031451256
jb723/cross_lingual_epoch2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3696969696969697,0.0376943031451256
jb723/cross_lingual_epoch2,hendrycksTest-high_school_geography,5-shot,accuracy,0.3787878787878788,0.0345608873199374
jb723/cross_lingual_epoch2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3787878787878788,0.0345608873199374
jb723/cross_lingual_epoch2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.4922279792746113,0.0360800322556965
jb723/cross_lingual_epoch2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.4922279792746113,0.0360800322556965
jb723/cross_lingual_epoch2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2923076923076923,0.0230604383808577
jb723/cross_lingual_epoch2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2923076923076923,0.0230604383808577
jb723/cross_lingual_epoch2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2074074074074074,0.0247207131939521
jb723/cross_lingual_epoch2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2074074074074074,0.0247207131939521
jb723/cross_lingual_epoch2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.361344537815126,0.03120469122515
jb723/cross_lingual_epoch2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.361344537815126,0.03120469122515
jb723/cross_lingual_epoch2,hendrycksTest-high_school_physics,5-shot,accuracy,0.23841059602649,0.0347918557259966
jb723/cross_lingual_epoch2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23841059602649,0.0347918557259966
jb723/cross_lingual_epoch2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.4385321100917431,0.0212747130739545
jb723/cross_lingual_epoch2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.4385321100917431,0.0212747130739545
jb723/cross_lingual_epoch2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1898148148148148,0.0267447148346919
jb723/cross_lingual_epoch2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1898148148148148,0.0267447148346919
jb723/cross_lingual_epoch2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3774509803921568,0.034022720443407
jb723/cross_lingual_epoch2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3774509803921568,0.034022720443407
jb723/cross_lingual_epoch2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.4388185654008439,0.0323026493154703
jb723/cross_lingual_epoch2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.4388185654008439,0.0323026493154703
jb723/cross_lingual_epoch2,hendrycksTest-human_aging,5-shot,accuracy,0.4932735426008968,0.0335547659623435
jb723/cross_lingual_epoch2,hendrycksTest-human_aging,5-shot,acc_norm,0.4932735426008968,0.0335547659623435
jb723/cross_lingual_epoch2,hendrycksTest-human_sexuality,5-shot,accuracy,0.4045801526717557,0.0430469379538066
jb723/cross_lingual_epoch2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.4045801526717557,0.0430469379538066
jb723/cross_lingual_epoch2,hendrycksTest-international_law,5-shot,accuracy,0.5206611570247934,0.0456045608638723
jb723/cross_lingual_epoch2,hendrycksTest-international_law,5-shot,acc_norm,0.5206611570247934,0.0456045608638723
jb723/cross_lingual_epoch2,hendrycksTest-jurisprudence,5-shot,accuracy,0.4444444444444444,0.0480375223519019
jb723/cross_lingual_epoch2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.4444444444444444,0.0480375223519019
jb723/cross_lingual_epoch2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3558282208588957,0.0376152138004673
jb723/cross_lingual_epoch2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3558282208588957,0.0376152138004673
jb723/cross_lingual_epoch2,hendrycksTest-machine_learning,5-shot,accuracy,0.3125,0.0439946505757152
jb723/cross_lingual_epoch2,hendrycksTest-machine_learning,5-shot,acc_norm,0.3125,0.0439946505757152
jb723/cross_lingual_epoch2,hendrycksTest-management,5-shot,accuracy,0.3689320388349514,0.0477761518115674
jb723/cross_lingual_epoch2,hendrycksTest-management,5-shot,acc_norm,0.3689320388349514,0.0477761518115674
jb723/cross_lingual_epoch2,hendrycksTest-marketing,5-shot,accuracy,0.6196581196581197,0.0318042520438409
jb723/cross_lingual_epoch2,hendrycksTest-marketing,5-shot,acc_norm,0.6196581196581197,0.0318042520438409
jb723/cross_lingual_epoch2,hendrycksTest-medical_genetics,5-shot,accuracy,0.39,0.0490207130000197
jb723/cross_lingual_epoch2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.39,0.0490207130000197
jb723/cross_lingual_epoch2,hendrycksTest-miscellaneous,5-shot,accuracy,0.4725415070242657,0.0178529812666339
jb723/cross_lingual_epoch2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.4725415070242657,0.0178529812666339
jb723/cross_lingual_epoch2,hendrycksTest-moral_disputes,5-shot,accuracy,0.4046242774566474,0.0264248165940098
jb723/cross_lingual_epoch2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.4046242774566474,0.0264248165940098
jb723/cross_lingual_epoch2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2379888268156424,0.0142426300705749
jb723/cross_lingual_epoch2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2379888268156424,0.0142426300705749
jb723/cross_lingual_epoch2,hendrycksTest-nutrition,5-shot,accuracy,0.4281045751633986,0.0283323974836642
jb723/cross_lingual_epoch2,hendrycksTest-nutrition,5-shot,acc_norm,0.4281045751633986,0.0283323974836642
jb723/cross_lingual_epoch2,hendrycksTest-philosophy,5-shot,accuracy,0.4758842443729904,0.0283650415425645
jb723/cross_lingual_epoch2,hendrycksTest-philosophy,5-shot,acc_norm,0.4758842443729904,0.0283650415425645
jb723/cross_lingual_epoch2,hendrycksTest-prehistory,5-shot,accuracy,0.4537037037037037,0.0277012284685426
jb723/cross_lingual_epoch2,hendrycksTest-prehistory,5-shot,acc_norm,0.4537037037037037,0.0277012284685426
jb723/cross_lingual_epoch2,hendrycksTest-professional_accounting,5-shot,accuracy,0.2765957446808511,0.026684564340461
jb723/cross_lingual_epoch2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2765957446808511,0.026684564340461
jb723/cross_lingual_epoch2,hendrycksTest-professional_law,5-shot,accuracy,0.2985658409387223,0.0116880601417942
jb723/cross_lingual_epoch2,hendrycksTest-professional_law,5-shot,acc_norm,0.2985658409387223,0.0116880601417942
jb723/cross_lingual_epoch2,hendrycksTest-professional_medicine,5-shot,accuracy,0.2573529411764705,0.0265565194700415
jb723/cross_lingual_epoch2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2573529411764705,0.0265565194700415
jb723/cross_lingual_epoch2,hendrycksTest-professional_psychology,5-shot,accuracy,0.369281045751634,0.0195243167448663
jb723/cross_lingual_epoch2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.369281045751634,0.0195243167448663
jb723/cross_lingual_epoch2,hendrycksTest-public_relations,5-shot,accuracy,0.4636363636363636,0.0477644916239619
jb723/cross_lingual_epoch2,hendrycksTest-public_relations,5-shot,acc_norm,0.4636363636363636,0.0477644916239619
jb723/cross_lingual_epoch2,hendrycksTest-security_studies,5-shot,accuracy,0.4122448979591837,0.0315123604467428
jb723/cross_lingual_epoch2,hendrycksTest-security_studies,5-shot,acc_norm,0.4122448979591837,0.0315123604467428
jb723/cross_lingual_epoch2,hendrycksTest-sociology,5-shot,accuracy,0.472636815920398,0.0353023551733468
jb723/cross_lingual_epoch2,hendrycksTest-sociology,5-shot,acc_norm,0.472636815920398,0.0353023551733468
jb723/cross_lingual_epoch2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.57,0.0497569851956242
jb723/cross_lingual_epoch2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.57,0.0497569851956242
jb723/cross_lingual_epoch2,hendrycksTest-virology,5-shot,accuracy,0.3855421686746988,0.0378913442461154
jb723/cross_lingual_epoch2,hendrycksTest-virology,5-shot,acc_norm,0.3855421686746988,0.0378913442461154
jb723/cross_lingual_epoch2,hendrycksTest-world_religions,5-shot,accuracy,0.4385964912280701,0.0380579750559045
jb723/cross_lingual_epoch2,hendrycksTest-world_religions,5-shot,acc_norm,0.4385964912280701,0.0380579750559045
jb723/cross_lingual_epoch2,truthfulqa:mc,0-shot,mc1,0.237454100367197,0.0148962774410418
jb723/cross_lingual_epoch2,truthfulqa:mc,0-shot,mc2,0.4789867119861502,0.0165407753436727
jb723/cross_lingual_epoch2,winogrande,5-shot,accuracy,0.6211523283346487,0.0136337246031803
jb723/cross_lingual_epoch2,drop,3-shot,accuracy,0.049601510067114,0.0022235145171999
jb723/cross_lingual_epoch2,drop,3-shot,f1,0.072944630872483,0.0024214277122181
jb723/cross_lingual_epoch2,gsm8k,5-shot,accuracy,0.0098559514783927,0.0027210765770416
jb723/cross_lingual_epoch2,minerva_math_precalc,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_prealgebra,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_num_theory,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_geometry,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_algebra,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,fld_default,0-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,fld_star,0-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_3da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_3ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_4da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_2ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_5ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_5da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_1dc,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_4ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_2dm,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_2da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,gsm8k_cot,5-shot,accuracy,0.0090978013646702,0.0026153265107756
jb723/cross_lingual_epoch2,anli_r2,0-shot,brier_score,0.874037701565474,
jb723/cross_lingual_epoch2,anli_r3,0-shot,brier_score,0.8702105204736998,
jb723/cross_lingual_epoch2,anli_r1,0-shot,brier_score,0.8702617825185673,
jb723/cross_lingual_epoch2,xnli_eu,0-shot,brier_score,1.3253474537453491,
jb723/cross_lingual_epoch2,xnli_vi,0-shot,brier_score,1.3276025007815453,
jb723/cross_lingual_epoch2,xnli_ru,0-shot,brier_score,1.2227407590964208,
jb723/cross_lingual_epoch2,xnli_zh,0-shot,brier_score,1.2264156406113862,
jb723/cross_lingual_epoch2,xnli_tr,0-shot,brier_score,1.2800307161337745,
jb723/cross_lingual_epoch2,xnli_fr,0-shot,brier_score,1.1092515762780133,
jb723/cross_lingual_epoch2,xnli_en,0-shot,brier_score,0.8103522850538913,
jb723/cross_lingual_epoch2,xnli_ur,0-shot,brier_score,1.1631258828395,
jb723/cross_lingual_epoch2,xnli_ar,0-shot,brier_score,1.274604804567007,
jb723/cross_lingual_epoch2,xnli_de,0-shot,brier_score,0.951505623463306,
jb723/cross_lingual_epoch2,xnli_hi,0-shot,brier_score,1.332992059541093,
jb723/cross_lingual_epoch2,xnli_es,0-shot,brier_score,1.1512902734559032,
jb723/cross_lingual_epoch2,xnli_bg,0-shot,brier_score,1.0172978900397067,
jb723/cross_lingual_epoch2,xnli_sw,0-shot,brier_score,0.9811618617891728,
jb723/cross_lingual_epoch2,xnli_el,0-shot,brier_score,1.2011718465559706,
jb723/cross_lingual_epoch2,xnli_th,0-shot,brier_score,1.332573065123302,
jb723/cross_lingual_epoch2,logiqa2,0-shot,brier_score,1.5381466250180291,
jb723/cross_lingual_epoch2,mathqa,0-shot,brier_score,1.0660116386252705,
jb723/cross_lingual_epoch2,lambada_standard,0-shot,perplexity,354.9488617195369,25.870375094453767
jb723/cross_lingual_epoch2,lambada_standard,0-shot,accuracy,0.2470405588977294,0.0060087203896928
jb723/cross_lingual_epoch2,lambada_openai,0-shot,perplexity,76.30917472321947,3.612279959481105
jb723/cross_lingual_epoch2,lambada_openai,0-shot,accuracy,0.2353968562002716,0.0059105846518009
HuggingFaceTB/SmolLM-360M,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.0356507967070831
HuggingFaceTB/SmolLM-360M,mmlu_formal_logic,0-shot,accuracy,0.2936507936507936,0.0407352432214712
HuggingFaceTB/SmolLM-360M,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.0230167056402621
HuggingFaceTB/SmolLM-360M,mmlu_moral_scenarios,0-shot,accuracy,0.2491620111731843,0.0144658938298599
HuggingFaceTB/SmolLM-360M,mmlu_high_school_world_history,0-shot,accuracy,0.2953586497890295,0.0296963387134228
HuggingFaceTB/SmolLM-360M,mmlu_moral_disputes,0-shot,accuracy,0.2658959537572254,0.0237862032555082
HuggingFaceTB/SmolLM-360M,mmlu_professional_law,0-shot,accuracy,0.2431551499348109,0.0109565566544173
HuggingFaceTB/SmolLM-360M,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.0332201579577674
HuggingFaceTB/SmolLM-360M,mmlu_high_school_us_history,0-shot,accuracy,0.2450980392156862,0.0301902824535019
HuggingFaceTB/SmolLM-360M,mmlu_philosophy,0-shot,accuracy,0.2090032154340836,0.0230931403983742
HuggingFaceTB/SmolLM-360M,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
HuggingFaceTB/SmolLM-360M,mmlu_international_law,0-shot,accuracy,0.2809917355371901,0.0410320383051451
HuggingFaceTB/SmolLM-360M,mmlu_high_school_european_history,0-shot,accuracy,0.2303030303030303,0.0328766675860348
HuggingFaceTB/SmolLM-360M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2176165803108808,0.0297786630377529
HuggingFaceTB/SmolLM-360M,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678638
HuggingFaceTB/SmolLM-360M,mmlu_high_school_geography,0-shot,accuracy,0.2474747474747475,0.0307463007421244
HuggingFaceTB/SmolLM-360M,mmlu_high_school_psychology,0-shot,accuracy,0.2073394495412844,0.0173814155636086
HuggingFaceTB/SmolLM-360M,mmlu_public_relations,0-shot,accuracy,0.2272727272727272,0.0401396455407277
HuggingFaceTB/SmolLM-360M,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.0456048021572068
HuggingFaceTB/SmolLM-360M,mmlu_sociology,0-shot,accuracy,0.2238805970149253,0.0294752502360171
HuggingFaceTB/SmolLM-360M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2435897435897435,0.0217637336841739
HuggingFaceTB/SmolLM-360M,mmlu_security_studies,0-shot,accuracy,0.2081632653061224,0.0259911176728132
HuggingFaceTB/SmolLM-360M,mmlu_professional_psychology,0-shot,accuracy,0.238562091503268,0.0172423858287795
HuggingFaceTB/SmolLM-360M,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
HuggingFaceTB/SmolLM-360M,mmlu_econometrics,0-shot,accuracy,0.2894736842105263,0.0426633944315939
HuggingFaceTB/SmolLM-360M,mmlu_miscellaneous,0-shot,accuracy,0.2822477650063857,0.0160953029698785
HuggingFaceTB/SmolLM-360M,mmlu_marketing,0-shot,accuracy,0.2863247863247863,0.0296143236904566
HuggingFaceTB/SmolLM-360M,mmlu_management,0-shot,accuracy,0.2912621359223301,0.0449867632057292
HuggingFaceTB/SmolLM-360M,mmlu_nutrition,0-shot,accuracy,0.2483660130718954,0.0247399813551135
HuggingFaceTB/SmolLM-360M,mmlu_medical_genetics,0-shot,accuracy,0.31,0.0464823198711731
HuggingFaceTB/SmolLM-360M,mmlu_human_aging,0-shot,accuracy,0.3228699551569506,0.0313814763757549
HuggingFaceTB/SmolLM-360M,mmlu_professional_medicine,0-shot,accuracy,0.3014705882352941,0.0278759821142731
HuggingFaceTB/SmolLM-360M,mmlu_college_medicine,0-shot,accuracy,0.2312138728323699,0.0321473730202946
HuggingFaceTB/SmolLM-360M,mmlu_business_ethics,0-shot,accuracy,0.25,0.0435194139889244
HuggingFaceTB/SmolLM-360M,mmlu_clinical_knowledge,0-shot,accuracy,0.2641509433962264,0.0271342916287416
HuggingFaceTB/SmolLM-360M,mmlu_global_facts,0-shot,accuracy,0.29,0.0456048021572068
HuggingFaceTB/SmolLM-360M,mmlu_virology,0-shot,accuracy,0.2771084337349397,0.0348433159268058
HuggingFaceTB/SmolLM-360M,mmlu_professional_accounting,0-shot,accuracy,0.2801418439716312,0.0267891723511402
HuggingFaceTB/SmolLM-360M,mmlu_college_physics,0-shot,accuracy,0.2549019607843137,0.0433643270799317
HuggingFaceTB/SmolLM-360M,mmlu_high_school_physics,0-shot,accuracy,0.2185430463576159,0.0337423555042569
HuggingFaceTB/SmolLM-360M,mmlu_high_school_biology,0-shot,accuracy,0.2064516129032258,0.0230258996171887
HuggingFaceTB/SmolLM-360M,mmlu_college_biology,0-shot,accuracy,0.2083333333333333,0.0339611620584533
HuggingFaceTB/SmolLM-360M,mmlu_anatomy,0-shot,accuracy,0.2222222222222222,0.0359144408419696
HuggingFaceTB/SmolLM-360M,mmlu_college_chemistry,0-shot,accuracy,0.15,0.0358870281282637
HuggingFaceTB/SmolLM-360M,mmlu_computer_security,0-shot,accuracy,0.3,0.0460566186471838
HuggingFaceTB/SmolLM-360M,mmlu_college_computer_science,0-shot,accuracy,0.2,0.0402015126103684
HuggingFaceTB/SmolLM-360M,mmlu_astronomy,0-shot,accuracy,0.2434210526315789,0.0349234966888423
HuggingFaceTB/SmolLM-360M,mmlu_college_mathematics,0-shot,accuracy,0.23,0.042295258468165
HuggingFaceTB/SmolLM-360M,mmlu_conceptual_physics,0-shot,accuracy,0.3021276595744681,0.0300175544718805
HuggingFaceTB/SmolLM-360M,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
HuggingFaceTB/SmolLM-360M,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276807
HuggingFaceTB/SmolLM-360M,mmlu_machine_learning,0-shot,accuracy,0.2142857142857142,0.0389464112004479
HuggingFaceTB/SmolLM-360M,mmlu_high_school_chemistry,0-shot,accuracy,0.2118226600985221,0.028748983689941
HuggingFaceTB/SmolLM-360M,mmlu_high_school_statistics,0-shot,accuracy,0.2268518518518518,0.0285616501024222
HuggingFaceTB/SmolLM-360M,mmlu_elementary_mathematics,0-shot,accuracy,0.2354497354497354,0.0218515098220317
HuggingFaceTB/SmolLM-360M,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.035659981741353
HuggingFaceTB/SmolLM-360M,mmlu_high_school_mathematics,0-shot,accuracy,0.2925925925925925,0.027738969632176
HuggingFaceTB/SmolLM-360M,arc_challenge,25-shot,accuracy,0.3651877133105802,0.0140702655192688
HuggingFaceTB/SmolLM-360M,arc_challenge,25-shot,acc_norm,0.386518771331058,0.0142300847619104
HuggingFaceTB/SmolLM-360M,hellaswag,10-shot,accuracy,0.4150567616012746,0.0049172481506018
HuggingFaceTB/SmolLM-360M,hellaswag,10-shot,acc_norm,0.5423222465644294,0.0049718741597776
HuggingFaceTB/SmolLM-360M,truthfulqa_mc2,0-shot,accuracy,0.3793017405276516,0.0142555453711488
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,bleu_max,23.51716504412742,0.7176975216279505
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,bleu_acc,0.2864137086903305,0.0158261424395023
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,bleu_diff,-7.083041541904997,0.7077513549659533
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge1_max,48.70952743752934,0.8300761683331853
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge1_acc,0.2753977968176254,0.0156381356677755
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge1_diff,-9.635957207171684,0.7485923569688212
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge2_max,32.566501096217806,0.9326341598087236
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge2_acc,0.2362301101591187,0.0148697550158711
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge2_diff,-10.895866455590795,0.9226904079378292
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rougeL_max,45.55323568193951,0.8340494740388494
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rougeL_acc,0.2607099143206854,0.0153688416207663
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rougeL_diff,-9.756463734915387,0.7580939692627898
HuggingFaceTB/SmolLM-360M,truthfulqa_mc1,0-shot,accuracy,0.2509179926560587,0.0151769850277076
HuggingFaceTB/SmolLM-360M,winogrande,5-shot,accuracy,0.5572217837411207,0.0139601573507849
HuggingFaceTB/SmolLM-360M,gsm8k,5-shot,accuracy,0.0136467020470053,0.0031957470754808
meta-llama/Meta-Llama-3-8B-Instruct,arc:challenge,25-shot,accuracy,0.5716723549488054,0.014460496367599
meta-llama/Meta-Llama-3-8B-Instruct,arc:challenge,25-shot,acc_norm,0.6075085324232082,0.0142696346356707
meta-llama/Meta-Llama-3-8B-Instruct,hellaswag,10-shot,accuracy,0.5898227444732125,0.0049086047320828
meta-llama/Meta-Llama-3-8B-Instruct,hellaswag,10-shot,acc_norm,0.7898824935271859,0.0040655928116959
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-anatomy,5-shot,accuracy,0.6518518518518519,0.0411532461033695
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-anatomy,5-shot,acc_norm,0.6518518518518519,0.0411532461033695
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-astronomy,5-shot,accuracy,0.7039473684210527,0.037150621549989
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-astronomy,5-shot,acc_norm,0.7039473684210527,0.037150621549989
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-business_ethics,5-shot,accuracy,0.69,0.0464823198711731
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-business_ethics,5-shot,acc_norm,0.69,0.0464823198711731
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7471698113207547,0.0267498997712412
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7471698113207547,0.0267498997712412
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_biology,5-shot,accuracy,0.7986111111111112,0.0335364746971383
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_biology,5-shot,acc_norm,0.7986111111111112,0.0335364746971383
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_chemistry,5-shot,accuracy,0.47,0.0501613558046592
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_chemistry,5-shot,acc_norm,0.47,0.0501613558046592
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_computer_science,5-shot,accuracy,0.59,0.049431107042371
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_computer_science,5-shot,acc_norm,0.59,0.049431107042371
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_mathematics,5-shot,accuracy,0.39,0.0490207130000197
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_mathematics,5-shot,acc_norm,0.39,0.0490207130000197
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_medicine,5-shot,accuracy,0.6358381502890174,0.036690724774169
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_medicine,5-shot,acc_norm,0.6358381502890174,0.036690724774169
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_physics,5-shot,accuracy,0.5,0.0497518595104994
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_physics,5-shot,acc_norm,0.5,0.0497518595104994
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-computer_security,5-shot,accuracy,0.77,0.042295258468165
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-computer_security,5-shot,acc_norm,0.77,0.042295258468165
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6042553191489362,0.0319675869783536
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6042553191489362,0.0319675869783536
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-econometrics,5-shot,accuracy,0.6052631578947368,0.0459818805781654
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-econometrics,5-shot,acc_norm,0.6052631578947368,0.0459818805781654
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6275862068965518,0.0402873153294755
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6275862068965518,0.0402873153294755
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4444444444444444,0.0255918577613821
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4444444444444444,0.0255918577613821
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-formal_logic,5-shot,accuracy,0.4841269841269841,0.044698818540726
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-formal_logic,5-shot,acc_norm,0.4841269841269841,0.044698818540726
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-global_facts,5-shot,accuracy,0.4,0.049236596391733
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-global_facts,5-shot,acc_norm,0.4,0.049236596391733
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_biology,5-shot,accuracy,0.7838709677419354,0.0234152934335685
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7838709677419354,0.0234152934335685
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5073891625615764,0.0351760354036101
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5073891625615764,0.0351760354036101
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.75,0.0435194139889244
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.75,0.0435194139889244
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7454545454545455,0.0340150671524903
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7454545454545455,0.0340150671524903
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_geography,5-shot,accuracy,0.8434343434343434,0.0258905203581414
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8434343434343434,0.0258905203581414
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.911917098445596,0.020453746601601
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.911917098445596,0.020453746601601
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.658974358974359,0.024035489676335
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.658974358974359,0.024035489676335
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4,0.0298696050953169
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4,0.0298696050953169
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7647058823529411,0.0275536144678638
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7647058823529411,0.0275536144678638
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_physics,5-shot,accuracy,0.4437086092715231,0.0405652790228173
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4437086092715231,0.0405652790228173
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8366972477064221,0.0158482558065015
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8366972477064221,0.0158482558065015
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5370370370370371,0.0340060362553827
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5370370370370371,0.0340060362553827
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8578431372549019,0.0245098039215686
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8578431372549019,0.0245098039215686
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8438818565400844,0.0236271594603186
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8438818565400844,0.0236271594603186
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_aging,5-shot,accuracy,0.7219730941704036,0.030069584874494
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_aging,5-shot,acc_norm,0.7219730941704036,0.030069584874494
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_sexuality,5-shot,accuracy,0.7786259541984732,0.0364129708131373
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7786259541984732,0.0364129708131373
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-international_law,5-shot,accuracy,0.8181818181818182,0.0352089395109765
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-international_law,5-shot,acc_norm,0.8181818181818182,0.0352089395109765
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-jurisprudence,5-shot,accuracy,0.7777777777777778,0.0401910747255735
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7777777777777778,0.0401910747255735
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7668711656441718,0.0332201579577674
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7668711656441718,0.0332201579577674
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-machine_learning,5-shot,accuracy,0.5446428571428571,0.0472683555371909
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-machine_learning,5-shot,acc_norm,0.5446428571428571,0.0472683555371909
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-management,5-shot,accuracy,0.7864077669902912,0.0405804201564603
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-management,5-shot,acc_norm,0.7864077669902912,0.0405804201564603
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-marketing,5-shot,accuracy,0.905982905982906,0.0191198927989249
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-marketing,5-shot,acc_norm,0.905982905982906,0.0191198927989249
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-medical_genetics,5-shot,accuracy,0.8,0.0402015126103684
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-medical_genetics,5-shot,acc_norm,0.8,0.0402015126103684
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-miscellaneous,5-shot,accuracy,0.7982120051085568,0.0143517021816368
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-miscellaneous,5-shot,acc_norm,0.7982120051085568,0.0143517021816368
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_disputes,5-shot,accuracy,0.7485549132947977,0.023357365785874
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7485549132947977,0.023357365785874
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4368715083798882,0.0165886808645306
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4368715083798882,0.0165886808645306
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-nutrition,5-shot,accuracy,0.7450980392156863,0.0249541843248799
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-nutrition,5-shot,acc_norm,0.7450980392156863,0.0249541843248799
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-philosophy,5-shot,accuracy,0.7138263665594855,0.0256702592421889
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-philosophy,5-shot,acc_norm,0.7138263665594855,0.0256702592421889
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-prehistory,5-shot,accuracy,0.7407407407407407,0.0243836655310354
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-prehistory,5-shot,acc_norm,0.7407407407407407,0.0243836655310354
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_accounting,5-shot,accuracy,0.5283687943262412,0.029779450957303
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5283687943262412,0.029779450957303
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_law,5-shot,accuracy,0.4784876140808344,0.0127584109410389
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_law,5-shot,acc_norm,0.4784876140808344,0.0127584109410389
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_medicine,5-shot,accuracy,0.7169117647058824,0.0273658611315138
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_medicine,5-shot,acc_norm,0.7169117647058824,0.0273658611315138
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_psychology,5-shot,accuracy,0.7058823529411765,0.0184334276494019
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7058823529411765,0.0184334276494019
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-public_relations,5-shot,accuracy,0.6454545454545455,0.0458200484150541
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-public_relations,5-shot,acc_norm,0.6454545454545455,0.0458200484150541
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-security_studies,5-shot,accuracy,0.7387755102040816,0.0281234293351427
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-security_studies,5-shot,acc_norm,0.7387755102040816,0.0281234293351427
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-sociology,5-shot,accuracy,0.8656716417910447,0.0241126782409007
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-sociology,5-shot,acc_norm,0.8656716417910447,0.0241126782409007
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.0348735088019777
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.0348735088019777
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-virology,5-shot,accuracy,0.5120481927710844,0.0389136449583581
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-virology,5-shot,acc_norm,0.5120481927710844,0.0389136449583581
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-world_religions,5-shot,accuracy,0.7777777777777778,0.0318857801768639
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-world_religions,5-shot,acc_norm,0.7777777777777778,0.0318857801768639
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa:mc,0-shot,mc1,0.3623011015911873,0.0168266468972622
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa:mc,0-shot,mc2,0.5164972283615512,0.0151968988187235
meta-llama/Meta-Llama-3-8B-Instruct,winogrande,5-shot,accuracy,0.7561168113654302,0.0120689232789081
meta-llama/Meta-Llama-3-8B-Instruct,gsm8k,5-shot,accuracy,0.755117513267627,0.0118448190278636
openlm-research/open_llama_3b_v2,minerva_math_precalc,5-shot,accuracy,0.0091575091575091,0.0040803060650489
openlm-research/open_llama_3b_v2,minerva_math_prealgebra,5-shot,accuracy,0.0436280137772675,0.0069252669305056
openlm-research/open_llama_3b_v2,minerva_math_num_theory,5-shot,accuracy,0.0129629629629629,0.0048721929845814
openlm-research/open_llama_3b_v2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0155038759689922,0.0041136172383604
openlm-research/open_llama_3b_v2,minerva_math_geometry,5-shot,accuracy,0.0146137787056367,0.0054887134436863
openlm-research/open_llama_3b_v2,minerva_math_counting_and_prob,5-shot,accuracy,0.0147679324894514,0.0055462385896684
openlm-research/open_llama_3b_v2,minerva_math_algebra,5-shot,accuracy,0.02106149957877,0.004169461854206
openlm-research/open_llama_3b_v2,fld_default,0-shot,accuracy,0.0,
openlm-research/open_llama_3b_v2,fld_star,0-shot,accuracy,0.0,
openlm-research/open_llama_3b_v2,arithmetic_3da,5-shot,accuracy,0.284,0.0100857752022693
openlm-research/open_llama_3b_v2,arithmetic_3ds,5-shot,accuracy,0.2605,0.0098167234365402
openlm-research/open_llama_3b_v2,arithmetic_4da,5-shot,accuracy,0.0825,0.006153519960474
openlm-research/open_llama_3b_v2,arithmetic_2ds,5-shot,accuracy,0.4195,0.0110372453715906
openlm-research/open_llama_3b_v2,arithmetic_5ds,5-shot,accuracy,0.015,0.0027186753387999
openlm-research/open_llama_3b_v2,arithmetic_5da,5-shot,accuracy,0.015,0.0027186753387999
openlm-research/open_llama_3b_v2,arithmetic_1dc,5-shot,accuracy,0.0955,0.0065735440015541
openlm-research/open_llama_3b_v2,arithmetic_4ds,5-shot,accuracy,0.101,0.0067396002185256
openlm-research/open_llama_3b_v2,arithmetic_2dm,5-shot,accuracy,0.1655,0.0083120045542488
openlm-research/open_llama_3b_v2,arithmetic_2da,5-shot,accuracy,0.4295,0.0110714119732554
openlm-research/open_llama_3b_v2,gsm8k_cot,5-shot,accuracy,0.048521607278241,0.005918468618921
openlm-research/open_llama_3b_v2,anli_r2,0-shot,brier_score,0.707706480342601,
openlm-research/open_llama_3b_v2,anli_r3,0-shot,brier_score,0.679183075689064,
openlm-research/open_llama_3b_v2,anli_r1,0-shot,brier_score,0.7235055238380341,
openlm-research/open_llama_3b_v2,xnli_eu,0-shot,brier_score,1.132908017291774,
openlm-research/open_llama_3b_v2,xnli_vi,0-shot,brier_score,0.971548338532371,
openlm-research/open_llama_3b_v2,xnli_ru,0-shot,brier_score,0.7523375161853497,
openlm-research/open_llama_3b_v2,xnli_zh,0-shot,brier_score,0.9867010701190116,
openlm-research/open_llama_3b_v2,xnli_tr,0-shot,brier_score,0.8808176289678832,
openlm-research/open_llama_3b_v2,xnli_fr,0-shot,brier_score,0.7460738931677325,
openlm-research/open_llama_3b_v2,xnli_en,0-shot,brier_score,0.6474238073900174,
openlm-research/open_llama_3b_v2,xnli_ur,0-shot,brier_score,1.3250861980436115,
openlm-research/open_llama_3b_v2,xnli_ar,0-shot,brier_score,1.234167496082821,
openlm-research/open_llama_3b_v2,xnli_de,0-shot,brier_score,0.812354579101372,
openlm-research/open_llama_3b_v2,xnli_hi,0-shot,brier_score,1.1542572623687906,
openlm-research/open_llama_3b_v2,xnli_es,0-shot,brier_score,0.8910867470387117,
openlm-research/open_llama_3b_v2,xnli_bg,0-shot,brier_score,0.859452606954678,
openlm-research/open_llama_3b_v2,xnli_sw,0-shot,brier_score,1.0713476113363063,
openlm-research/open_llama_3b_v2,xnli_el,0-shot,brier_score,1.1450771586115809,
openlm-research/open_llama_3b_v2,xnli_th,0-shot,brier_score,0.9689833936635036,
openlm-research/open_llama_3b_v2,logiqa2,0-shot,brier_score,1.017943999933386,
openlm-research/open_llama_3b_v2,mathqa,0-shot,brier_score,0.9356941842152772,
openlm-research/open_llama_3b_v2,lambada_standard,0-shot,perplexity,5.818821278870909,0.1381672177272013
openlm-research/open_llama_3b_v2,lambada_standard,0-shot,accuracy,0.5951872695517174,0.0068385806076515
openlm-research/open_llama_3b_v2,lambada_openai,0-shot,perplexity,4.561675691644536,0.1033064109499507
openlm-research/open_llama_3b_v2,lambada_openai,0-shot,accuracy,0.6735882010479333,0.006532692754359
openlm-research/open_llama_3b_v2,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.0356507967070831
openlm-research/open_llama_3b_v2,mmlu_formal_logic,0-shot,accuracy,0.238095238095238,0.038095238095238
openlm-research/open_llama_3b_v2,mmlu_prehistory,0-shot,accuracy,0.2839506172839506,0.0250894785237651
openlm-research/open_llama_3b_v2,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.0143559119647678
openlm-research/open_llama_3b_v2,mmlu_high_school_world_history,0-shot,accuracy,0.2489451476793249,0.0281469705994226
openlm-research/open_llama_3b_v2,mmlu_moral_disputes,0-shot,accuracy,0.2601156069364161,0.0236186783100693
openlm-research/open_llama_3b_v2,mmlu_professional_law,0-shot,accuracy,0.2470664928292047,0.0110157522552793
openlm-research/open_llama_3b_v2,mmlu_logical_fallacies,0-shot,accuracy,0.2085889570552147,0.0319219344893472
openlm-research/open_llama_3b_v2,mmlu_high_school_us_history,0-shot,accuracy,0.230392156862745,0.029554292605695
openlm-research/open_llama_3b_v2,mmlu_philosophy,0-shot,accuracy,0.270096463022508,0.0252180403734106
openlm-research/open_llama_3b_v2,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.0441434366685493
openlm-research/open_llama_3b_v2,mmlu_international_law,0-shot,accuracy,0.2809917355371901,0.0410320383051451
openlm-research/open_llama_3b_v2,mmlu_high_school_european_history,0-shot,accuracy,0.2424242424242424,0.0334640988105595
openlm-research/open_llama_3b_v2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.233160621761658,0.030516111371476
openlm-research/open_llama_3b_v2,mmlu_high_school_microeconomics,0-shot,accuracy,0.2899159663865546,0.029472485833136
openlm-research/open_llama_3b_v2,mmlu_high_school_geography,0-shot,accuracy,0.2171717171717171,0.0293766164849456
openlm-research/open_llama_3b_v2,mmlu_high_school_psychology,0-shot,accuracy,0.2403669724770642,0.018320607320964
openlm-research/open_llama_3b_v2,mmlu_public_relations,0-shot,accuracy,0.3181818181818182,0.044612721759105
openlm-research/open_llama_3b_v2,mmlu_us_foreign_policy,0-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_3b_v2,mmlu_sociology,0-shot,accuracy,0.2587064676616915,0.030965903123573
openlm-research/open_llama_3b_v2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2487179487179487,0.0219169577092138
openlm-research/open_llama_3b_v2,mmlu_security_studies,0-shot,accuracy,0.3224489795918367,0.0299231005636839
openlm-research/open_llama_3b_v2,mmlu_professional_psychology,0-shot,accuracy,0.2712418300653594,0.0179866153040303
openlm-research/open_llama_3b_v2,mmlu_human_sexuality,0-shot,accuracy,0.2442748091603053,0.0376833595972874
openlm-research/open_llama_3b_v2,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
openlm-research/open_llama_3b_v2,mmlu_miscellaneous,0-shot,accuracy,0.2835249042145594,0.0161173181668322
openlm-research/open_llama_3b_v2,mmlu_marketing,0-shot,accuracy,0.2991452991452991,0.0299969518583494
openlm-research/open_llama_3b_v2,mmlu_management,0-shot,accuracy,0.2524271844660194,0.0430125039969087
openlm-research/open_llama_3b_v2,mmlu_nutrition,0-shot,accuracy,0.261437908496732,0.0251609982142924
openlm-research/open_llama_3b_v2,mmlu_medical_genetics,0-shot,accuracy,0.22,0.0416333199893226
openlm-research/open_llama_3b_v2,mmlu_human_aging,0-shot,accuracy,0.3811659192825112,0.0325962511841682
openlm-research/open_llama_3b_v2,mmlu_professional_medicine,0-shot,accuracy,0.2169117647058823,0.0250358452277112
openlm-research/open_llama_3b_v2,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.0318620985164114
openlm-research/open_llama_3b_v2,mmlu_business_ethics,0-shot,accuracy,0.41,0.049431107042371
openlm-research/open_llama_3b_v2,mmlu_clinical_knowledge,0-shot,accuracy,0.290566037735849,0.0279432199893371
openlm-research/open_llama_3b_v2,mmlu_global_facts,0-shot,accuracy,0.31,0.0464823198711731
openlm-research/open_llama_3b_v2,mmlu_virology,0-shot,accuracy,0.3373493975903614,0.0368078369072758
openlm-research/open_llama_3b_v2,mmlu_professional_accounting,0-shot,accuracy,0.301418439716312,0.0273741288826311
openlm-research/open_llama_3b_v2,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
openlm-research/open_llama_3b_v2,mmlu_high_school_physics,0-shot,accuracy,0.3245033112582781,0.0382274693765875
openlm-research/open_llama_3b_v2,mmlu_high_school_biology,0-shot,accuracy,0.2354838709677419,0.0241376324293377
openlm-research/open_llama_3b_v2,mmlu_college_biology,0-shot,accuracy,0.2430555555555555,0.0358687928008034
openlm-research/open_llama_3b_v2,mmlu_anatomy,0-shot,accuracy,0.237037037037037,0.036737316839695
openlm-research/open_llama_3b_v2,mmlu_college_chemistry,0-shot,accuracy,0.24,0.0429234695990928
openlm-research/open_llama_3b_v2,mmlu_computer_security,0-shot,accuracy,0.32,0.046882617226215
openlm-research/open_llama_3b_v2,mmlu_college_computer_science,0-shot,accuracy,0.23,0.042295258468165
openlm-research/open_llama_3b_v2,mmlu_astronomy,0-shot,accuracy,0.2697368421052631,0.0361178056028489
openlm-research/open_llama_3b_v2,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215
openlm-research/open_llama_3b_v2,mmlu_conceptual_physics,0-shot,accuracy,0.3446808510638298,0.0310689859631221
openlm-research/open_llama_3b_v2,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
openlm-research/open_llama_3b_v2,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276808
openlm-research/open_llama_3b_v2,mmlu_machine_learning,0-shot,accuracy,0.2946428571428571,0.0432704093257873
openlm-research/open_llama_3b_v2,mmlu_high_school_chemistry,0-shot,accuracy,0.2413793103448276,0.0301083307180116
openlm-research/open_llama_3b_v2,mmlu_high_school_statistics,0-shot,accuracy,0.1944444444444444,0.0269914545020367
openlm-research/open_llama_3b_v2,mmlu_elementary_mathematics,0-shot,accuracy,0.2936507936507936,0.023456037383982
openlm-research/open_llama_3b_v2,mmlu_electrical_engineering,0-shot,accuracy,0.2068965517241379,0.0337567244956055
openlm-research/open_llama_3b_v2,mmlu_high_school_mathematics,0-shot,accuracy,0.2222222222222222,0.0253480974680978
openlm-research/open_llama_3b_v2,arc_challenge,25-shot,accuracy,0.3600682593856655,0.0140275168145851
openlm-research/open_llama_3b_v2,arc_challenge,25-shot,acc_norm,0.4078498293515358,0.0143610972884497
openlm-research/open_llama_3b_v2,truthfulqa_mc2,0-shot,accuracy,0.3458434880712128,0.0132125825996038
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,bleu_max,25.09984177135623,0.7599305897286084
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,bleu_acc,0.3011015911872705,0.0160589990261005
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,bleu_diff,-7.793264166700625,0.8253857817947617
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge1_max,50.57061039181146,0.859536655414322
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge1_acc,0.2668298653610771,0.0154836919392372
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge1_diff,-10.097862717598344,0.9307007939492592
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge2_max,34.0661565486233,0.9907235368522948
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge2_acc,0.2399020807833537,0.0149488126790621
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge2_diff,-11.881434651179656,1.0800608172022408
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rougeL_max,47.56194685447575,0.8711921370362652
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rougeL_acc,0.2631578947368421,0.015415241740237
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rougeL_diff,-10.243475244546104,0.9305534307619344
openlm-research/open_llama_3b_v2,truthfulqa_mc1,0-shot,accuracy,0.2141982864137087,0.0143621481556904
EleutherAI/gpt-neo-125M,minerva_math_precalc,5-shot,accuracy,0.0036630036630036,0.0025877573681934
EleutherAI/gpt-neo-125M,minerva_math_prealgebra,5-shot,accuracy,0.0022962112514351,0.0016227331369346
EleutherAI/gpt-neo-125M,minerva_math_num_theory,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417,0.001565259593407
EleutherAI/gpt-neo-125M,minerva_math_geometry,5-shot,accuracy,0.0041753653444676,0.0029493392170756
EleutherAI/gpt-neo-125M,minerva_math_counting_and_prob,5-shot,accuracy,0.0021097046413502,0.0021097046413502
EleutherAI/gpt-neo-125M,minerva_math_algebra,5-shot,accuracy,0.0033698399326032,0.0016827876052283
EleutherAI/gpt-neo-125M,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_3da,5-shot,accuracy,0.0005,0.0005
EleutherAI/gpt-neo-125M,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339
EleutherAI/gpt-neo-125M,arithmetic_4da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_2ds,5-shot,accuracy,0.001,0.0007069298939339
EleutherAI/gpt-neo-125M,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_1dc,5-shot,accuracy,0.0025,0.0011169148353275
EleutherAI/gpt-neo-125M,arithmetic_4ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_2dm,5-shot,accuracy,0.0285,0.0037216663472429
EleutherAI/gpt-neo-125M,arithmetic_2da,5-shot,accuracy,0.001,0.0007069298939339
EleutherAI/gpt-neo-125M,gsm8k_cot,5-shot,accuracy,0.0151630022744503,0.0033660229497263
EleutherAI/gpt-neo-125M,gsm8k,5-shot,accuracy,0.0174374526156178,0.0036054868679982
EleutherAI/gpt-neo-125M,anli_r2,0-shot,brier_score,0.9548365401800128,
EleutherAI/gpt-neo-125M,anli_r3,0-shot,brier_score,0.8891583885006976,
EleutherAI/gpt-neo-125M,anli_r1,0-shot,brier_score,0.962249018140792,
EleutherAI/gpt-neo-125M,xnli_eu,0-shot,brier_score,1.11770990438718,
EleutherAI/gpt-neo-125M,xnli_vi,0-shot,brier_score,0.849920028552591,
EleutherAI/gpt-neo-125M,xnli_ru,0-shot,brier_score,0.9576388869814576,
EleutherAI/gpt-neo-125M,xnli_zh,0-shot,brier_score,0.992966502083354,
EleutherAI/gpt-neo-125M,xnli_tr,0-shot,brier_score,0.9095949998078356,
EleutherAI/gpt-neo-125M,xnli_fr,0-shot,brier_score,0.9206902731267516,
EleutherAI/gpt-neo-125M,xnli_en,0-shot,brier_score,0.7190039933226706,
EleutherAI/gpt-neo-125M,xnli_ur,0-shot,brier_score,1.3284008366257785,
EleutherAI/gpt-neo-125M,xnli_ar,0-shot,brier_score,0.8957307885455548,
EleutherAI/gpt-neo-125M,xnli_de,0-shot,brier_score,0.9295157814794096,
EleutherAI/gpt-neo-125M,xnli_hi,0-shot,brier_score,0.789158069353759,
EleutherAI/gpt-neo-125M,xnli_es,0-shot,brier_score,1.042790212974061,
EleutherAI/gpt-neo-125M,xnli_bg,0-shot,brier_score,1.2387012114897136,
EleutherAI/gpt-neo-125M,xnli_sw,0-shot,brier_score,1.281388698211314,
EleutherAI/gpt-neo-125M,xnli_el,0-shot,brier_score,1.279779656312381,
EleutherAI/gpt-neo-125M,xnli_th,0-shot,brier_score,1.0078771683411278,
EleutherAI/gpt-neo-125M,logiqa2,0-shot,brier_score,1.1460935685457103,
EleutherAI/gpt-neo-125M,mathqa,0-shot,brier_score,1.022477382045219,
EleutherAI/gpt-neo-125M,lambada_standard,0-shot,perplexity,111.82437441637022,5.021215660258462
EleutherAI/gpt-neo-125M,lambada_standard,0-shot,accuracy,0.2606248787114302,0.0061157880293335
EleutherAI/gpt-neo-125M,lambada_openai,0-shot,perplexity,30.265471083236196,1.1271375050920782
EleutherAI/gpt-neo-125M,lambada_openai,0-shot,accuracy,0.3735687948767708,0.0067395990486083
jb723/LLaMA2-en-ko-7B-model,minerva_math_precalc,5-shot,accuracy,0.0256410256410256,0.0067706278007804
jb723/LLaMA2-en-ko-7B-model,minerva_math_prealgebra,5-shot,accuracy,0.0631458094144661,0.0082461008666693
jb723/LLaMA2-en-ko-7B-model,minerva_math_num_theory,5-shot,accuracy,0.0444444444444444,0.008876511687867
jb723/LLaMA2-en-ko-7B-model,minerva_math_intermediate_algebra,5-shot,accuracy,0.0431893687707641,0.0067685891847598
jb723/LLaMA2-en-ko-7B-model,minerva_math_geometry,5-shot,accuracy,0.0313152400835073,0.007966272499457
jb723/LLaMA2-en-ko-7B-model,minerva_math_counting_and_prob,5-shot,accuracy,0.0379746835443038,0.0087883989159183
jb723/LLaMA2-en-ko-7B-model,minerva_math_algebra,5-shot,accuracy,0.0362257792754844,0.0054256800066016
jb723/LLaMA2-en-ko-7B-model,fld_default,0-shot,accuracy,0.0,
jb723/LLaMA2-en-ko-7B-model,fld_star,0-shot,accuracy,0.0,
jb723/LLaMA2-en-ko-7B-model,arithmetic_3da,5-shot,accuracy,0.812,0.0087387746905128
jb723/LLaMA2-en-ko-7B-model,arithmetic_3ds,5-shot,accuracy,0.6985,0.0102640903530408
jb723/LLaMA2-en-ko-7B-model,arithmetic_4da,5-shot,accuracy,0.445,0.0111152721350992
jb723/LLaMA2-en-ko-7B-model,arithmetic_2ds,5-shot,accuracy,0.888,0.0070535718921847
jb723/LLaMA2-en-ko-7B-model,arithmetic_5ds,5-shot,accuracy,0.1365,0.0076787601003246
jb723/LLaMA2-en-ko-7B-model,arithmetic_5da,5-shot,accuracy,0.1975,0.0089042977409299
jb723/LLaMA2-en-ko-7B-model,arithmetic_1dc,5-shot,accuracy,0.1985,0.00892124819376
jb723/LLaMA2-en-ko-7B-model,arithmetic_4ds,5-shot,accuracy,0.324,0.0104674153157165
jb723/LLaMA2-en-ko-7B-model,arithmetic_2dm,5-shot,accuracy,0.2485,0.0096654324938228
jb723/LLaMA2-en-ko-7B-model,arithmetic_2da,5-shot,accuracy,0.913,0.0063035995814963
jb723/LLaMA2-en-ko-7B-model,gsm8k_cot,5-shot,accuracy,0.1046247156937073,0.0084306680820292
jb723/LLaMA2-en-ko-7B-model,gsm8k,5-shot,accuracy,0.0803639120545868,0.007488258573239
jb723/LLaMA2-en-ko-7B-model,anli_r2,0-shot,brier_score,0.7571293238745885,
jb723/LLaMA2-en-ko-7B-model,anli_r3,0-shot,brier_score,0.7917361451062274,
jb723/LLaMA2-en-ko-7B-model,anli_r1,0-shot,brier_score,0.790322608509962,
jb723/LLaMA2-en-ko-7B-model,xnli_eu,0-shot,brier_score,1.1616926264930671,
jb723/LLaMA2-en-ko-7B-model,xnli_vi,0-shot,brier_score,1.281568581631835,
jb723/LLaMA2-en-ko-7B-model,xnli_ru,0-shot,brier_score,1.0656470677761996,
jb723/LLaMA2-en-ko-7B-model,xnli_zh,0-shot,brier_score,1.2528903645729943,
jb723/LLaMA2-en-ko-7B-model,xnli_tr,0-shot,brier_score,1.1733266271961436,
jb723/LLaMA2-en-ko-7B-model,xnli_fr,0-shot,brier_score,0.9317029900667162,
jb723/LLaMA2-en-ko-7B-model,xnli_en,0-shot,brier_score,0.7583959203972753,
jb723/LLaMA2-en-ko-7B-model,xnli_ur,0-shot,brier_score,1.2177675272267834,
jb723/LLaMA2-en-ko-7B-model,xnli_ar,0-shot,brier_score,1.2410505846779556,
jb723/LLaMA2-en-ko-7B-model,xnli_de,0-shot,brier_score,1.0386205045640462,
jb723/LLaMA2-en-ko-7B-model,xnli_hi,0-shot,brier_score,1.293753345210115,
jb723/LLaMA2-en-ko-7B-model,xnli_es,0-shot,brier_score,1.1731481806637558,
jb723/LLaMA2-en-ko-7B-model,xnli_bg,0-shot,brier_score,1.2046814609921392,
jb723/LLaMA2-en-ko-7B-model,xnli_sw,0-shot,brier_score,0.9985997618380864,
jb723/LLaMA2-en-ko-7B-model,xnli_el,0-shot,brier_score,1.2034257796980916,
jb723/LLaMA2-en-ko-7B-model,xnli_th,0-shot,brier_score,1.1949904072552373,
jb723/LLaMA2-en-ko-7B-model,logiqa2,0-shot,brier_score,1.062929306890202,
jb723/LLaMA2-en-ko-7B-model,mathqa,0-shot,brier_score,0.9595240469786608,
jb723/LLaMA2-en-ko-7B-model,lambada_standard,0-shot,perplexity,13.492654865070444,0.760087072196989
jb723/LLaMA2-en-ko-7B-model,lambada_standard,0-shot,accuracy,0.5649136425383272,0.00690702196667
jb723/LLaMA2-en-ko-7B-model,lambada_openai,0-shot,perplexity,5.252813024296853,0.1956035665911124
jb723/LLaMA2-en-ko-7B-model,lambada_openai,0-shot,accuracy,0.6510770424995148,0.0066403815818314
jb723/LLaMA2-en-ko-7B-model,mmlu_world_religions,0-shot,accuracy,0.672514619883041,0.0359933577145602
jb723/LLaMA2-en-ko-7B-model,mmlu_formal_logic,0-shot,accuracy,0.246031746031746,0.0385227336492431
jb723/LLaMA2-en-ko-7B-model,mmlu_prehistory,0-shot,accuracy,0.5154320987654321,0.0278074900442761
jb723/LLaMA2-en-ko-7B-model,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705748
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_world_history,0-shot,accuracy,0.6751054852320675,0.0304860393891053
jb723/LLaMA2-en-ko-7B-model,mmlu_moral_disputes,0-shot,accuracy,0.5260115606936416,0.0268826434340228
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_law,0-shot,accuracy,0.3546284224250326,0.0122185764390901
jb723/LLaMA2-en-ko-7B-model,mmlu_logical_fallacies,0-shot,accuracy,0.4785276073619632,0.0392474687675113
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_us_history,0-shot,accuracy,0.5637254901960784,0.0348069313845703
jb723/LLaMA2-en-ko-7B-model,mmlu_philosophy,0-shot,accuracy,0.5594855305466238,0.0281964005741974
jb723/LLaMA2-en-ko-7B-model,mmlu_jurisprudence,0-shot,accuracy,0.4907407407407407,0.0483285355343705
jb723/LLaMA2-en-ko-7B-model,mmlu_international_law,0-shot,accuracy,0.6033057851239669,0.04465869780531
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_european_history,0-shot,accuracy,0.5757575757575758,0.0385926814207026
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6787564766839378,0.0336995086854906
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_microeconomics,0-shot,accuracy,0.4159663865546218,0.0320165010073961
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_geography,0-shot,accuracy,0.5909090909090909,0.03502975799413
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_psychology,0-shot,accuracy,0.6201834862385321,0.0208088256178662
jb723/LLaMA2-en-ko-7B-model,mmlu_public_relations,0-shot,accuracy,0.5545454545454546,0.0476054882146032
jb723/LLaMA2-en-ko-7B-model,mmlu_us_foreign_policy,0-shot,accuracy,0.64,0.0482418151324421
jb723/LLaMA2-en-ko-7B-model,mmlu_sociology,0-shot,accuracy,0.5870646766169154,0.0348152080336734
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4435897435897435,0.0251891498947641
jb723/LLaMA2-en-ko-7B-model,mmlu_security_studies,0-shot,accuracy,0.4489795918367347,0.0318421386668757
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_psychology,0-shot,accuracy,0.4477124183006536,0.0201169253474224
jb723/LLaMA2-en-ko-7B-model,mmlu_human_sexuality,0-shot,accuracy,0.549618320610687,0.0436364369852477
jb723/LLaMA2-en-ko-7B-model,mmlu_econometrics,0-shot,accuracy,0.3245614035087719,0.0440455615737476
jb723/LLaMA2-en-ko-7B-model,mmlu_miscellaneous,0-shot,accuracy,0.6360153256704981,0.0172056848090322
jb723/LLaMA2-en-ko-7B-model,mmlu_marketing,0-shot,accuracy,0.6623931623931624,0.0309802969926185
jb723/LLaMA2-en-ko-7B-model,mmlu_management,0-shot,accuracy,0.5825242718446602,0.0488284054821223
jb723/LLaMA2-en-ko-7B-model,mmlu_nutrition,0-shot,accuracy,0.4803921568627451,0.028607893699576
jb723/LLaMA2-en-ko-7B-model,mmlu_medical_genetics,0-shot,accuracy,0.56,0.0498887651569858
jb723/LLaMA2-en-ko-7B-model,mmlu_human_aging,0-shot,accuracy,0.5336322869955157,0.033481800170603
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_medicine,0-shot,accuracy,0.4338235294117647,0.0301056365700166
jb723/LLaMA2-en-ko-7B-model,mmlu_college_medicine,0-shot,accuracy,0.4335260115606936,0.0377862107909205
jb723/LLaMA2-en-ko-7B-model,mmlu_business_ethics,0-shot,accuracy,0.48,0.0502116731568677
jb723/LLaMA2-en-ko-7B-model,mmlu_clinical_knowledge,0-shot,accuracy,0.4679245283018868,0.0307094869925565
jb723/LLaMA2-en-ko-7B-model,mmlu_global_facts,0-shot,accuracy,0.39,0.0490207130000197
jb723/LLaMA2-en-ko-7B-model,mmlu_virology,0-shot,accuracy,0.4397590361445783,0.0386413992369912
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_accounting,0-shot,accuracy,0.3617021276595745,0.0286638201471994
jb723/LLaMA2-en-ko-7B-model,mmlu_college_physics,0-shot,accuracy,0.1470588235294117,0.0352406895156744
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_physics,0-shot,accuracy,0.3178807947019867,0.038020397601079
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_biology,0-shot,accuracy,0.5,0.0284440061994287
jb723/LLaMA2-en-ko-7B-model,mmlu_college_biology,0-shot,accuracy,0.4652777777777778,0.0417111585818161
jb723/LLaMA2-en-ko-7B-model,mmlu_anatomy,0-shot,accuracy,0.4444444444444444,0.0429259671825698
jb723/LLaMA2-en-ko-7B-model,mmlu_college_chemistry,0-shot,accuracy,0.37,0.048523658709391
jb723/LLaMA2-en-ko-7B-model,mmlu_computer_security,0-shot,accuracy,0.51,0.0502418393795691
jb723/LLaMA2-en-ko-7B-model,mmlu_college_computer_science,0-shot,accuracy,0.35,0.0479372485441101
jb723/LLaMA2-en-ko-7B-model,mmlu_astronomy,0-shot,accuracy,0.4407894736842105,0.0404031106249043
jb723/LLaMA2-en-ko-7B-model,mmlu_college_mathematics,0-shot,accuracy,0.33,0.047258156262526
jb723/LLaMA2-en-ko-7B-model,mmlu_conceptual_physics,0-shot,accuracy,0.4,0.0320256307610173
jb723/LLaMA2-en-ko-7B-model,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.0460566186471838
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_computer_science,0-shot,accuracy,0.41,0.049431107042371
jb723/LLaMA2-en-ko-7B-model,mmlu_machine_learning,0-shot,accuracy,0.3214285714285714,0.0443280405529151
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_chemistry,0-shot,accuracy,0.3201970443349753,0.0328264938530415
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_statistics,0-shot,accuracy,0.2824074074074074,0.0307013721115109
jb723/LLaMA2-en-ko-7B-model,mmlu_elementary_mathematics,0-shot,accuracy,0.3174603174603174,0.023973861998992
jb723/LLaMA2-en-ko-7B-model,mmlu_electrical_engineering,0-shot,accuracy,0.4758620689655172,0.0416180850350153
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.0264661175389599
jb723/LLaMA2-en-ko-7B-model,arc_challenge,25-shot,accuracy,0.5136518771331058,0.0146059434298609
jb723/LLaMA2-en-ko-7B-model,arc_challenge,25-shot,acc_norm,0.53839590443686,0.0145682455502963
jb723/LLaMA2-en-ko-7B-model,hellaswag,10-shot,accuracy,0.6069508066122287,0.0048742939648435
jb723/LLaMA2-en-ko-7B-model,hellaswag,10-shot,acc_norm,0.7893845847440749,0.0040691239053249
jb723/LLaMA2-en-ko-7B-model,truthfulqa_mc2,0-shot,accuracy,0.4095607803946665,0.015544245050976
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,bleu_max,20.89104935353322,0.690580917763846
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,bleu_acc,0.3439412484700122,0.0166290875142768
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,bleu_diff,-4.982714805919928,0.6525563274154834
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge1_max,46.0179025495015,0.8157872400574628
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge1_acc,0.3414932680538555,0.0166006886199508
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge1_diff,-7.817434391713708,0.7537330659697064
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge2_max,30.28488318154152,0.885344085472441
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge2_acc,0.2913096695226438,0.0159059870481848
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge2_diff,-8.624090028516706,0.8953473056334376
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rougeL_max,42.39510902130912,0.8131112049425046
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rougeL_acc,0.3133414932680538,0.0162380650690595
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rougeL_diff,-8.169098689998256,0.7490350953113237
jb723/LLaMA2-en-ko-7B-model,truthfulqa_mc1,0-shot,accuracy,0.2876376988984088,0.0158463151013947
jb723/LLaMA2-en-ko-7B-model,winogrande,5-shot,accuracy,0.7000789265982637,0.012878347526636
EleutherAI/pythia-410m-deduped,drop,3-shot,accuracy,0.0014681208053691,0.0003921042190298
EleutherAI/pythia-410m-deduped,drop,3-shot,f1,0.0425723573825505,0.0011637772390608
EleutherAI/pythia-410m-deduped,gsm8k,5-shot,accuracy,0.00303260045489,0.0015145735612245
EleutherAI/pythia-410m-deduped,winogrande,5-shot,accuracy,0.5438042620363063,0.0139984536109243
EleutherAI/pythia-410m-deduped,arc:challenge,25-shot,accuracy,0.2261092150170648,0.0122242020970632
EleutherAI/pythia-410m-deduped,arc:challenge,25-shot,acc_norm,0.2482935153583617,0.0126249128680897
EleutherAI/pythia-410m-deduped,hellaswag,10-shot,accuracy,0.344353714399522,0.0047418597531784
EleutherAI/pythia-410m-deduped,hellaswag,10-shot,acc_norm,0.4128659629555865,0.004913429010559
EleutherAI/pythia-410m-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-410m-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.18,0.0386122919665369
EleutherAI/pythia-410m-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.2740740740740741,0.03853254836552
EleutherAI/pythia-410m-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.2740740740740741,0.03853254836552
EleutherAI/pythia-410m-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.1513157894736842,0.0291626315968439
EleutherAI/pythia-410m-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.1513157894736842,0.0291626315968439
EleutherAI/pythia-410m-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-410m-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-410m-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2490566037735849,0.0266164829805017
EleutherAI/pythia-410m-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2490566037735849,0.0266164829805017
EleutherAI/pythia-410m-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.3263888888888889,0.0392106719898226
EleutherAI/pythia-410m-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.3263888888888889,0.0392106719898226
EleutherAI/pythia-410m-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-410m-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-410m-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-410m-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/pythia-410m-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338473
EleutherAI/pythia-410m-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338473
EleutherAI/pythia-410m-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2023121387283237,0.0306311455391988
EleutherAI/pythia-410m-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2023121387283237,0.0306311455391988
EleutherAI/pythia-410m-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.2352941176470588,0.0422077365917145
EleutherAI/pythia-410m-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.2352941176470588,0.0422077365917145
EleutherAI/pythia-410m-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338473
EleutherAI/pythia-410m-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338473
EleutherAI/pythia-410m-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.0275017529444124
EleutherAI/pythia-410m-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.0275017529444124
EleutherAI/pythia-410m-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.2105263157894736,0.0383515395439941
EleutherAI/pythia-410m-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.2105263157894736,0.0383515395439941
EleutherAI/pythia-410m-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
EleutherAI/pythia-410m-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
EleutherAI/pythia-410m-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2619047619047619,0.0226442126155252
EleutherAI/pythia-410m-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2619047619047619,0.0226442126155252
EleutherAI/pythia-410m-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.3095238095238095,0.0413491301830331
EleutherAI/pythia-410m-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.3095238095238095,0.0413491301830331
EleutherAI/pythia-410m-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-410m-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.2516129032258064,0.0246859792862399
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2516129032258064,0.0246859792862399
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.0305165307326944
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2242424242424242,0.032568666616811
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2242424242424242,0.032568666616811
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2323232323232323,0.0300886294902174
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2323232323232323,0.0300886294902174
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2279792746113989,0.0302769099451782
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2279792746113989,0.0302769099451782
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3128205128205128,0.0235075790206453
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3128205128205128,0.0235075790206453
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2666666666666666,0.0269624243250738
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2666666666666666,0.0269624243250738
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2016806722689075,0.0260643134063045
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2016806722689075,0.0260643134063045
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.2516556291390728,0.0354330423438998
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2516556291390728,0.0354330423438998
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2165137614678899,0.0176587105944431
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2165137614678899,0.0176587105944431
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2352941176470588,0.0297717752281456
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2352941176470588,0.0297717752281456
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2784810126582278,0.0291786823048425
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2784810126582278,0.0291786823048425
EleutherAI/pythia-410m-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.336322869955157,0.031708824268455
EleutherAI/pythia-410m-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.336322869955157,0.031708824268455
EleutherAI/pythia-410m-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
EleutherAI/pythia-410m-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2442748091603053,0.0376833595972874
EleutherAI/pythia-410m-deduped,hendrycksTest-international_law,5-shot,accuracy,0.3223140495867768,0.0426641636335216
EleutherAI/pythia-410m-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.3223140495867768,0.0426641636335216
EleutherAI/pythia-410m-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.0441434366685493
EleutherAI/pythia-410m-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.0441434366685493
EleutherAI/pythia-410m-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2269938650306748,0.0329109957861576
EleutherAI/pythia-410m-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2269938650306748,0.0329109957861576
EleutherAI/pythia-410m-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.3392857142857143,0.0449394906861353
EleutherAI/pythia-410m-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.3392857142857143,0.0449394906861353
EleutherAI/pythia-410m-deduped,hendrycksTest-management,5-shot,accuracy,0.2815533980582524,0.0445325483632646
EleutherAI/pythia-410m-deduped,hendrycksTest-management,5-shot,acc_norm,0.2815533980582524,0.0445325483632646
EleutherAI/pythia-410m-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2264957264957265,0.0274210072953929
EleutherAI/pythia-410m-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2264957264957265,0.0274210072953929
EleutherAI/pythia-410m-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-410m-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-410m-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2618135376756066,0.0157208386784452
EleutherAI/pythia-410m-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2618135376756066,0.0157208386784452
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2514450867052023,0.023357365785874
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2514450867052023,0.023357365785874
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.241340782122905,0.0143109995479614
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.241340782122905,0.0143109995479614
EleutherAI/pythia-410m-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2287581699346405,0.0240510297399122
EleutherAI/pythia-410m-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2287581699346405,0.0240510297399122
EleutherAI/pythia-410m-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/pythia-410m-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.0221224397724807
EleutherAI/pythia-410m-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2037037037037037,0.0224096745473041
EleutherAI/pythia-410m-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2037037037037037,0.0224096745473041
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2446808510638297,0.0256455536222667
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2446808510638297,0.0256455536222667
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2398956975228161,0.0109062826179816
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2398956975228161,0.0109062826179816
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.3235294117647059,0.0284182086194067
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3235294117647059,0.0284182086194067
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2450980392156862,0.0174018167114276
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2450980392156862,0.0174018167114276
EleutherAI/pythia-410m-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.0435027144292324
EleutherAI/pythia-410m-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.0435027144292324
EleutherAI/pythia-410m-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.3510204081632653,0.0305553167555736
EleutherAI/pythia-410m-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.3510204081632653,0.0305553167555736
EleutherAI/pythia-410m-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2786069651741293,0.031700561834973
EleutherAI/pythia-410m-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2786069651741293,0.031700561834973
EleutherAI/pythia-410m-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-410m-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.0409360180740332
EleutherAI/pythia-410m-deduped,hendrycksTest-virology,5-shot,accuracy,0.2650602409638554,0.0343602403794496
EleutherAI/pythia-410m-deduped,hendrycksTest-virology,5-shot,acc_norm,0.2650602409638554,0.0343602403794496
EleutherAI/pythia-410m-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.2865497076023391,0.0346782668570382
EleutherAI/pythia-410m-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.2865497076023391,0.0346782668570382
EleutherAI/pythia-410m-deduped,truthfulqa:mc,0-shot,mc1,0.237454100367197,0.0148962774410418
EleutherAI/pythia-410m-deduped,truthfulqa:mc,0-shot,mc2,0.4094757859220677,0.0145668542867676
AbacusResearch/Jallabi-34B,arc:challenge,25-shot,accuracy,0.6348122866894198,0.0140702655192688
AbacusResearch/Jallabi-34B,arc:challenge,25-shot,acc_norm,0.6604095563139932,0.0138390397628201
AbacusResearch/Jallabi-34B,hellaswag,10-shot,accuracy,0.6382194781915953,0.0047953370091182
AbacusResearch/Jallabi-34B,hellaswag,10-shot,acc_norm,0.8380800637323242,0.0036762448867232
AbacusResearch/Jallabi-34B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.51,0.0502418393795691
AbacusResearch/Jallabi-34B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.51,0.0502418393795691
AbacusResearch/Jallabi-34B,hendrycksTest-anatomy,5-shot,accuracy,0.7037037037037037,0.0394462416250111
AbacusResearch/Jallabi-34B,hendrycksTest-anatomy,5-shot,acc_norm,0.7037037037037037,0.0394462416250111
AbacusResearch/Jallabi-34B,hendrycksTest-astronomy,5-shot,accuracy,0.8552631578947368,0.0286319518459303
AbacusResearch/Jallabi-34B,hendrycksTest-astronomy,5-shot,acc_norm,0.8552631578947368,0.0286319518459303
AbacusResearch/Jallabi-34B,hendrycksTest-business_ethics,5-shot,accuracy,0.84,0.036845294917747
AbacusResearch/Jallabi-34B,hendrycksTest-business_ethics,5-shot,acc_norm,0.84,0.036845294917747
AbacusResearch/Jallabi-34B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8188679245283019,0.0237029635267577
AbacusResearch/Jallabi-34B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8188679245283019,0.0237029635267577
AbacusResearch/Jallabi-34B,hendrycksTest-college_biology,5-shot,accuracy,0.9166666666666666,0.0231125081760512
AbacusResearch/Jallabi-34B,hendrycksTest-college_biology,5-shot,acc_norm,0.9166666666666666,0.0231125081760512
AbacusResearch/Jallabi-34B,hendrycksTest-college_chemistry,5-shot,accuracy,0.53,0.0501613558046592
AbacusResearch/Jallabi-34B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.53,0.0501613558046592
AbacusResearch/Jallabi-34B,hendrycksTest-college_computer_science,5-shot,accuracy,0.65,0.0479372485441101
AbacusResearch/Jallabi-34B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.65,0.0479372485441101
AbacusResearch/Jallabi-34B,hendrycksTest-college_mathematics,5-shot,accuracy,0.45,0.0499999999999999
AbacusResearch/Jallabi-34B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.45,0.0499999999999999
AbacusResearch/Jallabi-34B,hendrycksTest-college_medicine,5-shot,accuracy,0.7052023121387283,0.0347659960751647
AbacusResearch/Jallabi-34B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7052023121387283,0.0347659960751647
AbacusResearch/Jallabi-34B,hendrycksTest-college_physics,5-shot,accuracy,0.5196078431372549,0.049713588843674
AbacusResearch/Jallabi-34B,hendrycksTest-college_physics,5-shot,acc_norm,0.5196078431372549,0.049713588843674
AbacusResearch/Jallabi-34B,hendrycksTest-computer_security,5-shot,accuracy,0.85,0.0358870281282636
AbacusResearch/Jallabi-34B,hendrycksTest-computer_security,5-shot,acc_norm,0.85,0.0358870281282636
AbacusResearch/Jallabi-34B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7659574468085106,0.0276784525782123
AbacusResearch/Jallabi-34B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7659574468085106,0.0276784525782123
AbacusResearch/Jallabi-34B,hendrycksTest-econometrics,5-shot,accuracy,0.5614035087719298,0.0466800073851045
AbacusResearch/Jallabi-34B,hendrycksTest-econometrics,5-shot,acc_norm,0.5614035087719298,0.0466800073851045
AbacusResearch/Jallabi-34B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7586206896551724,0.035659981741353
AbacusResearch/Jallabi-34B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7586206896551724,0.035659981741353
AbacusResearch/Jallabi-34B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.7407407407407407,0.0225698970749184
AbacusResearch/Jallabi-34B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.7407407407407407,0.0225698970749184
AbacusResearch/Jallabi-34B,hendrycksTest-formal_logic,5-shot,accuracy,0.5396825396825397,0.0445802912547097
AbacusResearch/Jallabi-34B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5396825396825397,0.0445802912547097
AbacusResearch/Jallabi-34B,hendrycksTest-global_facts,5-shot,accuracy,0.55,0.05
AbacusResearch/Jallabi-34B,hendrycksTest-global_facts,5-shot,acc_norm,0.55,0.05
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_biology,5-shot,accuracy,0.9032258064516128,0.0168189434163451
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.9032258064516128,0.0168189434163451
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6157635467980296,0.0342239856565755
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6157635467980296,0.0342239856565755
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.8,0.0402015126103684
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.8,0.0402015126103684
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8727272727272727,0.0260246576516561
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8727272727272727,0.0260246576516561
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_geography,5-shot,accuracy,0.9090909090909092,0.0204820867754242
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9090909090909092,0.0204820867754242
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9689119170984456,0.012525310625527
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9689119170984456,0.012525310625527
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8102564102564103,0.0198801654065887
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8102564102564103,0.0198801654065887
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4407407407407407,0.030270671157284
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4407407407407407,0.030270671157284
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8613445378151261,0.0224482644768325
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8613445378151261,0.0224482644768325
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_physics,5-shot,accuracy,0.4966887417218543,0.0408239337944965
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4966887417218543,0.0408239337944965
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9192660550458716,0.011680172292862
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9192660550458716,0.011680172292862
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6342592592592593,0.032847388576472
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6342592592592593,0.032847388576472
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9215686274509804,0.0188695146466589
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9215686274509804,0.0188695146466589
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.919831223628692,0.0176766799918916
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.919831223628692,0.0176766799918916
AbacusResearch/Jallabi-34B,hendrycksTest-human_aging,5-shot,accuracy,0.7757847533632287,0.0279915342585195
AbacusResearch/Jallabi-34B,hendrycksTest-human_aging,5-shot,acc_norm,0.7757847533632287,0.0279915342585195
AbacusResearch/Jallabi-34B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8549618320610687,0.0308846610895153
AbacusResearch/Jallabi-34B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8549618320610687,0.0308846610895153
AbacusResearch/Jallabi-34B,hendrycksTest-international_law,5-shot,accuracy,0.9173553719008264,0.0251353823566042
AbacusResearch/Jallabi-34B,hendrycksTest-international_law,5-shot,acc_norm,0.9173553719008264,0.0251353823566042
AbacusResearch/Jallabi-34B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8981481481481481,0.0292392726756327
AbacusResearch/Jallabi-34B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8981481481481481,0.0292392726756327
AbacusResearch/Jallabi-34B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8650306748466258,0.0268457650545538
AbacusResearch/Jallabi-34B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8650306748466258,0.0268457650545538
AbacusResearch/Jallabi-34B,hendrycksTest-machine_learning,5-shot,accuracy,0.6428571428571429,0.0454796099976437
AbacusResearch/Jallabi-34B,hendrycksTest-machine_learning,5-shot,acc_norm,0.6428571428571429,0.0454796099976437
AbacusResearch/Jallabi-34B,hendrycksTest-management,5-shot,accuracy,0.9029126213592232,0.0293159629188134
AbacusResearch/Jallabi-34B,hendrycksTest-management,5-shot,acc_norm,0.9029126213592232,0.0293159629188134
AbacusResearch/Jallabi-34B,hendrycksTest-marketing,5-shot,accuracy,0.9145299145299144,0.0183158916856258
AbacusResearch/Jallabi-34B,hendrycksTest-marketing,5-shot,acc_norm,0.9145299145299144,0.0183158916856258
AbacusResearch/Jallabi-34B,hendrycksTest-medical_genetics,5-shot,accuracy,0.89,0.031446603773522
AbacusResearch/Jallabi-34B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.89,0.031446603773522
AbacusResearch/Jallabi-34B,hendrycksTest-miscellaneous,5-shot,accuracy,0.913154533844189,0.0100702983777477
AbacusResearch/Jallabi-34B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.913154533844189,0.0100702983777477
AbacusResearch/Jallabi-34B,hendrycksTest-moral_disputes,5-shot,accuracy,0.8034682080924855,0.0213939614043638
AbacusResearch/Jallabi-34B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8034682080924855,0.0213939614043638
AbacusResearch/Jallabi-34B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6703910614525139,0.0157215310751838
AbacusResearch/Jallabi-34B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6703910614525139,0.0157215310751838
AbacusResearch/Jallabi-34B,hendrycksTest-nutrition,5-shot,accuracy,0.8431372549019608,0.0208237588375809
AbacusResearch/Jallabi-34B,hendrycksTest-nutrition,5-shot,acc_norm,0.8431372549019608,0.0208237588375809
AbacusResearch/Jallabi-34B,hendrycksTest-philosophy,5-shot,accuracy,0.819935691318328,0.0218234228577449
AbacusResearch/Jallabi-34B,hendrycksTest-philosophy,5-shot,acc_norm,0.819935691318328,0.0218234228577449
AbacusResearch/Jallabi-34B,hendrycksTest-prehistory,5-shot,accuracy,0.8672839506172839,0.0188773538395718
AbacusResearch/Jallabi-34B,hendrycksTest-prehistory,5-shot,acc_norm,0.8672839506172839,0.0188773538395718
AbacusResearch/Jallabi-34B,hendrycksTest-professional_accounting,5-shot,accuracy,0.6276595744680851,0.0288389214712514
AbacusResearch/Jallabi-34B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6276595744680851,0.0288389214712514
AbacusResearch/Jallabi-34B,hendrycksTest-professional_law,5-shot,accuracy,0.5775749674054759,0.0126156004757349
AbacusResearch/Jallabi-34B,hendrycksTest-professional_law,5-shot,acc_norm,0.5775749674054759,0.0126156004757349
AbacusResearch/Jallabi-34B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8272058823529411,0.0229660675855817
AbacusResearch/Jallabi-34B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8272058823529411,0.0229660675855817
AbacusResearch/Jallabi-34B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8104575163398693,0.0158561521899802
AbacusResearch/Jallabi-34B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8104575163398693,0.0158561521899802
AbacusResearch/Jallabi-34B,hendrycksTest-public_relations,5-shot,accuracy,0.7,0.0438931145464428
AbacusResearch/Jallabi-34B,hendrycksTest-public_relations,5-shot,acc_norm,0.7,0.0438931145464428
AbacusResearch/Jallabi-34B,hendrycksTest-security_studies,5-shot,accuracy,0.8408163265306122,0.0234209720691663
AbacusResearch/Jallabi-34B,hendrycksTest-security_studies,5-shot,acc_norm,0.8408163265306122,0.0234209720691663
AbacusResearch/Jallabi-34B,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.0220763261018246
AbacusResearch/Jallabi-34B,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.0220763261018246
AbacusResearch/Jallabi-34B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.9,0.0301511344577763
AbacusResearch/Jallabi-34B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.9,0.0301511344577763
AbacusResearch/Jallabi-34B,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721
AbacusResearch/Jallabi-34B,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721
AbacusResearch/Jallabi-34B,hendrycksTest-world_religions,5-shot,accuracy,0.9005847953216374,0.022949025579355
AbacusResearch/Jallabi-34B,hendrycksTest-world_religions,5-shot,acc_norm,0.9005847953216374,0.022949025579355
AbacusResearch/Jallabi-34B,truthfulqa:mc,0-shot,mc1,0.3659730722154223,0.0168629416840883
AbacusResearch/Jallabi-34B,truthfulqa:mc,0-shot,mc2,0.5146389940410719,0.0150205523543139
AbacusResearch/Jallabi-34B,winogrande,5-shot,accuracy,0.8145224940805051,0.0109239653031405
AbacusResearch/Jallabi-34B,gsm8k,5-shot,accuracy,0.6520090978013646,0.0131205810303821
mosaicml/mpt-7b-storywriter,arc:challenge,25-shot,accuracy,0.1732081911262798,0.0110586941832803
mosaicml/mpt-7b-storywriter,arc:challenge,25-shot,acc_norm,0.2039249146757679,0.0117742624787022
mosaicml/mpt-7b-storywriter,hellaswag,10-shot,accuracy,0.5461063533160725,0.0049685216080654
mosaicml/mpt-7b-storywriter,hellaswag,10-shot,acc_norm,0.7425811591316471,0.0043631851720471
mosaicml/mpt-7b-storywriter,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-anatomy,5-shot,accuracy,0.3259259259259259,0.040491220417025
mosaicml/mpt-7b-storywriter,hendrycksTest-anatomy,5-shot,acc_norm,0.3259259259259259,0.040491220417025
mosaicml/mpt-7b-storywriter,hendrycksTest-astronomy,5-shot,accuracy,0.1842105263157894,0.0315469804508223
mosaicml/mpt-7b-storywriter,hendrycksTest-astronomy,5-shot,acc_norm,0.1842105263157894,0.0315469804508223
mosaicml/mpt-7b-storywriter,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.0446196043338473
mosaicml/mpt-7b-storywriter,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.0446196043338473
mosaicml/mpt-7b-storywriter,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2226415094339622,0.0256042334708991
mosaicml/mpt-7b-storywriter,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2226415094339622,0.0256042334708991
mosaicml/mpt-7b-storywriter,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.0347659010430413
mosaicml/mpt-7b-storywriter,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.0347659010430413
mosaicml/mpt-7b-storywriter,hendrycksTest-college_chemistry,5-shot,accuracy,0.28,0.0451260859854212
mosaicml/mpt-7b-storywriter,hendrycksTest-college_chemistry,5-shot,acc_norm,0.28,0.0451260859854212
mosaicml/mpt-7b-storywriter,hendrycksTest-college_computer_science,5-shot,accuracy,0.15,0.0358870281282637
mosaicml/mpt-7b-storywriter,hendrycksTest-college_computer_science,5-shot,acc_norm,0.15,0.0358870281282637
mosaicml/mpt-7b-storywriter,hendrycksTest-college_mathematics,5-shot,accuracy,0.2,0.0402015126103684
mosaicml/mpt-7b-storywriter,hendrycksTest-college_mathematics,5-shot,acc_norm,0.2,0.0402015126103684
mosaicml/mpt-7b-storywriter,hendrycksTest-college_medicine,5-shot,accuracy,0.2138728323699422,0.0312651120617304
mosaicml/mpt-7b-storywriter,hendrycksTest-college_medicine,5-shot,acc_norm,0.2138728323699422,0.0312651120617304
mosaicml/mpt-7b-storywriter,hendrycksTest-college_physics,5-shot,accuracy,0.284313725490196,0.0448848285232901
mosaicml/mpt-7b-storywriter,hendrycksTest-college_physics,5-shot,acc_norm,0.284313725490196,0.0448848285232901
mosaicml/mpt-7b-storywriter,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.042295258468165
mosaicml/mpt-7b-storywriter,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.042295258468165
mosaicml/mpt-7b-storywriter,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.0275017529444124
mosaicml/mpt-7b-storywriter,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.0275017529444124
mosaicml/mpt-7b-storywriter,hendrycksTest-econometrics,5-shot,accuracy,0.2280701754385964,0.0394715278266941
mosaicml/mpt-7b-storywriter,hendrycksTest-econometrics,5-shot,acc_norm,0.2280701754385964,0.0394715278266941
mosaicml/mpt-7b-storywriter,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.0366466633722525
mosaicml/mpt-7b-storywriter,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.0366466633722525
mosaicml/mpt-7b-storywriter,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
mosaicml/mpt-7b-storywriter,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
mosaicml/mpt-7b-storywriter,hendrycksTest-formal_logic,5-shot,accuracy,0.2063492063492063,0.0361960452412425
mosaicml/mpt-7b-storywriter,hendrycksTest-formal_logic,5-shot,acc_norm,0.2063492063492063,0.0361960452412425
mosaicml/mpt-7b-storywriter,hendrycksTest-global_facts,5-shot,accuracy,0.21,0.0409360180740332
mosaicml/mpt-7b-storywriter,hendrycksTest-global_facts,5-shot,acc_norm,0.21,0.0409360180740332
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_biology,5-shot,accuracy,0.3096774193548387,0.0263027749835174
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3096774193548387,0.0263027749835174
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.0305165307326944
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.2,0.0402015126103684
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.2,0.0402015126103684
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000917
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2363636363636363,0.0331750593000917
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_geography,5-shot,accuracy,0.2474747474747475,0.0307463007421244
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2474747474747475,0.0307463007421244
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2279792746113989,0.0302769099451782
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2279792746113989,0.0302769099451782
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2051282051282051,0.0204732331735519
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2051282051282051,0.0204732331735519
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2407407407407407,0.0260671592222757
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2407407407407407,0.0260671592222757
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2310924369747899,0.0273814069278689
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2310924369747899,0.0273814069278689
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.0329798664847383
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.0329798664847383
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2091743119266055,0.0174379371733432
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2091743119266055,0.0174379371733432
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4398148148148148,0.0338517797604481
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4398148148148148,0.0338517797604481
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2450980392156862,0.0301902824535019
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2450980392156862,0.0301902824535019
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2362869198312236,0.0276521531441592
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2362869198312236,0.0276521531441592
mosaicml/mpt-7b-storywriter,hendrycksTest-human_aging,5-shot,accuracy,0.2242152466367713,0.0279915342585195
mosaicml/mpt-7b-storywriter,hendrycksTest-human_aging,5-shot,acc_norm,0.2242152466367713,0.0279915342585195
mosaicml/mpt-7b-storywriter,hendrycksTest-human_sexuality,5-shot,accuracy,0.2519083969465648,0.0380738711630608
mosaicml/mpt-7b-storywriter,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2519083969465648,0.0380738711630608
mosaicml/mpt-7b-storywriter,hendrycksTest-international_law,5-shot,accuracy,0.2479338842975206,0.039418975265163
mosaicml/mpt-7b-storywriter,hendrycksTest-international_law,5-shot,acc_norm,0.2479338842975206,0.039418975265163
mosaicml/mpt-7b-storywriter,hendrycksTest-jurisprudence,5-shot,accuracy,0.2129629629629629,0.0395783547198098
mosaicml/mpt-7b-storywriter,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2129629629629629,0.0395783547198098
mosaicml/mpt-7b-storywriter,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2515337423312883,0.0340899788685752
mosaicml/mpt-7b-storywriter,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2515337423312883,0.0340899788685752
mosaicml/mpt-7b-storywriter,hendrycksTest-machine_learning,5-shot,accuracy,0.1964285714285714,0.0377097004934701
mosaicml/mpt-7b-storywriter,hendrycksTest-machine_learning,5-shot,acc_norm,0.1964285714285714,0.0377097004934701
mosaicml/mpt-7b-storywriter,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.0391666776282258
mosaicml/mpt-7b-storywriter,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.0391666776282258
mosaicml/mpt-7b-storywriter,hendrycksTest-marketing,5-shot,accuracy,0.235042735042735,0.0277788359049354
mosaicml/mpt-7b-storywriter,hendrycksTest-marketing,5-shot,acc_norm,0.235042735042735,0.0277788359049354
mosaicml/mpt-7b-storywriter,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-miscellaneous,5-shot,accuracy,0.2592592592592592,0.0156710060093395
mosaicml/mpt-7b-storywriter,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2592592592592592,0.0156710060093395
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_disputes,5-shot,accuracy,0.2167630057803468,0.0221834776684128
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2167630057803468,0.0221834776684128
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2379888268156424,0.0142426300705749
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2379888268156424,0.0142426300705749
mosaicml/mpt-7b-storywriter,hendrycksTest-nutrition,5-shot,accuracy,0.2254901960784313,0.0239291555173512
mosaicml/mpt-7b-storywriter,hendrycksTest-nutrition,5-shot,acc_norm,0.2254901960784313,0.0239291555173512
mosaicml/mpt-7b-storywriter,hendrycksTest-philosophy,5-shot,accuracy,0.22508038585209,0.023720088516179
mosaicml/mpt-7b-storywriter,hendrycksTest-philosophy,5-shot,acc_norm,0.22508038585209,0.023720088516179
mosaicml/mpt-7b-storywriter,hendrycksTest-prehistory,5-shot,accuracy,0.25,0.0240934712326213
mosaicml/mpt-7b-storywriter,hendrycksTest-prehistory,5-shot,acc_norm,0.25,0.0240934712326213
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_accounting,5-shot,accuracy,0.2375886524822695,0.0253895125527299
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2375886524822695,0.0253895125527299
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_law,5-shot,accuracy,0.2496740547588005,0.0110545383778323
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_law,5-shot,acc_norm,0.2496740547588005,0.0110545383778323
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_medicine,5-shot,accuracy,0.2463235294117647,0.02617343857052
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2463235294117647,0.02617343857052
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2598039215686274,0.0177408995091777
mosaicml/mpt-7b-storywriter,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
mosaicml/mpt-7b-storywriter,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.0417234303870538
mosaicml/mpt-7b-storywriter,hendrycksTest-security_studies,5-shot,accuracy,0.2653061224489796,0.0282638899437845
mosaicml/mpt-7b-storywriter,hendrycksTest-security_studies,5-shot,acc_norm,0.2653061224489796,0.0282638899437845
mosaicml/mpt-7b-storywriter,hendrycksTest-sociology,5-shot,accuracy,0.2786069651741293,0.031700561834973
mosaicml/mpt-7b-storywriter,hendrycksTest-sociology,5-shot,acc_norm,0.2786069651741293,0.031700561834973
mosaicml/mpt-7b-storywriter,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.27,0.0446196043338473
mosaicml/mpt-7b-storywriter,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.27,0.0446196043338473
mosaicml/mpt-7b-storywriter,hendrycksTest-virology,5-shot,accuracy,0.2469879518072289,0.0335735198206453
mosaicml/mpt-7b-storywriter,hendrycksTest-virology,5-shot,acc_norm,0.2469879518072289,0.0335735198206453
mosaicml/mpt-7b-storywriter,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.0344629621708842
mosaicml/mpt-7b-storywriter,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.0344629621708842
mosaicml/mpt-7b-storywriter,truthfulqa:mc,0-shot,mc1,0.2362301101591187,0.014869755015871
mosaicml/mpt-7b-storywriter,truthfulqa:mc,0-shot,mc2,0.484398012309533,0.0162793199660191
mosaicml/mpt-7b-storywriter,drop,3-shot,accuracy,0.0006291946308724,0.0002568002749723
mosaicml/mpt-7b-storywriter,drop,3-shot,f1,0.0032026006711409,0.0005040610386397
mosaicml/mpt-7b-storywriter,gsm8k,5-shot,accuracy,0.0515542077331311,0.0060908879552628
mosaicml/mpt-7b-storywriter,winogrande,5-shot,accuracy,0.6827150749802684,0.0130805984113321
meta-llama/Meta-Llama-3-8B,arc:challenge,25-shot,accuracy,0.5409556313993175,0.0145622910736012
meta-llama/Meta-Llama-3-8B,arc:challenge,25-shot,acc_norm,0.5921501706484642,0.0143610972884497
meta-llama/Meta-Llama-3-8B,hellaswag,10-shot,accuracy,0.6176060545708026,0.0048497884239443
meta-llama/Meta-Llama-3-8B,hellaswag,10-shot,acc_norm,0.8201553475403306,0.0038327310175921
meta-llama/Meta-Llama-3-8B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.0460566186471838
meta-llama/Meta-Llama-3-8B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.0460566186471838
meta-llama/Meta-Llama-3-8B,hendrycksTest-anatomy,5-shot,accuracy,0.6888888888888889,0.0399926287661772
meta-llama/Meta-Llama-3-8B,hendrycksTest-anatomy,5-shot,acc_norm,0.6888888888888889,0.0399926287661772
meta-llama/Meta-Llama-3-8B,hendrycksTest-astronomy,5-shot,accuracy,0.6842105263157895,0.0378272898086546
meta-llama/Meta-Llama-3-8B,hendrycksTest-astronomy,5-shot,acc_norm,0.6842105263157895,0.0378272898086546
meta-llama/Meta-Llama-3-8B,hendrycksTest-business_ethics,5-shot,accuracy,0.62,0.0487831731214563
meta-llama/Meta-Llama-3-8B,hendrycksTest-business_ethics,5-shot,acc_norm,0.62,0.0487831731214563
meta-llama/Meta-Llama-3-8B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7471698113207547,0.0267498997712412
meta-llama/Meta-Llama-3-8B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7471698113207547,0.0267498997712412
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_biology,5-shot,accuracy,0.7777777777777778,0.0347659010430413
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_biology,5-shot,acc_norm,0.7777777777777778,0.0347659010430413
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_chemistry,5-shot,accuracy,0.46,0.0500908265962033
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.46,0.0500908265962033
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_computer_science,5-shot,accuracy,0.5,0.0502518907629606
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.5,0.0502518907629606
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_mathematics,5-shot,accuracy,0.34,0.0476095228569523
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.34,0.0476095228569523
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_medicine,5-shot,accuracy,0.653179190751445,0.0362914667015966
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_medicine,5-shot,acc_norm,0.653179190751445,0.0362914667015966
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_physics,5-shot,accuracy,0.5098039215686274,0.0497422946042281
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_physics,5-shot,acc_norm,0.5098039215686274,0.0497422946042281
meta-llama/Meta-Llama-3-8B,hendrycksTest-computer_security,5-shot,accuracy,0.81,0.0394277244403662
meta-llama/Meta-Llama-3-8B,hendrycksTest-computer_security,5-shot,acc_norm,0.81,0.0394277244403662
meta-llama/Meta-Llama-3-8B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.5829787234042553,0.0322327626671171
meta-llama/Meta-Llama-3-8B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.5829787234042553,0.0322327626671171
meta-llama/Meta-Llama-3-8B,hendrycksTest-econometrics,5-shot,accuracy,0.4912280701754385,0.0470288043204961
meta-llama/Meta-Llama-3-8B,hendrycksTest-econometrics,5-shot,acc_norm,0.4912280701754385,0.0470288043204961
meta-llama/Meta-Llama-3-8B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6413793103448275,0.0399662957487671
meta-llama/Meta-Llama-3-8B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6413793103448275,0.0399662957487671
meta-llama/Meta-Llama-3-8B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4259259259259259,0.0254671490454695
meta-llama/Meta-Llama-3-8B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4259259259259259,0.0254671490454695
meta-llama/Meta-Llama-3-8B,hendrycksTest-formal_logic,5-shot,accuracy,0.4841269841269841,0.044698818540726
meta-llama/Meta-Llama-3-8B,hendrycksTest-formal_logic,5-shot,acc_norm,0.4841269841269841,0.044698818540726
meta-llama/Meta-Llama-3-8B,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.0482418151324421
meta-llama/Meta-Llama-3-8B,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.0482418151324421
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_biology,5-shot,accuracy,0.7774193548387097,0.0236642166716425
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7774193548387097,0.0236642166716425
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.035128190778761
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.035128190778761
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.67,0.047258156262526
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.67,0.047258156262526
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7696969696969697,0.0328766675860349
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7696969696969697,0.0328766675860349
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_geography,5-shot,accuracy,0.797979797979798,0.0286062042892298
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.797979797979798,0.0286062042892298
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.8911917098445595,0.0224732533327687
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.8911917098445595,0.0224732533327687
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6461538461538462,0.0242437839940621
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6461538461538462,0.0242437839940621
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3888888888888889,0.0297232789614766
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3888888888888889,0.0297232789614766
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.726890756302521,0.0289420040409981
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.726890756302521,0.0289420040409981
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_physics,5-shot,accuracy,0.4569536423841059,0.0406732517424744
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4569536423841059,0.0406732517424744
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8385321100917431,0.0157762392561632
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8385321100917431,0.0157762392561632
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5416666666666666,0.0339811089029463
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5416666666666666,0.0339811089029463
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8529411764705882,0.0248574780802504
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8529411764705882,0.0248574780802504
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8185654008438819,0.0250859611445796
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8185654008438819,0.0250859611445796
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_aging,5-shot,accuracy,0.7130044843049327,0.0303603797102919
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_aging,5-shot,acc_norm,0.7130044843049327,0.0303603797102919
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_sexuality,5-shot,accuracy,0.7938931297709924,0.0354777100415946
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7938931297709924,0.0354777100415946
meta-llama/Meta-Llama-3-8B,hendrycksTest-international_law,5-shot,accuracy,0.8512396694214877,0.0324847008380719
meta-llama/Meta-Llama-3-8B,hendrycksTest-international_law,5-shot,acc_norm,0.8512396694214877,0.0324847008380719
meta-llama/Meta-Llama-3-8B,hendrycksTest-jurisprudence,5-shot,accuracy,0.7592592592592593,0.0413311944024383
meta-llama/Meta-Llama-3-8B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7592592592592593,0.0413311944024383
meta-llama/Meta-Llama-3-8B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7361963190184049,0.0346241993161562
meta-llama/Meta-Llama-3-8B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7361963190184049,0.0346241993161562
meta-llama/Meta-Llama-3-8B,hendrycksTest-machine_learning,5-shot,accuracy,0.5178571428571429,0.0474276236124301
meta-llama/Meta-Llama-3-8B,hendrycksTest-machine_learning,5-shot,acc_norm,0.5178571428571429,0.0474276236124301
meta-llama/Meta-Llama-3-8B,hendrycksTest-management,5-shot,accuracy,0.8737864077669902,0.0328818027880862
meta-llama/Meta-Llama-3-8B,hendrycksTest-management,5-shot,acc_norm,0.8737864077669902,0.0328818027880862
meta-llama/Meta-Llama-3-8B,hendrycksTest-marketing,5-shot,accuracy,0.8931623931623932,0.0202371490089909
meta-llama/Meta-Llama-3-8B,hendrycksTest-marketing,5-shot,acc_norm,0.8931623931623932,0.0202371490089909
meta-llama/Meta-Llama-3-8B,hendrycksTest-medical_genetics,5-shot,accuracy,0.77,0.042295258468165
meta-llama/Meta-Llama-3-8B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.77,0.042295258468165
meta-llama/Meta-Llama-3-8B,hendrycksTest-miscellaneous,5-shot,accuracy,0.8365261813537676,0.0132239286167416
meta-llama/Meta-Llama-3-8B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8365261813537676,0.0132239286167416
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_disputes,5-shot,accuracy,0.7398843930635838,0.0236186783100693
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7398843930635838,0.0236186783100693
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.423463687150838,0.0165254258987735
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.423463687150838,0.0165254258987735
meta-llama/Meta-Llama-3-8B,hendrycksTest-nutrition,5-shot,accuracy,0.7679738562091504,0.0241708408793408
meta-llama/Meta-Llama-3-8B,hendrycksTest-nutrition,5-shot,acc_norm,0.7679738562091504,0.0241708408793408
meta-llama/Meta-Llama-3-8B,hendrycksTest-philosophy,5-shot,accuracy,0.7459807073954984,0.0247238615047716
meta-llama/Meta-Llama-3-8B,hendrycksTest-philosophy,5-shot,acc_norm,0.7459807073954984,0.0247238615047716
meta-llama/Meta-Llama-3-8B,hendrycksTest-prehistory,5-shot,accuracy,0.7253086419753086,0.0248360578682946
meta-llama/Meta-Llama-3-8B,hendrycksTest-prehistory,5-shot,acc_norm,0.7253086419753086,0.0248360578682946
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_accounting,5-shot,accuracy,0.4858156028368794,0.029815494483682
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4858156028368794,0.029815494483682
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_law,5-shot,accuracy,0.4628422425032594,0.012734923579532
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_law,5-shot,acc_norm,0.4628422425032594,0.012734923579532
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_medicine,5-shot,accuracy,0.7242647058823529,0.0271462719366251
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.7242647058823529,0.0271462719366251
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_psychology,5-shot,accuracy,0.7238562091503268,0.0180872769356631
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7238562091503268,0.0180872769356631
meta-llama/Meta-Llama-3-8B,hendrycksTest-public_relations,5-shot,accuracy,0.7,0.0438931145464428
meta-llama/Meta-Llama-3-8B,hendrycksTest-public_relations,5-shot,acc_norm,0.7,0.0438931145464428
meta-llama/Meta-Llama-3-8B,hendrycksTest-security_studies,5-shot,accuracy,0.7510204081632653,0.0276829795229602
meta-llama/Meta-Llama-3-8B,hendrycksTest-security_studies,5-shot,acc_norm,0.7510204081632653,0.0276829795229602
meta-llama/Meta-Llama-3-8B,hendrycksTest-sociology,5-shot,accuracy,0.8656716417910447,0.0241126782409008
meta-llama/Meta-Llama-3-8B,hendrycksTest-sociology,5-shot,acc_norm,0.8656716417910447,0.0241126782409008
meta-llama/Meta-Llama-3-8B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.87,0.033799766898963
meta-llama/Meta-Llama-3-8B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.87,0.033799766898963
meta-llama/Meta-Llama-3-8B,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721
meta-llama/Meta-Llama-3-8B,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721
meta-llama/Meta-Llama-3-8B,hendrycksTest-world_religions,5-shot,accuracy,0.8362573099415205,0.0283809195961458
meta-llama/Meta-Llama-3-8B,hendrycksTest-world_religions,5-shot,acc_norm,0.8362573099415205,0.0283809195961458
meta-llama/Meta-Llama-3-8B,truthfulqa:mc,0-shot,mc1,0.2680538555691554,0.0155062047228345
meta-llama/Meta-Llama-3-8B,truthfulqa:mc,0-shot,mc2,0.4395226511050948,0.0139343637688421
meta-llama/Meta-Llama-3-8B,winogrande,5-shot,accuracy,0.771112865035517,0.0118073602240253
meta-llama/Meta-Llama-3-8B,gsm8k,5-shot,accuracy,0.5018953752843063,0.0137723857655697
meta-llama/Meta-Llama-3-8B,minerva_math_precalc,5-shot,accuracy,0.0622710622710622,0.0103510294727737
meta-llama/Meta-Llama-3-8B,minerva_math_prealgebra,5-shot,accuracy,0.2973593570608496,0.0154970063786603
meta-llama/Meta-Llama-3-8B,minerva_math_num_theory,5-shot,accuracy,0.0796296296296296,0.0116606908048272
meta-llama/Meta-Llama-3-8B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0564784053156146,0.0076862386783551
meta-llama/Meta-Llama-3-8B,minerva_math_geometry,5-shot,accuracy,0.1315240083507307,0.0154585045568475
meta-llama/Meta-Llama-3-8B,minerva_math_counting_and_prob,5-shot,accuracy,0.1392405063291139,0.0159181699553674
meta-llama/Meta-Llama-3-8B,minerva_math_algebra,5-shot,accuracy,0.2333614153327717,0.0122819554352804
meta-llama/Meta-Llama-3-8B,fld_default,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B,fld_star,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B,arithmetic_3da,5-shot,accuracy,0.999,0.0007069298939339
meta-llama/Meta-Llama-3-8B,arithmetic_3ds,5-shot,accuracy,0.827,0.0084599814209501
meta-llama/Meta-Llama-3-8B,arithmetic_4da,5-shot,accuracy,0.8865,0.0070946488299991
meta-llama/Meta-Llama-3-8B,arithmetic_2ds,5-shot,accuracy,0.998,0.0009992493430694
meta-llama/Meta-Llama-3-8B,arithmetic_5ds,5-shot,accuracy,0.716,0.0100857752022694
meta-llama/Meta-Llama-3-8B,arithmetic_5da,5-shot,accuracy,0.7945,0.009037461637895
meta-llama/Meta-Llama-3-8B,arithmetic_1dc,5-shot,accuracy,0.8715,0.0074847769467749
meta-llama/Meta-Llama-3-8B,arithmetic_4ds,5-shot,accuracy,0.798,0.0089798841395409
meta-llama/Meta-Llama-3-8B,arithmetic_2dm,5-shot,accuracy,0.798,0.0089798841395409
meta-llama/Meta-Llama-3-8B,arithmetic_2da,5-shot,accuracy,1.0,
meta-llama/Meta-Llama-3-8B,gsm8k_cot,5-shot,accuracy,0.5420773313115997,0.013723629649844
meta-llama/Meta-Llama-3-8B,anli_r2,0-shot,brier_score,0.7340467171859161,
meta-llama/Meta-Llama-3-8B,anli_r3,0-shot,brier_score,0.7121953651588336,
meta-llama/Meta-Llama-3-8B,anli_r1,0-shot,brier_score,0.7541976473288209,
meta-llama/Meta-Llama-3-8B,xnli_eu,0-shot,brier_score,0.8161378857768329,
meta-llama/Meta-Llama-3-8B,xnli_vi,0-shot,brier_score,0.7012948256730949,
meta-llama/Meta-Llama-3-8B,xnli_ru,0-shot,brier_score,0.7041160968968688,
meta-llama/Meta-Llama-3-8B,xnli_zh,0-shot,brier_score,0.8941874011270395,
meta-llama/Meta-Llama-3-8B,xnli_tr,0-shot,brier_score,0.7914917696056338,
meta-llama/Meta-Llama-3-8B,xnli_fr,0-shot,brier_score,0.744870423375577,
meta-llama/Meta-Llama-3-8B,xnli_en,0-shot,brier_score,0.6630590579212835,
meta-llama/Meta-Llama-3-8B,xnli_ur,0-shot,brier_score,1.2437930525145169,
meta-llama/Meta-Llama-3-8B,xnli_ar,0-shot,brier_score,1.2746409939019057,
meta-llama/Meta-Llama-3-8B,xnli_de,0-shot,brier_score,0.7785218513263985,
meta-llama/Meta-Llama-3-8B,xnli_hi,0-shot,brier_score,0.7929776988678373,
meta-llama/Meta-Llama-3-8B,xnli_es,0-shot,brier_score,0.8005855709367697,
meta-llama/Meta-Llama-3-8B,xnli_bg,0-shot,brier_score,0.8622344515881246,
meta-llama/Meta-Llama-3-8B,xnli_sw,0-shot,brier_score,0.84665245616744,
meta-llama/Meta-Llama-3-8B,xnli_el,0-shot,brier_score,0.929237628843266,
meta-llama/Meta-Llama-3-8B,xnli_th,0-shot,brier_score,0.7752077729549335,
meta-llama/Meta-Llama-3-8B,logiqa2,0-shot,brier_score,0.8788764591649608,
meta-llama/Meta-Llama-3-8B,mathqa,0-shot,brier_score,0.7661715680467305,
meta-llama/Meta-Llama-3-8B,lambada_standard,0-shot,perplexity,3.8644129999654666,0.0767209246494085
meta-llama/Meta-Llama-3-8B,lambada_standard,0-shot,accuracy,0.6877547059965069,0.0064561975253285
meta-llama/Meta-Llama-3-8B,lambada_openai,0-shot,perplexity,3.092042982548172,0.0568673106694778
meta-llama/Meta-Llama-3-8B,lambada_openai,0-shot,accuracy,0.7578109838928779,0.0059685624476109
facebook/opt-350m,minerva_math_precalc,5-shot,accuracy,0.0036630036630036,0.0025877573681934
facebook/opt-350m,minerva_math_prealgebra,5-shot,accuracy,0.0218140068886337,0.0049524353688735
facebook/opt-350m,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/opt-350m,minerva_math_intermediate_algebra,5-shot,accuracy,0.0033222591362126,0.0019159795218657
facebook/opt-350m,minerva_math_geometry,5-shot,accuracy,0.0020876826722338,0.0020876826722338
facebook/opt-350m,minerva_math_counting_and_prob,5-shot,accuracy,0.0021097046413502,0.0021097046413502
facebook/opt-350m,minerva_math_algebra,5-shot,accuracy,0.0151642796967144,0.0035485460431325
facebook/opt-350m,fld_default,0-shot,accuracy,0.0,
facebook/opt-350m,fld_star,0-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
facebook/opt-350m,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
facebook/opt-350m,arithmetic_4da,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_2ds,5-shot,accuracy,0.0095,0.00216961485391
facebook/opt-350m,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_1dc,5-shot,accuracy,0.002,0.0009992493430694
facebook/opt-350m,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_2dm,5-shot,accuracy,0.0185,0.0030138707185866
facebook/opt-350m,arithmetic_2da,5-shot,accuracy,0.0045,0.0014969954902233
facebook/opt-350m,gsm8k_cot,5-shot,accuracy,0.0166793025018953,0.0035275958887224
facebook/opt-350m,gsm8k,5-shot,accuracy,0.00303260045489,0.0015145735612245
facebook/opt-350m,anli_r2,0-shot,brier_score,0.8622047830185462,
facebook/opt-350m,anli_r3,0-shot,brier_score,0.8573734867626938,
facebook/opt-350m,anli_r1,0-shot,brier_score,0.875763881825301,
facebook/opt-350m,xnli_eu,0-shot,brier_score,1.2350745588223613,
facebook/opt-350m,xnli_vi,0-shot,brier_score,1.187962184278827,
facebook/opt-350m,xnli_ru,0-shot,brier_score,0.9510442144388732,
facebook/opt-350m,xnli_zh,0-shot,brier_score,1.2604084317893178,
facebook/opt-350m,xnli_tr,0-shot,brier_score,0.8578078728939282,
facebook/opt-350m,xnli_fr,0-shot,brier_score,0.8643029753225795,
facebook/opt-350m,xnli_en,0-shot,brier_score,0.7004467657014397,
facebook/opt-350m,xnli_ur,0-shot,brier_score,1.294615717341209,
facebook/opt-350m,xnli_ar,0-shot,brier_score,0.9077213776188938,
facebook/opt-350m,xnli_de,0-shot,brier_score,0.90423438746825,
facebook/opt-350m,xnli_hi,0-shot,brier_score,1.2717666974720738,
facebook/opt-350m,xnli_es,0-shot,brier_score,0.9307819515890196,
facebook/opt-350m,xnli_bg,0-shot,brier_score,1.2325643903521082,
facebook/opt-350m,xnli_sw,0-shot,brier_score,1.141700829535198,
facebook/opt-350m,xnli_el,0-shot,brier_score,1.0571822860452535,
facebook/opt-350m,xnli_th,0-shot,brier_score,1.294957782543984,
facebook/opt-350m,logiqa2,0-shot,brier_score,1.1601959747659685,
facebook/opt-350m,mathqa,0-shot,brier_score,1.0103387666530812,
facebook/opt-350m,lambada_standard,0-shot,perplexity,38.15763443103525,1.5528711068483714
facebook/opt-350m,lambada_standard,0-shot,accuracy,0.3578497962352028,0.0066785283258885
facebook/opt-350m,lambada_openai,0-shot,perplexity,16.398847797098593,0.555542932914263
facebook/opt-350m,lambada_openai,0-shot,accuracy,0.4513875412381137,0.0069329758883686
facebook/opt-350m,mmlu_world_religions,0-shot,accuracy,0.1812865497076023,0.02954774168764
facebook/opt-350m,mmlu_formal_logic,0-shot,accuracy,0.2301587301587301,0.037649508797906
facebook/opt-350m,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.0247486244905373
facebook/opt-350m,mmlu_moral_scenarios,0-shot,accuracy,0.2402234636871508,0.0142883438039253
facebook/opt-350m,mmlu_high_school_world_history,0-shot,accuracy,0.2067510548523206,0.0263616516683891
facebook/opt-350m,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763,0.0238680032625001
facebook/opt-350m,mmlu_professional_law,0-shot,accuracy,0.2398956975228161,0.0109062826179816
facebook/opt-350m,mmlu_logical_fallacies,0-shot,accuracy,0.2453987730061349,0.0338093981394335
facebook/opt-350m,mmlu_high_school_us_history,0-shot,accuracy,0.2598039215686274,0.0307785546786932
facebook/opt-350m,mmlu_philosophy,0-shot,accuracy,0.1897106109324758,0.0222681962587832
facebook/opt-350m,mmlu_jurisprudence,0-shot,accuracy,0.2222222222222222,0.0401910747255734
facebook/opt-350m,mmlu_international_law,0-shot,accuracy,0.371900826446281,0.044120158066245
facebook/opt-350m,mmlu_high_school_european_history,0-shot,accuracy,0.2666666666666666,0.0345313180188541
facebook/opt-350m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3160621761658031,0.0335539736968617
facebook/opt-350m,mmlu_high_school_microeconomics,0-shot,accuracy,0.2773109243697479,0.02907937453948
facebook/opt-350m,mmlu_high_school_geography,0-shot,accuracy,0.3585858585858585,0.0341690364039152
facebook/opt-350m,mmlu_high_school_psychology,0-shot,accuracy,0.3431192660550459,0.020354777736086
facebook/opt-350m,mmlu_public_relations,0-shot,accuracy,0.209090909090909,0.0389509101572413
facebook/opt-350m,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
facebook/opt-350m,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296
facebook/opt-350m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2794871794871795,0.0227523888397768
facebook/opt-350m,mmlu_security_studies,0-shot,accuracy,0.4,0.0313625024093589
facebook/opt-350m,mmlu_professional_psychology,0-shot,accuracy,0.2352941176470588,0.0171605872350463
facebook/opt-350m,mmlu_human_sexuality,0-shot,accuracy,0.2671755725190839,0.0388084830108239
facebook/opt-350m,mmlu_econometrics,0-shot,accuracy,0.2280701754385964,0.0394715278266941
facebook/opt-350m,mmlu_miscellaneous,0-shot,accuracy,0.2094508301404853,0.0145513105681437
facebook/opt-350m,mmlu_marketing,0-shot,accuracy,0.2051282051282051,0.0264535080540403
facebook/opt-350m,mmlu_management,0-shot,accuracy,0.1941747572815534,0.0391666776282258
facebook/opt-350m,mmlu_nutrition,0-shot,accuracy,0.2581699346405229,0.0250585033169581
facebook/opt-350m,mmlu_medical_genetics,0-shot,accuracy,0.31,0.0464823198711731
facebook/opt-350m,mmlu_human_aging,0-shot,accuracy,0.1614349775784753,0.0246939578991284
facebook/opt-350m,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091215
facebook/opt-350m,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
facebook/opt-350m,mmlu_business_ethics,0-shot,accuracy,0.21,0.0409360180740332
facebook/opt-350m,mmlu_clinical_knowledge,0-shot,accuracy,0.260377358490566,0.027008766090708
facebook/opt-350m,mmlu_global_facts,0-shot,accuracy,0.19,0.0394277244403662
facebook/opt-350m,mmlu_virology,0-shot,accuracy,0.1867469879518072,0.0303387491445005
facebook/opt-350m,mmlu_professional_accounting,0-shot,accuracy,0.2446808510638297,0.0256455536222667
facebook/opt-350m,mmlu_college_physics,0-shot,accuracy,0.2254901960784313,0.0415830753308328
facebook/opt-350m,mmlu_high_school_physics,0-shot,accuracy,0.3377483443708609,0.0386155754625516
facebook/opt-350m,mmlu_high_school_biology,0-shot,accuracy,0.2967741935483871,0.0259885007924118
facebook/opt-350m,mmlu_college_biology,0-shot,accuracy,0.2083333333333333,0.0339611620584533
facebook/opt-350m,mmlu_anatomy,0-shot,accuracy,0.2666666666666666,0.038201699145179
facebook/opt-350m,mmlu_college_chemistry,0-shot,accuracy,0.35,0.0479372485441102
facebook/opt-350m,mmlu_computer_security,0-shot,accuracy,0.18,0.0386122919665369
facebook/opt-350m,mmlu_college_computer_science,0-shot,accuracy,0.29,0.0456048021572068
facebook/opt-350m,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
facebook/opt-350m,mmlu_college_mathematics,0-shot,accuracy,0.27,0.0446196043338473
facebook/opt-350m,mmlu_conceptual_physics,0-shot,accuracy,0.3063829787234042,0.0301359064785175
facebook/opt-350m,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
facebook/opt-350m,mmlu_high_school_computer_science,0-shot,accuracy,0.19,0.0394277244403662
facebook/opt-350m,mmlu_machine_learning,0-shot,accuracy,0.2321428571428571,0.040073418097558
facebook/opt-350m,mmlu_high_school_chemistry,0-shot,accuracy,0.2955665024630542,0.0321049443375145
facebook/opt-350m,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-350m,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.0224945107675031
facebook/opt-350m,mmlu_electrical_engineering,0-shot,accuracy,0.3172413793103448,0.0387835237213862
facebook/opt-350m,mmlu_high_school_mathematics,0-shot,accuracy,0.2703703703703703,0.0270803728151456
facebook/opt-350m,arc_challenge,25-shot,accuracy,0.2073378839590443,0.0118469057829713
facebook/opt-350m,arc_challenge,25-shot,acc_norm,0.2397610921501706,0.0124763041274539
facebook/opt-350m,hellaswag,10-shot,accuracy,0.3212507468631747,0.004660025270817
facebook/opt-350m,hellaswag,10-shot,acc_norm,0.367257518422625,0.0048107231083782
facebook/opt-350m,truthfulqa_mc2,0-shot,accuracy,0.4101397354990324,0.0147056658299934
facebook/opt-350m,truthfulqa_gen,0-shot,bleu_max,16.445503288865865,0.5837064468134793
facebook/opt-350m,truthfulqa_gen,0-shot,bleu_acc,0.390452876376989,0.0170782307434314
facebook/opt-350m,truthfulqa_gen,0-shot,bleu_diff,-2.0622603654652294,0.549702898772285
facebook/opt-350m,truthfulqa_gen,0-shot,rouge1_max,35.862459464442495,0.8635314811502294
facebook/opt-350m,truthfulqa_gen,0-shot,rouge1_acc,0.2839657282741738,0.0157853708583967
facebook/opt-350m,truthfulqa_gen,0-shot,rouge1_diff,-5.37391209618839,0.7800863393842812
facebook/opt-350m,truthfulqa_gen,0-shot,rouge2_max,18.920614186005043,0.8752002965989032
facebook/opt-350m,truthfulqa_gen,0-shot,rouge2_acc,0.1811505507955936,0.0134826971878178
facebook/opt-350m,truthfulqa_gen,0-shot,rouge2_diff,-4.905442542549504,0.7931073647795669
facebook/opt-350m,truthfulqa_gen,0-shot,rougeL_max,33.32601122585248,0.8434189971154012
facebook/opt-350m,truthfulqa_gen,0-shot,rougeL_acc,0.2962056303549572,0.0159835951018113
facebook/opt-350m,truthfulqa_gen,0-shot,rougeL_diff,-4.789359756965765,0.7670328005263544
facebook/opt-350m,truthfulqa_mc1,0-shot,accuracy,0.2350061199510404,0.0148430615077316
facebook/opt-350m,winogrande,5-shot,accuracy,0.526440410418311,0.0140328238744072
aisingapore/sea-lion-7b-instruct,minerva_math_precalc,5-shot,accuracy,0.0128205128205128,0.0048189509824876
aisingapore/sea-lion-7b-instruct,minerva_math_prealgebra,5-shot,accuracy,0.0206659012629161,0.004823174633923
aisingapore/sea-lion-7b-instruct,minerva_math_num_theory,5-shot,accuracy,0.0185185185185185,0.0058069728079122
aisingapore/sea-lion-7b-instruct,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.0049000930886157
aisingapore/sea-lion-7b-instruct,minerva_math_geometry,5-shot,accuracy,0.0313152400835073,0.007966272499457
aisingapore/sea-lion-7b-instruct,minerva_math_counting_and_prob,5-shot,accuracy,0.0232067510548523,0.0069227384871433
aisingapore/sea-lion-7b-instruct,minerva_math_algebra,5-shot,accuracy,0.0193765796124684,0.0040026474981053
aisingapore/sea-lion-7b-instruct,fld_default,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,fld_star,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_3da,5-shot,accuracy,0.018,0.0029736208922129
aisingapore/sea-lion-7b-instruct,arithmetic_3ds,5-shot,accuracy,0.015,0.0027186753387999
aisingapore/sea-lion-7b-instruct,arithmetic_4da,5-shot,accuracy,0.001,0.0007069298939339
aisingapore/sea-lion-7b-instruct,arithmetic_2ds,5-shot,accuracy,0.105,0.0068564572122015
aisingapore/sea-lion-7b-instruct,arithmetic_5ds,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_5da,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_1dc,5-shot,accuracy,0.0635,0.0054542414114784
aisingapore/sea-lion-7b-instruct,arithmetic_4ds,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_2dm,5-shot,accuracy,0.0905,0.0064168109471424
aisingapore/sea-lion-7b-instruct,arithmetic_2da,5-shot,accuracy,0.125,0.0073969491973863
aisingapore/sea-lion-7b-instruct,gsm8k_cot,5-shot,accuracy,0.0303260045489006,0.0047234874655147
aisingapore/sea-lion-7b-instruct,gsm8k,5-shot,accuracy,0.0363912054586808,0.0051581134892311
aisingapore/sea-lion-7b-instruct,anli_r2,0-shot,brier_score,0.822362171539859,
aisingapore/sea-lion-7b-instruct,anli_r3,0-shot,brier_score,0.806465692022114,
aisingapore/sea-lion-7b-instruct,anli_r1,0-shot,brier_score,0.8656927316527164,
aisingapore/sea-lion-7b-instruct,xnli_eu,0-shot,brier_score,1.0647285791530434,
aisingapore/sea-lion-7b-instruct,xnli_vi,0-shot,brier_score,0.7605131981756559,
aisingapore/sea-lion-7b-instruct,xnli_ru,0-shot,brier_score,0.895224913639339,
aisingapore/sea-lion-7b-instruct,xnli_zh,0-shot,brier_score,1.0934355350658187,
aisingapore/sea-lion-7b-instruct,xnli_tr,0-shot,brier_score,0.9075091571950322,
aisingapore/sea-lion-7b-instruct,xnli_fr,0-shot,brier_score,0.864102393355566,
aisingapore/sea-lion-7b-instruct,xnli_en,0-shot,brier_score,0.6608052556326952,
aisingapore/sea-lion-7b-instruct,xnli_ur,0-shot,brier_score,1.308169358746148,
aisingapore/sea-lion-7b-instruct,xnli_ar,0-shot,brier_score,0.8825032212383281,
aisingapore/sea-lion-7b-instruct,xnli_de,0-shot,brier_score,0.8773428304376831,
aisingapore/sea-lion-7b-instruct,xnli_hi,0-shot,brier_score,0.9773682473278796,
aisingapore/sea-lion-7b-instruct,xnli_es,0-shot,brier_score,0.9557450117318852,
aisingapore/sea-lion-7b-instruct,xnli_bg,0-shot,brier_score,0.8633761295842486,
aisingapore/sea-lion-7b-instruct,xnli_sw,0-shot,brier_score,0.94799257162619,
aisingapore/sea-lion-7b-instruct,xnli_el,0-shot,brier_score,1.2823572586721372,
aisingapore/sea-lion-7b-instruct,xnli_th,0-shot,brier_score,0.806377402982851,
aisingapore/sea-lion-7b-instruct,logiqa2,0-shot,brier_score,1.134869575549907,
aisingapore/sea-lion-7b-instruct,mathqa,0-shot,brier_score,1.0274994164407336,
aisingapore/sea-lion-7b-instruct,lambada_standard,0-shot,perplexity,6.148366872576256,0.1664108176034716
aisingapore/sea-lion-7b-instruct,lambada_standard,0-shot,accuracy,0.594216960993596,0.006841188231378
aisingapore/sea-lion-7b-instruct,lambada_openai,0-shot,perplexity,4.7435583183157215,0.1180869043800317
aisingapore/sea-lion-7b-instruct,lambada_openai,0-shot,accuracy,0.6539879681738793,0.0066273903700315
aisingapore/sea-lion-7b-instruct,mmlu_world_religions,0-shot,accuracy,0.3742690058479532,0.0371160118538948
aisingapore/sea-lion-7b-instruct,mmlu_formal_logic,0-shot,accuracy,0.1984126984126984,0.0356701667527686
aisingapore/sea-lion-7b-instruct,mmlu_prehistory,0-shot,accuracy,0.2438271604938271,0.0238918795419596
aisingapore/sea-lion-7b-instruct,mmlu_moral_scenarios,0-shot,accuracy,0.2391061452513966,0.0142655541923311
aisingapore/sea-lion-7b-instruct,mmlu_high_school_world_history,0-shot,accuracy,0.2953586497890295,0.0296963387134228
aisingapore/sea-lion-7b-instruct,mmlu_moral_disputes,0-shot,accuracy,0.2832369942196532,0.0242579017053233
aisingapore/sea-lion-7b-instruct,mmlu_professional_law,0-shot,accuracy,0.2614080834419817,0.0112225281697713
aisingapore/sea-lion-7b-instruct,mmlu_logical_fallacies,0-shot,accuracy,0.1963190184049079,0.0312079703947092
aisingapore/sea-lion-7b-instruct,mmlu_high_school_us_history,0-shot,accuracy,0.2794117647058823,0.0314932810450795
aisingapore/sea-lion-7b-instruct,mmlu_philosophy,0-shot,accuracy,0.2668810289389067,0.0251226376088166
aisingapore/sea-lion-7b-instruct,mmlu_jurisprudence,0-shot,accuracy,0.2685185185185185,0.0428446796805219
aisingapore/sea-lion-7b-instruct,mmlu_international_law,0-shot,accuracy,0.2479338842975206,0.039418975265163
aisingapore/sea-lion-7b-instruct,mmlu_high_school_european_history,0-shot,accuracy,0.2363636363636363,0.0331750593000918
aisingapore/sea-lion-7b-instruct,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3471502590673575,0.0343569616836135
aisingapore/sea-lion-7b-instruct,mmlu_high_school_microeconomics,0-shot,accuracy,0.3361344537815126,0.0306847371151353
aisingapore/sea-lion-7b-instruct,mmlu_high_school_geography,0-shot,accuracy,0.3535353535353535,0.0340608672354715
aisingapore/sea-lion-7b-instruct,mmlu_high_school_psychology,0-shot,accuracy,0.2770642201834862,0.0191884825901695
aisingapore/sea-lion-7b-instruct,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.0417234303870538
aisingapore/sea-lion-7b-instruct,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.0464823198711731
aisingapore/sea-lion-7b-instruct,mmlu_sociology,0-shot,accuracy,0.2288557213930348,0.0297052840567724
aisingapore/sea-lion-7b-instruct,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2948717948717949,0.0231193627582322
aisingapore/sea-lion-7b-instruct,mmlu_security_studies,0-shot,accuracy,0.4081632653061224,0.0314646571282742
aisingapore/sea-lion-7b-instruct,mmlu_professional_psychology,0-shot,accuracy,0.2696078431372549,0.0179524491969878
aisingapore/sea-lion-7b-instruct,mmlu_human_sexuality,0-shot,accuracy,0.2977099236641221,0.040103589424622
aisingapore/sea-lion-7b-instruct,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
aisingapore/sea-lion-7b-instruct,mmlu_miscellaneous,0-shot,accuracy,0.247765006385696,0.0154380830805689
aisingapore/sea-lion-7b-instruct,mmlu_marketing,0-shot,accuracy,0.2606837606837607,0.0287603489565234
aisingapore/sea-lion-7b-instruct,mmlu_management,0-shot,accuracy,0.203883495145631,0.0398913985953177
aisingapore/sea-lion-7b-instruct,mmlu_nutrition,0-shot,accuracy,0.3202614379084967,0.0267161183801568
aisingapore/sea-lion-7b-instruct,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
aisingapore/sea-lion-7b-instruct,mmlu_human_aging,0-shot,accuracy,0.3139013452914798,0.0311467964829724
aisingapore/sea-lion-7b-instruct,mmlu_professional_medicine,0-shot,accuracy,0.3823529411764705,0.0295200956976877
aisingapore/sea-lion-7b-instruct,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.0318620985164114
aisingapore/sea-lion-7b-instruct,mmlu_business_ethics,0-shot,accuracy,0.36,0.0482418151324421
aisingapore/sea-lion-7b-instruct,mmlu_clinical_knowledge,0-shot,accuracy,0.2830188679245283,0.0277242364927009
aisingapore/sea-lion-7b-instruct,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
aisingapore/sea-lion-7b-instruct,mmlu_virology,0-shot,accuracy,0.2771084337349397,0.0348433159268058
aisingapore/sea-lion-7b-instruct,mmlu_professional_accounting,0-shot,accuracy,0.226950354609929,0.0249871063656429
aisingapore/sea-lion-7b-instruct,mmlu_college_physics,0-shot,accuracy,0.2450980392156862,0.0428010583736439
aisingapore/sea-lion-7b-instruct,mmlu_high_school_physics,0-shot,accuracy,0.2649006622516556,0.0360303854536038
aisingapore/sea-lion-7b-instruct,mmlu_high_school_biology,0-shot,accuracy,0.2354838709677419,0.0241376324293377
aisingapore/sea-lion-7b-instruct,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.0368565109589753
aisingapore/sea-lion-7b-instruct,mmlu_anatomy,0-shot,accuracy,0.2222222222222222,0.0359144408419696
aisingapore/sea-lion-7b-instruct,mmlu_college_chemistry,0-shot,accuracy,0.26,0.0440844002276807
aisingapore/sea-lion-7b-instruct,mmlu_computer_security,0-shot,accuracy,0.32,0.046882617226215
aisingapore/sea-lion-7b-instruct,mmlu_college_computer_science,0-shot,accuracy,0.27,0.0446196043338473
aisingapore/sea-lion-7b-instruct,mmlu_astronomy,0-shot,accuracy,0.2565789473684211,0.0355418036802569
aisingapore/sea-lion-7b-instruct,mmlu_college_mathematics,0-shot,accuracy,0.23,0.042295258468165
aisingapore/sea-lion-7b-instruct,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.0295131966255393
aisingapore/sea-lion-7b-instruct,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.0451260859854212
aisingapore/sea-lion-7b-instruct,mmlu_high_school_computer_science,0-shot,accuracy,0.27,0.0446196043338474
aisingapore/sea-lion-7b-instruct,mmlu_machine_learning,0-shot,accuracy,0.1696428571428571,0.0356236785009539
aisingapore/sea-lion-7b-instruct,mmlu_high_school_chemistry,0-shot,accuracy,0.1773399014778325,0.0268743372768083
aisingapore/sea-lion-7b-instruct,mmlu_high_school_statistics,0-shot,accuracy,0.4074074074074074,0.0335099160469604
aisingapore/sea-lion-7b-instruct,mmlu_elementary_mathematics,0-shot,accuracy,0.2301587301587301,0.0216792196636931
aisingapore/sea-lion-7b-instruct,mmlu_electrical_engineering,0-shot,accuracy,0.2137931034482758,0.0341652044774754
aisingapore/sea-lion-7b-instruct,mmlu_high_school_mathematics,0-shot,accuracy,0.2703703703703703,0.0270803728151456
aisingapore/sea-lion-7b-instruct,arc_challenge,25-shot,accuracy,0.3686006825938566,0.0140978106780421
aisingapore/sea-lion-7b-instruct,arc_challenge,25-shot,acc_norm,0.4052901023890785,0.0143468690602293
aisingapore/sea-lion-7b-instruct,hellaswag,10-shot,accuracy,0.5070703047201752,0.0049892825160553
aisingapore/sea-lion-7b-instruct,hellaswag,10-shot,acc_norm,0.68442541326429,0.0046379449659146
aisingapore/sea-lion-7b-instruct,truthfulqa_mc2,0-shot,accuracy,0.3625142626642516,0.0137853018325987
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,bleu_max,22.470062578446797,0.7295090033700214
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,bleu_acc,0.3170134638922888,0.0162892033744033
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,bleu_diff,-5.418278156499666,0.6905915780852375
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge1_max,46.358554536831306,0.8882483041479635
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge1_acc,0.2998776009791922,0.0160403529667136
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge1_diff,-8.48027229679748,0.8198393069016621
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge2_max,29.876694665726628,0.9732956920229632
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge2_acc,0.2276621787025703,0.014679255032111
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge2_diff,-8.531092668075676,0.9069547026809286
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rougeL_max,43.37977304846503,0.8897140288128731
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rougeL_acc,0.2876376988984088,0.0158463151013948
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rougeL_diff,-8.622396199581418,0.8195677121259433
aisingapore/sea-lion-7b-instruct,truthfulqa_mc1,0-shot,accuracy,0.2093023255813953,0.0142412194347858
aisingapore/sea-lion-7b-instruct,winogrande,5-shot,accuracy,0.6156274664561957,0.0136715676008361
Salesforce/codegen-2B-multi,minerva_math_precalc,5-shot,accuracy,0.0164835164835164,0.0054540297647667
Salesforce/codegen-2B-multi,minerva_math_prealgebra,5-shot,accuracy,0.0091848450057405,0.0032342427257629
Salesforce/codegen-2B-multi,minerva_math_num_theory,5-shot,accuracy,0.0148148148148148,0.0052037049875126
Salesforce/codegen-2B-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.0188261351052048,0.0045253304986684
Salesforce/codegen-2B-multi,minerva_math_geometry,5-shot,accuracy,0.0104384133611691,0.0046486271171846
Salesforce/codegen-2B-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.0063291139240506,0.003646382041065
Salesforce/codegen-2B-multi,minerva_math_algebra,5-shot,accuracy,0.016849199663016,0.0037372948497597
Salesforce/codegen-2B-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521
Salesforce/codegen-2B-multi,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646
Salesforce/codegen-2B-multi,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
Salesforce/codegen-2B-multi,arithmetic_2ds,5-shot,accuracy,0.0145,0.0026736583971427
Salesforce/codegen-2B-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_1dc,5-shot,accuracy,0.021,0.0032069677767574
Salesforce/codegen-2B-multi,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_2dm,5-shot,accuracy,0.017,0.0028913110935905
Salesforce/codegen-2B-multi,arithmetic_2da,5-shot,accuracy,0.0105,0.0022797968630709
Salesforce/codegen-2B-multi,gsm8k_cot,5-shot,accuracy,0.0242608036391205,0.0042380079000014
Salesforce/codegen-2B-multi,gsm8k,5-shot,accuracy,0.025018953752843,0.0043020450465642
Salesforce/codegen-2B-multi,anli_r2,0-shot,brier_score,0.9748658390993672,
Salesforce/codegen-2B-multi,anli_r3,0-shot,brier_score,0.9159103324247588,
Salesforce/codegen-2B-multi,anli_r1,0-shot,brier_score,1.0092709408874003,
Salesforce/codegen-2B-multi,xnli_eu,0-shot,brier_score,1.1666516618470189,
Salesforce/codegen-2B-multi,xnli_vi,0-shot,brier_score,1.2843093361039912,
Salesforce/codegen-2B-multi,xnli_ru,0-shot,brier_score,0.8090664913187244,
Salesforce/codegen-2B-multi,xnli_zh,0-shot,brier_score,0.904579300460954,
Salesforce/codegen-2B-multi,xnli_tr,0-shot,brier_score,0.9205007916072984,
Salesforce/codegen-2B-multi,xnli_fr,0-shot,brier_score,0.9220325735050248,
Salesforce/codegen-2B-multi,xnli_en,0-shot,brier_score,0.7458236186385612,
Salesforce/codegen-2B-multi,xnli_ur,0-shot,brier_score,1.3227919367050145,
Salesforce/codegen-2B-multi,xnli_ar,0-shot,brier_score,0.908786661893419,
Salesforce/codegen-2B-multi,xnli_de,0-shot,brier_score,0.9684063717218064,
Salesforce/codegen-2B-multi,xnli_hi,0-shot,brier_score,1.071163113090109,
Salesforce/codegen-2B-multi,xnli_es,0-shot,brier_score,1.0508250865994877,
Salesforce/codegen-2B-multi,xnli_bg,0-shot,brier_score,1.0973174685369405,
Salesforce/codegen-2B-multi,xnli_sw,0-shot,brier_score,1.0296151222717675,
Salesforce/codegen-2B-multi,xnli_el,0-shot,brier_score,0.9165722128481476,
Salesforce/codegen-2B-multi,xnli_th,0-shot,brier_score,1.0251103514516042,
Salesforce/codegen-2B-multi,logiqa2,0-shot,brier_score,1.1763968660378157,
Salesforce/codegen-2B-multi,mathqa,0-shot,brier_score,0.97725843880333,
Salesforce/codegen-2B-multi,lambada_standard,0-shot,perplexity,81.4029030188419,3.4255201511790587
Salesforce/codegen-2B-multi,lambada_standard,0-shot,accuracy,0.263729866097419,0.0061391793635698
Salesforce/codegen-2B-multi,lambada_openai,0-shot,perplexity,71.22943112687902,3.023615035564398
Salesforce/codegen-2B-multi,lambada_openai,0-shot,accuracy,0.2744032602367552,0.006216620663857
Salesforce/codegen-2B-multi,mmlu_world_religions,0-shot,accuracy,0.3099415204678362,0.0354697695939316
Salesforce/codegen-2B-multi,mmlu_formal_logic,0-shot,accuracy,0.2301587301587301,0.037649508797906
Salesforce/codegen-2B-multi,mmlu_prehistory,0-shot,accuracy,0.2160493827160493,0.0228991629184458
Salesforce/codegen-2B-multi,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
Salesforce/codegen-2B-multi,mmlu_high_school_world_history,0-shot,accuracy,0.2278481012658227,0.0273034845990694
Salesforce/codegen-2B-multi,mmlu_moral_disputes,0-shot,accuracy,0.2774566473988439,0.0241057126077543
Salesforce/codegen-2B-multi,mmlu_professional_law,0-shot,accuracy,0.2398956975228161,0.0109062826179816
Salesforce/codegen-2B-multi,mmlu_logical_fallacies,0-shot,accuracy,0.263803680981595,0.0346241993161562
Salesforce/codegen-2B-multi,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.0309645179269234
Salesforce/codegen-2B-multi,mmlu_philosophy,0-shot,accuracy,0.2122186495176848,0.0232227567974351
Salesforce/codegen-2B-multi,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
Salesforce/codegen-2B-multi,mmlu_international_law,0-shot,accuracy,0.2892561983471074,0.0413911272763546
Salesforce/codegen-2B-multi,mmlu_high_school_european_history,0-shot,accuracy,0.1939393939393939,0.030874145136562
Salesforce/codegen-2B-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.233160621761658,0.030516111371476
Salesforce/codegen-2B-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.1764705882352941,0.0247629026780579
Salesforce/codegen-2B-multi,mmlu_high_school_geography,0-shot,accuracy,0.2323232323232323,0.0300886294902174
Salesforce/codegen-2B-multi,mmlu_high_school_psychology,0-shot,accuracy,0.2256880733944954,0.017923087667803
Salesforce/codegen-2B-multi,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.0417234303870538
Salesforce/codegen-2B-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.22,0.0416333199893226
Salesforce/codegen-2B-multi,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
Salesforce/codegen-2B-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2025641025641025,0.0203776609703713
Salesforce/codegen-2B-multi,mmlu_security_studies,0-shot,accuracy,0.1755102040816326,0.02435280072297
Salesforce/codegen-2B-multi,mmlu_professional_psychology,0-shot,accuracy,0.2663398692810457,0.0178831881346671
Salesforce/codegen-2B-multi,mmlu_human_sexuality,0-shot,accuracy,0.2061068702290076,0.0354777100415946
Salesforce/codegen-2B-multi,mmlu_econometrics,0-shot,accuracy,0.1929824561403508,0.0371245485372136
Salesforce/codegen-2B-multi,mmlu_miscellaneous,0-shot,accuracy,0.2822477650063857,0.0160953029698785
Salesforce/codegen-2B-multi,mmlu_marketing,0-shot,accuracy,0.2735042735042735,0.0292025401534311
Salesforce/codegen-2B-multi,mmlu_management,0-shot,accuracy,0.2621359223300971,0.0435463107726059
Salesforce/codegen-2B-multi,mmlu_nutrition,0-shot,accuracy,0.2679738562091503,0.0253606037962425
Salesforce/codegen-2B-multi,mmlu_medical_genetics,0-shot,accuracy,0.27,0.0446196043338473
Salesforce/codegen-2B-multi,mmlu_human_aging,0-shot,accuracy,0.3004484304932735,0.0307693520082291
Salesforce/codegen-2B-multi,mmlu_professional_medicine,0-shot,accuracy,0.1948529411764706,0.0240605994234874
Salesforce/codegen-2B-multi,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
Salesforce/codegen-2B-multi,mmlu_business_ethics,0-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-2B-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.2528301886792453,0.0267498997712412
Salesforce/codegen-2B-multi,mmlu_global_facts,0-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-2B-multi,mmlu_virology,0-shot,accuracy,0.253012048192771,0.0338442915523313
Salesforce/codegen-2B-multi,mmlu_professional_accounting,0-shot,accuracy,0.2411347517730496,0.0255187310495377
Salesforce/codegen-2B-multi,mmlu_college_physics,0-shot,accuracy,0.1078431372549019,0.0308642821220601
Salesforce/codegen-2B-multi,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.0357370531476345
Salesforce/codegen-2B-multi,mmlu_high_school_biology,0-shot,accuracy,0.267741935483871,0.0251890066602123
Salesforce/codegen-2B-multi,mmlu_college_biology,0-shot,accuracy,0.2291666666666666,0.0351469746786238
Salesforce/codegen-2B-multi,mmlu_anatomy,0-shot,accuracy,0.3111111111111111,0.0399926287661772
Salesforce/codegen-2B-multi,mmlu_college_chemistry,0-shot,accuracy,0.17,0.0377525168068637
Salesforce/codegen-2B-multi,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-2B-multi,mmlu_college_computer_science,0-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-2B-multi,mmlu_astronomy,0-shot,accuracy,0.2434210526315789,0.0349234966888423
Salesforce/codegen-2B-multi,mmlu_college_mathematics,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-2B-multi,mmlu_conceptual_physics,0-shot,accuracy,0.2553191489361702,0.0285048564705142
Salesforce/codegen-2B-multi,mmlu_abstract_algebra,0-shot,accuracy,0.27,0.0446196043338473
Salesforce/codegen-2B-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.37,0.0485236587093909
Salesforce/codegen-2B-multi,mmlu_machine_learning,0-shot,accuracy,0.3214285714285714,0.0443280405529151
Salesforce/codegen-2B-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.2068965517241379,0.0285013781678939
Salesforce/codegen-2B-multi,mmlu_high_school_statistics,0-shot,accuracy,0.162037037037037,0.0251304536522684
Salesforce/codegen-2B-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.2619047619047619,0.0226442126155252
Salesforce/codegen-2B-multi,mmlu_electrical_engineering,0-shot,accuracy,0.3310344827586207,0.0392154531246712
Salesforce/codegen-2B-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.0264661175389599
Salesforce/codegen-2B-multi,arc_challenge,25-shot,accuracy,0.2218430034129692,0.0121416590681478
Salesforce/codegen-2B-multi,arc_challenge,25-shot,acc_norm,0.257679180887372,0.0127807705627683
Salesforce/codegen-2B-multi,hellaswag,10-shot,accuracy,0.3142800238996215,0.0046327973752897
Salesforce/codegen-2B-multi,hellaswag,10-shot,acc_norm,0.3666600278828918,0.0048090772053434
Salesforce/codegen-2B-multi,truthfulqa_mc2,0-shot,accuracy,0.4459179118660402,0.0152215761448719
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,bleu_max,12.757546154780195,0.4498040933988642
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,bleu_acc,0.379436964504284,0.0169870392661429
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,bleu_diff,-1.833763023713032,0.4401683091426479
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge1_max,30.34079217924218,0.759556986026027
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge1_acc,0.2766217870257038,0.0156596057553269
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge1_diff,-7.075553576988576,0.7258364783365392
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge2_max,13.032258561545998,0.7083547799321905
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge2_acc,0.1346389228886169,0.0119492022937054
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge2_diff,-6.167131616599388,0.6346861400524839
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rougeL_max,27.669505162919336,0.7249567476362944
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rougeL_acc,0.2790697674418604,0.0157021070906279
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rougeL_diff,-6.591009170287162,0.7117399236362356
Salesforce/codegen-2B-multi,truthfulqa_mc1,0-shot,accuracy,0.2545899632802937,0.0152501170791564
Salesforce/codegen-2B-multi,winogrande,5-shot,accuracy,0.5501183898973955,0.0139817119040497
jisukim8873/falcon-7B-case-1,arc:challenge,25-shot,accuracy,0.439419795221843,0.0145037478235801
jisukim8873/falcon-7B-case-1,arc:challenge,25-shot,acc_norm,0.4761092150170648,0.0145947017980716
jisukim8873/falcon-7B-case-1,hellaswag,10-shot,accuracy,0.5987851025692094,0.0048914265333906
jisukim8873/falcon-7B-case-1,hellaswag,10-shot,acc_norm,0.7868950408285202,0.004086642984916
jisukim8873/falcon-7B-case-1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-1,hendrycksTest-anatomy,5-shot,accuracy,0.2888888888888888,0.0391545063041425
jisukim8873/falcon-7B-case-1,hendrycksTest-anatomy,5-shot,acc_norm,0.2888888888888888,0.0391545063041425
jisukim8873/falcon-7B-case-1,hendrycksTest-astronomy,5-shot,accuracy,0.2368421052631578,0.0345977760681053
jisukim8873/falcon-7B-case-1,hendrycksTest-astronomy,5-shot,acc_norm,0.2368421052631578,0.0345977760681053
jisukim8873/falcon-7B-case-1,hendrycksTest-business_ethics,5-shot,accuracy,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-1,hendrycksTest-business_ethics,5-shot,acc_norm,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3056603773584905,0.0283532980733226
jisukim8873/falcon-7B-case-1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3056603773584905,0.0283532980733226
jisukim8873/falcon-7B-case-1,hendrycksTest-college_biology,5-shot,accuracy,0.25,0.036210341218895
jisukim8873/falcon-7B-case-1,hendrycksTest-college_biology,5-shot,acc_norm,0.25,0.036210341218895
jisukim8873/falcon-7B-case-1,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-1,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-1,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.0440844002276807
jisukim8873/falcon-7B-case-1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.0440844002276807
jisukim8873/falcon-7B-case-1,hendrycksTest-college_medicine,5-shot,accuracy,0.2716763005780346,0.0339175032232165
jisukim8873/falcon-7B-case-1,hendrycksTest-college_medicine,5-shot,acc_norm,0.2716763005780346,0.0339175032232165
jisukim8873/falcon-7B-case-1,hendrycksTest-college_physics,5-shot,accuracy,0.196078431372549,0.0395058186117996
jisukim8873/falcon-7B-case-1,hendrycksTest-college_physics,5-shot,acc_norm,0.196078431372549,0.0395058186117996
jisukim8873/falcon-7B-case-1,hendrycksTest-computer_security,5-shot,accuracy,0.39,0.0490207130000197
jisukim8873/falcon-7B-case-1,hendrycksTest-computer_security,5-shot,acc_norm,0.39,0.0490207130000197
jisukim8873/falcon-7B-case-1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.0303635821972381
jisukim8873/falcon-7B-case-1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.0303635821972381
jisukim8873/falcon-7B-case-1,hendrycksTest-econometrics,5-shot,accuracy,0.3070175438596491,0.0433913832257986
jisukim8873/falcon-7B-case-1,hendrycksTest-econometrics,5-shot,acc_norm,0.3070175438596491,0.0433913832257986
jisukim8873/falcon-7B-case-1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
jisukim8873/falcon-7B-case-1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
jisukim8873/falcon-7B-case-1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2804232804232804,0.0231352879743256
jisukim8873/falcon-7B-case-1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2804232804232804,0.0231352879743256
jisukim8873/falcon-7B-case-1,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.0371848900681811
jisukim8873/falcon-7B-case-1,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.0371848900681811
jisukim8873/falcon-7B-case-1,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-1,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_biology,5-shot,accuracy,0.3032258064516129,0.0261486859306717
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3032258064516129,0.0261486859306717
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3201970443349753,0.0328264938530415
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3201970443349753,0.0328264938530415
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.0476095228569523
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.0476095228569523
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2787878787878788,0.0350143870629678
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2787878787878788,0.0350143870629678
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_geography,5-shot,accuracy,0.2626262626262626,0.0313530500953308
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2626262626262626,0.0313530500953308
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.0325771407770966
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.0325771407770966
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2717948717948718,0.0225565510101323
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2717948717948718,0.0225565510101323
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.0288013921936312
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.0288013921936312
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.0368488152138902
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.0368488152138902
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3009174311926605,0.0196647513668021
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3009174311926605,0.0196647513668021
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1898148148148148,0.0267447148346919
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1898148148148148,0.0267447148346919
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2745098039215686,0.0313217980308329
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2745098039215686,0.0313217980308329
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-1,hendrycksTest-human_aging,5-shot,accuracy,0.4125560538116592,0.0330406217544929
jisukim8873/falcon-7B-case-1,hendrycksTest-human_aging,5-shot,acc_norm,0.4125560538116592,0.0330406217544929
jisukim8873/falcon-7B-case-1,hendrycksTest-human_sexuality,5-shot,accuracy,0.2519083969465648,0.0380738711630608
jisukim8873/falcon-7B-case-1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2519083969465648,0.0380738711630608
jisukim8873/falcon-7B-case-1,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.0429434084521209
jisukim8873/falcon-7B-case-1,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.0429434084521209
jisukim8873/falcon-7B-case-1,hendrycksTest-jurisprudence,5-shot,accuracy,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2760736196319018,0.0351238528370505
jisukim8873/falcon-7B-case-1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2760736196319018,0.0351238528370505
jisukim8873/falcon-7B-case-1,hendrycksTest-machine_learning,5-shot,accuracy,0.3482142857142857,0.0452182990283358
jisukim8873/falcon-7B-case-1,hendrycksTest-machine_learning,5-shot,acc_norm,0.3482142857142857,0.0452182990283358
jisukim8873/falcon-7B-case-1,hendrycksTest-management,5-shot,accuracy,0.3106796116504854,0.0458212416016155
jisukim8873/falcon-7B-case-1,hendrycksTest-management,5-shot,acc_norm,0.3106796116504854,0.0458212416016155
jisukim8873/falcon-7B-case-1,hendrycksTest-marketing,5-shot,accuracy,0.329059829059829,0.0307823215776881
jisukim8873/falcon-7B-case-1,hendrycksTest-marketing,5-shot,acc_norm,0.329059829059829,0.0307823215776881
jisukim8873/falcon-7B-case-1,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.0479372485441101
jisukim8873/falcon-7B-case-1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.0479372485441101
jisukim8873/falcon-7B-case-1,hendrycksTest-miscellaneous,5-shot,accuracy,0.367816091954023,0.0172438288918462
jisukim8873/falcon-7B-case-1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.367816091954023,0.0172438288918462
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_disputes,5-shot,accuracy,0.3323699421965317,0.0253611687496882
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3323699421965317,0.0253611687496882
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2625698324022346,0.0147168242730177
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2625698324022346,0.0147168242730177
jisukim8873/falcon-7B-case-1,hendrycksTest-nutrition,5-shot,accuracy,0.3366013071895425,0.0270579746244943
jisukim8873/falcon-7B-case-1,hendrycksTest-nutrition,5-shot,acc_norm,0.3366013071895425,0.0270579746244943
jisukim8873/falcon-7B-case-1,hendrycksTest-philosophy,5-shot,accuracy,0.315112540192926,0.0263852737034644
jisukim8873/falcon-7B-case-1,hendrycksTest-philosophy,5-shot,acc_norm,0.315112540192926,0.0263852737034644
jisukim8873/falcon-7B-case-1,hendrycksTest-prehistory,5-shot,accuracy,0.3055555555555556,0.0256308249756213
jisukim8873/falcon-7B-case-1,hendrycksTest-prehistory,5-shot,acc_norm,0.3055555555555556,0.0256308249756213
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_accounting,5-shot,accuracy,0.2588652482269503,0.0261295725271808
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2588652482269503,0.0261295725271808
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_law,5-shot,accuracy,0.2607561929595828,0.0112134715596023
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_law,5-shot,acc_norm,0.2607561929595828,0.0112134715596023
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.0237097882538117
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.0237097882538117
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2598039215686274,0.0177408995091777
jisukim8873/falcon-7B-case-1,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.0430911870994645
jisukim8873/falcon-7B-case-1,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.0430911870994645
jisukim8873/falcon-7B-case-1,hendrycksTest-security_studies,5-shot,accuracy,0.2040816326530612,0.0258012834750905
jisukim8873/falcon-7B-case-1,hendrycksTest-security_studies,5-shot,acc_norm,0.2040816326530612,0.0258012834750905
jisukim8873/falcon-7B-case-1,hendrycksTest-sociology,5-shot,accuracy,0.308457711442786,0.0326581958851269
jisukim8873/falcon-7B-case-1,hendrycksTest-sociology,5-shot,acc_norm,0.308457711442786,0.0326581958851269
jisukim8873/falcon-7B-case-1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.43,0.0497569851956242
jisukim8873/falcon-7B-case-1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.43,0.0497569851956242
jisukim8873/falcon-7B-case-1,hendrycksTest-virology,5-shot,accuracy,0.3433734939759036,0.036965843170106
jisukim8873/falcon-7B-case-1,hendrycksTest-virology,5-shot,acc_norm,0.3433734939759036,0.036965843170106
jisukim8873/falcon-7B-case-1,hendrycksTest-world_religions,5-shot,accuracy,0.3684210526315789,0.0369965801765687
jisukim8873/falcon-7B-case-1,hendrycksTest-world_religions,5-shot,acc_norm,0.3684210526315789,0.0369965801765687
jisukim8873/falcon-7B-case-1,truthfulqa:mc,0-shot,mc1,0.2570379436964504,0.015298077509485
jisukim8873/falcon-7B-case-1,truthfulqa:mc,0-shot,mc2,0.3778933788894488,0.0143981866732092
jisukim8873/falcon-7B-case-1,winogrande,5-shot,accuracy,0.7166535122336227,0.0126647517355053
jisukim8873/falcon-7B-case-1,gsm8k,5-shot,accuracy,0.0940106141015921,0.0080388198188724
jisukim8873/falcon-7B-case-1,minerva_math_precalc,5-shot,accuracy,0.0073260073260073,0.003652908089383
jisukim8873/falcon-7B-case-1,minerva_math_prealgebra,5-shot,accuracy,0.026406429391504,0.0054360577625739
jisukim8873/falcon-7B-case-1,minerva_math_num_theory,5-shot,accuracy,0.0129629629629629,0.0048721929845814
jisukim8873/falcon-7B-case-1,minerva_math_intermediate_algebra,5-shot,accuracy,0.0132890365448504,0.0038127511080199
jisukim8873/falcon-7B-case-1,minerva_math_geometry,5-shot,accuracy,0.0208768267223382,0.0065393857958139
jisukim8873/falcon-7B-case-1,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.0062753625139895
jisukim8873/falcon-7B-case-1,minerva_math_algebra,5-shot,accuracy,0.0185341196293176,0.0039163476763639
jisukim8873/falcon-7B-case-1,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-1,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-1,arithmetic_3da,5-shot,accuracy,0.202,0.0089798841395409
jisukim8873/falcon-7B-case-1,arithmetic_3ds,5-shot,accuracy,0.3245,0.0104716141234854
jisukim8873/falcon-7B-case-1,arithmetic_4da,5-shot,accuracy,0.0055,0.0016541593398342
jisukim8873/falcon-7B-case-1,arithmetic_2ds,5-shot,accuracy,0.4725,0.0111662087168635
jisukim8873/falcon-7B-case-1,arithmetic_5ds,5-shot,accuracy,0.035,0.0041104680966997
jisukim8873/falcon-7B-case-1,arithmetic_5da,5-shot,accuracy,0.0005,0.0005
jisukim8873/falcon-7B-case-1,arithmetic_1dc,5-shot,accuracy,0.091,0.0064327435900281
jisukim8873/falcon-7B-case-1,arithmetic_4ds,5-shot,accuracy,0.084,0.0062041313350712
jisukim8873/falcon-7B-case-1,arithmetic_2dm,5-shot,accuracy,0.2645,0.0098650156749564
jisukim8873/falcon-7B-case-1,arithmetic_2da,5-shot,accuracy,0.7235,0.0100036949151788
jisukim8873/falcon-7B-case-1,gsm8k_cot,5-shot,accuracy,0.0962850644427596,0.0081252641282158
jisukim8873/falcon-7B-case-1,anli_r2,0-shot,brier_score,0.941338151188042,
jisukim8873/falcon-7B-case-1,anli_r3,0-shot,brier_score,0.87237900881463,
jisukim8873/falcon-7B-case-1,anli_r1,0-shot,brier_score,0.9857934321426032,
jisukim8873/falcon-7B-case-1,xnli_eu,0-shot,brier_score,1.026246542896589,
jisukim8873/falcon-7B-case-1,xnli_vi,0-shot,brier_score,1.0279853059907973,
jisukim8873/falcon-7B-case-1,xnli_ru,0-shot,brier_score,0.8097107614311067,
jisukim8873/falcon-7B-case-1,xnli_zh,0-shot,brier_score,1.0156490749806204,
jisukim8873/falcon-7B-case-1,xnli_tr,0-shot,brier_score,0.9756836472864572,
jisukim8873/falcon-7B-case-1,xnli_fr,0-shot,brier_score,0.7603981109729621,
jisukim8873/falcon-7B-case-1,xnli_en,0-shot,brier_score,0.662787162600886,
jisukim8873/falcon-7B-case-1,xnli_ur,0-shot,brier_score,1.31954291668707,
jisukim8873/falcon-7B-case-1,xnli_ar,0-shot,brier_score,1.2990240082694595,
jisukim8873/falcon-7B-case-1,xnli_de,0-shot,brier_score,0.849940839182899,
jisukim8873/falcon-7B-case-1,xnli_hi,0-shot,brier_score,1.1637487081246372,
jisukim8873/falcon-7B-case-1,xnli_es,0-shot,brier_score,0.8194931547365195,
jisukim8873/falcon-7B-case-1,xnli_bg,0-shot,brier_score,0.9860919344218412,
jisukim8873/falcon-7B-case-1,xnli_sw,0-shot,brier_score,1.082079686536352,
jisukim8873/falcon-7B-case-1,xnli_el,0-shot,brier_score,0.8625720197689004,
jisukim8873/falcon-7B-case-1,xnli_th,0-shot,brier_score,0.9548472798777402,
jisukim8873/falcon-7B-case-1,logiqa2,0-shot,brier_score,1.05514711694456,
jisukim8873/falcon-7B-case-1,mathqa,0-shot,brier_score,0.9309727513874028,
jisukim8873/falcon-7B-case-1,lambada_standard,0-shot,perplexity,4.170478886814212,0.0917183231421987
jisukim8873/falcon-7B-case-1,lambada_standard,0-shot,accuracy,0.6625266834853484,0.0065876949385287
jisukim8873/falcon-7B-case-1,lambada_openai,0-shot,perplexity,3.3182500746294745,0.0688089665195753
jisukim8873/falcon-7B-case-1,lambada_openai,0-shot,accuracy,0.7343295167863381,0.0061535993728225
llama2_220M_nl_only_shuf-hf,logiqa2,0-shot,accuracy,0.2335,0.0107
llama2_220M_nl_only_shuf-hf,xnli,0-shot,accuracy,0.3428,0.0191
llama2_220M_nl_only_shuf-hf,fld,0-shot,accuracy,0.0,
llama2_220M_nl_only_shuf-hf,asdiv,5-shot,accuracy,0.0035,0.0012
llama2_220M_nl_only_shuf-hf,anli,0-shot,accuracy,0.3319,0.0149
llama2_220M_nl_only_shuf-hf,minerva_math_geometry,5-shot,accuracy,0.0,
llama2_220M_nl_only_shuf-hf,minerva_math_num_theory,5-shot,accuracy,0.0,
llama2_220M_nl_only_shuf-hf,minerva_math_prealgebra,5-shot,accuracy,0.0,
llama2_220M_nl_only_shuf-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_only_shuf-hf,minerva_math_algebra,5-shot,accuracy,0.0008,
llama2_220M_nl_only_shuf-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
llama2_220M_nl_only_shuf-hf,arithmetic,5-shot,accuracy,0.0065,0.0074
AbacusResearch/haLLawa4-7b,minerva_math_precalc,5-shot,accuracy,0.0879120879120879,0.0121295418211436
AbacusResearch/haLLawa4-7b,minerva_math_prealgebra,5-shot,accuracy,0.4064293915040183,0.0166521042550629
AbacusResearch/haLLawa4-7b,minerva_math_num_theory,5-shot,accuracy,0.15,0.015380154912113
AbacusResearch/haLLawa4-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0930232558139534,0.0096714273723563
AbacusResearch/haLLawa4-7b,minerva_math_geometry,5-shot,accuracy,0.1774530271398747,0.0174746349528059
AbacusResearch/haLLawa4-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.2172995780590717,0.0189625463403293
AbacusResearch/haLLawa4-7b,minerva_math_algebra,5-shot,accuracy,0.3294018534119629,0.0136474605974689
AbacusResearch/haLLawa4-7b,fld_default,0-shot,accuracy,0.0,
AbacusResearch/haLLawa4-7b,fld_star,0-shot,accuracy,0.0,
AbacusResearch/haLLawa4-7b,arithmetic_3da,5-shot,accuracy,0.9845,0.0027629136515502
AbacusResearch/haLLawa4-7b,arithmetic_3ds,5-shot,accuracy,0.985,0.0027186753387999
AbacusResearch/haLLawa4-7b,arithmetic_4da,5-shot,accuracy,0.948,0.0049659168503995
AbacusResearch/haLLawa4-7b,arithmetic_2ds,5-shot,accuracy,0.9955,0.0014969954902233
AbacusResearch/haLLawa4-7b,arithmetic_5ds,5-shot,accuracy,0.901,0.0066799559059512
AbacusResearch/haLLawa4-7b,arithmetic_5da,5-shot,accuracy,0.916,0.0062041313350712
AbacusResearch/haLLawa4-7b,arithmetic_1dc,5-shot,accuracy,0.757,0.0095927843067265
AbacusResearch/haLLawa4-7b,arithmetic_4ds,5-shot,accuracy,0.951,0.0048281627538629
AbacusResearch/haLLawa4-7b,arithmetic_2dm,5-shot,accuracy,0.6645,0.0105605699571051
AbacusResearch/haLLawa4-7b,arithmetic_2da,5-shot,accuracy,1.0,
AbacusResearch/haLLawa4-7b,gsm8k_cot,5-shot,accuracy,0.7581501137225171,0.0117948613713187
AbacusResearch/haLLawa4-7b,gsm8k,5-shot,accuracy,0.7429871114480667,0.0120367817574286
AbacusResearch/haLLawa4-7b,anli_r2,0-shot,brier_score,0.7666504741725055,
AbacusResearch/haLLawa4-7b,anli_r3,0-shot,brier_score,0.8663025386598154,
AbacusResearch/haLLawa4-7b,anli_r1,0-shot,brier_score,0.6573805818915781,
AbacusResearch/haLLawa4-7b,xnli_eu,0-shot,brier_score,1.1139982065153322,
AbacusResearch/haLLawa4-7b,xnli_vi,0-shot,brier_score,0.9999180808359712,
AbacusResearch/haLLawa4-7b,xnli_ru,0-shot,brier_score,0.9538905561506924,
AbacusResearch/haLLawa4-7b,xnli_zh,0-shot,brier_score,1.0601931277652965,
AbacusResearch/haLLawa4-7b,xnli_tr,0-shot,brier_score,1.0467850390267222,
AbacusResearch/haLLawa4-7b,xnli_fr,0-shot,brier_score,0.9334503115142728,
AbacusResearch/haLLawa4-7b,xnli_en,0-shot,brier_score,0.7398112895767275,
AbacusResearch/haLLawa4-7b,xnli_ur,0-shot,brier_score,1.2069939556444398,
AbacusResearch/haLLawa4-7b,xnli_ar,0-shot,brier_score,1.1995508224164844,
AbacusResearch/haLLawa4-7b,xnli_de,0-shot,brier_score,0.911948418032992,
AbacusResearch/haLLawa4-7b,xnli_hi,0-shot,brier_score,0.9461357371948408,
AbacusResearch/haLLawa4-7b,xnli_es,0-shot,brier_score,0.9585943286675972,
AbacusResearch/haLLawa4-7b,xnli_bg,0-shot,brier_score,0.9511880273157712,
AbacusResearch/haLLawa4-7b,xnli_sw,0-shot,brier_score,1.0399624826911418,
AbacusResearch/haLLawa4-7b,xnli_el,0-shot,brier_score,0.9893964628647416,
AbacusResearch/haLLawa4-7b,xnli_th,0-shot,brier_score,1.1061854373450353,
AbacusResearch/haLLawa4-7b,logiqa2,0-shot,brier_score,0.9626222319450616,
AbacusResearch/haLLawa4-7b,mathqa,0-shot,brier_score,0.982078209404206,
AbacusResearch/haLLawa4-7b,lambada_standard,0-shot,perplexity,4.423249636812455,0.1144835178622582
AbacusResearch/haLLawa4-7b,lambada_standard,0-shot,accuracy,0.6367164758393169,0.0067005116674768
AbacusResearch/haLLawa4-7b,lambada_openai,0-shot,perplexity,3.629649594000675,0.0843419532026633
AbacusResearch/haLLawa4-7b,lambada_openai,0-shot,accuracy,0.6959052978847273,0.0064090191789628
AbacusResearch/haLLawa4-7b,mmlu_world_religions,0-shot,accuracy,0.8304093567251462,0.0287821081054017
AbacusResearch/haLLawa4-7b,mmlu_formal_logic,0-shot,accuracy,0.4841269841269841,0.044698818540726
AbacusResearch/haLLawa4-7b,mmlu_prehistory,0-shot,accuracy,0.7253086419753086,0.0248360578682946
AbacusResearch/haLLawa4-7b,mmlu_moral_scenarios,0-shot,accuracy,0.4290502793296089,0.016553287863116
AbacusResearch/haLLawa4-7b,mmlu_high_school_world_history,0-shot,accuracy,0.8059071729957806,0.0257449025322909
AbacusResearch/haLLawa4-7b,mmlu_moral_disputes,0-shot,accuracy,0.7312138728323699,0.0238680032625001
AbacusResearch/haLLawa4-7b,mmlu_professional_law,0-shot,accuracy,0.470013037809648,0.012747248967079
AbacusResearch/haLLawa4-7b,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
AbacusResearch/haLLawa4-7b,mmlu_high_school_us_history,0-shot,accuracy,0.8627450980392157,0.0241522259628015
AbacusResearch/haLLawa4-7b,mmlu_philosophy,0-shot,accuracy,0.7009646302250804,0.0260033011178851
AbacusResearch/haLLawa4-7b,mmlu_jurisprudence,0-shot,accuracy,0.7870370370370371,0.0395783547198097
AbacusResearch/haLLawa4-7b,mmlu_international_law,0-shot,accuracy,0.768595041322314,0.0384985609879409
AbacusResearch/haLLawa4-7b,mmlu_high_school_european_history,0-shot,accuracy,0.7636363636363637,0.0331750593000918
AbacusResearch/haLLawa4-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9015544041450776,0.0215002495760334
AbacusResearch/haLLawa4-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.680672268907563,0.0302839955258844
AbacusResearch/haLLawa4-7b,mmlu_high_school_geography,0-shot,accuracy,0.7929292929292929,0.028869778460267
AbacusResearch/haLLawa4-7b,mmlu_high_school_psychology,0-shot,accuracy,0.8311926605504587,0.0160600562685303
AbacusResearch/haLLawa4-7b,mmlu_public_relations,0-shot,accuracy,0.6818181818181818,0.044612721759105
AbacusResearch/haLLawa4-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.86,0.0348735088019776
AbacusResearch/haLLawa4-7b,mmlu_sociology,0-shot,accuracy,0.8407960199004975,0.0258706467661691
AbacusResearch/haLLawa4-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6692307692307692,0.0238547956809711
AbacusResearch/haLLawa4-7b,mmlu_security_studies,0-shot,accuracy,0.7387755102040816,0.0281234293351427
AbacusResearch/haLLawa4-7b,mmlu_professional_psychology,0-shot,accuracy,0.6748366013071896,0.0189508867708063
AbacusResearch/haLLawa4-7b,mmlu_human_sexuality,0-shot,accuracy,0.8091603053435115,0.0344651335075259
AbacusResearch/haLLawa4-7b,mmlu_econometrics,0-shot,accuracy,0.4824561403508772,0.0470070803355103
AbacusResearch/haLLawa4-7b,mmlu_miscellaneous,0-shot,accuracy,0.8314176245210728,0.0133878957315436
AbacusResearch/haLLawa4-7b,mmlu_marketing,0-shot,accuracy,0.8760683760683761,0.0215864940012813
AbacusResearch/haLLawa4-7b,mmlu_management,0-shot,accuracy,0.7766990291262136,0.0412355318989143
AbacusResearch/haLLawa4-7b,mmlu_nutrition,0-shot,accuracy,0.7320261437908496,0.0253606037962425
AbacusResearch/haLLawa4-7b,mmlu_medical_genetics,0-shot,accuracy,0.7,0.0460566186471838
AbacusResearch/haLLawa4-7b,mmlu_human_aging,0-shot,accuracy,0.6816143497757847,0.0312658052251371
AbacusResearch/haLLawa4-7b,mmlu_professional_medicine,0-shot,accuracy,0.6727941176470589,0.0285014528603965
AbacusResearch/haLLawa4-7b,mmlu_college_medicine,0-shot,accuracy,0.6589595375722543,0.0361466542418082
AbacusResearch/haLLawa4-7b,mmlu_business_ethics,0-shot,accuracy,0.63,0.0485236587093909
AbacusResearch/haLLawa4-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.6830188679245283,0.0286372356398008
AbacusResearch/haLLawa4-7b,mmlu_global_facts,0-shot,accuracy,0.36,0.0482418151324421
AbacusResearch/haLLawa4-7b,mmlu_virology,0-shot,accuracy,0.5180722891566265,0.0388995125282721
AbacusResearch/haLLawa4-7b,mmlu_professional_accounting,0-shot,accuracy,0.4929078014184397,0.029824498559129
AbacusResearch/haLLawa4-7b,mmlu_college_physics,0-shot,accuracy,0.4509803921568627,0.0495121825239626
AbacusResearch/haLLawa4-7b,mmlu_high_school_physics,0-shot,accuracy,0.3774834437086092,0.0395802723112157
AbacusResearch/haLLawa4-7b,mmlu_high_school_biology,0-shot,accuracy,0.7806451612903226,0.0235407993587233
AbacusResearch/haLLawa4-7b,mmlu_college_biology,0-shot,accuracy,0.75,0.036210341218895
AbacusResearch/haLLawa4-7b,mmlu_anatomy,0-shot,accuracy,0.6148148148148148,0.0420392104015627
AbacusResearch/haLLawa4-7b,mmlu_college_chemistry,0-shot,accuracy,0.45,0.0499999999999999
AbacusResearch/haLLawa4-7b,mmlu_computer_security,0-shot,accuracy,0.75,0.0435194139889244
AbacusResearch/haLLawa4-7b,mmlu_college_computer_science,0-shot,accuracy,0.59,0.049431107042371
AbacusResearch/haLLawa4-7b,mmlu_astronomy,0-shot,accuracy,0.7105263157894737,0.0369067798613728
AbacusResearch/haLLawa4-7b,mmlu_college_mathematics,0-shot,accuracy,0.29,0.0456048021572068
AbacusResearch/haLLawa4-7b,mmlu_conceptual_physics,0-shot,accuracy,0.5659574468085107,0.0324003808679274
AbacusResearch/haLLawa4-7b,mmlu_abstract_algebra,0-shot,accuracy,0.35,0.0479372485441102
AbacusResearch/haLLawa4-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.7,0.0460566186471838
AbacusResearch/haLLawa4-7b,mmlu_machine_learning,0-shot,accuracy,0.4017857142857143,0.0465333314697364
AbacusResearch/haLLawa4-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.5123152709359606,0.0351692044422089
AbacusResearch/haLLawa4-7b,mmlu_high_school_statistics,0-shot,accuracy,0.5138888888888888,0.0340865586797774
AbacusResearch/haLLawa4-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.3968253968253968,0.0251971010742464
AbacusResearch/haLLawa4-7b,mmlu_electrical_engineering,0-shot,accuracy,0.5793103448275863,0.0411391498118926
AbacusResearch/haLLawa4-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.3185185185185185,0.0284065330906084
AbacusResearch/haLLawa4-7b,arc_challenge,25-shot,accuracy,0.7030716723549488,0.0133520259767252
AbacusResearch/haLLawa4-7b,arc_challenge,25-shot,acc_norm,0.7235494880546075,0.0130696624742524
AbacusResearch/haLLawa4-7b,hellaswag,10-shot,accuracy,0.7102170882294364,0.0045273436511307
AbacusResearch/haLLawa4-7b,hellaswag,10-shot,acc_norm,0.8841864170483967,0.0031934725302823
AbacusResearch/haLLawa4-7b,truthfulqa_mc2,0-shot,accuracy,0.7478686028704679,0.0141780447205111
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,bleu_max,15.495195703574325,0.5680370072696724
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,bleu_acc,0.5263157894736842,0.0174792411619754
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,bleu_diff,2.2914445163776342,0.4426566044128238
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge1_max,41.476176909090015,0.7163761236938211
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge1_acc,0.5703794369645043,0.0173292345804091
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge1_diff,4.5715005574824925,0.6975679934842908
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge2_max,26.293465437291108,0.7848741256847009
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge2_acc,0.4638922888616891,0.0174578004222686
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge2_diff,3.935472582571117,0.7546985023903435
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rougeL_max,37.83478014787854,0.7279950256366058
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rougeL_acc,0.5385556915544676,0.0174513841046374
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rougeL_diff,4.12214109311066,0.7002705028145758
AbacusResearch/haLLawa4-7b,truthfulqa_mc1,0-shot,accuracy,0.5936352509179926,0.0171938358120939
AbacusResearch/haLLawa4-7b,winogrande,5-shot,accuracy,0.8263614838200474,0.010646116480331
allenai/OLMo-7B-hf,minerva_math_precalc,5-shot,accuracy,0.0164835164835164,0.0054540297647667
allenai/OLMo-7B-hf,minerva_math_prealgebra,5-shot,accuracy,0.0355912743972445,0.0062812012527095
allenai/OLMo-7B-hf,minerva_math_num_theory,5-shot,accuracy,0.0148148148148148,0.0052037049875126
allenai/OLMo-7B-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.0188261351052048,0.0045253304986684
allenai/OLMo-7B-hf,minerva_math_geometry,5-shot,accuracy,0.0187891440501043,0.0062104164279974
allenai/OLMo-7B-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.0232067510548523,0.0069227384871433
allenai/OLMo-7B-hf,minerva_math_algebra,5-shot,accuracy,0.012636899747262,0.0032435184443521
allenai/OLMo-7B-hf,fld_default,0-shot,accuracy,0.0,
allenai/OLMo-7B-hf,fld_star,0-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521
allenai/OLMo-7B-hf,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
allenai/OLMo-7B-hf,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
allenai/OLMo-7B-hf,arithmetic_2ds,5-shot,accuracy,0.0155,0.0027629136515503
allenai/OLMo-7B-hf,arithmetic_5ds,5-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_5da,5-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_1dc,5-shot,accuracy,0.0065,0.0017973564602277
allenai/OLMo-7B-hf,arithmetic_4ds,5-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_2dm,5-shot,accuracy,0.029,0.0037532044004605
allenai/OLMo-7B-hf,arithmetic_2da,5-shot,accuracy,0.016,0.0028064101569415
allenai/OLMo-7B-hf,gsm8k_cot,5-shot,accuracy,0.0773313115996967,0.0073577135232223
allenai/OLMo-7B-hf,gsm8k,5-shot,accuracy,0.047005307050796,0.0058298983559371
allenai/OLMo-7B-hf,anli_r2,0-shot,brier_score,0.727415360551483,
allenai/OLMo-7B-hf,anli_r3,0-shot,brier_score,0.7329111882888333,
allenai/OLMo-7B-hf,anli_r1,0-shot,brier_score,0.7459395081085325,
allenai/OLMo-7B-hf,xnli_eu,0-shot,brier_score,0.8502840947105932,
allenai/OLMo-7B-hf,xnli_vi,0-shot,brier_score,0.8697900492853695,
allenai/OLMo-7B-hf,xnli_ru,0-shot,brier_score,0.7888266397325421,
allenai/OLMo-7B-hf,xnli_zh,0-shot,brier_score,0.9170790959685604,
allenai/OLMo-7B-hf,xnli_tr,0-shot,brier_score,0.9126604079971902,
allenai/OLMo-7B-hf,xnli_fr,0-shot,brier_score,0.6871829801737326,
allenai/OLMo-7B-hf,xnli_en,0-shot,brier_score,0.6203086496227678,
allenai/OLMo-7B-hf,xnli_ur,0-shot,brier_score,1.1489733890229683,
allenai/OLMo-7B-hf,xnli_ar,0-shot,brier_score,1.2297706224773113,
allenai/OLMo-7B-hf,xnli_de,0-shot,brier_score,0.8009175976298082,
allenai/OLMo-7B-hf,xnli_hi,0-shot,brier_score,0.8077524633855024,
allenai/OLMo-7B-hf,xnli_es,0-shot,brier_score,0.8277268635765521,
allenai/OLMo-7B-hf,xnli_bg,0-shot,brier_score,0.7504606344491516,
allenai/OLMo-7B-hf,xnli_sw,0-shot,brier_score,1.02550711834348,
allenai/OLMo-7B-hf,xnli_el,0-shot,brier_score,0.9957388418764128,
allenai/OLMo-7B-hf,xnli_th,0-shot,brier_score,1.048711074480121,
allenai/OLMo-7B-hf,logiqa2,0-shot,brier_score,0.9608102017283244,
allenai/OLMo-7B-hf,mathqa,0-shot,brier_score,0.9668035927315314,
allenai/OLMo-7B-hf,lambada_standard,0-shot,perplexity,5.030584263765433,0.1113066645751385
allenai/OLMo-7B-hf,lambada_standard,0-shot,accuracy,0.6405977100718029,0.0066849041360329
allenai/OLMo-7B-hf,lambada_openai,0-shot,perplexity,4.123391391255741,0.0868088602519733
allenai/OLMo-7B-hf,lambada_openai,0-shot,accuracy,0.6941587424801087,0.0064193271158926
allenai/OLMo-7B-hf,mmlu_world_religions,0-shot,accuracy,0.2631578947368421,0.0337731025220919
allenai/OLMo-7B-hf,mmlu_formal_logic,0-shot,accuracy,0.1984126984126984,0.0356701667527686
allenai/OLMo-7B-hf,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.0257731111696304
allenai/OLMo-7B-hf,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705749
allenai/OLMo-7B-hf,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
allenai/OLMo-7B-hf,mmlu_moral_disputes,0-shot,accuracy,0.2658959537572254,0.0237862032555082
allenai/OLMo-7B-hf,mmlu_professional_law,0-shot,accuracy,0.2607561929595828,0.0112134715596023
allenai/OLMo-7B-hf,mmlu_logical_fallacies,0-shot,accuracy,0.2392638036809816,0.0335195387952126
allenai/OLMo-7B-hf,mmlu_high_school_us_history,0-shot,accuracy,0.2205882352941176,0.029102254389674
allenai/OLMo-7B-hf,mmlu_philosophy,0-shot,accuracy,0.3247588424437299,0.026596782287697
allenai/OLMo-7B-hf,mmlu_jurisprudence,0-shot,accuracy,0.2129629629629629,0.0395783547198097
allenai/OLMo-7B-hf,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.040261875275912
allenai/OLMo-7B-hf,mmlu_high_school_european_history,0-shot,accuracy,0.2666666666666666,0.0345313180188541
allenai/OLMo-7B-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3316062176165803,0.0339763654108911
allenai/OLMo-7B-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.3235294117647059,0.0303883535518868
allenai/OLMo-7B-hf,mmlu_high_school_geography,0-shot,accuracy,0.2323232323232323,0.0300886294902174
allenai/OLMo-7B-hf,mmlu_high_school_psychology,0-shot,accuracy,0.2477064220183486,0.0185081436025478
allenai/OLMo-7B-hf,mmlu_public_relations,0-shot,accuracy,0.3272727272727272,0.0449429086625208
allenai/OLMo-7B-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.0429234695990928
allenai/OLMo-7B-hf,mmlu_sociology,0-shot,accuracy,0.3582089552238806,0.0339039304226881
allenai/OLMo-7B-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3435897435897436,0.0240786965806354
allenai/OLMo-7B-hf,mmlu_security_studies,0-shot,accuracy,0.4204081632653061,0.031601069934496
allenai/OLMo-7B-hf,mmlu_professional_psychology,0-shot,accuracy,0.2271241830065359,0.0169498532792123
allenai/OLMo-7B-hf,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.0391534540884783
allenai/OLMo-7B-hf,mmlu_econometrics,0-shot,accuracy,0.2982456140350877,0.0430368403353731
allenai/OLMo-7B-hf,mmlu_miscellaneous,0-shot,accuracy,0.2950191570881226,0.0163083637729327
allenai/OLMo-7B-hf,mmlu_marketing,0-shot,accuracy,0.2863247863247863,0.0296143236904566
allenai/OLMo-7B-hf,mmlu_management,0-shot,accuracy,0.2718446601941747,0.0440526802414092
allenai/OLMo-7B-hf,mmlu_nutrition,0-shot,accuracy,0.3398692810457516,0.0271219560713888
allenai/OLMo-7B-hf,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
allenai/OLMo-7B-hf,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.0314938467099413
allenai/OLMo-7B-hf,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091216
allenai/OLMo-7B-hf,mmlu_college_medicine,0-shot,accuracy,0.2427745664739884,0.0326926380614177
allenai/OLMo-7B-hf,mmlu_business_ethics,0-shot,accuracy,0.21,0.0409360180740332
allenai/OLMo-7B-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.2716981132075471,0.0273777066246707
allenai/OLMo-7B-hf,mmlu_global_facts,0-shot,accuracy,0.32,0.046882617226215
allenai/OLMo-7B-hf,mmlu_virology,0-shot,accuracy,0.4036144578313253,0.0381948614075839
allenai/OLMo-7B-hf,mmlu_professional_accounting,0-shot,accuracy,0.2234042553191489,0.0248479213580639
allenai/OLMo-7B-hf,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
allenai/OLMo-7B-hf,mmlu_high_school_physics,0-shot,accuracy,0.2980132450331126,0.0373453567678719
allenai/OLMo-7B-hf,mmlu_high_school_biology,0-shot,accuracy,0.3451612903225806,0.0270457465735343
allenai/OLMo-7B-hf,mmlu_college_biology,0-shot,accuracy,0.2430555555555555,0.0358687928008034
allenai/OLMo-7B-hf,mmlu_anatomy,0-shot,accuracy,0.2666666666666666,0.038201699145179
allenai/OLMo-7B-hf,mmlu_college_chemistry,0-shot,accuracy,0.29,0.0456048021572068
allenai/OLMo-7B-hf,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
allenai/OLMo-7B-hf,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
allenai/OLMo-7B-hf,mmlu_astronomy,0-shot,accuracy,0.2697368421052631,0.0361178056028489
allenai/OLMo-7B-hf,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276808
allenai/OLMo-7B-hf,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.0295131966255393
allenai/OLMo-7B-hf,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.0435194139889244
allenai/OLMo-7B-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.2,0.0402015126103684
allenai/OLMo-7B-hf,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.0428785875134045
allenai/OLMo-7B-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.3201970443349753,0.0328264938530415
allenai/OLMo-7B-hf,mmlu_high_school_statistics,0-shot,accuracy,0.4537037037037037,0.0339532272637579
allenai/OLMo-7B-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.2539682539682539,0.0224180428911139
allenai/OLMo-7B-hf,mmlu_electrical_engineering,0-shot,accuracy,0.2896551724137931,0.0378001923043801
allenai/OLMo-7B-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.2703703703703703,0.0270803728151456
allenai/OLMo-7B-hf,arc_challenge,25-shot,accuracy,0.4343003412969283,0.0144847030488573
allenai/OLMo-7B-hf,arc_challenge,25-shot,acc_norm,0.4616040955631399,0.0145682455502963
allenai/OLMo-7B-hf,hellaswag,10-shot,accuracy,0.5726946823341964,0.004936762568217
allenai/OLMo-7B-hf,hellaswag,10-shot,acc_norm,0.7694682334196375,0.0042031244890371
allenai/OLMo-7B-hf,truthfulqa_mc2,0-shot,accuracy,0.3585058314358672,0.0138027499848702
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,bleu_max,26.53940083328737,0.7779688490400837
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,bleu_acc,0.2937576499388005,0.0159450685812366
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,bleu_diff,-9.216867205020309,0.7821192582719276
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge1_max,51.94656110399984,0.8300725318977166
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge1_acc,0.3133414932680538,0.0162380650690596
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge1_diff,-11.3371350794366,0.8136895635471025
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge2_max,36.413967516398216,0.9679506184935704
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge2_acc,0.2582619339045288,0.0153218216884761
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge2_diff,-13.565291787723291,1.009721378967521
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rougeL_max,49.13417294832966,0.8355931924931606
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rougeL_acc,0.2937576499388005,0.0159450685812366
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rougeL_diff,-11.47267969384019,0.8120028631669012
allenai/OLMo-7B-hf,truthfulqa_mc1,0-shot,accuracy,0.244798041615667,0.015051869486715
allenai/OLMo-7B-hf,winogrande,5-shot,accuracy,0.6953433307024467,0.0129356464993253
Salesforce/codegen-6B-mono,minerva_math_precalc,5-shot,accuracy,0.0164835164835164,0.0054540297647667
Salesforce/codegen-6B-mono,minerva_math_prealgebra,5-shot,accuracy,0.0114810562571756,0.0036118008378658
Salesforce/codegen-6B-mono,minerva_math_num_theory,5-shot,accuracy,0.0148148148148148,0.0052037049875126
Salesforce/codegen-6B-mono,minerva_math_intermediate_algebra,5-shot,accuracy,0.0066445182724252,0.0027050844483854
Salesforce/codegen-6B-mono,minerva_math_geometry,5-shot,accuracy,0.0062630480167014,0.0036083997328878
Salesforce/codegen-6B-mono,minerva_math_counting_and_prob,5-shot,accuracy,0.0126582278481012,0.0051403138895788
Salesforce/codegen-6B-mono,minerva_math_algebra,5-shot,accuracy,0.0101095197978096,0.0029048017186508
Salesforce/codegen-6B-mono,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_3da,5-shot,accuracy,0.0035,0.0013208888574315
Salesforce/codegen-6B-mono,arithmetic_3ds,5-shot,accuracy,0.01,0.0022254159696827
Salesforce/codegen-6B-mono,arithmetic_4da,5-shot,accuracy,0.001,0.0007069298939339
Salesforce/codegen-6B-mono,arithmetic_2ds,5-shot,accuracy,0.0865,0.0062871805540846
Salesforce/codegen-6B-mono,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_1dc,5-shot,accuracy,0.112,0.0070535718921847
Salesforce/codegen-6B-mono,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_2dm,5-shot,accuracy,0.061,0.0053529269482644
Salesforce/codegen-6B-mono,arithmetic_2da,5-shot,accuracy,0.064,0.0054742107642788
Salesforce/codegen-6B-mono,gsm8k_cot,5-shot,accuracy,0.0280515542077331,0.0045482295338363
Salesforce/codegen-6B-mono,gsm8k,5-shot,accuracy,0.026535253980288,0.0044270459872651
Salesforce/codegen-6B-mono,anli_r2,0-shot,brier_score,0.8876594910752693,
Salesforce/codegen-6B-mono,anli_r3,0-shot,brier_score,0.9495729853127676,
Salesforce/codegen-6B-mono,anli_r1,0-shot,brier_score,0.9382038423017862,
Salesforce/codegen-6B-mono,xnli_eu,0-shot,brier_score,1.2520379152089611,
Salesforce/codegen-6B-mono,xnli_vi,0-shot,brier_score,1.1171458537014292,
Salesforce/codegen-6B-mono,xnli_ru,0-shot,brier_score,0.8728299042741055,
Salesforce/codegen-6B-mono,xnli_zh,0-shot,brier_score,0.921242092190122,
Salesforce/codegen-6B-mono,xnli_tr,0-shot,brier_score,0.9486527913660736,
Salesforce/codegen-6B-mono,xnli_fr,0-shot,brier_score,0.9557543141847248,
Salesforce/codegen-6B-mono,xnli_en,0-shot,brier_score,0.7389286092568025,
Salesforce/codegen-6B-mono,xnli_ur,0-shot,brier_score,1.3111515844555548,
Salesforce/codegen-6B-mono,xnli_ar,0-shot,brier_score,1.1548071078890445,
Salesforce/codegen-6B-mono,xnli_de,0-shot,brier_score,0.9638964115536972,
Salesforce/codegen-6B-mono,xnli_hi,0-shot,brier_score,0.7796514751173046,
Salesforce/codegen-6B-mono,xnli_es,0-shot,brier_score,1.0355571799263816,
Salesforce/codegen-6B-mono,xnli_bg,0-shot,brier_score,0.9432814063972744,
Salesforce/codegen-6B-mono,xnli_sw,0-shot,brier_score,1.1131222005854853,
Salesforce/codegen-6B-mono,xnli_el,0-shot,brier_score,0.8760007535828703,
Salesforce/codegen-6B-mono,xnli_th,0-shot,brier_score,0.936171807279324,
Salesforce/codegen-6B-mono,logiqa2,0-shot,brier_score,1.132872371789095,
Salesforce/codegen-6B-mono,mathqa,0-shot,brier_score,0.9827803171269596,
Salesforce/codegen-6B-mono,lambada_standard,0-shot,perplexity,87.13890878986679,3.482329775710741
Salesforce/codegen-6B-mono,lambada_standard,0-shot,accuracy,0.2342324859305259,0.0059004360242828
Salesforce/codegen-6B-mono,lambada_openai,0-shot,perplexity,67.5368425039748,2.888829135243724
Salesforce/codegen-6B-mono,lambada_openai,0-shot,accuracy,0.2798369881622355,0.0062543191321193
Salesforce/codegen-6B-mono,mmlu_world_religions,0-shot,accuracy,0.2923976608187134,0.0348864771345792
Salesforce/codegen-6B-mono,mmlu_formal_logic,0-shot,accuracy,0.2936507936507936,0.0407352432214712
Salesforce/codegen-6B-mono,mmlu_prehistory,0-shot,accuracy,0.2808641975308642,0.0250064697557992
Salesforce/codegen-6B-mono,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
Salesforce/codegen-6B-mono,mmlu_high_school_world_history,0-shot,accuracy,0.2573839662447257,0.0284588209914602
Salesforce/codegen-6B-mono,mmlu_moral_disputes,0-shot,accuracy,0.2572254335260115,0.0235329254310442
Salesforce/codegen-6B-mono,mmlu_professional_law,0-shot,accuracy,0.241851368970013,0.010936550813827
Salesforce/codegen-6B-mono,mmlu_logical_fallacies,0-shot,accuracy,0.2760736196319018,0.0351238528370505
Salesforce/codegen-6B-mono,mmlu_high_school_us_history,0-shot,accuracy,0.2401960784313725,0.0299837330559136
Salesforce/codegen-6B-mono,mmlu_philosophy,0-shot,accuracy,0.2282958199356913,0.0238393033113982
Salesforce/codegen-6B-mono,mmlu_jurisprudence,0-shot,accuracy,0.287037037037037,0.0437331304091476
Salesforce/codegen-6B-mono,mmlu_international_law,0-shot,accuracy,0.256198347107438,0.0398497965330287
Salesforce/codegen-6B-mono,mmlu_high_school_european_history,0-shot,accuracy,0.2363636363636363,0.0331750593000918
Salesforce/codegen-6B-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.238341968911917,0.0307489053639098
Salesforce/codegen-6B-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.3277310924369748,0.0304899114176732
Salesforce/codegen-6B-mono,mmlu_high_school_geography,0-shot,accuracy,0.2626262626262626,0.0313530500953308
Salesforce/codegen-6B-mono,mmlu_high_school_psychology,0-shot,accuracy,0.2330275229357798,0.0181256691808614
Salesforce/codegen-6B-mono,mmlu_public_relations,0-shot,accuracy,0.1909090909090909,0.0376442558598492
Salesforce/codegen-6B-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.33,0.047258156262526
Salesforce/codegen-6B-mono,mmlu_sociology,0-shot,accuracy,0.263681592039801,0.0311571508693555
Salesforce/codegen-6B-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2871794871794871,0.0229399254185306
Salesforce/codegen-6B-mono,mmlu_security_studies,0-shot,accuracy,0.2857142857142857,0.0289205832206755
Salesforce/codegen-6B-mono,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.0179179740695947
Salesforce/codegen-6B-mono,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
Salesforce/codegen-6B-mono,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
Salesforce/codegen-6B-mono,mmlu_miscellaneous,0-shot,accuracy,0.2681992337164751,0.0158424308352694
Salesforce/codegen-6B-mono,mmlu_marketing,0-shot,accuracy,0.2521367521367521,0.028447965476231
Salesforce/codegen-6B-mono,mmlu_management,0-shot,accuracy,0.1553398058252427,0.0358659473857397
Salesforce/codegen-6B-mono,mmlu_nutrition,0-shot,accuracy,0.2549019607843137,0.0249541843248799
Salesforce/codegen-6B-mono,mmlu_medical_genetics,0-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-6B-mono,mmlu_human_aging,0-shot,accuracy,0.2511210762331838,0.0291052208332246
Salesforce/codegen-6B-mono,mmlu_professional_medicine,0-shot,accuracy,0.3786764705882353,0.0294651336397761
Salesforce/codegen-6B-mono,mmlu_college_medicine,0-shot,accuracy,0.2023121387283237,0.0306311455391988
Salesforce/codegen-6B-mono,mmlu_business_ethics,0-shot,accuracy,0.35,0.0479372485441101
Salesforce/codegen-6B-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.2339622641509434,0.0260552969011529
Salesforce/codegen-6B-mono,mmlu_global_facts,0-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-6B-mono,mmlu_virology,0-shot,accuracy,0.2710843373493976,0.0346057990755302
Salesforce/codegen-6B-mono,mmlu_professional_accounting,0-shot,accuracy,0.2588652482269503,0.0261295725271808
Salesforce/codegen-6B-mono,mmlu_college_physics,0-shot,accuracy,0.1666666666666666,0.0370828466241654
Salesforce/codegen-6B-mono,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.0365860326276374
Salesforce/codegen-6B-mono,mmlu_high_school_biology,0-shot,accuracy,0.2096774193548387,0.0231578793490835
Salesforce/codegen-6B-mono,mmlu_college_biology,0-shot,accuracy,0.25,0.036210341218895
Salesforce/codegen-6B-mono,mmlu_anatomy,0-shot,accuracy,0.1925925925925926,0.0340654205850265
Salesforce/codegen-6B-mono,mmlu_college_chemistry,0-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-6B-mono,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-6B-mono,mmlu_college_computer_science,0-shot,accuracy,0.32,0.046882617226215
Salesforce/codegen-6B-mono,mmlu_astronomy,0-shot,accuracy,0.1973684210526315,0.0323898160169939
Salesforce/codegen-6B-mono,mmlu_college_mathematics,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-6B-mono,mmlu_conceptual_physics,0-shot,accuracy,0.3063829787234042,0.0301359064785175
Salesforce/codegen-6B-mono,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
Salesforce/codegen-6B-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-6B-mono,mmlu_machine_learning,0-shot,accuracy,0.2946428571428571,0.0432704093257873
Salesforce/codegen-6B-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.2758620689655172,0.0314471258167824
Salesforce/codegen-6B-mono,mmlu_high_school_statistics,0-shot,accuracy,0.375,0.0330169089872108
Salesforce/codegen-6B-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.0217659616721545
Salesforce/codegen-6B-mono,mmlu_electrical_engineering,0-shot,accuracy,0.2068965517241379,0.0337567244956055
Salesforce/codegen-6B-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.2777777777777778,0.0273091405882301
Salesforce/codegen-6B-mono,arc_challenge,25-shot,accuracy,0.2141638225255972,0.0119883832059665
Salesforce/codegen-6B-mono,arc_challenge,25-shot,acc_norm,0.2406143344709897,0.0124914685323905
Salesforce/codegen-6B-mono,hellaswag,10-shot,accuracy,0.311292571200956,0.0046207585796286
Salesforce/codegen-6B-mono,hellaswag,10-shot,acc_norm,0.358195578570006,0.0047849012485586
Salesforce/codegen-6B-mono,truthfulqa_mc2,0-shot,accuracy,0.4328537953113665,0.0154478365651011
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,bleu_max,17.25064603206814,0.6130969229491668
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,bleu_acc,0.3414932680538555,0.0166006886199508
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,bleu_diff,-1.7382338560986077,0.5783719458598973
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge1_max,39.52252821322913,0.8483702442864192
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge1_acc,0.3108935128518972,0.0162033166735596
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge1_diff,-3.784504356456098,0.8036994596352391
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge2_max,22.32239340912172,0.8946073218671986
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge2_acc,0.204406364749082,0.0141171743374326
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge2_diff,-3.2879573679240157,0.8115132417345173
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rougeL_max,36.318101595189205,0.8312620370594777
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rougeL_acc,0.2986536107711138,0.0160215706137685
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rougeL_diff,-3.747656033460749,0.7943779862877182
Salesforce/codegen-6B-mono,truthfulqa_mc1,0-shot,accuracy,0.2631578947368421,0.015415241740237
Salesforce/codegen-6B-mono,winogrande,5-shot,accuracy,0.5390686661404893,0.0140095216809803
jisukim8873/falcon-7B-case-4,arc:challenge,25-shot,accuracy,0.439419795221843,0.0145037478235801
jisukim8873/falcon-7B-case-4,arc:challenge,25-shot,acc_norm,0.4761092150170648,0.0145947017980716
jisukim8873/falcon-7B-case-4,hellaswag,10-shot,accuracy,0.5990838478390759,0.0048908247185303
jisukim8873/falcon-7B-case-4,hellaswag,10-shot,acc_norm,0.7893845847440749,0.0040691239053249
jisukim8873/falcon-7B-case-4,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-4,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-4,hendrycksTest-anatomy,5-shot,accuracy,0.2888888888888888,0.0391545063041425
jisukim8873/falcon-7B-case-4,hendrycksTest-anatomy,5-shot,acc_norm,0.2888888888888888,0.0391545063041425
jisukim8873/falcon-7B-case-4,hendrycksTest-astronomy,5-shot,accuracy,0.2368421052631578,0.0345977760681053
jisukim8873/falcon-7B-case-4,hendrycksTest-astronomy,5-shot,acc_norm,0.2368421052631578,0.0345977760681053
jisukim8873/falcon-7B-case-4,hendrycksTest-business_ethics,5-shot,accuracy,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-4,hendrycksTest-business_ethics,5-shot,acc_norm,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-4,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3056603773584905,0.0283532980733226
jisukim8873/falcon-7B-case-4,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3056603773584905,0.0283532980733226
jisukim8873/falcon-7B-case-4,hendrycksTest-college_biology,5-shot,accuracy,0.25,0.036210341218895
jisukim8873/falcon-7B-case-4,hendrycksTest-college_biology,5-shot,acc_norm,0.25,0.036210341218895
jisukim8873/falcon-7B-case-4,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-4,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-4,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-4,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-4,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.0440844002276807
jisukim8873/falcon-7B-case-4,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.0440844002276807
jisukim8873/falcon-7B-case-4,hendrycksTest-college_medicine,5-shot,accuracy,0.2716763005780346,0.0339175032232165
jisukim8873/falcon-7B-case-4,hendrycksTest-college_medicine,5-shot,acc_norm,0.2716763005780346,0.0339175032232165
jisukim8873/falcon-7B-case-4,hendrycksTest-college_physics,5-shot,accuracy,0.196078431372549,0.0395058186117996
jisukim8873/falcon-7B-case-4,hendrycksTest-college_physics,5-shot,acc_norm,0.196078431372549,0.0395058186117996
jisukim8873/falcon-7B-case-4,hendrycksTest-computer_security,5-shot,accuracy,0.39,0.0490207130000197
jisukim8873/falcon-7B-case-4,hendrycksTest-computer_security,5-shot,acc_norm,0.39,0.0490207130000197
jisukim8873/falcon-7B-case-4,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.0303635821972381
jisukim8873/falcon-7B-case-4,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.0303635821972381
jisukim8873/falcon-7B-case-4,hendrycksTest-econometrics,5-shot,accuracy,0.3070175438596491,0.0433913832257986
jisukim8873/falcon-7B-case-4,hendrycksTest-econometrics,5-shot,acc_norm,0.3070175438596491,0.0433913832257986
jisukim8873/falcon-7B-case-4,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
jisukim8873/falcon-7B-case-4,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
jisukim8873/falcon-7B-case-4,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2804232804232804,0.0231352879743256
jisukim8873/falcon-7B-case-4,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2804232804232804,0.0231352879743256
jisukim8873/falcon-7B-case-4,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.0371848900681811
jisukim8873/falcon-7B-case-4,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.0371848900681811
jisukim8873/falcon-7B-case-4,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-4,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_biology,5-shot,accuracy,0.3032258064516129,0.0261486859306717
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3032258064516129,0.0261486859306717
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3201970443349753,0.0328264938530415
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3201970443349753,0.0328264938530415
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.0476095228569523
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.0476095228569523
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2787878787878788,0.0350143870629678
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2787878787878788,0.0350143870629678
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_geography,5-shot,accuracy,0.2626262626262626,0.0313530500953308
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2626262626262626,0.0313530500953308
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.0325771407770966
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.0325771407770966
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2717948717948718,0.0225565510101323
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2717948717948718,0.0225565510101323
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.0288013921936312
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.0288013921936312
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.0368488152138902
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.0368488152138902
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3009174311926605,0.0196647513668021
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3009174311926605,0.0196647513668021
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1898148148148148,0.0267447148346919
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1898148148148148,0.0267447148346919
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2745098039215686,0.0313217980308329
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2745098039215686,0.0313217980308329
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-4,hendrycksTest-human_aging,5-shot,accuracy,0.4125560538116592,0.0330406217544929
jisukim8873/falcon-7B-case-4,hendrycksTest-human_aging,5-shot,acc_norm,0.4125560538116592,0.0330406217544929
jisukim8873/falcon-7B-case-4,hendrycksTest-human_sexuality,5-shot,accuracy,0.2519083969465648,0.0380738711630608
jisukim8873/falcon-7B-case-4,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2519083969465648,0.0380738711630608
jisukim8873/falcon-7B-case-4,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.0429434084521209
jisukim8873/falcon-7B-case-4,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.0429434084521209
jisukim8873/falcon-7B-case-4,hendrycksTest-jurisprudence,5-shot,accuracy,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-4,hendrycksTest-jurisprudence,5-shot,acc_norm,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-4,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2760736196319018,0.0351238528370505
jisukim8873/falcon-7B-case-4,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2760736196319018,0.0351238528370505
jisukim8873/falcon-7B-case-4,hendrycksTest-machine_learning,5-shot,accuracy,0.3482142857142857,0.0452182990283358
jisukim8873/falcon-7B-case-4,hendrycksTest-machine_learning,5-shot,acc_norm,0.3482142857142857,0.0452182990283358
jisukim8873/falcon-7B-case-4,hendrycksTest-management,5-shot,accuracy,0.3106796116504854,0.0458212416016155
jisukim8873/falcon-7B-case-4,hendrycksTest-management,5-shot,acc_norm,0.3106796116504854,0.0458212416016155
jisukim8873/falcon-7B-case-4,hendrycksTest-marketing,5-shot,accuracy,0.329059829059829,0.0307823215776881
jisukim8873/falcon-7B-case-4,hendrycksTest-marketing,5-shot,acc_norm,0.329059829059829,0.0307823215776881
jisukim8873/falcon-7B-case-4,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.0479372485441101
jisukim8873/falcon-7B-case-4,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.0479372485441101
jisukim8873/falcon-7B-case-4,hendrycksTest-miscellaneous,5-shot,accuracy,0.367816091954023,0.0172438288918462
jisukim8873/falcon-7B-case-4,hendrycksTest-miscellaneous,5-shot,acc_norm,0.367816091954023,0.0172438288918462
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_disputes,5-shot,accuracy,0.3323699421965317,0.0253611687496882
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3323699421965317,0.0253611687496882
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2625698324022346,0.0147168242730177
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2625698324022346,0.0147168242730177
jisukim8873/falcon-7B-case-4,hendrycksTest-nutrition,5-shot,accuracy,0.3366013071895425,0.0270579746244943
jisukim8873/falcon-7B-case-4,hendrycksTest-nutrition,5-shot,acc_norm,0.3366013071895425,0.0270579746244943
jisukim8873/falcon-7B-case-4,hendrycksTest-philosophy,5-shot,accuracy,0.315112540192926,0.0263852737034644
jisukim8873/falcon-7B-case-4,hendrycksTest-philosophy,5-shot,acc_norm,0.315112540192926,0.0263852737034644
jisukim8873/falcon-7B-case-4,hendrycksTest-prehistory,5-shot,accuracy,0.3055555555555556,0.0256308249756213
jisukim8873/falcon-7B-case-4,hendrycksTest-prehistory,5-shot,acc_norm,0.3055555555555556,0.0256308249756213
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_accounting,5-shot,accuracy,0.2588652482269503,0.0261295725271808
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2588652482269503,0.0261295725271808
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_law,5-shot,accuracy,0.2607561929595828,0.0112134715596023
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_law,5-shot,acc_norm,0.2607561929595828,0.0112134715596023
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.0237097882538117
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.0237097882538117
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2598039215686274,0.0177408995091777
jisukim8873/falcon-7B-case-4,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.0430911870994645
jisukim8873/falcon-7B-case-4,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.0430911870994645
jisukim8873/falcon-7B-case-4,hendrycksTest-security_studies,5-shot,accuracy,0.2040816326530612,0.0258012834750905
jisukim8873/falcon-7B-case-4,hendrycksTest-security_studies,5-shot,acc_norm,0.2040816326530612,0.0258012834750905
jisukim8873/falcon-7B-case-4,hendrycksTest-sociology,5-shot,accuracy,0.308457711442786,0.0326581958851269
jisukim8873/falcon-7B-case-4,hendrycksTest-sociology,5-shot,acc_norm,0.308457711442786,0.0326581958851269
jisukim8873/falcon-7B-case-4,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.43,0.0497569851956242
jisukim8873/falcon-7B-case-4,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.43,0.0497569851956242
jisukim8873/falcon-7B-case-4,hendrycksTest-virology,5-shot,accuracy,0.3433734939759036,0.036965843170106
jisukim8873/falcon-7B-case-4,hendrycksTest-virology,5-shot,acc_norm,0.3433734939759036,0.036965843170106
jisukim8873/falcon-7B-case-4,hendrycksTest-world_religions,5-shot,accuracy,0.3684210526315789,0.0369965801765687
jisukim8873/falcon-7B-case-4,hendrycksTest-world_religions,5-shot,acc_norm,0.3684210526315789,0.0369965801765687
jisukim8873/falcon-7B-case-4,truthfulqa:mc,0-shot,mc1,0.2570379436964504,0.015298077509485
jisukim8873/falcon-7B-case-4,truthfulqa:mc,0-shot,mc2,0.3778933788894488,0.0143981866732092
jisukim8873/falcon-7B-case-4,winogrande,5-shot,accuracy,0.7016574585635359,0.0128588850100304
jisukim8873/falcon-7B-case-4,gsm8k,5-shot,accuracy,0.0780894617134192,0.0073906544811082
jisukim8873/falcon-7B-case-4,minerva_math_precalc,5-shot,accuracy,0.0073260073260073,0.003652908089383
jisukim8873/falcon-7B-case-4,minerva_math_prealgebra,5-shot,accuracy,0.026406429391504,0.0054360577625739
jisukim8873/falcon-7B-case-4,minerva_math_num_theory,5-shot,accuracy,0.0129629629629629,0.0048721929845814
jisukim8873/falcon-7B-case-4,minerva_math_intermediate_algebra,5-shot,accuracy,0.0132890365448504,0.0038127511080199
jisukim8873/falcon-7B-case-4,minerva_math_geometry,5-shot,accuracy,0.0208768267223382,0.0065393857958139
jisukim8873/falcon-7B-case-4,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.0062753625139895
jisukim8873/falcon-7B-case-4,minerva_math_algebra,5-shot,accuracy,0.0185341196293176,0.0039163476763639
jisukim8873/falcon-7B-case-4,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-4,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-4,arithmetic_3da,5-shot,accuracy,0.202,0.0089798841395409
jisukim8873/falcon-7B-case-4,arithmetic_3ds,5-shot,accuracy,0.3245,0.0104716141234854
jisukim8873/falcon-7B-case-4,arithmetic_4da,5-shot,accuracy,0.0055,0.0016541593398342
jisukim8873/falcon-7B-case-4,arithmetic_2ds,5-shot,accuracy,0.4725,0.0111662087168635
jisukim8873/falcon-7B-case-4,arithmetic_5ds,5-shot,accuracy,0.035,0.0041104680966997
jisukim8873/falcon-7B-case-4,arithmetic_5da,5-shot,accuracy,0.0005,0.0005
jisukim8873/falcon-7B-case-4,arithmetic_1dc,5-shot,accuracy,0.091,0.0064327435900281
jisukim8873/falcon-7B-case-4,arithmetic_4ds,5-shot,accuracy,0.084,0.0062041313350712
jisukim8873/falcon-7B-case-4,arithmetic_2dm,5-shot,accuracy,0.2645,0.0098650156749564
jisukim8873/falcon-7B-case-4,arithmetic_2da,5-shot,accuracy,0.7235,0.0100036949151788
jisukim8873/falcon-7B-case-4,gsm8k_cot,5-shot,accuracy,0.0962850644427596,0.0081252641282158
jisukim8873/falcon-7B-case-4,anli_r2,0-shot,brier_score,0.941338151188042,
jisukim8873/falcon-7B-case-4,anli_r3,0-shot,brier_score,0.87237900881463,
jisukim8873/falcon-7B-case-4,anli_r1,0-shot,brier_score,0.9857934321426032,
jisukim8873/falcon-7B-case-4,xnli_eu,0-shot,brier_score,1.026246542896589,
jisukim8873/falcon-7B-case-4,xnli_vi,0-shot,brier_score,1.0279853059907973,
jisukim8873/falcon-7B-case-4,xnli_ru,0-shot,brier_score,0.8097107614311067,
jisukim8873/falcon-7B-case-4,xnli_zh,0-shot,brier_score,1.0156490749806204,
jisukim8873/falcon-7B-case-4,xnli_tr,0-shot,brier_score,0.9756836472864572,
jisukim8873/falcon-7B-case-4,xnli_fr,0-shot,brier_score,0.7603981109729621,
jisukim8873/falcon-7B-case-4,xnli_en,0-shot,brier_score,0.662787162600886,
jisukim8873/falcon-7B-case-4,xnli_ur,0-shot,brier_score,1.31954291668707,
jisukim8873/falcon-7B-case-4,xnli_ar,0-shot,brier_score,1.2990240082694595,
jisukim8873/falcon-7B-case-4,xnli_de,0-shot,brier_score,0.849940839182899,
jisukim8873/falcon-7B-case-4,xnli_hi,0-shot,brier_score,1.1637487081246372,
jisukim8873/falcon-7B-case-4,xnli_es,0-shot,brier_score,0.8194931547365195,
jisukim8873/falcon-7B-case-4,xnli_bg,0-shot,brier_score,0.9860919344218412,
jisukim8873/falcon-7B-case-4,xnli_sw,0-shot,brier_score,1.082079686536352,
jisukim8873/falcon-7B-case-4,xnli_el,0-shot,brier_score,0.8625720197689004,
jisukim8873/falcon-7B-case-4,xnli_th,0-shot,brier_score,0.9548472798777402,
jisukim8873/falcon-7B-case-4,logiqa2,0-shot,brier_score,1.05514711694456,
jisukim8873/falcon-7B-case-4,mathqa,0-shot,brier_score,0.9309727513874028,
jisukim8873/falcon-7B-case-4,lambada_standard,0-shot,perplexity,4.170478886814212,0.0917183231421987
jisukim8873/falcon-7B-case-4,lambada_standard,0-shot,accuracy,0.6625266834853484,0.0065876949385287
jisukim8873/falcon-7B-case-4,lambada_openai,0-shot,perplexity,3.3182500746294745,0.0688089665195753
jisukim8873/falcon-7B-case-4,lambada_openai,0-shot,accuracy,0.7343295167863381,0.0061535993728225
jisukim8873/falcon-7B-case-4,mmlu_world_religions,0-shot,accuracy,0.3625730994152046,0.0368713061556206
jisukim8873/falcon-7B-case-4,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.0371848900681811
jisukim8873/falcon-7B-case-4,mmlu_prehistory,0-shot,accuracy,0.3024691358024691,0.025557653981868
jisukim8873/falcon-7B-case-4,mmlu_moral_scenarios,0-shot,accuracy,0.2603351955307262,0.0146762520093194
jisukim8873/falcon-7B-case-4,mmlu_high_school_world_history,0-shot,accuracy,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-4,mmlu_moral_disputes,0-shot,accuracy,0.3294797687861271,0.0253052581318797
jisukim8873/falcon-7B-case-4,mmlu_professional_law,0-shot,accuracy,0.2633637548891786,0.0112495064036052
jisukim8873/falcon-7B-case-4,mmlu_logical_fallacies,0-shot,accuracy,0.2760736196319018,0.0351238528370505
jisukim8873/falcon-7B-case-4,mmlu_high_school_us_history,0-shot,accuracy,0.2794117647058823,0.0314932810450795
jisukim8873/falcon-7B-case-4,mmlu_philosophy,0-shot,accuracy,0.3086816720257235,0.0262369658811532
jisukim8873/falcon-7B-case-4,mmlu_jurisprudence,0-shot,accuracy,0.3518518518518518,0.0461663111180171
jisukim8873/falcon-7B-case-4,mmlu_international_law,0-shot,accuracy,0.3305785123966942,0.0429434084521209
jisukim8873/falcon-7B-case-4,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.0354656301962433
jisukim8873/falcon-7B-case-4,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2797927461139896,0.032396370467357
jisukim8873/falcon-7B-case-4,mmlu_high_school_microeconomics,0-shot,accuracy,0.2647058823529412,0.0286574912850719
jisukim8873/falcon-7B-case-4,mmlu_high_school_geography,0-shot,accuracy,0.2777777777777778,0.0319117822671354
jisukim8873/falcon-7B-case-4,mmlu_high_school_psychology,0-shot,accuracy,0.3064220183486238,0.0197655172204585
jisukim8873/falcon-7B-case-4,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.0426579211094058
jisukim8873/falcon-7B-case-4,mmlu_us_foreign_policy,0-shot,accuracy,0.42,0.0496044963748858
jisukim8873/falcon-7B-case-4,mmlu_sociology,0-shot,accuracy,0.3034825870646766,0.0325100681645861
jisukim8873/falcon-7B-case-4,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2743589743589744,0.0226227657674932
jisukim8873/falcon-7B-case-4,mmlu_security_studies,0-shot,accuracy,0.2081632653061224,0.0259911176728132
jisukim8873/falcon-7B-case-4,mmlu_professional_psychology,0-shot,accuracy,0.2581699346405229,0.01770453165325
jisukim8873/falcon-7B-case-4,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
jisukim8873/falcon-7B-case-4,mmlu_econometrics,0-shot,accuracy,0.3157894736842105,0.04372748290278
jisukim8873/falcon-7B-case-4,mmlu_miscellaneous,0-shot,accuracy,0.3652618135376756,0.0172185300288386
jisukim8873/falcon-7B-case-4,mmlu_marketing,0-shot,accuracy,0.3333333333333333,0.0308827369741386
jisukim8873/falcon-7B-case-4,mmlu_management,0-shot,accuracy,0.3398058252427184,0.0468976593727813
jisukim8873/falcon-7B-case-4,mmlu_nutrition,0-shot,accuracy,0.3300653594771242,0.0269256546536156
jisukim8873/falcon-7B-case-4,mmlu_medical_genetics,0-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-4,mmlu_human_aging,0-shot,accuracy,0.4170403587443946,0.0330926693607172
jisukim8873/falcon-7B-case-4,mmlu_professional_medicine,0-shot,accuracy,0.1948529411764706,0.0240605994234874
jisukim8873/falcon-7B-case-4,mmlu_college_medicine,0-shot,accuracy,0.2716763005780346,0.0339175032232165
jisukim8873/falcon-7B-case-4,mmlu_business_ethics,0-shot,accuracy,0.2,0.0402015126103684
jisukim8873/falcon-7B-case-4,mmlu_clinical_knowledge,0-shot,accuracy,0.3056603773584905,0.0283532980733226
jisukim8873/falcon-7B-case-4,mmlu_global_facts,0-shot,accuracy,0.37,0.0485236587093909
jisukim8873/falcon-7B-case-4,mmlu_virology,0-shot,accuracy,0.3313253012048193,0.0366431477728808
jisukim8873/falcon-7B-case-4,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539,0.0258921511567094
jisukim8873/falcon-7B-case-4,mmlu_college_physics,0-shot,accuracy,0.196078431372549,0.0395058186117996
jisukim8873/falcon-7B-case-4,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.0368488152138902
jisukim8873/falcon-7B-case-4,mmlu_high_school_biology,0-shot,accuracy,0.3096774193548387,0.0263027749835174
jisukim8873/falcon-7B-case-4,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
jisukim8873/falcon-7B-case-4,mmlu_anatomy,0-shot,accuracy,0.2962962962962963,0.0394462416250111
jisukim8873/falcon-7B-case-4,mmlu_college_chemistry,0-shot,accuracy,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-4,mmlu_computer_security,0-shot,accuracy,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-4,mmlu_college_computer_science,0-shot,accuracy,0.35,0.0479372485441101
jisukim8873/falcon-7B-case-4,mmlu_astronomy,0-shot,accuracy,0.2302631578947368,0.0342605942440316
jisukim8873/falcon-7B-case-4,mmlu_college_mathematics,0-shot,accuracy,0.27,0.0446196043338474
jisukim8873/falcon-7B-case-4,mmlu_conceptual_physics,0-shot,accuracy,0.3106382978723404,0.0302512375792131
jisukim8873/falcon-7B-case-4,mmlu_abstract_algebra,0-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-4,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-4,mmlu_machine_learning,0-shot,accuracy,0.3482142857142857,0.0452182990283358
jisukim8873/falcon-7B-case-4,mmlu_high_school_chemistry,0-shot,accuracy,0.3201970443349753,0.0328264938530415
jisukim8873/falcon-7B-case-4,mmlu_high_school_statistics,0-shot,accuracy,0.1851851851851851,0.0264919147273551
jisukim8873/falcon-7B-case-4,mmlu_elementary_mathematics,0-shot,accuracy,0.2777777777777778,0.0230681888482611
jisukim8873/falcon-7B-case-4,mmlu_electrical_engineering,0-shot,accuracy,0.2620689655172414,0.0366466633722525
jisukim8873/falcon-7B-case-4,mmlu_high_school_mathematics,0-shot,accuracy,0.2777777777777778,0.0273091405882301
jisukim8873/falcon-7B-case-4,arc_challenge,25-shot,accuracy,0.4453924914675768,0.014523987638344
jisukim8873/falcon-7B-case-4,arc_challenge,25-shot,acc_norm,0.4735494880546075,0.0145909313581201
jisukim8873/falcon-7B-case-4,truthfulqa_mc2,0-shot,accuracy,0.3783457816103618,0.0144071955188638
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,bleu_max,25.491727663339983,0.7876825292141532
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,bleu_acc,0.2974296205630355,0.0160026514873609
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,bleu_diff,-7.301689206799327,0.7522941543496632
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge1_max,51.09004512811695,0.8130980891432411
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge1_acc,0.3011015911872705,0.0160589990261005
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge1_diff,-9.608532182458044,0.7813130202579182
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge2_max,35.83529943497114,0.9345260348229252
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge2_acc,0.2692778457772338,0.0155285666370872
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge2_diff,-10.914652626830508,0.943625020821263
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rougeL_max,47.82154692891093,0.8290897847436749
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rougeL_acc,0.3023255813953488,0.016077509266133
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rougeL_diff,-9.662039631078054,0.7901057890971249
jisukim8873/falcon-7B-case-4,truthfulqa_mc1,0-shot,accuracy,0.2594859241126071,0.0153454094855579
frank098/orca_mini_3b_juniper,minerva_math_precalc,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_prealgebra,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_num_theory,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_geometry,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_algebra,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,fld_default,0-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,fld_star,0-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_3da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_3ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_4da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_2ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_5ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_5da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_1dc,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_4ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_2dm,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_2da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,gsm8k_cot,5-shot,accuracy,0.0121304018195602,0.0030152942428909
frank098/orca_mini_3b_juniper,gsm8k,5-shot,accuracy,0.0128885519332827,0.0031069012664996
frank098/orca_mini_3b_juniper,anli_r2,0-shot,brier_score,0.9441265812558454,
frank098/orca_mini_3b_juniper,anli_r3,0-shot,brier_score,0.9703161154980474,
frank098/orca_mini_3b_juniper,anli_r1,0-shot,brier_score,0.9194398384386012,
frank098/orca_mini_3b_juniper,xnli_eu,0-shot,brier_score,1.2999740747701036,
frank098/orca_mini_3b_juniper,xnli_vi,0-shot,brier_score,1.0362620270912626,
frank098/orca_mini_3b_juniper,xnli_ru,0-shot,brier_score,1.0761920507221865,
frank098/orca_mini_3b_juniper,xnli_zh,0-shot,brier_score,1.14055510107051,
frank098/orca_mini_3b_juniper,xnli_tr,0-shot,brier_score,1.0280785216228991,
frank098/orca_mini_3b_juniper,xnli_fr,0-shot,brier_score,1.0706704552738813,
frank098/orca_mini_3b_juniper,xnli_en,0-shot,brier_score,0.9206631257574506,
frank098/orca_mini_3b_juniper,xnli_ur,0-shot,brier_score,1.2254000998967638,
frank098/orca_mini_3b_juniper,xnli_ar,0-shot,brier_score,1.022517089906402,
frank098/orca_mini_3b_juniper,xnli_de,0-shot,brier_score,0.9989803721354296,
frank098/orca_mini_3b_juniper,xnli_hi,0-shot,brier_score,0.9769413427617684,
frank098/orca_mini_3b_juniper,xnli_es,0-shot,brier_score,1.1242290380210678,
frank098/orca_mini_3b_juniper,xnli_bg,0-shot,brier_score,1.068234109399256,
frank098/orca_mini_3b_juniper,xnli_sw,0-shot,brier_score,1.11414698513575,
frank098/orca_mini_3b_juniper,xnli_el,0-shot,brier_score,1.2572221142753026,
frank098/orca_mini_3b_juniper,xnli_th,0-shot,brier_score,0.8811067149691696,
frank098/orca_mini_3b_juniper,logiqa2,0-shot,brier_score,1.5569003832753865,
frank098/orca_mini_3b_juniper,mathqa,0-shot,brier_score,1.177340881036448,
frank098/orca_mini_3b_juniper,lambada_standard,0-shot,perplexity,4.406981980729325e+29,2.995589241082757e+29
frank098/orca_mini_3b_juniper,lambada_standard,0-shot,accuracy,0.0489035513293227,0.0030046545800346
frank098/orca_mini_3b_juniper,lambada_openai,0-shot,perplexity,161290686836746.56,87566827028421.67
frank098/orca_mini_3b_juniper,lambada_openai,0-shot,accuracy,0.1589365418202988,0.0050937583116287
frank098/orca_mini_3b_juniper,mmlu_world_religions,0-shot,accuracy,0.2573099415204678,0.0335279984416186
frank098/orca_mini_3b_juniper,mmlu_formal_logic,0-shot,accuracy,0.1904761904761904,0.0351220741230205
frank098/orca_mini_3b_juniper,mmlu_prehistory,0-shot,accuracy,0.2438271604938271,0.0238918795419596
frank098/orca_mini_3b_juniper,mmlu_moral_scenarios,0-shot,accuracy,0.2502793296089385,0.0144875008528504
frank098/orca_mini_3b_juniper,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.029041333510598
frank098/orca_mini_3b_juniper,mmlu_moral_disputes,0-shot,accuracy,0.2890173410404624,0.0244051739357832
frank098/orca_mini_3b_juniper,mmlu_professional_law,0-shot,accuracy,0.2444589308996088,0.0109764250131138
frank098/orca_mini_3b_juniper,mmlu_logical_fallacies,0-shot,accuracy,0.2085889570552147,0.0319219344893472
frank098/orca_mini_3b_juniper,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.0305875913516042
frank098/orca_mini_3b_juniper,mmlu_philosophy,0-shot,accuracy,0.3054662379421222,0.0261605844501404
frank098/orca_mini_3b_juniper,mmlu_jurisprudence,0-shot,accuracy,0.2407407407407407,0.0413311944024383
frank098/orca_mini_3b_juniper,mmlu_international_law,0-shot,accuracy,0.3553719008264462,0.0436923632657398
frank098/orca_mini_3b_juniper,mmlu_high_school_european_history,0-shot,accuracy,0.2666666666666666,0.0345313180188541
frank098/orca_mini_3b_juniper,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2642487046632124,0.0318215505091664
frank098/orca_mini_3b_juniper,mmlu_high_school_microeconomics,0-shot,accuracy,0.2100840336134453,0.0264613987174718
frank098/orca_mini_3b_juniper,mmlu_high_school_geography,0-shot,accuracy,0.2424242424242424,0.030532892233932
frank098/orca_mini_3b_juniper,mmlu_high_school_psychology,0-shot,accuracy,0.2568807339449541,0.0187324929283424
frank098/orca_mini_3b_juniper,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.0417234303870538
frank098/orca_mini_3b_juniper,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
frank098/orca_mini_3b_juniper,mmlu_sociology,0-shot,accuracy,0.2587064676616915,0.030965903123573
frank098/orca_mini_3b_juniper,mmlu_high_school_macroeconomics,0-shot,accuracy,0.258974358974359,0.0222111068100616
frank098/orca_mini_3b_juniper,mmlu_security_studies,0-shot,accuracy,0.2612244897959184,0.0281234293351427
frank098/orca_mini_3b_juniper,mmlu_professional_psychology,0-shot,accuracy,0.2745098039215686,0.0180540274588151
frank098/orca_mini_3b_juniper,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.0372767357559691
frank098/orca_mini_3b_juniper,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
frank098/orca_mini_3b_juniper,mmlu_miscellaneous,0-shot,accuracy,0.2924648786717752,0.0162670006845986
frank098/orca_mini_3b_juniper,mmlu_marketing,0-shot,accuracy,0.3076923076923077,0.030236389942173
frank098/orca_mini_3b_juniper,mmlu_management,0-shot,accuracy,0.2815533980582524,0.0445325483632646
frank098/orca_mini_3b_juniper,mmlu_nutrition,0-shot,accuracy,0.2450980392156862,0.0246300489798247
frank098/orca_mini_3b_juniper,mmlu_medical_genetics,0-shot,accuracy,0.19,0.0394277244403662
frank098/orca_mini_3b_juniper,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.0312658052251371
frank098/orca_mini_3b_juniper,mmlu_professional_medicine,0-shot,accuracy,0.1875,0.0237097882538117
frank098/orca_mini_3b_juniper,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
frank098/orca_mini_3b_juniper,mmlu_business_ethics,0-shot,accuracy,0.26,0.0440844002276807
frank098/orca_mini_3b_juniper,mmlu_clinical_knowledge,0-shot,accuracy,0.2566037735849056,0.0268806478890519
frank098/orca_mini_3b_juniper,mmlu_global_facts,0-shot,accuracy,0.28,0.0451260859854212
frank098/orca_mini_3b_juniper,mmlu_virology,0-shot,accuracy,0.2710843373493976,0.0346057990755302
frank098/orca_mini_3b_juniper,mmlu_professional_accounting,0-shot,accuracy,0.2446808510638297,0.0256455536222667
frank098/orca_mini_3b_juniper,mmlu_college_physics,0-shot,accuracy,0.1568627450980392,0.0361866481993624
frank098/orca_mini_3b_juniper,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.0371018572611999
frank098/orca_mini_3b_juniper,mmlu_high_school_biology,0-shot,accuracy,0.2290322580645161,0.0239049143117826
frank098/orca_mini_3b_juniper,mmlu_college_biology,0-shot,accuracy,0.2847222222222222,0.0377380999068693
frank098/orca_mini_3b_juniper,mmlu_anatomy,0-shot,accuracy,0.3259259259259259,0.040491220417025
frank098/orca_mini_3b_juniper,mmlu_college_chemistry,0-shot,accuracy,0.18,0.0386122919665369
frank098/orca_mini_3b_juniper,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276807
frank098/orca_mini_3b_juniper,mmlu_college_computer_science,0-shot,accuracy,0.3,0.0460566186471838
frank098/orca_mini_3b_juniper,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.0358349617636106
frank098/orca_mini_3b_juniper,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276807
frank098/orca_mini_3b_juniper,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.0295131966255393
frank098/orca_mini_3b_juniper,mmlu_abstract_algebra,0-shot,accuracy,0.33,0.047258156262526
frank098/orca_mini_3b_juniper,mmlu_high_school_computer_science,0-shot,accuracy,0.32,0.046882617226215
frank098/orca_mini_3b_juniper,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.0428785875134045
frank098/orca_mini_3b_juniper,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.0305165307326944
frank098/orca_mini_3b_juniper,mmlu_high_school_statistics,0-shot,accuracy,0.2129629629629629,0.0279209631479936
frank098/orca_mini_3b_juniper,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.0217659616721545
frank098/orca_mini_3b_juniper,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.0375283395800333
frank098/orca_mini_3b_juniper,mmlu_high_school_mathematics,0-shot,accuracy,0.2666666666666666,0.0269624243250738
frank098/orca_mini_3b_juniper,arc_challenge,25-shot,accuracy,0.2081911262798634,0.011864866118448
frank098/orca_mini_3b_juniper,arc_challenge,25-shot,acc_norm,0.2721843003412969,0.0130066004064237
frank098/orca_mini_3b_juniper,hellaswag,10-shot,accuracy,0.2951603266281617,0.004551826272978
frank098/orca_mini_3b_juniper,hellaswag,10-shot,acc_norm,0.3249352718581956,0.0046739348371504
frank098/orca_mini_3b_juniper,truthfulqa_mc2,0-shot,accuracy,,
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,bleu_max,2.121881599584391,0.0190920515249663
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,bleu_acc,0.2864137086903305,0.0158261424395023
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,bleu_diff,-0.0112174653443582,0.0149454936408175
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge1_max,9.582083815386978,0.2347410115210295
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge1_acc,0.3659730722154223,0.0168629416840884
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge1_diff,-0.333425109597994,0.2422336819399393
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge2_max,0.1546129832975945,0.0354116675900053
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge2_acc,0.01468788249694,0.0042113508796163
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge2_diff,-0.0632375992776247,0.0474843344079796
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rougeL_max,9.028388478244056,0.2132891822974668
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rougeL_acc,0.3647490820073439,0.0168509610617201
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rougeL_diff,-0.1809295365131247,0.2223460836971422
frank098/orca_mini_3b_juniper,truthfulqa_mc1,0-shot,accuracy,0.2399020807833537,0.0149488126790621
frank098/orca_mini_3b_juniper,winogrande,5-shot,accuracy,0.5406471981057617,0.0140059738238251
llama2_220M_nl_only_shuf-hf,gsm8k,5-shot,accuracy,0.0,0.0
llama2_220M_nl_only_shuf-hf,minerva_math_precalc,5-shot,accuracy,0.0055,
llama2_220M_nl_only_shuf-hf,minerva_math,5-shot,accuracy,0.0026,
llama2_220M_nl_only_shuf-hf,gsm8k_cot,5-shot,accuracy,0.0136,
llama2_220M_nl_only_shuf-hf,mathqa,0-shot,accuracy,0.2208,0.0076
AbacusResearch/jaLLAbi,minerva_math_precalc,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_prealgebra,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_num_theory,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_geometry,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_algebra,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,fld_default,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,fld_star,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_3da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_3ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_4da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_2ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_5ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_5da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_1dc,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_4ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_2dm,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_2da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,gsm8k_cot,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,gsm8k,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,anli_r2,0-shot,brier_score,,
AbacusResearch/jaLLAbi,anli_r3,0-shot,brier_score,,
AbacusResearch/jaLLAbi,anli_r1,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_eu,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_vi,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_ru,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_zh,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_tr,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_fr,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_en,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_ur,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_ar,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_de,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_hi,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_es,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_bg,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_sw,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_el,0-shot,brier_score,,
AbacusResearch/jaLLAbi,xnli_th,0-shot,brier_score,,
AbacusResearch/jaLLAbi,logiqa2,0-shot,brier_score,,
AbacusResearch/jaLLAbi,mathqa,0-shot,brier_score,,
AbacusResearch/jaLLAbi,lambada_standard,0-shot,perplexity,,
AbacusResearch/jaLLAbi,lambada_standard,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,lambada_openai,0-shot,perplexity,,
AbacusResearch/jaLLAbi,lambada_openai,0-shot,accuracy,0.0,
kevin009/flyingllama-v2,arc:challenge,25-shot,accuracy,0.2158703071672355,0.0120229753600306
kevin009/flyingllama-v2,arc:challenge,25-shot,acc_norm,0.2474402730375426,0.0126103526632926
kevin009/flyingllama-v2,hellaswag,10-shot,accuracy,0.327325234017128,0.0046827807905083
kevin009/flyingllama-v2,hellaswag,10-shot,acc_norm,0.3843855805616411,0.0048545552940175
kevin009/flyingllama-v2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.19,0.0394277244403662
kevin009/flyingllama-v2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.19,0.0394277244403662
kevin009/flyingllama-v2,hendrycksTest-anatomy,5-shot,accuracy,0.2666666666666666,0.038201699145179
kevin009/flyingllama-v2,hendrycksTest-anatomy,5-shot,acc_norm,0.2666666666666666,0.038201699145179
kevin009/flyingllama-v2,hendrycksTest-astronomy,5-shot,accuracy,0.1973684210526315,0.0323898160169939
kevin009/flyingllama-v2,hendrycksTest-astronomy,5-shot,acc_norm,0.1973684210526315,0.0323898160169939
kevin009/flyingllama-v2,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.0416333199893226
kevin009/flyingllama-v2,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.0416333199893226
kevin009/flyingllama-v2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2415094339622641,0.0263414803711183
kevin009/flyingllama-v2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2415094339622641,0.0263414803711183
kevin009/flyingllama-v2,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.0365394696944209
kevin009/flyingllama-v2,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.0365394696944209
kevin009/flyingllama-v2,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
kevin009/flyingllama-v2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
kevin009/flyingllama-v2,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441101
kevin009/flyingllama-v2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441101
kevin009/flyingllama-v2,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
kevin009/flyingllama-v2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
kevin009/flyingllama-v2,hendrycksTest-college_medicine,5-shot,accuracy,0.2369942196531791,0.0324241475748309
kevin009/flyingllama-v2,hendrycksTest-college_medicine,5-shot,acc_norm,0.2369942196531791,0.0324241475748309
kevin009/flyingllama-v2,hendrycksTest-college_physics,5-shot,accuracy,0.3137254901960784,0.0461703482700671
kevin009/flyingllama-v2,hendrycksTest-college_physics,5-shot,acc_norm,0.3137254901960784,0.0461703482700671
kevin009/flyingllama-v2,hendrycksTest-computer_security,5-shot,accuracy,0.22,0.0416333199893227
kevin009/flyingllama-v2,hendrycksTest-computer_security,5-shot,acc_norm,0.22,0.0416333199893227
kevin009/flyingllama-v2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.1957446808510638,0.0259378531399771
kevin009/flyingllama-v2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.1957446808510638,0.0259378531399771
kevin009/flyingllama-v2,hendrycksTest-econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
kevin009/flyingllama-v2,hendrycksTest-econometrics,5-shot,acc_norm,0.2368421052631578,0.0399942387928133
kevin009/flyingllama-v2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
kevin009/flyingllama-v2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
kevin009/flyingllama-v2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2486772486772486,0.0222618176924001
kevin009/flyingllama-v2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2486772486772486,0.0222618176924001
kevin009/flyingllama-v2,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.0380952380952381
kevin009/flyingllama-v2,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.0380952380952381
kevin009/flyingllama-v2,hendrycksTest-global_facts,5-shot,accuracy,0.21,0.0409360180740332
kevin009/flyingllama-v2,hendrycksTest-global_facts,5-shot,acc_norm,0.21,0.0409360180740332
kevin009/flyingllama-v2,hendrycksTest-high_school_biology,5-shot,accuracy,0.2903225806451613,0.0258221061194158
kevin009/flyingllama-v2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2903225806451613,0.0258221061194158
kevin009/flyingllama-v2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2758620689655172,0.0314471258167824
kevin009/flyingllama-v2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2758620689655172,0.0314471258167824
kevin009/flyingllama-v2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.3,0.0460566186471838
kevin009/flyingllama-v2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.3,0.0460566186471838
kevin009/flyingllama-v2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2303030303030303,0.0328766675860348
kevin009/flyingllama-v2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2303030303030303,0.0328766675860348
kevin009/flyingllama-v2,hendrycksTest-high_school_geography,5-shot,accuracy,0.3484848484848485,0.033948539651564
kevin009/flyingllama-v2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3484848484848485,0.033948539651564
kevin009/flyingllama-v2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3367875647668393,0.0341078025183618
kevin009/flyingllama-v2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3367875647668393,0.0341078025183618
kevin009/flyingllama-v2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3358974358974358,0.0239467247415639
kevin009/flyingllama-v2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3358974358974358,0.0239467247415639
kevin009/flyingllama-v2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2703703703703703,0.0270803728151456
kevin009/flyingllama-v2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2703703703703703,0.0270803728151456
kevin009/flyingllama-v2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.0268415143229589
kevin009/flyingllama-v2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2184873949579832,0.0268415143229589
kevin009/flyingllama-v2,hendrycksTest-high_school_physics,5-shot,accuracy,0.3311258278145695,0.0384258171865986
kevin009/flyingllama-v2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3311258278145695,0.0384258171865986
kevin009/flyingllama-v2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2972477064220183,0.0195957072246435
kevin009/flyingllama-v2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2972477064220183,0.0195957072246435
kevin009/flyingllama-v2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
kevin009/flyingllama-v2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
kevin009/flyingllama-v2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.230392156862745,0.029554292605695
kevin009/flyingllama-v2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.230392156862745,0.029554292605695
kevin009/flyingllama-v2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2489451476793249,0.0281469705994226
kevin009/flyingllama-v2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2489451476793249,0.0281469705994226
kevin009/flyingllama-v2,hendrycksTest-human_aging,5-shot,accuracy,0.1659192825112107,0.0249675531965471
kevin009/flyingllama-v2,hendrycksTest-human_aging,5-shot,acc_norm,0.1659192825112107,0.0249675531965471
kevin009/flyingllama-v2,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
kevin009/flyingllama-v2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
kevin009/flyingllama-v2,hendrycksTest-international_law,5-shot,accuracy,0.3223140495867768,0.0426641636335216
kevin009/flyingllama-v2,hendrycksTest-international_law,5-shot,acc_norm,0.3223140495867768,0.0426641636335216
kevin009/flyingllama-v2,hendrycksTest-jurisprudence,5-shot,accuracy,0.2685185185185185,0.0428446796805219
kevin009/flyingllama-v2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2685185185185185,0.0428446796805219
kevin009/flyingllama-v2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
kevin009/flyingllama-v2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.263803680981595,0.0346241993161562
kevin009/flyingllama-v2,hendrycksTest-machine_learning,5-shot,accuracy,0.1875,0.0370468111477387
kevin009/flyingllama-v2,hendrycksTest-machine_learning,5-shot,acc_norm,0.1875,0.0370468111477387
kevin009/flyingllama-v2,hendrycksTest-management,5-shot,accuracy,0.174757281553398,0.0376017800602662
kevin009/flyingllama-v2,hendrycksTest-management,5-shot,acc_norm,0.174757281553398,0.0376017800602662
kevin009/flyingllama-v2,hendrycksTest-marketing,5-shot,accuracy,0.2222222222222222,0.0272360139461966
kevin009/flyingllama-v2,hendrycksTest-marketing,5-shot,acc_norm,0.2222222222222222,0.0272360139461966
kevin009/flyingllama-v2,hendrycksTest-medical_genetics,5-shot,accuracy,0.22,0.0416333199893226
kevin009/flyingllama-v2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.22,0.0416333199893226
kevin009/flyingllama-v2,hendrycksTest-miscellaneous,5-shot,accuracy,0.2656449553001277,0.0157943024878887
kevin009/flyingllama-v2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2656449553001277,0.0157943024878887
kevin009/flyingllama-v2,hendrycksTest-moral_disputes,5-shot,accuracy,0.2312138728323699,0.0226986571678557
kevin009/flyingllama-v2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2312138728323699,0.0226986571678557
kevin009/flyingllama-v2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
kevin009/flyingllama-v2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2469273743016759,0.0144222922048088
kevin009/flyingllama-v2,hendrycksTest-nutrition,5-shot,accuracy,0.2777777777777778,0.0256468630971379
kevin009/flyingllama-v2,hendrycksTest-nutrition,5-shot,acc_norm,0.2777777777777778,0.0256468630971379
kevin009/flyingllama-v2,hendrycksTest-philosophy,5-shot,accuracy,0.1961414790996784,0.022552447780478
kevin009/flyingllama-v2,hendrycksTest-philosophy,5-shot,acc_norm,0.1961414790996784,0.022552447780478
kevin009/flyingllama-v2,hendrycksTest-prehistory,5-shot,accuracy,0.2222222222222222,0.0231323762345433
kevin009/flyingllama-v2,hendrycksTest-prehistory,5-shot,acc_norm,0.2222222222222222,0.0231323762345433
kevin009/flyingllama-v2,hendrycksTest-professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
kevin009/flyingllama-v2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2482269503546099,0.0257700156442903
kevin009/flyingllama-v2,hendrycksTest-professional_law,5-shot,accuracy,0.272490221642764,0.0113716582943115
kevin009/flyingllama-v2,hendrycksTest-professional_law,5-shot,acc_norm,0.272490221642764,0.0113716582943115
kevin009/flyingllama-v2,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.0302114796091215
kevin009/flyingllama-v2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.0302114796091215
kevin009/flyingllama-v2,hendrycksTest-professional_psychology,5-shot,accuracy,0.230392156862745,0.017035229258034
kevin009/flyingllama-v2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.230392156862745,0.017035229258034
kevin009/flyingllama-v2,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
kevin009/flyingllama-v2,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.0417234303870538
kevin009/flyingllama-v2,hendrycksTest-security_studies,5-shot,accuracy,0.3265306122448979,0.0300210562384403
kevin009/flyingllama-v2,hendrycksTest-security_studies,5-shot,acc_norm,0.3265306122448979,0.0300210562384403
kevin009/flyingllama-v2,hendrycksTest-sociology,5-shot,accuracy,0.2786069651741293,0.031700561834973
kevin009/flyingllama-v2,hendrycksTest-sociology,5-shot,acc_norm,0.2786069651741293,0.031700561834973
kevin009/flyingllama-v2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.0456048021572068
kevin009/flyingllama-v2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.0456048021572068
kevin009/flyingllama-v2,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.0362933532994786
kevin009/flyingllama-v2,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.0362933532994786
kevin009/flyingllama-v2,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.0344629621708842
kevin009/flyingllama-v2,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.0344629621708842
kevin009/flyingllama-v2,truthfulqa:mc,0-shot,mc1,0.2435740514075887,0.0150263548249107
kevin009/flyingllama-v2,truthfulqa:mc,0-shot,mc2,0.4129929701796201,0.0149389059454407
kevin009/flyingllama-v2,winogrande,5-shot,accuracy,0.5027624309392266,0.0140522712116164
kevin009/flyingllama-v2,gsm8k,5-shot,accuracy,0.0,
kevin009/flyingllama-v2,anli_r2,0-shot,brier_score,1.1735330591237014,
kevin009/flyingllama-v2,anli_r3,0-shot,brier_score,1.1385554531177986,
kevin009/flyingllama-v2,anli_r1,0-shot,brier_score,1.19478847252206,
kevin009/flyingllama-v2,xnli_eu,0-shot,brier_score,1.2452653292406854,
kevin009/flyingllama-v2,xnli_vi,0-shot,brier_score,1.3055660713550172,
kevin009/flyingllama-v2,xnli_ru,0-shot,brier_score,0.8653403177730274,
kevin009/flyingllama-v2,xnli_zh,0-shot,brier_score,1.2545233907564624,
kevin009/flyingllama-v2,xnli_tr,0-shot,brier_score,0.9473726872904684,
kevin009/flyingllama-v2,xnli_fr,0-shot,brier_score,0.8657109098182422,
kevin009/flyingllama-v2,xnli_en,0-shot,brier_score,0.7220869891227981,
kevin009/flyingllama-v2,xnli_ur,0-shot,brier_score,1.2730658722542465,
kevin009/flyingllama-v2,xnli_ar,0-shot,brier_score,1.0108144142280742,
kevin009/flyingllama-v2,xnli_de,0-shot,brier_score,0.8540767754991446,
kevin009/flyingllama-v2,xnli_hi,0-shot,brier_score,1.012100384865812,
kevin009/flyingllama-v2,xnli_es,0-shot,brier_score,0.9670230383724598,
kevin009/flyingllama-v2,xnli_bg,0-shot,brier_score,0.9439847791011456,
kevin009/flyingllama-v2,xnli_sw,0-shot,brier_score,1.003416980540908,
kevin009/flyingllama-v2,xnli_el,0-shot,brier_score,1.107850707104091,
kevin009/flyingllama-v2,xnli_th,0-shot,brier_score,1.1658594816739027,
kevin009/flyingllama-v2,logiqa2,0-shot,brier_score,1.1554316673601366,
kevin009/flyingllama-v2,mathqa,0-shot,brier_score,1.0239110371606883,
kevin009/flyingllama-v2,lambada_standard,0-shot,perplexity,62.83149180042299,2.838643274776153
kevin009/flyingllama-v2,lambada_standard,0-shot,accuracy,0.3114690471569959,0.0064518053202612
kevin009/flyingllama-v2,lambada_openai,0-shot,perplexity,19.64791044748978,0.7536882287369607
kevin009/flyingllama-v2,lambada_openai,0-shot,accuracy,0.4257713953037065,0.0068887864909364
EleutherAI/pythia-70m-deduped,drop,3-shot,accuracy,0.0012583892617449,0.0003630560893119
EleutherAI/pythia-70m-deduped,drop,3-shot,f1,0.0230002097315436,0.0009427318515971
EleutherAI/pythia-70m-deduped,gsm8k,5-shot,accuracy,0.0090978013646702,0.0026153265107756
EleutherAI/pythia-70m-deduped,winogrande,5-shot,accuracy,0.4925019731649566,0.0140509055212285
EleutherAI/pythia-70m-deduped,arc:challenge,25-shot,accuracy,0.1834470989761092,0.0113101701795545
EleutherAI/pythia-70m-deduped,arc:challenge,25-shot,acc_norm,0.2107508532423208,0.0119182717548521
EleutherAI/pythia-70m-deduped,hellaswag,10-shot,accuracy,0.2685719976100378,0.0044231093132989
EleutherAI/pythia-70m-deduped,hellaswag,10-shot,acc_norm,0.2750448117904799,0.0044562426019505
EleutherAI/pythia-70m-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-70m-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-70m-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.3555555555555555,0.0413517674972038
EleutherAI/pythia-70m-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.3555555555555555,0.0413517674972038
EleutherAI/pythia-70m-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.1842105263157894,0.0315469804508223
EleutherAI/pythia-70m-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.1842105263157894,0.0315469804508223
EleutherAI/pythia-70m-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-70m-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/pythia-70m-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2339622641509434,0.0260552969011529
EleutherAI/pythia-70m-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2339622641509434,0.0260552969011529
EleutherAI/pythia-70m-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2291666666666666,0.0351469746786238
EleutherAI/pythia-70m-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2291666666666666,0.0351469746786238
EleutherAI/pythia-70m-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-70m-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-70m-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-70m-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-70m-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-70m-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/pythia-70m-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2023121387283237,0.0306311455391988
EleutherAI/pythia-70m-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2023121387283237,0.0306311455391988
EleutherAI/pythia-70m-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
EleutherAI/pythia-70m-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.2254901960784313,0.0415830753308328
EleutherAI/pythia-70m-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-70m-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-70m-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2893617021276595,0.0296440065770096
EleutherAI/pythia-70m-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2893617021276595,0.0296440065770096
EleutherAI/pythia-70m-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984367
EleutherAI/pythia-70m-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.0409698513984367
EleutherAI/pythia-70m-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2068965517241379,0.0337567244956055
EleutherAI/pythia-70m-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2068965517241379,0.0337567244956055
EleutherAI/pythia-70m-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2539682539682539,0.0224180428911139
EleutherAI/pythia-70m-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2539682539682539,0.0224180428911139
EleutherAI/pythia-70m-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.2063492063492063,0.0361960452412425
EleutherAI/pythia-70m-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.2063492063492063,0.0361960452412425
EleutherAI/pythia-70m-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-70m-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.3258064516129032,0.0266620105785671
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3258064516129032,0.0266620105785671
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2807881773399014,0.0316185633535861
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2807881773399014,0.0316185633535861
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2424242424242424,0.0334640988105595
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2424242424242424,0.0334640988105595
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.0296202278747904
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.0296202278747904
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3523316062176165,0.0344747828641435
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3523316062176165,0.0344747828641435
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2846153846153846,0.0228783227997062
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2846153846153846,0.0228783227997062
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2310924369747899,0.0273814069278689
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2310924369747899,0.0273814069278689
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.0329798664847383
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.0329798664847383
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2403669724770642,0.018320607320964
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2403669724770642,0.018320607320964
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4583333333333333,0.0339811089029463
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4583333333333333,0.0339811089029463
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2598039215686274,0.0307785546786932
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2827004219409282,0.0293128141539559
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2827004219409282,0.0293128141539559
EleutherAI/pythia-70m-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.2556053811659193,0.0292758910039699
EleutherAI/pythia-70m-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.2556053811659193,0.0292758910039699
EleutherAI/pythia-70m-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2900763358778626,0.0398006624646776
EleutherAI/pythia-70m-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2900763358778626,0.0398006624646776
EleutherAI/pythia-70m-deduped,hendrycksTest-international_law,5-shot,accuracy,0.1983471074380165,0.0364011827199094
EleutherAI/pythia-70m-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.1983471074380165,0.0364011827199094
EleutherAI/pythia-70m-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2037037037037037,0.0389354251882484
EleutherAI/pythia-70m-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2037037037037037,0.0389354251882484
EleutherAI/pythia-70m-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2515337423312883,0.0340899788685752
EleutherAI/pythia-70m-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2515337423312883,0.0340899788685752
EleutherAI/pythia-70m-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.1785714285714285,0.036352091215778
EleutherAI/pythia-70m-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.1785714285714285,0.036352091215778
EleutherAI/pythia-70m-deduped,hendrycksTest-management,5-shot,accuracy,0.1844660194174757,0.0384042362728827
EleutherAI/pythia-70m-deduped,hendrycksTest-management,5-shot,acc_norm,0.1844660194174757,0.0384042362728827
EleutherAI/pythia-70m-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2008547008547008,0.0262467729468904
EleutherAI/pythia-70m-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2008547008547008,0.0262467729468904
EleutherAI/pythia-70m-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.34,0.0476095228569523
EleutherAI/pythia-70m-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.34,0.0476095228569523
EleutherAI/pythia-70m-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2375478927203065,0.0152187330461501
EleutherAI/pythia-70m-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2375478927203065,0.0152187330461501
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2658959537572254,0.0237862032555082
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2658959537572254,0.0237862032555082
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2670391061452514,0.0147965026225625
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2670391061452514,0.0147965026225625
EleutherAI/pythia-70m-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.0252616912197294
EleutherAI/pythia-70m-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.0252616912197294
EleutherAI/pythia-70m-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.1961414790996784,0.022552447780478
EleutherAI/pythia-70m-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.1961414790996784,0.022552447780478
EleutherAI/pythia-70m-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2345679012345679,0.0235768817440057
EleutherAI/pythia-70m-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2345679012345679,0.0235768817440057
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2588652482269503,0.0261295725271808
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2588652482269503,0.0261295725271808
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2392438070404172,0.0108961236526766
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2392438070404172,0.0108961236526766
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.4154411764705882,0.0299353427078777
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4154411764705882,0.0299353427078777
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2483660130718954,0.0174794870013647
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2483660130718954,0.0174794870013647
EleutherAI/pythia-70m-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-70m-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-70m-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.2285714285714285,0.0268821449223077
EleutherAI/pythia-70m-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.2285714285714285,0.0268821449223077
EleutherAI/pythia-70m-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2487562189054726,0.0305676759389167
EleutherAI/pythia-70m-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2487562189054726,0.0305676759389167
EleutherAI/pythia-70m-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/pythia-70m-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.32,0.046882617226215
EleutherAI/pythia-70m-deduped,hendrycksTest-virology,5-shot,accuracy,0.1927710843373494,0.0307098240505652
EleutherAI/pythia-70m-deduped,hendrycksTest-virology,5-shot,acc_norm,0.1927710843373494,0.0307098240505652
EleutherAI/pythia-70m-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.3099415204678362,0.0354697695939316
EleutherAI/pythia-70m-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.3099415204678362,0.0354697695939316
EleutherAI/pythia-70m-deduped,truthfulqa:mc,0-shot,mc1,0.2521419828641371,0.0152015222462999
EleutherAI/pythia-70m-deduped,truthfulqa:mc,0-shot,mc2,0.4751438547560591,0.0157221331521173
EleutherAI/pythia-70m-deduped,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.0358252944257312
EleutherAI/pythia-70m-deduped,mmlu_formal_logic,0-shot,accuracy,0.2142857142857142,0.0367006645104718
EleutherAI/pythia-70m-deduped,mmlu_prehistory,0-shot,accuracy,0.2530864197530864,0.024191808600713
EleutherAI/pythia-70m-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.2581005586592179,0.0146351856165278
EleutherAI/pythia-70m-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.2953586497890295,0.0296963387134228
EleutherAI/pythia-70m-deduped,mmlu_moral_disputes,0-shot,accuracy,0.2601156069364161,0.0236186783100693
EleutherAI/pythia-70m-deduped,mmlu_professional_law,0-shot,accuracy,0.2366362451108213,0.0108551373515727
EleutherAI/pythia-70m-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.2760736196319018,0.0351238528370505
EleutherAI/pythia-70m-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.2941176470588235,0.0319800166011507
EleutherAI/pythia-70m-deduped,mmlu_philosophy,0-shot,accuracy,0.1929260450160771,0.0224115167809113
EleutherAI/pythia-70m-deduped,mmlu_jurisprudence,0-shot,accuracy,0.2129629629629629,0.0395783547198098
EleutherAI/pythia-70m-deduped,mmlu_international_law,0-shot,accuracy,0.256198347107438,0.0398497965330287
EleutherAI/pythia-70m-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.2606060606060606,0.0342774317581652
EleutherAI/pythia-70m-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.0346971379170437
EleutherAI/pythia-70m-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.2226890756302521,0.0270254334988823
EleutherAI/pythia-70m-deduped,mmlu_high_school_geography,0-shot,accuracy,0.202020202020202,0.0286062042892298
EleutherAI/pythia-70m-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.2348623853211009,0.0181751105103435
EleutherAI/pythia-70m-deduped,mmlu_public_relations,0-shot,accuracy,0.209090909090909,0.0389509101572413
EleutherAI/pythia-70m-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-70m-deduped,mmlu_sociology,0-shot,accuracy,0.2487562189054726,0.0305676759389167
EleutherAI/pythia-70m-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2948717948717949,0.0231193627582322
EleutherAI/pythia-70m-deduped,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904
EleutherAI/pythia-70m-deduped,mmlu_professional_psychology,0-shot,accuracy,0.2483660130718954,0.0174794870013647
EleutherAI/pythia-70m-deduped,mmlu_human_sexuality,0-shot,accuracy,0.2900763358778626,0.0398006624646776
EleutherAI/pythia-70m-deduped,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.0418577442402205
EleutherAI/pythia-70m-deduped,mmlu_miscellaneous,0-shot,accuracy,0.2439335887611749,0.0153572126658294
EleutherAI/pythia-70m-deduped,mmlu_marketing,0-shot,accuracy,0.2008547008547008,0.0262467729468904
EleutherAI/pythia-70m-deduped,mmlu_management,0-shot,accuracy,0.1941747572815534,0.0391666776282258
EleutherAI/pythia-70m-deduped,mmlu_nutrition,0-shot,accuracy,0.2875816993464052,0.0259178061171471
EleutherAI/pythia-70m-deduped,mmlu_medical_genetics,0-shot,accuracy,0.33,0.047258156262526
EleutherAI/pythia-70m-deduped,mmlu_human_aging,0-shot,accuracy,0.2600896860986547,0.0294424955858574
EleutherAI/pythia-70m-deduped,mmlu_professional_medicine,0-shot,accuracy,0.4191176470588235,0.0299728071704646
EleutherAI/pythia-70m-deduped,mmlu_college_medicine,0-shot,accuracy,0.1965317919075144,0.0302995746647881
EleutherAI/pythia-70m-deduped,mmlu_business_ethics,0-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-70m-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.2528301886792453,0.0267498997712412
EleutherAI/pythia-70m-deduped,mmlu_global_facts,0-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-70m-deduped,mmlu_virology,0-shot,accuracy,0.1746987951807229,0.0295603262112568
EleutherAI/pythia-70m-deduped,mmlu_professional_accounting,0-shot,accuracy,0.2446808510638297,0.0256455536222667
EleutherAI/pythia-70m-deduped,mmlu_college_physics,0-shot,accuracy,0.2352941176470588,0.0422077365917145
EleutherAI/pythia-70m-deduped,mmlu_high_school_physics,0-shot,accuracy,0.1788079470198675,0.0312874485060072
EleutherAI/pythia-70m-deduped,mmlu_high_school_biology,0-shot,accuracy,0.3161290322580645,0.0264508744890427
EleutherAI/pythia-70m-deduped,mmlu_college_biology,0-shot,accuracy,0.2152777777777778,0.0343707934410613
EleutherAI/pythia-70m-deduped,mmlu_anatomy,0-shot,accuracy,0.3259259259259259,0.040491220417025
EleutherAI/pythia-70m-deduped,mmlu_college_chemistry,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-70m-deduped,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-70m-deduped,mmlu_college_computer_science,0-shot,accuracy,0.32,0.046882617226215
EleutherAI/pythia-70m-deduped,mmlu_astronomy,0-shot,accuracy,0.1907894736842105,0.031975658210325
EleutherAI/pythia-70m-deduped,mmlu_college_mathematics,0-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-70m-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2893617021276595,0.0296440065770096
EleutherAI/pythia-70m-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-70m-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-70m-deduped,mmlu_machine_learning,0-shot,accuracy,0.2232142857142857,0.0395230196770251
EleutherAI/pythia-70m-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.2758620689655172,0.0314471258167824
EleutherAI/pythia-70m-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.4629629629629629,0.0340060362553827
EleutherAI/pythia-70m-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.0224945107675031
EleutherAI/pythia-70m-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.2068965517241379,0.0337567244956055
EleutherAI/pythia-70m-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.2259259259259259,0.0254975326396095
EleutherAI/pythia-70m-deduped,arc_challenge,25-shot,accuracy,0.1697952218430034,0.0109717751577842
EleutherAI/pythia-70m-deduped,arc_challenge,25-shot,acc_norm,0.2133105802047781,0.0119709717423263
EleutherAI/pythia-70m-deduped,truthfulqa_mc2,0-shot,accuracy,0.4767651653188276,0.0156738356361674
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,bleu_max,46.031535492951335,0.8673187818828474
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,bleu_acc,0.795593635250918,0.0141171743374326
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,bleu_diff,39.793171656866846,0.9094412465216882
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge1_max,69.81726690246458,1.1081734621228825
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge1_acc,0.799265605875153,0.0140220457174821
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge1_diff,56.95442784511964,1.3527087958065618
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge2_max,63.72306529658852,1.2425333117054245
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge2_acc,0.7552019583843329,0.0150518694867149
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge2_diff,60.61075595094692,1.3860618275238243
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rougeL_max,69.31439415404608,1.1276933183097786
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rougeL_acc,0.8102815177478581,0.013725485265185
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rougeL_diff,57.784969619564976,1.3395186035921138
EleutherAI/pythia-70m-deduped,truthfulqa_mc1,0-shot,accuracy,0.2582619339045288,0.0153218216884761
Salesforce/codegen-16B-multi,minerva_math_precalc,5-shot,accuracy,0.0347985347985348,0.0078503896676978
Salesforce/codegen-16B-multi,minerva_math_prealgebra,5-shot,accuracy,0.0378874856486796,0.0064729342846006
Salesforce/codegen-16B-multi,minerva_math_num_theory,5-shot,accuracy,0.0222222222222222,0.0063492063492063
Salesforce/codegen-16B-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.0243632336655592,0.005133437461128
Salesforce/codegen-16B-multi,minerva_math_geometry,5-shot,accuracy,0.022964509394572,0.0068512498787692
Salesforce/codegen-16B-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.0168776371308016,0.0059228268948526
Salesforce/codegen-16B-multi,minerva_math_algebra,5-shot,accuracy,0.0269587194608256,0.0047029776820065
Salesforce/codegen-16B-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,arithmetic_3da,5-shot,accuracy,0.022,0.0032807593162018
Salesforce/codegen-16B-multi,arithmetic_3ds,5-shot,accuracy,0.0145,0.0026736583971427
Salesforce/codegen-16B-multi,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521
Salesforce/codegen-16B-multi,arithmetic_2ds,5-shot,accuracy,0.166,0.008322056735817
Salesforce/codegen-16B-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,arithmetic_1dc,5-shot,accuracy,0.147,0.0079200292569988
Salesforce/codegen-16B-multi,arithmetic_4ds,5-shot,accuracy,0.001,0.0007069298939339
Salesforce/codegen-16B-multi,arithmetic_2dm,5-shot,accuracy,0.0975,0.0066346728963996
Salesforce/codegen-16B-multi,arithmetic_2da,5-shot,accuracy,0.1715,0.0084308608520929
Salesforce/codegen-16B-multi,gsm8k_cot,5-shot,accuracy,0.0318423047763457,0.0048363485582609
Salesforce/codegen-16B-multi,gsm8k,5-shot,accuracy,0.0341167551175132,0.0050002126007732
Salesforce/codegen-16B-multi,anli_r2,0-shot,brier_score,0.7344945652955857,
Salesforce/codegen-16B-multi,anli_r3,0-shot,brier_score,0.7512937712393907,
Salesforce/codegen-16B-multi,anli_r1,0-shot,brier_score,0.7409617387492144,
Salesforce/codegen-16B-multi,xnli_eu,0-shot,brier_score,1.0499948811805857,
Salesforce/codegen-16B-multi,xnli_vi,0-shot,brier_score,1.049439742537846,
Salesforce/codegen-16B-multi,xnli_ru,0-shot,brier_score,0.9528824583761204,
Salesforce/codegen-16B-multi,xnli_zh,0-shot,brier_score,1.046794144815665,
Salesforce/codegen-16B-multi,xnli_tr,0-shot,brier_score,0.9505776051805006,
Salesforce/codegen-16B-multi,xnli_fr,0-shot,brier_score,0.8746123551516378,
Salesforce/codegen-16B-multi,xnli_en,0-shot,brier_score,0.7168910989376758,
Salesforce/codegen-16B-multi,xnli_ur,0-shot,brier_score,1.2504543392394984,
Salesforce/codegen-16B-multi,xnli_ar,0-shot,brier_score,1.0299487744752638,
Salesforce/codegen-16B-multi,xnli_de,0-shot,brier_score,0.91751888763192,
Salesforce/codegen-16B-multi,xnli_hi,0-shot,brier_score,1.0205470500057452,
Salesforce/codegen-16B-multi,xnli_es,0-shot,brier_score,0.9198569977209587,
Salesforce/codegen-16B-multi,xnli_bg,0-shot,brier_score,0.8427184705601842,
Salesforce/codegen-16B-multi,xnli_sw,0-shot,brier_score,0.9401072122195172,
Salesforce/codegen-16B-multi,xnli_el,0-shot,brier_score,0.9761407972947892,
Salesforce/codegen-16B-multi,xnli_th,0-shot,brier_score,0.8572219481864177,
Salesforce/codegen-16B-multi,logiqa2,0-shot,brier_score,1.1546592524731565,
Salesforce/codegen-16B-multi,mathqa,0-shot,brier_score,0.953439313800392,
Salesforce/codegen-16B-multi,lambada_standard,0-shot,perplexity,25.851477547441903,0.9461878766495389
Salesforce/codegen-16B-multi,lambada_standard,0-shot,accuracy,0.3512516980399767,0.0066505782255734
Salesforce/codegen-16B-multi,lambada_openai,0-shot,perplexity,15.404938907332896,0.5077743811614407
Salesforce/codegen-16B-multi,lambada_openai,0-shot,accuracy,0.4255773335920823,0.006888380073136
Salesforce/codegen-16B-multi,mmlu_world_religions,0-shot,accuracy,0.327485380116959,0.0359933577145602
Salesforce/codegen-16B-multi,mmlu_formal_logic,0-shot,accuracy,0.3015873015873015,0.0410494726990339
Salesforce/codegen-16B-multi,mmlu_prehistory,0-shot,accuracy,0.3086419753086419,0.0257026402606037
Salesforce/codegen-16B-multi,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
Salesforce/codegen-16B-multi,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
Salesforce/codegen-16B-multi,mmlu_moral_disputes,0-shot,accuracy,0.2254335260115607,0.0224972301909675
Salesforce/codegen-16B-multi,mmlu_professional_law,0-shot,accuracy,0.2588005215123859,0.0111861090465646
Salesforce/codegen-16B-multi,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.0332201579577674
Salesforce/codegen-16B-multi,mmlu_high_school_us_history,0-shot,accuracy,0.2745098039215686,0.0313217980308329
Salesforce/codegen-16B-multi,mmlu_philosophy,0-shot,accuracy,0.2733118971061093,0.0253117659754261
Salesforce/codegen-16B-multi,mmlu_jurisprudence,0-shot,accuracy,0.1944444444444444,0.0382607632488486
Salesforce/codegen-16B-multi,mmlu_international_law,0-shot,accuracy,0.2727272727272727,0.040655781409087
Salesforce/codegen-16B-multi,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.0354656301962433
Salesforce/codegen-16B-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2590673575129533,0.0316187791793541
Salesforce/codegen-16B-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.2941176470588235,0.029597329730978
Salesforce/codegen-16B-multi,mmlu_high_school_geography,0-shot,accuracy,0.2424242424242424,0.030532892233932
Salesforce/codegen-16B-multi,mmlu_high_school_psychology,0-shot,accuracy,0.218348623853211,0.0177126005287227
Salesforce/codegen-16B-multi,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
Salesforce/codegen-16B-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.33,0.047258156262526
Salesforce/codegen-16B-multi,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296
Salesforce/codegen-16B-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2615384615384615,0.0222821412042044
Salesforce/codegen-16B-multi,mmlu_security_studies,0-shot,accuracy,0.3795918367346939,0.0310672112628724
Salesforce/codegen-16B-multi,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
Salesforce/codegen-16B-multi,mmlu_human_sexuality,0-shot,accuracy,0.3358778625954198,0.0414231377199666
Salesforce/codegen-16B-multi,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
Salesforce/codegen-16B-multi,mmlu_miscellaneous,0-shot,accuracy,0.2669220945083014,0.0158184508947775
Salesforce/codegen-16B-multi,mmlu_marketing,0-shot,accuracy,0.2735042735042735,0.0292025401534311
Salesforce/codegen-16B-multi,mmlu_management,0-shot,accuracy,0.1941747572815534,0.0391666776282258
Salesforce/codegen-16B-multi,mmlu_nutrition,0-shot,accuracy,0.2516339869281045,0.0248480182638751
Salesforce/codegen-16B-multi,mmlu_medical_genetics,0-shot,accuracy,0.35,0.0479372485441101
Salesforce/codegen-16B-multi,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.0312658052251371
Salesforce/codegen-16B-multi,mmlu_professional_medicine,0-shot,accuracy,0.2610294117647059,0.0266792522701031
Salesforce/codegen-16B-multi,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.0318620985164114
Salesforce/codegen-16B-multi,mmlu_business_ethics,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-16B-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.2452830188679245,0.0264803571798957
Salesforce/codegen-16B-multi,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
Salesforce/codegen-16B-multi,mmlu_virology,0-shot,accuracy,0.2650602409638554,0.0343602403794496
Salesforce/codegen-16B-multi,mmlu_professional_accounting,0-shot,accuracy,0.1879432624113475,0.0233052307697142
Salesforce/codegen-16B-multi,mmlu_college_physics,0-shot,accuracy,0.2254901960784313,0.0415830753308328
Salesforce/codegen-16B-multi,mmlu_high_school_physics,0-shot,accuracy,0.3377483443708609,0.0386155754625517
Salesforce/codegen-16B-multi,mmlu_high_school_biology,0-shot,accuracy,0.3032258064516129,0.0261486859306717
Salesforce/codegen-16B-multi,mmlu_college_biology,0-shot,accuracy,0.3055555555555556,0.0385208469600853
Salesforce/codegen-16B-multi,mmlu_anatomy,0-shot,accuracy,0.2888888888888888,0.0391545063041425
Salesforce/codegen-16B-multi,mmlu_college_chemistry,0-shot,accuracy,0.39,0.0490207130000197
Salesforce/codegen-16B-multi,mmlu_computer_security,0-shot,accuracy,0.33,0.047258156262526
Salesforce/codegen-16B-multi,mmlu_college_computer_science,0-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-16B-multi,mmlu_astronomy,0-shot,accuracy,0.2763157894736842,0.0363905756995292
Salesforce/codegen-16B-multi,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215
Salesforce/codegen-16B-multi,mmlu_conceptual_physics,0-shot,accuracy,0.2680851063829787,0.0289573427883423
Salesforce/codegen-16B-multi,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-16B-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-16B-multi,mmlu_machine_learning,0-shot,accuracy,0.2767857142857143,0.0424662433669762
Salesforce/codegen-16B-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.0305165307326944
Salesforce/codegen-16B-multi,mmlu_high_school_statistics,0-shot,accuracy,0.375,0.0330169089872108
Salesforce/codegen-16B-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.2645502645502645,0.0227174678977086
Salesforce/codegen-16B-multi,mmlu_electrical_engineering,0-shot,accuracy,0.3310344827586207,0.0392154531246712
Salesforce/codegen-16B-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
Salesforce/codegen-16B-multi,arc_challenge,25-shot,accuracy,0.2849829351535836,0.0131913481798387
Salesforce/codegen-16B-multi,arc_challenge,25-shot,acc_norm,0.3370307167235495,0.0138134766529022
Salesforce/codegen-16B-multi,hellaswag,10-shot,accuracy,0.3936466839275044,0.0048755957928506
Salesforce/codegen-16B-multi,hellaswag,10-shot,acc_norm,0.5127464648476399,0.0049881597447425
Salesforce/codegen-16B-multi,truthfulqa_mc2,0-shot,accuracy,0.4327862564456793,0.014741511136754
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,bleu_max,17.563201921535452,0.6084253447461239
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,bleu_acc,0.3769889840881273,0.0169655175789303
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,bleu_diff,-2.9441042316118144,0.6521697109915429
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge1_max,40.11259761742152,0.8426548513555856
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge1_acc,0.3353733170134639,0.0165275340396689
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge1_diff,-5.161390138276828,0.9272668238645784
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge2_max,21.567769834989484,0.9243570922115246
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge2_acc,0.1909424724602203,0.0137592858426857
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge2_diff,-7.056555663275975,0.9635864851509316
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rougeL_max,37.40770287988807,0.8348939987940525
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rougeL_acc,0.3206854345165238,0.0163391703732809
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rougeL_diff,-5.184697052173686,0.9244121757045392
Salesforce/codegen-16B-multi,truthfulqa_mc1,0-shot,accuracy,0.2570379436964504,0.015298077509485
Salesforce/codegen-16B-multi,winogrande,5-shot,accuracy,0.55327545382794,0.0139724883716166
facebook/opt-6.7b,arc:challenge,25-shot,accuracy,0.3472696245733788,0.0139130345296204
facebook/opt-6.7b,arc:challenge,25-shot,acc_norm,0.3916382252559727,0.0142641221249382
facebook/opt-6.7b,hellaswag,10-shot,accuracy,0.5072694682334197,0.0049892540118957
facebook/opt-6.7b,hellaswag,10-shot,acc_norm,0.6890061740689106,0.0046195423920063
facebook/opt-6.7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.0435194139889244
facebook/opt-6.7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.0435194139889244
facebook/opt-6.7b,hendrycksTest-anatomy,5-shot,accuracy,0.3481481481481481,0.0411532461033695
facebook/opt-6.7b,hendrycksTest-anatomy,5-shot,acc_norm,0.3481481481481481,0.0411532461033695
facebook/opt-6.7b,hendrycksTest-astronomy,5-shot,accuracy,0.1447368421052631,0.0286319518459303
facebook/opt-6.7b,hendrycksTest-astronomy,5-shot,acc_norm,0.1447368421052631,0.0286319518459303
facebook/opt-6.7b,hendrycksTest-business_ethics,5-shot,accuracy,0.16,0.036845294917747
facebook/opt-6.7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.16,0.036845294917747
facebook/opt-6.7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2037735849056604,0.0247907845017754
facebook/opt-6.7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2037735849056604,0.0247907845017754
facebook/opt-6.7b,hendrycksTest-college_biology,5-shot,accuracy,0.2291666666666666,0.0351469746786238
facebook/opt-6.7b,hendrycksTest-college_biology,5-shot,acc_norm,0.2291666666666666,0.0351469746786238
facebook/opt-6.7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
facebook/opt-6.7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
facebook/opt-6.7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
facebook/opt-6.7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
facebook/opt-6.7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338474
facebook/opt-6.7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338474
facebook/opt-6.7b,hendrycksTest-college_medicine,5-shot,accuracy,0.2312138728323699,0.0321473730202946
facebook/opt-6.7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2312138728323699,0.0321473730202946
facebook/opt-6.7b,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
facebook/opt-6.7b,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
facebook/opt-6.7b,hendrycksTest-computer_security,5-shot,accuracy,0.24,0.0429234695990928
facebook/opt-6.7b,hendrycksTest-computer_security,5-shot,acc_norm,0.24,0.0429234695990928
facebook/opt-6.7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.0289573427883423
facebook/opt-6.7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.0289573427883423
facebook/opt-6.7b,hendrycksTest-econometrics,5-shot,accuracy,0.2280701754385964,0.0394715278266941
facebook/opt-6.7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2280701754385964,0.0394715278266941
facebook/opt-6.7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2758620689655172,0.0372456361977463
facebook/opt-6.7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2758620689655172,0.0372456361977463
facebook/opt-6.7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2142857142857142,0.0211328591827544
facebook/opt-6.7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2142857142857142,0.0211328591827544
facebook/opt-6.7b,hendrycksTest-formal_logic,5-shot,accuracy,0.1666666666666666,0.0333333333333333
facebook/opt-6.7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.1666666666666666,0.0333333333333333
facebook/opt-6.7b,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.0460566186471838
facebook/opt-6.7b,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.0460566186471838
facebook/opt-6.7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2225806451612903,0.0236642166716425
facebook/opt-6.7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2225806451612903,0.0236642166716425
facebook/opt-6.7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2463054187192118,0.0303150992856177
facebook/opt-6.7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2463054187192118,0.0303150992856177
facebook/opt-6.7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.23,0.042295258468165
facebook/opt-6.7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.23,0.042295258468165
facebook/opt-6.7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000918
facebook/opt-6.7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2363636363636363,0.0331750593000918
facebook/opt-6.7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.1969696969696969,0.0283356097324633
facebook/opt-6.7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1969696969696969,0.0283356097324633
facebook/opt-6.7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2435233160621761,0.0309754363868454
facebook/opt-6.7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2435233160621761,0.0309754363868454
facebook/opt-6.7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2230769230769231,0.0211077301272439
facebook/opt-6.7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2230769230769231,0.0211077301272439
facebook/opt-6.7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
facebook/opt-6.7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
facebook/opt-6.7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.1848739495798319,0.0252159928779542
facebook/opt-6.7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.1848739495798319,0.0252159928779542
facebook/opt-6.7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.1854304635761589,0.0317328438429428
facebook/opt-6.7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.1854304635761589,0.0317328438429428
facebook/opt-6.7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2587155963302752,0.0187760523196196
facebook/opt-6.7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2587155963302752,0.0187760523196196
facebook/opt-6.7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2037037037037037,0.027467401804058
facebook/opt-6.7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2037037037037037,0.027467401804058
facebook/opt-6.7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2401960784313725,0.0299837330559136
facebook/opt-6.7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2401960784313725,0.0299837330559136
facebook/opt-6.7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2616033755274262,0.0286095167169949
facebook/opt-6.7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2616033755274262,0.0286095167169949
facebook/opt-6.7b,hendrycksTest-human_aging,5-shot,accuracy,0.3452914798206278,0.0319110019283579
facebook/opt-6.7b,hendrycksTest-human_aging,5-shot,acc_norm,0.3452914798206278,0.0319110019283579
facebook/opt-6.7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2137404580152671,0.0359546161177469
facebook/opt-6.7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2137404580152671,0.0359546161177469
facebook/opt-6.7b,hendrycksTest-international_law,5-shot,accuracy,0.2314049586776859,0.0384985609879408
facebook/opt-6.7b,hendrycksTest-international_law,5-shot,acc_norm,0.2314049586776859,0.0384985609879408
facebook/opt-6.7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2592592592592592,0.0423651125809463
facebook/opt-6.7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2592592592592592,0.0423651125809463
facebook/opt-6.7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2515337423312883,0.0340899788685752
facebook/opt-6.7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2515337423312883,0.0340899788685752
facebook/opt-6.7b,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257873
facebook/opt-6.7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257873
facebook/opt-6.7b,hendrycksTest-management,5-shot,accuracy,0.233009708737864,0.0418583259892831
facebook/opt-6.7b,hendrycksTest-management,5-shot,acc_norm,0.233009708737864,0.0418583259892831
facebook/opt-6.7b,hendrycksTest-marketing,5-shot,accuracy,0.3034188034188034,0.0301182101069426
facebook/opt-6.7b,hendrycksTest-marketing,5-shot,acc_norm,0.3034188034188034,0.0301182101069426
facebook/opt-6.7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.0460566186471838
facebook/opt-6.7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.0460566186471838
facebook/opt-6.7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2758620689655172,0.0159828147746956
facebook/opt-6.7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2758620689655172,0.0159828147746956
facebook/opt-6.7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2341040462427745,0.0227971102780711
facebook/opt-6.7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2341040462427745,0.0227971102780711
facebook/opt-6.7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/opt-6.7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
facebook/opt-6.7b,hendrycksTest-nutrition,5-shot,accuracy,0.2287581699346405,0.0240510297399122
facebook/opt-6.7b,hendrycksTest-nutrition,5-shot,acc_norm,0.2287581699346405,0.0240510297399122
facebook/opt-6.7b,hendrycksTest-philosophy,5-shot,accuracy,0.247588424437299,0.0245138799736219
facebook/opt-6.7b,hendrycksTest-philosophy,5-shot,acc_norm,0.247588424437299,0.0245138799736219
facebook/opt-6.7b,hendrycksTest-prehistory,5-shot,accuracy,0.2839506172839506,0.0250894785237651
facebook/opt-6.7b,hendrycksTest-prehistory,5-shot,acc_norm,0.2839506172839506,0.0250894785237651
facebook/opt-6.7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
facebook/opt-6.7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.0267891723511402
facebook/opt-6.7b,hendrycksTest-professional_law,5-shot,accuracy,0.241199478487614,0.0109264961020349
facebook/opt-6.7b,hendrycksTest-professional_law,5-shot,acc_norm,0.241199478487614,0.0109264961020349
facebook/opt-6.7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.2720588235294117,0.0270330411516814
facebook/opt-6.7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2720588235294117,0.0270330411516814
facebook/opt-6.7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2532679738562091,0.0175934868953668
facebook/opt-6.7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2532679738562091,0.0175934868953668
facebook/opt-6.7b,hendrycksTest-public_relations,5-shot,accuracy,0.3363636363636363,0.045253935963025
facebook/opt-6.7b,hendrycksTest-public_relations,5-shot,acc_norm,0.3363636363636363,0.045253935963025
facebook/opt-6.7b,hendrycksTest-security_studies,5-shot,accuracy,0.1959183673469387,0.0254093019532256
facebook/opt-6.7b,hendrycksTest-security_studies,5-shot,acc_norm,0.1959183673469387,0.0254093019532256
facebook/opt-6.7b,hendrycksTest-sociology,5-shot,accuracy,0.2238805970149253,0.0294752502360171
facebook/opt-6.7b,hendrycksTest-sociology,5-shot,acc_norm,0.2238805970149253,0.0294752502360171
facebook/opt-6.7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.0409360180740332
facebook/opt-6.7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.0409360180740332
facebook/opt-6.7b,hendrycksTest-virology,5-shot,accuracy,0.3253012048192771,0.0364716852368322
facebook/opt-6.7b,hendrycksTest-virology,5-shot,acc_norm,0.3253012048192771,0.0364716852368322
facebook/opt-6.7b,hendrycksTest-world_religions,5-shot,accuracy,0.2280701754385964,0.0321809379560235
facebook/opt-6.7b,hendrycksTest-world_religions,5-shot,acc_norm,0.2280701754385964,0.0321809379560235
facebook/opt-6.7b,truthfulqa:mc,0-shot,mc1,0.211750305997552,0.0143020683539256
facebook/opt-6.7b,truthfulqa:mc,0-shot,mc2,0.3512214978869498,0.0135071325293533
facebook/opt-6.7b,drop,3-shot,accuracy,0.0011535234899328,0.0003476179896857
facebook/opt-6.7b,drop,3-shot,f1,0.0486063338926175,0.0011917611903016
facebook/opt-6.7b,gsm8k,5-shot,accuracy,0.0257771038665655,0.0043650429536218
facebook/opt-6.7b,winogrande,5-shot,accuracy,0.654301499605367,0.0133665969519343
cerebras/Cerebras-GPT-13B,arc:challenge,25-shot,accuracy,0.3378839590443686,0.0138220479222835
cerebras/Cerebras-GPT-13B,arc:challenge,25-shot,acc_norm,0.3813993174061433,0.0141943890866852
cerebras/Cerebras-GPT-13B,hellaswag,10-shot,accuracy,0.4498107946624178,0.0049645796857124
cerebras/Cerebras-GPT-13B,hellaswag,10-shot,acc_norm,0.6000796654052978,0.004888805003103
cerebras/Cerebras-GPT-13B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.042295258468165
cerebras/Cerebras-GPT-13B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.042295258468165
cerebras/Cerebras-GPT-13B,hendrycksTest-anatomy,5-shot,accuracy,0.2592592592592592,0.0378571446506665
cerebras/Cerebras-GPT-13B,hendrycksTest-anatomy,5-shot,acc_norm,0.2592592592592592,0.0378571446506665
cerebras/Cerebras-GPT-13B,hendrycksTest-astronomy,5-shot,accuracy,0.2171052631578947,0.0335504530488292
cerebras/Cerebras-GPT-13B,hendrycksTest-astronomy,5-shot,acc_norm,0.2171052631578947,0.0335504530488292
cerebras/Cerebras-GPT-13B,hendrycksTest-business_ethics,5-shot,accuracy,0.19,0.0394277244403662
cerebras/Cerebras-GPT-13B,hendrycksTest-business_ethics,5-shot,acc_norm,0.19,0.0394277244403662
cerebras/Cerebras-GPT-13B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2528301886792453,0.0267498997712412
cerebras/Cerebras-GPT-13B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2528301886792453,0.0267498997712412
cerebras/Cerebras-GPT-13B,hendrycksTest-college_biology,5-shot,accuracy,0.2916666666666667,0.0380096806055485
cerebras/Cerebras-GPT-13B,hendrycksTest-college_biology,5-shot,acc_norm,0.2916666666666667,0.0380096806055485
cerebras/Cerebras-GPT-13B,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.0416333199893227
cerebras/Cerebras-GPT-13B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.0416333199893227
cerebras/Cerebras-GPT-13B,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
cerebras/Cerebras-GPT-13B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
cerebras/Cerebras-GPT-13B,hendrycksTest-college_mathematics,5-shot,accuracy,0.34,0.0476095228569523
cerebras/Cerebras-GPT-13B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.34,0.0476095228569523
cerebras/Cerebras-GPT-13B,hendrycksTest-college_medicine,5-shot,accuracy,0.2312138728323699,0.0321473730202947
cerebras/Cerebras-GPT-13B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2312138728323699,0.0321473730202947
cerebras/Cerebras-GPT-13B,hendrycksTest-college_physics,5-shot,accuracy,0.196078431372549,0.0395058186117996
cerebras/Cerebras-GPT-13B,hendrycksTest-college_physics,5-shot,acc_norm,0.196078431372549,0.0395058186117996
cerebras/Cerebras-GPT-13B,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
cerebras/Cerebras-GPT-13B,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
cerebras/Cerebras-GPT-13B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2553191489361702,0.0285048564705141
cerebras/Cerebras-GPT-13B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2553191489361702,0.0285048564705141
cerebras/Cerebras-GPT-13B,hendrycksTest-econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
cerebras/Cerebras-GPT-13B,hendrycksTest-econometrics,5-shot,acc_norm,0.2368421052631578,0.0399942387928133
cerebras/Cerebras-GPT-13B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
cerebras/Cerebras-GPT-13B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
cerebras/Cerebras-GPT-13B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2433862433862433,0.0221011287874154
cerebras/Cerebras-GPT-13B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2433862433862433,0.0221011287874154
cerebras/Cerebras-GPT-13B,hendrycksTest-formal_logic,5-shot,accuracy,0.3015873015873015,0.0410494726990339
cerebras/Cerebras-GPT-13B,hendrycksTest-formal_logic,5-shot,acc_norm,0.3015873015873015,0.0410494726990339
cerebras/Cerebras-GPT-13B,hendrycksTest-global_facts,5-shot,accuracy,0.2,0.0402015126103684
cerebras/Cerebras-GPT-13B,hendrycksTest-global_facts,5-shot,acc_norm,0.2,0.0402015126103684
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2483870967741935,0.024580028921481
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2483870967741935,0.024580028921481
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2266009852216748,0.0294548638352929
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2266009852216748,0.0294548638352929
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.23,0.042295258468165
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.23,0.042295258468165
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2606060606060606,0.0342774317581652
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2606060606060606,0.0342774317581652
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_geography,5-shot,accuracy,0.2373737373737373,0.0303137105381988
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2373737373737373,0.0303137105381988
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.1968911917098445,0.0286978739718606
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.1968911917098445,0.0286978739718606
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3179487179487179,0.0236108843089278
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3179487179487179,0.0236108843089278
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2444444444444444,0.0262027665346521
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2444444444444444,0.0262027665346521
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2773109243697479,0.02907937453948
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2773109243697479,0.02907937453948
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2317880794701986,0.0344540627198705
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2317880794701986,0.0344540627198705
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2825688073394495,0.0193042434977071
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2825688073394495,0.0193042434977071
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4537037037037037,0.0339532272637579
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4537037037037037,0.0339532272637579
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.0305875913516042
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.0305875913516042
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2784810126582278,0.0291786823048425
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2784810126582278,0.0291786823048425
cerebras/Cerebras-GPT-13B,hendrycksTest-human_aging,5-shot,accuracy,0.2511210762331838,0.0291052208332246
cerebras/Cerebras-GPT-13B,hendrycksTest-human_aging,5-shot,acc_norm,0.2511210762331838,0.0291052208332246
cerebras/Cerebras-GPT-13B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2290076335877862,0.0368534663171185
cerebras/Cerebras-GPT-13B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2290076335877862,0.0368534663171185
cerebras/Cerebras-GPT-13B,hendrycksTest-international_law,5-shot,accuracy,0.2644628099173554,0.040261875275912
cerebras/Cerebras-GPT-13B,hendrycksTest-international_law,5-shot,acc_norm,0.2644628099173554,0.040261875275912
cerebras/Cerebras-GPT-13B,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.041860917913946
cerebras/Cerebras-GPT-13B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.041860917913946
cerebras/Cerebras-GPT-13B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2453987730061349,0.0338093981394335
cerebras/Cerebras-GPT-13B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2453987730061349,0.0338093981394335
cerebras/Cerebras-GPT-13B,hendrycksTest-machine_learning,5-shot,accuracy,0.2767857142857143,0.0424662433669762
cerebras/Cerebras-GPT-13B,hendrycksTest-machine_learning,5-shot,acc_norm,0.2767857142857143,0.0424662433669762
cerebras/Cerebras-GPT-13B,hendrycksTest-management,5-shot,accuracy,0.2427184466019417,0.0424502248638449
cerebras/Cerebras-GPT-13B,hendrycksTest-management,5-shot,acc_norm,0.2427184466019417,0.0424502248638449
cerebras/Cerebras-GPT-13B,hendrycksTest-marketing,5-shot,accuracy,0.2521367521367521,0.028447965476231
cerebras/Cerebras-GPT-13B,hendrycksTest-marketing,5-shot,acc_norm,0.2521367521367521,0.028447965476231
cerebras/Cerebras-GPT-13B,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.0451260859854212
cerebras/Cerebras-GPT-13B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.0451260859854212
cerebras/Cerebras-GPT-13B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2835249042145594,0.0161173181668322
cerebras/Cerebras-GPT-13B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2835249042145594,0.0161173181668322
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2726256983240223,0.0148933917352495
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2726256983240223,0.0148933917352495
cerebras/Cerebras-GPT-13B,hendrycksTest-nutrition,5-shot,accuracy,0.2581699346405229,0.0250585033169581
cerebras/Cerebras-GPT-13B,hendrycksTest-nutrition,5-shot,acc_norm,0.2581699346405229,0.0250585033169581
cerebras/Cerebras-GPT-13B,hendrycksTest-philosophy,5-shot,accuracy,0.270096463022508,0.0252180403734106
cerebras/Cerebras-GPT-13B,hendrycksTest-philosophy,5-shot,acc_norm,0.270096463022508,0.0252180403734106
cerebras/Cerebras-GPT-13B,hendrycksTest-prehistory,5-shot,accuracy,0.2438271604938271,0.0238918795419596
cerebras/Cerebras-GPT-13B,hendrycksTest-prehistory,5-shot,acc_norm,0.2438271604938271,0.0238918795419596
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2482269503546099,0.0257700156442903
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_law,5-shot,accuracy,0.2516297262059974,0.0110832762804418
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_law,5-shot,acc_norm,0.2516297262059974,0.0110832762804418
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_medicine,5-shot,accuracy,0.1911764705882352,0.0238868819224403
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1911764705882352,0.0238868819224403
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2418300653594771,0.0173227892077843
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2418300653594771,0.0173227892077843
cerebras/Cerebras-GPT-13B,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.0435027144292324
cerebras/Cerebras-GPT-13B,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.0435027144292324
cerebras/Cerebras-GPT-13B,hendrycksTest-security_studies,5-shot,accuracy,0.3551020408163265,0.0306356551503876
cerebras/Cerebras-GPT-13B,hendrycksTest-security_studies,5-shot,acc_norm,0.3551020408163265,0.0306356551503876
cerebras/Cerebras-GPT-13B,hendrycksTest-sociology,5-shot,accuracy,0.2587064676616915,0.030965903123573
cerebras/Cerebras-GPT-13B,hendrycksTest-sociology,5-shot,acc_norm,0.2587064676616915,0.030965903123573
cerebras/Cerebras-GPT-13B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.24,0.0429234695990928
cerebras/Cerebras-GPT-13B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.24,0.0429234695990928
cerebras/Cerebras-GPT-13B,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.0359156679782466
cerebras/Cerebras-GPT-13B,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.0359156679782466
cerebras/Cerebras-GPT-13B,hendrycksTest-world_religions,5-shot,accuracy,0.3040935672514619,0.0352821125824523
cerebras/Cerebras-GPT-13B,hendrycksTest-world_religions,5-shot,acc_norm,0.3040935672514619,0.0352821125824523
cerebras/Cerebras-GPT-13B,truthfulqa:mc,0-shot,mc1,0.2276621787025703,0.014679255032111
cerebras/Cerebras-GPT-13B,truthfulqa:mc,0-shot,mc2,0.3918546474465412,0.013884078720404
cerebras/Cerebras-GPT-13B,drop,3-shot,accuracy,0.0003145973154362,0.0001816137946883
cerebras/Cerebras-GPT-13B,drop,3-shot,f1,0.0438915687919464,0.0011058022021902
cerebras/Cerebras-GPT-13B,gsm8k,5-shot,accuracy,0.0128885519332827,0.0031069012664996
cerebras/Cerebras-GPT-13B,winogrande,5-shot,accuracy,0.5982636148382005,0.0137784392666494
facebook/opt-2.7b,drop,3-shot,accuracy,0.0010486577181208,0.0003314581465219
facebook/opt-2.7b,drop,3-shot,f1,0.0476740771812081,0.0011986644527763
facebook/opt-2.7b,gsm8k,5-shot,accuracy,0.0219863532979529,0.00403916275811
facebook/opt-2.7b,winogrande,5-shot,accuracy,0.6164167324388319,0.013666275889539
facebook/opt-2.7b,arc:challenge,25-shot,accuracy,0.3097269624573379,0.0135120584152383
facebook/opt-2.7b,arc:challenge,25-shot,acc_norm,0.3395904436860068,0.0138390397628201
facebook/opt-2.7b,hellaswag,10-shot,accuracy,0.4598685520812587,0.0049736830262021
facebook/opt-2.7b,hellaswag,10-shot,acc_norm,0.6177056363274248,0.0048495478191344
facebook/opt-2.7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.2,0.0402015126103684
facebook/opt-2.7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.2,0.0402015126103684
facebook/opt-2.7b,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.0374985070917402
facebook/opt-2.7b,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.0374985070917402
facebook/opt-2.7b,hendrycksTest-astronomy,5-shot,accuracy,0.1710526315789473,0.030643607071677
facebook/opt-2.7b,hendrycksTest-astronomy,5-shot,acc_norm,0.1710526315789473,0.030643607071677
facebook/opt-2.7b,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.0409360180740332
facebook/opt-2.7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.0409360180740332
facebook/opt-2.7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2226415094339622,0.025604233470899
facebook/opt-2.7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2226415094339622,0.025604233470899
facebook/opt-2.7b,hendrycksTest-college_biology,5-shot,accuracy,0.2083333333333333,0.0339611620584533
facebook/opt-2.7b,hendrycksTest-college_biology,5-shot,acc_norm,0.2083333333333333,0.0339611620584533
facebook/opt-2.7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.19,0.0394277244403662
facebook/opt-2.7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.19,0.0394277244403662
facebook/opt-2.7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.29,0.0456048021572068
facebook/opt-2.7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.29,0.0456048021572068
facebook/opt-2.7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.23,0.042295258468165
facebook/opt-2.7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.23,0.042295258468165
facebook/opt-2.7b,hendrycksTest-college_medicine,5-shot,accuracy,0.2485549132947976,0.0329530469681831
facebook/opt-2.7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2485549132947976,0.0329530469681831
facebook/opt-2.7b,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
facebook/opt-2.7b,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
facebook/opt-2.7b,hendrycksTest-computer_security,5-shot,accuracy,0.26,0.0440844002276807
facebook/opt-2.7b,hendrycksTest-computer_security,5-shot,acc_norm,0.26,0.0440844002276807
facebook/opt-2.7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.225531914893617,0.0273210784173875
facebook/opt-2.7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.225531914893617,0.0273210784173875
facebook/opt-2.7b,hendrycksTest-econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
facebook/opt-2.7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2456140350877192,0.0404933929774814
facebook/opt-2.7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2827586206896552,0.0375283395800333
facebook/opt-2.7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2827586206896552,0.0375283395800333
facebook/opt-2.7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2248677248677248,0.0215020960782291
facebook/opt-2.7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2248677248677248,0.0215020960782291
facebook/opt-2.7b,hendrycksTest-formal_logic,5-shot,accuracy,0.1904761904761904,0.0351220741230205
facebook/opt-2.7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.1904761904761904,0.0351220741230205
facebook/opt-2.7b,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
facebook/opt-2.7b,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
facebook/opt-2.7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2161290322580645,0.0234152934335685
facebook/opt-2.7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2161290322580645,0.0234152934335685
facebook/opt-2.7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
facebook/opt-2.7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.0305165307326944
facebook/opt-2.7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
facebook/opt-2.7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526
facebook/opt-2.7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000918
facebook/opt-2.7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2363636363636363,0.0331750593000918
facebook/opt-2.7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.2171717171717171,0.0293766164849456
facebook/opt-2.7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2171717171717171,0.0293766164849456
facebook/opt-2.7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3367875647668393,0.0341078025183618
facebook/opt-2.7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3367875647668393,0.0341078025183618
facebook/opt-2.7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3487179487179487,0.0241627802840177
facebook/opt-2.7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3487179487179487,0.0241627802840177
facebook/opt-2.7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2814814814814815,0.0274200193509452
facebook/opt-2.7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2814814814814815,0.0274200193509452
facebook/opt-2.7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2142857142857142,0.0266535315967154
facebook/opt-2.7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2142857142857142,0.0266535315967154
facebook/opt-2.7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.3178807947019867,0.038020397601079
facebook/opt-2.7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3178807947019867,0.038020397601079
facebook/opt-2.7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.326605504587156,0.0201069908899373
facebook/opt-2.7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.326605504587156,0.0201069908899373
facebook/opt-2.7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4074074074074074,0.0335099160469604
facebook/opt-2.7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4074074074074074,0.0335099160469604
facebook/opt-2.7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.0303915336927415
facebook/opt-2.7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.0303915336927415
facebook/opt-2.7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2362869198312236,0.0276521531441592
facebook/opt-2.7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2362869198312236,0.0276521531441592
facebook/opt-2.7b,hendrycksTest-human_aging,5-shot,accuracy,0.2017937219730941,0.0269361119128022
facebook/opt-2.7b,hendrycksTest-human_aging,5-shot,acc_norm,0.2017937219730941,0.0269361119128022
facebook/opt-2.7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.1984732824427481,0.0349814938546247
facebook/opt-2.7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.1984732824427481,0.0349814938546247
facebook/opt-2.7b,hendrycksTest-international_law,5-shot,accuracy,0.2479338842975206,0.039418975265163
facebook/opt-2.7b,hendrycksTest-international_law,5-shot,acc_norm,0.2479338842975206,0.039418975265163
facebook/opt-2.7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
facebook/opt-2.7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
facebook/opt-2.7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2269938650306748,0.0329109957861576
facebook/opt-2.7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2269938650306748,0.0329109957861576
facebook/opt-2.7b,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.0428785875134045
facebook/opt-2.7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.0428785875134045
facebook/opt-2.7b,hendrycksTest-management,5-shot,accuracy,0.3689320388349514,0.0477761518115673
facebook/opt-2.7b,hendrycksTest-management,5-shot,acc_norm,0.3689320388349514,0.0477761518115673
facebook/opt-2.7b,hendrycksTest-marketing,5-shot,accuracy,0.2393162393162393,0.0279518268089243
facebook/opt-2.7b,hendrycksTest-marketing,5-shot,acc_norm,0.2393162393162393,0.0279518268089243
facebook/opt-2.7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.0479372485441101
facebook/opt-2.7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.0479372485441101
facebook/opt-2.7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2464878671775223,0.0154113087696869
facebook/opt-2.7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2464878671775223,0.0154113087696869
facebook/opt-2.7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2427745664739884,0.0230836585869842
facebook/opt-2.7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2427745664739884,0.0230836585869842
facebook/opt-2.7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.235754189944134,0.0141963756862908
facebook/opt-2.7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.235754189944134,0.0141963756862908
facebook/opt-2.7b,hendrycksTest-nutrition,5-shot,accuracy,0.1928104575163398,0.0225893188881767
facebook/opt-2.7b,hendrycksTest-nutrition,5-shot,acc_norm,0.1928104575163398,0.0225893188881767
facebook/opt-2.7b,hendrycksTest-philosophy,5-shot,accuracy,0.315112540192926,0.0263852737034644
facebook/opt-2.7b,hendrycksTest-philosophy,5-shot,acc_norm,0.315112540192926,0.0263852737034644
facebook/opt-2.7b,hendrycksTest-prehistory,5-shot,accuracy,0.2407407407407407,0.0237885835516585
facebook/opt-2.7b,hendrycksTest-prehistory,5-shot,acc_norm,0.2407407407407407,0.0237885835516585
facebook/opt-2.7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2978723404255319,0.0272816083444694
facebook/opt-2.7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2978723404255319,0.0272816083444694
facebook/opt-2.7b,hendrycksTest-professional_law,5-shot,accuracy,0.2522816166883963,0.0110927890568752
facebook/opt-2.7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2522816166883963,0.0110927890568752
facebook/opt-2.7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.3198529411764705,0.0283329595140312
facebook/opt-2.7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3198529411764705,0.0283329595140312
facebook/opt-2.7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2581699346405229,0.01770453165325
facebook/opt-2.7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2581699346405229,0.01770453165325
facebook/opt-2.7b,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
facebook/opt-2.7b,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
facebook/opt-2.7b,hendrycksTest-security_studies,5-shot,accuracy,0.2081632653061224,0.0259911176728132
facebook/opt-2.7b,hendrycksTest-security_studies,5-shot,acc_norm,0.2081632653061224,0.0259911176728132
facebook/opt-2.7b,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.030769444967296
facebook/opt-2.7b,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.030769444967296
facebook/opt-2.7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.25,0.0435194139889244
facebook/opt-2.7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.25,0.0435194139889244
facebook/opt-2.7b,hendrycksTest-virology,5-shot,accuracy,0.2349397590361445,0.0330053318612892
facebook/opt-2.7b,hendrycksTest-virology,5-shot,acc_norm,0.2349397590361445,0.0330053318612892
facebook/opt-2.7b,hendrycksTest-world_religions,5-shot,accuracy,0.175438596491228,0.0291708855007276
facebook/opt-2.7b,hendrycksTest-world_religions,5-shot,acc_norm,0.175438596491228,0.0291708855007276
facebook/opt-2.7b,truthfulqa:mc,0-shot,mc1,0.2276621787025703,0.014679255032111
facebook/opt-2.7b,truthfulqa:mc,0-shot,mc2,0.3742541799029018,0.0138230327317664
01-ai/Yi-9B,mmlu_world_religions,0-shot,accuracy,0.8421052631578947,0.0279667858591608
01-ai/Yi-9B,mmlu_formal_logic,0-shot,accuracy,0.5793650793650794,0.0441543822674374
01-ai/Yi-9B,mmlu_prehistory,0-shot,accuracy,0.7716049382716049,0.0233582118406262
01-ai/Yi-9B,mmlu_moral_scenarios,0-shot,accuracy,0.3955307262569832,0.0163534154100757
01-ai/Yi-9B,mmlu_high_school_world_history,0-shot,accuracy,0.8227848101265823,0.0248563641845032
01-ai/Yi-9B,mmlu_moral_disputes,0-shot,accuracy,0.7658959537572254,0.0227971102780711
01-ai/Yi-9B,mmlu_professional_law,0-shot,accuracy,0.4941329856584094,0.0127693569252165
01-ai/Yi-9B,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
01-ai/Yi-9B,mmlu_high_school_us_history,0-shot,accuracy,0.8676470588235294,0.0237842975209188
01-ai/Yi-9B,mmlu_philosophy,0-shot,accuracy,0.77491961414791,0.023720088516179
01-ai/Yi-9B,mmlu_jurisprudence,0-shot,accuracy,0.7870370370370371,0.0395783547198097
01-ai/Yi-9B,mmlu_international_law,0-shot,accuracy,0.8264462809917356,0.0345727283691767
01-ai/Yi-9B,mmlu_high_school_european_history,0-shot,accuracy,0.7696969696969697,0.0328766675860349
01-ai/Yi-9B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.932642487046632,0.0180883938390789
01-ai/Yi-9B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8235294117647058,0.0247629026780579
01-ai/Yi-9B,mmlu_high_school_geography,0-shot,accuracy,0.8535353535353535,0.0251909211146039
01-ai/Yi-9B,mmlu_high_school_psychology,0-shot,accuracy,0.8660550458715597,0.0146028114355926
01-ai/Yi-9B,mmlu_public_relations,0-shot,accuracy,0.7363636363636363,0.0422022469297198
01-ai/Yi-9B,mmlu_us_foreign_policy,0-shot,accuracy,0.9,0.0301511344577763
01-ai/Yi-9B,mmlu_sociology,0-shot,accuracy,0.8606965174129353,0.0244844871629139
01-ai/Yi-9B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.764102564102564,0.0215259654074087
01-ai/Yi-9B,mmlu_security_studies,0-shot,accuracy,0.763265306122449,0.0272128358840731
01-ai/Yi-9B,mmlu_professional_psychology,0-shot,accuracy,0.7058823529411765,0.0184334276494018
01-ai/Yi-9B,mmlu_human_sexuality,0-shot,accuracy,0.7862595419847328,0.0359546161177469
01-ai/Yi-9B,mmlu_econometrics,0-shot,accuracy,0.5263157894736842,0.0469708513664786
01-ai/Yi-9B,mmlu_miscellaneous,0-shot,accuracy,0.8390804597701149,0.0131402255156117
01-ai/Yi-9B,mmlu_marketing,0-shot,accuracy,0.905982905982906,0.0191198927989249
01-ai/Yi-9B,mmlu_management,0-shot,accuracy,0.8252427184466019,0.0376017800602662
01-ai/Yi-9B,mmlu_nutrition,0-shot,accuracy,0.7549019607843137,0.0246300489798247
01-ai/Yi-9B,mmlu_medical_genetics,0-shot,accuracy,0.75,0.0435194139889244
01-ai/Yi-9B,mmlu_human_aging,0-shot,accuracy,0.7488789237668162,0.0291052208332246
01-ai/Yi-9B,mmlu_professional_medicine,0-shot,accuracy,0.7058823529411765,0.0276784686421447
01-ai/Yi-9B,mmlu_college_medicine,0-shot,accuracy,0.7283236994219653,0.0339175032232166
01-ai/Yi-9B,mmlu_business_ethics,0-shot,accuracy,0.79,0.0409360180740332
01-ai/Yi-9B,mmlu_clinical_knowledge,0-shot,accuracy,0.7283018867924528,0.0273777066246707
01-ai/Yi-9B,mmlu_global_facts,0-shot,accuracy,0.34,0.0476095228569523
01-ai/Yi-9B,mmlu_virology,0-shot,accuracy,0.5060240963855421,0.0389221219533304
01-ai/Yi-9B,mmlu_professional_accounting,0-shot,accuracy,0.5602836879432624,0.0296099120755941
01-ai/Yi-9B,mmlu_college_physics,0-shot,accuracy,0.4607843137254901,0.0495985996638418
01-ai/Yi-9B,mmlu_high_school_physics,0-shot,accuracy,0.4105960264900662,0.0401668959484992
01-ai/Yi-9B,mmlu_high_school_biology,0-shot,accuracy,0.8483870967741935,0.0204026166544167
01-ai/Yi-9B,mmlu_college_biology,0-shot,accuracy,0.8263888888888888,0.0316747338379571
01-ai/Yi-9B,mmlu_anatomy,0-shot,accuracy,0.5851851851851851,0.042561937679014
01-ai/Yi-9B,mmlu_college_chemistry,0-shot,accuracy,0.53,0.0501613558046592
01-ai/Yi-9B,mmlu_computer_security,0-shot,accuracy,0.82,0.0386122919665369
01-ai/Yi-9B,mmlu_college_computer_science,0-shot,accuracy,0.61,0.0490207130000197
01-ai/Yi-9B,mmlu_astronomy,0-shot,accuracy,0.756578947368421,0.0349234966888423
01-ai/Yi-9B,mmlu_college_mathematics,0-shot,accuracy,0.44,0.0498887651569858
01-ai/Yi-9B,mmlu_conceptual_physics,0-shot,accuracy,0.7106382978723405,0.0296440065770096
01-ai/Yi-9B,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.0460566186471838
01-ai/Yi-9B,mmlu_high_school_computer_science,0-shot,accuracy,0.85,0.0358870281282636
01-ai/Yi-9B,mmlu_machine_learning,0-shot,accuracy,0.5714285714285714,0.0469711392301021
01-ai/Yi-9B,mmlu_high_school_chemistry,0-shot,accuracy,0.5862068965517241,0.0346530448840679
01-ai/Yi-9B,mmlu_high_school_statistics,0-shot,accuracy,0.6805555555555556,0.0317987634217685
01-ai/Yi-9B,mmlu_elementary_mathematics,0-shot,accuracy,0.5978835978835979,0.0252530325549976
01-ai/Yi-9B,mmlu_electrical_engineering,0-shot,accuracy,0.7310344827586207,0.0369518331165023
01-ai/Yi-9B,mmlu_high_school_mathematics,0-shot,accuracy,0.4037037037037037,0.0299148123422276
01-ai/Yi-9B,arc_challenge,25-shot,accuracy,0.5708191126279863,0.0144640858948706
01-ai/Yi-9B,arc_challenge,25-shot,acc_norm,0.6092150170648464,0.0142585638805137
01-ai/Yi-9B,hellaswag,10-shot,accuracy,0.5907189802828122,0.0049069629803282
01-ai/Yi-9B,hellaswag,10-shot,acc_norm,0.7898824935271859,0.004065592811696
01-ai/Yi-9B,truthfulqa_mc2,0-shot,accuracy,0.4241894920837938,0.014704655194116
01-ai/Yi-9B,truthfulqa_gen,0-shot,bleu_max,30.339360489971614,0.8638727006346777
01-ai/Yi-9B,truthfulqa_gen,0-shot,bleu_acc,0.4418604651162791,0.0173847674789862
01-ai/Yi-9B,truthfulqa_gen,0-shot,bleu_diff,1.3722910316943433,1.0157369467932418
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge1_max,54.26710006444303,0.972813269778497
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge1_acc,0.423500611995104,0.0172974214485347
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge1_diff,2.2642753980731283,1.2861505543673577
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge2_max,39.64525782768942,1.1477096591774218
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge2_acc,0.3574051407588739,0.0167765996767294
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge2_diff,1.7760823421690082,1.4195555435665703
01-ai/Yi-9B,truthfulqa_gen,0-shot,rougeL_max,51.8047383804989,0.9914125732871116
01-ai/Yi-9B,truthfulqa_gen,0-shot,rougeL_acc,0.4247246022031823,0.0173040009571674
01-ai/Yi-9B,truthfulqa_gen,0-shot,rougeL_diff,1.766099304090031,1.3064312031215068
01-ai/Yi-9B,truthfulqa_mc1,0-shot,accuracy,0.2778457772337821,0.0156809293640246
01-ai/Yi-9B,winogrande,5-shot,accuracy,0.7655880031570639,0.0119061301062379
01-ai/Yi-9B,gsm8k,5-shot,accuracy,0.5072024260803639,0.0137710557519728
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_world_religions,0-shot,accuracy,0.2280701754385964,0.0321809379560235
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_formal_logic,0-shot,accuracy,0.1587301587301587,0.0326845401301174
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_prehistory,0-shot,accuracy,0.1975308641975308,0.0221528899278989
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_moral_scenarios,0-shot,accuracy,0.235754189944134,0.0141963756862908
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_world_history,0-shot,accuracy,0.2616033755274262,0.0286095167169949
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_moral_disputes,0-shot,accuracy,0.2254335260115607,0.0224972301909675
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_professional_law,0-shot,accuracy,0.2281616688396349,0.0107179921920478
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_logical_fallacies,0-shot,accuracy,0.3067484662576687,0.0362308991572414
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_us_history,0-shot,accuracy,0.2156862745098039,0.0288674314498493
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_philosophy,0-shot,accuracy,0.2379421221864952,0.0241851506478187
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_jurisprudence,0-shot,accuracy,0.2037037037037037,0.0389354251882484
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_international_law,0-shot,accuracy,0.2975206611570248,0.0417334914808349
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_european_history,0-shot,accuracy,0.2363636363636363,0.0331750593000917
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2953367875647668,0.0329229663915514
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.1974789915966386,0.0258591641220514
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_geography,0-shot,accuracy,0.1969696969696969,0.0283356097324633
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_psychology,0-shot,accuracy,0.2403669724770642,0.018320607320964
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_public_relations,0-shot,accuracy,0.209090909090909,0.0389509101572413
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.25,0.0435194139889244
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_sociology,0-shot,accuracy,0.2189054726368159,0.029239174636647
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2358974358974359,0.0215259654074087
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_security_studies,0-shot,accuracy,0.3959183673469387,0.0313080289906568
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_professional_psychology,0-shot,accuracy,0.2516339869281045,0.0175558180913222
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_human_sexuality,0-shot,accuracy,0.2061068702290076,0.0354777100415946
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_econometrics,0-shot,accuracy,0.2280701754385964,0.0394715278266941
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_miscellaneous,0-shot,accuracy,0.2413793103448276,0.015302380123542
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_marketing,0-shot,accuracy,0.2777777777777778,0.0293431147980944
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_management,0-shot,accuracy,0.1553398058252427,0.0358659473857397
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_nutrition,0-shot,accuracy,0.2712418300653594,0.0254577566966678
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_human_aging,0-shot,accuracy,0.2914798206278027,0.0305002831765459
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_professional_medicine,0-shot,accuracy,0.4301470588235294,0.0300749719173028
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_college_medicine,0-shot,accuracy,0.2312138728323699,0.0321473730202947
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_business_ethics,0-shot,accuracy,0.29,0.0456048021572068
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.2377358490566037,0.0261998088075619
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_professional_accounting,0-shot,accuracy,0.2127659574468085,0.0244146129743077
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_college_physics,0-shot,accuracy,0.1862745098039215,0.0387395871414935
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_physics,0-shot,accuracy,0.2185430463576159,0.0337423555042569
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_biology,0-shot,accuracy,0.2096774193548387,0.0231578793490835
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_college_biology,0-shot,accuracy,0.25,0.036210341218895
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_anatomy,0-shot,accuracy,0.2814814814814815,0.0388500424580025
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_college_chemistry,0-shot,accuracy,0.21,0.0409360180740332
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_computer_security,0-shot,accuracy,0.23,0.042295258468165
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_college_computer_science,0-shot,accuracy,0.16,0.036845294917747
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_college_mathematics,0-shot,accuracy,0.2,0.0402015126103684
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_conceptual_physics,0-shot,accuracy,0.3021276595744681,0.0300175544718805
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.32,0.046882617226215
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.0449394906861353
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.0310898260029375
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_statistics,0-shot,accuracy,0.3101851851851852,0.0315469628565662
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.2116402116402116,0.0210373315052628
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_electrical_engineering,0-shot,accuracy,0.2068965517241379,0.0337567244956055
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.2444444444444444,0.0262027665346521
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,arc_challenge,25-shot,accuracy,0.204778156996587,0.0117925443385134
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,arc_challenge,25-shot,acc_norm,0.2542662116040955,0.0127249999451577
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,hellaswag,10-shot,accuracy,0.3151762597092212,0.0046363655348197
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,hellaswag,10-shot,acc_norm,0.3642700657239593,0.0048024139199326
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_mc2,0-shot,accuracy,0.4119951860487219,0.0149363810952365
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,bleu_max,15.95698975333046,0.6081263704679888
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,bleu_acc,0.3072215422276622,0.016150201321323
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,bleu_diff,-3.7245464479483457,0.5332523284321755
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge1_max,38.49688357363435,0.8065996999065227
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge1_acc,0.2876376988984088,0.0158463151013948
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge1_diff,-5.987855138021341,0.6547523558599384
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge2_max,21.23103016715041,0.8532924387537144
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge2_acc,0.1995104039167686,0.0139899299675596
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge2_diff,-6.083463853920763,0.7019618612886404
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rougeL_max,35.43473176198696,0.7987346666351347
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rougeL_acc,0.2864137086903305,0.0158261424395023
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rougeL_diff,-6.12901789205229,0.6371927433731587
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,truthfulqa_mc1,0-shot,accuracy,0.244798041615667,0.0150518694867149
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,winogrande,5-shot,accuracy,0.505130228887135,0.0140517459617905
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,gsm8k,5-shot,accuracy,0.0144,
facebook/opt-125m,drop,3-shot,accuracy,0.0009437919463087,0.0003144653119413
facebook/opt-125m,drop,3-shot,f1,0.0336220637583893,0.0010370707914668
facebook/opt-125m,gsm8k,5-shot,accuracy,0.0181956027293404,0.0036816118940738
facebook/opt-125m,winogrande,5-shot,accuracy,0.5090765588003157,0.0140501700944977
facebook/opt-125m,arc:challenge,25-shot,accuracy,0.2039249146757679,0.0117742624787022
facebook/opt-125m,arc:challenge,25-shot,acc_norm,0.2286689419795221,0.0122728535825407
facebook/opt-125m,hellaswag,10-shot,accuracy,0.291077474606652,0.0045333077585213
facebook/opt-125m,hellaswag,10-shot,acc_norm,0.3175662218681537,0.0046457830480046
facebook/opt-125m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
facebook/opt-125m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
facebook/opt-125m,hendrycksTest-anatomy,5-shot,accuracy,0.2444444444444444,0.0371253783361486
facebook/opt-125m,hendrycksTest-anatomy,5-shot,acc_norm,0.2444444444444444,0.0371253783361486
facebook/opt-125m,hendrycksTest-astronomy,5-shot,accuracy,0.2697368421052631,0.0361178056028489
facebook/opt-125m,hendrycksTest-astronomy,5-shot,acc_norm,0.2697368421052631,0.0361178056028489
facebook/opt-125m,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.0409360180740332
facebook/opt-125m,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.0409360180740332
facebook/opt-125m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2150943396226415,0.0252883945028913
facebook/opt-125m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2150943396226415,0.0252883945028913
facebook/opt-125m,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.0347659010430413
facebook/opt-125m,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.0347659010430413
facebook/opt-125m,hendrycksTest-college_chemistry,5-shot,accuracy,0.28,0.0451260859854212
facebook/opt-125m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.28,0.0451260859854212
facebook/opt-125m,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.047258156262526
facebook/opt-125m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.047258156262526
facebook/opt-125m,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.0440844002276807
facebook/opt-125m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.0440844002276807
facebook/opt-125m,hendrycksTest-college_medicine,5-shot,accuracy,0.2023121387283237,0.0306311455391988
facebook/opt-125m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2023121387283237,0.0306311455391988
facebook/opt-125m,hendrycksTest-college_physics,5-shot,accuracy,0.3725490196078431,0.0481084014808263
facebook/opt-125m,hendrycksTest-college_physics,5-shot,acc_norm,0.3725490196078431,0.0481084014808263
facebook/opt-125m,hendrycksTest-computer_security,5-shot,accuracy,0.18,0.0386122919665369
facebook/opt-125m,hendrycksTest-computer_security,5-shot,acc_norm,0.18,0.0386122919665369
facebook/opt-125m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3191489361702128,0.03047297336338
facebook/opt-125m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3191489361702128,0.03047297336338
facebook/opt-125m,hendrycksTest-econometrics,5-shot,accuracy,0.2280701754385964,0.0394715278266941
facebook/opt-125m,hendrycksTest-econometrics,5-shot,acc_norm,0.2280701754385964,0.0394715278266941
facebook/opt-125m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
facebook/opt-125m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
facebook/opt-125m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.0224945107675031
facebook/opt-125m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.0224945107675031
facebook/opt-125m,hendrycksTest-formal_logic,5-shot,accuracy,0.1428571428571428,0.031298431857438
facebook/opt-125m,hendrycksTest-formal_logic,5-shot,acc_norm,0.1428571428571428,0.031298431857438
facebook/opt-125m,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.0386122919665369
facebook/opt-125m,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.0386122919665369
facebook/opt-125m,hendrycksTest-high_school_biology,5-shot,accuracy,0.3161290322580645,0.0264508744890427
facebook/opt-125m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3161290322580645,0.0264508744890427
facebook/opt-125m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2955665024630542,0.0321049443375145
facebook/opt-125m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2955665024630542,0.0321049443375145
facebook/opt-125m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.0394277244403662
facebook/opt-125m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.0394277244403662
facebook/opt-125m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2121212121212121,0.0319227156954829
facebook/opt-125m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2121212121212121,0.0319227156954829
facebook/opt-125m,hendrycksTest-high_school_geography,5-shot,accuracy,0.2727272727272727,0.0317307123907172
facebook/opt-125m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2727272727272727,0.0317307123907172
facebook/opt-125m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3678756476683937,0.0348017566846603
facebook/opt-125m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3678756476683937,0.0348017566846603
facebook/opt-125m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3435897435897436,0.0240786965806354
facebook/opt-125m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3435897435897436,0.0240786965806354
facebook/opt-125m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
facebook/opt-125m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
facebook/opt-125m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3529411764705882,0.0310419413040592
facebook/opt-125m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3529411764705882,0.0310419413040592
facebook/opt-125m,hendrycksTest-high_school_physics,5-shot,accuracy,0.3112582781456953,0.0378044585052673
facebook/opt-125m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3112582781456953,0.0378044585052673
facebook/opt-125m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2275229357798165,0.0179744635787765
facebook/opt-125m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2275229357798165,0.0179744635787765
facebook/opt-125m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-125m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
facebook/opt-125m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
facebook/opt-125m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2598039215686274,0.0307785546786932
facebook/opt-125m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2573839662447257,0.0284588209914603
facebook/opt-125m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2573839662447257,0.0284588209914603
facebook/opt-125m,hendrycksTest-human_aging,5-shot,accuracy,0.2017937219730941,0.0269361119128022
facebook/opt-125m,hendrycksTest-human_aging,5-shot,acc_norm,0.2017937219730941,0.0269361119128022
facebook/opt-125m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
facebook/opt-125m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
facebook/opt-125m,hendrycksTest-international_law,5-shot,accuracy,0.3801652892561983,0.0443132450196843
facebook/opt-125m,hendrycksTest-international_law,5-shot,acc_norm,0.3801652892561983,0.0443132450196843
facebook/opt-125m,hendrycksTest-jurisprudence,5-shot,accuracy,0.2129629629629629,0.0395783547198098
facebook/opt-125m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2129629629629629,0.0395783547198098
facebook/opt-125m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2208588957055214,0.0325917739274217
facebook/opt-125m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2208588957055214,0.0325917739274217
facebook/opt-125m,hendrycksTest-machine_learning,5-shot,accuracy,0.1517857142857142,0.0340570283818569
facebook/opt-125m,hendrycksTest-machine_learning,5-shot,acc_norm,0.1517857142857142,0.0340570283818569
facebook/opt-125m,hendrycksTest-management,5-shot,accuracy,0.1844660194174757,0.0384042362728827
facebook/opt-125m,hendrycksTest-management,5-shot,acc_norm,0.1844660194174757,0.0384042362728827
facebook/opt-125m,hendrycksTest-marketing,5-shot,accuracy,0.1965811965811965,0.0260353860989512
facebook/opt-125m,hendrycksTest-marketing,5-shot,acc_norm,0.1965811965811965,0.0260353860989512
facebook/opt-125m,hendrycksTest-medical_genetics,5-shot,accuracy,0.34,0.0476095228569523
facebook/opt-125m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.34,0.0476095228569523
facebook/opt-125m,hendrycksTest-miscellaneous,5-shot,accuracy,0.2541507024265645,0.0155692546920457
facebook/opt-125m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2541507024265645,0.0155692546920457
facebook/opt-125m,hendrycksTest-moral_disputes,5-shot,accuracy,0.2312138728323699,0.0226986571678557
facebook/opt-125m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2312138728323699,0.0226986571678557
facebook/opt-125m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/opt-125m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
facebook/opt-125m,hendrycksTest-nutrition,5-shot,accuracy,0.2712418300653594,0.0254577566966678
facebook/opt-125m,hendrycksTest-nutrition,5-shot,acc_norm,0.2712418300653594,0.0254577566966678
facebook/opt-125m,hendrycksTest-philosophy,5-shot,accuracy,0.2379421221864952,0.0241851506478187
facebook/opt-125m,hendrycksTest-philosophy,5-shot,acc_norm,0.2379421221864952,0.0241851506478187
facebook/opt-125m,hendrycksTest-prehistory,5-shot,accuracy,0.2932098765432099,0.0253298881719009
facebook/opt-125m,hendrycksTest-prehistory,5-shot,acc_norm,0.2932098765432099,0.0253298881719009
facebook/opt-125m,hendrycksTest-professional_accounting,5-shot,accuracy,0.2659574468085106,0.0263580656988805
facebook/opt-125m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2659574468085106,0.0263580656988805
facebook/opt-125m,hendrycksTest-professional_law,5-shot,accuracy,0.2529335071707953,0.0111022687138399
facebook/opt-125m,hendrycksTest-professional_law,5-shot,acc_norm,0.2529335071707953,0.0111022687138399
facebook/opt-125m,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.0302114796091215
facebook/opt-125m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.0302114796091215
facebook/opt-125m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2222222222222222,0.0168190283757363
facebook/opt-125m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2222222222222222,0.0168190283757363
facebook/opt-125m,hendrycksTest-public_relations,5-shot,accuracy,0.2272727272727272,0.0401396455407277
facebook/opt-125m,hendrycksTest-public_relations,5-shot,acc_norm,0.2272727272727272,0.0401396455407277
facebook/opt-125m,hendrycksTest-security_studies,5-shot,accuracy,0.2489795918367346,0.0276829795229602
facebook/opt-125m,hendrycksTest-security_studies,5-shot,acc_norm,0.2489795918367346,0.0276829795229602
facebook/opt-125m,hendrycksTest-sociology,5-shot,accuracy,0.2338308457711442,0.0299294154083483
facebook/opt-125m,hendrycksTest-sociology,5-shot,acc_norm,0.2338308457711442,0.0299294154083483
facebook/opt-125m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.32,0.046882617226215
facebook/opt-125m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.32,0.046882617226215
facebook/opt-125m,hendrycksTest-virology,5-shot,accuracy,0.2048192771084337,0.0314178429166392
facebook/opt-125m,hendrycksTest-virology,5-shot,acc_norm,0.2048192771084337,0.0314178429166392
facebook/opt-125m,hendrycksTest-world_religions,5-shot,accuracy,0.175438596491228,0.0291708855007276
facebook/opt-125m,hendrycksTest-world_religions,5-shot,acc_norm,0.175438596491228,0.0291708855007276
facebook/opt-125m,truthfulqa:mc,0-shot,mc1,0.2399020807833537,0.0149488126790621
facebook/opt-125m,truthfulqa:mc,0-shot,mc2,0.4286875802615585,0.0150584695912639
jisukim8873/falcon-7B-case-0,arc:challenge,25-shot,accuracy,0.4505119453924914,0.0145396460984716
jisukim8873/falcon-7B-case-0,arc:challenge,25-shot,acc_norm,0.4914675767918088,0.0146092631656321
jisukim8873/falcon-7B-case-0,hellaswag,10-shot,accuracy,0.5967934674367655,0.0048953903414456
jisukim8873/falcon-7B-case-0,hellaswag,10-shot,acc_norm,0.782513443537144,0.0041169313831573
jisukim8873/falcon-7B-case-0,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-0,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-0,hendrycksTest-anatomy,5-shot,accuracy,0.3851851851851852,0.0420392104015627
jisukim8873/falcon-7B-case-0,hendrycksTest-anatomy,5-shot,acc_norm,0.3851851851851852,0.0420392104015627
jisukim8873/falcon-7B-case-0,hendrycksTest-astronomy,5-shot,accuracy,0.2302631578947368,0.0342605942440316
jisukim8873/falcon-7B-case-0,hendrycksTest-astronomy,5-shot,acc_norm,0.2302631578947368,0.0342605942440316
jisukim8873/falcon-7B-case-0,hendrycksTest-business_ethics,5-shot,accuracy,0.25,0.0435194139889244
jisukim8873/falcon-7B-case-0,hendrycksTest-business_ethics,5-shot,acc_norm,0.25,0.0435194139889244
jisukim8873/falcon-7B-case-0,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2943396226415094,0.0280491863156952
jisukim8873/falcon-7B-case-0,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2943396226415094,0.0280491863156952
jisukim8873/falcon-7B-case-0,hendrycksTest-college_biology,5-shot,accuracy,0.2777777777777778,0.0374555479146245
jisukim8873/falcon-7B-case-0,hendrycksTest-college_biology,5-shot,acc_norm,0.2777777777777778,0.0374555479146245
jisukim8873/falcon-7B-case-0,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-0,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-0,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-0,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-0,hendrycksTest-college_mathematics,5-shot,accuracy,0.21,0.0409360180740332
jisukim8873/falcon-7B-case-0,hendrycksTest-college_mathematics,5-shot,acc_norm,0.21,0.0409360180740332
jisukim8873/falcon-7B-case-0,hendrycksTest-college_medicine,5-shot,accuracy,0.2312138728323699,0.0321473730202947
jisukim8873/falcon-7B-case-0,hendrycksTest-college_medicine,5-shot,acc_norm,0.2312138728323699,0.0321473730202947
jisukim8873/falcon-7B-case-0,hendrycksTest-college_physics,5-shot,accuracy,0.1764705882352941,0.037932811853078
jisukim8873/falcon-7B-case-0,hendrycksTest-college_physics,5-shot,acc_norm,0.1764705882352941,0.037932811853078
jisukim8873/falcon-7B-case-0,hendrycksTest-computer_security,5-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-0,hendrycksTest-computer_security,5-shot,acc_norm,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-0,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3446808510638298,0.0310689859631221
jisukim8873/falcon-7B-case-0,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3446808510638298,0.0310689859631221
jisukim8873/falcon-7B-case-0,hendrycksTest-econometrics,5-shot,accuracy,0.3508771929824561,0.0448953935027069
jisukim8873/falcon-7B-case-0,hendrycksTest-econometrics,5-shot,acc_norm,0.3508771929824561,0.0448953935027069
jisukim8873/falcon-7B-case-0,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2689655172413793,0.0369518331165023
jisukim8873/falcon-7B-case-0,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2689655172413793,0.0369518331165023
jisukim8873/falcon-7B-case-0,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
jisukim8873/falcon-7B-case-0,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
jisukim8873/falcon-7B-case-0,hendrycksTest-formal_logic,5-shot,accuracy,0.1666666666666666,0.0333333333333333
jisukim8873/falcon-7B-case-0,hendrycksTest-formal_logic,5-shot,acc_norm,0.1666666666666666,0.0333333333333333
jisukim8873/falcon-7B-case-0,hendrycksTest-global_facts,5-shot,accuracy,0.27,0.0446196043338474
jisukim8873/falcon-7B-case-0,hendrycksTest-global_facts,5-shot,acc_norm,0.27,0.0446196043338474
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_biology,5-shot,accuracy,0.3258064516129032,0.0266620105785671
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3258064516129032,0.0266620105785671
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2807881773399014,0.031618563353586
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2807881773399014,0.031618563353586
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3515151515151515,0.0372820699868265
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3515151515151515,0.0372820699868265
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_geography,5-shot,accuracy,0.2727272727272727,0.0317307123907172
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2727272727272727,0.0317307123907172
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2590673575129533,0.0316187791793541
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2590673575129533,0.0316187791793541
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2282051282051282,0.0212783938635862
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2282051282051282,0.0212783938635862
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2444444444444444,0.0262027665346521
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2444444444444444,0.0262027665346521
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2436974789915966,0.0278868280783805
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2436974789915966,0.0278868280783805
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2807339449541284,0.0192660550458716
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2807339449541284,0.0192660550458716
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1898148148148148,0.0267447148346919
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1898148148148148,0.0267447148346919
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2794117647058823,0.0314932810450795
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2794117647058823,0.0314932810450795
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3080168776371308,0.0300523893356057
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3080168776371308,0.0300523893356057
jisukim8873/falcon-7B-case-0,hendrycksTest-human_aging,5-shot,accuracy,0.4170403587443946,0.0330926693607172
jisukim8873/falcon-7B-case-0,hendrycksTest-human_aging,5-shot,acc_norm,0.4170403587443946,0.0330926693607172
jisukim8873/falcon-7B-case-0,hendrycksTest-human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
jisukim8873/falcon-7B-case-0,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2442748091603053,0.0376833595972874
jisukim8873/falcon-7B-case-0,hendrycksTest-international_law,5-shot,accuracy,0.3553719008264462,0.0436923632657398
jisukim8873/falcon-7B-case-0,hendrycksTest-international_law,5-shot,acc_norm,0.3553719008264462,0.0436923632657398
jisukim8873/falcon-7B-case-0,hendrycksTest-jurisprudence,5-shot,accuracy,0.3333333333333333,0.0455723951349775
jisukim8873/falcon-7B-case-0,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3333333333333333,0.0455723951349775
jisukim8873/falcon-7B-case-0,hendrycksTest-logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
jisukim8873/falcon-7B-case-0,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.263803680981595,0.0346241993161562
jisukim8873/falcon-7B-case-0,hendrycksTest-machine_learning,5-shot,accuracy,0.3660714285714285,0.0457237235873743
jisukim8873/falcon-7B-case-0,hendrycksTest-machine_learning,5-shot,acc_norm,0.3660714285714285,0.0457237235873743
jisukim8873/falcon-7B-case-0,hendrycksTest-management,5-shot,accuracy,0.3203883495145631,0.0462028408228004
jisukim8873/falcon-7B-case-0,hendrycksTest-management,5-shot,acc_norm,0.3203883495145631,0.0462028408228004
jisukim8873/falcon-7B-case-0,hendrycksTest-marketing,5-shot,accuracy,0.3333333333333333,0.0308827369741386
jisukim8873/falcon-7B-case-0,hendrycksTest-marketing,5-shot,acc_norm,0.3333333333333333,0.0308827369741386
jisukim8873/falcon-7B-case-0,hendrycksTest-medical_genetics,5-shot,accuracy,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-0,hendrycksTest-medical_genetics,5-shot,acc_norm,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-0,hendrycksTest-miscellaneous,5-shot,accuracy,0.3627075351213282,0.0171927086746023
jisukim8873/falcon-7B-case-0,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3627075351213282,0.0171927086746023
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_disputes,5-shot,accuracy,0.3236994219653179,0.0251901813276084
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3236994219653179,0.0251901813276084
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_scenarios,5-shot,accuracy,0.259217877094972,0.0146557808374977
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.259217877094972,0.0146557808374977
jisukim8873/falcon-7B-case-0,hendrycksTest-nutrition,5-shot,accuracy,0.3137254901960784,0.0265689210154571
jisukim8873/falcon-7B-case-0,hendrycksTest-nutrition,5-shot,acc_norm,0.3137254901960784,0.0265689210154571
jisukim8873/falcon-7B-case-0,hendrycksTest-philosophy,5-shot,accuracy,0.3054662379421222,0.0261605844501404
jisukim8873/falcon-7B-case-0,hendrycksTest-philosophy,5-shot,acc_norm,0.3054662379421222,0.0261605844501404
jisukim8873/falcon-7B-case-0,hendrycksTest-prehistory,5-shot,accuracy,0.2808641975308642,0.0250064697557992
jisukim8873/falcon-7B-case-0,hendrycksTest-prehistory,5-shot,acc_norm,0.2808641975308642,0.0250064697557992
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_accounting,5-shot,accuracy,0.2659574468085106,0.0263580656988805
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2659574468085106,0.0263580656988805
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_law,5-shot,accuracy,0.2764015645371577,0.0114221531945535
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_law,5-shot,acc_norm,0.2764015645371577,0.0114221531945535
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_medicine,5-shot,accuracy,0.1801470588235294,0.0233451636165448
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1801470588235294,0.0233451636165448
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_psychology,5-shot,accuracy,0.315359477124183,0.0187980862848868
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_psychology,5-shot,acc_norm,0.315359477124183,0.0187980862848868
jisukim8873/falcon-7B-case-0,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.0435027144292324
jisukim8873/falcon-7B-case-0,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.0435027144292324
jisukim8873/falcon-7B-case-0,hendrycksTest-security_studies,5-shot,accuracy,0.236734693877551,0.0272128358840731
jisukim8873/falcon-7B-case-0,hendrycksTest-security_studies,5-shot,acc_norm,0.236734693877551,0.0272128358840731
jisukim8873/falcon-7B-case-0,hendrycksTest-sociology,5-shot,accuracy,0.2736318407960199,0.031524391865554
jisukim8873/falcon-7B-case-0,hendrycksTest-sociology,5-shot,acc_norm,0.2736318407960199,0.031524391865554
jisukim8873/falcon-7B-case-0,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.44,0.0498887651569858
jisukim8873/falcon-7B-case-0,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.44,0.0498887651569858
jisukim8873/falcon-7B-case-0,hendrycksTest-virology,5-shot,accuracy,0.3132530120481928,0.0361080501803102
jisukim8873/falcon-7B-case-0,hendrycksTest-virology,5-shot,acc_norm,0.3132530120481928,0.0361080501803102
jisukim8873/falcon-7B-case-0,hendrycksTest-world_religions,5-shot,accuracy,0.391812865497076,0.037439798259264
jisukim8873/falcon-7B-case-0,hendrycksTest-world_religions,5-shot,acc_norm,0.391812865497076,0.037439798259264
jisukim8873/falcon-7B-case-0,truthfulqa:mc,0-shot,mc1,0.2472460220318237,0.0151024047973596
jisukim8873/falcon-7B-case-0,truthfulqa:mc,0-shot,mc2,0.3617782578587819,0.0143039527862541
jisukim8873/falcon-7B-case-0,winogrande,5-shot,accuracy,0.7182320441988951,0.0126433260118529
jisukim8873/falcon-7B-case-0,gsm8k,5-shot,accuracy,0.0864291129643669,0.0077400443371037
jisukim8873/falcon-7B-case-0,minerva_math_precalc,5-shot,accuracy,0.0073260073260073,0.003652908089383
jisukim8873/falcon-7B-case-0,minerva_math_prealgebra,5-shot,accuracy,0.0344431687715269,0.0061827380104872
jisukim8873/falcon-7B-case-0,minerva_math_num_theory,5-shot,accuracy,0.0222222222222222,0.0063492063492063
jisukim8873/falcon-7B-case-0,minerva_math_intermediate_algebra,5-shot,accuracy,0.0110741971207087,0.0034844537978317
jisukim8873/falcon-7B-case-0,minerva_math_geometry,5-shot,accuracy,0.0208768267223382,0.0065393857958139
jisukim8873/falcon-7B-case-0,minerva_math_counting_and_prob,5-shot,accuracy,0.0126582278481012,0.0051403138895788
jisukim8873/falcon-7B-case-0,minerva_math_algebra,5-shot,accuracy,0.0151642796967144,0.0035485460431325
jisukim8873/falcon-7B-case-0,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-0,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-0,arithmetic_3da,5-shot,accuracy,0.041,0.004435012363831
jisukim8873/falcon-7B-case-0,arithmetic_3ds,5-shot,accuracy,0.105,0.0068564572122015
jisukim8873/falcon-7B-case-0,arithmetic_4da,5-shot,accuracy,0.002,0.0009992493430694
jisukim8873/falcon-7B-case-0,arithmetic_2ds,5-shot,accuracy,0.261,0.0098228175118923
jisukim8873/falcon-7B-case-0,arithmetic_5ds,5-shot,accuracy,0.0085,0.0020532859010609
jisukim8873/falcon-7B-case-0,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-0,arithmetic_1dc,5-shot,accuracy,0.095,0.0065581250752216
jisukim8873/falcon-7B-case-0,arithmetic_4ds,5-shot,accuracy,0.0595,0.0052909235428201
jisukim8873/falcon-7B-case-0,arithmetic_2dm,5-shot,accuracy,0.0805,0.0060850956602667
jisukim8873/falcon-7B-case-0,arithmetic_2da,5-shot,accuracy,0.3425,0.010613821253479
jisukim8873/falcon-7B-case-0,gsm8k_cot,5-shot,accuracy,0.1084154662623199,0.0085638525066274
jisukim8873/falcon-7B-case-0,anli_r2,0-shot,brier_score,0.990088442664038,
jisukim8873/falcon-7B-case-0,anli_r3,0-shot,brier_score,0.899582509397577,
jisukim8873/falcon-7B-case-0,anli_r1,0-shot,brier_score,1.0281641108435755,
jisukim8873/falcon-7B-case-0,xnli_eu,0-shot,brier_score,1.0537250229688973,
jisukim8873/falcon-7B-case-0,xnli_vi,0-shot,brier_score,0.9839825713551124,
jisukim8873/falcon-7B-case-0,xnli_ru,0-shot,brier_score,0.8064365352145315,
jisukim8873/falcon-7B-case-0,xnli_zh,0-shot,brier_score,1.0075338003232164,
jisukim8873/falcon-7B-case-0,xnli_tr,0-shot,brier_score,1.0171633908679167,
jisukim8873/falcon-7B-case-0,xnli_fr,0-shot,brier_score,0.7287345172911353,
jisukim8873/falcon-7B-case-0,xnli_en,0-shot,brier_score,0.6626521979549853,
jisukim8873/falcon-7B-case-0,xnli_ur,0-shot,brier_score,1.2911106062968751,
jisukim8873/falcon-7B-case-0,xnli_ar,0-shot,brier_score,1.3000289724546177,
jisukim8873/falcon-7B-case-0,xnli_de,0-shot,brier_score,0.8378848288444989,
jisukim8873/falcon-7B-case-0,xnli_hi,0-shot,brier_score,1.200629112469935,
jisukim8873/falcon-7B-case-0,xnli_es,0-shot,brier_score,0.8086580822433482,
jisukim8873/falcon-7B-case-0,xnli_bg,0-shot,brier_score,0.925558474060932,
jisukim8873/falcon-7B-case-0,xnli_sw,0-shot,brier_score,0.9589266465708508,
jisukim8873/falcon-7B-case-0,xnli_el,0-shot,brier_score,0.8660838695165314,
jisukim8873/falcon-7B-case-0,xnli_th,0-shot,brier_score,0.9644722384510832,
jisukim8873/falcon-7B-case-0,logiqa2,0-shot,brier_score,1.0462410059838376,
jisukim8873/falcon-7B-case-0,mathqa,0-shot,brier_score,0.9489730788685252,
jisukim8873/falcon-7B-case-0,lambada_standard,0-shot,perplexity,4.28074845242095,0.0951415273721243
jisukim8873/falcon-7B-case-0,lambada_standard,0-shot,accuracy,0.6621385600620997,0.0065895510546543
jisukim8873/falcon-7B-case-0,lambada_openai,0-shot,perplexity,3.416541255218945,0.072432987277523
jisukim8873/falcon-7B-case-0,lambada_openai,0-shot,accuracy,0.7288957888608577,0.0061931689138917
mosaicml/mpt-7b-instruct,minerva_math_precalc,5-shot,accuracy,0.0293040293040293,0.0072244873054596
mosaicml/mpt-7b-instruct,minerva_math_prealgebra,5-shot,accuracy,0.0677382319173364,0.0085197379927099
mosaicml/mpt-7b-instruct,minerva_math_num_theory,5-shot,accuracy,0.0444444444444444,0.008876511687867
mosaicml/mpt-7b-instruct,minerva_math_intermediate_algebra,5-shot,accuracy,0.0321151716500553,0.0058703459557963
mosaicml/mpt-7b-instruct,minerva_math_geometry,5-shot,accuracy,0.0501043841336116,0.0099784217843236
mosaicml/mpt-7b-instruct,minerva_math_counting_and_prob,5-shot,accuracy,0.0590717299578059,0.0108402059416553
mosaicml/mpt-7b-instruct,minerva_math_algebra,5-shot,accuracy,0.0404380791912384,0.0057199129215533
mosaicml/mpt-7b-instruct,fld_default,0-shot,accuracy,0.0,
mosaicml/mpt-7b-instruct,fld_star,0-shot,accuracy,0.0,
mosaicml/mpt-7b-instruct,arithmetic_3da,5-shot,accuracy,0.0505,0.0048976390673687
mosaicml/mpt-7b-instruct,arithmetic_3ds,5-shot,accuracy,0.0445,0.0046119963416213
mosaicml/mpt-7b-instruct,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521
mosaicml/mpt-7b-instruct,arithmetic_2ds,5-shot,accuracy,0.2525,0.0097169483142738
mosaicml/mpt-7b-instruct,arithmetic_5ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b-instruct,arithmetic_5da,5-shot,accuracy,0.0005,0.0005
mosaicml/mpt-7b-instruct,arithmetic_1dc,5-shot,accuracy,0.0135,0.0025811249685073
mosaicml/mpt-7b-instruct,arithmetic_4ds,5-shot,accuracy,0.002,0.0009992493430694
mosaicml/mpt-7b-instruct,arithmetic_2dm,5-shot,accuracy,0.067,0.0055920600468687
mosaicml/mpt-7b-instruct,arithmetic_2da,5-shot,accuracy,0.1985,0.0089212481937601
mosaicml/mpt-7b-instruct,gsm8k_cot,5-shot,accuracy,0.0697498104624715,0.0070163895710138
mosaicml/mpt-7b-instruct,gsm8k,5-shot,accuracy,0.0280515542077331,0.0045482295338363
mosaicml/mpt-7b-instruct,anli_r2,0-shot,brier_score,0.7896846312992308,
mosaicml/mpt-7b-instruct,anli_r3,0-shot,brier_score,0.7829638521132672,
mosaicml/mpt-7b-instruct,anli_r1,0-shot,brier_score,0.815948761399128,
mosaicml/mpt-7b-instruct,xnli_eu,0-shot,brier_score,1.0863038087250383,
mosaicml/mpt-7b-instruct,xnli_vi,0-shot,brier_score,1.0752130995950049,
mosaicml/mpt-7b-instruct,xnli_ru,0-shot,brier_score,0.8232916030715194,
mosaicml/mpt-7b-instruct,xnli_zh,0-shot,brier_score,1.0965246448802113,
mosaicml/mpt-7b-instruct,xnli_tr,0-shot,brier_score,0.9706318085768088,
mosaicml/mpt-7b-instruct,xnli_fr,0-shot,brier_score,0.7747493883608759,
mosaicml/mpt-7b-instruct,xnli_en,0-shot,brier_score,0.6437717014643014,
mosaicml/mpt-7b-instruct,xnli_ur,0-shot,brier_score,1.31440196299888,
mosaicml/mpt-7b-instruct,xnli_ar,0-shot,brier_score,1.264810737749815,
mosaicml/mpt-7b-instruct,xnli_de,0-shot,brier_score,0.8408303311766104,
mosaicml/mpt-7b-instruct,xnli_hi,0-shot,brier_score,1.0110715039028983,
mosaicml/mpt-7b-instruct,xnli_es,0-shot,brier_score,0.8694036206904238,
mosaicml/mpt-7b-instruct,xnli_bg,0-shot,brier_score,0.880489846093007,
mosaicml/mpt-7b-instruct,xnli_sw,0-shot,brier_score,1.1727384594477277,
mosaicml/mpt-7b-instruct,xnli_el,0-shot,brier_score,1.037015650900093,
mosaicml/mpt-7b-instruct,xnli_th,0-shot,brier_score,0.9186444184570116,
mosaicml/mpt-7b-instruct,logiqa2,0-shot,brier_score,1.0058818905060056,
mosaicml/mpt-7b-instruct,mathqa,0-shot,brier_score,0.9402290653146276,
mosaicml/mpt-7b-instruct,lambada_standard,0-shot,perplexity,4.596967497234068,0.1131243101678481
mosaicml/mpt-7b-instruct,lambada_standard,0-shot,accuracy,0.6262371434116049,0.0067403050865517
mosaicml/mpt-7b-instruct,lambada_openai,0-shot,perplexity,3.6623178667968417,0.0865005544070399
mosaicml/mpt-7b-instruct,lambada_openai,0-shot,accuracy,0.673394139336309,0.0065336930212616
mosaicml/mpt-7b-instruct,mmlu_world_religions,0-shot,accuracy,0.2222222222222222,0.0318857801768639
mosaicml/mpt-7b-instruct,mmlu_formal_logic,0-shot,accuracy,0.2063492063492063,0.0361960452412425
mosaicml/mpt-7b-instruct,mmlu_prehistory,0-shot,accuracy,0.324074074074074,0.0260417662027171
mosaicml/mpt-7b-instruct,mmlu_moral_scenarios,0-shot,accuracy,0.241340782122905,0.0143109995479614
mosaicml/mpt-7b-instruct,mmlu_high_school_world_history,0-shot,accuracy,0.2447257383966244,0.0279856993870364
mosaicml/mpt-7b-instruct,mmlu_moral_disputes,0-shot,accuracy,0.3208092485549133,0.0251310002336479
mosaicml/mpt-7b-instruct,mmlu_professional_law,0-shot,accuracy,0.2842242503259452,0.011519880596516
mosaicml/mpt-7b-instruct,mmlu_logical_fallacies,0-shot,accuracy,0.3128834355828221,0.036429145782924
mosaicml/mpt-7b-instruct,mmlu_high_school_us_history,0-shot,accuracy,0.2450980392156862,0.0301902824535019
mosaicml/mpt-7b-instruct,mmlu_philosophy,0-shot,accuracy,0.315112540192926,0.0263852737034644
mosaicml/mpt-7b-instruct,mmlu_jurisprudence,0-shot,accuracy,0.3148148148148148,0.0448993107359131
mosaicml/mpt-7b-instruct,mmlu_international_law,0-shot,accuracy,0.3223140495867768,0.0426641636335216
mosaicml/mpt-7b-instruct,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.0354656301962433
mosaicml/mpt-7b-instruct,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.0346971379170437
mosaicml/mpt-7b-instruct,mmlu_high_school_microeconomics,0-shot,accuracy,0.3319327731092437,0.0305886970137836
mosaicml/mpt-7b-instruct,mmlu_high_school_geography,0-shot,accuracy,0.3434343434343434,0.0338320122324444
mosaicml/mpt-7b-instruct,mmlu_high_school_psychology,0-shot,accuracy,0.3321100917431193,0.0201926829854233
mosaicml/mpt-7b-instruct,mmlu_public_relations,0-shot,accuracy,0.3727272727272727,0.0463138131942546
mosaicml/mpt-7b-instruct,mmlu_us_foreign_policy,0-shot,accuracy,0.36,0.0482418151324421
mosaicml/mpt-7b-instruct,mmlu_sociology,0-shot,accuracy,0.2487562189054726,0.0305676759389167
mosaicml/mpt-7b-instruct,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3538461538461538,0.0242437839940621
mosaicml/mpt-7b-instruct,mmlu_security_studies,0-shot,accuracy,0.4285714285714285,0.0316809116123388
mosaicml/mpt-7b-instruct,mmlu_professional_psychology,0-shot,accuracy,0.315359477124183,0.0187980862848868
mosaicml/mpt-7b-instruct,mmlu_human_sexuality,0-shot,accuracy,0.3969465648854962,0.0429113567100922
mosaicml/mpt-7b-instruct,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.0418577442402205
mosaicml/mpt-7b-instruct,mmlu_miscellaneous,0-shot,accuracy,0.334610472541507,0.0168734686415921
mosaicml/mpt-7b-instruct,mmlu_marketing,0-shot,accuracy,0.3376068376068376,0.0309802969926185
mosaicml/mpt-7b-instruct,mmlu_management,0-shot,accuracy,0.2718446601941747,0.0440526802414092
mosaicml/mpt-7b-instruct,mmlu_nutrition,0-shot,accuracy,0.3169934640522875,0.0266432784745087
mosaicml/mpt-7b-instruct,mmlu_medical_genetics,0-shot,accuracy,0.37,0.0485236587093909
mosaicml/mpt-7b-instruct,mmlu_human_aging,0-shot,accuracy,0.3408071748878923,0.0318114974705536
mosaicml/mpt-7b-instruct,mmlu_professional_medicine,0-shot,accuracy,0.2389705882352941,0.025905280644893
mosaicml/mpt-7b-instruct,mmlu_college_medicine,0-shot,accuracy,0.3179190751445087,0.0355068398916558
mosaicml/mpt-7b-instruct,mmlu_business_ethics,0-shot,accuracy,0.29,0.0456048021572068
mosaicml/mpt-7b-instruct,mmlu_clinical_knowledge,0-shot,accuracy,0.3547169811320754,0.0294451753281995
mosaicml/mpt-7b-instruct,mmlu_global_facts,0-shot,accuracy,0.29,0.0456048021572068
mosaicml/mpt-7b-instruct,mmlu_virology,0-shot,accuracy,0.3614457831325301,0.0374005938202932
mosaicml/mpt-7b-instruct,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539,0.0258921511567094
mosaicml/mpt-7b-instruct,mmlu_college_physics,0-shot,accuracy,0.2450980392156862,0.0428010583736439
mosaicml/mpt-7b-instruct,mmlu_high_school_physics,0-shot,accuracy,0.3377483443708609,0.0386155754625516
mosaicml/mpt-7b-instruct,mmlu_high_school_biology,0-shot,accuracy,0.3870967741935484,0.0277093596750324
mosaicml/mpt-7b-instruct,mmlu_college_biology,0-shot,accuracy,0.3263888888888889,0.0392106719898226
mosaicml/mpt-7b-instruct,mmlu_anatomy,0-shot,accuracy,0.2888888888888888,0.0391545063041425
mosaicml/mpt-7b-instruct,mmlu_college_chemistry,0-shot,accuracy,0.36,0.0482418151324421
mosaicml/mpt-7b-instruct,mmlu_computer_security,0-shot,accuracy,0.37,0.0485236587093909
mosaicml/mpt-7b-instruct,mmlu_college_computer_science,0-shot,accuracy,0.37,0.0485236587093909
mosaicml/mpt-7b-instruct,mmlu_astronomy,0-shot,accuracy,0.3157894736842105,0.0378272898086546
mosaicml/mpt-7b-instruct,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215
mosaicml/mpt-7b-instruct,mmlu_conceptual_physics,0-shot,accuracy,0.3489361702127659,0.0311585221313577
mosaicml/mpt-7b-instruct,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
mosaicml/mpt-7b-instruct,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.0476095228569523
mosaicml/mpt-7b-instruct,mmlu_machine_learning,0-shot,accuracy,0.3303571428571428,0.0446428571428571
mosaicml/mpt-7b-instruct,mmlu_high_school_chemistry,0-shot,accuracy,0.2857142857142857,0.0317852971064275
mosaicml/mpt-7b-instruct,mmlu_high_school_statistics,0-shot,accuracy,0.375,0.0330169089872108
mosaicml/mpt-7b-instruct,mmlu_elementary_mathematics,0-shot,accuracy,0.2777777777777778,0.0230681888482611
mosaicml/mpt-7b-instruct,mmlu_electrical_engineering,0-shot,accuracy,0.3517241379310344,0.0397923663749741
mosaicml/mpt-7b-instruct,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
mosaicml/mpt-7b-instruct,arc_challenge,25-shot,accuracy,0.4479522184300341,0.0145320114982116
mosaicml/mpt-7b-instruct,arc_challenge,25-shot,acc_norm,0.4965870307167235,0.014611050403244
mosaicml/mpt-7b-instruct,hellaswag,10-shot,accuracy,0.58105954989046,0.0049237725818484
mosaicml/mpt-7b-instruct,hellaswag,10-shot,acc_norm,0.7791276638119896,0.0041398679751162
mosaicml/mpt-7b-instruct,truthfulqa_mc2,0-shot,accuracy,0.3515581077253508,0.0137824111037383
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,bleu_max,22.020255818812327,0.7190998542826786
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,bleu_acc,0.3072215422276622,0.016150201321323
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,bleu_diff,-7.310042815538973,0.7057991080780003
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge1_max,45.74195109027613,0.8459789920841821
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge1_acc,0.2962056303549572,0.0159835951018113
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge1_diff,-10.270084976230358,0.8292877682668228
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge2_max,29.20419401648544,0.9485549352289394
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge2_acc,0.222766217870257,0.0145665069613967
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge2_diff,-11.797169376853873,0.9223640474246654
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rougeL_max,42.95963549103364,0.8505381892909781
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rougeL_acc,0.2864137086903305,0.0158261424395023
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rougeL_diff,-10.523682874537876,0.8392765224735843
mosaicml/mpt-7b-instruct,truthfulqa_mc1,0-shot,accuracy,0.226438188494492,0.0146513373246025
mosaicml/mpt-7b-instruct,winogrande,5-shot,accuracy,0.7048145224940805,0.0128194107417547
EleutherAI/pythia-410M,anli,0-shot,accuracy,0.3381,0.0148
EleutherAI/pythia-410M,logiqa2,0-shot,accuracy,0.2252,0.0105
EleutherAI/pythia-410M,mathqa,0-shot,accuracy,0.2379,0.0078
EleutherAI/pythia-410M,asdiv,5-shot,accuracy,0.0017,0.0009
EleutherAI/pythia-410M,hellaswag,10-shot,accuracy,0.3391754630551682,0.0047246191934275
EleutherAI/pythia-410M,xnli,0-shot,accuracy,0.3699,0.0318
EleutherAI/pythia-410M,arithmetic,5-shot,accuracy,0.008,0.0086
EleutherAI/gpt-j-6b,minerva_math_precalc,5-shot,accuracy,0.0183150183150183,0.0057436967316536
EleutherAI/gpt-j-6b,minerva_math_prealgebra,5-shot,accuracy,0.044776119402985,0.0070115847106233
EleutherAI/gpt-j-6b,minerva_math_num_theory,5-shot,accuracy,0.0222222222222222,0.0063492063492063
EleutherAI/gpt-j-6b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0232558139534883,0.0050182572516275
EleutherAI/gpt-j-6b,minerva_math_geometry,5-shot,accuracy,0.022964509394572,0.0068512498787692
EleutherAI/gpt-j-6b,minerva_math_counting_and_prob,5-shot,accuracy,0.0295358649789029,0.0077845591269482
EleutherAI/gpt-j-6b,minerva_math_algebra,5-shot,accuracy,0.0235888795282224,0.0044068439310235
EleutherAI/gpt-j-6b,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-j-6b,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-j-6b,arithmetic_3da,5-shot,accuracy,0.087,0.0063035995814963
EleutherAI/gpt-j-6b,arithmetic_3ds,5-shot,accuracy,0.0465,0.0047095610180239
EleutherAI/gpt-j-6b,arithmetic_4da,5-shot,accuracy,0.007,0.0018647355360237
EleutherAI/gpt-j-6b,arithmetic_2ds,5-shot,accuracy,0.2175,0.0092271038101
EleutherAI/gpt-j-6b,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-j-6b,arithmetic_5da,5-shot,accuracy,0.0005,0.0005
EleutherAI/gpt-j-6b,arithmetic_1dc,5-shot,accuracy,0.089,0.0063686560505294
EleutherAI/gpt-j-6b,arithmetic_4ds,5-shot,accuracy,0.0055,0.0016541593398342
EleutherAI/gpt-j-6b,arithmetic_2dm,5-shot,accuracy,0.1395,0.007749187050909
EleutherAI/gpt-j-6b,arithmetic_2da,5-shot,accuracy,0.24,0.0095522574720012
EleutherAI/gpt-j-6b,gsm8k_cot,5-shot,accuracy,0.0333586050037907,0.0049462826491737
EleutherAI/gpt-j-6b,gsm8k,5-shot,accuracy,0.0295678544351781,0.0046658931342207
EleutherAI/gpt-j-6b,anli_r2,0-shot,brier_score,0.7625067534704538,
EleutherAI/gpt-j-6b,anli_r3,0-shot,brier_score,0.7268456375297242,
EleutherAI/gpt-j-6b,anli_r1,0-shot,brier_score,0.7767404198801905,
EleutherAI/gpt-j-6b,xnli_eu,0-shot,brier_score,0.840514896839387,
EleutherAI/gpt-j-6b,xnli_vi,0-shot,brier_score,0.758218907162874,
EleutherAI/gpt-j-6b,xnli_ru,0-shot,brier_score,0.7743489919871677,
EleutherAI/gpt-j-6b,xnli_zh,0-shot,brier_score,0.9538503257434168,
EleutherAI/gpt-j-6b,xnli_tr,0-shot,brier_score,0.8753439505463679,
EleutherAI/gpt-j-6b,xnli_fr,0-shot,brier_score,0.7600564801060831,
EleutherAI/gpt-j-6b,xnli_en,0-shot,brier_score,0.631686417358844,
EleutherAI/gpt-j-6b,xnli_ur,0-shot,brier_score,0.9619292774250549,
EleutherAI/gpt-j-6b,xnli_ar,0-shot,brier_score,1.1116475345057533,
EleutherAI/gpt-j-6b,xnli_de,0-shot,brier_score,0.8152549958871562,
EleutherAI/gpt-j-6b,xnli_hi,0-shot,brier_score,0.7460150666217235,
EleutherAI/gpt-j-6b,xnli_es,0-shot,brier_score,0.7914512101723771,
EleutherAI/gpt-j-6b,xnli_bg,0-shot,brier_score,0.7416509603093395,
EleutherAI/gpt-j-6b,xnli_sw,0-shot,brier_score,0.8512933047395764,
EleutherAI/gpt-j-6b,xnli_el,0-shot,brier_score,1.089738388519557,
EleutherAI/gpt-j-6b,xnli_th,0-shot,brier_score,0.828907727940754,
EleutherAI/gpt-j-6b,logiqa2,0-shot,brier_score,1.1310390010190252,
EleutherAI/gpt-j-6b,mathqa,0-shot,brier_score,0.9512998262714332,
EleutherAI/gpt-j-6b,lambada_standard,0-shot,perplexity,5.680920517213677,0.1332091302564743
EleutherAI/gpt-j-6b,lambada_standard,0-shot,accuracy,0.6136231321560256,0.0067837290469494
EleutherAI/gpt-j-6b,lambada_openai,0-shot,perplexity,4.1024027240281375,0.0883354227270602
EleutherAI/gpt-j-6b,lambada_openai,0-shot,accuracy,0.6830972249175238,0.0064821093705663
EleutherAI/gpt-j-6b,mmlu_world_religions,0-shot,accuracy,0.3450292397660818,0.036459813773888
EleutherAI/gpt-j-6b,mmlu_formal_logic,0-shot,accuracy,0.1825396825396825,0.0345507101910215
EleutherAI/gpt-j-6b,mmlu_prehistory,0-shot,accuracy,0.3086419753086419,0.0257026402606037
EleutherAI/gpt-j-6b,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
EleutherAI/gpt-j-6b,mmlu_high_school_world_history,0-shot,accuracy,0.2869198312236287,0.0294437730225946
EleutherAI/gpt-j-6b,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763,0.0238680032625001
EleutherAI/gpt-j-6b,mmlu_professional_law,0-shot,accuracy,0.288135593220339,0.0115671406613245
EleutherAI/gpt-j-6b,mmlu_logical_fallacies,0-shot,accuracy,0.2453987730061349,0.0338093981394335
EleutherAI/gpt-j-6b,mmlu_high_school_us_history,0-shot,accuracy,0.284313725490196,0.0316600967939981
EleutherAI/gpt-j-6b,mmlu_philosophy,0-shot,accuracy,0.2668810289389067,0.0251226376088166
EleutherAI/gpt-j-6b,mmlu_jurisprudence,0-shot,accuracy,0.2685185185185185,0.0428446796805219
EleutherAI/gpt-j-6b,mmlu_international_law,0-shot,accuracy,0.2479338842975206,0.039418975265163
EleutherAI/gpt-j-6b,mmlu_high_school_european_history,0-shot,accuracy,0.2787878787878788,0.0350143870629678
EleutherAI/gpt-j-6b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2176165803108808,0.0297786630377529
EleutherAI/gpt-j-6b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2647058823529412,0.0286574912850719
EleutherAI/gpt-j-6b,mmlu_high_school_geography,0-shot,accuracy,0.2323232323232323,0.0300886294902174
EleutherAI/gpt-j-6b,mmlu_high_school_psychology,0-shot,accuracy,0.2330275229357798,0.0181256691808615
EleutherAI/gpt-j-6b,mmlu_public_relations,0-shot,accuracy,0.3454545454545454,0.0455461961754105
EleutherAI/gpt-j-6b,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,mmlu_sociology,0-shot,accuracy,0.2885572139303483,0.0320384104021332
EleutherAI/gpt-j-6b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2358974358974359,0.0215259654074087
EleutherAI/gpt-j-6b,mmlu_security_studies,0-shot,accuracy,0.3591836734693877,0.0307135604551084
EleutherAI/gpt-j-6b,mmlu_professional_psychology,0-shot,accuracy,0.2794117647058823,0.0181528710515388
EleutherAI/gpt-j-6b,mmlu_human_sexuality,0-shot,accuracy,0.2213740458015267,0.0364129708131372
EleutherAI/gpt-j-6b,mmlu_econometrics,0-shot,accuracy,0.2807017543859649,0.042270544512322
EleutherAI/gpt-j-6b,mmlu_miscellaneous,0-shot,accuracy,0.3128991060025543,0.016580935940304
EleutherAI/gpt-j-6b,mmlu_marketing,0-shot,accuracy,0.2735042735042735,0.0292025401534311
EleutherAI/gpt-j-6b,mmlu_management,0-shot,accuracy,0.203883495145631,0.0398913985953177
EleutherAI/gpt-j-6b,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.0252616912197294
EleutherAI/gpt-j-6b,mmlu_medical_genetics,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.0314938467099413
EleutherAI/gpt-j-6b,mmlu_professional_medicine,0-shot,accuracy,0.2389705882352941,0.025905280644893
EleutherAI/gpt-j-6b,mmlu_college_medicine,0-shot,accuracy,0.2716763005780346,0.0339175032232165
EleutherAI/gpt-j-6b,mmlu_business_ethics,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,mmlu_clinical_knowledge,0-shot,accuracy,0.2566037735849056,0.0268806478890519
EleutherAI/gpt-j-6b,mmlu_global_facts,0-shot,accuracy,0.22,0.0416333199893227
EleutherAI/gpt-j-6b,mmlu_virology,0-shot,accuracy,0.3373493975903614,0.0368078369072758
EleutherAI/gpt-j-6b,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.026684564340461
EleutherAI/gpt-j-6b,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/gpt-j-6b,mmlu_high_school_physics,0-shot,accuracy,0.2649006622516556,0.0360303854536038
EleutherAI/gpt-j-6b,mmlu_high_school_biology,0-shot,accuracy,0.2096774193548387,0.0231578793490835
EleutherAI/gpt-j-6b,mmlu_college_biology,0-shot,accuracy,0.25,0.036210341218895
EleutherAI/gpt-j-6b,mmlu_anatomy,0-shot,accuracy,0.2740740740740741,0.03853254836552
EleutherAI/gpt-j-6b,mmlu_college_chemistry,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/gpt-j-6b,mmlu_computer_security,0-shot,accuracy,0.41,0.049431107042371
EleutherAI/gpt-j-6b,mmlu_college_computer_science,0-shot,accuracy,0.23,0.042295258468165
EleutherAI/gpt-j-6b,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.0358349617636106
EleutherAI/gpt-j-6b,mmlu_college_mathematics,0-shot,accuracy,0.31,0.0464823198711731
EleutherAI/gpt-j-6b,mmlu_conceptual_physics,0-shot,accuracy,0.3361702127659574,0.0308816185206769
EleutherAI/gpt-j-6b,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,mmlu_high_school_computer_science,0-shot,accuracy,0.15,0.0358870281282637
EleutherAI/gpt-j-6b,mmlu_machine_learning,0-shot,accuracy,0.3839285714285714,0.0461614307502854
EleutherAI/gpt-j-6b,mmlu_high_school_chemistry,0-shot,accuracy,0.2463054187192118,0.0303150992856177
EleutherAI/gpt-j-6b,mmlu_high_school_statistics,0-shot,accuracy,0.1574074074074074,0.0248371735182423
EleutherAI/gpt-j-6b,mmlu_elementary_mathematics,0-shot,accuracy,0.2354497354497354,0.0218515098220317
EleutherAI/gpt-j-6b,mmlu_electrical_engineering,0-shot,accuracy,0.2896551724137931,0.0378001923043801
EleutherAI/gpt-j-6b,mmlu_high_school_mathematics,0-shot,accuracy,0.2851851851851852,0.0275285992103404
EleutherAI/gpt-j-6b,arc_challenge,25-shot,accuracy,0.3617747440273037,0.014041957945038
EleutherAI/gpt-j-6b,arc_challenge,25-shot,acc_norm,0.4087030716723549,0.014365750345427
EleutherAI/gpt-j-6b,hellaswag,10-shot,accuracy,0.4945230033857797,0.0049894820406101
EleutherAI/gpt-j-6b,hellaswag,10-shot,acc_norm,0.675363473411671,0.0046728193558385
EleutherAI/gpt-j-6b,truthfulqa_mc2,0-shot,accuracy,0.3595710320927587,0.0134610199064229
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,bleu_max,22.80807652676079,0.729394081143316
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,bleu_acc,0.2998776009791922,0.0160403529667136
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,bleu_diff,-7.044836167856834,0.7649652886092126
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge1_max,46.0394772961559,0.8908991054603245
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge1_acc,0.2558139534883721,0.0152741762192833
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge1_diff,-10.481586263318324,0.9003692764067813
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge2_max,29.614709663039555,0.9851662196235104
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge2_acc,0.2031823745410036,0.0140856665263408
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge2_diff,-11.40429278947376,1.0230238475013214
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rougeL_max,43.38528981216948,0.8926391261722091
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rougeL_acc,0.2435740514075887,0.0150263548249107
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rougeL_diff,-10.689938323819645,0.9132036865292475
EleutherAI/gpt-j-6b,truthfulqa_mc1,0-shot,accuracy,0.2019583843329253,0.0140539574415123
EleutherAI/gpt-j-6b,winogrande,5-shot,accuracy,0.65982636148382,0.0133152187624174
bigscience/bloom-1b7,gsm8k,5-shot,accuracy,0.0022744503411675,0.0013121578148674
bigscience/bloom-1b7,arc:challenge,25-shot,accuracy,0.2721843003412969,0.0130066004064237
bigscience/bloom-1b7,arc:challenge,25-shot,acc_norm,0.3063139931740614,0.0134705844172765
bigscience/bloom-1b7,hellaswag,10-shot,accuracy,0.3762198765186217,0.0048344619979448
bigscience/bloom-1b7,hellaswag,10-shot,acc_norm,0.4794861581358295,0.0049855800659464
bigscience/bloom-1b7,hendrycksTest-abstract_algebra,5-shot,accuracy,0.18,0.0386122919665369
bigscience/bloom-1b7,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.18,0.0386122919665369
bigscience/bloom-1b7,hendrycksTest-anatomy,5-shot,accuracy,0.2444444444444444,0.0371253783361486
bigscience/bloom-1b7,hendrycksTest-anatomy,5-shot,acc_norm,0.2444444444444444,0.0371253783361486
bigscience/bloom-1b7,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.0358349617636106
bigscience/bloom-1b7,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.0358349617636106
bigscience/bloom-1b7,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-1b7,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.0409360180740332
bigscience/bloom-1b7,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2981132075471698,0.0281528379424938
bigscience/bloom-1b7,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2981132075471698,0.0281528379424938
bigscience/bloom-1b7,hendrycksTest-college_biology,5-shot,accuracy,0.2777777777777778,0.0374555479146245
bigscience/bloom-1b7,hendrycksTest-college_biology,5-shot,acc_norm,0.2777777777777778,0.0374555479146245
bigscience/bloom-1b7,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.0416333199893226
bigscience/bloom-1b7,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.0416333199893226
bigscience/bloom-1b7,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.047258156262526
bigscience/bloom-1b7,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.047258156262526
bigscience/bloom-1b7,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
bigscience/bloom-1b7,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
bigscience/bloom-1b7,hendrycksTest-college_medicine,5-shot,accuracy,0.2427745664739884,0.0326926380614177
bigscience/bloom-1b7,hendrycksTest-college_medicine,5-shot,acc_norm,0.2427745664739884,0.0326926380614177
bigscience/bloom-1b7,hendrycksTest-college_physics,5-shot,accuracy,0.2647058823529412,0.0438986995680877
bigscience/bloom-1b7,hendrycksTest-college_physics,5-shot,acc_norm,0.2647058823529412,0.0438986995680877
bigscience/bloom-1b7,hendrycksTest-computer_security,5-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-1b7,hendrycksTest-computer_security,5-shot,acc_norm,0.21,0.0409360180740332
bigscience/bloom-1b7,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.0275017529444124
bigscience/bloom-1b7,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.0275017529444124
bigscience/bloom-1b7,hendrycksTest-econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
bigscience/bloom-1b7,hendrycksTest-econometrics,5-shot,acc_norm,0.2368421052631578,0.0399942387928133
bigscience/bloom-1b7,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
bigscience/bloom-1b7,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
bigscience/bloom-1b7,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2328042328042328,0.0217659616721545
bigscience/bloom-1b7,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2328042328042328,0.0217659616721545
bigscience/bloom-1b7,hendrycksTest-formal_logic,5-shot,accuracy,0.3571428571428571,0.0428571428571428
bigscience/bloom-1b7,hendrycksTest-formal_logic,5-shot,acc_norm,0.3571428571428571,0.0428571428571428
bigscience/bloom-1b7,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
bigscience/bloom-1b7,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
bigscience/bloom-1b7,hendrycksTest-high_school_biology,5-shot,accuracy,0.2870967741935484,0.0257365427455945
bigscience/bloom-1b7,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2870967741935484,0.0257365427455945
bigscience/bloom-1b7,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2807881773399014,0.031618563353586
bigscience/bloom-1b7,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2807881773399014,0.031618563353586
bigscience/bloom-1b7,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.22,0.0416333199893226
bigscience/bloom-1b7,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.22,0.0416333199893226
bigscience/bloom-1b7,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3090909090909091,0.0360854101157396
bigscience/bloom-1b7,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3090909090909091,0.0360854101157396
bigscience/bloom-1b7,hendrycksTest-high_school_geography,5-shot,accuracy,0.3636363636363636,0.0342730865299993
bigscience/bloom-1b7,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3636363636363636,0.0342730865299993
bigscience/bloom-1b7,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3834196891191709,0.0350898423629534
bigscience/bloom-1b7,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3834196891191709,0.0350898423629534
bigscience/bloom-1b7,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.358974358974359,0.0243217384846023
bigscience/bloom-1b7,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.358974358974359,0.0243217384846023
bigscience/bloom-1b7,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2555555555555555,0.026593939101844
bigscience/bloom-1b7,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2555555555555555,0.026593939101844
bigscience/bloom-1b7,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2857142857142857,0.0293445725006343
bigscience/bloom-1b7,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2857142857142857,0.0293445725006343
bigscience/bloom-1b7,hendrycksTest-high_school_physics,5-shot,accuracy,0.3311258278145695,0.0384258171865986
bigscience/bloom-1b7,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3311258278145695,0.0384258171865986
bigscience/bloom-1b7,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3486238532110092,0.0204312540907143
bigscience/bloom-1b7,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3486238532110092,0.0204312540907143
bigscience/bloom-1b7,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
bigscience/bloom-1b7,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
bigscience/bloom-1b7,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.0305875913516042
bigscience/bloom-1b7,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.0305875913516042
bigscience/bloom-1b7,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2362869198312236,0.0276521531441592
bigscience/bloom-1b7,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2362869198312236,0.0276521531441592
bigscience/bloom-1b7,hendrycksTest-human_aging,5-shot,accuracy,0.1300448430493273,0.0225745194241748
bigscience/bloom-1b7,hendrycksTest-human_aging,5-shot,acc_norm,0.1300448430493273,0.0225745194241748
bigscience/bloom-1b7,hendrycksTest-human_sexuality,5-shot,accuracy,0.2290076335877862,0.0368534663171185
bigscience/bloom-1b7,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2290076335877862,0.0368534663171185
bigscience/bloom-1b7,hendrycksTest-international_law,5-shot,accuracy,0.2148760330578512,0.0374949244870969
bigscience/bloom-1b7,hendrycksTest-international_law,5-shot,acc_norm,0.2148760330578512,0.0374949244870969
bigscience/bloom-1b7,hendrycksTest-jurisprudence,5-shot,accuracy,0.2407407407407407,0.0413311944024383
bigscience/bloom-1b7,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2407407407407407,0.0413311944024383
bigscience/bloom-1b7,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2699386503067484,0.0348782516849789
bigscience/bloom-1b7,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2699386503067484,0.0348782516849789
bigscience/bloom-1b7,hendrycksTest-machine_learning,5-shot,accuracy,0.1785714285714285,0.036352091215778
bigscience/bloom-1b7,hendrycksTest-machine_learning,5-shot,acc_norm,0.1785714285714285,0.036352091215778
bigscience/bloom-1b7,hendrycksTest-management,5-shot,accuracy,0.3883495145631068,0.0482572933735639
bigscience/bloom-1b7,hendrycksTest-management,5-shot,acc_norm,0.3883495145631068,0.0482572933735639
bigscience/bloom-1b7,hendrycksTest-marketing,5-shot,accuracy,0.2393162393162393,0.0279518268089243
bigscience/bloom-1b7,hendrycksTest-marketing,5-shot,acc_norm,0.2393162393162393,0.0279518268089243
bigscience/bloom-1b7,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.0429234695990928
bigscience/bloom-1b7,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.0429234695990928
bigscience/bloom-1b7,hendrycksTest-miscellaneous,5-shot,accuracy,0.2056194125159642,0.0144525004567858
bigscience/bloom-1b7,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2056194125159642,0.0144525004567858
bigscience/bloom-1b7,hendrycksTest-moral_disputes,5-shot,accuracy,0.26878612716763,0.0238680032625001
bigscience/bloom-1b7,hendrycksTest-moral_disputes,5-shot,acc_norm,0.26878612716763,0.0238680032625001
bigscience/bloom-1b7,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2726256983240223,0.0148933917352495
bigscience/bloom-1b7,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2726256983240223,0.0148933917352495
bigscience/bloom-1b7,hendrycksTest-nutrition,5-shot,accuracy,0.2777777777777778,0.0256468630971378
bigscience/bloom-1b7,hendrycksTest-nutrition,5-shot,acc_norm,0.2777777777777778,0.0256468630971378
bigscience/bloom-1b7,hendrycksTest-philosophy,5-shot,accuracy,0.2829581993569132,0.0255830624899848
bigscience/bloom-1b7,hendrycksTest-philosophy,5-shot,acc_norm,0.2829581993569132,0.0255830624899848
bigscience/bloom-1b7,hendrycksTest-prehistory,5-shot,accuracy,0.2253086419753086,0.0232462026478197
bigscience/bloom-1b7,hendrycksTest-prehistory,5-shot,acc_norm,0.2253086419753086,0.0232462026478197
bigscience/bloom-1b7,hendrycksTest-professional_accounting,5-shot,accuracy,0.2695035460992908,0.0264690368185906
bigscience/bloom-1b7,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2695035460992908,0.0264690368185906
bigscience/bloom-1b7,hendrycksTest-professional_law,5-shot,accuracy,0.2692307692307692,0.0113287344031403
bigscience/bloom-1b7,hendrycksTest-professional_law,5-shot,acc_norm,0.2692307692307692,0.0113287344031403
bigscience/bloom-1b7,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.0302114796091215
bigscience/bloom-1b7,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.0302114796091215
bigscience/bloom-1b7,hendrycksTest-professional_psychology,5-shot,accuracy,0.2875816993464052,0.0183116530536482
bigscience/bloom-1b7,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2875816993464052,0.0183116530536482
bigscience/bloom-1b7,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.0435027144292324
bigscience/bloom-1b7,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.0435027144292324
bigscience/bloom-1b7,hendrycksTest-security_studies,5-shot,accuracy,0.4,0.0313625024093589
bigscience/bloom-1b7,hendrycksTest-security_studies,5-shot,acc_norm,0.4,0.0313625024093589
bigscience/bloom-1b7,hendrycksTest-sociology,5-shot,accuracy,0.263681592039801,0.0311571508693555
bigscience/bloom-1b7,hendrycksTest-sociology,5-shot,acc_norm,0.263681592039801,0.0311571508693555
bigscience/bloom-1b7,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276807
bigscience/bloom-1b7,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.0440844002276807
bigscience/bloom-1b7,hendrycksTest-virology,5-shot,accuracy,0.1987951807228915,0.0310693902607894
bigscience/bloom-1b7,hendrycksTest-virology,5-shot,acc_norm,0.1987951807228915,0.0310693902607894
bigscience/bloom-1b7,hendrycksTest-world_religions,5-shot,accuracy,0.2573099415204678,0.0335279984416186
bigscience/bloom-1b7,hendrycksTest-world_religions,5-shot,acc_norm,0.2573099415204678,0.0335279984416186
bigscience/bloom-1b7,truthfulqa:mc,0-shot,mc1,0.244798041615667,0.015051869486715
bigscience/bloom-1b7,truthfulqa:mc,0-shot,mc2,0.4130906271199276,0.0144359260039389
bigscience/bloom-1b7,drop,3-shot,accuracy,0.0009437919463087,0.0003144653119413
bigscience/bloom-1b7,drop,3-shot,f1,0.0502569211409396,0.001266142736173
bigscience/bloom-1b7,winogrande,5-shot,accuracy,0.5445935280189423,0.0139964850377297
EleutherAI/pythia-6.9b-deduped,arc:challenge,25-shot,accuracy,0.3728668941979522,0.0141311767601311
EleutherAI/pythia-6.9b-deduped,arc:challenge,25-shot,acc_norm,0.4129692832764505,0.0143883449353983
EleutherAI/pythia-6.9b-deduped,hellaswag,10-shot,accuracy,0.492531368253336,0.0049892247157845
EleutherAI/pythia-6.9b-deduped,hellaswag,10-shot,acc_norm,0.6704839673371839,0.0046907683938544
EleutherAI/pythia-6.9b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-6.9b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-6.9b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.237037037037037,0.036737316839695
EleutherAI/pythia-6.9b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.237037037037037,0.036737316839695
EleutherAI/pythia-6.9b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.2368421052631578,0.0345977760681053
EleutherAI/pythia-6.9b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.2368421052631578,0.0345977760681053
EleutherAI/pythia-6.9b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-6.9b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-6.9b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2528301886792453,0.0267498997712412
EleutherAI/pythia-6.9b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2528301886792453,0.0267498997712412
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.0365394696944209
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.0365394696944209
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.0386122919665369
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441101
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441101
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.1965317919075144,0.0302995746647881
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.1965317919075144,0.0302995746647881
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
EleutherAI/pythia-6.9b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-6.9b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-6.9b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2936170212765957,0.0297716427124912
EleutherAI/pythia-6.9b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2936170212765957,0.0297716427124912
EleutherAI/pythia-6.9b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.2280701754385964,0.0394715278266941
EleutherAI/pythia-6.9b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.2280701754385964,0.0394715278266941
EleutherAI/pythia-6.9b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3034482758620689,0.0383122604885033
EleutherAI/pythia-6.9b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3034482758620689,0.0383122604885033
EleutherAI/pythia-6.9b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2407407407407407,0.0220190800122179
EleutherAI/pythia-6.9b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2407407407407407,0.0220190800122179
EleutherAI/pythia-6.9b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.2142857142857142,0.0367006645104718
EleutherAI/pythia-6.9b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.2142857142857142,0.0367006645104718
EleutherAI/pythia-6.9b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-6.9b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.2387096774193548,0.0242510712622088
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2387096774193548,0.0242510712622088
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3054187192118227,0.032406615658684
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3054187192118227,0.032406615658684
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.0394277244403662
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.0394277244403662
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2373737373737373,0.0303137105381988
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2373737373737373,0.0303137105381988
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2538860103626943,0.0314102478056532
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2538860103626943,0.0314102478056532
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2538461538461538,0.0220660543787262
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2538461538461538,0.0220660543787262
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2666666666666666,0.0269624243250738
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2666666666666666,0.0269624243250738
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.0268415143229589
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2184873949579832,0.0268415143229589
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.3112582781456953,0.0378044585052673
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3112582781456953,0.0378044585052673
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.218348623853211,0.0177126005287227
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.218348623853211,0.0177126005287227
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.449074074074074,0.0339223840532161
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.449074074074074,0.0339223840532161
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.0305875913516042
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.0305875913516042
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2278481012658227,0.0273034845990694
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2278481012658227,0.0273034845990694
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.3766816143497757,0.0325211348992918
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.3766816143497757,0.0325211348992918
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-6.9b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.0429434084521209
EleutherAI/pythia-6.9b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.0429434084521209
EleutherAI/pythia-6.9b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
EleutherAI/pythia-6.9b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
EleutherAI/pythia-6.9b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3128834355828221,0.036429145782924
EleutherAI/pythia-6.9b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3128834355828221,0.036429145782924
EleutherAI/pythia-6.9b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.3303571428571428,0.0446428571428571
EleutherAI/pythia-6.9b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.3303571428571428,0.0446428571428571
EleutherAI/pythia-6.9b-deduped,hendrycksTest-management,5-shot,accuracy,0.2815533980582524,0.0445325483632646
EleutherAI/pythia-6.9b-deduped,hendrycksTest-management,5-shot,acc_norm,0.2815533980582524,0.0445325483632646
EleutherAI/pythia-6.9b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2478632478632478,0.0282863240755643
EleutherAI/pythia-6.9b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2478632478632478,0.0282863240755643
EleutherAI/pythia-6.9b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-6.9b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-6.9b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2899106002554278,0.0162250179447709
EleutherAI/pythia-6.9b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2899106002554278,0.0162250179447709
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2138728323699422,0.0220757092517571
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2138728323699422,0.0220757092517571
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.0143559119647678
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.0143559119647678
EleutherAI/pythia-6.9b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2516339869281045,0.0248480182638752
EleutherAI/pythia-6.9b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2516339869281045,0.0248480182638752
EleutherAI/pythia-6.9b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2861736334405145,0.0256702592421889
EleutherAI/pythia-6.9b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2861736334405145,0.0256702592421889
EleutherAI/pythia-6.9b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.0245692236004608
EleutherAI/pythia-6.9b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.0245692236004608
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.0267891723511402
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2555410691003911,0.0111398578335985
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2555410691003911,0.0111398578335985
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.213235294117647,0.0248809715122942
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.213235294117647,0.0248809715122942
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2777777777777778,0.0181202242514845
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2777777777777778,0.0181202242514845
EleutherAI/pythia-6.9b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-6.9b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-6.9b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.310204081632653,0.0296134598724843
EleutherAI/pythia-6.9b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.310204081632653,0.0296134598724843
EleutherAI/pythia-6.9b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-6.9b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-6.9b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-6.9b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-6.9b-deduped,hendrycksTest-virology,5-shot,accuracy,0.3493975903614458,0.0371172519074075
EleutherAI/pythia-6.9b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.3493975903614458,0.0371172519074075
EleutherAI/pythia-6.9b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.0350877192982456
EleutherAI/pythia-6.9b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.0350877192982456
EleutherAI/pythia-6.9b-deduped,truthfulqa:mc,0-shot,mc1,0.215422276621787,0.0143919026524276
EleutherAI/pythia-6.9b-deduped,truthfulqa:mc,0-shot,mc2,0.3519458488266109,0.0132309516078646
EleutherAI/pythia-6.9b-deduped,drop,3-shot,accuracy,0.0007340604026845,0.0002773614457335
EleutherAI/pythia-6.9b-deduped,drop,3-shot,f1,0.0449580536912753,0.0011424943224633
EleutherAI/pythia-6.9b-deduped,gsm8k,5-shot,accuracy,0.0166793025018953,0.0035275958887224
EleutherAI/pythia-6.9b-deduped,winogrande,5-shot,accuracy,0.6408839779005525,0.0134831152021202
Qwen/Qwen-7B,arc:challenge,25-shot,accuracy,0.4965870307167235,0.014611050403244
Qwen/Qwen-7B,arc:challenge,25-shot,acc_norm,0.5136518771331058,0.0146059434298609
Qwen/Qwen-7B,hellaswag,10-shot,accuracy,0.5759808803027285,0.0049318319538
Qwen/Qwen-7B,hellaswag,10-shot,acc_norm,0.7847042421828321,0.0041018734073546
Qwen/Qwen-7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.0464823198711731
Qwen/Qwen-7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.0464823198711731
Qwen/Qwen-7B,hendrycksTest-anatomy,5-shot,accuracy,0.5037037037037037,0.0431922362581133
Qwen/Qwen-7B,hendrycksTest-anatomy,5-shot,acc_norm,0.5037037037037037,0.0431922362581133
Qwen/Qwen-7B,hendrycksTest-astronomy,5-shot,accuracy,0.6447368421052632,0.0389473448701331
Qwen/Qwen-7B,hendrycksTest-astronomy,5-shot,acc_norm,0.6447368421052632,0.0389473448701331
Qwen/Qwen-7B,hendrycksTest-business_ethics,5-shot,accuracy,0.68,0.046882617226215
Qwen/Qwen-7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.68,0.046882617226215
Qwen/Qwen-7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.630188679245283,0.0297114218801079
Qwen/Qwen-7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.630188679245283,0.0297114218801079
Qwen/Qwen-7B,hendrycksTest-college_biology,5-shot,accuracy,0.6805555555555556,0.0389907368735733
Qwen/Qwen-7B,hendrycksTest-college_biology,5-shot,acc_norm,0.6805555555555556,0.0389907368735733
Qwen/Qwen-7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.48,0.0502116731568677
Qwen/Qwen-7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.48,0.0502116731568677
Qwen/Qwen-7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.53,0.0501613558046592
Qwen/Qwen-7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.53,0.0501613558046592
Qwen/Qwen-7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
Qwen/Qwen-7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
Qwen/Qwen-7B,hendrycksTest-college_medicine,5-shot,accuracy,0.5838150289017341,0.0375851777540494
Qwen/Qwen-7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.5838150289017341,0.0375851777540494
Qwen/Qwen-7B,hendrycksTest-college_physics,5-shot,accuracy,0.3725490196078431,0.0481084014808263
Qwen/Qwen-7B,hendrycksTest-college_physics,5-shot,acc_norm,0.3725490196078431,0.0481084014808263
Qwen/Qwen-7B,hendrycksTest-computer_security,5-shot,accuracy,0.73,0.0446196043338473
Qwen/Qwen-7B,hendrycksTest-computer_security,5-shot,acc_norm,0.73,0.0446196043338473
Qwen/Qwen-7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.5319148936170213,0.0326193691846738
Qwen/Qwen-7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.5319148936170213,0.0326193691846738
Qwen/Qwen-7B,hendrycksTest-econometrics,5-shot,accuracy,0.3421052631578947,0.0446291753533693
Qwen/Qwen-7B,hendrycksTest-econometrics,5-shot,acc_norm,0.3421052631578947,0.0446291753533693
Qwen/Qwen-7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.593103448275862,0.0409379398126623
Qwen/Qwen-7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.593103448275862,0.0409379398126623
Qwen/Qwen-7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4285714285714285,0.0254871871478593
Qwen/Qwen-7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4285714285714285,0.0254871871478593
Qwen/Qwen-7B,hendrycksTest-formal_logic,5-shot,accuracy,0.4206349206349206,0.0441543822674374
Qwen/Qwen-7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.4206349206349206,0.0441543822674374
Qwen/Qwen-7B,hendrycksTest-global_facts,5-shot,accuracy,0.38,0.0487831731214563
Qwen/Qwen-7B,hendrycksTest-global_facts,5-shot,acc_norm,0.38,0.0487831731214563
Qwen/Qwen-7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.7193548387096774,0.0255606047210228
Qwen/Qwen-7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7193548387096774,0.0255606047210228
Qwen/Qwen-7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5123152709359606,0.0351692044422089
Qwen/Qwen-7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5123152709359606,0.0351692044422089
Qwen/Qwen-7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.67,0.047258156262526
Qwen/Qwen-7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.67,0.047258156262526
Qwen/Qwen-7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.6666666666666666,0.0368105086916154
Qwen/Qwen-7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.6666666666666666,0.0368105086916154
Qwen/Qwen-7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.7525252525252525,0.0307463007421244
Qwen/Qwen-7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.7525252525252525,0.0307463007421244
Qwen/Qwen-7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.844559585492228,0.0261484834691533
Qwen/Qwen-7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.844559585492228,0.0261484834691533
Qwen/Qwen-7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6102564102564103,0.024726967886647
Qwen/Qwen-7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6102564102564103,0.024726967886647
Qwen/Qwen-7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3,0.0279404571362284
Qwen/Qwen-7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3,0.0279404571362284
Qwen/Qwen-7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.5840336134453782,0.0320165010073961
Qwen/Qwen-7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.5840336134453782,0.0320165010073961
Qwen/Qwen-7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.3642384105960264,0.0392911178124274
Qwen/Qwen-7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3642384105960264,0.0392911178124274
Qwen/Qwen-7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.781651376146789,0.0177126005287227
Qwen/Qwen-7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.781651376146789,0.0177126005287227
Qwen/Qwen-7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4212962962962963,0.0336746213889607
Qwen/Qwen-7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4212962962962963,0.0336746213889607
Qwen/Qwen-7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.7401960784313726,0.0307785546786932
Qwen/Qwen-7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.7401960784313726,0.0307785546786932
Qwen/Qwen-7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.759493670886076,0.0278207819811496
Qwen/Qwen-7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.759493670886076,0.0278207819811496
Qwen/Qwen-7B,hendrycksTest-human_aging,5-shot,accuracy,0.6367713004484304,0.0322779044285049
Qwen/Qwen-7B,hendrycksTest-human_aging,5-shot,acc_norm,0.6367713004484304,0.0322779044285049
Qwen/Qwen-7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.7022900763358778,0.040103589424622
Qwen/Qwen-7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7022900763358778,0.040103589424622
Qwen/Qwen-7B,hendrycksTest-international_law,5-shot,accuracy,0.7768595041322314,0.0380075447522873
Qwen/Qwen-7B,hendrycksTest-international_law,5-shot,acc_norm,0.7768595041322314,0.0380075447522873
Qwen/Qwen-7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.7407407407407407,0.0423651125809463
Qwen/Qwen-7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7407407407407407,0.0423651125809463
Qwen/Qwen-7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.6625766871165644,0.0371490840993557
Qwen/Qwen-7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.6625766871165644,0.0371490840993557
Qwen/Qwen-7B,hendrycksTest-machine_learning,5-shot,accuracy,0.375,0.0459509138808629
Qwen/Qwen-7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.375,0.0459509138808629
Qwen/Qwen-7B,hendrycksTest-management,5-shot,accuracy,0.7766990291262136,0.0412355318989143
Qwen/Qwen-7B,hendrycksTest-management,5-shot,acc_norm,0.7766990291262136,0.0412355318989143
Qwen/Qwen-7B,hendrycksTest-marketing,5-shot,accuracy,0.8333333333333334,0.0244149473045436
Qwen/Qwen-7B,hendrycksTest-marketing,5-shot,acc_norm,0.8333333333333334,0.0244149473045436
Qwen/Qwen-7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.71,0.0456048021572068
Qwen/Qwen-7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.71,0.0456048021572068
Qwen/Qwen-7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.7841634738186463,0.0147116843861399
Qwen/Qwen-7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.7841634738186463,0.0147116843861399
Qwen/Qwen-7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.6676300578034682,0.0253611687496882
Qwen/Qwen-7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.6676300578034682,0.0253611687496882
Qwen/Qwen-7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.3150837988826815,0.0155368508524736
Qwen/Qwen-7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.3150837988826815,0.0155368508524736
Qwen/Qwen-7B,hendrycksTest-nutrition,5-shot,accuracy,0.6764705882352942,0.0267874531119064
Qwen/Qwen-7B,hendrycksTest-nutrition,5-shot,acc_norm,0.6764705882352942,0.0267874531119064
Qwen/Qwen-7B,hendrycksTest-philosophy,5-shot,accuracy,0.6752411575562701,0.026596782287697
Qwen/Qwen-7B,hendrycksTest-philosophy,5-shot,acc_norm,0.6752411575562701,0.026596782287697
Qwen/Qwen-7B,hendrycksTest-prehistory,5-shot,accuracy,0.6882716049382716,0.0257731111696304
Qwen/Qwen-7B,hendrycksTest-prehistory,5-shot,acc_norm,0.6882716049382716,0.0257731111696304
Qwen/Qwen-7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.4290780141843971,0.0295259143025585
Qwen/Qwen-7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4290780141843971,0.0295259143025585
Qwen/Qwen-7B,hendrycksTest-professional_law,5-shot,accuracy,0.4198174706649283,0.0126049608160873
Qwen/Qwen-7B,hendrycksTest-professional_law,5-shot,acc_norm,0.4198174706649283,0.0126049608160873
Qwen/Qwen-7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.5735294117647058,0.0300426158327148
Qwen/Qwen-7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.5735294117647058,0.0300426158327148
Qwen/Qwen-7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.5931372549019608,0.0198738020050611
Qwen/Qwen-7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.5931372549019608,0.0198738020050611
Qwen/Qwen-7B,hendrycksTest-public_relations,5-shot,accuracy,0.6545454545454545,0.0455461961754105
Qwen/Qwen-7B,hendrycksTest-public_relations,5-shot,acc_norm,0.6545454545454545,0.0455461961754105
Qwen/Qwen-7B,hendrycksTest-security_studies,5-shot,accuracy,0.7061224489795919,0.0291627384102497
Qwen/Qwen-7B,hendrycksTest-security_studies,5-shot,acc_norm,0.7061224489795919,0.0291627384102497
Qwen/Qwen-7B,hendrycksTest-sociology,5-shot,accuracy,0.8059701492537313,0.0279626776047689
Qwen/Qwen-7B,hendrycksTest-sociology,5-shot,acc_norm,0.8059701492537313,0.0279626776047689
Qwen/Qwen-7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.8,0.0402015126103684
Qwen/Qwen-7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.8,0.0402015126103684
Qwen/Qwen-7B,hendrycksTest-virology,5-shot,accuracy,0.4819277108433735,0.0388995125282721
Qwen/Qwen-7B,hendrycksTest-virology,5-shot,acc_norm,0.4819277108433735,0.0388995125282721
Qwen/Qwen-7B,hendrycksTest-world_religions,5-shot,accuracy,0.7543859649122807,0.0330140594698725
Qwen/Qwen-7B,hendrycksTest-world_religions,5-shot,acc_norm,0.7543859649122807,0.0330140594698725
Qwen/Qwen-7B,truthfulqa:mc,0-shot,mc1,0.3243574051407589,0.0163879767796479
Qwen/Qwen-7B,truthfulqa:mc,0-shot,mc2,0.4778593054989757,0.0150187776960515
Qwen/Qwen-7B,drop,3-shot,accuracy,0.0416317114093959,0.0020455872163586
Qwen/Qwen-7B,drop,3-shot,f1,0.0925429949664433,0.0022297798333443
Qwen/Qwen-7B,gsm8k,5-shot,accuracy,0.4495830174374526,0.0137022900478847
Qwen/Qwen-7B,winogrande,5-shot,accuracy,0.7269139700078927,0.0125220201058694
01-ai/Yi-34B,arc:challenge,25-shot,accuracy,0.6160409556313993,0.0142124449806518
01-ai/Yi-34B,arc:challenge,25-shot,acc_norm,0.6459044368600683,0.0139754541227565
01-ai/Yi-34B,hellaswag,10-shot,accuracy,0.656144194383589,0.0047402292124734
01-ai/Yi-34B,hellaswag,10-shot,acc_norm,0.8569010157339175,0.0034945810763985
01-ai/Yi-34B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.45,0.0499999999999999
01-ai/Yi-34B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.45,0.0499999999999999
01-ai/Yi-34B,hendrycksTest-anatomy,5-shot,accuracy,0.7481481481481481,0.0374985070917402
01-ai/Yi-34B,hendrycksTest-anatomy,5-shot,acc_norm,0.7481481481481481,0.0374985070917402
01-ai/Yi-34B,hendrycksTest-astronomy,5-shot,accuracy,0.9013157894736842,0.0242702277375227
01-ai/Yi-34B,hendrycksTest-astronomy,5-shot,acc_norm,0.9013157894736842,0.0242702277375227
01-ai/Yi-34B,hendrycksTest-business_ethics,5-shot,accuracy,0.79,0.0409360180740332
01-ai/Yi-34B,hendrycksTest-business_ethics,5-shot,acc_norm,0.79,0.0409360180740332
01-ai/Yi-34B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7924528301886793,0.0249599180289112
01-ai/Yi-34B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7924528301886793,0.0249599180289112
01-ai/Yi-34B,hendrycksTest-college_biology,5-shot,accuracy,0.8819444444444444,0.0269833465033093
01-ai/Yi-34B,hendrycksTest-college_biology,5-shot,acc_norm,0.8819444444444444,0.0269833465033093
01-ai/Yi-34B,hendrycksTest-college_chemistry,5-shot,accuracy,0.49,0.0502418393795691
01-ai/Yi-34B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.49,0.0502418393795691
01-ai/Yi-34B,hendrycksTest-college_computer_science,5-shot,accuracy,0.65,0.0479372485441101
01-ai/Yi-34B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.65,0.0479372485441101
01-ai/Yi-34B,hendrycksTest-college_mathematics,5-shot,accuracy,0.48,0.0502116731568677
01-ai/Yi-34B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.48,0.0502116731568677
01-ai/Yi-34B,hendrycksTest-college_medicine,5-shot,accuracy,0.7109826589595376,0.0345642574508699
01-ai/Yi-34B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7109826589595376,0.0345642574508699
01-ai/Yi-34B,hendrycksTest-college_physics,5-shot,accuracy,0.5,0.0497518595104994
01-ai/Yi-34B,hendrycksTest-college_physics,5-shot,acc_norm,0.5,0.0497518595104994
01-ai/Yi-34B,hendrycksTest-computer_security,5-shot,accuracy,0.82,0.0386122919665369
01-ai/Yi-34B,hendrycksTest-computer_security,5-shot,acc_norm,0.82,0.0386122919665369
01-ai/Yi-34B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7702127659574468,0.0275017529444124
01-ai/Yi-34B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7702127659574468,0.0275017529444124
01-ai/Yi-34B,hendrycksTest-econometrics,5-shot,accuracy,0.5526315789473685,0.0467747300449119
01-ai/Yi-34B,hendrycksTest-econometrics,5-shot,acc_norm,0.5526315789473685,0.0467747300449119
01-ai/Yi-34B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.8,0.0333333333333333
01-ai/Yi-34B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.8,0.0333333333333333
01-ai/Yi-34B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.656084656084656,0.0244644266255964
01-ai/Yi-34B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.656084656084656,0.0244644266255964
01-ai/Yi-34B,hendrycksTest-formal_logic,5-shot,accuracy,0.5634920634920635,0.0443593289285146
01-ai/Yi-34B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5634920634920635,0.0443593289285146
01-ai/Yi-34B,hendrycksTest-global_facts,5-shot,accuracy,0.52,0.0502116731568677
01-ai/Yi-34B,hendrycksTest-global_facts,5-shot,acc_norm,0.52,0.0502116731568677
01-ai/Yi-34B,hendrycksTest-high_school_biology,5-shot,accuracy,0.8806451612903226,0.0184434113253153
01-ai/Yi-34B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8806451612903226,0.0184434113253153
01-ai/Yi-34B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.645320197044335,0.0336612448905144
01-ai/Yi-34B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.645320197044335,0.0336612448905144
01-ai/Yi-34B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.82,0.0386122919665369
01-ai/Yi-34B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.82,0.0386122919665369
01-ai/Yi-34B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8666666666666667,0.0265444353127064
01-ai/Yi-34B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8666666666666667,0.0265444353127064
01-ai/Yi-34B,hendrycksTest-high_school_geography,5-shot,accuracy,0.8939393939393939,0.0219380477388531
01-ai/Yi-34B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8939393939393939,0.0219380477388531
01-ai/Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9792746113989638,0.010281417011909
01-ai/Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9792746113989638,0.010281417011909
01-ai/Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8,0.0202808050625357
01-ai/Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8,0.0202808050625357
01-ai/Yi-34B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4444444444444444,0.0302967712860673
01-ai/Yi-34B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4444444444444444,0.0302967712860673
01-ai/Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8571428571428571,0.0227302081193065
01-ai/Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8571428571428571,0.0227302081193065
01-ai/Yi-34B,hendrycksTest-high_school_physics,5-shot,accuracy,0.5165562913907285,0.0408024418562897
01-ai/Yi-34B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5165562913907285,0.0408024418562897
01-ai/Yi-34B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9155963302752294,0.0119188193273348
01-ai/Yi-34B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9155963302752294,0.0119188193273348
01-ai/Yi-34B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6388888888888888,0.0327577348610099
01-ai/Yi-34B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6388888888888888,0.0327577348610099
01-ai/Yi-34B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9166666666666666,0.0193984521358139
01-ai/Yi-34B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9166666666666666,0.0193984521358139
01-ai/Yi-34B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.919831223628692,0.0176766799918916
01-ai/Yi-34B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.919831223628692,0.0176766799918916
01-ai/Yi-34B,hendrycksTest-human_aging,5-shot,accuracy,0.7937219730941704,0.0271571504795638
01-ai/Yi-34B,hendrycksTest-human_aging,5-shot,acc_norm,0.7937219730941704,0.0271571504795638
01-ai/Yi-34B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8625954198473282,0.0301948239968044
01-ai/Yi-34B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8625954198473282,0.0301948239968044
01-ai/Yi-34B,hendrycksTest-international_law,5-shot,accuracy,0.9090909090909092,0.0262431940540738
01-ai/Yi-34B,hendrycksTest-international_law,5-shot,acc_norm,0.9090909090909092,0.0262431940540738
01-ai/Yi-34B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8888888888888888,0.0303815967566516
01-ai/Yi-34B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8888888888888888,0.0303815967566516
01-ai/Yi-34B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8834355828220859,0.0252123272105071
01-ai/Yi-34B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8834355828220859,0.0252123272105071
01-ai/Yi-34B,hendrycksTest-machine_learning,5-shot,accuracy,0.5982142857142857,0.0465333314697364
01-ai/Yi-34B,hendrycksTest-machine_learning,5-shot,acc_norm,0.5982142857142857,0.0465333314697364
01-ai/Yi-34B,hendrycksTest-management,5-shot,accuracy,0.912621359223301,0.0279606891259706
01-ai/Yi-34B,hendrycksTest-management,5-shot,acc_norm,0.912621359223301,0.0279606891259706
01-ai/Yi-34B,hendrycksTest-marketing,5-shot,accuracy,0.9316239316239316,0.0165346276843113
01-ai/Yi-34B,hendrycksTest-marketing,5-shot,acc_norm,0.9316239316239316,0.0165346276843113
01-ai/Yi-34B,hendrycksTest-medical_genetics,5-shot,accuracy,0.87,0.033799766898963
01-ai/Yi-34B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.87,0.033799766898963
01-ai/Yi-34B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9054916985951468,0.010461015338193
01-ai/Yi-34B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9054916985951468,0.010461015338193
01-ai/Yi-34B,hendrycksTest-moral_disputes,5-shot,accuracy,0.8294797687861272,0.0202479615693037
01-ai/Yi-34B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8294797687861272,0.0202479615693037
01-ai/Yi-34B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6446927374301676,0.0160069899348031
01-ai/Yi-34B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6446927374301676,0.0160069899348031
01-ai/Yi-34B,hendrycksTest-nutrition,5-shot,accuracy,0.8627450980392157,0.0197040391838598
01-ai/Yi-34B,hendrycksTest-nutrition,5-shot,acc_norm,0.8627450980392157,0.0197040391838598
01-ai/Yi-34B,hendrycksTest-philosophy,5-shot,accuracy,0.8392282958199357,0.0208623880823918
01-ai/Yi-34B,hendrycksTest-philosophy,5-shot,acc_norm,0.8392282958199357,0.0208623880823918
01-ai/Yi-34B,hendrycksTest-prehistory,5-shot,accuracy,0.8827160493827161,0.0179031126152811
01-ai/Yi-34B,hendrycksTest-prehistory,5-shot,acc_norm,0.8827160493827161,0.0179031126152811
01-ai/Yi-34B,hendrycksTest-professional_accounting,5-shot,accuracy,0.6702127659574468,0.0280459469420424
01-ai/Yi-34B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6702127659574468,0.0280459469420424
01-ai/Yi-34B,hendrycksTest-professional_law,5-shot,accuracy,0.6049543676662321,0.0124857278132515
01-ai/Yi-34B,hendrycksTest-professional_law,5-shot,acc_norm,0.6049543676662321,0.0124857278132515
01-ai/Yi-34B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8125,0.0237097882538117
01-ai/Yi-34B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8125,0.0237097882538117
01-ai/Yi-34B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8186274509803921,0.0155886434953704
01-ai/Yi-34B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8186274509803921,0.0155886434953704
01-ai/Yi-34B,hendrycksTest-public_relations,5-shot,accuracy,0.7363636363636363,0.0422022469297198
01-ai/Yi-34B,hendrycksTest-public_relations,5-shot,acc_norm,0.7363636363636363,0.0422022469297198
01-ai/Yi-34B,hendrycksTest-security_studies,5-shot,accuracy,0.8448979591836735,0.0231747988612186
01-ai/Yi-34B,hendrycksTest-security_studies,5-shot,acc_norm,0.8448979591836735,0.0231747988612186
01-ai/Yi-34B,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.0220763261018246
01-ai/Yi-34B,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.0220763261018246
01-ai/Yi-34B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.9,0.0301511344577763
01-ai/Yi-34B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.9,0.0301511344577763
01-ai/Yi-34B,hendrycksTest-virology,5-shot,accuracy,0.5783132530120482,0.0384445318177091
01-ai/Yi-34B,hendrycksTest-virology,5-shot,acc_norm,0.5783132530120482,0.0384445318177091
01-ai/Yi-34B,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.0251729843501557
01-ai/Yi-34B,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.0251729843501557
01-ai/Yi-34B,truthfulqa:mc,0-shot,mc1,0.4075887392900856,0.0172019492345531
01-ai/Yi-34B,truthfulqa:mc,0-shot,mc2,0.5623083932983032,0.0151659636710398
01-ai/Yi-34B,drop,3-shot,accuracy,0.6081166107382551,0.0049993266298801
01-ai/Yi-34B,drop,3-shot,f1,0.6419882550335565,0.0047482393511563
01-ai/Yi-34B,gsm8k,5-shot,accuracy,0.5064442759666414,0.0137713407656997
01-ai/Yi-34B,winogrande,5-shot,accuracy,0.8303078137332282,0.0105495426473636
01-ai/Yi-34B,mmlu_world_religions,0-shot,accuracy,0.8654970760233918,0.0261682213446622
01-ai/Yi-34B,mmlu_formal_logic,0-shot,accuracy,0.5555555555555556,0.0444444444444444
01-ai/Yi-34B,mmlu_prehistory,0-shot,accuracy,0.8734567901234568,0.0184986005587909
01-ai/Yi-34B,mmlu_moral_scenarios,0-shot,accuracy,0.6458100558659218,0.0159956449472992
01-ai/Yi-34B,mmlu_high_school_world_history,0-shot,accuracy,0.9240506329113924,0.0172446332510656
01-ai/Yi-34B,mmlu_moral_disputes,0-shot,accuracy,0.8323699421965318,0.0201105799197348
01-ai/Yi-34B,mmlu_professional_law,0-shot,accuracy,0.5971316818774446,0.012526955577118
01-ai/Yi-34B,mmlu_logical_fallacies,0-shot,accuracy,0.8834355828220859,0.025212327210507
01-ai/Yi-34B,mmlu_high_school_us_history,0-shot,accuracy,0.9117647058823528,0.0199073997913169
01-ai/Yi-34B,mmlu_philosophy,0-shot,accuracy,0.8392282958199357,0.0208623880823919
01-ai/Yi-34B,mmlu_jurisprudence,0-shot,accuracy,0.8981481481481481,0.0292392726756327
01-ai/Yi-34B,mmlu_international_law,0-shot,accuracy,0.9173553719008264,0.0251353823566042
01-ai/Yi-34B,mmlu_high_school_european_history,0-shot,accuracy,0.8666666666666667,0.0265444353127064
01-ai/Yi-34B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9792746113989638,0.010281417011909
01-ai/Yi-34B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8529411764705882,0.0230054594466739
01-ai/Yi-34B,mmlu_high_school_geography,0-shot,accuracy,0.8888888888888888,0.0223907876382167
01-ai/Yi-34B,mmlu_high_school_psychology,0-shot,accuracy,0.9155963302752294,0.0119188193273348
01-ai/Yi-34B,mmlu_public_relations,0-shot,accuracy,0.7363636363636363,0.0422022469297198
01-ai/Yi-34B,mmlu_us_foreign_policy,0-shot,accuracy,0.91,0.0287623491264661
01-ai/Yi-34B,mmlu_sociology,0-shot,accuracy,0.8855721393034826,0.0225093453251017
01-ai/Yi-34B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7871794871794872,0.020752423722128
01-ai/Yi-34B,mmlu_security_studies,0-shot,accuracy,0.8530612244897959,0.0226654004172176
01-ai/Yi-34B,mmlu_professional_psychology,0-shot,accuracy,0.8235294117647058,0.0154225120662625
01-ai/Yi-34B,mmlu_human_sexuality,0-shot,accuracy,0.8625954198473282,0.0301948239968044
01-ai/Yi-34B,mmlu_econometrics,0-shot,accuracy,0.5526315789473685,0.0467747300449119
01-ai/Yi-34B,mmlu_miscellaneous,0-shot,accuracy,0.9054916985951468,0.010461015338193
01-ai/Yi-34B,mmlu_marketing,0-shot,accuracy,0.9273504273504274,0.0170043685681323
01-ai/Yi-34B,mmlu_management,0-shot,accuracy,0.912621359223301,0.0279606891259706
01-ai/Yi-34B,mmlu_nutrition,0-shot,accuracy,0.869281045751634,0.0193018736242152
01-ai/Yi-34B,mmlu_medical_genetics,0-shot,accuracy,0.87,0.033799766898963
01-ai/Yi-34B,mmlu_human_aging,0-shot,accuracy,0.7892376681614349,0.0273730955005401
01-ai/Yi-34B,mmlu_professional_medicine,0-shot,accuracy,0.8235294117647058,0.0231574683085593
01-ai/Yi-34B,mmlu_college_medicine,0-shot,accuracy,0.7052023121387283,0.0347659960751647
01-ai/Yi-34B,mmlu_business_ethics,0-shot,accuracy,0.79,0.0409360180740332
01-ai/Yi-34B,mmlu_clinical_knowledge,0-shot,accuracy,0.7924528301886793,0.0249599180289112
01-ai/Yi-34B,mmlu_global_facts,0-shot,accuracy,0.52,0.0502116731568677
01-ai/Yi-34B,mmlu_virology,0-shot,accuracy,0.572289156626506,0.0385159768371853
01-ai/Yi-34B,mmlu_professional_accounting,0-shot,accuracy,0.6631205673758865,0.0281955348739667
01-ai/Yi-34B,mmlu_college_physics,0-shot,accuracy,0.4803921568627451,0.049713588843674
01-ai/Yi-34B,mmlu_high_school_physics,0-shot,accuracy,0.5231788079470199,0.0407809385916308
01-ai/Yi-34B,mmlu_high_school_biology,0-shot,accuracy,0.8806451612903226,0.0184434113253154
01-ai/Yi-34B,mmlu_college_biology,0-shot,accuracy,0.875,0.0276561049292943
01-ai/Yi-34B,mmlu_anatomy,0-shot,accuracy,0.7481481481481481,0.0374985070917402
01-ai/Yi-34B,mmlu_college_chemistry,0-shot,accuracy,0.48,0.0502116731568677
01-ai/Yi-34B,mmlu_computer_security,0-shot,accuracy,0.82,0.0386122919665369
01-ai/Yi-34B,mmlu_college_computer_science,0-shot,accuracy,0.64,0.0482418151324421
01-ai/Yi-34B,mmlu_astronomy,0-shot,accuracy,0.9013157894736842,0.0242702277375227
01-ai/Yi-34B,mmlu_college_mathematics,0-shot,accuracy,0.48,0.0502116731568677
01-ai/Yi-34B,mmlu_conceptual_physics,0-shot,accuracy,0.774468085106383,0.0273210784173875
01-ai/Yi-34B,mmlu_abstract_algebra,0-shot,accuracy,0.44,0.0498887651569858
01-ai/Yi-34B,mmlu_high_school_computer_science,0-shot,accuracy,0.82,0.0386122919665369
01-ai/Yi-34B,mmlu_machine_learning,0-shot,accuracy,0.6160714285714286,0.0461614307502854
01-ai/Yi-34B,mmlu_high_school_chemistry,0-shot,accuracy,0.6403940886699507,0.0337645824650956
01-ai/Yi-34B,mmlu_high_school_statistics,0-shot,accuracy,0.6481481481481481,0.0325685057029364
01-ai/Yi-34B,mmlu_elementary_mathematics,0-shot,accuracy,0.6507936507936508,0.0245522922093426
01-ai/Yi-34B,mmlu_electrical_engineering,0-shot,accuracy,0.8,0.0333333333333333
01-ai/Yi-34B,mmlu_high_school_mathematics,0-shot,accuracy,0.4666666666666667,0.0304177169617174
01-ai/Yi-34B,arc_challenge,25-shot,accuracy,0.6049488054607508,0.0142858982929381
01-ai/Yi-34B,arc_challenge,25-shot,acc_norm,0.6459044368600683,0.0139754541227565
01-ai/Yi-34B,truthfulqa_mc2,0-shot,accuracy,0.562494191768382,0.0151562147312676
01-ai/Yi-34B,truthfulqa_gen,0-shot,bleu_max,20.330647080409832,0.7024796684315535
01-ai/Yi-34B,truthfulqa_gen,0-shot,bleu_acc,0.4871481028151774,0.0174977179442998
01-ai/Yi-34B,truthfulqa_gen,0-shot,bleu_diff,1.203485922819323,0.588166050547877
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge1_max,45.71688577672627,0.8227479818577124
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge1_acc,0.5140758873929009,0.0174965637170427
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge1_diff,2.52766094456488,0.781731669558233
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge2_max,29.91034642330512,0.9393337389294316
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge2_acc,0.3769889840881273,0.0169655175789303
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge2_diff,0.485430202097819,0.8945392225724594
01-ai/Yi-34B,truthfulqa_gen,0-shot,rougeL_max,42.9851750782942,0.8216423235693433
01-ai/Yi-34B,truthfulqa_gen,0-shot,rougeL_acc,0.5361077111383109,0.0174578004222686
01-ai/Yi-34B,truthfulqa_gen,0-shot,rougeL_diff,2.5484074342195404,0.7888693977638205
01-ai/Yi-34B,truthfulqa_mc1,0-shot,accuracy,0.4075887392900856,0.0172019492345531
Salesforce/codegen-6B-multi,minerva_math_precalc,5-shot,accuracy,0.0109890109890109,0.0044656184273314
Salesforce/codegen-6B-multi,minerva_math_prealgebra,5-shot,accuracy,0.0011481056257175,0.0011481056257175
Salesforce/codegen-6B-multi,minerva_math_num_theory,5-shot,accuracy,0.0018518518518518,0.0018518518518518
Salesforce/codegen-6B-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.0099667774086378,0.003307493466972
Salesforce/codegen-6B-multi,minerva_math_geometry,5-shot,accuracy,0.0020876826722338,0.0020876826722338
Salesforce/codegen-6B-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.0063291139240506,0.003646382041065
Salesforce/codegen-6B-multi,minerva_math_algebra,5-shot,accuracy,0.008424599831508,0.0026539648581165
Salesforce/codegen-6B-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,arithmetic_3da,5-shot,accuracy,0.004,0.0014117352790976
Salesforce/codegen-6B-multi,arithmetic_3ds,5-shot,accuracy,0.009,0.0021122809627113
Salesforce/codegen-6B-multi,arithmetic_4da,5-shot,accuracy,0.002,0.0009992493430694
Salesforce/codegen-6B-multi,arithmetic_2ds,5-shot,accuracy,0.0615,0.0053733892149953
Salesforce/codegen-6B-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,arithmetic_1dc,5-shot,accuracy,0.093,0.0064958908780204
Salesforce/codegen-6B-multi,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005
Salesforce/codegen-6B-multi,arithmetic_2dm,5-shot,accuracy,0.039,0.0043299970481765
Salesforce/codegen-6B-multi,arithmetic_2da,5-shot,accuracy,0.052,0.0049659168503995
Salesforce/codegen-6B-multi,gsm8k_cot,5-shot,accuracy,0.026535253980288,0.0044270459872651
Salesforce/codegen-6B-multi,gsm8k,5-shot,accuracy,0.0098559514783927,0.0027210765770416
Salesforce/codegen-6B-multi,anli_r2,0-shot,brier_score,0.7472883683198149,
Salesforce/codegen-6B-multi,anli_r3,0-shot,brier_score,0.7776246952766512,
Salesforce/codegen-6B-multi,anli_r1,0-shot,brier_score,0.7736621012350583,
Salesforce/codegen-6B-multi,xnli_eu,0-shot,brier_score,1.0388889959896963,
Salesforce/codegen-6B-multi,xnli_vi,0-shot,brier_score,1.2154173472405334,
Salesforce/codegen-6B-multi,xnli_ru,0-shot,brier_score,0.8730313226525406,
Salesforce/codegen-6B-multi,xnli_zh,0-shot,brier_score,0.9828620736086736,
Salesforce/codegen-6B-multi,xnli_tr,0-shot,brier_score,1.0519111336327809,
Salesforce/codegen-6B-multi,xnli_fr,0-shot,brier_score,1.0820785121586356,
Salesforce/codegen-6B-multi,xnli_en,0-shot,brier_score,0.7349387098851814,
Salesforce/codegen-6B-multi,xnli_ur,0-shot,brier_score,1.3175247071589906,
Salesforce/codegen-6B-multi,xnli_ar,0-shot,brier_score,1.0337470838746723,
Salesforce/codegen-6B-multi,xnli_de,0-shot,brier_score,0.9493973793623972,
Salesforce/codegen-6B-multi,xnli_hi,0-shot,brier_score,0.9824796835327766,
Salesforce/codegen-6B-multi,xnli_es,0-shot,brier_score,1.2007293365919631,
Salesforce/codegen-6B-multi,xnli_bg,0-shot,brier_score,0.957486536267458,
Salesforce/codegen-6B-multi,xnli_sw,0-shot,brier_score,0.8975406853203322,
Salesforce/codegen-6B-multi,xnli_el,0-shot,brier_score,0.8985200188087877,
Salesforce/codegen-6B-multi,xnli_th,0-shot,brier_score,0.9106758374214105,
Salesforce/codegen-6B-multi,logiqa2,0-shot,brier_score,1.1702528650156314,
Salesforce/codegen-6B-multi,mathqa,0-shot,brier_score,0.959189809900026,
Salesforce/codegen-6B-multi,lambada_standard,0-shot,perplexity,71.09167247462723,2.933823524943788
Salesforce/codegen-6B-multi,lambada_standard,0-shot,accuracy,0.2544149039394527,0.0060678097640315
Salesforce/codegen-6B-multi,lambada_openai,0-shot,perplexity,46.16956426063179,1.827364394951524
Salesforce/codegen-6B-multi,lambada_openai,0-shot,accuracy,0.3027362701339026,0.0064009264675299
Salesforce/codegen-6B-multi,mmlu_world_religions,0-shot,accuracy,0.2807017543859649,0.0344629621708842
Salesforce/codegen-6B-multi,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.0371848900681811
Salesforce/codegen-6B-multi,mmlu_prehistory,0-shot,accuracy,0.2376543209876543,0.0236835918370085
Salesforce/codegen-6B-multi,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.0143559119647678
Salesforce/codegen-6B-multi,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
Salesforce/codegen-6B-multi,mmlu_moral_disputes,0-shot,accuracy,0.2861271676300578,0.0243321467791341
Salesforce/codegen-6B-multi,mmlu_professional_law,0-shot,accuracy,0.258148631029987,0.0111769237193133
Salesforce/codegen-6B-multi,mmlu_logical_fallacies,0-shot,accuracy,0.2760736196319018,0.0351238528370505
Salesforce/codegen-6B-multi,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.0303915336927415
Salesforce/codegen-6B-multi,mmlu_philosophy,0-shot,accuracy,0.3022508038585209,0.0260827006953996
Salesforce/codegen-6B-multi,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
Salesforce/codegen-6B-multi,mmlu_international_law,0-shot,accuracy,0.3305785123966942,0.0429434084521209
Salesforce/codegen-6B-multi,mmlu_high_school_european_history,0-shot,accuracy,0.2787878787878788,0.0350143870629678
Salesforce/codegen-6B-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2072538860103626,0.0292528232918036
Salesforce/codegen-6B-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.2563025210084033,0.0283596208705339
Salesforce/codegen-6B-multi,mmlu_high_school_geography,0-shot,accuracy,0.207070707070707,0.028869778460267
Salesforce/codegen-6B-multi,mmlu_high_school_psychology,0-shot,accuracy,0.2532110091743119,0.018644073041375
Salesforce/codegen-6B-multi,mmlu_public_relations,0-shot,accuracy,0.3181818181818182,0.044612721759105
Salesforce/codegen-6B-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.18,0.0386122919665369
Salesforce/codegen-6B-multi,mmlu_sociology,0-shot,accuracy,0.2388059701492537,0.0301477759354092
Salesforce/codegen-6B-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2205128205128205,0.0210206726808279
Salesforce/codegen-6B-multi,mmlu_security_studies,0-shot,accuracy,0.1755102040816326,0.02435280072297
Salesforce/codegen-6B-multi,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.0176678416123789
Salesforce/codegen-6B-multi,mmlu_human_sexuality,0-shot,accuracy,0.1984732824427481,0.0349814938546247
Salesforce/codegen-6B-multi,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.0418577442402205
Salesforce/codegen-6B-multi,mmlu_miscellaneous,0-shot,accuracy,0.2860791826309067,0.0161608714051275
Salesforce/codegen-6B-multi,mmlu_marketing,0-shot,accuracy,0.2264957264957265,0.0274210072953929
Salesforce/codegen-6B-multi,mmlu_management,0-shot,accuracy,0.174757281553398,0.0376017800602662
Salesforce/codegen-6B-multi,mmlu_nutrition,0-shot,accuracy,0.261437908496732,0.0251609982142924
Salesforce/codegen-6B-multi,mmlu_medical_genetics,0-shot,accuracy,0.26,0.0440844002276807
Salesforce/codegen-6B-multi,mmlu_human_aging,0-shot,accuracy,0.3901345291479821,0.0327376672545915
Salesforce/codegen-6B-multi,mmlu_professional_medicine,0-shot,accuracy,0.1911764705882352,0.0238868819224403
Salesforce/codegen-6B-multi,mmlu_college_medicine,0-shot,accuracy,0.2427745664739884,0.0326926380614177
Salesforce/codegen-6B-multi,mmlu_business_ethics,0-shot,accuracy,0.26,0.0440844002276807
Salesforce/codegen-6B-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.2716981132075471,0.0273777066246707
Salesforce/codegen-6B-multi,mmlu_global_facts,0-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-6B-multi,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.0361080501803102
Salesforce/codegen-6B-multi,mmlu_professional_accounting,0-shot,accuracy,0.226950354609929,0.0249871063656429
Salesforce/codegen-6B-multi,mmlu_college_physics,0-shot,accuracy,0.196078431372549,0.0395058186117996
Salesforce/codegen-6B-multi,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.0368488152138902
Salesforce/codegen-6B-multi,mmlu_high_school_biology,0-shot,accuracy,0.2354838709677419,0.0241376324293377
Salesforce/codegen-6B-multi,mmlu_college_biology,0-shot,accuracy,0.2847222222222222,0.0377380999068693
Salesforce/codegen-6B-multi,mmlu_anatomy,0-shot,accuracy,0.2888888888888888,0.0391545063041425
Salesforce/codegen-6B-multi,mmlu_college_chemistry,0-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-6B-multi,mmlu_computer_security,0-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-6B-multi,mmlu_college_computer_science,0-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-6B-multi,mmlu_astronomy,0-shot,accuracy,0.2171052631578947,0.0335504530488292
Salesforce/codegen-6B-multi,mmlu_college_mathematics,0-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-6B-multi,mmlu_conceptual_physics,0-shot,accuracy,0.3446808510638298,0.0310689859631221
Salesforce/codegen-6B-multi,mmlu_abstract_algebra,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-6B-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-6B-multi,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.0428785875134045
Salesforce/codegen-6B-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.0305165307326944
Salesforce/codegen-6B-multi,mmlu_high_school_statistics,0-shot,accuracy,0.2685185185185185,0.0302252261600124
Salesforce/codegen-6B-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.2407407407407407,0.0220190800122178
Salesforce/codegen-6B-multi,mmlu_electrical_engineering,0-shot,accuracy,0.3103448275862069,0.0385528961637894
Salesforce/codegen-6B-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.2444444444444444,0.0262027665346521
Salesforce/codegen-6B-multi,arc_challenge,25-shot,accuracy,0.2244027303754266,0.0121914049386038
Salesforce/codegen-6B-multi,arc_challenge,25-shot,acc_norm,0.2679180887372013,0.0129420301951364
Salesforce/codegen-6B-multi,hellaswag,10-shot,accuracy,0.3350926110336586,0.0047105814966393
Salesforce/codegen-6B-multi,hellaswag,10-shot,acc_norm,0.4110734913363871,0.0049102296432627
Salesforce/codegen-6B-multi,truthfulqa_mc2,0-shot,accuracy,0.456190954016092,0.0151797102790657
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,bleu_max,18.4162863206651,0.6674446234293927
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,bleu_acc,0.4186046511627907,0.0172700152844768
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,bleu_diff,2.02768707937965,0.7080695144029784
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge1_max,39.01657959182641,0.9129251342011888
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge1_acc,0.3671970624235006,0.0168748050014531
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge1_diff,0.7733335673342009,1.0566035740226651
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge2_max,21.648507625466436,1.002315340981092
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge2_acc,0.237454100367197,0.0148962774410418
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge2_diff,1.2461063185143295,1.0682476326083843
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rougeL_max,36.58025015195491,0.9084853751956918
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rougeL_acc,0.3671970624235006,0.0168748050014531
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rougeL_diff,1.0039000965278462,1.069567876870668
Salesforce/codegen-6B-multi,truthfulqa_mc1,0-shot,accuracy,0.2680538555691554,0.0155062047228345
Salesforce/codegen-6B-multi,winogrande,5-shot,accuracy,0.5390686661404893,0.0140095216809803
meta-llama/Meta-Llama-3-70B,arc:challenge,25-shot,accuracy,0.6510238907849829,0.0139289334613825
meta-llama/Meta-Llama-3-70B,arc:challenge,25-shot,acc_norm,0.6877133105802048,0.013542598541688
meta-llama/Meta-Llama-3-70B,hellaswag,10-shot,accuracy,0.6951802429794861,0.0045939026019793
meta-llama/Meta-Llama-3-70B,hellaswag,10-shot,acc_norm,0.8798048197570205,0.0032452503945652
meta-llama/Meta-Llama-3-70B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.48,0.0502116731568677
meta-llama/Meta-Llama-3-70B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.48,0.0502116731568677
meta-llama/Meta-Llama-3-70B,hendrycksTest-anatomy,5-shot,accuracy,0.7851851851851852,0.0354785419856082
meta-llama/Meta-Llama-3-70B,hendrycksTest-anatomy,5-shot,acc_norm,0.7851851851851852,0.0354785419856082
meta-llama/Meta-Llama-3-70B,hendrycksTest-astronomy,5-shot,accuracy,0.9210526315789472,0.0219443428182479
meta-llama/Meta-Llama-3-70B,hendrycksTest-astronomy,5-shot,acc_norm,0.9210526315789472,0.0219443428182479
meta-llama/Meta-Llama-3-70B,hendrycksTest-business_ethics,5-shot,accuracy,0.82,0.0386122919665369
meta-llama/Meta-Llama-3-70B,hendrycksTest-business_ethics,5-shot,acc_norm,0.82,0.0386122919665369
meta-llama/Meta-Llama-3-70B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8566037735849057,0.0215703349766249
meta-llama/Meta-Llama-3-70B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8566037735849057,0.0215703349766249
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_biology,5-shot,accuracy,0.9375,0.0202421961134779
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_biology,5-shot,acc_norm,0.9375,0.0202421961134779
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_chemistry,5-shot,accuracy,0.62,0.0487831731214563
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.62,0.0487831731214563
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_computer_science,5-shot,accuracy,0.73,0.0446196043338473
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.73,0.0446196043338473
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_mathematics,5-shot,accuracy,0.52,0.0502116731568677
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.52,0.0502116731568677
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_medicine,5-shot,accuracy,0.7976878612716763,0.0306311455391988
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7976878612716763,0.0306311455391988
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_physics,5-shot,accuracy,0.4901960784313725,0.0497422946042281
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_physics,5-shot,acc_norm,0.4901960784313725,0.0497422946042281
meta-llama/Meta-Llama-3-70B,hendrycksTest-computer_security,5-shot,accuracy,0.83,0.0377525168068637
meta-llama/Meta-Llama-3-70B,hendrycksTest-computer_security,5-shot,acc_norm,0.83,0.0377525168068637
meta-llama/Meta-Llama-3-70B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.825531914893617,0.0248094423355039
meta-llama/Meta-Llama-3-70B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.825531914893617,0.0248094423355039
meta-llama/Meta-Llama-3-70B,hendrycksTest-econometrics,5-shot,accuracy,0.7280701754385965,0.0418577442402205
meta-llama/Meta-Llama-3-70B,hendrycksTest-econometrics,5-shot,acc_norm,0.7280701754385965,0.0418577442402205
meta-llama/Meta-Llama-3-70B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7517241379310344,0.0360010569272777
meta-llama/Meta-Llama-3-70B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7517241379310344,0.0360010569272777
meta-llama/Meta-Llama-3-70B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.626984126984127,0.0249069904589925
meta-llama/Meta-Llama-3-70B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.626984126984127,0.0249069904589925
meta-llama/Meta-Llama-3-70B,hendrycksTest-formal_logic,5-shot,accuracy,0.6507936507936508,0.0426390689279513
meta-llama/Meta-Llama-3-70B,hendrycksTest-formal_logic,5-shot,acc_norm,0.6507936507936508,0.0426390689279513
meta-llama/Meta-Llama-3-70B,hendrycksTest-global_facts,5-shot,accuracy,0.48,0.0502116731568677
meta-llama/Meta-Llama-3-70B,hendrycksTest-global_facts,5-shot,acc_norm,0.48,0.0502116731568677
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_biology,5-shot,accuracy,0.9032258064516128,0.0168189434163451
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.9032258064516128,0.0168189434163451
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.7093596059113301,0.0319474007226554
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.7093596059113301,0.0319474007226554
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.86,0.0348735088019777
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.86,0.0348735088019777
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8666666666666667,0.0265444353127064
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8666666666666667,0.0265444353127064
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_geography,5-shot,accuracy,0.9393939393939394,0.0169999949274216
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9393939393939394,0.0169999949274216
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9792746113989638,0.010281417011909
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9792746113989638,0.010281417011909
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8256410256410256,0.0192372498034052
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8256410256410256,0.0192372498034052
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4925925925925926,0.0304821923951914
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4925925925925926,0.0304821923951914
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8865546218487395,0.0206002257502048
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8865546218487395,0.0206002257502048
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_physics,5-shot,accuracy,0.5695364238410596,0.0404280996139563
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5695364238410596,0.0404280996139563
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9412844036697248,0.010079470534014
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9412844036697248,0.010079470534014
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.7314814814814815,0.0302252261600124
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.7314814814814815,0.0302252261600124
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9558823529411764,0.0144131987057048
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9558823529411764,0.0144131987057048
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.940928270042194,0.0153465974638886
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.940928270042194,0.0153465974638886
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_aging,5-shot,accuracy,0.8251121076233184,0.0254952846264449
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_aging,5-shot,acc_norm,0.8251121076233184,0.0254952846264449
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8854961832061069,0.0279274737535974
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8854961832061069,0.0279274737535974
meta-llama/Meta-Llama-3-70B,hendrycksTest-international_law,5-shot,accuracy,0.8925619834710744,0.0282688121925406
meta-llama/Meta-Llama-3-70B,hendrycksTest-international_law,5-shot,acc_norm,0.8925619834710744,0.0282688121925406
meta-llama/Meta-Llama-3-70B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8611111111111112,0.0334327006286962
meta-llama/Meta-Llama-3-70B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8611111111111112,0.0334327006286962
meta-llama/Meta-Llama-3-70B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8773006134969326,0.0257773284269789
meta-llama/Meta-Llama-3-70B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8773006134969326,0.0257773284269789
meta-llama/Meta-Llama-3-70B,hendrycksTest-machine_learning,5-shot,accuracy,0.625,0.0459509138808629
meta-llama/Meta-Llama-3-70B,hendrycksTest-machine_learning,5-shot,acc_norm,0.625,0.0459509138808629
meta-llama/Meta-Llama-3-70B,hendrycksTest-management,5-shot,accuracy,0.912621359223301,0.0279606891259706
meta-llama/Meta-Llama-3-70B,hendrycksTest-management,5-shot,acc_norm,0.912621359223301,0.0279606891259706
meta-llama/Meta-Llama-3-70B,hendrycksTest-marketing,5-shot,accuracy,0.9401709401709402,0.0155375142632538
meta-llama/Meta-Llama-3-70B,hendrycksTest-marketing,5-shot,acc_norm,0.9401709401709402,0.0155375142632538
meta-llama/Meta-Llama-3-70B,hendrycksTest-medical_genetics,5-shot,accuracy,0.91,0.0287623491264661
meta-llama/Meta-Llama-3-70B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.91,0.0287623491264661
meta-llama/Meta-Llama-3-70B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9195402298850576,0.0097268313161418
meta-llama/Meta-Llama-3-70B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9195402298850576,0.0097268313161418
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_disputes,5-shot,accuracy,0.846820809248555,0.0193903701089699
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.846820809248555,0.0193903701089699
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6145251396648045,0.0162779270396381
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6145251396648045,0.0162779270396381
meta-llama/Meta-Llama-3-70B,hendrycksTest-nutrition,5-shot,accuracy,0.869281045751634,0.0193018736242152
meta-llama/Meta-Llama-3-70B,hendrycksTest-nutrition,5-shot,acc_norm,0.869281045751634,0.0193018736242152
meta-llama/Meta-Llama-3-70B,hendrycksTest-philosophy,5-shot,accuracy,0.864951768488746,0.0194115202473351
meta-llama/Meta-Llama-3-70B,hendrycksTest-philosophy,5-shot,acc_norm,0.864951768488746,0.0194115202473351
meta-llama/Meta-Llama-3-70B,hendrycksTest-prehistory,5-shot,accuracy,0.9166666666666666,0.0153784949853727
meta-llama/Meta-Llama-3-70B,hendrycksTest-prehistory,5-shot,acc_norm,0.9166666666666666,0.0153784949853727
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_accounting,5-shot,accuracy,0.6382978723404256,0.0286638201471994
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6382978723404256,0.0286638201471994
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_law,5-shot,accuracy,0.6166883963494133,0.0124176036629011
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_law,5-shot,acc_norm,0.6166883963494133,0.0124176036629011
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8676470588235294,0.0205851341892206
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8676470588235294,0.0205851341892206
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8562091503267973,0.0141949854694191
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8562091503267973,0.0141949854694191
meta-llama/Meta-Llama-3-70B,hendrycksTest-public_relations,5-shot,accuracy,0.7545454545454545,0.0412206650287828
meta-llama/Meta-Llama-3-70B,hendrycksTest-public_relations,5-shot,acc_norm,0.7545454545454545,0.0412206650287828
meta-llama/Meta-Llama-3-70B,hendrycksTest-security_studies,5-shot,accuracy,0.8489795918367347,0.0229230040947368
meta-llama/Meta-Llama-3-70B,hendrycksTest-security_studies,5-shot,acc_norm,0.8489795918367347,0.0229230040947368
meta-llama/Meta-Llama-3-70B,hendrycksTest-sociology,5-shot,accuracy,0.9353233830845772,0.017391600291491
meta-llama/Meta-Llama-3-70B,hendrycksTest-sociology,5-shot,acc_norm,0.9353233830845772,0.017391600291491
meta-llama/Meta-Llama-3-70B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.94,0.0238683256575941
meta-llama/Meta-Llama-3-70B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.94,0.0238683256575941
meta-llama/Meta-Llama-3-70B,hendrycksTest-virology,5-shot,accuracy,0.5843373493975904,0.0383672217659805
meta-llama/Meta-Llama-3-70B,hendrycksTest-virology,5-shot,acc_norm,0.5843373493975904,0.0383672217659805
meta-llama/Meta-Llama-3-70B,hendrycksTest-world_religions,5-shot,accuracy,0.9064327485380116,0.0223359932311632
meta-llama/Meta-Llama-3-70B,hendrycksTest-world_religions,5-shot,acc_norm,0.9064327485380116,0.0223359932311632
meta-llama/Meta-Llama-3-70B,truthfulqa:mc,0-shot,mc1,0.2986536107711138,0.0160215706137685
meta-llama/Meta-Llama-3-70B,truthfulqa:mc,0-shot,mc2,0.4556236820150089,0.0139634213238178
meta-llama/Meta-Llama-3-70B,winogrande,5-shot,accuracy,0.8531965272296764,0.0099466274402507
meta-llama/Meta-Llama-3-70B,gsm8k,5-shot,accuracy,0.7687642153146323,0.0116135875031666
model_name,minerva_math_precalc,5-shot,accuracy,0.1355311355311355,0.0146620928473954
model_name,minerva_math_prealgebra,5-shot,accuracy,0.4890929965556831,0.0169475538965277
model_name,minerva_math_num_theory,5-shot,accuracy,0.1703703703703703,0.0161936511111117
model_name,minerva_math_intermediate_algebra,5-shot,accuracy,0.1173864894795127,0.0107174403304311
model_name,minerva_math_geometry,5-shot,accuracy,0.2129436325678496,0.0187249772732632
model_name,minerva_math_counting_and_prob,5-shot,accuracy,0.1962025316455696,0.0182597593715657
model_name,minerva_math_algebra,5-shot,accuracy,0.4018534119629318,0.0142362399840764
model_name,fld_default,0-shot,accuracy,0.0,
model_name,fld_star,0-shot,accuracy,0.0,
model_name,arithmetic_3da,5-shot,accuracy,0.9945,0.0016541593398342
model_name,arithmetic_3ds,5-shot,accuracy,0.9995,0.0005
model_name,arithmetic_4da,5-shot,accuracy,0.983,0.0028913110935905
model_name,arithmetic_2ds,5-shot,accuracy,1.0,
model_name,arithmetic_5ds,5-shot,accuracy,0.982,0.0029736208922129
model_name,arithmetic_5da,5-shot,accuracy,0.9755,0.0034577236625362
model_name,arithmetic_1dc,5-shot,accuracy,0.935,0.0055138644661141
model_name,arithmetic_4ds,5-shot,accuracy,0.995,0.0015775754727385
model_name,arithmetic_2dm,5-shot,accuracy,0.9525,0.0047574354011167
model_name,arithmetic_2da,5-shot,accuracy,1.0,
model_name,gsm8k_cot,5-shot,accuracy,0.6072782410917361,0.0134517453495865
model_name,gsm8k,5-shot,accuracy,0.5837755875663382,0.0135777883346526
mistralai/Mixtral-8x7B-v0.1,arc:challenge,25-shot,accuracy,0.6373720136518771,0.014049106564955
mistralai/Mixtral-8x7B-v0.1,arc:challenge,25-shot,acc_norm,0.6638225255972696,0.0138048550262057
mistralai/Mixtral-8x7B-v0.1,hellaswag,10-shot,accuracy,0.6670981876120294,0.0047028862731894
mistralai/Mixtral-8x7B-v0.1,hellaswag,10-shot,acc_norm,0.8653654650468035,0.0034063520713417
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.34,0.0476095228569523
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.34,0.0476095228569523
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.7185185185185186,0.0388500424580025
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.7185185185185186,0.0388500424580025
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.8289473684210527,0.030643607071677
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.8289473684210527,0.030643607071677
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.76,0.0429234695990928
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.76,0.0429234695990928
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7849056603773585,0.0252883945028913
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7849056603773585,0.0252883945028913
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.8680555555555556,0.0283009683820444
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.8680555555555556,0.0283009683820444
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.54,0.0500908265962033
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.54,0.0500908265962033
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.63,0.0485236587093909
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.63,0.0485236587093909
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.46,0.0500908265962033
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.46,0.0500908265962033
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.6994219653179191,0.0349610148119117
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.6994219653179191,0.0349610148119117
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.4607843137254901,0.0495985996638418
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.4607843137254901,0.0495985996638418
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.81,0.0394277244403662
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.81,0.0394277244403662
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6808510638297872,0.03047297336338
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6808510638297872,0.03047297336338
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.6491228070175439,0.0448953935027069
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.6491228070175439,0.0448953935027069
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6896551724137931,0.0385528961637894
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6896551724137931,0.0385528961637894
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4814814814814814,0.0257336419918389
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4814814814814814,0.0257336419918389
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.5634920634920635,0.0443593289285146
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.5634920634920635,0.0443593289285146
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.51,0.0502418393795691
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.51,0.0502418393795691
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.8419354838709677,0.0207528315118752
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8419354838709677,0.0207528315118752
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6354679802955665,0.0338640574606209
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6354679802955665,0.0338640574606209
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.72,0.0451260859854212
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.72,0.0451260859854212
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8181818181818182,0.0301176889295035
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8181818181818182,0.0301176889295035
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.8636363636363636,0.0244501559731898
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8636363636363636,0.0244501559731898
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9378238341968912,0.0174269741542405
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9378238341968912,0.0174269741542405
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7051282051282052,0.0231193627582323
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.7051282051282052,0.0231193627582323
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3851851851851852,0.0296709061246308
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3851851851851852,0.0296709061246308
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7857142857142857,0.0266535315967154
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7857142857142857,0.0266535315967154
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.4900662251655629,0.0408167710724843
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4900662251655629,0.0408167710724843
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8807339449541285,0.0138957292925889
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8807339449541285,0.0138957292925889
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6481481481481481,0.0325685057029364
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6481481481481481,0.0325685057029364
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8480392156862745,0.0251956584289317
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8480392156862745,0.0251956584289317
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.890295358649789,0.0203434007348688
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.890295358649789,0.0203434007348688
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.7802690582959642,0.0277901770643835
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.7802690582959642,0.0277901770643835
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.8091603053435115,0.0344651335075259
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8091603053435115,0.0344651335075259
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.8760330578512396,0.0300830987160352
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.8760330578512396,0.0300830987160352
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.8333333333333334,0.0360281417639264
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8333333333333334,0.0360281417639264
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7730061349693251,0.0329109957861576
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7730061349693251,0.0329109957861576
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.5357142857142857,0.0473366789005375
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.5357142857142857,0.0473366789005375
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-management,5-shot,accuracy,0.883495145631068,0.031766839486404
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-management,5-shot,acc_norm,0.883495145631068,0.031766839486404
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.9188034188034188,0.0178937849040185
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.9188034188034188,0.0178937849040185
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.78,0.0416333199893226
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.78,0.0416333199893226
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.8748403575989783,0.0118329542393057
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8748403575989783,0.0118329542393057
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.7976878612716763,0.0216280773801961
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7976878612716763,0.0216280773801961
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4011173184357542,0.016392221899407
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4011173184357542,0.016392221899407
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.8235294117647058,0.0218285960531084
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.8235294117647058,0.0218285960531084
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.7845659163987139,0.0233502254754714
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.7845659163987139,0.0233502254754714
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.8395061728395061,0.020423955354778
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.8395061728395061,0.020423955354778
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.5177304964539007,0.0298087396422377
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5177304964539007,0.0298087396422377
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.5319426336375489,0.0127441497048696
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.5319426336375489,0.0127441497048696
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.8125,0.0237097882538117
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8125,0.0237097882538117
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.7843137254901961,0.0166393193503132
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7843137254901961,0.0166393193503132
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.7,0.0438931145464428
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.7,0.0438931145464428
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.7877551020408163,0.0261769671978667
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.7877551020408163,0.0261769671978667
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.0220763261018246
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.0220763261018246
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.0272659924344291
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5120481927710844,0.0389136449583581
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5120481927710844,0.0389136449583581
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.0251729843501557
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.0251729843501557
mistralai/Mixtral-8x7B-v0.1,truthfulqa:mc,0-shot,mc1,0.3182374541003672,0.0163059886489206
mistralai/Mixtral-8x7B-v0.1,truthfulqa:mc,0-shot,mc2,0.4680543300316138,0.0141201705429739
mistralai/Mixtral-8x7B-v0.1,winogrande,5-shot,accuracy,0.8176795580110497,0.0108515655942672
mistralai/Mixtral-8x7B-v0.1,gsm8k,5-shot,accuracy,0.5852918877937832,0.0135706238423045
Salesforce/codegen-6B-multi,arc:challenge,25-shot,accuracy,0.2380546075085324,0.0124457700280262
Salesforce/codegen-6B-multi,arc:challenge,25-shot,acc_norm,0.2721843003412969,0.0130066004064237
Salesforce/codegen-6B-multi,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215
Salesforce/codegen-6B-multi,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215
Salesforce/codegen-6B-multi,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.0388500424580025
Salesforce/codegen-6B-multi,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.0388500424580025
Salesforce/codegen-6B-multi,hendrycksTest-astronomy,5-shot,accuracy,0.2039473684210526,0.0327900040631005
Salesforce/codegen-6B-multi,hendrycksTest-astronomy,5-shot,acc_norm,0.2039473684210526,0.0327900040631005
Salesforce/codegen-6B-multi,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.0440844002276807
Salesforce/codegen-6B-multi,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.0440844002276807
Salesforce/codegen-6B-multi,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2716981132075471,0.0273777066246707
Salesforce/codegen-6B-multi,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2716981132075471,0.0273777066246707
Salesforce/codegen-6B-multi,hendrycksTest-college_biology,5-shot,accuracy,0.2916666666666667,0.0380096806055485
Salesforce/codegen-6B-multi,hendrycksTest-college_biology,5-shot,acc_norm,0.2916666666666667,0.0380096806055485
Salesforce/codegen-6B-multi,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-6B-multi,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
Salesforce/codegen-6B-multi,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-6B-multi,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.0440844002276808
Salesforce/codegen-6B-multi,hendrycksTest-college_mathematics,5-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-6B-multi,hendrycksTest-college_mathematics,5-shot,acc_norm,0.23,0.042295258468165
Salesforce/codegen-6B-multi,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
Salesforce/codegen-6B-multi,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
Salesforce/codegen-6B-multi,hendrycksTest-college_physics,5-shot,accuracy,0.196078431372549,0.0395058186117996
Salesforce/codegen-6B-multi,hendrycksTest-college_physics,5-shot,acc_norm,0.196078431372549,0.0395058186117996
Salesforce/codegen-6B-multi,hendrycksTest-computer_security,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-6B-multi,hendrycksTest-computer_security,5-shot,acc_norm,0.29,0.0456048021572068
Salesforce/codegen-6B-multi,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3446808510638298,0.0310689859631221
Salesforce/codegen-6B-multi,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3446808510638298,0.0310689859631221
Salesforce/codegen-6B-multi,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
Salesforce/codegen-6B-multi,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
Salesforce/codegen-6B-multi,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3034482758620689,0.0383122604885033
Salesforce/codegen-6B-multi,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3034482758620689,0.0383122604885033
Salesforce/codegen-6B-multi,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.246031746031746,0.0221820372029483
Salesforce/codegen-6B-multi,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.246031746031746,0.0221820372029483
Salesforce/codegen-6B-multi,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.0371848900681811
Salesforce/codegen-6B-multi,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.0371848900681811
Salesforce/codegen-6B-multi,hendrycksTest-global_facts,5-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-6B-multi,hendrycksTest-global_facts,5-shot,acc_norm,0.23,0.042295258468165
Salesforce/codegen-6B-multi,hendrycksTest-high_school_biology,5-shot,accuracy,0.232258064516129,0.0240222561303082
Salesforce/codegen-6B-multi,hendrycksTest-high_school_biology,5-shot,acc_norm,0.232258064516129,0.0240222561303082
Salesforce/codegen-6B-multi,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
Salesforce/codegen-6B-multi,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.0305165307326944
Salesforce/codegen-6B-multi,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
Salesforce/codegen-6B-multi,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526
Salesforce/codegen-6B-multi,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2666666666666666,0.0345313180188541
Salesforce/codegen-6B-multi,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2666666666666666,0.0345313180188541
Salesforce/codegen-6B-multi,hendrycksTest-high_school_geography,5-shot,accuracy,0.2171717171717171,0.0293766164849456
Salesforce/codegen-6B-multi,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2171717171717171,0.0293766164849456
Salesforce/codegen-6B-multi,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2072538860103626,0.0292528232918036
Salesforce/codegen-6B-multi,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2072538860103626,0.0292528232918036
Salesforce/codegen-6B-multi,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2205128205128205,0.0210206726808279
Salesforce/codegen-6B-multi,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2205128205128205,0.0210206726808279
Salesforce/codegen-6B-multi,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2444444444444444,0.0262027665346521
Salesforce/codegen-6B-multi,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2444444444444444,0.0262027665346521
Salesforce/codegen-6B-multi,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2521008403361344,0.0282055450332777
Salesforce/codegen-6B-multi,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2521008403361344,0.0282055450332777
Salesforce/codegen-6B-multi,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.0368488152138902
Salesforce/codegen-6B-multi,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.0368488152138902
Salesforce/codegen-6B-multi,hendrycksTest-high_school_psychology,5-shot,accuracy,0.255045871559633,0.0186885008565358
Salesforce/codegen-6B-multi,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.255045871559633,0.0186885008565358
Salesforce/codegen-6B-multi,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2638888888888889,0.0300582027043098
Salesforce/codegen-6B-multi,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2638888888888889,0.0300582027043098
Salesforce/codegen-6B-multi,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2647058823529412,0.0309645179269234
Salesforce/codegen-6B-multi,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2647058823529412,0.0309645179269234
Salesforce/codegen-6B-multi,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2827004219409282,0.0293128141539559
Salesforce/codegen-6B-multi,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2827004219409282,0.0293128141539559
Salesforce/codegen-6B-multi,hendrycksTest-human_aging,5-shot,accuracy,0.4035874439461883,0.0329280281933031
Salesforce/codegen-6B-multi,hendrycksTest-human_aging,5-shot,acc_norm,0.4035874439461883,0.0329280281933031
Salesforce/codegen-6B-multi,hendrycksTest-human_sexuality,5-shot,accuracy,0.1984732824427481,0.0349814938546247
Salesforce/codegen-6B-multi,hendrycksTest-human_sexuality,5-shot,acc_norm,0.1984732824427481,0.0349814938546247
Salesforce/codegen-6B-multi,hendrycksTest-international_law,5-shot,accuracy,0.3471074380165289,0.0434572457029253
Salesforce/codegen-6B-multi,hendrycksTest-international_law,5-shot,acc_norm,0.3471074380165289,0.0434572457029253
Salesforce/codegen-6B-multi,hendrycksTest-jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
Salesforce/codegen-6B-multi,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2314814814814814,0.0407749470925262
Salesforce/codegen-6B-multi,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
Salesforce/codegen-6B-multi,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.0355903953161734
Salesforce/codegen-6B-multi,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.0428785875134045
Salesforce/codegen-6B-multi,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.0428785875134045
Salesforce/codegen-6B-multi,hendrycksTest-management,5-shot,accuracy,0.174757281553398,0.0376017800602662
Salesforce/codegen-6B-multi,hendrycksTest-management,5-shot,acc_norm,0.174757281553398,0.0376017800602662
Salesforce/codegen-6B-multi,hendrycksTest-marketing,5-shot,accuracy,0.2222222222222222,0.0272360139461966
Salesforce/codegen-6B-multi,hendrycksTest-marketing,5-shot,acc_norm,0.2222222222222222,0.0272360139461966
Salesforce/codegen-6B-multi,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-6B-multi,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.0429234695990928
Salesforce/codegen-6B-multi,hendrycksTest-miscellaneous,5-shot,accuracy,0.2848020434227331,0.0161391740965225
Salesforce/codegen-6B-multi,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2848020434227331,0.0161391740965225
Salesforce/codegen-6B-multi,hendrycksTest-moral_disputes,5-shot,accuracy,0.291907514450867,0.0244769940762473
Salesforce/codegen-6B-multi,hendrycksTest-moral_disputes,5-shot,acc_norm,0.291907514450867,0.0244769940762473
Salesforce/codegen-6B-multi,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.0143559119647678
Salesforce/codegen-6B-multi,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.0143559119647678
Salesforce/codegen-6B-multi,hendrycksTest-nutrition,5-shot,accuracy,0.2483660130718954,0.0247399813551135
Salesforce/codegen-6B-multi,hendrycksTest-nutrition,5-shot,acc_norm,0.2483660130718954,0.0247399813551135
Salesforce/codegen-6B-multi,hendrycksTest-philosophy,5-shot,accuracy,0.3118971061093247,0.0263118580718541
Salesforce/codegen-6B-multi,hendrycksTest-philosophy,5-shot,acc_norm,0.3118971061093247,0.0263118580718541
Salesforce/codegen-6B-multi,hendrycksTest-prehistory,5-shot,accuracy,0.2407407407407407,0.0237885835516585
Salesforce/codegen-6B-multi,hendrycksTest-prehistory,5-shot,acc_norm,0.2407407407407407,0.0237885835516585
Salesforce/codegen-6B-multi,hendrycksTest-professional_accounting,5-shot,accuracy,0.2234042553191489,0.0248479213580639
Salesforce/codegen-6B-multi,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2234042553191489,0.0248479213580639
Salesforce/codegen-6B-multi,hendrycksTest-professional_law,5-shot,accuracy,0.2659713168187744,0.0112850331655512
Salesforce/codegen-6B-multi,hendrycksTest-professional_law,5-shot,acc_norm,0.2659713168187744,0.0112850331655512
Salesforce/codegen-6B-multi,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.0237097882538117
Salesforce/codegen-6B-multi,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.0237097882538117
Salesforce/codegen-6B-multi,hendrycksTest-professional_psychology,5-shot,accuracy,0.2532679738562091,0.0175934868953668
Salesforce/codegen-6B-multi,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2532679738562091,0.0175934868953668
Salesforce/codegen-6B-multi,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.0435027144292324
Salesforce/codegen-6B-multi,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.0435027144292324
Salesforce/codegen-6B-multi,hendrycksTest-security_studies,5-shot,accuracy,0.1795918367346938,0.0245732935895856
Salesforce/codegen-6B-multi,hendrycksTest-security_studies,5-shot,acc_norm,0.1795918367346938,0.0245732935895856
Salesforce/codegen-6B-multi,hendrycksTest-sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
Salesforce/codegen-6B-multi,hendrycksTest-sociology,5-shot,acc_norm,0.2388059701492537,0.0301477759354092
Salesforce/codegen-6B-multi,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.18,0.0386122919665369
Salesforce/codegen-6B-multi,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.18,0.0386122919665369
Salesforce/codegen-6B-multi,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.0359156679782466
Salesforce/codegen-6B-multi,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.0359156679782466
Salesforce/codegen-6B-multi,hendrycksTest-world_religions,5-shot,accuracy,0.2748538011695906,0.0342404292469158
Salesforce/codegen-6B-multi,hendrycksTest-world_religions,5-shot,acc_norm,0.2748538011695906,0.0342404292469158
Salesforce/codegen-6B-multi,truthfulqa:mc,0-shot,mc1,0.2705018359853121,0.0155507783328428
Salesforce/codegen-6B-multi,truthfulqa:mc,0-shot,mc2,0.4565419940938498,0.0151753240043781
Salesforce/codegen-6B-multi,drop,3-shot,accuracy,0.0010486577181208,0.0003314581465219
Salesforce/codegen-6B-multi,drop,3-shot,f1,0.040598783557047,0.0011641328961688
openlm-research/open_llama_7b,minerva_math_precalc,5-shot,accuracy,0.0274725274725274,0.0070016757767122
openlm-research/open_llama_7b,minerva_math_prealgebra,5-shot,accuracy,0.0562571756601607,0.0078118908661403
openlm-research/open_llama_7b,minerva_math_num_theory,5-shot,accuracy,0.0166666666666666,0.0055141728150896
openlm-research/open_llama_7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0177187153931339,0.0043926922934928
openlm-research/open_llama_7b,minerva_math_geometry,5-shot,accuracy,0.0250521920668058,0.0071482478380138
openlm-research/open_llama_7b,minerva_math_counting_and_prob,5-shot,accuracy,0.0358649789029535,0.0085501483229898
openlm-research/open_llama_7b,minerva_math_algebra,5-shot,accuracy,0.033698399326032,0.0052398474232848
openlm-research/open_llama_7b,fld_default,0-shot,accuracy,0.0,
openlm-research/open_llama_7b,fld_star,0-shot,accuracy,0.0,
openlm-research/open_llama_7b,arithmetic_3da,5-shot,accuracy,0.578,0.0110462215035167
openlm-research/open_llama_7b,arithmetic_3ds,5-shot,accuracy,0.387,0.0108937981172181
openlm-research/open_llama_7b,arithmetic_4da,5-shot,accuracy,0.328,0.0105006252940375
openlm-research/open_llama_7b,arithmetic_2ds,5-shot,accuracy,0.471,0.0111643101403737
openlm-research/open_llama_7b,arithmetic_5ds,5-shot,accuracy,0.1505,0.0079973028845175
openlm-research/open_llama_7b,arithmetic_5da,5-shot,accuracy,0.1685,0.0083719125329717
openlm-research/open_llama_7b,arithmetic_1dc,5-shot,accuracy,0.066,0.005553144938623
openlm-research/open_llama_7b,arithmetic_4ds,5-shot,accuracy,0.2865,0.0101123689115113
openlm-research/open_llama_7b,arithmetic_2dm,5-shot,accuracy,0.167,0.0083420797854954
openlm-research/open_llama_7b,arithmetic_2da,5-shot,accuracy,0.786,0.0091730077965745
openlm-research/open_llama_7b,gsm8k_cot,5-shot,accuracy,0.0606520090978013,0.0065747333814058
openlm-research/open_llama_7b,gsm8k,5-shot,accuracy,0.0159211523881728,0.003447819272389
openlm-research/open_llama_7b,anli_r2,0-shot,brier_score,0.7315652755987271,
openlm-research/open_llama_7b,anli_r3,0-shot,brier_score,0.7126834543344663,
openlm-research/open_llama_7b,anli_r1,0-shot,brier_score,0.7524228409842385,
openlm-research/open_llama_7b,xnli_eu,0-shot,brier_score,1.0796208109080396,
openlm-research/open_llama_7b,xnli_vi,0-shot,brier_score,0.9749295501186998,
openlm-research/open_llama_7b,xnli_ru,0-shot,brier_score,0.9068002030674046,
openlm-research/open_llama_7b,xnli_zh,0-shot,brier_score,0.9572324638810988,
openlm-research/open_llama_7b,xnli_tr,0-shot,brier_score,0.8002787399839949,
openlm-research/open_llama_7b,xnli_fr,0-shot,brier_score,0.7601606558680033,
openlm-research/open_llama_7b,xnli_en,0-shot,brier_score,0.6418456490546002,
openlm-research/open_llama_7b,xnli_ur,0-shot,brier_score,1.3165241257894102,
openlm-research/open_llama_7b,xnli_ar,0-shot,brier_score,1.0829988270046658,
openlm-research/open_llama_7b,xnli_de,0-shot,brier_score,0.8950139994287923,
openlm-research/open_llama_7b,xnli_hi,0-shot,brier_score,0.8444982412224025,
openlm-research/open_llama_7b,xnli_es,0-shot,brier_score,0.8538788249083591,
openlm-research/open_llama_7b,xnli_bg,0-shot,brier_score,0.9184994666520818,
openlm-research/open_llama_7b,xnli_sw,0-shot,brier_score,0.8773492580411928,
openlm-research/open_llama_7b,xnli_el,0-shot,brier_score,0.9607628544532028,
openlm-research/open_llama_7b,xnli_th,0-shot,brier_score,0.9321633089585536,
openlm-research/open_llama_7b,logiqa2,0-shot,brier_score,1.0022188312341742,
openlm-research/open_llama_7b,mathqa,0-shot,brier_score,0.939020029059052,
openlm-research/open_llama_7b,lambada_standard,0-shot,perplexity,5.162686665204201,0.1147058772292268
openlm-research/open_llama_7b,lambada_standard,0-shot,accuracy,0.6374927226858141,0.0066974279967628
openlm-research/open_llama_7b,lambada_openai,0-shot,perplexity,3.96851611110645,0.0852160804138069
openlm-research/open_llama_7b,lambada_openai,0-shot,accuracy,0.7036677663496992,0.0063618781796918
openlm-research/open_llama_7b,mmlu_world_religions,0-shot,accuracy,0.3684210526315789,0.0369965801765687
openlm-research/open_llama_7b,mmlu_formal_logic,0-shot,accuracy,0.246031746031746,0.0385227336492431
openlm-research/open_llama_7b,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.0257731111696304
openlm-research/open_llama_7b,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
openlm-research/open_llama_7b,mmlu_high_school_world_history,0-shot,accuracy,0.2953586497890295,0.0296963387134228
openlm-research/open_llama_7b,mmlu_moral_disputes,0-shot,accuracy,0.3352601156069364,0.0254160037731655
openlm-research/open_llama_7b,mmlu_professional_law,0-shot,accuracy,0.2627118644067797,0.0112405455149956
openlm-research/open_llama_7b,mmlu_logical_fallacies,0-shot,accuracy,0.3006134969325153,0.0360251131880677
openlm-research/open_llama_7b,mmlu_high_school_us_history,0-shot,accuracy,0.3235294117647059,0.0328347205610856
openlm-research/open_llama_7b,mmlu_philosophy,0-shot,accuracy,0.2829581993569132,0.0255830624899848
openlm-research/open_llama_7b,mmlu_jurisprudence,0-shot,accuracy,0.3425925925925926,0.0458790474130181
openlm-research/open_llama_7b,mmlu_international_law,0-shot,accuracy,0.396694214876033,0.04465869780531
openlm-research/open_llama_7b,mmlu_high_school_european_history,0-shot,accuracy,0.296969696969697,0.0356796977226804
openlm-research/open_llama_7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3523316062176165,0.0344747828641435
openlm-research/open_llama_7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.3025210084033613,0.0298379623882919
openlm-research/open_llama_7b,mmlu_high_school_geography,0-shot,accuracy,0.3484848484848485,0.033948539651564
openlm-research/open_llama_7b,mmlu_high_school_psychology,0-shot,accuracy,0.3577981651376147,0.0205520607848278
openlm-research/open_llama_7b,mmlu_public_relations,0-shot,accuracy,0.4,0.0469237132203465
openlm-research/open_llama_7b,mmlu_us_foreign_policy,0-shot,accuracy,0.4,0.049236596391733
openlm-research/open_llama_7b,mmlu_sociology,0-shot,accuracy,0.2487562189054726,0.0305676759389167
openlm-research/open_llama_7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3564102564102564,0.0242831405294673
openlm-research/open_llama_7b,mmlu_security_studies,0-shot,accuracy,0.2448979591836734,0.0275296374401749
openlm-research/open_llama_7b,mmlu_professional_psychology,0-shot,accuracy,0.2696078431372549,0.0179524491969878
openlm-research/open_llama_7b,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
openlm-research/open_llama_7b,mmlu_econometrics,0-shot,accuracy,0.2807017543859649,0.0422705445123219
openlm-research/open_llama_7b,mmlu_miscellaneous,0-shot,accuracy,0.367816091954023,0.0172438288918462
openlm-research/open_llama_7b,mmlu_marketing,0-shot,accuracy,0.358974358974359,0.0314261699379192
openlm-research/open_llama_7b,mmlu_management,0-shot,accuracy,0.2524271844660194,0.0430125039969087
openlm-research/open_llama_7b,mmlu_nutrition,0-shot,accuracy,0.3202614379084967,0.0267161183801568
openlm-research/open_llama_7b,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
openlm-research/open_llama_7b,mmlu_human_aging,0-shot,accuracy,0.273542600896861,0.0299185867077988
openlm-research/open_llama_7b,mmlu_professional_medicine,0-shot,accuracy,0.2463235294117647,0.02617343857052
openlm-research/open_llama_7b,mmlu_college_medicine,0-shot,accuracy,0.3236994219653179,0.0356760379963917
openlm-research/open_llama_7b,mmlu_business_ethics,0-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_7b,mmlu_clinical_knowledge,0-shot,accuracy,0.3962264150943396,0.0301027937817911
openlm-research/open_llama_7b,mmlu_global_facts,0-shot,accuracy,0.33,0.047258156262526
openlm-research/open_llama_7b,mmlu_virology,0-shot,accuracy,0.3554216867469879,0.0372621435432241
openlm-research/open_llama_7b,mmlu_professional_accounting,0-shot,accuracy,0.2659574468085106,0.0263580656988805
openlm-research/open_llama_7b,mmlu_college_physics,0-shot,accuracy,0.1862745098039215,0.0387395871414935
openlm-research/open_llama_7b,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.0357370531476345
openlm-research/open_llama_7b,mmlu_high_school_biology,0-shot,accuracy,0.3096774193548387,0.0263027749835174
openlm-research/open_llama_7b,mmlu_college_biology,0-shot,accuracy,0.3194444444444444,0.0389907368735733
openlm-research/open_llama_7b,mmlu_anatomy,0-shot,accuracy,0.3259259259259259,0.040491220417025
openlm-research/open_llama_7b,mmlu_college_chemistry,0-shot,accuracy,0.24,0.0429234695990928
openlm-research/open_llama_7b,mmlu_computer_security,0-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_7b,mmlu_college_computer_science,0-shot,accuracy,0.31,0.0464823198711731
openlm-research/open_llama_7b,mmlu_astronomy,0-shot,accuracy,0.2434210526315789,0.0349234966888423
openlm-research/open_llama_7b,mmlu_college_mathematics,0-shot,accuracy,0.29,0.0456048021572068
openlm-research/open_llama_7b,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.03047297336338
openlm-research/open_llama_7b,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.0451260859854212
openlm-research/open_llama_7b,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.0460566186471838
openlm-research/open_llama_7b,mmlu_machine_learning,0-shot,accuracy,0.2410714285714285,0.0405986724695268
openlm-research/open_llama_7b,mmlu_high_school_chemistry,0-shot,accuracy,0.2561576354679803,0.0307127300709826
openlm-research/open_llama_7b,mmlu_high_school_statistics,0-shot,accuracy,0.3888888888888889,0.0332470891180911
openlm-research/open_llama_7b,mmlu_elementary_mathematics,0-shot,accuracy,0.2724867724867725,0.0229309730716333
openlm-research/open_llama_7b,mmlu_electrical_engineering,0-shot,accuracy,0.3103448275862069,0.0385528961637894
openlm-research/open_llama_7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2481481481481481,0.0263357394040558
openlm-research/open_llama_7b,arc_challenge,25-shot,accuracy,0.4334470989761092,0.0144813762245589
openlm-research/open_llama_7b,arc_challenge,25-shot,acc_norm,0.4735494880546075,0.0145909313581201
openlm-research/open_llama_7b,hellaswag,10-shot,accuracy,0.5364469229237204,0.0049765071210762
openlm-research/open_llama_7b,hellaswag,10-shot,acc_norm,0.7197769368651663,0.0044819026375056
openlm-research/open_llama_7b,truthfulqa_mc2,0-shot,accuracy,0.3514211609144709,0.0135509600506105
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,bleu_max,27.152576174533763,0.8095916667785428
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,bleu_acc,0.3255813953488372,0.0164039894699078
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,bleu_diff,-7.3095593587963394,0.886002774295994
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge1_max,52.08716441858857,0.8595002597325482
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge1_acc,0.3072215422276622,0.016150201321323
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge1_diff,-10.058290429072064,0.9747973784961548
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge2_max,36.143564978969415,1.0316941608267736
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge2_acc,0.2545899632802937,0.0152501170791564
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge2_diff,-11.837884926202603,1.164433626396958
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rougeL_max,49.3236769159205,0.8804470956979424
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rougeL_acc,0.2974296205630355,0.0160026514873609
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rougeL_diff,-10.22935083591232,0.9840130390427738
openlm-research/open_llama_7b,truthfulqa_mc1,0-shot,accuracy,0.2313341493268053,0.0147619451748626
openlm-research/open_llama_7b,winogrande,5-shot,accuracy,0.6795580110497238,0.0131150854576817
jisukim8873/falcon-7B-case-c,arc:challenge,25-shot,accuracy,0.454778156996587,0.0145515070608363
jisukim8873/falcon-7B-case-c,arc:challenge,25-shot,acc_norm,0.4854948805460751,0.01460524108137
jisukim8873/falcon-7B-case-c,hellaswag,10-shot,accuracy,0.5984863572993427,0.0048920264572947
jisukim8873/falcon-7B-case-c,hellaswag,10-shot,acc_norm,0.7866958773152758,0.0040880347451953
jisukim8873/falcon-7B-case-c,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-c,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-c,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.0388500424580025
jisukim8873/falcon-7B-case-c,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.0388500424580025
jisukim8873/falcon-7B-case-c,hendrycksTest-astronomy,5-shot,accuracy,0.2434210526315789,0.0349234966888423
jisukim8873/falcon-7B-case-c,hendrycksTest-astronomy,5-shot,acc_norm,0.2434210526315789,0.0349234966888423
jisukim8873/falcon-7B-case-c,hendrycksTest-business_ethics,5-shot,accuracy,0.2,0.0402015126103684
jisukim8873/falcon-7B-case-c,hendrycksTest-business_ethics,5-shot,acc_norm,0.2,0.0402015126103684
jisukim8873/falcon-7B-case-c,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2981132075471698,0.0281528379424938
jisukim8873/falcon-7B-case-c,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2981132075471698,0.0281528379424938
jisukim8873/falcon-7B-case-c,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.0371617743756601
jisukim8873/falcon-7B-case-c,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.0371617743756601
jisukim8873/falcon-7B-case-c,hendrycksTest-college_chemistry,5-shot,accuracy,0.19,0.0394277244403662
jisukim8873/falcon-7B-case-c,hendrycksTest-college_chemistry,5-shot,acc_norm,0.19,0.0394277244403662
jisukim8873/falcon-7B-case-c,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-c,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-c,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338473
jisukim8873/falcon-7B-case-c,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338473
jisukim8873/falcon-7B-case-c,hendrycksTest-college_medicine,5-shot,accuracy,0.3179190751445087,0.0355068398916558
jisukim8873/falcon-7B-case-c,hendrycksTest-college_medicine,5-shot,acc_norm,0.3179190751445087,0.0355068398916558
jisukim8873/falcon-7B-case-c,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
jisukim8873/falcon-7B-case-c,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
jisukim8873/falcon-7B-case-c,hendrycksTest-computer_security,5-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-c,hendrycksTest-computer_security,5-shot,acc_norm,0.33,0.047258156262526
jisukim8873/falcon-7B-case-c,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3234042553191489,0.0305794427736103
jisukim8873/falcon-7B-case-c,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3234042553191489,0.0305794427736103
jisukim8873/falcon-7B-case-c,hendrycksTest-econometrics,5-shot,accuracy,0.2719298245614035,0.0418577442402205
jisukim8873/falcon-7B-case-c,hendrycksTest-econometrics,5-shot,acc_norm,0.2719298245614035,0.0418577442402205
jisukim8873/falcon-7B-case-c,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.035659981741353
jisukim8873/falcon-7B-case-c,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.035659981741353
jisukim8873/falcon-7B-case-c,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2698412698412698,0.022860838309232
jisukim8873/falcon-7B-case-c,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2698412698412698,0.022860838309232
jisukim8873/falcon-7B-case-c,hendrycksTest-formal_logic,5-shot,accuracy,0.1825396825396825,0.0345507101910214
jisukim8873/falcon-7B-case-c,hendrycksTest-formal_logic,5-shot,acc_norm,0.1825396825396825,0.0345507101910214
jisukim8873/falcon-7B-case-c,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-c,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_biology,5-shot,accuracy,0.3129032258064516,0.0263775670286458
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3129032258064516,0.0263775670286458
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2906403940886699,0.0319474007226554
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2906403940886699,0.0319474007226554
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3272727272727272,0.0366397499439124
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3272727272727272,0.0366397499439124
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_geography,5-shot,accuracy,0.2929292929292929,0.0324249795817881
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2929292929292929,0.0324249795817881
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.310880829015544,0.0334036190627658
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.310880829015544,0.0334036190627658
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2666666666666666,0.0224212736129237
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2666666666666666,0.0224212736129237
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2815126050420168,0.0292135494143721
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2815126050420168,0.0292135494143721
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_physics,5-shot,accuracy,0.304635761589404,0.0375794992294334
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_physics,5-shot,acc_norm,0.304635761589404,0.0375794992294334
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2678899082568807,0.0189874622579786
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2678899082568807,0.0189874622579786
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2083333333333333,0.0276969107130939
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2083333333333333,0.0276969107130939
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2745098039215686,0.0313217980308329
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2745098039215686,0.0313217980308329
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3459915611814346,0.0309648105887867
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3459915611814346,0.0309648105887867
jisukim8873/falcon-7B-case-c,hendrycksTest-human_aging,5-shot,accuracy,0.2959641255605381,0.0306365913486998
jisukim8873/falcon-7B-case-c,hendrycksTest-human_aging,5-shot,acc_norm,0.2959641255605381,0.0306365913486998
jisukim8873/falcon-7B-case-c,hendrycksTest-human_sexuality,5-shot,accuracy,0.1984732824427481,0.0349814938546247
jisukim8873/falcon-7B-case-c,hendrycksTest-human_sexuality,5-shot,acc_norm,0.1984732824427481,0.0349814938546247
jisukim8873/falcon-7B-case-c,hendrycksTest-international_law,5-shot,accuracy,0.3223140495867768,0.0426641636335216
jisukim8873/falcon-7B-case-c,hendrycksTest-international_law,5-shot,acc_norm,0.3223140495867768,0.0426641636335216
jisukim8873/falcon-7B-case-c,hendrycksTest-jurisprudence,5-shot,accuracy,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-c,hendrycksTest-jurisprudence,5-shot,acc_norm,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-c,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3190184049079754,0.0366199755107383
jisukim8873/falcon-7B-case-c,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3190184049079754,0.0366199755107383
jisukim8873/falcon-7B-case-c,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257873
jisukim8873/falcon-7B-case-c,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257873
jisukim8873/falcon-7B-case-c,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.0430125039969087
jisukim8873/falcon-7B-case-c,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.0430125039969087
jisukim8873/falcon-7B-case-c,hendrycksTest-marketing,5-shot,accuracy,0.3034188034188034,0.0301182101069426
jisukim8873/falcon-7B-case-c,hendrycksTest-marketing,5-shot,acc_norm,0.3034188034188034,0.0301182101069426
jisukim8873/falcon-7B-case-c,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.0456048021572068
jisukim8873/falcon-7B-case-c,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.0456048021572068
jisukim8873/falcon-7B-case-c,hendrycksTest-miscellaneous,5-shot,accuracy,0.3805874840357598,0.0173625641260754
jisukim8873/falcon-7B-case-c,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3805874840357598,0.0173625641260754
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_disputes,5-shot,accuracy,0.338150289017341,0.0254697701494001
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_disputes,5-shot,acc_norm,0.338150289017341,0.0254697701494001
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2569832402234637,0.0146144658219663
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2569832402234637,0.0146144658219663
jisukim8873/falcon-7B-case-c,hendrycksTest-nutrition,5-shot,accuracy,0.284313725490196,0.0258291632727574
jisukim8873/falcon-7B-case-c,hendrycksTest-nutrition,5-shot,acc_norm,0.284313725490196,0.0258291632727574
jisukim8873/falcon-7B-case-c,hendrycksTest-philosophy,5-shot,accuracy,0.3536977491961415,0.0271552081032008
jisukim8873/falcon-7B-case-c,hendrycksTest-philosophy,5-shot,acc_norm,0.3536977491961415,0.0271552081032008
jisukim8873/falcon-7B-case-c,hendrycksTest-prehistory,5-shot,accuracy,0.3086419753086419,0.0257026402606037
jisukim8873/falcon-7B-case-c,hendrycksTest-prehistory,5-shot,acc_norm,0.3086419753086419,0.0257026402606037
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_law,5-shot,accuracy,0.2737940026075619,0.0113886121679794
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_law,5-shot,acc_norm,0.2737940026075619,0.0113886121679794
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_medicine,5-shot,accuracy,0.1948529411764706,0.0240605994234874
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1948529411764706,0.0240605994234874
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_psychology,5-shot,accuracy,0.2859477124183006,0.0182804850729546
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2859477124183006,0.0182804850729546
jisukim8873/falcon-7B-case-c,hendrycksTest-public_relations,5-shot,accuracy,0.2363636363636363,0.0406930631972137
jisukim8873/falcon-7B-case-c,hendrycksTest-public_relations,5-shot,acc_norm,0.2363636363636363,0.0406930631972137
jisukim8873/falcon-7B-case-c,hendrycksTest-security_studies,5-shot,accuracy,0.2244897959183673,0.0267114305555384
jisukim8873/falcon-7B-case-c,hendrycksTest-security_studies,5-shot,acc_norm,0.2244897959183673,0.0267114305555384
jisukim8873/falcon-7B-case-c,hendrycksTest-sociology,5-shot,accuracy,0.2985074626865671,0.0323574378935504
jisukim8873/falcon-7B-case-c,hendrycksTest-sociology,5-shot,acc_norm,0.2985074626865671,0.0323574378935504
jisukim8873/falcon-7B-case-c,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.41,0.049431107042371
jisukim8873/falcon-7B-case-c,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.41,0.049431107042371
jisukim8873/falcon-7B-case-c,hendrycksTest-virology,5-shot,accuracy,0.3554216867469879,0.0372621435432241
jisukim8873/falcon-7B-case-c,hendrycksTest-virology,5-shot,acc_norm,0.3554216867469879,0.0372621435432241
jisukim8873/falcon-7B-case-c,hendrycksTest-world_religions,5-shot,accuracy,0.3742690058479532,0.0371160118538948
jisukim8873/falcon-7B-case-c,hendrycksTest-world_religions,5-shot,acc_norm,0.3742690058479532,0.0371160118538948
jisukim8873/falcon-7B-case-c,truthfulqa:mc,0-shot,mc1,0.2668298653610771,0.0154836919392372
jisukim8873/falcon-7B-case-c,truthfulqa:mc,0-shot,mc2,0.3826492372963991,0.0144019218916559
jisukim8873/falcon-7B-case-c,winogrande,5-shot,accuracy,0.7008681925808997,0.0128686390660915
jisukim8873/falcon-7B-case-c,gsm8k,5-shot,accuracy,0.0735405610310841,0.0071898357543652
jisukim8873/falcon-7B-case-c,minerva_math_precalc,5-shot,accuracy,0.0128205128205128,0.0048189509824876
jisukim8873/falcon-7B-case-c,minerva_math_prealgebra,5-shot,accuracy,0.018369690011481,0.0045526605206746
jisukim8873/falcon-7B-case-c,minerva_math_num_theory,5-shot,accuracy,0.0129629629629629,0.0048721929845814
jisukim8873/falcon-7B-case-c,minerva_math_intermediate_algebra,5-shot,accuracy,0.0177187153931339,0.0043926922934928
jisukim8873/falcon-7B-case-c,minerva_math_geometry,5-shot,accuracy,0.0125260960334029,0.0050869413896779
jisukim8873/falcon-7B-case-c,minerva_math_counting_and_prob,5-shot,accuracy,0.0126582278481012,0.0051403138895788
jisukim8873/falcon-7B-case-c,minerva_math_algebra,5-shot,accuracy,0.0143218197135636,0.003450041570937
jisukim8873/falcon-7B-case-c,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-c,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-c,arithmetic_3da,5-shot,accuracy,0.2725,0.0099584868695182
jisukim8873/falcon-7B-case-c,arithmetic_3ds,5-shot,accuracy,0.468,0.0111602094576028
jisukim8873/falcon-7B-case-c,arithmetic_4da,5-shot,accuracy,0.029,0.0037532044004605
jisukim8873/falcon-7B-case-c,arithmetic_2ds,5-shot,accuracy,0.4425,0.0111089414117476
jisukim8873/falcon-7B-case-c,arithmetic_5ds,5-shot,accuracy,0.0885,0.0063524839256793
jisukim8873/falcon-7B-case-c,arithmetic_5da,5-shot,accuracy,0.012,0.0024353573624298
jisukim8873/falcon-7B-case-c,arithmetic_1dc,5-shot,accuracy,0.0725,0.0057998874426297
jisukim8873/falcon-7B-case-c,arithmetic_4ds,5-shot,accuracy,0.122,0.0073201634132167
jisukim8873/falcon-7B-case-c,arithmetic_2dm,5-shot,accuracy,0.1895,0.0087654601502614
jisukim8873/falcon-7B-case-c,arithmetic_2da,5-shot,accuracy,0.768,0.009441004516636
jisukim8873/falcon-7B-case-c,gsm8k_cot,5-shot,accuracy,0.1076573161485974,0.0085374840030233
jisukim8873/falcon-7B-case-c,anli_r2,0-shot,brier_score,0.8780178026204086,
jisukim8873/falcon-7B-case-c,anli_r3,0-shot,brier_score,0.7901916928644884,
jisukim8873/falcon-7B-case-c,anli_r1,0-shot,brier_score,0.9055919262241392,
jisukim8873/falcon-7B-case-c,xnli_eu,0-shot,brier_score,1.001290310490029,
jisukim8873/falcon-7B-case-c,xnli_vi,0-shot,brier_score,0.9798397634266888,
jisukim8873/falcon-7B-case-c,xnli_ru,0-shot,brier_score,0.859574174744215,
jisukim8873/falcon-7B-case-c,xnli_zh,0-shot,brier_score,0.894488082940237,
jisukim8873/falcon-7B-case-c,xnli_tr,0-shot,brier_score,0.9948912466030564,
jisukim8873/falcon-7B-case-c,xnli_fr,0-shot,brier_score,0.7337998938941023,
jisukim8873/falcon-7B-case-c,xnli_en,0-shot,brier_score,0.6616939089924683,
jisukim8873/falcon-7B-case-c,xnli_ur,0-shot,brier_score,1.3066034233367336,
jisukim8873/falcon-7B-case-c,xnli_ar,0-shot,brier_score,1.2889195153240864,
jisukim8873/falcon-7B-case-c,xnli_de,0-shot,brier_score,0.8343456649143227,
jisukim8873/falcon-7B-case-c,xnli_hi,0-shot,brier_score,1.077691017146163,
jisukim8873/falcon-7B-case-c,xnli_es,0-shot,brier_score,0.811152479987488,
jisukim8873/falcon-7B-case-c,xnli_bg,0-shot,brier_score,0.9276588116275865,
jisukim8873/falcon-7B-case-c,xnli_sw,0-shot,brier_score,1.0923473059839404,
jisukim8873/falcon-7B-case-c,xnli_el,0-shot,brier_score,1.0102156151166493,
jisukim8873/falcon-7B-case-c,xnli_th,0-shot,brier_score,0.9468478602772348,
jisukim8873/falcon-7B-case-c,logiqa2,0-shot,brier_score,1.0599129907264797,
jisukim8873/falcon-7B-case-c,mathqa,0-shot,brier_score,0.953944844085626,
jisukim8873/falcon-7B-case-c,lambada_standard,0-shot,perplexity,4.013535326746964,0.0861706441628093
jisukim8873/falcon-7B-case-c,lambada_standard,0-shot,accuracy,0.6790219289734135,0.0065041665597646
jisukim8873/falcon-7B-case-c,lambada_openai,0-shot,perplexity,3.2959199182547474,0.0689224816777712
jisukim8873/falcon-7B-case-c,lambada_openai,0-shot,accuracy,0.7376285658839511,0.0061289942084308
tiiuae/falcon-7b,gsm8k,5-shot,accuracy,0.0523123578468536,0.0061330577089592
tiiuae/falcon-7b,drop,3-shot,accuracy,0.0010486577181208,0.0003314581465219
tiiuae/falcon-7b,drop,3-shot,f1,0.048246644295302,0.0012232481165562
tiiuae/falcon-7b,winogrande,5-shot,accuracy,0.7237569060773481,0.0125668150156981
tiiuae/falcon-7b,arc:challenge,25-shot,accuracy,0.4360068259385665,0.0144912256992309
tiiuae/falcon-7b,arc:challenge,25-shot,acc_norm,0.4786689419795222,0.0145980879731271
tiiuae/falcon-7b,hellaswag,10-shot,accuracy,0.5797649870543716,0.0049258777057711
tiiuae/falcon-7b,hellaswag,10-shot,acc_norm,0.7813184624576778,0.0041250728166303
tiiuae/falcon-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.0435194139889244
tiiuae/falcon-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.0435194139889244
tiiuae/falcon-7b,hendrycksTest-anatomy,5-shot,accuracy,0.2,0.0345547370232543
tiiuae/falcon-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.2,0.0345547370232543
tiiuae/falcon-7b,hendrycksTest-astronomy,5-shot,accuracy,0.2434210526315789,0.0349234966888423
tiiuae/falcon-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.2434210526315789,0.0349234966888423
tiiuae/falcon-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.0409360180740332
tiiuae/falcon-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.0409360180740332
tiiuae/falcon-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2981132075471698,0.0281528379424938
tiiuae/falcon-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2981132075471698,0.0281528379424938
tiiuae/falcon-7b,hendrycksTest-college_biology,5-shot,accuracy,0.2361111111111111,0.0355144661081082
tiiuae/falcon-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.2361111111111111,0.0355144661081082
tiiuae/falcon-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.21,0.0409360180740332
tiiuae/falcon-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.21,0.0409360180740332
tiiuae/falcon-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.0446196043338473
tiiuae/falcon-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.0446196043338473
tiiuae/falcon-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338473
tiiuae/falcon-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338473
tiiuae/falcon-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.2485549132947976,0.0329530469681831
tiiuae/falcon-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2485549132947976,0.0329530469681831
tiiuae/falcon-7b,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
tiiuae/falcon-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
tiiuae/falcon-7b,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
tiiuae/falcon-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
tiiuae/falcon-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2851063829787234,0.0295131966255393
tiiuae/falcon-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2851063829787234,0.0295131966255393
tiiuae/falcon-7b,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
tiiuae/falcon-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
tiiuae/falcon-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2896551724137931,0.0378001923043801
tiiuae/falcon-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2896551724137931,0.0378001923043801
tiiuae/falcon-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2486772486772486,0.0222618176924001
tiiuae/falcon-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2486772486772486,0.0222618176924001
tiiuae/falcon-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.2142857142857142,0.0367006645104718
tiiuae/falcon-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2142857142857142,0.0367006645104718
tiiuae/falcon-7b,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
tiiuae/falcon-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
tiiuae/falcon-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2516129032258064,0.0246859792862399
tiiuae/falcon-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2516129032258064,0.0246859792862399
tiiuae/falcon-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
tiiuae/falcon-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.0305165307326944
tiiuae/falcon-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.0464823198711731
tiiuae/falcon-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.0464823198711731
tiiuae/falcon-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2424242424242424,0.0334640988105595
tiiuae/falcon-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2424242424242424,0.0334640988105595
tiiuae/falcon-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.1969696969696969,0.0283356097324633
tiiuae/falcon-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1969696969696969,0.0283356097324633
tiiuae/falcon-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2590673575129533,0.0316187791793541
tiiuae/falcon-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2590673575129533,0.0316187791793541
tiiuae/falcon-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2435897435897435,0.0217637336841739
tiiuae/falcon-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2435897435897435,0.0217637336841739
tiiuae/falcon-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
tiiuae/falcon-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
tiiuae/falcon-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2731092436974789,0.0289420040409981
tiiuae/falcon-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2731092436974789,0.0289420040409981
tiiuae/falcon-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.3112582781456953,0.0378044585052673
tiiuae/falcon-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3112582781456953,0.0378044585052673
tiiuae/falcon-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2348623853211009,0.0181751105103435
tiiuae/falcon-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2348623853211009,0.0181751105103435
tiiuae/falcon-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1527777777777778,0.0245363260261342
tiiuae/falcon-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1527777777777778,0.0245363260261342
tiiuae/falcon-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3186274509803921,0.0327028718148207
tiiuae/falcon-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3186274509803921,0.0327028718148207
tiiuae/falcon-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2658227848101265,0.0287567996296583
tiiuae/falcon-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2658227848101265,0.0287567996296583
tiiuae/falcon-7b,hendrycksTest-human_aging,5-shot,accuracy,0.4573991031390134,0.0334357770558306
tiiuae/falcon-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.4573991031390134,0.0334357770558306
tiiuae/falcon-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2900763358778626,0.0398006624646776
tiiuae/falcon-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2900763358778626,0.0398006624646776
tiiuae/falcon-7b,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.0389687898507041
tiiuae/falcon-7b,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.0389687898507041
tiiuae/falcon-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.3148148148148148,0.0448993107359131
tiiuae/falcon-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3148148148148148,0.0448993107359131
tiiuae/falcon-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2576687116564417,0.0343615082784691
tiiuae/falcon-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2576687116564417,0.0343615082784691
tiiuae/falcon-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.3839285714285714,0.0461614307502854
tiiuae/falcon-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.3839285714285714,0.0461614307502854
tiiuae/falcon-7b,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.0430125039969087
tiiuae/falcon-7b,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.0430125039969087
tiiuae/falcon-7b,hendrycksTest-marketing,5-shot,accuracy,0.3119658119658119,0.0303515273233449
tiiuae/falcon-7b,hendrycksTest-marketing,5-shot,acc_norm,0.3119658119658119,0.0303515273233449
tiiuae/falcon-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.0456048021572068
tiiuae/falcon-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.0456048021572068
tiiuae/falcon-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.3065134099616858,0.0164869528930415
tiiuae/falcon-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3065134099616858,0.0164869528930415
tiiuae/falcon-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2890173410404624,0.0244051739357832
tiiuae/falcon-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2890173410404624,0.0244051739357832
tiiuae/falcon-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2391061452513966,0.0142655541923311
tiiuae/falcon-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2391061452513966,0.0142655541923311
tiiuae/falcon-7b,hendrycksTest-nutrition,5-shot,accuracy,0.2810457516339869,0.0257388547978187
tiiuae/falcon-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.2810457516339869,0.0257388547978187
tiiuae/falcon-7b,hendrycksTest-philosophy,5-shot,accuracy,0.2958199356913183,0.0259223717888187
tiiuae/falcon-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.2958199356913183,0.0259223717888187
tiiuae/falcon-7b,hendrycksTest-prehistory,5-shot,accuracy,0.3209876543209876,0.0259765660108627
tiiuae/falcon-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.3209876543209876,0.0259765660108627
tiiuae/falcon-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.301418439716312,0.0273741288826311
tiiuae/falcon-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.301418439716312,0.0273741288826311
tiiuae/falcon-7b,hendrycksTest-professional_law,5-shot,accuracy,0.2496740547588005,0.0110545383778323
tiiuae/falcon-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2496740547588005,0.0110545383778323
tiiuae/falcon-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.2794117647058823,0.0272572026061149
tiiuae/falcon-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2794117647058823,0.0272572026061149
tiiuae/falcon-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2679738562091503,0.0179179740695947
tiiuae/falcon-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2679738562091503,0.0179179740695947
tiiuae/falcon-7b,hendrycksTest-public_relations,5-shot,accuracy,0.3545454545454545,0.0458200484150541
tiiuae/falcon-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.3545454545454545,0.0458200484150541
tiiuae/falcon-7b,hendrycksTest-security_studies,5-shot,accuracy,0.2693877551020408,0.0284012520290229
tiiuae/falcon-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.2693877551020408,0.0284012520290229
tiiuae/falcon-7b,hendrycksTest-sociology,5-shot,accuracy,0.3532338308457711,0.0337979061179677
tiiuae/falcon-7b,hendrycksTest-sociology,5-shot,acc_norm,0.3532338308457711,0.0337979061179677
tiiuae/falcon-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.39,0.0490207130000197
tiiuae/falcon-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.39,0.0490207130000197
tiiuae/falcon-7b,hendrycksTest-virology,5-shot,accuracy,0.3795180722891566,0.0377779882274801
tiiuae/falcon-7b,hendrycksTest-virology,5-shot,acc_norm,0.3795180722891566,0.0377779882274801
tiiuae/falcon-7b,hendrycksTest-world_religions,5-shot,accuracy,0.3450292397660818,0.036459813773888
tiiuae/falcon-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.3450292397660818,0.036459813773888
tiiuae/falcon-7b,truthfulqa:mc,0-shot,mc1,0.2239902080783353,0.0145949643294742
tiiuae/falcon-7b,truthfulqa:mc,0-shot,mc2,0.34263825539848,0.0132755582996423
tiiuae/falcon-7b,minerva_math_precalc,5-shot,accuracy,0.0256410256410256,0.0067706278007804
tiiuae/falcon-7b,minerva_math_prealgebra,5-shot,accuracy,0.0482204362801377,0.0072631352121036
tiiuae/falcon-7b,minerva_math_num_theory,5-shot,accuracy,0.0148148148148148,0.0052037049875126
tiiuae/falcon-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.0049000930886157
tiiuae/falcon-7b,minerva_math_geometry,5-shot,accuracy,0.0187891440501043,0.0062104164279973
tiiuae/falcon-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.0232067510548523,0.0069227384871433
tiiuae/falcon-7b,minerva_math_algebra,5-shot,accuracy,0.0193765796124684,0.0040026474981053
tiiuae/falcon-7b,fld_default,0-shot,accuracy,0.0,
tiiuae/falcon-7b,fld_star,0-shot,accuracy,0.0,
tiiuae/falcon-7b,arithmetic_3da,5-shot,accuracy,0.148,0.007942262887231
tiiuae/falcon-7b,arithmetic_3ds,5-shot,accuracy,0.415,0.0110203549902922
tiiuae/falcon-7b,arithmetic_4da,5-shot,accuracy,0.0035,0.0013208888574315
tiiuae/falcon-7b,arithmetic_2ds,5-shot,accuracy,0.5495,0.0111281981199428
tiiuae/falcon-7b,arithmetic_5ds,5-shot,accuracy,0.082,0.0061365159833742
tiiuae/falcon-7b,arithmetic_5da,5-shot,accuracy,0.0,
tiiuae/falcon-7b,arithmetic_1dc,5-shot,accuracy,0.08,0.0060678174992828
tiiuae/falcon-7b,arithmetic_4ds,5-shot,accuracy,0.11,0.0069981774229882
tiiuae/falcon-7b,arithmetic_2dm,5-shot,accuracy,0.193,0.0088269166320189
tiiuae/falcon-7b,arithmetic_2da,5-shot,accuracy,0.799,0.0089632399184064
tiiuae/falcon-7b,gsm8k_cot,5-shot,accuracy,0.0720242608036391,0.0071211479835371
tiiuae/falcon-7b,anli_r2,0-shot,brier_score,0.7192933891095322,
tiiuae/falcon-7b,anli_r3,0-shot,brier_score,0.7153934424883512,
tiiuae/falcon-7b,anli_r1,0-shot,brier_score,0.7383201344636011,
tiiuae/falcon-7b,xnli_eu,0-shot,brier_score,0.9861818673503582,
tiiuae/falcon-7b,xnli_vi,0-shot,brier_score,0.9775102639166516,
tiiuae/falcon-7b,xnli_ru,0-shot,brier_score,0.8089869795920962,
tiiuae/falcon-7b,xnli_zh,0-shot,brier_score,1.0012841309225609,
tiiuae/falcon-7b,xnli_tr,0-shot,brier_score,0.9627727685034017,
tiiuae/falcon-7b,xnli_fr,0-shot,brier_score,0.7468798318836768,
tiiuae/falcon-7b,xnli_en,0-shot,brier_score,0.6597330345759265,
tiiuae/falcon-7b,xnli_ur,0-shot,brier_score,1.301829941445937,
tiiuae/falcon-7b,xnli_ar,0-shot,brier_score,1.2469610015956896,
tiiuae/falcon-7b,xnli_de,0-shot,brier_score,0.8410614069823841,
tiiuae/falcon-7b,xnli_hi,0-shot,brier_score,1.132654288498891,
tiiuae/falcon-7b,xnli_es,0-shot,brier_score,0.8190608375779431,
tiiuae/falcon-7b,xnli_bg,0-shot,brier_score,0.8950252179652899,
tiiuae/falcon-7b,xnli_sw,0-shot,brier_score,1.112644389463286,
tiiuae/falcon-7b,xnli_el,0-shot,brier_score,0.8724657539692695,
tiiuae/falcon-7b,xnli_th,0-shot,brier_score,0.9463512763116716,
tiiuae/falcon-7b,logiqa2,0-shot,brier_score,1.039383873940183,
tiiuae/falcon-7b,mathqa,0-shot,brier_score,0.9190095944774516,
tiiuae/falcon-7b,lambada_standard,0-shot,perplexity,3.9540875826593176,0.0773500433563723
tiiuae/falcon-7b,lambada_standard,0-shot,accuracy,0.6881428294197555,0.0064540040616187
tiiuae/falcon-7b,lambada_openai,0-shot,perplexity,3.367914054594161,0.0644944390146436
tiiuae/falcon-7b,lambada_openai,0-shot,accuracy,0.7457791577721715,0.0060662844467191
facebook/xglm-4.5B,drop,3-shot,accuracy,0.0648070469798657,0.002521165644662
facebook/xglm-4.5B,drop,3-shot,f1,0.114801803691275,0.0027659324477286
facebook/xglm-4.5B,arc:challenge,25-shot,accuracy,0.2977815699658703,0.0133630801072444
facebook/xglm-4.5B,arc:challenge,25-shot,acc_norm,0.3148464163822526,0.0135726577030849
facebook/xglm-4.5B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.2,0.0402015126103684
facebook/xglm-4.5B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.2,0.0402015126103684
facebook/xglm-4.5B,hendrycksTest-anatomy,5-shot,accuracy,0.3037037037037037,0.0397255288478513
facebook/xglm-4.5B,hendrycksTest-anatomy,5-shot,acc_norm,0.3037037037037037,0.0397255288478513
facebook/xglm-4.5B,hendrycksTest-astronomy,5-shot,accuracy,0.2105263157894736,0.0331767278753315
facebook/xglm-4.5B,hendrycksTest-astronomy,5-shot,acc_norm,0.2105263157894736,0.0331767278753315
facebook/xglm-4.5B,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.042295258468165
facebook/xglm-4.5B,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.042295258468165
facebook/xglm-4.5B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2226415094339622,0.0256042334708991
facebook/xglm-4.5B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2226415094339622,0.0256042334708991
facebook/xglm-4.5B,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
facebook/xglm-4.5B,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.0368565109589753
facebook/xglm-4.5B,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
facebook/xglm-4.5B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
facebook/xglm-4.5B,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-4.5B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
facebook/xglm-4.5B,hendrycksTest-college_mathematics,5-shot,accuracy,0.29,0.0456048021572068
facebook/xglm-4.5B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.29,0.0456048021572068
facebook/xglm-4.5B,hendrycksTest-college_medicine,5-shot,accuracy,0.2716763005780346,0.0339175032232165
facebook/xglm-4.5B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2716763005780346,0.0339175032232165
facebook/xglm-4.5B,hendrycksTest-college_physics,5-shot,accuracy,0.196078431372549,0.0395058186117996
facebook/xglm-4.5B,hendrycksTest-college_physics,5-shot,acc_norm,0.196078431372549,0.0395058186117996
facebook/xglm-4.5B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-4.5B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
facebook/xglm-4.5B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2425531914893617,0.0280202262712002
facebook/xglm-4.5B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2425531914893617,0.0280202262712002
facebook/xglm-4.5B,hendrycksTest-econometrics,5-shot,accuracy,0.2807017543859649,0.0422705445123219
facebook/xglm-4.5B,hendrycksTest-econometrics,5-shot,acc_norm,0.2807017543859649,0.0422705445123219
facebook/xglm-4.5B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2,0.0333333333333333
facebook/xglm-4.5B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2,0.0333333333333333
facebook/xglm-4.5B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2513227513227513,0.0223404823396438
facebook/xglm-4.5B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2513227513227513,0.0223404823396438
facebook/xglm-4.5B,hendrycksTest-formal_logic,5-shot,accuracy,0.1746031746031746,0.033954900208561
facebook/xglm-4.5B,hendrycksTest-formal_logic,5-shot,acc_norm,0.1746031746031746,0.033954900208561
facebook/xglm-4.5B,hendrycksTest-global_facts,5-shot,accuracy,0.34,0.0476095228569523
facebook/xglm-4.5B,hendrycksTest-global_facts,5-shot,acc_norm,0.34,0.0476095228569523
facebook/xglm-4.5B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2580645161290322,0.0248924691724628
facebook/xglm-4.5B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2580645161290322,0.0248924691724628
facebook/xglm-4.5B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2610837438423645,0.0309037969521145
facebook/xglm-4.5B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2610837438423645,0.0309037969521145
facebook/xglm-4.5B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.0464823198711731
facebook/xglm-4.5B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.0464823198711731
facebook/xglm-4.5B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.206060606060606,0.0315841532404771
facebook/xglm-4.5B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.206060606060606,0.0315841532404771
facebook/xglm-4.5B,hendrycksTest-high_school_geography,5-shot,accuracy,0.2727272727272727,0.0317307123907172
facebook/xglm-4.5B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2727272727272727,0.0317307123907172
facebook/xglm-4.5B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2642487046632124,0.0318215505091664
facebook/xglm-4.5B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2642487046632124,0.0318215505091664
facebook/xglm-4.5B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2564102564102564,0.0221390811039715
facebook/xglm-4.5B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2564102564102564,0.0221390811039715
facebook/xglm-4.5B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.0264661175389599
facebook/xglm-4.5B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.0264661175389599
facebook/xglm-4.5B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2226890756302521,0.0270254334988823
facebook/xglm-4.5B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2226890756302521,0.0270254334988823
facebook/xglm-4.5B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
facebook/xglm-4.5B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
facebook/xglm-4.5B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2477064220183486,0.0185081436025478
facebook/xglm-4.5B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2477064220183486,0.0185081436025478
facebook/xglm-4.5B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.287037037037037,0.030851992993257
facebook/xglm-4.5B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.287037037037037,0.030851992993257
facebook/xglm-4.5B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2058823529411764,0.0283794494515886
facebook/xglm-4.5B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2058823529411764,0.0283794494515886
facebook/xglm-4.5B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2151898734177215,0.0267508269946761
facebook/xglm-4.5B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2151898734177215,0.0267508269946761
facebook/xglm-4.5B,hendrycksTest-human_aging,5-shot,accuracy,0.2242152466367713,0.0279915342585195
facebook/xglm-4.5B,hendrycksTest-human_aging,5-shot,acc_norm,0.2242152466367713,0.0279915342585195
facebook/xglm-4.5B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2671755725190839,0.0388084830108239
facebook/xglm-4.5B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2671755725190839,0.0388084830108239
facebook/xglm-4.5B,hendrycksTest-international_law,5-shot,accuracy,0.3388429752066115,0.0432076780753667
facebook/xglm-4.5B,hendrycksTest-international_law,5-shot,acc_norm,0.3388429752066115,0.0432076780753667
facebook/xglm-4.5B,hendrycksTest-jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
facebook/xglm-4.5B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2314814814814814,0.0407749470925262
facebook/xglm-4.5B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
facebook/xglm-4.5B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.0355903953161734
facebook/xglm-4.5B,hendrycksTest-machine_learning,5-shot,accuracy,0.2410714285714285,0.0405986724695268
facebook/xglm-4.5B,hendrycksTest-machine_learning,5-shot,acc_norm,0.2410714285714285,0.0405986724695268
facebook/xglm-4.5B,hendrycksTest-management,5-shot,accuracy,0.2912621359223301,0.0449867632057292
facebook/xglm-4.5B,hendrycksTest-management,5-shot,acc_norm,0.2912621359223301,0.0449867632057292
facebook/xglm-4.5B,hendrycksTest-marketing,5-shot,accuracy,0.2222222222222222,0.0272360139461966
facebook/xglm-4.5B,hendrycksTest-marketing,5-shot,acc_norm,0.2222222222222222,0.0272360139461966
facebook/xglm-4.5B,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.0429234695990928
facebook/xglm-4.5B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.0429234695990928
facebook/xglm-4.5B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2043422733077905,0.0144191239809318
facebook/xglm-4.5B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2043422733077905,0.0144191239809318
facebook/xglm-4.5B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2716763005780346,0.0239485129054683
facebook/xglm-4.5B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2716763005780346,0.0239485129054683
facebook/xglm-4.5B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.241340782122905,0.0143109995479614
facebook/xglm-4.5B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.241340782122905,0.0143109995479614
facebook/xglm-4.5B,hendrycksTest-nutrition,5-shot,accuracy,0.261437908496732,0.0251609982142924
facebook/xglm-4.5B,hendrycksTest-nutrition,5-shot,acc_norm,0.261437908496732,0.0251609982142924
facebook/xglm-4.5B,hendrycksTest-philosophy,5-shot,accuracy,0.3022508038585209,0.0260827006953996
facebook/xglm-4.5B,hendrycksTest-philosophy,5-shot,acc_norm,0.3022508038585209,0.0260827006953996
facebook/xglm-4.5B,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.0245692236004608
facebook/xglm-4.5B,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.0245692236004608
facebook/xglm-4.5B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2765957446808511,0.026684564340461
facebook/xglm-4.5B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2765957446808511,0.026684564340461
facebook/xglm-4.5B,hendrycksTest-professional_law,5-shot,accuracy,0.273142112125163,0.0113801505678304
facebook/xglm-4.5B,hendrycksTest-professional_law,5-shot,acc_norm,0.273142112125163,0.0113801505678304
facebook/xglm-4.5B,hendrycksTest-professional_medicine,5-shot,accuracy,0.3272058823529412,0.0285014528603965
facebook/xglm-4.5B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3272058823529412,0.0285014528603965
facebook/xglm-4.5B,hendrycksTest-professional_psychology,5-shot,accuracy,0.25,0.0175178188450144
facebook/xglm-4.5B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25,0.0175178188450144
facebook/xglm-4.5B,hendrycksTest-public_relations,5-shot,accuracy,0.2363636363636363,0.0406930631972137
facebook/xglm-4.5B,hendrycksTest-public_relations,5-shot,acc_norm,0.2363636363636363,0.0406930631972137
facebook/xglm-4.5B,hendrycksTest-security_studies,5-shot,accuracy,0.236734693877551,0.0272128358840731
facebook/xglm-4.5B,hendrycksTest-security_studies,5-shot,acc_norm,0.236734693877551,0.0272128358840731
facebook/xglm-4.5B,hendrycksTest-sociology,5-shot,accuracy,0.3134328358208955,0.0328018820534864
facebook/xglm-4.5B,hendrycksTest-sociology,5-shot,acc_norm,0.3134328358208955,0.0328018820534864
facebook/xglm-4.5B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-4.5B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.0451260859854212
facebook/xglm-4.5B,hendrycksTest-virology,5-shot,accuracy,0.1867469879518072,0.0303387491445005
facebook/xglm-4.5B,hendrycksTest-virology,5-shot,acc_norm,0.1867469879518072,0.0303387491445005
facebook/xglm-4.5B,hendrycksTest-world_religions,5-shot,accuracy,0.2222222222222222,0.0318857801768639
facebook/xglm-4.5B,hendrycksTest-world_religions,5-shot,acc_norm,0.2222222222222222,0.0318857801768639
facebook/xglm-4.5B,truthfulqa:mc,0-shot,mc1,0.208078335373317,0.0142105034735766
facebook/xglm-4.5B,truthfulqa:mc,0-shot,mc2,0.3583939485040638,0.0137859917623068
jisukim8873/falcon-7B-case-8,arc:challenge,25-shot,accuracy,0.4598976109215017,0.0145643188569248
jisukim8873/falcon-7B-case-8,arc:challenge,25-shot,acc_norm,0.4948805460750853,0.0146106248903091
jisukim8873/falcon-7B-case-8,hellaswag,10-shot,accuracy,0.5965943039235212,0.0048957821077864
jisukim8873/falcon-7B-case-8,hellaswag,10-shot,acc_norm,0.7855008962358097,0.0040963551251175
jisukim8873/falcon-7B-case-8,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-8,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.047258156262526
jisukim8873/falcon-7B-case-8,hendrycksTest-anatomy,5-shot,accuracy,0.3111111111111111,0.0399926287661772
jisukim8873/falcon-7B-case-8,hendrycksTest-anatomy,5-shot,acc_norm,0.3111111111111111,0.0399926287661772
jisukim8873/falcon-7B-case-8,hendrycksTest-astronomy,5-shot,accuracy,0.2894736842105263,0.0369067798613728
jisukim8873/falcon-7B-case-8,hendrycksTest-astronomy,5-shot,acc_norm,0.2894736842105263,0.0369067798613728
jisukim8873/falcon-7B-case-8,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.0409360180740332
jisukim8873/falcon-7B-case-8,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.0409360180740332
jisukim8873/falcon-7B-case-8,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3433962264150943,0.0292245264691247
jisukim8873/falcon-7B-case-8,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3433962264150943,0.0292245264691247
jisukim8873/falcon-7B-case-8,hendrycksTest-college_biology,5-shot,accuracy,0.2847222222222222,0.0377380999068693
jisukim8873/falcon-7B-case-8,hendrycksTest-college_biology,5-shot,acc_norm,0.2847222222222222,0.0377380999068693
jisukim8873/falcon-7B-case-8,hendrycksTest-college_chemistry,5-shot,accuracy,0.11,0.031446603773522
jisukim8873/falcon-7B-case-8,hendrycksTest-college_chemistry,5-shot,acc_norm,0.11,0.031446603773522
jisukim8873/falcon-7B-case-8,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-8,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-8,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-8,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-8,hendrycksTest-college_medicine,5-shot,accuracy,0.2832369942196532,0.0343556805604787
jisukim8873/falcon-7B-case-8,hendrycksTest-college_medicine,5-shot,acc_norm,0.2832369942196532,0.0343556805604787
jisukim8873/falcon-7B-case-8,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
jisukim8873/falcon-7B-case-8,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
jisukim8873/falcon-7B-case-8,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.0487831731214563
jisukim8873/falcon-7B-case-8,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.0487831731214563
jisukim8873/falcon-7B-case-8,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3276595744680851,0.030683020843231
jisukim8873/falcon-7B-case-8,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3276595744680851,0.030683020843231
jisukim8873/falcon-7B-case-8,hendrycksTest-econometrics,5-shot,accuracy,0.2105263157894736,0.0383515395439942
jisukim8873/falcon-7B-case-8,hendrycksTest-econometrics,5-shot,acc_norm,0.2105263157894736,0.0383515395439942
jisukim8873/falcon-7B-case-8,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.0366466633722525
jisukim8873/falcon-7B-case-8,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.0366466633722525
jisukim8873/falcon-7B-case-8,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
jisukim8873/falcon-7B-case-8,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
jisukim8873/falcon-7B-case-8,hendrycksTest-formal_logic,5-shot,accuracy,0.1825396825396825,0.0345507101910214
jisukim8873/falcon-7B-case-8,hendrycksTest-formal_logic,5-shot,acc_norm,0.1825396825396825,0.0345507101910214
jisukim8873/falcon-7B-case-8,hendrycksTest-global_facts,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-8,hendrycksTest-global_facts,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_biology,5-shot,accuracy,0.3161290322580645,0.0264508744890427
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3161290322580645,0.0264508744890427
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3300492610837438,0.0330853042622825
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3300492610837438,0.0330853042622825
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3818181818181818,0.0379371317116563
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3818181818181818,0.0379371317116563
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_geography,5-shot,accuracy,0.3383838383838384,0.033711241426263
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3383838383838384,0.033711241426263
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2797927461139896,0.032396370467357
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2797927461139896,0.032396370467357
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2641025641025641,0.0223521937374532
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2641025641025641,0.0223521937374532
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2592592592592592,0.0267192407837121
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2899159663865546,0.029472485833136
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2899159663865546,0.029472485833136
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_physics,5-shot,accuracy,0.3112582781456953,0.0378044585052673
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3112582781456953,0.0378044585052673
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2990825688073394,0.0196304172854151
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2990825688073394,0.0196304172854151
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2268518518518518,0.0285616501024222
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2268518518518518,0.0285616501024222
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_us_history,5-shot,accuracy,0.284313725490196,0.0316600967939981
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.284313725490196,0.0316600967939981
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2953586497890295,0.0296963387134228
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2953586497890295,0.0296963387134228
jisukim8873/falcon-7B-case-8,hendrycksTest-human_aging,5-shot,accuracy,0.3901345291479821,0.0327376672545915
jisukim8873/falcon-7B-case-8,hendrycksTest-human_aging,5-shot,acc_norm,0.3901345291479821,0.0327376672545915
jisukim8873/falcon-7B-case-8,hendrycksTest-human_sexuality,5-shot,accuracy,0.3282442748091603,0.0411843856580629
jisukim8873/falcon-7B-case-8,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3282442748091603,0.0411843856580629
jisukim8873/falcon-7B-case-8,hendrycksTest-international_law,5-shot,accuracy,0.371900826446281,0.044120158066245
jisukim8873/falcon-7B-case-8,hendrycksTest-international_law,5-shot,acc_norm,0.371900826446281,0.044120158066245
jisukim8873/falcon-7B-case-8,hendrycksTest-jurisprudence,5-shot,accuracy,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-8,hendrycksTest-jurisprudence,5-shot,acc_norm,0.324074074074074,0.0452459600703004
jisukim8873/falcon-7B-case-8,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
jisukim8873/falcon-7B-case-8,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.0355903953161734
jisukim8873/falcon-7B-case-8,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.0428785875134045
jisukim8873/falcon-7B-case-8,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.0428785875134045
jisukim8873/falcon-7B-case-8,hendrycksTest-management,5-shot,accuracy,0.2427184466019417,0.0424502248638449
jisukim8873/falcon-7B-case-8,hendrycksTest-management,5-shot,acc_norm,0.2427184466019417,0.0424502248638449
jisukim8873/falcon-7B-case-8,hendrycksTest-marketing,5-shot,accuracy,0.3504273504273504,0.0312561082442187
jisukim8873/falcon-7B-case-8,hendrycksTest-marketing,5-shot,acc_norm,0.3504273504273504,0.0312561082442187
jisukim8873/falcon-7B-case-8,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.0464823198711731
jisukim8873/falcon-7B-case-8,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.0464823198711731
jisukim8873/falcon-7B-case-8,hendrycksTest-miscellaneous,5-shot,accuracy,0.3729246487867177,0.0172928682694539
jisukim8873/falcon-7B-case-8,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3729246487867177,0.0172928682694539
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_disputes,5-shot,accuracy,0.3670520231213873,0.025950054337654
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3670520231213873,0.025950054337654
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2402234636871508,0.0142883438039253
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2402234636871508,0.0142883438039253
jisukim8873/falcon-7B-case-8,hendrycksTest-nutrition,5-shot,accuracy,0.3039215686274509,0.0263366134690466
jisukim8873/falcon-7B-case-8,hendrycksTest-nutrition,5-shot,acc_norm,0.3039215686274509,0.0263366134690466
jisukim8873/falcon-7B-case-8,hendrycksTest-philosophy,5-shot,accuracy,0.3504823151125402,0.0270986526213017
jisukim8873/falcon-7B-case-8,hendrycksTest-philosophy,5-shot,acc_norm,0.3504823151125402,0.0270986526213017
jisukim8873/falcon-7B-case-8,hendrycksTest-prehistory,5-shot,accuracy,0.3148148148148148,0.0258422487009021
jisukim8873/falcon-7B-case-8,hendrycksTest-prehistory,5-shot,acc_norm,0.3148148148148148,0.0258422487009021
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.0267891723511402
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_law,5-shot,accuracy,0.2737940026075619,0.0113886121679793
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_law,5-shot,acc_norm,0.2737940026075619,0.0113886121679793
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_medicine,5-shot,accuracy,0.2683823529411764,0.0269174812243772
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2683823529411764,0.0269174812243772
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_psychology,5-shot,accuracy,0.2990196078431372,0.018521756215423
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2990196078431372,0.018521756215423
jisukim8873/falcon-7B-case-8,hendrycksTest-public_relations,5-shot,accuracy,0.3363636363636363,0.045253935963025
jisukim8873/falcon-7B-case-8,hendrycksTest-public_relations,5-shot,acc_norm,0.3363636363636363,0.045253935963025
jisukim8873/falcon-7B-case-8,hendrycksTest-security_studies,5-shot,accuracy,0.3183673469387755,0.029822533793982
jisukim8873/falcon-7B-case-8,hendrycksTest-security_studies,5-shot,acc_norm,0.3183673469387755,0.029822533793982
jisukim8873/falcon-7B-case-8,hendrycksTest-sociology,5-shot,accuracy,0.3880597014925373,0.0344578996436275
jisukim8873/falcon-7B-case-8,hendrycksTest-sociology,5-shot,acc_norm,0.3880597014925373,0.0344578996436275
jisukim8873/falcon-7B-case-8,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.49,0.0502418393795691
jisukim8873/falcon-7B-case-8,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.49,0.0502418393795691
jisukim8873/falcon-7B-case-8,hendrycksTest-virology,5-shot,accuracy,0.3614457831325301,0.0374005938202932
jisukim8873/falcon-7B-case-8,hendrycksTest-virology,5-shot,acc_norm,0.3614457831325301,0.0374005938202932
jisukim8873/falcon-7B-case-8,hendrycksTest-world_religions,5-shot,accuracy,0.3508771929824561,0.0366029883404916
jisukim8873/falcon-7B-case-8,hendrycksTest-world_religions,5-shot,acc_norm,0.3508771929824561,0.0366029883404916
jisukim8873/falcon-7B-case-8,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.0153218216884761
jisukim8873/falcon-7B-case-8,truthfulqa:mc,0-shot,mc2,0.3757559144452952,0.0143047144954415
jisukim8873/falcon-7B-case-8,winogrande,5-shot,accuracy,0.7048145224940805,0.0128194107417547
jisukim8873/falcon-7B-case-8,gsm8k,5-shot,accuracy,0.0750568612585291,0.0072576331454866
jisukim8873/falcon-7B-case-8,minerva_math_precalc,5-shot,accuracy,0.0164835164835164,0.0054540297647667
jisukim8873/falcon-7B-case-8,minerva_math_prealgebra,5-shot,accuracy,0.0367393800229621,0.0063779070882108
jisukim8873/falcon-7B-case-8,minerva_math_num_theory,5-shot,accuracy,0.0074074074074074,0.0036933821684372
jisukim8873/falcon-7B-case-8,minerva_math_intermediate_algebra,5-shot,accuracy,0.0155038759689922,0.0041136172383604
jisukim8873/falcon-7B-case-8,minerva_math_geometry,5-shot,accuracy,0.0104384133611691,0.0046486271171846
jisukim8873/falcon-7B-case-8,minerva_math_counting_and_prob,5-shot,accuracy,0.0210970464135021,0.0066076963656265
jisukim8873/falcon-7B-case-8,minerva_math_algebra,5-shot,accuracy,0.0193765796124684,0.0040026474981053
jisukim8873/falcon-7B-case-8,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-8,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-8,arithmetic_3da,5-shot,accuracy,0.2185,0.0092423798771147
jisukim8873/falcon-7B-case-8,arithmetic_3ds,5-shot,accuracy,0.646,0.0106957561490434
jisukim8873/falcon-7B-case-8,arithmetic_4da,5-shot,accuracy,0.0085,0.0020532859010609
jisukim8873/falcon-7B-case-8,arithmetic_2ds,5-shot,accuracy,0.6345,0.0107709276035409
jisukim8873/falcon-7B-case-8,arithmetic_5ds,5-shot,accuracy,0.056,0.005142491867889
jisukim8873/falcon-7B-case-8,arithmetic_5da,5-shot,accuracy,0.0025,0.0011169148353275
jisukim8873/falcon-7B-case-8,arithmetic_1dc,5-shot,accuracy,0.053,0.0050107937521926
jisukim8873/falcon-7B-case-8,arithmetic_4ds,5-shot,accuracy,0.1425,0.0078184038472926
jisukim8873/falcon-7B-case-8,arithmetic_2dm,5-shot,accuracy,0.2425,0.0095860743482774
jisukim8873/falcon-7B-case-8,arithmetic_2da,5-shot,accuracy,0.716,0.0100857752022694
jisukim8873/falcon-7B-case-8,gsm8k_cot,5-shot,accuracy,0.089461713419257,0.0078615830499397
jisukim8873/falcon-7B-case-8,anli_r2,0-shot,brier_score,0.8665276269305205,
jisukim8873/falcon-7B-case-8,anli_r3,0-shot,brier_score,0.8318243198290138,
jisukim8873/falcon-7B-case-8,anli_r1,0-shot,brier_score,0.8939690888764009,
jisukim8873/falcon-7B-case-8,xnli_eu,0-shot,brier_score,1.0427344013967832,
jisukim8873/falcon-7B-case-8,xnli_vi,0-shot,brier_score,1.0487135581024136,
jisukim8873/falcon-7B-case-8,xnli_ru,0-shot,brier_score,0.8221328820374796,
jisukim8873/falcon-7B-case-8,xnli_zh,0-shot,brier_score,1.024430866668074,
jisukim8873/falcon-7B-case-8,xnli_tr,0-shot,brier_score,0.9970781849876954,
jisukim8873/falcon-7B-case-8,xnli_fr,0-shot,brier_score,0.7436567906026784,
jisukim8873/falcon-7B-case-8,xnli_en,0-shot,brier_score,0.6425702304442446,
jisukim8873/falcon-7B-case-8,xnli_ur,0-shot,brier_score,1.3117764974375834,
jisukim8873/falcon-7B-case-8,xnli_ar,0-shot,brier_score,1.2508515010911752,
jisukim8873/falcon-7B-case-8,xnli_de,0-shot,brier_score,0.8387305217972838,
jisukim8873/falcon-7B-case-8,xnli_hi,0-shot,brier_score,1.1022494172288069,
jisukim8873/falcon-7B-case-8,xnli_es,0-shot,brier_score,0.8133629952055023,
jisukim8873/falcon-7B-case-8,xnli_bg,0-shot,brier_score,0.9484830535778096,
jisukim8873/falcon-7B-case-8,xnli_sw,0-shot,brier_score,1.0888680824673866,
jisukim8873/falcon-7B-case-8,xnli_el,0-shot,brier_score,0.9888240024756008,
jisukim8873/falcon-7B-case-8,xnli_th,0-shot,brier_score,0.9812608603086016,
jisukim8873/falcon-7B-case-8,logiqa2,0-shot,brier_score,1.059925394028478,
jisukim8873/falcon-7B-case-8,mathqa,0-shot,brier_score,0.9414128848103128,
jisukim8873/falcon-7B-case-8,lambada_standard,0-shot,perplexity,4.213587010663793,0.0909206003026987
jisukim8873/falcon-7B-case-8,lambada_standard,0-shot,accuracy,0.6557345235784979,0.0066194641433124
jisukim8873/falcon-7B-case-8,lambada_openai,0-shot,perplexity,3.391177573570244,0.0719254154383281
jisukim8873/falcon-7B-case-8,lambada_openai,0-shot,accuracy,0.7269551717446148,0.0062070160102915
EleutherAI/pythia-410m,arc:challenge,25-shot,accuracy,0.2312286689419795,0.0123208588347722
EleutherAI/pythia-410m,arc:challenge,25-shot,acc_norm,0.2619453924914676,0.0128490548268581
EleutherAI/pythia-410m,hellaswag,10-shot,accuracy,0.3394742083250348,0.0047256309115203
EleutherAI/pythia-410m,hellaswag,10-shot,acc_norm,0.4084843656642103,0.004905489494005
EleutherAI/pythia-410m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-410m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.0435194139889244
EleutherAI/pythia-410m,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.0388500424580025
EleutherAI/pythia-410m,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.0388500424580025
EleutherAI/pythia-410m,hendrycksTest-astronomy,5-shot,accuracy,0.2171052631578947,0.0335504530488292
EleutherAI/pythia-410m,hendrycksTest-astronomy,5-shot,acc_norm,0.2171052631578947,0.0335504530488292
EleutherAI/pythia-410m,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.0446196043338473
EleutherAI/pythia-410m,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.0446196043338473
EleutherAI/pythia-410m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2490566037735849,0.0266164829805017
EleutherAI/pythia-410m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2490566037735849,0.0266164829805017
EleutherAI/pythia-410m,hendrycksTest-college_biology,5-shot,accuracy,0.2777777777777778,0.0374555479146245
EleutherAI/pythia-410m,hendrycksTest-college_biology,5-shot,acc_norm,0.2777777777777778,0.0374555479146245
EleutherAI/pythia-410m,hendrycksTest-college_chemistry,5-shot,accuracy,0.42,0.0496044963748858
EleutherAI/pythia-410m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.42,0.0496044963748858
EleutherAI/pythia-410m,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.047258156262526
EleutherAI/pythia-410m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.047258156262526
EleutherAI/pythia-410m,hendrycksTest-college_mathematics,5-shot,accuracy,0.34,0.0476095228569523
EleutherAI/pythia-410m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.34,0.0476095228569523
EleutherAI/pythia-410m,hendrycksTest-college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177498
EleutherAI/pythia-410m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2080924855491329,0.0309528902177498
EleutherAI/pythia-410m,hendrycksTest-college_physics,5-shot,accuracy,0.2450980392156862,0.0428010583736439
EleutherAI/pythia-410m,hendrycksTest-college_physics,5-shot,acc_norm,0.2450980392156862,0.0428010583736439
EleutherAI/pythia-410m,hendrycksTest-computer_security,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-410m,hendrycksTest-computer_security,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-410m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2893617021276595,0.0296440065770096
EleutherAI/pythia-410m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2893617021276595,0.0296440065770096
EleutherAI/pythia-410m,hendrycksTest-econometrics,5-shot,accuracy,0.2280701754385964,0.0394715278266941
EleutherAI/pythia-410m,hendrycksTest-econometrics,5-shot,acc_norm,0.2280701754385964,0.0394715278266941
EleutherAI/pythia-410m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2275862068965517,0.0349395038013118
EleutherAI/pythia-410m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2275862068965517,0.0349395038013118
EleutherAI/pythia-410m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.238095238095238,0.0219358780811847
EleutherAI/pythia-410m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.238095238095238,0.0219358780811847
EleutherAI/pythia-410m,hendrycksTest-formal_logic,5-shot,accuracy,0.2936507936507936,0.0407352432214712
EleutherAI/pythia-410m,hendrycksTest-formal_logic,5-shot,acc_norm,0.2936507936507936,0.0407352432214712
EleutherAI/pythia-410m,hendrycksTest-global_facts,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-410m,hendrycksTest-global_facts,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/pythia-410m,hendrycksTest-high_school_biology,5-shot,accuracy,0.2935483870967741,0.0259060870213192
EleutherAI/pythia-410m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2935483870967741,0.0259060870213192
EleutherAI/pythia-410m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2610837438423645,0.0309037969521144
EleutherAI/pythia-410m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2610837438423645,0.0309037969521144
EleutherAI/pythia-410m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-410m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-410m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-410m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-410m,hendrycksTest-high_school_geography,5-shot,accuracy,0.303030303030303,0.0327428791402686
EleutherAI/pythia-410m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.303030303030303,0.0327428791402686
EleutherAI/pythia-410m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.233160621761658,0.030516111371476
EleutherAI/pythia-410m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.233160621761658,0.030516111371476
EleutherAI/pythia-410m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.358974358974359,0.0243217384846023
EleutherAI/pythia-410m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.358974358974359,0.0243217384846023
EleutherAI/pythia-410m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
EleutherAI/pythia-410m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
EleutherAI/pythia-410m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2394957983193277,0.0277220654933612
EleutherAI/pythia-410m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2394957983193277,0.0277220654933612
EleutherAI/pythia-410m,hendrycksTest-high_school_physics,5-shot,accuracy,0.2913907284768212,0.0371018572611999
EleutherAI/pythia-410m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2913907284768212,0.0371018572611999
EleutherAI/pythia-410m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.326605504587156,0.0201069908899373
EleutherAI/pythia-410m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.326605504587156,0.0201069908899373
EleutherAI/pythia-410m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2696078431372549,0.0311455706594867
EleutherAI/pythia-410m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2696078431372549,0.0311455706594867
EleutherAI/pythia-410m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2236286919831223,0.0271232982052299
EleutherAI/pythia-410m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2236286919831223,0.0271232982052299
EleutherAI/pythia-410m,hendrycksTest-human_aging,5-shot,accuracy,0.2511210762331838,0.0291052208332246
EleutherAI/pythia-410m,hendrycksTest-human_aging,5-shot,acc_norm,0.2511210762331838,0.0291052208332246
EleutherAI/pythia-410m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-410m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-410m,hendrycksTest-international_law,5-shot,accuracy,0.4049586776859504,0.0448113775594246
EleutherAI/pythia-410m,hendrycksTest-international_law,5-shot,acc_norm,0.4049586776859504,0.0448113775594246
EleutherAI/pythia-410m,hendrycksTest-jurisprudence,5-shot,accuracy,0.2129629629629629,0.0395783547198098
EleutherAI/pythia-410m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2129629629629629,0.0395783547198098
EleutherAI/pythia-410m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2822085889570552,0.0353611788666474
EleutherAI/pythia-410m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2822085889570552,0.0353611788666474
EleutherAI/pythia-410m,hendrycksTest-machine_learning,5-shot,accuracy,0.2142857142857142,0.0389464112004479
EleutherAI/pythia-410m,hendrycksTest-machine_learning,5-shot,acc_norm,0.2142857142857142,0.0389464112004479
EleutherAI/pythia-410m,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.0435463107726059
EleutherAI/pythia-410m,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.0435463107726059
EleutherAI/pythia-410m,hendrycksTest-marketing,5-shot,accuracy,0.2051282051282051,0.0264535080540403
EleutherAI/pythia-410m,hendrycksTest-marketing,5-shot,acc_norm,0.2051282051282051,0.0264535080540403
EleutherAI/pythia-410m,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-410m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-410m,hendrycksTest-miscellaneous,5-shot,accuracy,0.2579821200510855,0.0156458301883489
EleutherAI/pythia-410m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2579821200510855,0.0156458301883489
EleutherAI/pythia-410m,hendrycksTest-moral_disputes,5-shot,accuracy,0.2543352601156069,0.0234458262765455
EleutherAI/pythia-410m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2543352601156069,0.0234458262765455
EleutherAI/pythia-410m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2446927374301676,0.0143781698840984
EleutherAI/pythia-410m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2446927374301676,0.0143781698840984
EleutherAI/pythia-410m,hendrycksTest-nutrition,5-shot,accuracy,0.2549019607843137,0.0249541843248799
EleutherAI/pythia-410m,hendrycksTest-nutrition,5-shot,acc_norm,0.2549019607843137,0.0249541843248799
EleutherAI/pythia-410m,hendrycksTest-philosophy,5-shot,accuracy,0.2443729903536977,0.0244061620946689
EleutherAI/pythia-410m,hendrycksTest-philosophy,5-shot,acc_norm,0.2443729903536977,0.0244061620946689
EleutherAI/pythia-410m,hendrycksTest-prehistory,5-shot,accuracy,0.2253086419753086,0.0232462026478197
EleutherAI/pythia-410m,hendrycksTest-prehistory,5-shot,acc_norm,0.2253086419753086,0.0232462026478197
EleutherAI/pythia-410m,hendrycksTest-professional_accounting,5-shot,accuracy,0.2446808510638297,0.0256455536222667
EleutherAI/pythia-410m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2446808510638297,0.0256455536222667
EleutherAI/pythia-410m,hendrycksTest-professional_law,5-shot,accuracy,0.241199478487614,0.0109264961020349
EleutherAI/pythia-410m,hendrycksTest-professional_law,5-shot,acc_norm,0.241199478487614,0.0109264961020349
EleutherAI/pythia-410m,hendrycksTest-professional_medicine,5-shot,accuracy,0.4448529411764705,0.0301875320603293
EleutherAI/pythia-410m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4448529411764705,0.0301875320603293
EleutherAI/pythia-410m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2777777777777778,0.0181202242514845
EleutherAI/pythia-410m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2777777777777778,0.0181202242514845
EleutherAI/pythia-410m,hendrycksTest-public_relations,5-shot,accuracy,0.209090909090909,0.0389509101572413
EleutherAI/pythia-410m,hendrycksTest-public_relations,5-shot,acc_norm,0.209090909090909,0.0389509101572413
EleutherAI/pythia-410m,hendrycksTest-security_studies,5-shot,accuracy,0.4040816326530612,0.0314147080258658
EleutherAI/pythia-410m,hendrycksTest-security_studies,5-shot,acc_norm,0.4040816326530612,0.0314147080258658
EleutherAI/pythia-410m,hendrycksTest-sociology,5-shot,accuracy,0.2338308457711442,0.0299294154083483
EleutherAI/pythia-410m,hendrycksTest-sociology,5-shot,acc_norm,0.2338308457711442,0.0299294154083483
EleutherAI/pythia-410m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-410m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.25,0.0435194139889244
EleutherAI/pythia-410m,hendrycksTest-virology,5-shot,accuracy,0.2710843373493976,0.0346057990755302
EleutherAI/pythia-410m,hendrycksTest-virology,5-shot,acc_norm,0.2710843373493976,0.0346057990755302
EleutherAI/pythia-410m,hendrycksTest-world_religions,5-shot,accuracy,0.2748538011695906,0.0342404292469158
EleutherAI/pythia-410m,hendrycksTest-world_religions,5-shot,acc_norm,0.2748538011695906,0.0342404292469158
EleutherAI/pythia-410m,truthfulqa:mc,0-shot,mc1,0.237454100367197,0.0148962774410418
EleutherAI/pythia-410m,truthfulqa:mc,0-shot,mc2,0.4121958367286861,0.0145644511579495
EleutherAI/pythia-410m,winogrande,5-shot,accuracy,0.5311760063141279,0.0140251426406395
EleutherAI/pythia-410m,drop,3-shot,accuracy,0.0018875838926174,0.0004445109990559
EleutherAI/pythia-410m,drop,3-shot,f1,0.0446004614093961,0.0012188499729627
EleutherAI/pythia-410m,gsm8k,5-shot,accuracy,0.0068233510235026,0.0022675371022545
bigscience/bloom-7b1,minerva_math_precalc,5-shot,accuracy,0.0036630036630036,0.0025877573681934
bigscience/bloom-7b1,minerva_math_prealgebra,5-shot,accuracy,0.0011481056257175,0.0011481056257175
bigscience/bloom-7b1,minerva_math_num_theory,5-shot,accuracy,0.0037037037037037,0.0026164834572311
bigscience/bloom-7b1,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417,0.001565259593407
bigscience/bloom-7b1,minerva_math_geometry,5-shot,accuracy,0.0,
bigscience/bloom-7b1,minerva_math_counting_and_prob,5-shot,accuracy,0.0042194092827004,0.002980417365102
bigscience/bloom-7b1,minerva_math_algebra,5-shot,accuracy,0.0058972198820556,0.002223294328831
bigscience/bloom-7b1,fld_default,0-shot,accuracy,0.0,
bigscience/bloom-7b1,fld_star,0-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_3da,5-shot,accuracy,0.002,0.0009992493430694
bigscience/bloom-7b1,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005
bigscience/bloom-7b1,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
bigscience/bloom-7b1,arithmetic_2ds,5-shot,accuracy,0.0135,0.0025811249685073
bigscience/bloom-7b1,arithmetic_5ds,5-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_5da,5-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_1dc,5-shot,accuracy,0.024,0.0034231358327511
bigscience/bloom-7b1,arithmetic_4ds,5-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_2dm,5-shot,accuracy,0.0315,0.0039065977208918
bigscience/bloom-7b1,arithmetic_2da,5-shot,accuracy,0.0185,0.0030138707185866
bigscience/bloom-7b1,gsm8k_cot,5-shot,accuracy,0.0219863532979529,0.00403916275811
bigscience/bloom-7b1,gsm8k,5-shot,accuracy,0.0136467020470053,0.0031957470754807
bigscience/bloom-7b1,anli_r2,0-shot,brier_score,0.9456261005824846,
bigscience/bloom-7b1,anli_r3,0-shot,brier_score,0.9124531167930634,
bigscience/bloom-7b1,anli_r1,0-shot,brier_score,0.982579018108182,
bigscience/bloom-7b1,xnli_eu,0-shot,brier_score,0.7070982982123462,
bigscience/bloom-7b1,xnli_vi,0-shot,brier_score,0.7579336684566961,
bigscience/bloom-7b1,xnli_ru,0-shot,brier_score,0.7718940275749611,
bigscience/bloom-7b1,xnli_zh,0-shot,brier_score,1.1474241210913452,
bigscience/bloom-7b1,xnli_tr,0-shot,brier_score,0.8440682891485775,
bigscience/bloom-7b1,xnli_fr,0-shot,brier_score,0.7413355596902762,
bigscience/bloom-7b1,xnli_en,0-shot,brier_score,0.6356461345686089,
bigscience/bloom-7b1,xnli_ur,0-shot,brier_score,0.925162434749102,
bigscience/bloom-7b1,xnli_ar,0-shot,brier_score,1.120036997482241,
bigscience/bloom-7b1,xnli_de,0-shot,brier_score,0.892294512080338,
bigscience/bloom-7b1,xnli_hi,0-shot,brier_score,0.7053726297546287,
bigscience/bloom-7b1,xnli_es,0-shot,brier_score,0.7988409860889734,
bigscience/bloom-7b1,xnli_bg,0-shot,brier_score,0.9255423577067828,
bigscience/bloom-7b1,xnli_sw,0-shot,brier_score,0.9813718635919856,
bigscience/bloom-7b1,xnli_el,0-shot,brier_score,0.92712628550691,
bigscience/bloom-7b1,xnli_th,0-shot,brier_score,1.032461389997439,
bigscience/bloom-7b1,logiqa2,0-shot,brier_score,1.1331202068301027,
bigscience/bloom-7b1,mathqa,0-shot,brier_score,0.9772026799355674,
bigscience/bloom-7b1,lambada_standard,0-shot,perplexity,7.352795006135428,0.202079419708096
bigscience/bloom-7b1,lambada_standard,0-shot,accuracy,0.581797011449641,0.0068721302440514
bigscience/bloom-7b1,lambada_openai,0-shot,perplexity,6.619167022578283,0.1761060825914246
bigscience/bloom-7b1,lambada_openai,0-shot,accuracy,0.5761692218125364,0.0068846734549169
bigscience/bloom-7b1,mmlu_world_religions,0-shot,accuracy,0.2923976608187134,0.0348864771345792
bigscience/bloom-7b1,mmlu_formal_logic,0-shot,accuracy,0.1666666666666666,0.0333333333333333
bigscience/bloom-7b1,mmlu_prehistory,0-shot,accuracy,0.2623456790123457,0.0244772228561351
bigscience/bloom-7b1,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
bigscience/bloom-7b1,mmlu_high_school_world_history,0-shot,accuracy,0.2658227848101265,0.0287567996296583
bigscience/bloom-7b1,mmlu_moral_disputes,0-shot,accuracy,0.2427745664739884,0.0230836585869842
bigscience/bloom-7b1,mmlu_professional_law,0-shot,accuracy,0.2542372881355932,0.0111211290078406
bigscience/bloom-7b1,mmlu_logical_fallacies,0-shot,accuracy,0.263803680981595,0.0346241993161562
bigscience/bloom-7b1,mmlu_high_school_us_history,0-shot,accuracy,0.2745098039215686,0.0313217980308329
bigscience/bloom-7b1,mmlu_philosophy,0-shot,accuracy,0.2733118971061093,0.0253117659754261
bigscience/bloom-7b1,mmlu_jurisprudence,0-shot,accuracy,0.287037037037037,0.0437331304091476
bigscience/bloom-7b1,mmlu_international_law,0-shot,accuracy,0.3553719008264462,0.0436923632657398
bigscience/bloom-7b1,mmlu_high_school_european_history,0-shot,accuracy,0.2363636363636363,0.0331750593000917
bigscience/bloom-7b1,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2072538860103626,0.0292528232918036
bigscience/bloom-7b1,mmlu_high_school_microeconomics,0-shot,accuracy,0.226890756302521,0.0272053715382794
bigscience/bloom-7b1,mmlu_high_school_geography,0-shot,accuracy,0.2323232323232323,0.0300886294902174
bigscience/bloom-7b1,mmlu_high_school_psychology,0-shot,accuracy,0.2440366972477064,0.0184152863514164
bigscience/bloom-7b1,mmlu_public_relations,0-shot,accuracy,0.3272727272727272,0.0449429086625209
bigscience/bloom-7b1,mmlu_us_foreign_policy,0-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-7b1,mmlu_sociology,0-shot,accuracy,0.2388059701492537,0.0301477759354092
bigscience/bloom-7b1,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2307692307692307,0.0213620277252227
bigscience/bloom-7b1,mmlu_security_studies,0-shot,accuracy,0.3142857142857143,0.0297193294224174
bigscience/bloom-7b1,mmlu_professional_psychology,0-shot,accuracy,0.261437908496732,0.017776947157528
bigscience/bloom-7b1,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.0372767357559691
bigscience/bloom-7b1,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
bigscience/bloom-7b1,mmlu_miscellaneous,0-shot,accuracy,0.2899106002554278,0.0162250179447709
bigscience/bloom-7b1,mmlu_marketing,0-shot,accuracy,0.2478632478632478,0.0282863240755643
bigscience/bloom-7b1,mmlu_management,0-shot,accuracy,0.2524271844660194,0.0430125039969087
bigscience/bloom-7b1,mmlu_nutrition,0-shot,accuracy,0.2352941176470588,0.0242886194660461
bigscience/bloom-7b1,mmlu_medical_genetics,0-shot,accuracy,0.26,0.0440844002276807
bigscience/bloom-7b1,mmlu_human_aging,0-shot,accuracy,0.3632286995515695,0.032277904428505
bigscience/bloom-7b1,mmlu_professional_medicine,0-shot,accuracy,0.2058823529411764,0.0245622043141423
bigscience/bloom-7b1,mmlu_college_medicine,0-shot,accuracy,0.2080924855491329,0.0309528902177498
bigscience/bloom-7b1,mmlu_business_ethics,0-shot,accuracy,0.26,0.0440844002276808
bigscience/bloom-7b1,mmlu_clinical_knowledge,0-shot,accuracy,0.2754716981132075,0.027495663683724
bigscience/bloom-7b1,mmlu_global_facts,0-shot,accuracy,0.32,0.046882617226215
bigscience/bloom-7b1,mmlu_virology,0-shot,accuracy,0.3072289156626506,0.0359156679782466
bigscience/bloom-7b1,mmlu_professional_accounting,0-shot,accuracy,0.2659574468085106,0.0263580656988805
bigscience/bloom-7b1,mmlu_college_physics,0-shot,accuracy,0.2058823529411764,0.0402338227361774
bigscience/bloom-7b1,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649,0.0347918557259966
bigscience/bloom-7b1,mmlu_high_school_biology,0-shot,accuracy,0.2516129032258064,0.0246859792862399
bigscience/bloom-7b1,mmlu_college_biology,0-shot,accuracy,0.1944444444444444,0.03309615177059
bigscience/bloom-7b1,mmlu_anatomy,0-shot,accuracy,0.2444444444444444,0.0371253783361486
bigscience/bloom-7b1,mmlu_college_chemistry,0-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-7b1,mmlu_computer_security,0-shot,accuracy,0.23,0.042295258468165
bigscience/bloom-7b1,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
bigscience/bloom-7b1,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
bigscience/bloom-7b1,mmlu_college_mathematics,0-shot,accuracy,0.33,0.047258156262526
bigscience/bloom-7b1,mmlu_conceptual_physics,0-shot,accuracy,0.3234042553191489,0.0305794427736103
bigscience/bloom-7b1,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
bigscience/bloom-7b1,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.047258156262526
bigscience/bloom-7b1,mmlu_machine_learning,0-shot,accuracy,0.3035714285714285,0.0436422615584104
bigscience/bloom-7b1,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.0312709071329769
bigscience/bloom-7b1,mmlu_high_school_statistics,0-shot,accuracy,0.3842592592592592,0.0331735451431074
bigscience/bloom-7b1,mmlu_elementary_mathematics,0-shot,accuracy,0.2777777777777778,0.0230681888482611
bigscience/bloom-7b1,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.0360010569272777
bigscience/bloom-7b1,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
bigscience/bloom-7b1,arc_challenge,25-shot,accuracy,0.3694539249146757,0.0141045783664918
bigscience/bloom-7b1,arc_challenge,25-shot,acc_norm,0.3984641638225256,0.0143069460527355
bigscience/bloom-7b1,hellaswag,10-shot,accuracy,0.4622585142401912,0.0049755460189506
bigscience/bloom-7b1,hellaswag,10-shot,acc_norm,0.6199960167297351,0.0048439543384514
bigscience/bloom-7b1,truthfulqa_mc2,0-shot,accuracy,0.3889181333038942,0.0140162573575294
bigscience/bloom-7b1,truthfulqa_gen,0-shot,bleu_max,6.2009331295626025,0.4001608995947457
bigscience/bloom-7b1,truthfulqa_gen,0-shot,bleu_acc,0.1481028151774786,0.0124345527503192
bigscience/bloom-7b1,truthfulqa_gen,0-shot,bleu_diff,-2.008955984058176,0.3653377242408805
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge1_max,15.973477335995934,0.7756937195072751
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge1_acc,0.1481028151774786,0.0124345527503192
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge1_diff,-3.50475730525046,0.4764981051843214
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge2_max,9.298241101337537,0.6208670674741479
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge2_acc,0.0893512851897184,0.0099857516767558
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge2_diff,-3.88603405826616,0.5670349584056208
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rougeL_max,15.125543867609249,0.7432106357285632
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rougeL_acc,0.1432068543451652,0.0122623812587307
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rougeL_diff,-3.509115227685031,0.483901807348036
bigscience/bloom-7b1,truthfulqa_mc1,0-shot,accuracy,0.2239902080783353,0.0145949643294742
bigscience/bloom-7b1,winogrande,5-shot,accuracy,0.654301499605367,0.0133665969519343
llama2_220M_nl_0_code_100,minerva_math_algebra,5-shot,accuracy,0.0058972198820556,0.002223294328831
llama2_220M_nl_0_code_100,minerva_math_counting_and_prob,5-shot,accuracy,0.0042194092827004,0.002980417365102
llama2_220M_nl_0_code_100,minerva_math_geometry,5-shot,accuracy,0.0041753653444676,0.0029493392170756
llama2_220M_nl_0_code_100,minerva_math_intermediate_algebra,5-shot,accuracy,0.0044296788482834,0.0022111531423787
llama2_220M_nl_0_code_100,minerva_math_num_theory,5-shot,accuracy,0.0037037037037037,0.0026164834572311
llama2_220M_nl_0_code_100,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458,0.0034284443646836
llama2_220M_nl_0_code_100,minerva_math_precalc,5-shot,accuracy,0.0036630036630036,0.0025877573681934
llama2_220M_nl_0_code_100,gsm8k,5-shot,accuracy,0.0197119029567854,0.0038289829787357
llama2_220M_nl_0_code_100,gsm8k_cot,5-shot,accuracy,0.0204700530705079,0.0039004133859157
llama2_220M_nl_0_code_100,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_2ds,5-shot,accuracy,0.0245,0.0034577236625362
llama2_220M_nl_0_code_100,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
llama2_220M_nl_0_code_100,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
llama2_220M_nl_0_code_100,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_2dm,5-shot,accuracy,0.0265,0.0035923985947876
llama2_220M_nl_0_code_100,arithmetic_1dc,5-shot,accuracy,0.038,0.0042763469891703
llama2_220M_nl_0_code_100,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646
llama2_220M_nl_0_code_100,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
llama2_220M_nl_0_code_100,xnli_ar,0-shot,brier_score,0.8176539670232567,
llama2_220M_nl_0_code_100,xnli_bg,0-shot,brier_score,0.8604648517139746,
llama2_220M_nl_0_code_100,xnli_de,0-shot,brier_score,0.940391152727248,
llama2_220M_nl_0_code_100,xnli_el,0-shot,brier_score,1.0431339612029014,
llama2_220M_nl_0_code_100,xnli_en,0-shot,brier_score,0.8164394084114779,
llama2_220M_nl_0_code_100,xnli_es,0-shot,brier_score,1.2047493721711582,
llama2_220M_nl_0_code_100,xnli_fr,0-shot,brier_score,1.02051418253936,
llama2_220M_nl_0_code_100,xnli_hi,0-shot,brier_score,1.1271171129959119,
llama2_220M_nl_0_code_100,xnli_ru,0-shot,brier_score,0.8951802179371576,
llama2_220M_nl_0_code_100,xnli_sw,0-shot,brier_score,0.9851942231444415,
llama2_220M_nl_0_code_100,xnli_th,0-shot,brier_score,0.923504499165945,
llama2_220M_nl_0_code_100,xnli_tr,0-shot,brier_score,0.9844416659876318,
llama2_220M_nl_0_code_100,xnli_ur,0-shot,brier_score,1.2881919341963368,
llama2_220M_nl_0_code_100,xnli_vi,0-shot,brier_score,0.8189555595429009,
llama2_220M_nl_0_code_100,xnli_zh,0-shot,brier_score,0.9499888101388722,
llama2_220M_nl_0_code_100,anli_r1,0-shot,brier_score,0.8742678754335909,
llama2_220M_nl_0_code_100,anli_r3,0-shot,brier_score,0.8729445717458411,
llama2_220M_nl_0_code_100,anli_r2,0-shot,brier_score,0.8796440783116556,
llama2_220M_nl_0_code_100,logiqa2,0-shot,brier_score,1.1645386553978825,
llama2_220M_nl_0_code_100,lambada_standard,0-shot,perplexity,591.6803510463476,28.025331080505545
llama2_220M_nl_0_code_100,lambada_standard,0-shot,accuracy,0.149233456239084,0.0049642126051581
llama2_220M_nl_0_code_100,lambada_openai,0-shot,perplexity,445.5204646626984,21.81250787402026
llama2_220M_nl_0_code_100,lambada_openai,0-shot,accuracy,0.175431787308364,0.0052988242006676
google/gemma-2-9b,mmlu_world_religions,0-shot,accuracy,0.8596491228070176,0.0266405825391332
google/gemma-2-9b,mmlu_formal_logic,0-shot,accuracy,0.4603174603174603,0.04458029125470973
google/gemma-2-9b,mmlu_prehistory,0-shot,accuracy,0.7777777777777778,0.023132376234543332
google/gemma-2-9b,mmlu_moral_scenarios,0-shot,accuracy,0.2905027932960894,0.015183844307206151
google/gemma-2-9b,mmlu_high_school_world_history,0-shot,accuracy,0.8523206751054853,0.02309432958259569
google/gemma-2-9b,mmlu_moral_disputes,0-shot,accuracy,0.7341040462427746,0.02378620325550829
google/gemma-2-9b,mmlu_professional_law,0-shot,accuracy,0.5352020860495437,0.012738547371303956
google/gemma-2-9b,mmlu_logical_fallacies,0-shot,accuracy,0.8282208588957055,0.02963471727237103
google/gemma-2-9b,mmlu_high_school_us_history,0-shot,accuracy,0.8725490196078431,0.02340553048084631
google/gemma-2-9b,mmlu_philosophy,0-shot,accuracy,0.7331189710610932,0.025122637608816646
google/gemma-2-9b,mmlu_jurisprudence,0-shot,accuracy,0.8148148148148148,0.03755265865037182
google/gemma-2-9b,mmlu_international_law,0-shot,accuracy,0.8429752066115702,0.03321244842547128
google/gemma-2-9b,mmlu_high_school_european_history,0-shot,accuracy,0.7757575757575758,0.03256866661681102
google/gemma-2-9b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9326424870466321,0.018088393839078922
google/gemma-2-9b,mmlu_high_school_microeconomics,0-shot,accuracy,0.8151260504201681,0.025215992877954205
google/gemma-2-9b,mmlu_high_school_geography,0-shot,accuracy,0.8787878787878788,0.023253157951942067
google/gemma-2-9b,mmlu_high_school_psychology,0-shot,accuracy,0.8990825688073395,0.012914673545364441
google/gemma-2-9b,mmlu_public_relations,0-shot,accuracy,0.7636363636363637,0.04069306319721376
google/gemma-2-9b,mmlu_us_foreign_policy,0-shot,accuracy,0.91,0.028762349126466115
google/gemma-2-9b,mmlu_sociology,0-shot,accuracy,0.8606965174129353,0.024484487162913973
google/gemma-2-9b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.764102564102564,0.02152596540740872
google/gemma-2-9b,mmlu_security_studies,0-shot,accuracy,0.7591836734693878,0.027372942201788163
google/gemma-2-9b,mmlu_professional_psychology,0-shot,accuracy,0.7434640522875817,0.01766784161237897
google/gemma-2-9b,mmlu_human_sexuality,0-shot,accuracy,0.7938931297709924,0.03547771004159462
google/gemma-2-9b,mmlu_econometrics,0-shot,accuracy,0.5614035087719298,0.04668000738510455
google/gemma-2-9b,mmlu_miscellaneous,0-shot,accuracy,0.8492975734355045,0.012793420883120821
google/gemma-2-9b,mmlu_marketing,0-shot,accuracy,0.8974358974358975,0.019875655027867457
google/gemma-2-9b,mmlu_management,0-shot,accuracy,0.7961165048543689,0.0398913985953177
google/gemma-2-9b,mmlu_nutrition,0-shot,accuracy,0.7549019607843137,0.024630048979824775
google/gemma-2-9b,mmlu_medical_genetics,0-shot,accuracy,0.82,0.03861229196653694
google/gemma-2-9b,mmlu_human_aging,0-shot,accuracy,0.7668161434977578,0.028380391147094702
google/gemma-2-9b,mmlu_professional_medicine,0-shot,accuracy,0.7683823529411765,0.025626533803777565
google/gemma-2-9b,mmlu_college_medicine,0-shot,accuracy,0.6820809248554913,0.03550683989165582
google/gemma-2-9b,mmlu_business_ethics,0-shot,accuracy,0.67,0.04725815626252607
google/gemma-2-9b,mmlu_clinical_knowledge,0-shot,accuracy,0.769811320754717,0.025907897122408173
google/gemma-2-9b,mmlu_global_facts,0-shot,accuracy,0.5,0.050251890762960605
google/gemma-2-9b,mmlu_virology,0-shot,accuracy,0.5180722891566265,0.038899512528272166
google/gemma-2-9b,mmlu_professional_accounting,0-shot,accuracy,0.5283687943262412,0.029779450957303062
google/gemma-2-9b,mmlu_college_physics,0-shot,accuracy,0.5294117647058824,0.049665709039785295
google/gemma-2-9b,mmlu_high_school_physics,0-shot,accuracy,0.5562913907284768,0.04056527902281731
google/gemma-2-9b,mmlu_high_school_biology,0-shot,accuracy,0.8870967741935484,0.018003603325863614
google/gemma-2-9b,mmlu_college_biology,0-shot,accuracy,0.8680555555555556,0.028300968382044423
google/gemma-2-9b,mmlu_anatomy,0-shot,accuracy,0.7037037037037037,0.03944624162501116
google/gemma-2-9b,mmlu_college_chemistry,0-shot,accuracy,0.57,0.04975698519562428
google/gemma-2-9b,mmlu_computer_security,0-shot,accuracy,0.76,0.042923469599092816
google/gemma-2-9b,mmlu_college_computer_science,0-shot,accuracy,0.52,0.050211673156867795
google/gemma-2-9b,mmlu_astronomy,0-shot,accuracy,0.7631578947368421,0.03459777606810537
google/gemma-2-9b,mmlu_college_mathematics,0-shot,accuracy,0.45,0.049999999999999996
google/gemma-2-9b,mmlu_conceptual_physics,0-shot,accuracy,0.6851063829787234,0.030363582197238153
google/gemma-2-9b,mmlu_abstract_algebra,0-shot,accuracy,0.42,0.049604496374885836
google/gemma-2-9b,mmlu_high_school_computer_science,0-shot,accuracy,0.77,0.04229525846816506
google/gemma-2-9b,mmlu_machine_learning,0-shot,accuracy,0.45535714285714285,0.04726835553719099
google/gemma-2-9b,mmlu_high_school_chemistry,0-shot,accuracy,0.6551724137931034,0.033442837442804574
google/gemma-2-9b,mmlu_high_school_statistics,0-shot,accuracy,0.6620370370370371,0.03225941352631296
google/gemma-2-9b,mmlu_elementary_mathematics,0-shot,accuracy,0.5846560846560847,0.025379524910778398
google/gemma-2-9b,mmlu_electrical_engineering,0-shot,accuracy,0.6896551724137931,0.03855289616378947
google/gemma-2-9b,mmlu_high_school_mathematics,0-shot,accuracy,0.45555555555555555,0.03036486250482443
google/gemma-2-9b,arc_challenge,25-shot,accuracy,0.643344709897611,0.0139980569026201
google/gemma-2-9b,arc_challenge,25-shot,acc_norm,0.681740614334471,0.0136119939169714
google/gemma-2-9b,hellaswag,10-shot,accuracy,0.635929097789285,0.0048018528813297
google/gemma-2-9b,hellaswag,10-shot,acc_norm,0.8253335988846843,0.0037890554870031
google/gemma-2-9b,truthfulqa_mc2,0-shot,accuracy,0.4553219595410078,0.0142106288362367
google/gemma-2-9b,truthfulqa_gen,0-shot,bleu_max,33.48367702247776,0.8625505675030039
google/gemma-2-9b,truthfulqa_gen,0-shot,bleu_acc,0.430844553243574,0.0173352724753323
google/gemma-2-9b,truthfulqa_gen,0-shot,bleu_diff,-0.1987085728976373,1.0675737646404135
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge1_max,59.11557604984375,0.9001849627161471
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge1_acc,0.419828641370869,0.0172770303017757
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge1_diff,0.3244784775406543,1.280521042715171
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge2_max,44.49834625015418,1.106551965177841
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge2_acc,0.3745410036719706,0.0169435351284053
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge2_diff,-1.3262594872227933,1.471263187794327
google/gemma-2-9b,truthfulqa_gen,0-shot,rougeL_max,56.491514242375466,0.9276044446810214
google/gemma-2-9b,truthfulqa_gen,0-shot,rougeL_acc,0.405140758873929,0.0171856117277533
google/gemma-2-9b,truthfulqa_gen,0-shot,rougeL_diff,-0.0652215322008431,1.2948177926334004
google/gemma-2-9b,truthfulqa_mc1,0-shot,accuracy,0.3011015911872705,0.0160589990261006
google/gemma-2-9b,winogrande,5-shot,accuracy,0.7876874506708761,0.0114933846872497
google/gemma-2-9b,gsm8k,5-shot,accuracy,0.6747536012130402,0.0129039047525439
llama2_220M_nl_20_code_80,minerva_math_algebra,5-shot,accuracy,0.0050547598989048,0.0020592425835567
llama2_220M_nl_20_code_80,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,minerva_math_geometry,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,minerva_math_intermediate_algebra,5-shot,accuracy,0.0055370985603543,0.0024707690436948
llama2_220M_nl_20_code_80,minerva_math_num_theory,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,minerva_math_prealgebra,5-shot,accuracy,0.0022962112514351,0.0016227331369346
llama2_220M_nl_20_code_80,minerva_math_precalc,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,gsm8k,5-shot,accuracy,0.0181956027293404,0.0036816118940738
llama2_220M_nl_20_code_80,gsm8k_cot,5-shot,accuracy,0.0197119029567854,0.0038289829787357
llama2_220M_nl_20_code_80,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_2ds,5-shot,accuracy,0.0245,0.0034577236625362
llama2_220M_nl_20_code_80,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
llama2_220M_nl_20_code_80,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
llama2_220M_nl_20_code_80,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_2dm,5-shot,accuracy,0.02,0.003131278085898
llama2_220M_nl_20_code_80,arithmetic_1dc,5-shot,accuracy,0.0055,0.0016541593398342
llama2_220M_nl_20_code_80,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646
llama2_220M_nl_20_code_80,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
llama2_220M_nl_20_code_80,xnli_ar,0-shot,brier_score,0.881660852799555,
llama2_220M_nl_20_code_80,xnli_bg,0-shot,brier_score,1.0362569716887076,
llama2_220M_nl_20_code_80,xnli_de,0-shot,brier_score,0.9019002876123214,
llama2_220M_nl_20_code_80,xnli_el,0-shot,brier_score,1.024320155206327,
llama2_220M_nl_20_code_80,xnli_en,0-shot,brier_score,0.7519860173387918,
llama2_220M_nl_20_code_80,xnli_es,0-shot,brier_score,1.062707961857973,
llama2_220M_nl_20_code_80,xnli_fr,0-shot,brier_score,0.9163038926997147,
llama2_220M_nl_20_code_80,xnli_hi,0-shot,brier_score,0.8668019485688128,
llama2_220M_nl_20_code_80,xnli_ru,0-shot,brier_score,0.9025895589348444,
llama2_220M_nl_20_code_80,xnli_sw,0-shot,brier_score,0.8724676187079832,
llama2_220M_nl_20_code_80,xnli_th,0-shot,brier_score,1.2900461396841212,
llama2_220M_nl_20_code_80,xnli_tr,0-shot,brier_score,0.8282279023949818,
llama2_220M_nl_20_code_80,xnli_ur,0-shot,brier_score,1.3235136429955503,
llama2_220M_nl_20_code_80,xnli_vi,0-shot,brier_score,0.9966685161971115,
llama2_220M_nl_20_code_80,xnli_zh,0-shot,brier_score,1.0404418296870241,
llama2_220M_nl_20_code_80,anli_r1,0-shot,brier_score,1.0391472679195413,
llama2_220M_nl_20_code_80,anli_r3,0-shot,brier_score,0.9958872355378052,
llama2_220M_nl_20_code_80,anli_r2,0-shot,brier_score,1.0353838728717688,
llama2_220M_nl_20_code_80,logiqa2,0-shot,brier_score,1.199012763133361,
llama2_220M_nl_20_code_80,lambada_standard,0-shot,perplexity,156.01458439376142,6.551228189535927
llama2_220M_nl_20_code_80,lambada_standard,0-shot,accuracy,0.2150203764797205,0.00572375549579
llama2_220M_nl_20_code_80,lambada_openai,0-shot,perplexity,65.5641350743855,2.61090295400462
llama2_220M_nl_20_code_80,lambada_openai,0-shot,accuracy,0.2872113332039588,0.0063036668455252
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli,0-shot,accuracy,0.352,0.0255
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mathqa,0-shot,accuracy,0.2191,0.0076
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math_algebra,5-shot,accuracy,0.0084,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,arithmetic,5-shot,accuracy,0.0086,0.0089
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math_precalc,5-shot,accuracy,0.0165,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,fld,0-shot,accuracy,0.0,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math_geometry,5-shot,accuracy,0.0063,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,asdiv,5-shot,accuracy,0.0056,0.0016
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math_num_theory,5-shot,accuracy,0.0093,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math_prealgebra,5-shot,accuracy,0.0138,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.0133,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,anli,0-shot,accuracy,0.3337,0.0145
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,gsm8k_cot,5-shot,accuracy,0.0053,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.0063,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,minerva_math,5-shot,accuracy,0.0108,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,logiqa2,0-shot,accuracy,0.2392,0.0108
AbacusResearch/haLLAwa3,minerva_math_precalc,5-shot,accuracy,0.0695970695970696,0.0109001571826533
AbacusResearch/haLLAwa3,minerva_math_prealgebra,5-shot,accuracy,0.3892078071182548,0.0165301914653455
AbacusResearch/haLLAwa3,minerva_math_num_theory,5-shot,accuracy,0.1129629629629629,0.0136346668800742
AbacusResearch/haLLAwa3,minerva_math_intermediate_algebra,5-shot,accuracy,0.0830564784053156,0.0091887149889854
AbacusResearch/haLLAwa3,minerva_math_geometry,5-shot,accuracy,0.162839248434238,0.0168876813564654
AbacusResearch/haLLAwa3,minerva_math_counting_and_prob,5-shot,accuracy,0.1772151898734177,0.0175575144493642
AbacusResearch/haLLAwa3,minerva_math_algebra,5-shot,accuracy,0.2923336141533277,0.0132072171040511
AbacusResearch/haLLAwa3,fld_default,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa3,fld_star,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa3,arithmetic_3da,5-shot,accuracy,0.983,0.0028913110935905
AbacusResearch/haLLAwa3,arithmetic_3ds,5-shot,accuracy,0.978,0.0032807593162018
AbacusResearch/haLLAwa3,arithmetic_4da,5-shot,accuracy,0.932,0.0056306173663253
AbacusResearch/haLLAwa3,arithmetic_2ds,5-shot,accuracy,0.993,0.0018647355360237
AbacusResearch/haLLAwa3,arithmetic_5ds,5-shot,accuracy,0.883,0.0071889735477559
AbacusResearch/haLLAwa3,arithmetic_5da,5-shot,accuracy,0.906,0.0065271204716035
AbacusResearch/haLLAwa3,arithmetic_1dc,5-shot,accuracy,0.7045,0.0102049961280241
AbacusResearch/haLLAwa3,arithmetic_4ds,5-shot,accuracy,0.934,0.005553144938623
AbacusResearch/haLLAwa3,arithmetic_2dm,5-shot,accuracy,0.639,0.0107423088113914
AbacusResearch/haLLAwa3,arithmetic_2da,5-shot,accuracy,0.9965,0.0013208888574315
AbacusResearch/haLLAwa3,gsm8k_cot,5-shot,accuracy,0.7134192570128886,0.0124548416683377
AbacusResearch/haLLAwa3,gsm8k,5-shot,accuracy,0.6823351023502654,0.0128240666214888
AbacusResearch/haLLAwa3,anli_r2,0-shot,brier_score,0.7822788074399861,
AbacusResearch/haLLAwa3,anli_r3,0-shot,brier_score,0.8264083512806276,
AbacusResearch/haLLAwa3,anli_r1,0-shot,brier_score,0.7075167500716043,
AbacusResearch/haLLAwa3,xnli_eu,0-shot,brier_score,1.1457721381774524,
AbacusResearch/haLLAwa3,xnli_vi,0-shot,brier_score,0.9869158803328254,
AbacusResearch/haLLAwa3,xnli_ru,0-shot,brier_score,0.8716743740754499,
AbacusResearch/haLLAwa3,xnli_zh,0-shot,brier_score,1.090951879740664,
AbacusResearch/haLLAwa3,xnli_tr,0-shot,brier_score,1.0701876481592625,
AbacusResearch/haLLAwa3,xnli_fr,0-shot,brier_score,0.8327048649607862,
AbacusResearch/haLLAwa3,xnli_en,0-shot,brier_score,0.712823188151288,
AbacusResearch/haLLAwa3,xnli_ur,0-shot,brier_score,1.239951455555465,
AbacusResearch/haLLAwa3,xnli_ar,0-shot,brier_score,1.2690967154108113,
AbacusResearch/haLLAwa3,xnli_de,0-shot,brier_score,0.9143433937106298,
AbacusResearch/haLLAwa3,xnli_hi,0-shot,brier_score,0.9583571701761374,
AbacusResearch/haLLAwa3,xnli_es,0-shot,brier_score,0.9546231307238467,
AbacusResearch/haLLAwa3,xnli_bg,0-shot,brier_score,0.9223977711612796,
AbacusResearch/haLLAwa3,xnli_sw,0-shot,brier_score,1.003001165064226,
AbacusResearch/haLLAwa3,xnli_el,0-shot,brier_score,0.9462798720866608,
AbacusResearch/haLLAwa3,xnli_th,0-shot,brier_score,1.0379297621590724,
AbacusResearch/haLLAwa3,logiqa2,0-shot,brier_score,0.9965660380454944,
AbacusResearch/haLLAwa3,mathqa,0-shot,brier_score,0.9484637194920996,
AbacusResearch/haLLAwa3,lambada_standard,0-shot,perplexity,3.80981815470925,0.1028360488313632
AbacusResearch/haLLAwa3,lambada_standard,0-shot,accuracy,0.6586454492528624,0.0066060334598652
AbacusResearch/haLLAwa3,lambada_openai,0-shot,perplexity,3.1571427801436447,0.0748057214301652
AbacusResearch/haLLAwa3,lambada_openai,0-shot,accuracy,0.7143411604890355,0.0062934493900561
AbacusResearch/haLLAwa3,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.0283809195961458
AbacusResearch/haLLAwa3,mmlu_formal_logic,0-shot,accuracy,0.4761904761904761,0.0446706262840327
AbacusResearch/haLLAwa3,mmlu_prehistory,0-shot,accuracy,0.7222222222222222,0.0249220011688863
AbacusResearch/haLLAwa3,mmlu_moral_scenarios,0-shot,accuracy,0.4692737430167598,0.0166908961619443
AbacusResearch/haLLAwa3,mmlu_high_school_world_history,0-shot,accuracy,0.8143459915611815,0.0253104953769448
AbacusResearch/haLLAwa3,mmlu_moral_disputes,0-shot,accuracy,0.7398843930635838,0.0236186783100693
AbacusResearch/haLLAwa3,mmlu_professional_law,0-shot,accuracy,0.4667535853976532,0.0127419743338972
AbacusResearch/haLLAwa3,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
AbacusResearch/haLLAwa3,mmlu_high_school_us_history,0-shot,accuracy,0.8333333333333334,0.026156867523931
AbacusResearch/haLLAwa3,mmlu_philosophy,0-shot,accuracy,0.7106109324758842,0.0257558659226329
AbacusResearch/haLLAwa3,mmlu_jurisprudence,0-shot,accuracy,0.8425925925925926,0.0352070399051796
AbacusResearch/haLLAwa3,mmlu_international_law,0-shot,accuracy,0.768595041322314,0.0384985609879408
AbacusResearch/haLLAwa3,mmlu_high_school_european_history,0-shot,accuracy,0.7696969696969697,0.0328766675860348
AbacusResearch/haLLAwa3,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8808290155440415,0.0233819353481214
AbacusResearch/haLLAwa3,mmlu_high_school_microeconomics,0-shot,accuracy,0.6596638655462185,0.0307780574229316
AbacusResearch/haLLAwa3,mmlu_high_school_geography,0-shot,accuracy,0.7929292929292929,0.028869778460267
AbacusResearch/haLLAwa3,mmlu_high_school_psychology,0-shot,accuracy,0.8385321100917431,0.0157762392561632
AbacusResearch/haLLAwa3,mmlu_public_relations,0-shot,accuracy,0.6272727272727273,0.0463138131942546
AbacusResearch/haLLAwa3,mmlu_us_foreign_policy,0-shot,accuracy,0.85,0.0358870281282637
AbacusResearch/haLLAwa3,mmlu_sociology,0-shot,accuracy,0.8507462686567164,0.025196929874827
AbacusResearch/haLLAwa3,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6717948717948717,0.0238076331986572
AbacusResearch/haLLAwa3,mmlu_security_studies,0-shot,accuracy,0.726530612244898,0.0285355603371284
AbacusResearch/haLLAwa3,mmlu_professional_psychology,0-shot,accuracy,0.6486928104575164,0.0193126760657865
AbacusResearch/haLLAwa3,mmlu_human_sexuality,0-shot,accuracy,0.7862595419847328,0.0359546161177469
AbacusResearch/haLLAwa3,mmlu_econometrics,0-shot,accuracy,0.4561403508771929,0.0468547304190778
AbacusResearch/haLLAwa3,mmlu_miscellaneous,0-shot,accuracy,0.8263090676883781,0.0135474156586622
AbacusResearch/haLLAwa3,mmlu_marketing,0-shot,accuracy,0.8803418803418803,0.0212627194004069
AbacusResearch/haLLAwa3,mmlu_management,0-shot,accuracy,0.8155339805825242,0.0384042362728827
AbacusResearch/haLLAwa3,mmlu_nutrition,0-shot,accuracy,0.7483660130718954,0.0248480182638751
AbacusResearch/haLLAwa3,mmlu_medical_genetics,0-shot,accuracy,0.68,0.046882617226215
AbacusResearch/haLLAwa3,mmlu_human_aging,0-shot,accuracy,0.6905829596412556,0.0310244117405722
AbacusResearch/haLLAwa3,mmlu_professional_medicine,0-shot,accuracy,0.6801470588235294,0.0283329595140312
AbacusResearch/haLLAwa3,mmlu_college_medicine,0-shot,accuracy,0.6358381502890174,0.036690724774169
AbacusResearch/haLLAwa3,mmlu_business_ethics,0-shot,accuracy,0.6,0.049236596391733
AbacusResearch/haLLAwa3,mmlu_clinical_knowledge,0-shot,accuracy,0.7018867924528301,0.0281528379424938
AbacusResearch/haLLAwa3,mmlu_global_facts,0-shot,accuracy,0.35,0.0479372485441101
AbacusResearch/haLLAwa3,mmlu_virology,0-shot,accuracy,0.536144578313253,0.0388231085089059
AbacusResearch/haLLAwa3,mmlu_professional_accounting,0-shot,accuracy,0.4680851063829787,0.0297666750758738
AbacusResearch/haLLAwa3,mmlu_college_physics,0-shot,accuracy,0.3823529411764705,0.0483550369610722
AbacusResearch/haLLAwa3,mmlu_high_school_physics,0-shot,accuracy,0.3178807947019867,0.038020397601079
AbacusResearch/haLLAwa3,mmlu_high_school_biology,0-shot,accuracy,0.7612903225806451,0.0242510712622088
AbacusResearch/haLLAwa3,mmlu_college_biology,0-shot,accuracy,0.7361111111111112,0.0368565109589753
AbacusResearch/haLLAwa3,mmlu_anatomy,0-shot,accuracy,0.6148148148148148,0.0420392104015627
AbacusResearch/haLLAwa3,mmlu_college_chemistry,0-shot,accuracy,0.46,0.0500908265962033
AbacusResearch/haLLAwa3,mmlu_computer_security,0-shot,accuracy,0.79,0.0409360180740332
AbacusResearch/haLLAwa3,mmlu_college_computer_science,0-shot,accuracy,0.55,0.05
AbacusResearch/haLLAwa3,mmlu_astronomy,0-shot,accuracy,0.6973684210526315,0.0373852067611966
AbacusResearch/haLLAwa3,mmlu_college_mathematics,0-shot,accuracy,0.29,0.0456048021572068
AbacusResearch/haLLAwa3,mmlu_conceptual_physics,0-shot,accuracy,0.574468085106383,0.0323214691622446
AbacusResearch/haLLAwa3,mmlu_abstract_algebra,0-shot,accuracy,0.34,0.0476095228569523
AbacusResearch/haLLAwa3,mmlu_high_school_computer_science,0-shot,accuracy,0.68,0.046882617226215
AbacusResearch/haLLAwa3,mmlu_machine_learning,0-shot,accuracy,0.4553571428571428,0.0472683555371909
AbacusResearch/haLLAwa3,mmlu_high_school_chemistry,0-shot,accuracy,0.5024630541871922,0.0351794503869106
AbacusResearch/haLLAwa3,mmlu_high_school_statistics,0-shot,accuracy,0.5046296296296297,0.0340982551916357
AbacusResearch/haLLAwa3,mmlu_elementary_mathematics,0-shot,accuracy,0.41005291005291,0.0253312024389444
AbacusResearch/haLLAwa3,mmlu_electrical_engineering,0-shot,accuracy,0.5655172413793104,0.0413074087955549
AbacusResearch/haLLAwa3,mmlu_high_school_mathematics,0-shot,accuracy,0.3666666666666666,0.029381620726465
AbacusResearch/haLLAwa3,arc_challenge,25-shot,accuracy,0.6399317406143344,0.0140275168145851
AbacusResearch/haLLAwa3,arc_challenge,25-shot,acc_norm,0.6800341296928327,0.0136313458070161
AbacusResearch/haLLAwa3,hellaswag,10-shot,accuracy,0.7013543118900617,0.0045672877757005
AbacusResearch/haLLAwa3,hellaswag,10-shot,acc_norm,0.8702449711212906,0.0033534696250276
AbacusResearch/haLLAwa3,truthfulqa_mc2,0-shot,accuracy,0.6405912523524916,0.0153074198744009
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,bleu_max,26.60703488440451,0.8059789583751055
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,bleu_acc,0.5458996328029376,0.0174295930913235
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,bleu_diff,8.310410176146629,0.8246277137737857
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge1_max,53.367218868579776,0.8757933437838162
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge1_acc,0.5667074663402693,0.0173470244501074
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge1_diff,11.738773993934805,1.1820094019339218
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge2_max,40.14406041744083,1.0363145440664192
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge2_acc,0.5226438188494492,0.0174855422584896
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge2_diff,11.844606657455826,1.2675724960588215
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rougeL_max,50.376439907152886,0.9112608531974776
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rougeL_acc,0.5471236230110159,0.017425589848314
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rougeL_diff,11.36007163528697,1.1945981073742722
AbacusResearch/haLLAwa3,truthfulqa_mc1,0-shot,accuracy,0.4651162790697674,0.0174608499758739
AbacusResearch/haLLAwa3,winogrande,5-shot,accuracy,0.7916337805840569,0.0114145543999877
openlm-research/open_llama_7b,drop,3-shot,accuracy,0.0008389261744966,0.0002964962989801
openlm-research/open_llama_7b,drop,3-shot,f1,0.0549664429530202,0.0013409914814286
openlm-research/open_llama_7b,arc:challenge,25-shot,accuracy,0.4343003412969283,0.0144847030488573
openlm-research/open_llama_7b,arc:challenge,25-shot,acc_norm,0.470136518771331,0.0145853058400071
openlm-research/open_llama_7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.0460566186471838
openlm-research/open_llama_7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.0460566186471838
openlm-research/open_llama_7b,hendrycksTest-anatomy,5-shot,accuracy,0.3333333333333333,0.0407231481187683
openlm-research/open_llama_7b,hendrycksTest-anatomy,5-shot,acc_norm,0.3333333333333333,0.0407231481187683
openlm-research/open_llama_7b,hendrycksTest-astronomy,5-shot,accuracy,0.2434210526315789,0.0349234966888423
openlm-research/open_llama_7b,hendrycksTest-astronomy,5-shot,acc_norm,0.2434210526315789,0.0349234966888423
openlm-research/open_llama_7b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.047258156262526
openlm-research/open_llama_7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.047258156262526
openlm-research/open_llama_7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3886792452830189,0.0300004854486759
openlm-research/open_llama_7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3886792452830189,0.0300004854486759
openlm-research/open_llama_7b,hendrycksTest-college_biology,5-shot,accuracy,0.3263888888888889,0.0392106719898226
openlm-research/open_llama_7b,hendrycksTest-college_biology,5-shot,acc_norm,0.3263888888888889,0.0392106719898226
openlm-research/open_llama_7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
openlm-research/open_llama_7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
openlm-research/open_llama_7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
openlm-research/open_llama_7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
openlm-research/open_llama_7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.0451260859854212
openlm-research/open_llama_7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.0451260859854212
openlm-research/open_llama_7b,hendrycksTest-college_medicine,5-shot,accuracy,0.3410404624277456,0.0361466542418082
openlm-research/open_llama_7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.3410404624277456,0.0361466542418082
openlm-research/open_llama_7b,hendrycksTest-college_physics,5-shot,accuracy,0.1862745098039215,0.0387395871414935
openlm-research/open_llama_7b,hendrycksTest-college_physics,5-shot,acc_norm,0.1862745098039215,0.0387395871414935
openlm-research/open_llama_7b,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.0487831731214563
openlm-research/open_llama_7b,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.0487831731214563
openlm-research/open_llama_7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3106382978723404,0.0302512375792131
openlm-research/open_llama_7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3106382978723404,0.0302512375792131
openlm-research/open_llama_7b,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.0426633944315939
openlm-research/open_llama_7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.0426633944315939
openlm-research/open_llama_7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3034482758620689,0.0383122604885033
openlm-research/open_llama_7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3034482758620689,0.0383122604885033
openlm-research/open_llama_7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2698412698412698,0.022860838309232
openlm-research/open_llama_7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2698412698412698,0.022860838309232
openlm-research/open_llama_7b,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.0380952380952381
openlm-research/open_llama_7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.0380952380952381
openlm-research/open_llama_7b,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526
openlm-research/open_llama_7b,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526
openlm-research/open_llama_7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.3096774193548387,0.0263027749835174
openlm-research/open_llama_7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3096774193548387,0.0263027749835174
openlm-research/open_llama_7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
openlm-research/open_llama_7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.0305165307326944
openlm-research/open_llama_7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.0456048021572068
openlm-research/open_llama_7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.0456048021572068
openlm-research/open_llama_7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2909090909090909,0.0354656301962433
openlm-research/open_llama_7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2909090909090909,0.0354656301962433
openlm-research/open_llama_7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.3383838383838384,0.033711241426263
openlm-research/open_llama_7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3383838383838384,0.033711241426263
openlm-research/open_llama_7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3575129533678756,0.03458816042181
openlm-research/open_llama_7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3575129533678756,0.03458816042181
openlm-research/open_llama_7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3487179487179487,0.0241627802840177
openlm-research/open_llama_7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3487179487179487,0.0241627802840177
openlm-research/open_llama_7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2407407407407407,0.0260671592222757
openlm-research/open_llama_7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2407407407407407,0.0260671592222757
openlm-research/open_llama_7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2899159663865546,0.029472485833136
openlm-research/open_llama_7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2899159663865546,0.029472485833136
openlm-research/open_llama_7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2516556291390728,0.0354330423438998
openlm-research/open_llama_7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2516556291390728,0.0354330423438998
openlm-research/open_llama_7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3467889908256881,0.020406097104093
openlm-research/open_llama_7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3467889908256881,0.020406097104093
openlm-research/open_llama_7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.375,0.0330169089872108
openlm-research/open_llama_7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.375,0.0330169089872108
openlm-research/open_llama_7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3235294117647059,0.0328347205610856
openlm-research/open_llama_7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3235294117647059,0.0328347205610856
openlm-research/open_llama_7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3248945147679324,0.0304860393891052
openlm-research/open_llama_7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3248945147679324,0.0304860393891052
openlm-research/open_llama_7b,hendrycksTest-human_aging,5-shot,accuracy,0.2869955156950672,0.0303603797102919
openlm-research/open_llama_7b,hendrycksTest-human_aging,5-shot,acc_norm,0.2869955156950672,0.0303603797102919
openlm-research/open_llama_7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2671755725190839,0.0388084830108239
openlm-research/open_llama_7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2671755725190839,0.0388084830108239
openlm-research/open_llama_7b,hendrycksTest-international_law,5-shot,accuracy,0.3636363636363636,0.0439132628672407
openlm-research/open_llama_7b,hendrycksTest-international_law,5-shot,acc_norm,0.3636363636363636,0.0439132628672407
openlm-research/open_llama_7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.3518518518518518,0.0461663111180171
openlm-research/open_llama_7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3518518518518518,0.0461663111180171
openlm-research/open_llama_7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
openlm-research/open_llama_7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.0355903953161734
openlm-research/open_llama_7b,hendrycksTest-machine_learning,5-shot,accuracy,0.25,0.0410997468263393
openlm-research/open_llama_7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.25,0.0410997468263393
openlm-research/open_llama_7b,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.0430125039969087
openlm-research/open_llama_7b,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.0430125039969087
openlm-research/open_llama_7b,hendrycksTest-marketing,5-shot,accuracy,0.3717948717948718,0.0316609889188807
openlm-research/open_llama_7b,hendrycksTest-marketing,5-shot,acc_norm,0.3717948717948718,0.0316609889188807
openlm-research/open_llama_7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.0446196043338474
openlm-research/open_llama_7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.0446196043338474
openlm-research/open_llama_7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.3690932311621966,0.0172562831091246
openlm-research/open_llama_7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3690932311621966,0.0172562831091246
openlm-research/open_llama_7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.3294797687861271,0.0253052581318797
openlm-research/open_llama_7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3294797687861271,0.0253052581318797
openlm-research/open_llama_7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
openlm-research/open_llama_7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2469273743016759,0.0144222922048088
openlm-research/open_llama_7b,hendrycksTest-nutrition,5-shot,accuracy,0.326797385620915,0.0268572946632814
openlm-research/open_llama_7b,hendrycksTest-nutrition,5-shot,acc_norm,0.326797385620915,0.0268572946632814
openlm-research/open_llama_7b,hendrycksTest-philosophy,5-shot,accuracy,0.2861736334405145,0.0256702592421889
openlm-research/open_llama_7b,hendrycksTest-philosophy,5-shot,acc_norm,0.2861736334405145,0.0256702592421889
openlm-research/open_llama_7b,hendrycksTest-prehistory,5-shot,accuracy,0.3117283950617284,0.0257731111696304
openlm-research/open_llama_7b,hendrycksTest-prehistory,5-shot,acc_norm,0.3117283950617284,0.0257731111696304
openlm-research/open_llama_7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
openlm-research/open_llama_7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
openlm-research/open_llama_7b,hendrycksTest-professional_law,5-shot,accuracy,0.2522816166883963,0.0110927890568752
openlm-research/open_llama_7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2522816166883963,0.0110927890568752
openlm-research/open_llama_7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.2389705882352941,0.025905280644893
openlm-research/open_llama_7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2389705882352941,0.025905280644893
openlm-research/open_llama_7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2696078431372549,0.0179524491969878
openlm-research/open_llama_7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2696078431372549,0.0179524491969878
openlm-research/open_llama_7b,hendrycksTest-public_relations,5-shot,accuracy,0.4181818181818181,0.0472457740573157
openlm-research/open_llama_7b,hendrycksTest-public_relations,5-shot,acc_norm,0.4181818181818181,0.0472457740573157
openlm-research/open_llama_7b,hendrycksTest-security_studies,5-shot,accuracy,0.2448979591836734,0.0275296374401749
openlm-research/open_llama_7b,hendrycksTest-security_studies,5-shot,acc_norm,0.2448979591836734,0.0275296374401749
openlm-research/open_llama_7b,hendrycksTest-sociology,5-shot,accuracy,0.2487562189054726,0.0305676759389167
openlm-research/open_llama_7b,hendrycksTest-sociology,5-shot,acc_norm,0.2487562189054726,0.0305676759389167
openlm-research/open_llama_7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.39,0.0490207130000197
openlm-research/open_llama_7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.39,0.0490207130000197
openlm-research/open_llama_7b,hendrycksTest-virology,5-shot,accuracy,0.3493975903614458,0.0371172519074075
openlm-research/open_llama_7b,hendrycksTest-virology,5-shot,acc_norm,0.3493975903614458,0.0371172519074075
openlm-research/open_llama_7b,hendrycksTest-world_religions,5-shot,accuracy,0.391812865497076,0.037439798259264
openlm-research/open_llama_7b,hendrycksTest-world_religions,5-shot,acc_norm,0.391812865497076,0.037439798259264
openlm-research/open_llama_7b,truthfulqa:mc,0-shot,mc1,0.2350061199510404,0.0148430615077316
openlm-research/open_llama_7b,truthfulqa:mc,0-shot,mc2,0.3484730707265278,0.0135501017526561
llama2_220M_nl_100_code_0,minerva_math_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_geometry,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_num_theory,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_prealgebra,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_precalc,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,gsm8k,5-shot,accuracy,0.0136467020470053,0.0031957470754808
llama2_220M_nl_100_code_0,gsm8k_cot,5-shot,accuracy,0.0197119029567854,0.0038289829787357
llama2_220M_nl_100_code_0,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_2ds,5-shot,accuracy,0.0245,0.0034577236625362
llama2_220M_nl_100_code_0,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
llama2_220M_nl_100_code_0,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
llama2_220M_nl_100_code_0,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_2dm,5-shot,accuracy,0.0225,0.0033169829948455
llama2_220M_nl_100_code_0,arithmetic_1dc,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646
llama2_220M_nl_100_code_0,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
llama2_220M_nl_100_code_0,xnli_ar,0-shot,brier_score,0.9134361549440044,
llama2_220M_nl_100_code_0,xnli_bg,0-shot,brier_score,1.1059027504585148,
llama2_220M_nl_100_code_0,xnli_de,0-shot,brier_score,0.9182664656781208,
llama2_220M_nl_100_code_0,xnli_el,0-shot,brier_score,1.014666443682828,
llama2_220M_nl_100_code_0,xnli_en,0-shot,brier_score,0.7165642552011161,
llama2_220M_nl_100_code_0,xnli_es,0-shot,brier_score,1.068453256738231,
llama2_220M_nl_100_code_0,xnli_fr,0-shot,brier_score,0.9966650739305976,
llama2_220M_nl_100_code_0,xnli_hi,0-shot,brier_score,1.085659903781425,
llama2_220M_nl_100_code_0,xnli_ru,0-shot,brier_score,0.8896788290214876,
llama2_220M_nl_100_code_0,xnli_sw,0-shot,brier_score,0.94306595191456,
llama2_220M_nl_100_code_0,xnli_th,0-shot,brier_score,0.925220386341976,
llama2_220M_nl_100_code_0,xnli_tr,0-shot,brier_score,1.1127636740614657,
llama2_220M_nl_100_code_0,xnli_ur,0-shot,brier_score,1.319010883603471,
llama2_220M_nl_100_code_0,xnli_vi,0-shot,brier_score,1.0261665968416116,
llama2_220M_nl_100_code_0,xnli_zh,0-shot,brier_score,1.311822001659871,
llama2_220M_nl_100_code_0,anli_r1,0-shot,brier_score,0.8449496978059758,
llama2_220M_nl_100_code_0,anli_r3,0-shot,brier_score,0.857186090421117,
llama2_220M_nl_100_code_0,anli_r2,0-shot,brier_score,0.8635985195897238,
llama2_220M_nl_100_code_0,mathqa,0-shot,brier_score,1.0389827270143788,
llama2_220M_nl_100_code_0,logiqa2,0-shot,brier_score,1.172976871272462,
llama2_220M_nl_100_code_0,lambada_standard,0-shot,perplexity,111.71321527976326,4.383054748380545
llama2_220M_nl_100_code_0,lambada_standard,0-shot,accuracy,0.2154084999029691,0.0057275025395164
llama2_220M_nl_100_code_0,lambada_openai,0-shot,perplexity,47.444161536621785,1.7285309338117685
llama2_220M_nl_100_code_0,lambada_openai,0-shot,accuracy,0.2980787890549194,0.0063726753246935
facebook/xglm-7.5B,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_prealgebra,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_algebra,5-shot,accuracy,0.0,
facebook/xglm-7.5B,fld_default,0-shot,accuracy,0.0,
facebook/xglm-7.5B,fld_star,0-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_3da,5-shot,accuracy,0.0005,0.0005
facebook/xglm-7.5B,arithmetic_3ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
facebook/xglm-7.5B,arithmetic_2ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_1dc,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_2dm,5-shot,accuracy,0.0045,0.0014969954902233
facebook/xglm-7.5B,arithmetic_2da,5-shot,accuracy,0.0015,0.0008655920660521
facebook/xglm-7.5B,gsm8k_cot,5-shot,accuracy,0.00303260045489,0.0015145735612245
facebook/xglm-7.5B,gsm8k,5-shot,accuracy,0.0022744503411675,0.0013121578148674
facebook/xglm-7.5B,anli_r2,0-shot,brier_score,0.7252401406403721,
facebook/xglm-7.5B,anli_r3,0-shot,brier_score,0.7345576777518716,
facebook/xglm-7.5B,anli_r1,0-shot,brier_score,0.7403058522962276,
facebook/xglm-7.5B,xnli_eu,0-shot,brier_score,0.7249978471618856,
facebook/xglm-7.5B,xnli_vi,0-shot,brier_score,0.7449262360122225,
facebook/xglm-7.5B,xnli_ru,0-shot,brier_score,0.776968330898939,
facebook/xglm-7.5B,xnli_zh,0-shot,brier_score,1.0503581196137082,
facebook/xglm-7.5B,xnli_tr,0-shot,brier_score,0.7725432543659072,
facebook/xglm-7.5B,xnli_fr,0-shot,brier_score,0.8140567425559275,
facebook/xglm-7.5B,xnli_en,0-shot,brier_score,0.6517840395340417,
facebook/xglm-7.5B,xnli_ur,0-shot,brier_score,0.8831297423089415,
facebook/xglm-7.5B,xnli_ar,0-shot,brier_score,1.2554211575716625,
facebook/xglm-7.5B,xnli_de,0-shot,brier_score,0.7983353916849525,
facebook/xglm-7.5B,xnli_hi,0-shot,brier_score,0.7335594240977125,
facebook/xglm-7.5B,xnli_es,0-shot,brier_score,0.8237671750458347,
facebook/xglm-7.5B,xnli_bg,0-shot,brier_score,0.8170940878498713,
facebook/xglm-7.5B,xnli_sw,0-shot,brier_score,0.7365080187868047,
facebook/xglm-7.5B,xnli_el,0-shot,brier_score,0.8709162316623921,
facebook/xglm-7.5B,xnli_th,0-shot,brier_score,0.8133822575764703,
facebook/xglm-7.5B,logiqa2,0-shot,brier_score,1.0735932488634556,
facebook/xglm-7.5B,mathqa,0-shot,brier_score,0.988707059624341,
facebook/xglm-7.5B,lambada_standard,0-shot,perplexity,8.331266240940502,0.2216916155919944
facebook/xglm-7.5B,lambada_standard,0-shot,accuracy,0.5460896565107705,0.0069363194754447
facebook/xglm-7.5B,lambada_openai,0-shot,perplexity,7.454260315964878,0.1983394109813953
facebook/xglm-7.5B,lambada_openai,0-shot,accuracy,0.5561808655152338,0.0069218646952863
facebook/xglm-7.5B,mmlu_world_religions,0-shot,accuracy,0.2748538011695906,0.0342404292469158
facebook/xglm-7.5B,mmlu_formal_logic,0-shot,accuracy,0.3174603174603174,0.0416345303130285
facebook/xglm-7.5B,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.0247486244905373
facebook/xglm-7.5B,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/xglm-7.5B,mmlu_high_school_world_history,0-shot,accuracy,0.2531645569620253,0.0283046579430352
facebook/xglm-7.5B,mmlu_moral_disputes,0-shot,accuracy,0.2312138728323699,0.0226986571678557
facebook/xglm-7.5B,mmlu_professional_law,0-shot,accuracy,0.2438070404172099,0.0109665079721784
facebook/xglm-7.5B,mmlu_logical_fallacies,0-shot,accuracy,0.2515337423312883,0.0340899788685752
facebook/xglm-7.5B,mmlu_high_school_us_history,0-shot,accuracy,0.2352941176470588,0.0297717752281456
facebook/xglm-7.5B,mmlu_philosophy,0-shot,accuracy,0.2315112540192926,0.0239565327666391
facebook/xglm-7.5B,mmlu_jurisprudence,0-shot,accuracy,0.2129629629629629,0.0395783547198097
facebook/xglm-7.5B,mmlu_international_law,0-shot,accuracy,0.3471074380165289,0.0434572457029253
facebook/xglm-7.5B,mmlu_high_school_european_history,0-shot,accuracy,0.2848484848484848,0.0352439084451178
facebook/xglm-7.5B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.0346971379170437
facebook/xglm-7.5B,mmlu_high_school_microeconomics,0-shot,accuracy,0.3319327731092437,0.0305886970137836
facebook/xglm-7.5B,mmlu_high_school_geography,0-shot,accuracy,0.3636363636363636,0.0342730865299993
facebook/xglm-7.5B,mmlu_high_school_psychology,0-shot,accuracy,0.2623853211009174,0.0188618850215347
facebook/xglm-7.5B,mmlu_public_relations,0-shot,accuracy,0.2454545454545454,0.0412206650287828
facebook/xglm-7.5B,mmlu_us_foreign_policy,0-shot,accuracy,0.22,0.0416333199893226
facebook/xglm-7.5B,mmlu_sociology,0-shot,accuracy,0.2388059701492537,0.0301477759354092
facebook/xglm-7.5B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3564102564102564,0.0242831405294673
facebook/xglm-7.5B,mmlu_security_studies,0-shot,accuracy,0.2857142857142857,0.0289205832206756
facebook/xglm-7.5B,mmlu_professional_psychology,0-shot,accuracy,0.2647058823529412,0.0178480895749132
facebook/xglm-7.5B,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.0372767357559691
facebook/xglm-7.5B,mmlu_econometrics,0-shot,accuracy,0.219298245614035,0.0389243110651875
facebook/xglm-7.5B,mmlu_miscellaneous,0-shot,accuracy,0.2605363984674329,0.015696008563807
facebook/xglm-7.5B,mmlu_marketing,0-shot,accuracy,0.188034188034188,0.0255981936866522
facebook/xglm-7.5B,mmlu_management,0-shot,accuracy,0.2815533980582524,0.0445325483632646
facebook/xglm-7.5B,mmlu_nutrition,0-shot,accuracy,0.2777777777777778,0.0256468630971379
facebook/xglm-7.5B,mmlu_medical_genetics,0-shot,accuracy,0.31,0.0464823198711731
facebook/xglm-7.5B,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.0314938467099413
facebook/xglm-7.5B,mmlu_professional_medicine,0-shot,accuracy,0.4522058823529412,0.0302337585515964
facebook/xglm-7.5B,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/xglm-7.5B,mmlu_business_ethics,0-shot,accuracy,0.25,0.0435194139889244
facebook/xglm-7.5B,mmlu_clinical_knowledge,0-shot,accuracy,0.3018867924528302,0.0282542003444386
facebook/xglm-7.5B,mmlu_global_facts,0-shot,accuracy,0.3,0.0460566186471838
facebook/xglm-7.5B,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.0352948680151111
facebook/xglm-7.5B,mmlu_professional_accounting,0-shot,accuracy,0.2588652482269503,0.0261295725271808
facebook/xglm-7.5B,mmlu_college_physics,0-shot,accuracy,0.196078431372549,0.0395058186117996
facebook/xglm-7.5B,mmlu_high_school_physics,0-shot,accuracy,0.304635761589404,0.0375794992294334
facebook/xglm-7.5B,mmlu_high_school_biology,0-shot,accuracy,0.332258064516129,0.0267955608481227
facebook/xglm-7.5B,mmlu_college_biology,0-shot,accuracy,0.2916666666666667,0.0380096806055485
facebook/xglm-7.5B,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.0374985070917402
facebook/xglm-7.5B,mmlu_college_chemistry,0-shot,accuracy,0.27,0.0446196043338473
facebook/xglm-7.5B,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-7.5B,mmlu_college_computer_science,0-shot,accuracy,0.31,0.0464823198711731
facebook/xglm-7.5B,mmlu_astronomy,0-shot,accuracy,0.2171052631578947,0.0335504530488292
facebook/xglm-7.5B,mmlu_college_mathematics,0-shot,accuracy,0.25,0.0435194139889244
facebook/xglm-7.5B,mmlu_conceptual_physics,0-shot,accuracy,0.2808510638297872,0.0293791704641248
facebook/xglm-7.5B,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.0435194139889244
facebook/xglm-7.5B,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.0456048021572068
facebook/xglm-7.5B,mmlu_machine_learning,0-shot,accuracy,0.2321428571428571,0.040073418097558
facebook/xglm-7.5B,mmlu_high_school_chemistry,0-shot,accuracy,0.3004926108374384,0.0322579947623348
facebook/xglm-7.5B,mmlu_high_school_statistics,0-shot,accuracy,0.4212962962962963,0.0336746213889607
facebook/xglm-7.5B,mmlu_elementary_mathematics,0-shot,accuracy,0.2671957671957672,0.0227896731457765
facebook/xglm-7.5B,mmlu_electrical_engineering,0-shot,accuracy,0.2137931034482758,0.0341652044774754
facebook/xglm-7.5B,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
facebook/xglm-7.5B,arc_challenge,25-shot,accuracy,0.310580204778157,0.013522292098053
facebook/xglm-7.5B,arc_challenge,25-shot,acc_norm,0.3387372013651877,0.0138305689279743
facebook/xglm-7.5B,hellaswag,10-shot,accuracy,0.4496116311491734,0.0049643787624252
facebook/xglm-7.5B,hellaswag,10-shot,acc_norm,0.6077474606652061,0.0048725463026418
facebook/xglm-7.5B,truthfulqa_mc2,0-shot,accuracy,0.3638299521711414,0.0136498235022498
facebook/xglm-7.5B,truthfulqa_gen,0-shot,bleu_max,6.098360177088277,0.3118236431976185
facebook/xglm-7.5B,truthfulqa_gen,0-shot,bleu_acc,0.2558139534883721,0.0152741762192833
facebook/xglm-7.5B,truthfulqa_gen,0-shot,bleu_diff,-2.2599605981406405,0.2506337182789708
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge1_max,21.77941671530608,0.5244944853770919
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge1_acc,0.3072215422276622,0.016150201321323
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge1_diff,-3.390869478818047,0.3904300399631195
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge2_max,11.203254328402076,0.4899639367721538
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge2_acc,0.1848225214198286,0.0135880911760495
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge2_diff,-4.243657550945619,0.4037168891317911
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rougeL_max,19.71659064835585,0.5021969423588765
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rougeL_acc,0.2864137086903305,0.0158261424395023
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rougeL_diff,-3.591440069116983,0.3705952465315688
facebook/xglm-7.5B,truthfulqa_mc1,0-shot,accuracy,0.2068543451652386,0.0141795914967283
facebook/xglm-7.5B,winogrande,5-shot,accuracy,0.5872138910812944,0.013837060648682
EleutherAI/pythia-12b-deduped,drop,3-shot,accuracy,0.0008389261744966,0.0002964962989801
EleutherAI/pythia-12b-deduped,drop,3-shot,f1,0.0454823825503357,0.0011460514648967
EleutherAI/pythia-12b-deduped,gsm8k,5-shot,accuracy,0.0144048521607278,0.0032820559171369
EleutherAI/pythia-12b-deduped,winogrande,5-shot,accuracy,0.664561957379637,0.0132695759048514
EleutherAI/pythia-12b-deduped,arc:challenge,25-shot,accuracy,0.378839590443686,0.0141759154900003
EleutherAI/pythia-12b-deduped,arc:challenge,25-shot,acc_norm,0.4138225255972696,0.014392730009221
EleutherAI/pythia-12b-deduped,hellaswag,10-shot,accuracy,0.5183230432184823,0.0049864298081467
EleutherAI/pythia-12b-deduped,hellaswag,10-shot,acc_norm,0.7026488747261501,0.0045615820098345
EleutherAI/pythia-12b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-12b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/pythia-12b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.2296296296296296,0.0363338441407346
EleutherAI/pythia-12b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.2296296296296296,0.0363338441407346
EleutherAI/pythia-12b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.0358349617636106
EleutherAI/pythia-12b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.0358349617636106
EleutherAI/pythia-12b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-12b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-12b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.260377358490566,0.0270087660907081
EleutherAI/pythia-12b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.260377358490566,0.0270087660907081
EleutherAI/pythia-12b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
EleutherAI/pythia-12b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.0368565109589753
EleutherAI/pythia-12b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.19,0.0394277244403662
EleutherAI/pythia-12b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.19,0.0394277244403662
EleutherAI/pythia-12b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.22,0.0416333199893227
EleutherAI/pythia-12b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.22,0.0416333199893227
EleutherAI/pythia-12b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-12b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-12b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2254335260115607,0.0318620985164114
EleutherAI/pythia-12b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2254335260115607,0.0318620985164114
EleutherAI/pythia-12b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.1862745098039215,0.0387395871414935
EleutherAI/pythia-12b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.1862745098039215,0.0387395871414935
EleutherAI/pythia-12b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-12b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-12b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.0289573427883423
EleutherAI/pythia-12b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.0289573427883423
EleutherAI/pythia-12b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948935
EleutherAI/pythia-12b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948935
EleutherAI/pythia-12b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2275862068965517,0.0349395038013118
EleutherAI/pythia-12b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2275862068965517,0.0349395038013118
EleutherAI/pythia-12b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2645502645502645,0.0227174678977086
EleutherAI/pythia-12b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2645502645502645,0.0227174678977086
EleutherAI/pythia-12b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.1904761904761904,0.0351220741230205
EleutherAI/pythia-12b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.1904761904761904,0.0351220741230205
EleutherAI/pythia-12b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-12b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.2387096774193548,0.0242510712622088
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2387096774193548,0.0242510712622088
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2660098522167488,0.0310898260029375
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2660098522167488,0.0310898260029375
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.0296202278747904
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.0296202278747904
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2072538860103626,0.0292528232918036
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2072538860103626,0.0292528232918036
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.258974358974359,0.0222111068100616
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.258974358974359,0.0222111068100616
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2740740740740741,0.0271959348040856
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2740740740740741,0.0271959348040856
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2815126050420168,0.0292135494143721
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2815126050420168,0.0292135494143721
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.2251655629139073,0.0341043528200893
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2251655629139073,0.0341043528200893
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2036697247706422,0.0172667420876307
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2036697247706422,0.0172667420876307
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1666666666666666,0.0254164283887674
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1666666666666666,0.0254164283887674
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2450980392156862,0.0301902824535019
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2450980392156862,0.0301902824535019
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2742616033755274,0.029041333510598
EleutherAI/pythia-12b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.3677130044843049,0.0323619835092827
EleutherAI/pythia-12b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.3677130044843049,0.0323619835092827
EleutherAI/pythia-12b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/pythia-12b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
EleutherAI/pythia-12b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.4628099173553719,0.0455171119610421
EleutherAI/pythia-12b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.4628099173553719,0.0455171119610421
EleutherAI/pythia-12b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.0441434366685493
EleutherAI/pythia-12b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.0441434366685493
EleutherAI/pythia-12b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2331288343558282,0.0332201579577674
EleutherAI/pythia-12b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2331288343558282,0.0332201579577674
EleutherAI/pythia-12b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.25,0.0410997468263393
EleutherAI/pythia-12b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.25,0.0410997468263393
EleutherAI/pythia-12b-deduped,hendrycksTest-management,5-shot,accuracy,0.2233009708737864,0.0412355318989143
EleutherAI/pythia-12b-deduped,hendrycksTest-management,5-shot,acc_norm,0.2233009708737864,0.0412355318989143
EleutherAI/pythia-12b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2649572649572649,0.0289112088027494
EleutherAI/pythia-12b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2649572649572649,0.0289112088027494
EleutherAI/pythia-12b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-12b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.21,0.0409360180740332
EleutherAI/pythia-12b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2924648786717752,0.0162670006845986
EleutherAI/pythia-12b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2924648786717752,0.0162670006845986
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.3092485549132948,0.0248831405700717
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3092485549132948,0.0248831405700717
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.0143559119647678
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.0143559119647678
EleutherAI/pythia-12b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2973856209150327,0.0261739085067185
EleutherAI/pythia-12b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2973856209150327,0.0261739085067185
EleutherAI/pythia-12b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.270096463022508,0.0252180403734106
EleutherAI/pythia-12b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.270096463022508,0.0252180403734106
EleutherAI/pythia-12b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2592592592592592,0.0243836655310354
EleutherAI/pythia-12b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2592592592592592,0.0243836655310354
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2482269503546099,0.0257700156442903
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2614080834419817,0.0112225281697713
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2614080834419817,0.0112225281697713
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.1985294117647058,0.0242310133705411
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1985294117647058,0.0242310133705411
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2924836601307189,0.0184034157101097
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2924836601307189,0.0184034157101097
EleutherAI/pythia-12b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
EleutherAI/pythia-12b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.0417234303870538
EleutherAI/pythia-12b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.2530612244897959,0.0278330238713996
EleutherAI/pythia-12b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.2530612244897959,0.0278330238713996
EleutherAI/pythia-12b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2686567164179104,0.0313432835820895
EleutherAI/pythia-12b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2686567164179104,0.0313432835820895
EleutherAI/pythia-12b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-12b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.0409360180740332
EleutherAI/pythia-12b-deduped,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.0362933532994786
EleutherAI/pythia-12b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.0362933532994786
EleutherAI/pythia-12b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.3508771929824561,0.0366029883404916
EleutherAI/pythia-12b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.3508771929824561,0.0366029883404916
EleutherAI/pythia-12b-deduped,truthfulqa:mc,0-shot,mc1,0.2093023255813953,0.0142412194347858
EleutherAI/pythia-12b-deduped,truthfulqa:mc,0-shot,mc2,0.329988729051366,0.0131220986038542
facebook/opt-350m,arc:challenge,25-shot,accuracy,0.2056313993174061,0.0118107452607425
facebook/opt-350m,arc:challenge,25-shot,acc_norm,0.2354948805460751,0.0123994518550047
facebook/opt-350m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.0416333199893226
facebook/opt-350m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.0416333199893226
facebook/opt-350m,hendrycksTest-anatomy,5-shot,accuracy,0.2740740740740741,0.03853254836552
facebook/opt-350m,hendrycksTest-anatomy,5-shot,acc_norm,0.2740740740740741,0.03853254836552
facebook/opt-350m,hendrycksTest-astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
facebook/opt-350m,hendrycksTest-astronomy,5-shot,acc_norm,0.1776315789473684,0.0311031823831233
facebook/opt-350m,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.0409360180740332
facebook/opt-350m,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.0409360180740332
facebook/opt-350m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.0272572603224948
facebook/opt-350m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.0272572603224948
facebook/opt-350m,hendrycksTest-college_biology,5-shot,accuracy,0.2291666666666666,0.0351469746786238
facebook/opt-350m,hendrycksTest-college_biology,5-shot,acc_norm,0.2291666666666666,0.0351469746786238
facebook/opt-350m,hendrycksTest-college_chemistry,5-shot,accuracy,0.35,0.0479372485441102
facebook/opt-350m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.35,0.0479372485441102
facebook/opt-350m,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.0460566186471838
facebook/opt-350m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.0460566186471838
facebook/opt-350m,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338473
facebook/opt-350m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338473
facebook/opt-350m,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/opt-350m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
facebook/opt-350m,hendrycksTest-college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
facebook/opt-350m,hendrycksTest-college_physics,5-shot,acc_norm,0.2254901960784313,0.0415830753308328
facebook/opt-350m,hendrycksTest-computer_security,5-shot,accuracy,0.18,0.0386122919665369
facebook/opt-350m,hendrycksTest-computer_security,5-shot,acc_norm,0.18,0.0386122919665369
facebook/opt-350m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2723404255319149,0.0291012906983867
facebook/opt-350m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2723404255319149,0.0291012906983867
facebook/opt-350m,hendrycksTest-econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
facebook/opt-350m,hendrycksTest-econometrics,5-shot,acc_norm,0.2368421052631578,0.0399942387928133
facebook/opt-350m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.0380614268730999
facebook/opt-350m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.0380614268730999
facebook/opt-350m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.0224945107675031
facebook/opt-350m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.0224945107675031
facebook/opt-350m,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.0380952380952381
facebook/opt-350m,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.0380952380952381
facebook/opt-350m,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.0394277244403662
facebook/opt-350m,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.0394277244403662
facebook/opt-350m,hendrycksTest-high_school_biology,5-shot,accuracy,0.3,0.0260693622953351
facebook/opt-350m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3,0.0260693622953351
facebook/opt-350m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2906403940886699,0.0319474007226553
facebook/opt-350m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2906403940886699,0.0319474007226553
facebook/opt-350m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.0394277244403662
facebook/opt-350m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.0394277244403662
facebook/opt-350m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2666666666666666,0.0345313180188541
facebook/opt-350m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2666666666666666,0.0345313180188541
facebook/opt-350m,hendrycksTest-high_school_geography,5-shot,accuracy,0.3585858585858585,0.0341690364039152
facebook/opt-350m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3585858585858585,0.0341690364039152
facebook/opt-350m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3471502590673575,0.0343569616836135
facebook/opt-350m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3471502590673575,0.0343569616836135
facebook/opt-350m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2743589743589744,0.0226227657674932
facebook/opt-350m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2743589743589744,0.0226227657674932
facebook/opt-350m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2777777777777778,0.0273091405882301
facebook/opt-350m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2777777777777778,0.0273091405882301
facebook/opt-350m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2731092436974789,0.0289420040409981
facebook/opt-350m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2731092436974789,0.0289420040409981
facebook/opt-350m,hendrycksTest-high_school_physics,5-shot,accuracy,0.3311258278145695,0.0384258171865986
facebook/opt-350m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3311258278145695,0.0384258171865986
facebook/opt-350m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.326605504587156,0.0201069908899373
facebook/opt-350m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.326605504587156,0.0201069908899373
facebook/opt-350m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-350m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
facebook/opt-350m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
facebook/opt-350m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2598039215686274,0.0307785546786932
facebook/opt-350m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2067510548523206,0.0263616516683891
facebook/opt-350m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2067510548523206,0.0263616516683891
facebook/opt-350m,hendrycksTest-human_aging,5-shot,accuracy,0.1345291479820627,0.0229011837615755
facebook/opt-350m,hendrycksTest-human_aging,5-shot,acc_norm,0.1345291479820627,0.0229011837615755
facebook/opt-350m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
facebook/opt-350m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
facebook/opt-350m,hendrycksTest-international_law,5-shot,accuracy,0.371900826446281,0.044120158066245
facebook/opt-350m,hendrycksTest-international_law,5-shot,acc_norm,0.371900826446281,0.044120158066245
facebook/opt-350m,hendrycksTest-jurisprudence,5-shot,accuracy,0.2222222222222222,0.0401910747255734
facebook/opt-350m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2222222222222222,0.0401910747255734
facebook/opt-350m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2453987730061349,0.0338093981394335
facebook/opt-350m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2453987730061349,0.0338093981394335
facebook/opt-350m,hendrycksTest-machine_learning,5-shot,accuracy,0.1785714285714285,0.036352091215778
facebook/opt-350m,hendrycksTest-machine_learning,5-shot,acc_norm,0.1785714285714285,0.036352091215778
facebook/opt-350m,hendrycksTest-management,5-shot,accuracy,0.2233009708737864,0.0412355318989143
facebook/opt-350m,hendrycksTest-management,5-shot,acc_norm,0.2233009708737864,0.0412355318989143
facebook/opt-350m,hendrycksTest-marketing,5-shot,accuracy,0.2136752136752136,0.0268534503770091
facebook/opt-350m,hendrycksTest-marketing,5-shot,acc_norm,0.2136752136752136,0.0268534503770091
facebook/opt-350m,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.0456048021572068
facebook/opt-350m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.0456048021572068
facebook/opt-350m,hendrycksTest-miscellaneous,5-shot,accuracy,0.2056194125159642,0.0144525004567858
facebook/opt-350m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2056194125159642,0.0144525004567858
facebook/opt-350m,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
facebook/opt-350m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
facebook/opt-350m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
facebook/opt-350m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2469273743016759,0.0144222922048088
facebook/opt-350m,hendrycksTest-nutrition,5-shot,accuracy,0.261437908496732,0.0251609982142924
facebook/opt-350m,hendrycksTest-nutrition,5-shot,acc_norm,0.261437908496732,0.0251609982142924
facebook/opt-350m,hendrycksTest-philosophy,5-shot,accuracy,0.1961414790996784,0.022552447780478
facebook/opt-350m,hendrycksTest-philosophy,5-shot,acc_norm,0.1961414790996784,0.022552447780478
facebook/opt-350m,hendrycksTest-prehistory,5-shot,accuracy,0.2716049382716049,0.0247486244905373
facebook/opt-350m,hendrycksTest-prehistory,5-shot,acc_norm,0.2716049382716049,0.0247486244905373
facebook/opt-350m,hendrycksTest-professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
facebook/opt-350m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2482269503546099,0.0257700156442903
facebook/opt-350m,hendrycksTest-professional_law,5-shot,accuracy,0.241199478487614,0.0109264961020349
facebook/opt-350m,hendrycksTest-professional_law,5-shot,acc_norm,0.241199478487614,0.0109264961020349
facebook/opt-350m,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.0302114796091215
facebook/opt-350m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.0302114796091215
facebook/opt-350m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2418300653594771,0.0173227892077843
facebook/opt-350m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2418300653594771,0.0173227892077843
facebook/opt-350m,hendrycksTest-public_relations,5-shot,accuracy,0.1909090909090909,0.0376442558598492
facebook/opt-350m,hendrycksTest-public_relations,5-shot,acc_norm,0.1909090909090909,0.0376442558598492
facebook/opt-350m,hendrycksTest-security_studies,5-shot,accuracy,0.3959183673469387,0.0313080289906568
facebook/opt-350m,hendrycksTest-security_studies,5-shot,acc_norm,0.3959183673469387,0.0313080289906568
facebook/opt-350m,hendrycksTest-sociology,5-shot,accuracy,0.2189054726368159,0.029239174636647
facebook/opt-350m,hendrycksTest-sociology,5-shot,acc_norm,0.2189054726368159,0.029239174636647
facebook/opt-350m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276807
facebook/opt-350m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.0440844002276807
facebook/opt-350m,hendrycksTest-virology,5-shot,accuracy,0.1927710843373494,0.0307098240505652
facebook/opt-350m,hendrycksTest-virology,5-shot,acc_norm,0.1927710843373494,0.0307098240505652
facebook/opt-350m,hendrycksTest-world_religions,5-shot,accuracy,0.1871345029239766,0.029913127232368
facebook/opt-350m,hendrycksTest-world_religions,5-shot,acc_norm,0.1871345029239766,0.029913127232368
facebook/opt-350m,truthfulqa:mc,0-shot,mc1,0.2325581395348837,0.0147891575310805
facebook/opt-350m,truthfulqa:mc,0-shot,mc2,0.4082853277784469,0.0146700170810973
facebook/opt-350m,drop,3-shot,accuracy,0.0006291946308724,0.0002568002749723
facebook/opt-350m,drop,3-shot,f1,0.0415981543624162,0.0011509154641292
aisingapore/sea-lion-7b,minerva_math_precalc,5-shot,accuracy,0.0183150183150183,0.0057436967316536
aisingapore/sea-lion-7b,minerva_math_prealgebra,5-shot,accuracy,0.026406429391504,0.0054360577625739
aisingapore/sea-lion-7b,minerva_math_num_theory,5-shot,accuracy,0.0185185185185185,0.0058069728079122
aisingapore/sea-lion-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0254706533776301,0.0052458302725593
aisingapore/sea-lion-7b,minerva_math_geometry,5-shot,accuracy,0.0167014613778705,0.005861462425818
aisingapore/sea-lion-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.0084388185654008,0.004206007207713
aisingapore/sea-lion-7b,minerva_math_algebra,5-shot,accuracy,0.0202190395956192,0.0040869790805184
aisingapore/sea-lion-7b,fld_default,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b,fld_star,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b,arithmetic_3da,5-shot,accuracy,0.0055,0.0016541593398342
aisingapore/sea-lion-7b,arithmetic_3ds,5-shot,accuracy,0.017,0.0028913110935905
aisingapore/sea-lion-7b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
aisingapore/sea-lion-7b,arithmetic_2ds,5-shot,accuracy,0.1115,0.0070397907871727
aisingapore/sea-lion-7b,arithmetic_5ds,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b,arithmetic_5da,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b,arithmetic_1dc,5-shot,accuracy,0.068,0.0056306173663253
aisingapore/sea-lion-7b,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005
aisingapore/sea-lion-7b,arithmetic_2dm,5-shot,accuracy,0.0935,0.006511534000335
aisingapore/sea-lion-7b,arithmetic_2da,5-shot,accuracy,0.087,0.0063035995814963
aisingapore/sea-lion-7b,gsm8k_cot,5-shot,accuracy,0.0333586050037907,0.0049462826491737
aisingapore/sea-lion-7b,gsm8k,5-shot,accuracy,0.0257771038665655,0.0043650429536218
aisingapore/sea-lion-7b,anli_r2,0-shot,brier_score,0.7371776965553825,
aisingapore/sea-lion-7b,anli_r3,0-shot,brier_score,0.7142007390796328,
aisingapore/sea-lion-7b,anli_r1,0-shot,brier_score,0.7412739329209331,
aisingapore/sea-lion-7b,xnli_eu,0-shot,brier_score,1.113847354553935,
aisingapore/sea-lion-7b,xnli_vi,0-shot,brier_score,0.7514185268043483,
aisingapore/sea-lion-7b,xnli_ru,0-shot,brier_score,0.8063321258911998,
aisingapore/sea-lion-7b,xnli_zh,0-shot,brier_score,1.0744744185180417,
aisingapore/sea-lion-7b,xnli_tr,0-shot,brier_score,0.8904322785337762,
aisingapore/sea-lion-7b,xnli_fr,0-shot,brier_score,0.7973751206359644,
aisingapore/sea-lion-7b,xnli_en,0-shot,brier_score,0.6864404290885256,
aisingapore/sea-lion-7b,xnli_ur,0-shot,brier_score,1.3105023198255918,
aisingapore/sea-lion-7b,xnli_ar,0-shot,brier_score,0.8566459473172638,
aisingapore/sea-lion-7b,xnli_de,0-shot,brier_score,0.8048918665563971,
aisingapore/sea-lion-7b,xnli_hi,0-shot,brier_score,1.0018933981610276,
aisingapore/sea-lion-7b,xnli_es,0-shot,brier_score,0.9309039010456585,
aisingapore/sea-lion-7b,xnli_bg,0-shot,brier_score,0.7926756354946493,
aisingapore/sea-lion-7b,xnli_sw,0-shot,brier_score,0.9224405259782638,
aisingapore/sea-lion-7b,xnli_el,0-shot,brier_score,1.1955451021051349,
aisingapore/sea-lion-7b,xnli_th,0-shot,brier_score,0.7636640796291084,
aisingapore/sea-lion-7b,logiqa2,0-shot,brier_score,1.11082674926113,
aisingapore/sea-lion-7b,mathqa,0-shot,brier_score,0.9547448253077948,
aisingapore/sea-lion-7b,lambada_standard,0-shot,perplexity,6.716223440457293,0.171380584266027
aisingapore/sea-lion-7b,lambada_standard,0-shot,accuracy,0.5767514069474092,0.0068834183570844
aisingapore/sea-lion-7b,lambada_openai,0-shot,perplexity,5.278455783743519,0.1252746999189692
aisingapore/sea-lion-7b,lambada_openai,0-shot,accuracy,0.638074907820687,0.0066951028476774
aisingapore/sea-lion-7b,mmlu_world_religions,0-shot,accuracy,0.3099415204678362,0.0354697695939316
aisingapore/sea-lion-7b,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.0393253768039287
aisingapore/sea-lion-7b,mmlu_prehistory,0-shot,accuracy,0.25,0.0240934712326213
aisingapore/sea-lion-7b,mmlu_moral_scenarios,0-shot,accuracy,0.2726256983240223,0.0148933917352496
aisingapore/sea-lion-7b,mmlu_high_school_world_history,0-shot,accuracy,0.2827004219409282,0.0293128141539559
aisingapore/sea-lion-7b,mmlu_moral_disputes,0-shot,accuracy,0.2976878612716763,0.024617055388677
aisingapore/sea-lion-7b,mmlu_professional_law,0-shot,accuracy,0.288135593220339,0.0115671406613245
aisingapore/sea-lion-7b,mmlu_logical_fallacies,0-shot,accuracy,0.2515337423312883,0.0340899788685752
aisingapore/sea-lion-7b,mmlu_high_school_us_history,0-shot,accuracy,0.2598039215686274,0.0307785546786932
aisingapore/sea-lion-7b,mmlu_philosophy,0-shot,accuracy,0.3054662379421222,0.0261605844501404
aisingapore/sea-lion-7b,mmlu_jurisprudence,0-shot,accuracy,0.287037037037037,0.0437331304091476
aisingapore/sea-lion-7b,mmlu_international_law,0-shot,accuracy,0.2148760330578512,0.0374949244870969
aisingapore/sea-lion-7b,mmlu_high_school_european_history,0-shot,accuracy,0.2666666666666666,0.0345313180188541
aisingapore/sea-lion-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2901554404145077,0.0327526446779151
aisingapore/sea-lion-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2310924369747899,0.0273814069278689
aisingapore/sea-lion-7b,mmlu_high_school_geography,0-shot,accuracy,0.3535353535353535,0.0340608672354715
aisingapore/sea-lion-7b,mmlu_high_school_psychology,0-shot,accuracy,0.273394495412844,0.0191092998460982
aisingapore/sea-lion-7b,mmlu_public_relations,0-shot,accuracy,0.2818181818181818,0.0430911870994646
aisingapore/sea-lion-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.0464823198711731
aisingapore/sea-lion-7b,mmlu_sociology,0-shot,accuracy,0.2885572139303483,0.0320384104021332
aisingapore/sea-lion-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2923076923076923,0.0230604383808577
aisingapore/sea-lion-7b,mmlu_security_studies,0-shot,accuracy,0.4408163265306122,0.0317841911417536
aisingapore/sea-lion-7b,mmlu_professional_psychology,0-shot,accuracy,0.2630718954248366,0.0178126765423206
aisingapore/sea-lion-7b,mmlu_human_sexuality,0-shot,accuracy,0.2824427480916031,0.0394840612576836
aisingapore/sea-lion-7b,mmlu_econometrics,0-shot,accuracy,0.2807017543859649,0.0422705445123219
aisingapore/sea-lion-7b,mmlu_miscellaneous,0-shot,accuracy,0.2605363984674329,0.015696008563807
aisingapore/sea-lion-7b,mmlu_marketing,0-shot,accuracy,0.2606837606837607,0.0287603489565234
aisingapore/sea-lion-7b,mmlu_management,0-shot,accuracy,0.233009708737864,0.0418583259892831
aisingapore/sea-lion-7b,mmlu_nutrition,0-shot,accuracy,0.2810457516339869,0.0257388547978187
aisingapore/sea-lion-7b,mmlu_medical_genetics,0-shot,accuracy,0.23,0.042295258468165
aisingapore/sea-lion-7b,mmlu_human_aging,0-shot,accuracy,0.2017937219730941,0.0269361119128022
aisingapore/sea-lion-7b,mmlu_professional_medicine,0-shot,accuracy,0.1985294117647058,0.024231013370541
aisingapore/sea-lion-7b,mmlu_college_medicine,0-shot,accuracy,0.3583815028901734,0.0365634365335315
aisingapore/sea-lion-7b,mmlu_business_ethics,0-shot,accuracy,0.31,0.0464823198711731
aisingapore/sea-lion-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.2943396226415094,0.0280491863156952
aisingapore/sea-lion-7b,mmlu_global_facts,0-shot,accuracy,0.29,0.0456048021572068
aisingapore/sea-lion-7b,mmlu_virology,0-shot,accuracy,0.2590361445783132,0.0341064661407185
aisingapore/sea-lion-7b,mmlu_professional_accounting,0-shot,accuracy,0.2695035460992908,0.0264690368185906
aisingapore/sea-lion-7b,mmlu_college_physics,0-shot,accuracy,0.1568627450980392,0.0361866481993624
aisingapore/sea-lion-7b,mmlu_high_school_physics,0-shot,accuracy,0.2317880794701986,0.0344540627198705
aisingapore/sea-lion-7b,mmlu_high_school_biology,0-shot,accuracy,0.232258064516129,0.0240222561303082
aisingapore/sea-lion-7b,mmlu_college_biology,0-shot,accuracy,0.3263888888888889,0.0392106719898226
aisingapore/sea-lion-7b,mmlu_anatomy,0-shot,accuracy,0.2296296296296296,0.0363338441407346
aisingapore/sea-lion-7b,mmlu_college_chemistry,0-shot,accuracy,0.21,0.0409360180740332
aisingapore/sea-lion-7b,mmlu_computer_security,0-shot,accuracy,0.31,0.0464823198711731
aisingapore/sea-lion-7b,mmlu_college_computer_science,0-shot,accuracy,0.31,0.0464823198711731
aisingapore/sea-lion-7b,mmlu_astronomy,0-shot,accuracy,0.2697368421052631,0.0361178056028489
aisingapore/sea-lion-7b,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276808
aisingapore/sea-lion-7b,mmlu_conceptual_physics,0-shot,accuracy,0.2382978723404255,0.0278512529738897
aisingapore/sea-lion-7b,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.0435194139889244
aisingapore/sea-lion-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.28,0.0451260859854212
aisingapore/sea-lion-7b,mmlu_machine_learning,0-shot,accuracy,0.2589285714285714,0.0415775153986562
aisingapore/sea-lion-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.2266009852216748,0.0294548638352929
aisingapore/sea-lion-7b,mmlu_high_school_statistics,0-shot,accuracy,0.2546296296296296,0.0297112758600053
aisingapore/sea-lion-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.2513227513227513,0.0223404823396438
aisingapore/sea-lion-7b,mmlu_electrical_engineering,0-shot,accuracy,0.3172413793103448,0.0387835237213862
aisingapore/sea-lion-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2259259259259259,0.0254975326396095
aisingapore/sea-lion-7b,arc_challenge,25-shot,accuracy,0.3592150170648464,0.0140202241558391
aisingapore/sea-lion-7b,arc_challenge,25-shot,acc_norm,0.3959044368600682,0.0142912283935365
aisingapore/sea-lion-7b,hellaswag,10-shot,accuracy,0.5088627763393746,0.0049889974671344
aisingapore/sea-lion-7b,hellaswag,10-shot,acc_norm,0.6869149571798446,0.0046280086619551
aisingapore/sea-lion-7b,truthfulqa_mc2,0-shot,accuracy,0.3512619484298876,0.0134585430998279
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,bleu_max,22.656533340238912,0.7120387543349416
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,bleu_acc,0.2986536107711138,0.0160215706137685
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,bleu_diff,-9.533528953913963,0.7845698517561239
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge1_max,47.07013430195858,0.8588618247484433
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge1_acc,0.2521419828641371,0.0152015222462999
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge1_diff,-11.523044856861963,0.8632903667062516
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge2_max,29.865710554472003,0.9675510003865858
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge2_acc,0.193390452876377,0.013826240752599
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge2_diff,-14.306988948573508,1.009761759343484
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rougeL_max,44.404211913477454,0.8669444299614717
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rougeL_acc,0.2472460220318237,0.0151024047973596
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rougeL_diff,-11.962363986877255,0.869977470816951
aisingapore/sea-lion-7b,truthfulqa_mc1,0-shot,accuracy,0.204406364749082,0.0141171743374326
aisingapore/sea-lion-7b,winogrande,5-shot,accuracy,0.6156274664561957,0.0136715676008361
jisukim8873/falcon-7B-case-6,arc:challenge,25-shot,accuracy,0.4274744027303754,0.0144568629446506
jisukim8873/falcon-7B-case-6,arc:challenge,25-shot,acc_norm,0.4650170648464163,0.0145755839220196
jisukim8873/falcon-7B-case-6,hellaswag,10-shot,accuracy,0.5976897032463653,0.0048936170149753
jisukim8873/falcon-7B-case-6,hellaswag,10-shot,acc_norm,0.7849034056960765,0.0041004959781084
jisukim8873/falcon-7B-case-6,hendrycksTest-abstract_algebra,5-shot,accuracy,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-6,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-6,hendrycksTest-anatomy,5-shot,accuracy,0.2962962962962963,0.0394462416250111
jisukim8873/falcon-7B-case-6,hendrycksTest-anatomy,5-shot,acc_norm,0.2962962962962963,0.0394462416250111
jisukim8873/falcon-7B-case-6,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.0373852067611966
jisukim8873/falcon-7B-case-6,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.0373852067611966
jisukim8873/falcon-7B-case-6,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.042295258468165
jisukim8873/falcon-7B-case-6,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.042295258468165
jisukim8873/falcon-7B-case-6,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.0282542003444386
jisukim8873/falcon-7B-case-6,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.0282542003444386
jisukim8873/falcon-7B-case-6,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
jisukim8873/falcon-7B-case-6,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.0368565109589753
jisukim8873/falcon-7B-case-6,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-6,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-6,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
jisukim8873/falcon-7B-case-6,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
jisukim8873/falcon-7B-case-6,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-6,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-6,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225943
jisukim8873/falcon-7B-case-6,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.0336876293225943
jisukim8873/falcon-7B-case-6,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
jisukim8873/falcon-7B-case-6,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
jisukim8873/falcon-7B-case-6,hendrycksTest-computer_security,5-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-6,hendrycksTest-computer_security,5-shot,acc_norm,0.33,0.047258156262526
jisukim8873/falcon-7B-case-6,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.0303635821972381
jisukim8873/falcon-7B-case-6,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.0303635821972381
jisukim8873/falcon-7B-case-6,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.0426633944315939
jisukim8873/falcon-7B-case-6,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.0426633944315939
jisukim8873/falcon-7B-case-6,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2758620689655172,0.0372456361977463
jisukim8873/falcon-7B-case-6,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2758620689655172,0.0372456361977463
jisukim8873/falcon-7B-case-6,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
jisukim8873/falcon-7B-case-6,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
jisukim8873/falcon-7B-case-6,hendrycksTest-formal_logic,5-shot,accuracy,0.1349206349206349,0.0305571015894175
jisukim8873/falcon-7B-case-6,hendrycksTest-formal_logic,5-shot,acc_norm,0.1349206349206349,0.0305571015894175
jisukim8873/falcon-7B-case-6,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-6,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_biology,5-shot,accuracy,0.332258064516129,0.0267955608481227
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_biology,5-shot,acc_norm,0.332258064516129,0.0267955608481227
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3497536945812808,0.0335540090496956
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3497536945812808,0.0335540090496956
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.0464823198711731
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.0464823198711731
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3151515151515151,0.0362773057502241
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3151515151515151,0.0362773057502241
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_geography,5-shot,accuracy,0.303030303030303,0.0327428791402686
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_geography,5-shot,acc_norm,0.303030303030303,0.0327428791402686
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2590673575129533,0.0316187791793541
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2590673575129533,0.0316187791793541
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2461538461538461,0.021840866990423
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2461538461538461,0.021840866990423
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2444444444444444,0.0262027665346521
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2444444444444444,0.0262027665346521
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2436974789915966,0.0278868280783805
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2436974789915966,0.0278868280783805
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_physics,5-shot,accuracy,0.2781456953642384,0.0365860326276374
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2781456953642384,0.0365860326276374
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2899082568807339,0.0194530666092015
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2899082568807339,0.0194530666092015
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1944444444444444,0.0269914545020367
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1944444444444444,0.0269914545020367
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2745098039215686,0.0313217980308328
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2745098039215686,0.0313217980308328
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3164556962025316,0.0302749748802189
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3164556962025316,0.0302749748802189
jisukim8873/falcon-7B-case-6,hendrycksTest-human_aging,5-shot,accuracy,0.3766816143497757,0.0325211348992918
jisukim8873/falcon-7B-case-6,hendrycksTest-human_aging,5-shot,acc_norm,0.3766816143497757,0.0325211348992918
jisukim8873/falcon-7B-case-6,hendrycksTest-human_sexuality,5-shot,accuracy,0.2671755725190839,0.0388084830108239
jisukim8873/falcon-7B-case-6,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2671755725190839,0.0388084830108239
jisukim8873/falcon-7B-case-6,hendrycksTest-international_law,5-shot,accuracy,0.4132231404958678,0.044950878435484
jisukim8873/falcon-7B-case-6,hendrycksTest-international_law,5-shot,acc_norm,0.4132231404958678,0.044950878435484
jisukim8873/falcon-7B-case-6,hendrycksTest-jurisprudence,5-shot,accuracy,0.3148148148148148,0.0448993107359131
jisukim8873/falcon-7B-case-6,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3148148148148148,0.0448993107359131
jisukim8873/falcon-7B-case-6,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
jisukim8873/falcon-7B-case-6,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.0355903953161734
jisukim8873/falcon-7B-case-6,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.0428785875134045
jisukim8873/falcon-7B-case-6,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.0428785875134045
jisukim8873/falcon-7B-case-6,hendrycksTest-management,5-shot,accuracy,0.3203883495145631,0.0462028408228003
jisukim8873/falcon-7B-case-6,hendrycksTest-management,5-shot,acc_norm,0.3203883495145631,0.0462028408228003
jisukim8873/falcon-7B-case-6,hendrycksTest-marketing,5-shot,accuracy,0.3076923076923077,0.030236389942173
jisukim8873/falcon-7B-case-6,hendrycksTest-marketing,5-shot,acc_norm,0.3076923076923077,0.030236389942173
jisukim8873/falcon-7B-case-6,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.0464823198711731
jisukim8873/falcon-7B-case-6,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.0464823198711731
jisukim8873/falcon-7B-case-6,hendrycksTest-miscellaneous,5-shot,accuracy,0.3537675606641124,0.0170981847081619
jisukim8873/falcon-7B-case-6,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3537675606641124,0.0170981847081619
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_disputes,5-shot,accuracy,0.3236994219653179,0.0251901813276084
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3236994219653179,0.0251901813276084
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2469273743016759,0.0144222922048088
jisukim8873/falcon-7B-case-6,hendrycksTest-nutrition,5-shot,accuracy,0.3202614379084967,0.0267161183801568
jisukim8873/falcon-7B-case-6,hendrycksTest-nutrition,5-shot,acc_norm,0.3202614379084967,0.0267161183801568
jisukim8873/falcon-7B-case-6,hendrycksTest-philosophy,5-shot,accuracy,0.3183279742765273,0.026457225067811
jisukim8873/falcon-7B-case-6,hendrycksTest-philosophy,5-shot,acc_norm,0.3183279742765273,0.026457225067811
jisukim8873/falcon-7B-case-6,hendrycksTest-prehistory,5-shot,accuracy,0.2777777777777778,0.0249220011688863
jisukim8873/falcon-7B-case-6,hendrycksTest-prehistory,5-shot,acc_norm,0.2777777777777778,0.0249220011688863
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_accounting,5-shot,accuracy,0.2411347517730496,0.0255187310495377
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2411347517730496,0.0255187310495377
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_law,5-shot,accuracy,0.2627118644067797,0.0112405455149956
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_law,5-shot,acc_norm,0.2627118644067797,0.0112405455149956
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_medicine,5-shot,accuracy,0.213235294117647,0.0248809715122942
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_medicine,5-shot,acc_norm,0.213235294117647,0.0248809715122942
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_psychology,5-shot,accuracy,0.2630718954248366,0.0178126765423206
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2630718954248366,0.0178126765423206
jisukim8873/falcon-7B-case-6,hendrycksTest-public_relations,5-shot,accuracy,0.2454545454545454,0.0412206650287828
jisukim8873/falcon-7B-case-6,hendrycksTest-public_relations,5-shot,acc_norm,0.2454545454545454,0.0412206650287828
jisukim8873/falcon-7B-case-6,hendrycksTest-security_studies,5-shot,accuracy,0.2448979591836734,0.0275296374401749
jisukim8873/falcon-7B-case-6,hendrycksTest-security_studies,5-shot,acc_norm,0.2448979591836734,0.0275296374401749
jisukim8873/falcon-7B-case-6,hendrycksTest-sociology,5-shot,accuracy,0.3034825870646766,0.0325100681645861
jisukim8873/falcon-7B-case-6,hendrycksTest-sociology,5-shot,acc_norm,0.3034825870646766,0.0325100681645861
jisukim8873/falcon-7B-case-6,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.42,0.0496044963748858
jisukim8873/falcon-7B-case-6,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.42,0.0496044963748858
jisukim8873/falcon-7B-case-6,hendrycksTest-virology,5-shot,accuracy,0.3253012048192771,0.0364716852368322
jisukim8873/falcon-7B-case-6,hendrycksTest-virology,5-shot,acc_norm,0.3253012048192771,0.0364716852368322
jisukim8873/falcon-7B-case-6,hendrycksTest-world_religions,5-shot,accuracy,0.3391812865497076,0.036310534964889
jisukim8873/falcon-7B-case-6,hendrycksTest-world_religions,5-shot,acc_norm,0.3391812865497076,0.036310534964889
jisukim8873/falcon-7B-case-6,truthfulqa:mc,0-shot,mc1,0.2509179926560587,0.0151769850277076
jisukim8873/falcon-7B-case-6,truthfulqa:mc,0-shot,mc2,0.364571668218642,0.0141174160418799
jisukim8873/falcon-7B-case-6,winogrande,5-shot,accuracy,0.7008681925808997,0.0128686390660915
jisukim8873/falcon-7B-case-6,gsm8k,5-shot,accuracy,0.0765731614859742,0.0073245648814515
jisukim8873/falcon-7B-case-6,minerva_math_precalc,5-shot,accuracy,0.0018315018315018,0.0018315018315018
jisukim8873/falcon-7B-case-6,minerva_math_prealgebra,5-shot,accuracy,0.0275545350172215,0.0055497004803932
jisukim8873/falcon-7B-case-6,minerva_math_num_theory,5-shot,accuracy,0.0111111111111111,0.0045150037076946
jisukim8873/falcon-7B-case-6,minerva_math_intermediate_algebra,5-shot,accuracy,0.0077519379844961,0.0029201960269643
jisukim8873/falcon-7B-case-6,minerva_math_geometry,5-shot,accuracy,0.0041753653444676,0.0029493392170756
jisukim8873/falcon-7B-case-6,minerva_math_counting_and_prob,5-shot,accuracy,0.0168776371308016,0.0059228268948526
jisukim8873/falcon-7B-case-6,minerva_math_algebra,5-shot,accuracy,0.0109519797809604,0.0030221266536702
jisukim8873/falcon-7B-case-6,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-6,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-6,arithmetic_3da,5-shot,accuracy,0.103,0.0067984269728115
jisukim8873/falcon-7B-case-6,arithmetic_3ds,5-shot,accuracy,0.069,0.0056688241976526
jisukim8873/falcon-7B-case-6,arithmetic_4da,5-shot,accuracy,0.002,0.0009992493430694
jisukim8873/falcon-7B-case-6,arithmetic_2ds,5-shot,accuracy,0.2985,0.0102348058420915
jisukim8873/falcon-7B-case-6,arithmetic_5ds,5-shot,accuracy,0.0015,0.0008655920660521
jisukim8873/falcon-7B-case-6,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-6,arithmetic_1dc,5-shot,accuracy,0.081,0.0061023044056758
jisukim8873/falcon-7B-case-6,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005
jisukim8873/falcon-7B-case-6,arithmetic_2dm,5-shot,accuracy,0.148,0.007942262887231
jisukim8873/falcon-7B-case-6,arithmetic_2da,5-shot,accuracy,0.468,0.0111602094576028
jisukim8873/falcon-7B-case-6,gsm8k_cot,5-shot,accuracy,0.1008339651250947,0.0082940311921266
jisukim8873/falcon-7B-case-6,anli_r2,0-shot,brier_score,1.0215867022310283,
jisukim8873/falcon-7B-case-6,anli_r3,0-shot,brier_score,0.922754052447002,
jisukim8873/falcon-7B-case-6,anli_r1,0-shot,brier_score,1.0587549732497896,
jisukim8873/falcon-7B-case-6,xnli_eu,0-shot,brier_score,1.0846362802483993,
jisukim8873/falcon-7B-case-6,xnli_vi,0-shot,brier_score,0.9955744880152678,
jisukim8873/falcon-7B-case-6,xnli_ru,0-shot,brier_score,0.8139283536030013,
jisukim8873/falcon-7B-case-6,xnli_zh,0-shot,brier_score,1.0232273505528495,
jisukim8873/falcon-7B-case-6,xnli_tr,0-shot,brier_score,1.0068194344208217,
jisukim8873/falcon-7B-case-6,xnli_fr,0-shot,brier_score,0.7378926188868539,
jisukim8873/falcon-7B-case-6,xnli_en,0-shot,brier_score,0.6473635963303874,
jisukim8873/falcon-7B-case-6,xnli_ur,0-shot,brier_score,1.3048334974182785,
jisukim8873/falcon-7B-case-6,xnli_ar,0-shot,brier_score,1.2965712362618014,
jisukim8873/falcon-7B-case-6,xnli_de,0-shot,brier_score,0.8442829185961966,
jisukim8873/falcon-7B-case-6,xnli_hi,0-shot,brier_score,1.068890027784789,
jisukim8873/falcon-7B-case-6,xnli_es,0-shot,brier_score,0.8113170196363131,
jisukim8873/falcon-7B-case-6,xnli_bg,0-shot,brier_score,0.8936463916487021,
jisukim8873/falcon-7B-case-6,xnli_sw,0-shot,brier_score,1.0700519786794918,
jisukim8873/falcon-7B-case-6,xnli_el,0-shot,brier_score,0.9588694161285166,
jisukim8873/falcon-7B-case-6,xnli_th,0-shot,brier_score,0.98350811120129,
jisukim8873/falcon-7B-case-6,logiqa2,0-shot,brier_score,1.0489832657150588,
jisukim8873/falcon-7B-case-6,mathqa,0-shot,brier_score,0.9350069504644906,
jisukim8873/falcon-7B-case-6,lambada_standard,0-shot,perplexity,4.177882741614251,0.0905948144738662
jisukim8873/falcon-7B-case-6,lambada_standard,0-shot,accuracy,0.6652435474480884,0.0065745629301564
jisukim8873/falcon-7B-case-6,lambada_openai,0-shot,perplexity,3.327303471293643,0.069729228543628
jisukim8873/falcon-7B-case-6,lambada_openai,0-shot,accuracy,0.7349117019212109,0.0061492894021581
jb723/llama2-ko-7B-model,arc:challenge,25-shot,accuracy,0.5349829351535836,0.0145755839220196
jb723/llama2-ko-7B-model,arc:challenge,25-shot,acc_norm,0.5631399317406144,0.0144944215842565
jb723/llama2-ko-7B-model,hellaswag,10-shot,accuracy,0.6069508066122287,0.0048742939648435
jb723/llama2-ko-7B-model,hellaswag,10-shot,acc_norm,0.7893845847440749,0.0040691239053249
jb723/llama2-ko-7B-model,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.0464823198711731
jb723/llama2-ko-7B-model,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.0464823198711731
jb723/llama2-ko-7B-model,hendrycksTest-anatomy,5-shot,accuracy,0.4592592592592592,0.0430497969246424
jb723/llama2-ko-7B-model,hendrycksTest-anatomy,5-shot,acc_norm,0.4592592592592592,0.0430497969246424
jb723/llama2-ko-7B-model,hendrycksTest-astronomy,5-shot,accuracy,0.4605263157894737,0.0405624225224903
jb723/llama2-ko-7B-model,hendrycksTest-astronomy,5-shot,acc_norm,0.4605263157894737,0.0405624225224903
jb723/llama2-ko-7B-model,hendrycksTest-business_ethics,5-shot,accuracy,0.45,0.05
jb723/llama2-ko-7B-model,hendrycksTest-business_ethics,5-shot,acc_norm,0.45,0.05
jb723/llama2-ko-7B-model,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4566037735849056,0.0306567486967394
jb723/llama2-ko-7B-model,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4566037735849056,0.0306567486967394
jb723/llama2-ko-7B-model,hendrycksTest-college_biology,5-shot,accuracy,0.4375,0.0414841573939415
jb723/llama2-ko-7B-model,hendrycksTest-college_biology,5-shot,acc_norm,0.4375,0.0414841573939415
jb723/llama2-ko-7B-model,hendrycksTest-college_chemistry,5-shot,accuracy,0.36,0.0482418151324421
jb723/llama2-ko-7B-model,hendrycksTest-college_chemistry,5-shot,acc_norm,0.36,0.0482418151324421
jb723/llama2-ko-7B-model,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.047258156262526
jb723/llama2-ko-7B-model,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.047258156262526
jb723/llama2-ko-7B-model,hendrycksTest-college_mathematics,5-shot,accuracy,0.37,0.0485236587093909
jb723/llama2-ko-7B-model,hendrycksTest-college_mathematics,5-shot,acc_norm,0.37,0.0485236587093909
jb723/llama2-ko-7B-model,hendrycksTest-college_medicine,5-shot,accuracy,0.4046242774566474,0.0374246119388724
jb723/llama2-ko-7B-model,hendrycksTest-college_medicine,5-shot,acc_norm,0.4046242774566474,0.0374246119388724
jb723/llama2-ko-7B-model,hendrycksTest-college_physics,5-shot,accuracy,0.1470588235294117,0.0352406895156744
jb723/llama2-ko-7B-model,hendrycksTest-college_physics,5-shot,acc_norm,0.1470588235294117,0.0352406895156744
jb723/llama2-ko-7B-model,hendrycksTest-computer_security,5-shot,accuracy,0.52,0.0502116731568677
jb723/llama2-ko-7B-model,hendrycksTest-computer_security,5-shot,acc_norm,0.52,0.0502116731568677
jb723/llama2-ko-7B-model,hendrycksTest-conceptual_physics,5-shot,accuracy,0.4127659574468085,0.0321847114140035
jb723/llama2-ko-7B-model,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.4127659574468085,0.0321847114140035
jb723/llama2-ko-7B-model,hendrycksTest-econometrics,5-shot,accuracy,0.3245614035087719,0.0440455615737476
jb723/llama2-ko-7B-model,hendrycksTest-econometrics,5-shot,acc_norm,0.3245614035087719,0.0440455615737476
jb723/llama2-ko-7B-model,hendrycksTest-electrical_engineering,5-shot,accuracy,0.4482758620689655,0.0414431181087815
jb723/llama2-ko-7B-model,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.4482758620689655,0.0414431181087815
jb723/llama2-ko-7B-model,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3174603174603174,0.023973861998992
jb723/llama2-ko-7B-model,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3174603174603174,0.023973861998992
jb723/llama2-ko-7B-model,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.0380952380952381
jb723/llama2-ko-7B-model,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.0380952380952381
jb723/llama2-ko-7B-model,hendrycksTest-global_facts,5-shot,accuracy,0.39,0.0490207130000197
jb723/llama2-ko-7B-model,hendrycksTest-global_facts,5-shot,acc_norm,0.39,0.0490207130000197
jb723/llama2-ko-7B-model,hendrycksTest-high_school_biology,5-shot,accuracy,0.4935483870967742,0.0284416382335405
jb723/llama2-ko-7B-model,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4935483870967742,0.0284416382335405
jb723/llama2-ko-7B-model,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3300492610837438,0.0330853042622825
jb723/llama2-ko-7B-model,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3300492610837438,0.0330853042622825
jb723/llama2-ko-7B-model,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.41,0.049431107042371
jb723/llama2-ko-7B-model,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.41,0.049431107042371
jb723/llama2-ko-7B-model,hendrycksTest-high_school_european_history,5-shot,accuracy,0.5878787878787879,0.0384356699358871
jb723/llama2-ko-7B-model,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.5878787878787879,0.0384356699358871
jb723/llama2-ko-7B-model,hendrycksTest-high_school_geography,5-shot,accuracy,0.5757575757575758,0.0352122490884158
jb723/llama2-ko-7B-model,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5757575757575758,0.0352122490884158
jb723/llama2-ko-7B-model,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.689119170984456,0.0334036190627658
jb723/llama2-ko-7B-model,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.689119170984456,0.0334036190627658
jb723/llama2-ko-7B-model,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4461538461538462,0.0252035717730283
jb723/llama2-ko-7B-model,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4461538461538462,0.0252035717730283
jb723/llama2-ko-7B-model,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
jb723/llama2-ko-7B-model,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2592592592592592,0.0267192407837121
jb723/llama2-ko-7B-model,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.407563025210084,0.0319186337447846
jb723/llama2-ko-7B-model,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.407563025210084,0.0319186337447846
jb723/llama2-ko-7B-model,hendrycksTest-high_school_physics,5-shot,accuracy,0.3178807947019867,0.038020397601079
jb723/llama2-ko-7B-model,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3178807947019867,0.038020397601079
jb723/llama2-ko-7B-model,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6146788990825688,0.0208658508527941
jb723/llama2-ko-7B-model,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6146788990825688,0.0208658508527941
jb723/llama2-ko-7B-model,hendrycksTest-high_school_statistics,5-shot,accuracy,0.287037037037037,0.030851992993257
jb723/llama2-ko-7B-model,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.287037037037037,0.030851992993257
jb723/llama2-ko-7B-model,hendrycksTest-high_school_us_history,5-shot,accuracy,0.5833333333333334,0.0346022832723917
jb723/llama2-ko-7B-model,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.5833333333333334,0.0346022832723917
jb723/llama2-ko-7B-model,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6666666666666666,0.0306858205966107
jb723/llama2-ko-7B-model,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6666666666666666,0.0306858205966107
jb723/llama2-ko-7B-model,hendrycksTest-human_aging,5-shot,accuracy,0.5246636771300448,0.0335169516765262
jb723/llama2-ko-7B-model,hendrycksTest-human_aging,5-shot,acc_norm,0.5246636771300448,0.0335169516765262
jb723/llama2-ko-7B-model,hendrycksTest-human_sexuality,5-shot,accuracy,0.5343511450381679,0.0437492856059973
jb723/llama2-ko-7B-model,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5343511450381679,0.0437492856059973
jb723/llama2-ko-7B-model,hendrycksTest-international_law,5-shot,accuracy,0.6446280991735537,0.0436923632657398
jb723/llama2-ko-7B-model,hendrycksTest-international_law,5-shot,acc_norm,0.6446280991735537,0.0436923632657398
jb723/llama2-ko-7B-model,hendrycksTest-jurisprudence,5-shot,accuracy,0.5,0.0483368244522831
jb723/llama2-ko-7B-model,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5,0.0483368244522831
jb723/llama2-ko-7B-model,hendrycksTest-logical_fallacies,5-shot,accuracy,0.4478527607361963,0.039069474794566
jb723/llama2-ko-7B-model,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.4478527607361963,0.039069474794566
jb723/llama2-ko-7B-model,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257872
jb723/llama2-ko-7B-model,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257872
jb723/llama2-ko-7B-model,hendrycksTest-management,5-shot,accuracy,0.5631067961165048,0.0491114710736577
jb723/llama2-ko-7B-model,hendrycksTest-management,5-shot,acc_norm,0.5631067961165048,0.0491114710736577
jb723/llama2-ko-7B-model,hendrycksTest-marketing,5-shot,accuracy,0.6752136752136753,0.0306790227654988
jb723/llama2-ko-7B-model,hendrycksTest-marketing,5-shot,acc_norm,0.6752136752136753,0.0306790227654988
jb723/llama2-ko-7B-model,hendrycksTest-medical_genetics,5-shot,accuracy,0.53,0.0501613558046591
jb723/llama2-ko-7B-model,hendrycksTest-medical_genetics,5-shot,acc_norm,0.53,0.0501613558046591
jb723/llama2-ko-7B-model,hendrycksTest-miscellaneous,5-shot,accuracy,0.6577266922094508,0.0169670317664136
jb723/llama2-ko-7B-model,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6577266922094508,0.0169670317664136
jb723/llama2-ko-7B-model,hendrycksTest-moral_disputes,5-shot,accuracy,0.5202312138728323,0.0268970499963828
jb723/llama2-ko-7B-model,hendrycksTest-moral_disputes,5-shot,acc_norm,0.5202312138728323,0.0268970499963828
jb723/llama2-ko-7B-model,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2379888268156424,0.0142426300705749
jb723/llama2-ko-7B-model,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2379888268156424,0.0142426300705749
jb723/llama2-ko-7B-model,hendrycksTest-nutrition,5-shot,accuracy,0.477124183006536,0.0285999367760897
jb723/llama2-ko-7B-model,hendrycksTest-nutrition,5-shot,acc_norm,0.477124183006536,0.0285999367760897
jb723/llama2-ko-7B-model,hendrycksTest-philosophy,5-shot,accuracy,0.5562700964630225,0.0282176835566523
jb723/llama2-ko-7B-model,hendrycksTest-philosophy,5-shot,acc_norm,0.5562700964630225,0.0282176835566523
jb723/llama2-ko-7B-model,hendrycksTest-prehistory,5-shot,accuracy,0.5216049382716049,0.0277947601050087
jb723/llama2-ko-7B-model,hendrycksTest-prehistory,5-shot,acc_norm,0.5216049382716049,0.0277947601050087
jb723/llama2-ko-7B-model,hendrycksTest-professional_accounting,5-shot,accuracy,0.3475177304964539,0.0284066278095909
jb723/llama2-ko-7B-model,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3475177304964539,0.0284066278095909
jb723/llama2-ko-7B-model,hendrycksTest-professional_law,5-shot,accuracy,0.3552803129074315,0.012223623364044
jb723/llama2-ko-7B-model,hendrycksTest-professional_law,5-shot,acc_norm,0.3552803129074315,0.012223623364044
jb723/llama2-ko-7B-model,hendrycksTest-professional_medicine,5-shot,accuracy,0.4411764705882353,0.0301619119307671
jb723/llama2-ko-7B-model,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4411764705882353,0.0301619119307671
jb723/llama2-ko-7B-model,hendrycksTest-professional_psychology,5-shot,accuracy,0.4526143790849673,0.0201367909184925
jb723/llama2-ko-7B-model,hendrycksTest-professional_psychology,5-shot,acc_norm,0.4526143790849673,0.0201367909184925
jb723/llama2-ko-7B-model,hendrycksTest-public_relations,5-shot,accuracy,0.5272727272727272,0.0478200179138006
jb723/llama2-ko-7B-model,hendrycksTest-public_relations,5-shot,acc_norm,0.5272727272727272,0.0478200179138006
jb723/llama2-ko-7B-model,hendrycksTest-security_studies,5-shot,accuracy,0.4326530612244898,0.0317175282406266
jb723/llama2-ko-7B-model,hendrycksTest-security_studies,5-shot,acc_norm,0.4326530612244898,0.0317175282406266
jb723/llama2-ko-7B-model,hendrycksTest-sociology,5-shot,accuracy,0.572139303482587,0.0349854198840779
jb723/llama2-ko-7B-model,hendrycksTest-sociology,5-shot,acc_norm,0.572139303482587,0.0349854198840779
jb723/llama2-ko-7B-model,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.63,0.0485236587093909
jb723/llama2-ko-7B-model,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.63,0.0485236587093909
jb723/llama2-ko-7B-model,hendrycksTest-virology,5-shot,accuracy,0.4216867469879518,0.0384445318177091
jb723/llama2-ko-7B-model,hendrycksTest-virology,5-shot,acc_norm,0.4216867469879518,0.0384445318177091
jb723/llama2-ko-7B-model,hendrycksTest-world_religions,5-shot,accuracy,0.6842105263157895,0.0356507967070831
jb723/llama2-ko-7B-model,hendrycksTest-world_religions,5-shot,acc_norm,0.6842105263157895,0.0356507967070831
jb723/llama2-ko-7B-model,truthfulqa:mc,0-shot,mc1,0.2815177478580171,0.015744027248256
jb723/llama2-ko-7B-model,truthfulqa:mc,0-shot,mc2,0.4097811489004275,0.0155522913358376
jb723/llama2-ko-7B-model,drop,3-shot,accuracy,0.2342701342281879,0.0043374642431385
jb723/llama2-ko-7B-model,drop,3-shot,f1,0.3152516778523505,0.0043537257125576
jb723/llama2-ko-7B-model,gsm8k,5-shot,accuracy,0.0803639120545868,0.007488258573239
jb723/llama2-ko-7B-model,winogrande,5-shot,accuracy,0.7000789265982637,0.012878347526636
jb723/llama2-ko-7B-model,mmlu_world_religions,0-shot,accuracy,0.672514619883041,0.0359933577145602
jb723/llama2-ko-7B-model,mmlu_formal_logic,0-shot,accuracy,0.246031746031746,0.0385227336492431
jb723/llama2-ko-7B-model,mmlu_prehistory,0-shot,accuracy,0.5154320987654321,0.0278074900442761
jb723/llama2-ko-7B-model,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705748
jb723/llama2-ko-7B-model,mmlu_high_school_world_history,0-shot,accuracy,0.6751054852320675,0.0304860393891053
jb723/llama2-ko-7B-model,mmlu_moral_disputes,0-shot,accuracy,0.5260115606936416,0.0268826434340228
jb723/llama2-ko-7B-model,mmlu_professional_law,0-shot,accuracy,0.3546284224250326,0.0122185764390901
jb723/llama2-ko-7B-model,mmlu_logical_fallacies,0-shot,accuracy,0.4785276073619632,0.0392474687675113
jb723/llama2-ko-7B-model,mmlu_high_school_us_history,0-shot,accuracy,0.5637254901960784,0.0348069313845703
jb723/llama2-ko-7B-model,mmlu_philosophy,0-shot,accuracy,0.5594855305466238,0.0281964005741974
jb723/llama2-ko-7B-model,mmlu_jurisprudence,0-shot,accuracy,0.4907407407407407,0.0483285355343705
jb723/llama2-ko-7B-model,mmlu_international_law,0-shot,accuracy,0.6033057851239669,0.04465869780531
jb723/llama2-ko-7B-model,mmlu_high_school_european_history,0-shot,accuracy,0.5757575757575758,0.0385926814207026
jb723/llama2-ko-7B-model,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6787564766839378,0.0336995086854906
jb723/llama2-ko-7B-model,mmlu_high_school_microeconomics,0-shot,accuracy,0.4159663865546218,0.0320165010073961
jb723/llama2-ko-7B-model,mmlu_high_school_geography,0-shot,accuracy,0.5909090909090909,0.03502975799413
jb723/llama2-ko-7B-model,mmlu_high_school_psychology,0-shot,accuracy,0.6201834862385321,0.0208088256178662
jb723/llama2-ko-7B-model,mmlu_public_relations,0-shot,accuracy,0.5545454545454546,0.0476054882146032
jb723/llama2-ko-7B-model,mmlu_us_foreign_policy,0-shot,accuracy,0.64,0.0482418151324421
jb723/llama2-ko-7B-model,mmlu_sociology,0-shot,accuracy,0.5870646766169154,0.0348152080336734
jb723/llama2-ko-7B-model,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4435897435897435,0.0251891498947641
jb723/llama2-ko-7B-model,mmlu_security_studies,0-shot,accuracy,0.4489795918367347,0.0318421386668757
jb723/llama2-ko-7B-model,mmlu_professional_psychology,0-shot,accuracy,0.4477124183006536,0.0201169253474224
jb723/llama2-ko-7B-model,mmlu_human_sexuality,0-shot,accuracy,0.549618320610687,0.0436364369852477
jb723/llama2-ko-7B-model,mmlu_econometrics,0-shot,accuracy,0.3245614035087719,0.0440455615737476
jb723/llama2-ko-7B-model,mmlu_miscellaneous,0-shot,accuracy,0.6360153256704981,0.0172056848090322
jb723/llama2-ko-7B-model,mmlu_marketing,0-shot,accuracy,0.6623931623931624,0.0309802969926185
jb723/llama2-ko-7B-model,mmlu_management,0-shot,accuracy,0.5825242718446602,0.0488284054821223
jb723/llama2-ko-7B-model,mmlu_nutrition,0-shot,accuracy,0.4803921568627451,0.028607893699576
jb723/llama2-ko-7B-model,mmlu_medical_genetics,0-shot,accuracy,0.56,0.0498887651569858
jb723/llama2-ko-7B-model,mmlu_human_aging,0-shot,accuracy,0.5336322869955157,0.033481800170603
jb723/llama2-ko-7B-model,mmlu_professional_medicine,0-shot,accuracy,0.4338235294117647,0.0301056365700166
jb723/llama2-ko-7B-model,mmlu_college_medicine,0-shot,accuracy,0.4335260115606936,0.0377862107909205
jb723/llama2-ko-7B-model,mmlu_business_ethics,0-shot,accuracy,0.48,0.0502116731568677
jb723/llama2-ko-7B-model,mmlu_clinical_knowledge,0-shot,accuracy,0.4679245283018868,0.0307094869925565
jb723/llama2-ko-7B-model,mmlu_global_facts,0-shot,accuracy,0.39,0.0490207130000197
jb723/llama2-ko-7B-model,mmlu_virology,0-shot,accuracy,0.4397590361445783,0.0386413992369912
jb723/llama2-ko-7B-model,mmlu_professional_accounting,0-shot,accuracy,0.3617021276595745,0.0286638201471994
jb723/llama2-ko-7B-model,mmlu_college_physics,0-shot,accuracy,0.1470588235294117,0.0352406895156744
jb723/llama2-ko-7B-model,mmlu_high_school_physics,0-shot,accuracy,0.3178807947019867,0.038020397601079
jb723/llama2-ko-7B-model,mmlu_high_school_biology,0-shot,accuracy,0.5,0.0284440061994287
jb723/llama2-ko-7B-model,mmlu_college_biology,0-shot,accuracy,0.4652777777777778,0.0417111585818161
jb723/llama2-ko-7B-model,mmlu_anatomy,0-shot,accuracy,0.4444444444444444,0.0429259671825698
jb723/llama2-ko-7B-model,mmlu_college_chemistry,0-shot,accuracy,0.37,0.048523658709391
jb723/llama2-ko-7B-model,mmlu_computer_security,0-shot,accuracy,0.51,0.0502418393795691
jb723/llama2-ko-7B-model,mmlu_college_computer_science,0-shot,accuracy,0.35,0.0479372485441101
jb723/llama2-ko-7B-model,mmlu_astronomy,0-shot,accuracy,0.4407894736842105,0.0404031106249043
jb723/llama2-ko-7B-model,mmlu_college_mathematics,0-shot,accuracy,0.33,0.047258156262526
jb723/llama2-ko-7B-model,mmlu_conceptual_physics,0-shot,accuracy,0.4,0.0320256307610173
jb723/llama2-ko-7B-model,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.0460566186471838
jb723/llama2-ko-7B-model,mmlu_high_school_computer_science,0-shot,accuracy,0.41,0.049431107042371
jb723/llama2-ko-7B-model,mmlu_machine_learning,0-shot,accuracy,0.3214285714285714,0.0443280405529151
jb723/llama2-ko-7B-model,mmlu_high_school_chemistry,0-shot,accuracy,0.3201970443349753,0.0328264938530415
jb723/llama2-ko-7B-model,mmlu_high_school_statistics,0-shot,accuracy,0.2824074074074074,0.0307013721115109
jb723/llama2-ko-7B-model,mmlu_elementary_mathematics,0-shot,accuracy,0.3174603174603174,0.023973861998992
jb723/llama2-ko-7B-model,mmlu_electrical_engineering,0-shot,accuracy,0.4758620689655172,0.0416180850350153
jb723/llama2-ko-7B-model,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.0264661175389599
jb723/llama2-ko-7B-model,arc_challenge,25-shot,accuracy,0.5136518771331058,0.0146059434298609
jb723/llama2-ko-7B-model,arc_challenge,25-shot,acc_norm,0.53839590443686,0.0145682455502963
jb723/llama2-ko-7B-model,truthfulqa_mc2,0-shot,accuracy,0.4095609976995192,0.0155442492936554
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,bleu_max,20.89104935353322,0.690580917763846
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,bleu_acc,0.3439412484700122,0.0166290875142768
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,bleu_diff,-4.982714805919928,0.6525563274154834
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge1_max,46.0179025495015,0.8157872400574628
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge1_acc,0.3414932680538555,0.0166006886199508
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge1_diff,-7.817434391713708,0.7537330659697064
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge2_max,30.28488318154152,0.885344085472441
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge2_acc,0.2913096695226438,0.0159059870481848
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge2_diff,-8.624090028516706,0.8953473056334376
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rougeL_max,42.39510902130912,0.8131112049425046
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rougeL_acc,0.3133414932680538,0.0162380650690595
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rougeL_diff,-8.169098689998256,0.7490350953113237
jb723/llama2-ko-7B-model,truthfulqa_mc1,0-shot,accuracy,0.2876376988984088,0.0158463151013947
cerebras/Cerebras-GPT-1.3B,arc:challenge,25-shot,accuracy,0.2372013651877133,0.0124303998292608
cerebras/Cerebras-GPT-1.3B,arc:challenge,25-shot,acc_norm,0.2627986348122867,0.0128625231753513
cerebras/Cerebras-GPT-1.3B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.0435194139889244
cerebras/Cerebras-GPT-1.3B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.0435194139889244
cerebras/Cerebras-GPT-1.3B,hendrycksTest-anatomy,5-shot,accuracy,0.2074074074074074,0.0350255317067831
cerebras/Cerebras-GPT-1.3B,hendrycksTest-anatomy,5-shot,acc_norm,0.2074074074074074,0.0350255317067831
cerebras/Cerebras-GPT-1.3B,hendrycksTest-astronomy,5-shot,accuracy,0.2105263157894736,0.0331767278753315
cerebras/Cerebras-GPT-1.3B,hendrycksTest-astronomy,5-shot,acc_norm,0.2105263157894736,0.0331767278753315
cerebras/Cerebras-GPT-1.3B,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.0416333199893226
cerebras/Cerebras-GPT-1.3B,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.0416333199893226
cerebras/Cerebras-GPT-1.3B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.0272572603224948
cerebras/Cerebras-GPT-1.3B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.0272572603224948
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008034
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_biology,5-shot,acc_norm,0.2430555555555555,0.0358687928008034
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_computer_science,5-shot,accuracy,0.34,0.0476095228569523
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.34,0.0476095228569523
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225943
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.0336876293225943
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_physics,5-shot,acc_norm,0.2254901960784313,0.0415830753308328
cerebras/Cerebras-GPT-1.3B,hendrycksTest-computer_security,5-shot,accuracy,0.21,0.0409360180740332
cerebras/Cerebras-GPT-1.3B,hendrycksTest-computer_security,5-shot,acc_norm,0.21,0.0409360180740332
cerebras/Cerebras-GPT-1.3B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.0292418838696288
cerebras/Cerebras-GPT-1.3B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.0292418838696288
cerebras/Cerebras-GPT-1.3B,hendrycksTest-econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
cerebras/Cerebras-GPT-1.3B,hendrycksTest-econometrics,5-shot,acc_norm,0.2456140350877192,0.0404933929774814
cerebras/Cerebras-GPT-1.3B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2344827586206896,0.0353062587434659
cerebras/Cerebras-GPT-1.3B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2344827586206896,0.0353062587434659
cerebras/Cerebras-GPT-1.3B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2407407407407407,0.0220190800122178
cerebras/Cerebras-GPT-1.3B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2407407407407407,0.0220190800122178
cerebras/Cerebras-GPT-1.3B,hendrycksTest-formal_logic,5-shot,accuracy,0.2619047619047619,0.0393253768039287
cerebras/Cerebras-GPT-1.3B,hendrycksTest-formal_logic,5-shot,acc_norm,0.2619047619047619,0.0393253768039287
cerebras/Cerebras-GPT-1.3B,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
cerebras/Cerebras-GPT-1.3B,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2032258064516129,0.0228916879845549
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2032258064516129,0.0228916879845549
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2266009852216748,0.0294548638352929
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2266009852216748,0.0294548638352929
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.0456048021572068
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.0456048021572068
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2606060606060606,0.0342774317581652
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2606060606060606,0.0342774317581652
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_geography,5-shot,accuracy,0.3131313131313131,0.0330420508781365
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3131313131313131,0.0330420508781365
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3471502590673575,0.0343569616836135
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3471502590673575,0.0343569616836135
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3230769230769231,0.0237108885019705
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3230769230769231,0.0237108885019705
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2592592592592592,0.0267192407837121
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2226890756302521,0.0270254334988823
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2226890756302521,0.0270254334988823
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_physics,5-shot,accuracy,0.23841059602649,0.0347918557259965
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23841059602649,0.0347918557259965
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3431192660550459,0.020354777736086
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3431192660550459,0.020354777736086
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2205882352941176,0.029102254389674
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2205882352941176,0.029102254389674
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2742616033755274,0.029041333510598
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_aging,5-shot,accuracy,0.2331838565022421,0.0283803911470947
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_aging,5-shot,acc_norm,0.2331838565022421,0.0283803911470947
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
cerebras/Cerebras-GPT-1.3B,hendrycksTest-international_law,5-shot,accuracy,0.2809917355371901,0.0410320383051451
cerebras/Cerebras-GPT-1.3B,hendrycksTest-international_law,5-shot,acc_norm,0.2809917355371901,0.0410320383051451
cerebras/Cerebras-GPT-1.3B,hendrycksTest-jurisprudence,5-shot,accuracy,0.2407407407407407,0.0413311944024383
cerebras/Cerebras-GPT-1.3B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2407407407407407,0.0413311944024383
cerebras/Cerebras-GPT-1.3B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3006134969325153,0.0360251131880677
cerebras/Cerebras-GPT-1.3B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3006134969325153,0.0360251131880677
cerebras/Cerebras-GPT-1.3B,hendrycksTest-machine_learning,5-shot,accuracy,0.3035714285714285,0.0436422615584104
cerebras/Cerebras-GPT-1.3B,hendrycksTest-machine_learning,5-shot,acc_norm,0.3035714285714285,0.0436422615584104
cerebras/Cerebras-GPT-1.3B,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.0391666776282258
cerebras/Cerebras-GPT-1.3B,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.0391666776282258
cerebras/Cerebras-GPT-1.3B,hendrycksTest-marketing,5-shot,accuracy,0.2948717948717949,0.0298725777088911
cerebras/Cerebras-GPT-1.3B,hendrycksTest-marketing,5-shot,acc_norm,0.2948717948717949,0.0298725777088911
cerebras/Cerebras-GPT-1.3B,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.0451260859854212
cerebras/Cerebras-GPT-1.3B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.0451260859854212
cerebras/Cerebras-GPT-1.3B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2464878671775223,0.0154113087696869
cerebras/Cerebras-GPT-1.3B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2464878671775223,0.0154113087696869
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2368715083798882,0.0142195707881039
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2368715083798882,0.0142195707881039
cerebras/Cerebras-GPT-1.3B,hendrycksTest-nutrition,5-shot,accuracy,0.2941176470588235,0.026090162504279
cerebras/Cerebras-GPT-1.3B,hendrycksTest-nutrition,5-shot,acc_norm,0.2941176470588235,0.026090162504279
cerebras/Cerebras-GPT-1.3B,hendrycksTest-philosophy,5-shot,accuracy,0.2636655948553054,0.0250255385005323
cerebras/Cerebras-GPT-1.3B,hendrycksTest-philosophy,5-shot,acc_norm,0.2636655948553054,0.0250255385005323
cerebras/Cerebras-GPT-1.3B,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.0245692236004608
cerebras/Cerebras-GPT-1.3B,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.0245692236004608
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.0267891723511402
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_law,5-shot,accuracy,0.2379400260756193,0.0108757007876942
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_law,5-shot,acc_norm,0.2379400260756193,0.0108757007876942
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_medicine,5-shot,accuracy,0.4191176470588235,0.0299728071704646
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4191176470588235,0.0299728071704646
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2777777777777778,0.0181202242514845
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2777777777777778,0.0181202242514845
cerebras/Cerebras-GPT-1.3B,hendrycksTest-public_relations,5-shot,accuracy,0.1636363636363636,0.0354343305429867
cerebras/Cerebras-GPT-1.3B,hendrycksTest-public_relations,5-shot,acc_norm,0.1636363636363636,0.0354343305429867
cerebras/Cerebras-GPT-1.3B,hendrycksTest-security_studies,5-shot,accuracy,0.236734693877551,0.0272128358840731
cerebras/Cerebras-GPT-1.3B,hendrycksTest-security_studies,5-shot,acc_norm,0.236734693877551,0.0272128358840731
cerebras/Cerebras-GPT-1.3B,hendrycksTest-sociology,5-shot,accuracy,0.2288557213930348,0.0297052840567724
cerebras/Cerebras-GPT-1.3B,hendrycksTest-sociology,5-shot,acc_norm,0.2288557213930348,0.0297052840567724
cerebras/Cerebras-GPT-1.3B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.19,0.0394277244403662
cerebras/Cerebras-GPT-1.3B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.19,0.0394277244403662
cerebras/Cerebras-GPT-1.3B,hendrycksTest-virology,5-shot,accuracy,0.3313253012048193,0.0366431477728808
cerebras/Cerebras-GPT-1.3B,hendrycksTest-virology,5-shot,acc_norm,0.3313253012048193,0.0366431477728808
cerebras/Cerebras-GPT-1.3B,hendrycksTest-world_religions,5-shot,accuracy,0.3040935672514619,0.0352821125824523
cerebras/Cerebras-GPT-1.3B,hendrycksTest-world_religions,5-shot,acc_norm,0.3040935672514619,0.0352821125824523
cerebras/Cerebras-GPT-1.3B,truthfulqa:mc,0-shot,mc1,0.244798041615667,0.015051869486715
cerebras/Cerebras-GPT-1.3B,truthfulqa:mc,0-shot,mc2,0.4269871364718513,0.0148972487230952
cerebras/Cerebras-GPT-1.3B,drop,3-shot,accuracy,0.0004194630872483,0.0002096985470782
cerebras/Cerebras-GPT-1.3B,drop,3-shot,f1,0.0369620385906041,0.0010536462556224
Monero/Manticore-13b-Chat-Pyg-Guanaco,drop,3-shot,accuracy,0.1636954697986577,0.0037891361135837
Monero/Manticore-13b-Chat-Pyg-Guanaco,drop,3-shot,f1,0.2562237835570473,0.003909791858313
Monero/Manticore-13b-Chat-Pyg-Guanaco,gsm8k,5-shot,accuracy,0.2289613343442001,0.0115734128924182
Monero/Manticore-13b-Chat-Pyg-Guanaco,winogrande,5-shot,accuracy,0.739542225730071,0.0123348336719982
Monero/Manticore-13b-Chat-Pyg-Guanaco,arc:challenge,25-shot,accuracy,0.5290102389078498,0.0145867763552943
Monero/Manticore-13b-Chat-Pyg-Guanaco,arc:challenge,25-shot,acc_norm,0.568259385665529,0.0144745914271961
Monero/Manticore-13b-Chat-Pyg-Guanaco,hellaswag,10-shot,accuracy,0.6149173471420036,0.0048562033747154
Monero/Manticore-13b-Chat-Pyg-Guanaco,hellaswag,10-shot,acc_norm,0.823043218482374,0.0038085217687699
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-abstract_algebra,5-shot,accuracy,0.4,0.049236596391733
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.4,0.049236596391733
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-anatomy,5-shot,accuracy,0.4740740740740741,0.0431353169675057
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-anatomy,5-shot,acc_norm,0.4740740740740741,0.0431353169675057
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-astronomy,5-shot,accuracy,0.4736842105263157,0.0406330273148667
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-astronomy,5-shot,acc_norm,0.4736842105263157,0.0406330273148667
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-business_ethics,5-shot,accuracy,0.45,0.05
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-business_ethics,5-shot,acc_norm,0.45,0.05
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.5056603773584906,0.0307709007638513
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.5056603773584906,0.0307709007638513
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_biology,5-shot,accuracy,0.5069444444444444,0.0418080675029493
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_biology,5-shot,acc_norm,0.5069444444444444,0.0418080675029493
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.0464823198711731
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.0464823198711731
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_computer_science,5-shot,accuracy,0.43,0.0497569851956242
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_computer_science,5-shot,acc_norm,0.43,0.0497569851956242
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_mathematics,5-shot,accuracy,0.32,0.046882617226215
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_mathematics,5-shot,acc_norm,0.32,0.046882617226215
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_medicine,5-shot,accuracy,0.3757225433526011,0.0369282076726486
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_medicine,5-shot,acc_norm,0.3757225433526011,0.0369282076726486
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_physics,5-shot,accuracy,0.2352941176470588,0.0422077365917145
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_physics,5-shot,acc_norm,0.2352941176470588,0.0422077365917145
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-computer_security,5-shot,accuracy,0.6,0.049236596391733
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-computer_security,5-shot,acc_norm,0.6,0.049236596391733
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-conceptual_physics,5-shot,accuracy,0.4127659574468085,0.0321847114140035
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.4127659574468085,0.0321847114140035
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-econometrics,5-shot,accuracy,0.3508771929824561,0.044895393502707
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-econometrics,5-shot,acc_norm,0.3508771929824561,0.044895393502707
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3793103448275862,0.0404346186191674
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3793103448275862,0.0404346186191674
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.291005291005291,0.0233938265004848
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.291005291005291,0.0233938265004848
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-formal_logic,5-shot,accuracy,0.3015873015873015,0.0410494726990339
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-formal_logic,5-shot,acc_norm,0.3015873015873015,0.0410494726990339
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.0482418151324421
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.0482418151324421
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_biology,5-shot,accuracy,0.5129032258064516,0.0284345331526818
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_biology,5-shot,acc_norm,0.5129032258064516,0.0284345331526818
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3448275862068966,0.0334428374428045
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3448275862068966,0.0334428374428045
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.49,0.0502418393795691
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.49,0.0502418393795691
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_european_history,5-shot,accuracy,0.5515151515151515,0.0388356597795692
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.5515151515151515,0.0388356597795692
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_geography,5-shot,accuracy,0.5808080808080808,0.0351552072867041
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5808080808080808,0.0351552072867041
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6839378238341969,0.0335539736968617
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6839378238341969,0.0335539736968617
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4128205128205128,0.0249626835643318
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4128205128205128,0.0249626835643318
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2962962962962963,0.0278408114958719
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2962962962962963,0.0278408114958719
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.4873949579831932,0.0324681676575217
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.4873949579831932,0.0324681676575217
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_physics,5-shot,accuracy,0.2582781456953642,0.0357370531476345
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2582781456953642,0.0357370531476345
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6293577981651376,0.0207074581643529
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6293577981651376,0.0207074581643529
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3009259259259259,0.0312803908432988
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3009259259259259,0.0312803908432988
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_us_history,5-shot,accuracy,0.6323529411764706,0.0338413204567411
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.6323529411764706,0.0338413204567411
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6371308016877637,0.0312992082553021
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6371308016877637,0.0312992082553021
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_aging,5-shot,accuracy,0.5919282511210763,0.0329857460784282
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_aging,5-shot,acc_norm,0.5919282511210763,0.0329857460784282
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_sexuality,5-shot,accuracy,0.5267175572519084,0.0437902493655389
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5267175572519084,0.0437902493655389
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-international_law,5-shot,accuracy,0.5702479338842975,0.0451908202131977
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-international_law,5-shot,acc_norm,0.5702479338842975,0.0451908202131977
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-jurisprudence,5-shot,accuracy,0.6018518518518519,0.0473233261597881
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-jurisprudence,5-shot,acc_norm,0.6018518518518519,0.0473233261597881
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-logical_fallacies,5-shot,accuracy,0.5214723926380368,0.0392474687675112
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.5214723926380368,0.0392474687675112
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-machine_learning,5-shot,accuracy,0.3571428571428571,0.0454796099976437
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-machine_learning,5-shot,acc_norm,0.3571428571428571,0.0454796099976437
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-management,5-shot,accuracy,0.6407766990291263,0.0475045839904169
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-management,5-shot,acc_norm,0.6407766990291263,0.0475045839904169
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-marketing,5-shot,accuracy,0.7222222222222222,0.0293431147980944
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-marketing,5-shot,acc_norm,0.7222222222222222,0.0293431147980944
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-medical_genetics,5-shot,accuracy,0.47,0.0501613558046592
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-medical_genetics,5-shot,acc_norm,0.47,0.0501613558046592
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-miscellaneous,5-shot,accuracy,0.6551724137931034,0.0169971233461134
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6551724137931034,0.0169971233461134
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_disputes,5-shot,accuracy,0.5317919075144508,0.0268646243667566
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_disputes,5-shot,acc_norm,0.5317919075144508,0.0268646243667566
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2625698324022346,0.0147168242730177
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2625698324022346,0.0147168242730177
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-nutrition,5-shot,accuracy,0.4705882352941176,0.0285803410651382
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-nutrition,5-shot,acc_norm,0.4705882352941176,0.0285803410651382
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-philosophy,5-shot,accuracy,0.5080385852090032,0.0283944213709845
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-philosophy,5-shot,acc_norm,0.5080385852090032,0.0283944213709845
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-prehistory,5-shot,accuracy,0.5648148148148148,0.0275860062216077
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-prehistory,5-shot,acc_norm,0.5648148148148148,0.0275860062216077
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_accounting,5-shot,accuracy,0.4078014184397163,0.0293160117763435
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4078014184397163,0.0293160117763435
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_law,5-shot,accuracy,0.3878748370273794,0.0124449983096756
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_law,5-shot,acc_norm,0.3878748370273794,0.0124449983096756
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_medicine,5-shot,accuracy,0.4816176470588235,0.0303523033953519
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4816176470588235,0.0303523033953519
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_psychology,5-shot,accuracy,0.4640522875816993,0.020175488765484
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_psychology,5-shot,acc_norm,0.4640522875816993,0.020175488765484
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-public_relations,5-shot,accuracy,0.5636363636363636,0.0475018505890729
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-public_relations,5-shot,acc_norm,0.5636363636363636,0.0475018505890729
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-security_studies,5-shot,accuracy,0.5224489795918368,0.0319769411871367
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-security_studies,5-shot,acc_norm,0.5224489795918368,0.0319769411871367
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-sociology,5-shot,accuracy,0.5671641791044776,0.0350349092367328
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-sociology,5-shot,acc_norm,0.5671641791044776,0.0350349092367328
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.76,0.0429234695990928
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.76,0.0429234695990928
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-virology,5-shot,accuracy,0.4216867469879518,0.0384445318177091
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-virology,5-shot,acc_norm,0.4216867469879518,0.0384445318177091
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-world_religions,5-shot,accuracy,0.6842105263157895,0.0356507967070831
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-world_religions,5-shot,acc_norm,0.6842105263157895,0.0356507967070831
Monero/Manticore-13b-Chat-Pyg-Guanaco,truthfulqa:mc,0-shot,mc1,0.3647490820073439,0.0168509610617201
Monero/Manticore-13b-Chat-Pyg-Guanaco,truthfulqa:mc,0-shot,mc2,0.5228532383120019,0.0158776692139558
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_precalc,5-shot,accuracy,0.0183150183150183,0.0057436967316536
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_prealgebra,5-shot,accuracy,0.0619977037887485,0.0081757975120621
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_num_theory,5-shot,accuracy,0.024074074074074,0.0066022025098153
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_intermediate_algebra,5-shot,accuracy,0.0210409745293466,0.0047787236233196
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_geometry,5-shot,accuracy,0.0104384133611691,0.0046486271171846
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_counting_and_prob,5-shot,accuracy,0.0421940928270042,0.0092434482090771
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_algebra,5-shot,accuracy,0.0429654591406908,0.0058881815249496
Monero/Manticore-13b-Chat-Pyg-Guanaco,fld_default,0-shot,accuracy,0.0,
Monero/Manticore-13b-Chat-Pyg-Guanaco,fld_star,0-shot,accuracy,0.0,
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_3da,5-shot,accuracy,0.594,0.0109837298382917
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_3ds,5-shot,accuracy,0.2335,0.0094622218226434
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_4da,5-shot,accuracy,0.5165,0.0111770451448083
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_2ds,5-shot,accuracy,0.21,0.0091099668357175
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_5ds,5-shot,accuracy,0.0525,0.0049884183022857
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_5da,5-shot,accuracy,0.2795,0.0100369440131227
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_1dc,5-shot,accuracy,0.087,0.0063035995814963
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_4ds,5-shot,accuracy,0.192,0.0088094723679514
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_2dm,5-shot,accuracy,0.0375,0.0042492238057645
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_2da,5-shot,accuracy,0.652,0.0106538609140624
Monero/Manticore-13b-Chat-Pyg-Guanaco,gsm8k_cot,5-shot,accuracy,0.244882486732373,0.0118448190278636
Monero/Manticore-13b-Chat-Pyg-Guanaco,anli_r2,0-shot,brier_score,0.8852013154077675,
Monero/Manticore-13b-Chat-Pyg-Guanaco,anli_r3,0-shot,brier_score,0.7816093138893003,
Monero/Manticore-13b-Chat-Pyg-Guanaco,anli_r1,0-shot,brier_score,0.8665860062055024,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_eu,0-shot,brier_score,1.295309492627305,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_vi,0-shot,brier_score,1.028457872160094,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_ru,0-shot,brier_score,0.9424584802883822,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_zh,0-shot,brier_score,1.073220222743302,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_tr,0-shot,brier_score,1.1494729899079483,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_fr,0-shot,brier_score,0.8836516075827534,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_en,0-shot,brier_score,0.8049487003810644,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_ur,0-shot,brier_score,1.2526672854095398,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_ar,0-shot,brier_score,1.249788260932463,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_de,0-shot,brier_score,0.9308427131328508,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_hi,0-shot,brier_score,1.014295071171663,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_es,0-shot,brier_score,0.932183728871887,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_bg,0-shot,brier_score,0.9824751588952392,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_sw,0-shot,brier_score,1.0143704119021475,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_el,0-shot,brier_score,1.021583604470639,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_th,0-shot,brier_score,1.1706481918320806,
Monero/Manticore-13b-Chat-Pyg-Guanaco,logiqa2,0-shot,brier_score,1.0546915532913337,
Monero/Manticore-13b-Chat-Pyg-Guanaco,mathqa,0-shot,brier_score,0.998681916345562,
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_standard,0-shot,perplexity,4.071811300991231,0.1190874112994679
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_standard,0-shot,accuracy,0.6567048321366195,0.0066150179044336
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_openai,0-shot,perplexity,3.103419189766064,0.0808779815613
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_openai,0-shot,accuracy,0.7155055307587813,0.0062857265569445
mosaicml/mpt-30b,arc:challenge,25-shot,accuracy,0.5290102389078498,0.0145867763552943
mosaicml/mpt-30b,arc:challenge,25-shot,acc_norm,0.5597269624573379,0.0145067695248042
mosaicml/mpt-30b,hellaswag,10-shot,accuracy,0.6195976897032464,0.0048449353275992
mosaicml/mpt-30b,hellaswag,10-shot,acc_norm,0.8242381995618403,0.0037983950550215
mosaicml/mpt-30b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.0464823198711731
mosaicml/mpt-30b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.0464823198711731
mosaicml/mpt-30b,hendrycksTest-anatomy,5-shot,accuracy,0.4888888888888889,0.0431827549197797
mosaicml/mpt-30b,hendrycksTest-anatomy,5-shot,acc_norm,0.4888888888888889,0.0431827549197797
mosaicml/mpt-30b,hendrycksTest-astronomy,5-shot,accuracy,0.4078947368421052,0.0399930971277747
mosaicml/mpt-30b,hendrycksTest-astronomy,5-shot,acc_norm,0.4078947368421052,0.0399930971277747
mosaicml/mpt-30b,hendrycksTest-business_ethics,5-shot,accuracy,0.53,0.0501613558046591
mosaicml/mpt-30b,hendrycksTest-business_ethics,5-shot,acc_norm,0.53,0.0501613558046591
mosaicml/mpt-30b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4867924528301887,0.0307621348745004
mosaicml/mpt-30b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4867924528301887,0.0307621348745004
mosaicml/mpt-30b,hendrycksTest-college_biology,5-shot,accuracy,0.5138888888888888,0.04179596617581
mosaicml/mpt-30b,hendrycksTest-college_biology,5-shot,acc_norm,0.5138888888888888,0.04179596617581
mosaicml/mpt-30b,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.0464823198711731
mosaicml/mpt-30b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.0464823198711731
mosaicml/mpt-30b,hendrycksTest-college_computer_science,5-shot,accuracy,0.42,0.0496044963748858
mosaicml/mpt-30b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.42,0.0496044963748858
mosaicml/mpt-30b,hendrycksTest-college_mathematics,5-shot,accuracy,0.35,0.0479372485441102
mosaicml/mpt-30b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.35,0.0479372485441102
mosaicml/mpt-30b,hendrycksTest-college_medicine,5-shot,accuracy,0.4508670520231214,0.0379401267469702
mosaicml/mpt-30b,hendrycksTest-college_medicine,5-shot,acc_norm,0.4508670520231214,0.0379401267469702
mosaicml/mpt-30b,hendrycksTest-college_physics,5-shot,accuracy,0.3039215686274509,0.0457666540320776
mosaicml/mpt-30b,hendrycksTest-college_physics,5-shot,acc_norm,0.3039215686274509,0.0457666540320776
mosaicml/mpt-30b,hendrycksTest-computer_security,5-shot,accuracy,0.6,0.049236596391733
mosaicml/mpt-30b,hendrycksTest-computer_security,5-shot,acc_norm,0.6,0.049236596391733
mosaicml/mpt-30b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.4170212765957446,0.0322327626671171
mosaicml/mpt-30b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.4170212765957446,0.0322327626671171
mosaicml/mpt-30b,hendrycksTest-econometrics,5-shot,accuracy,0.2982456140350877,0.0430368403353731
mosaicml/mpt-30b,hendrycksTest-econometrics,5-shot,acc_norm,0.2982456140350877,0.0430368403353731
mosaicml/mpt-30b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5172413793103449,0.0416418872016937
mosaicml/mpt-30b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5172413793103449,0.0416418872016937
mosaicml/mpt-30b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3306878306878307,0.024229965298425
mosaicml/mpt-30b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3306878306878307,0.024229965298425
mosaicml/mpt-30b,hendrycksTest-formal_logic,5-shot,accuracy,0.2857142857142857,0.0404061017820884
mosaicml/mpt-30b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2857142857142857,0.0404061017820884
mosaicml/mpt-30b,hendrycksTest-global_facts,5-shot,accuracy,0.38,0.0487831731214563
mosaicml/mpt-30b,hendrycksTest-global_facts,5-shot,acc_norm,0.38,0.0487831731214563
mosaicml/mpt-30b,hendrycksTest-high_school_biology,5-shot,accuracy,0.5419354838709678,0.0283437872505406
mosaicml/mpt-30b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.5419354838709678,0.0283437872505406
mosaicml/mpt-30b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.354679802955665,0.0336612448905145
mosaicml/mpt-30b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.354679802955665,0.0336612448905145
mosaicml/mpt-30b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.53,0.0501613558046591
mosaicml/mpt-30b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.53,0.0501613558046591
mosaicml/mpt-30b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.6,0.0382546027838002
mosaicml/mpt-30b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.6,0.0382546027838002
mosaicml/mpt-30b,hendrycksTest-high_school_geography,5-shot,accuracy,0.5959595959595959,0.0349613097205612
mosaicml/mpt-30b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5959595959595959,0.0349613097205612
mosaicml/mpt-30b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6476683937823834,0.0344747828641435
mosaicml/mpt-30b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6476683937823834,0.0344747828641435
mosaicml/mpt-30b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4692307692307692,0.0253029588908501
mosaicml/mpt-30b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4692307692307692,0.0253029588908501
mosaicml/mpt-30b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2814814814814815,0.0274200193509452
mosaicml/mpt-30b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2814814814814815,0.0274200193509452
mosaicml/mpt-30b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.4411764705882353,0.0322529423239964
mosaicml/mpt-30b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.4411764705882353,0.0322529423239964
mosaicml/mpt-30b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2980132450331126,0.0373453567678719
mosaicml/mpt-30b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2980132450331126,0.0373453567678719
mosaicml/mpt-30b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6770642201834862,0.0200481159234153
mosaicml/mpt-30b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6770642201834862,0.0200481159234153
mosaicml/mpt-30b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3657407407407407,0.032847388576472
mosaicml/mpt-30b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3657407407407407,0.032847388576472
mosaicml/mpt-30b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.6666666666666666,0.0330861111323643
mosaicml/mpt-30b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.6666666666666666,0.0330861111323643
mosaicml/mpt-30b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6708860759493671,0.0305873262947023
mosaicml/mpt-30b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6708860759493671,0.0305873262947023
mosaicml/mpt-30b,hendrycksTest-human_aging,5-shot,accuracy,0.5381165919282511,0.0334601501197322
mosaicml/mpt-30b,hendrycksTest-human_aging,5-shot,acc_norm,0.5381165919282511,0.0334601501197322
mosaicml/mpt-30b,hendrycksTest-human_sexuality,5-shot,accuracy,0.5343511450381679,0.0437492856059973
mosaicml/mpt-30b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5343511450381679,0.0437492856059973
mosaicml/mpt-30b,hendrycksTest-international_law,5-shot,accuracy,0.4297520661157025,0.0451908202131977
mosaicml/mpt-30b,hendrycksTest-international_law,5-shot,acc_norm,0.4297520661157025,0.0451908202131977
mosaicml/mpt-30b,hendrycksTest-jurisprudence,5-shot,accuracy,0.4722222222222222,0.0482621729413989
mosaicml/mpt-30b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.4722222222222222,0.0482621729413989
mosaicml/mpt-30b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.4785276073619632,0.0392474687675113
mosaicml/mpt-30b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.4785276073619632,0.0392474687675113
mosaicml/mpt-30b,hendrycksTest-machine_learning,5-shot,accuracy,0.4553571428571428,0.047268355537191
mosaicml/mpt-30b,hendrycksTest-machine_learning,5-shot,acc_norm,0.4553571428571428,0.047268355537191
mosaicml/mpt-30b,hendrycksTest-management,5-shot,accuracy,0.5631067961165048,0.0491114710736577
mosaicml/mpt-30b,hendrycksTest-management,5-shot,acc_norm,0.5631067961165048,0.0491114710736577
mosaicml/mpt-30b,hendrycksTest-marketing,5-shot,accuracy,0.7136752136752137,0.0296143236904566
mosaicml/mpt-30b,hendrycksTest-marketing,5-shot,acc_norm,0.7136752136752137,0.0296143236904566
mosaicml/mpt-30b,hendrycksTest-medical_genetics,5-shot,accuracy,0.5,0.0502518907629606
mosaicml/mpt-30b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.5,0.0502518907629606
mosaicml/mpt-30b,hendrycksTest-miscellaneous,5-shot,accuracy,0.6871008939974457,0.016580935940304
mosaicml/mpt-30b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6871008939974457,0.016580935940304
mosaicml/mpt-30b,hendrycksTest-moral_disputes,5-shot,accuracy,0.5115606936416185,0.0269118986863779
mosaicml/mpt-30b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.5115606936416185,0.0269118986863779
mosaicml/mpt-30b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2692737430167597,0.0148356165828826
mosaicml/mpt-30b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2692737430167597,0.0148356165828826
mosaicml/mpt-30b,hendrycksTest-nutrition,5-shot,accuracy,0.5098039215686274,0.0286244125501679
mosaicml/mpt-30b,hendrycksTest-nutrition,5-shot,acc_norm,0.5098039215686274,0.0286244125501679
mosaicml/mpt-30b,hendrycksTest-philosophy,5-shot,accuracy,0.5466237942122186,0.0282743598548942
mosaicml/mpt-30b,hendrycksTest-philosophy,5-shot,acc_norm,0.5466237942122186,0.0282743598548942
mosaicml/mpt-30b,hendrycksTest-prehistory,5-shot,accuracy,0.5679012345679012,0.0275630109716066
mosaicml/mpt-30b,hendrycksTest-prehistory,5-shot,acc_norm,0.5679012345679012,0.0275630109716066
mosaicml/mpt-30b,hendrycksTest-professional_accounting,5-shot,accuracy,0.3617021276595745,0.0286638201471994
mosaicml/mpt-30b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3617021276595745,0.0286638201471994
mosaicml/mpt-30b,hendrycksTest-professional_law,5-shot,accuracy,0.378096479791395,0.012384878406798
mosaicml/mpt-30b,hendrycksTest-professional_law,5-shot,acc_norm,0.378096479791395,0.012384878406798
mosaicml/mpt-30b,hendrycksTest-professional_medicine,5-shot,accuracy,0.3823529411764705,0.0295200956976877
mosaicml/mpt-30b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3823529411764705,0.0295200956976877
mosaicml/mpt-30b,hendrycksTest-professional_psychology,5-shot,accuracy,0.4526143790849673,0.0201367909184925
mosaicml/mpt-30b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.4526143790849673,0.0201367909184925
mosaicml/mpt-30b,hendrycksTest-public_relations,5-shot,accuracy,0.5727272727272728,0.0473819870354548
mosaicml/mpt-30b,hendrycksTest-public_relations,5-shot,acc_norm,0.5727272727272728,0.0473819870354548
mosaicml/mpt-30b,hendrycksTest-security_studies,5-shot,accuracy,0.5510204081632653,0.0318421386668757
mosaicml/mpt-30b,hendrycksTest-security_studies,5-shot,acc_norm,0.5510204081632653,0.0318421386668757
mosaicml/mpt-30b,hendrycksTest-sociology,5-shot,accuracy,0.5472636815920398,0.0351970271757691
mosaicml/mpt-30b,hendrycksTest-sociology,5-shot,acc_norm,0.5472636815920398,0.0351970271757691
mosaicml/mpt-30b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.67,0.047258156262526
mosaicml/mpt-30b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.67,0.047258156262526
mosaicml/mpt-30b,hendrycksTest-virology,5-shot,accuracy,0.4457831325301205,0.038695433234721
mosaicml/mpt-30b,hendrycksTest-virology,5-shot,acc_norm,0.4457831325301205,0.038695433234721
mosaicml/mpt-30b,hendrycksTest-world_religions,5-shot,accuracy,0.6783625730994152,0.0358252944257312
mosaicml/mpt-30b,hendrycksTest-world_religions,5-shot,acc_norm,0.6783625730994152,0.0358252944257312
mosaicml/mpt-30b,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.0153218216884761
mosaicml/mpt-30b,truthfulqa:mc,0-shot,mc2,0.3841558252351552,0.013607507438444
mosaicml/mpt-30b,winogrande,5-shot,accuracy,0.7490134175217048,0.0121857762205161
mosaicml/mpt-30b,gsm8k,5-shot,accuracy,0.1690674753601213,0.0103241714454973
EleutherAI/gpt-neo-125M,mmlu_world_religions,0-shot,accuracy,0.2046783625730994,0.0309444597785332
EleutherAI/gpt-neo-125M,mmlu_formal_logic,0-shot,accuracy,0.246031746031746,0.0385227336492431
EleutherAI/gpt-neo-125M,mmlu_prehistory,0-shot,accuracy,0.2160493827160493,0.0228991629184457
EleutherAI/gpt-neo-125M,mmlu_moral_scenarios,0-shot,accuracy,0.2245810055865922,0.0139568036665446
EleutherAI/gpt-neo-125M,mmlu_high_school_world_history,0-shot,accuracy,0.2236286919831223,0.0271232982052299
EleutherAI/gpt-neo-125M,mmlu_moral_disputes,0-shot,accuracy,0.2485549132947976,0.0232675284321001
EleutherAI/gpt-neo-125M,mmlu_professional_law,0-shot,accuracy,0.2470664928292047,0.0110157522552793
EleutherAI/gpt-neo-125M,mmlu_logical_fallacies,0-shot,accuracy,0.2453987730061349,0.0338093981394335
EleutherAI/gpt-neo-125M,mmlu_high_school_us_history,0-shot,accuracy,0.2990196078431372,0.0321332571737361
EleutherAI/gpt-neo-125M,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/gpt-neo-125M,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
EleutherAI/gpt-neo-125M,mmlu_international_law,0-shot,accuracy,0.2231404958677686,0.0380075447522873
EleutherAI/gpt-neo-125M,mmlu_high_school_european_history,0-shot,accuracy,0.2484848484848484,0.033744026441394
EleutherAI/gpt-neo-125M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3575129533678756,0.03458816042181
EleutherAI/gpt-neo-125M,mmlu_high_school_microeconomics,0-shot,accuracy,0.2310924369747899,0.0273814069278689
EleutherAI/gpt-neo-125M,mmlu_high_school_geography,0-shot,accuracy,0.3636363636363636,0.0342730865299993
EleutherAI/gpt-neo-125M,mmlu_high_school_psychology,0-shot,accuracy,0.2788990825688073,0.0192274688764635
EleutherAI/gpt-neo-125M,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.0417234303870538
EleutherAI/gpt-neo-125M,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.0446196043338473
EleutherAI/gpt-neo-125M,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/gpt-neo-125M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2512820512820513,0.0219920166623705
EleutherAI/gpt-neo-125M,mmlu_security_studies,0-shot,accuracy,0.4,0.0313625024093589
EleutherAI/gpt-neo-125M,mmlu_professional_psychology,0-shot,accuracy,0.2712418300653594,0.0179866153040302
EleutherAI/gpt-neo-125M,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/gpt-neo-125M,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neo-125M,mmlu_miscellaneous,0-shot,accuracy,0.2401021711366538,0.0152746852137341
EleutherAI/gpt-neo-125M,mmlu_marketing,0-shot,accuracy,0.2692307692307692,0.0290585883037488
EleutherAI/gpt-neo-125M,mmlu_management,0-shot,accuracy,0.2524271844660194,0.0430125039969087
EleutherAI/gpt-neo-125M,mmlu_nutrition,0-shot,accuracy,0.2712418300653594,0.0254577566966678
EleutherAI/gpt-neo-125M,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/gpt-neo-125M,mmlu_human_aging,0-shot,accuracy,0.2780269058295964,0.030069584874494
EleutherAI/gpt-neo-125M,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091215
EleutherAI/gpt-neo-125M,mmlu_college_medicine,0-shot,accuracy,0.1907514450867052,0.0299578513298693
EleutherAI/gpt-neo-125M,mmlu_business_ethics,0-shot,accuracy,0.23,0.042295258468165
EleutherAI/gpt-neo-125M,mmlu_clinical_knowledge,0-shot,accuracy,0.2566037735849056,0.0268806478890519
EleutherAI/gpt-neo-125M,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/gpt-neo-125M,mmlu_virology,0-shot,accuracy,0.1987951807228915,0.0310693902607894
EleutherAI/gpt-neo-125M,mmlu_professional_accounting,0-shot,accuracy,0.2411347517730496,0.0255187310495377
EleutherAI/gpt-neo-125M,mmlu_college_physics,0-shot,accuracy,0.2254901960784313,0.0415830753308328
EleutherAI/gpt-neo-125M,mmlu_high_school_physics,0-shot,accuracy,0.3178807947019867,0.038020397601079
EleutherAI/gpt-neo-125M,mmlu_high_school_biology,0-shot,accuracy,0.2516129032258064,0.0246859792862399
EleutherAI/gpt-neo-125M,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.0371617743756601
EleutherAI/gpt-neo-125M,mmlu_anatomy,0-shot,accuracy,0.2740740740740741,0.03853254836552
EleutherAI/gpt-neo-125M,mmlu_college_chemistry,0-shot,accuracy,0.24,0.0429234695990928
EleutherAI/gpt-neo-125M,mmlu_computer_security,0-shot,accuracy,0.19,0.0394277244403662
EleutherAI/gpt-neo-125M,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
EleutherAI/gpt-neo-125M,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/gpt-neo-125M,mmlu_college_mathematics,0-shot,accuracy,0.27,0.0446196043338473
EleutherAI/gpt-neo-125M,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.0295131966255393
EleutherAI/gpt-neo-125M,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/gpt-neo-125M,mmlu_high_school_computer_science,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/gpt-neo-125M,mmlu_machine_learning,0-shot,accuracy,0.2678571428571428,0.0420327729146776
EleutherAI/gpt-neo-125M,mmlu_high_school_chemistry,0-shot,accuracy,0.2758620689655172,0.0314471258167824
EleutherAI/gpt-neo-125M,mmlu_high_school_statistics,0-shot,accuracy,0.4583333333333333,0.0339811089029463
EleutherAI/gpt-neo-125M,mmlu_elementary_mathematics,0-shot,accuracy,0.2407407407407407,0.0220190800122178
EleutherAI/gpt-neo-125M,mmlu_electrical_engineering,0-shot,accuracy,0.2344827586206896,0.0353062587434659
EleutherAI/gpt-neo-125M,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.0264661175389599
EleutherAI/gpt-neo-125M,arc_challenge,25-shot,accuracy,0.2013651877133105,0.0117189274774442
EleutherAI/gpt-neo-125M,arc_challenge,25-shot,acc_norm,0.2389078498293515,0.0124610713763166
EleutherAI/gpt-neo-125M,hellaswag,10-shot,accuracy,0.2849034056960765,0.0045044595539097
EleutherAI/gpt-neo-125M,hellaswag,10-shot,acc_norm,0.3020314678350926,0.0045820047447133
EleutherAI/gpt-neo-125M,truthfulqa_mc2,0-shot,accuracy,0.4557937072774711,0.0153993033391361
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,bleu_max,13.82766815700744,0.5154784183208384
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,bleu_acc,0.3916768665850673,0.0170877958817696
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,bleu_diff,-0.3071316802783103,0.480611755738601
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge1_max,34.23389894145755,0.7612633125881414
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge1_acc,0.3586291309669522,0.016789289499502
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge1_diff,-1.3810176678838904,0.7542591368961628
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge2_max,16.07909085959659,0.7862657983849908
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge2_acc,0.1909424724602203,0.0137592858426857
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge2_diff,-2.05928101776518,0.7203850974001921
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rougeL_max,31.690463611915124,0.7444436261611048
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rougeL_acc,0.3659730722154223,0.0168629416840883
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rougeL_diff,-0.9361203963395062,0.7449460193846943
EleutherAI/gpt-neo-125M,truthfulqa_mc1,0-shot,accuracy,0.2582619339045288,0.0153218216884761
EleutherAI/gpt-neo-125M,winogrande,5-shot,accuracy,0.510655090765588,0.0140492945362904
google/gemma-2-2b,mmlu_world_religions,0-shot,accuracy,0.695906432748538,0.0352821125824523
google/gemma-2-2b,mmlu_formal_logic,0-shot,accuracy,0.3253968253968254,0.0419059643887113
google/gemma-2-2b,mmlu_prehistory,0-shot,accuracy,0.5679012345679012,0.0275630109716066
google/gemma-2-2b,mmlu_moral_scenarios,0-shot,accuracy,0.2759776536312849,0.0149501030024753
google/gemma-2-2b,mmlu_high_school_world_history,0-shot,accuracy,0.6666666666666666,0.0306858205966108
google/gemma-2-2b,mmlu_moral_disputes,0-shot,accuracy,0.6040462427745664,0.0263298133419462
google/gemma-2-2b,mmlu_professional_law,0-shot,accuracy,0.4067796610169492,0.0125463255965695
google/gemma-2-2b,mmlu_logical_fallacies,0-shot,accuracy,0.6073619631901841,0.0383674090783102
google/gemma-2-2b,mmlu_high_school_us_history,0-shot,accuracy,0.6715686274509803,0.0329624511017222
google/gemma-2-2b,mmlu_philosophy,0-shot,accuracy,0.5852090032154341,0.0279826804597595
google/gemma-2-2b,mmlu_jurisprudence,0-shot,accuracy,0.6018518518518519,0.0473233261597881
google/gemma-2-2b,mmlu_international_law,0-shot,accuracy,0.6859504132231405,0.0423696475304101
google/gemma-2-2b,mmlu_high_school_european_history,0-shot,accuracy,0.6666666666666666,0.0368105086916154
google/gemma-2-2b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.772020725388601,0.0302769099451782
google/gemma-2-2b,mmlu_high_school_microeconomics,0-shot,accuracy,0.6134453781512605,0.0316314580755237
google/gemma-2-2b,mmlu_high_school_geography,0-shot,accuracy,0.7474747474747475,0.0309540554703659
google/gemma-2-2b,mmlu_high_school_psychology,0-shot,accuracy,0.7394495412844037,0.01881918203485
google/gemma-2-2b,mmlu_public_relations,0-shot,accuracy,0.6,0.0469237132203465
google/gemma-2-2b,mmlu_us_foreign_policy,0-shot,accuracy,0.72,0.0451260859854212
google/gemma-2-2b,mmlu_sociology,0-shot,accuracy,0.7512437810945274,0.0305676759389167
google/gemma-2-2b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.5307692307692308,0.0253029588908501
google/gemma-2-2b,mmlu_security_studies,0-shot,accuracy,0.5918367346938775,0.0314646571282742
google/gemma-2-2b,mmlu_professional_psychology,0-shot,accuracy,0.5441176470588235,0.0201489394204157
google/gemma-2-2b,mmlu_human_sexuality,0-shot,accuracy,0.5954198473282443,0.0430469379538066
google/gemma-2-2b,mmlu_econometrics,0-shot,accuracy,0.3245614035087719,0.0440455615737476
google/gemma-2-2b,mmlu_miscellaneous,0-shot,accuracy,0.6704980842911877,0.0168083222617404
google/gemma-2-2b,mmlu_marketing,0-shot,accuracy,0.7948717948717948,0.0264535080540403
google/gemma-2-2b,mmlu_management,0-shot,accuracy,0.6893203883495146,0.0458212416016155
google/gemma-2-2b,mmlu_nutrition,0-shot,accuracy,0.6045751633986928,0.0279967231806314
google/gemma-2-2b,mmlu_medical_genetics,0-shot,accuracy,0.7,0.0460566186471838
google/gemma-2-2b,mmlu_human_aging,0-shot,accuracy,0.5874439461883408,0.0330406217544929
google/gemma-2-2b,mmlu_professional_medicine,0-shot,accuracy,0.4154411764705882,0.0299353427078777
google/gemma-2-2b,mmlu_college_medicine,0-shot,accuracy,0.6127167630057804,0.0371432590630206
google/gemma-2-2b,mmlu_business_ethics,0-shot,accuracy,0.54,0.0500908265962033
google/gemma-2-2b,mmlu_clinical_knowledge,0-shot,accuracy,0.5811320754716981,0.0303650508291152
google/gemma-2-2b,mmlu_global_facts,0-shot,accuracy,0.3,0.0460566186471838
google/gemma-2-2b,mmlu_virology,0-shot,accuracy,0.4698795180722891,0.0388542542086676
google/gemma-2-2b,mmlu_professional_accounting,0-shot,accuracy,0.3971631205673759,0.0291898056735871
google/gemma-2-2b,mmlu_college_physics,0-shot,accuracy,0.3823529411764705,0.0483550369610722
google/gemma-2-2b,mmlu_high_school_physics,0-shot,accuracy,0.4039735099337748,0.0400648568536534
google/gemma-2-2b,mmlu_high_school_biology,0-shot,accuracy,0.6709677419354839,0.0267294990683499
google/gemma-2-2b,mmlu_college_biology,0-shot,accuracy,0.6180555555555556,0.0406299078414666
google/gemma-2-2b,mmlu_anatomy,0-shot,accuracy,0.5185185185185185,0.0431637859951132
google/gemma-2-2b,mmlu_college_chemistry,0-shot,accuracy,0.42,0.0496044963748858
google/gemma-2-2b,mmlu_computer_security,0-shot,accuracy,0.64,0.0482418151324421
google/gemma-2-2b,mmlu_college_computer_science,0-shot,accuracy,0.45,0.05
google/gemma-2-2b,mmlu_astronomy,0-shot,accuracy,0.5657894736842105,0.0403356566784832
google/gemma-2-2b,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215
google/gemma-2-2b,mmlu_conceptual_physics,0-shot,accuracy,0.4723404255319149,0.0326359711840976
google/gemma-2-2b,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
google/gemma-2-2b,mmlu_high_school_computer_science,0-shot,accuracy,0.48,0.0502116731568677
google/gemma-2-2b,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.0449394906861353
google/gemma-2-2b,mmlu_high_school_chemistry,0-shot,accuracy,0.4729064039408867,0.035128190778761
google/gemma-2-2b,mmlu_high_school_statistics,0-shot,accuracy,0.4629629629629629,0.0340060362553827
google/gemma-2-2b,mmlu_elementary_mathematics,0-shot,accuracy,0.3439153439153439,0.0244644266255964
google/gemma-2-2b,mmlu_electrical_engineering,0-shot,accuracy,0.6068965517241379,0.0407032901370707
google/gemma-2-2b,mmlu_high_school_mathematics,0-shot,accuracy,0.3,0.0279404571362284
google/gemma-2-2b,arc_challenge,25-shot,accuracy,0.5008532423208191,0.0146113695298132
google/gemma-2-2b,arc_challenge,25-shot,acc_norm,0.5435153583617748,0.0145559497604964
google/gemma-2-2b,hellaswag,10-shot,accuracy,0.5613423620792671,0.0049520870831289
google/gemma-2-2b,hellaswag,10-shot,acc_norm,0.744672376020713,0.0043515406039885
google/gemma-2-2b,truthfulqa_mc2,0-shot,accuracy,0.3622657549504685,0.0137736187852996
google/gemma-2-2b,truthfulqa_gen,0-shot,bleu_max,28.77597099246472,0.8088400452045031
google/gemma-2-2b,truthfulqa_gen,0-shot,bleu_acc,0.2962056303549572,0.0159835951018113
google/gemma-2-2b,truthfulqa_gen,0-shot,bleu_diff,-8.481338149090616,0.8892475779963439
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge1_max,54.25443225505836,0.8465255595845338
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge1_acc,0.2668298653610771,0.0154836919392372
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge1_diff,-10.29473637169676,0.9369283983381804
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge2_max,38.93076935593885,0.9966155175968516
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge2_acc,0.2496940024479804,0.0151522869071481
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge2_diff,-12.10709092268605,1.139799305832914
google/gemma-2-2b,truthfulqa_gen,0-shot,rougeL_max,51.455864843719006,0.8655454011373582
google/gemma-2-2b,truthfulqa_gen,0-shot,rougeL_acc,0.2705018359853121,0.0155507783328428
google/gemma-2-2b,truthfulqa_gen,0-shot,rougeL_diff,-10.280143405614036,0.9525463151211888
google/gemma-2-2b,truthfulqa_mc1,0-shot,accuracy,0.241126070991432,0.0149748272797523
google/gemma-2-2b,winogrande,5-shot,accuracy,0.7158642462509865,0.0126753927867727
google/gemma-2-2b,gsm8k,5-shot,accuracy,0.2623199393479909,0.0121169124199256
mosaicml/mpt-30b-instruct,arc:challenge,25-shot,accuracy,0.5426621160409556,0.014558106543924
mosaicml/mpt-30b-instruct,arc:challenge,25-shot,acc_norm,0.5844709897610921,0.0144013666412163
mosaicml/mpt-30b-instruct,hellaswag,10-shot,accuracy,0.652459669388568,0.0047521589368718
mosaicml/mpt-30b-instruct,hellaswag,10-shot,acc_norm,0.8430591515634336,0.0036300159898964
mosaicml/mpt-30b-instruct,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.047258156262526
mosaicml/mpt-30b-instruct,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.047258156262526
mosaicml/mpt-30b-instruct,hendrycksTest-anatomy,5-shot,accuracy,0.4222222222222222,0.0426676340409958
mosaicml/mpt-30b-instruct,hendrycksTest-anatomy,5-shot,acc_norm,0.4222222222222222,0.0426676340409958
mosaicml/mpt-30b-instruct,hendrycksTest-astronomy,5-shot,accuracy,0.5,0.0406894229385579
mosaicml/mpt-30b-instruct,hendrycksTest-astronomy,5-shot,acc_norm,0.5,0.0406894229385579
mosaicml/mpt-30b-instruct,hendrycksTest-business_ethics,5-shot,accuracy,0.5,0.0502518907629606
mosaicml/mpt-30b-instruct,hendrycksTest-business_ethics,5-shot,acc_norm,0.5,0.0502518907629606
mosaicml/mpt-30b-instruct,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.5169811320754717,0.0307551203641199
mosaicml/mpt-30b-instruct,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.5169811320754717,0.0307551203641199
mosaicml/mpt-30b-instruct,hendrycksTest-college_biology,5-shot,accuracy,0.5347222222222222,0.0417111585818161
mosaicml/mpt-30b-instruct,hendrycksTest-college_biology,5-shot,acc_norm,0.5347222222222222,0.0417111585818161
mosaicml/mpt-30b-instruct,hendrycksTest-college_chemistry,5-shot,accuracy,0.32,0.046882617226215
mosaicml/mpt-30b-instruct,hendrycksTest-college_chemistry,5-shot,acc_norm,0.32,0.046882617226215
mosaicml/mpt-30b-instruct,hendrycksTest-college_computer_science,5-shot,accuracy,0.47,0.0501613558046592
mosaicml/mpt-30b-instruct,hendrycksTest-college_computer_science,5-shot,acc_norm,0.47,0.0501613558046592
mosaicml/mpt-30b-instruct,hendrycksTest-college_mathematics,5-shot,accuracy,0.33,0.047258156262526
mosaicml/mpt-30b-instruct,hendrycksTest-college_mathematics,5-shot,acc_norm,0.33,0.047258156262526
mosaicml/mpt-30b-instruct,hendrycksTest-college_medicine,5-shot,accuracy,0.4508670520231214,0.0379401267469702
mosaicml/mpt-30b-instruct,hendrycksTest-college_medicine,5-shot,acc_norm,0.4508670520231214,0.0379401267469702
mosaicml/mpt-30b-instruct,hendrycksTest-college_physics,5-shot,accuracy,0.3529411764705882,0.0475512961606294
mosaicml/mpt-30b-instruct,hendrycksTest-college_physics,5-shot,acc_norm,0.3529411764705882,0.0475512961606294
mosaicml/mpt-30b-instruct,hendrycksTest-computer_security,5-shot,accuracy,0.64,0.0482418151324421
mosaicml/mpt-30b-instruct,hendrycksTest-computer_security,5-shot,acc_norm,0.64,0.0482418151324421
mosaicml/mpt-30b-instruct,hendrycksTest-conceptual_physics,5-shot,accuracy,0.4638297872340425,0.0326003851183577
mosaicml/mpt-30b-instruct,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.4638297872340425,0.0326003851183577
mosaicml/mpt-30b-instruct,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.0426633944315939
mosaicml/mpt-30b-instruct,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.0426633944315939
mosaicml/mpt-30b-instruct,hendrycksTest-electrical_engineering,5-shot,accuracy,0.4620689655172413,0.0415465967170754
mosaicml/mpt-30b-instruct,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.4620689655172413,0.0415465967170754
mosaicml/mpt-30b-instruct,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3201058201058201,0.0240268463928735
mosaicml/mpt-30b-instruct,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3201058201058201,0.0240268463928735
mosaicml/mpt-30b-instruct,hendrycksTest-formal_logic,5-shot,accuracy,0.3492063492063492,0.0426390689279513
mosaicml/mpt-30b-instruct,hendrycksTest-formal_logic,5-shot,acc_norm,0.3492063492063492,0.0426390689279513
mosaicml/mpt-30b-instruct,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
mosaicml/mpt-30b-instruct,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_biology,5-shot,accuracy,0.532258064516129,0.0283847477888133
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_biology,5-shot,acc_norm,0.532258064516129,0.0283847477888133
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3793103448275862,0.0341396380590623
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3793103448275862,0.0341396380590623
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.51,0.0502418393795691
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.51,0.0502418393795691
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_european_history,5-shot,accuracy,0.6484848484848484,0.0372820699868265
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.6484848484848484,0.0372820699868265
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_geography,5-shot,accuracy,0.6464646464646465,0.0340608672354715
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_geography,5-shot,acc_norm,0.6464646464646465,0.0340608672354715
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6735751295336787,0.0338402862114329
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6735751295336787,0.0338402862114329
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4589743589743589,0.0252655254912842
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4589743589743589,0.0252655254912842
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3296296296296296,0.0286612011165245
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3296296296296296,0.0286612011165245
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.4453781512605042,0.0322841062671639
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.4453781512605042,0.0322841062671639
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_physics,5-shot,accuracy,0.3245033112582781,0.0382274693765875
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3245033112582781,0.0382274693765875
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6587155963302752,0.0203286128165924
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6587155963302752,0.0203286128165924
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2916666666666667,0.0309986663045605
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2916666666666667,0.0309986663045605
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_us_history,5-shot,accuracy,0.7205882352941176,0.0314932810450795
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.7205882352941176,0.0314932810450795
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6877637130801688,0.030165137867847
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6877637130801688,0.030165137867847
mosaicml/mpt-30b-instruct,hendrycksTest-human_aging,5-shot,accuracy,0.5919282511210763,0.0329857460784282
mosaicml/mpt-30b-instruct,hendrycksTest-human_aging,5-shot,acc_norm,0.5919282511210763,0.0329857460784282
mosaicml/mpt-30b-instruct,hendrycksTest-human_sexuality,5-shot,accuracy,0.5877862595419847,0.0431717119487025
mosaicml/mpt-30b-instruct,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5877862595419847,0.0431717119487025
mosaicml/mpt-30b-instruct,hendrycksTest-international_law,5-shot,accuracy,0.4462809917355372,0.0453793517794788
mosaicml/mpt-30b-instruct,hendrycksTest-international_law,5-shot,acc_norm,0.4462809917355372,0.0453793517794788
mosaicml/mpt-30b-instruct,hendrycksTest-jurisprudence,5-shot,accuracy,0.5740740740740741,0.0478034362693678
mosaicml/mpt-30b-instruct,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5740740740740741,0.0478034362693678
mosaicml/mpt-30b-instruct,hendrycksTest-logical_fallacies,5-shot,accuracy,0.4907975460122699,0.0392770560078744
mosaicml/mpt-30b-instruct,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.4907975460122699,0.0392770560078744
mosaicml/mpt-30b-instruct,hendrycksTest-machine_learning,5-shot,accuracy,0.375,0.0459509138808629
mosaicml/mpt-30b-instruct,hendrycksTest-machine_learning,5-shot,acc_norm,0.375,0.0459509138808629
mosaicml/mpt-30b-instruct,hendrycksTest-management,5-shot,accuracy,0.6116504854368932,0.0482572933735639
mosaicml/mpt-30b-instruct,hendrycksTest-management,5-shot,acc_norm,0.6116504854368932,0.0482572933735639
mosaicml/mpt-30b-instruct,hendrycksTest-marketing,5-shot,accuracy,0.7051282051282052,0.0298725777088911
mosaicml/mpt-30b-instruct,hendrycksTest-marketing,5-shot,acc_norm,0.7051282051282052,0.0298725777088911
mosaicml/mpt-30b-instruct,hendrycksTest-medical_genetics,5-shot,accuracy,0.46,0.0500908265962033
mosaicml/mpt-30b-instruct,hendrycksTest-medical_genetics,5-shot,acc_norm,0.46,0.0500908265962033
mosaicml/mpt-30b-instruct,hendrycksTest-miscellaneous,5-shot,accuracy,0.6922094508301405,0.0165060450451556
mosaicml/mpt-30b-instruct,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6922094508301405,0.0165060450451556
mosaicml/mpt-30b-instruct,hendrycksTest-moral_disputes,5-shot,accuracy,0.4971098265895953,0.026918645383239
mosaicml/mpt-30b-instruct,hendrycksTest-moral_disputes,5-shot,acc_norm,0.4971098265895953,0.026918645383239
mosaicml/mpt-30b-instruct,hendrycksTest-moral_scenarios,5-shot,accuracy,0.3027932960893855,0.0153668603863971
mosaicml/mpt-30b-instruct,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.3027932960893855,0.0153668603863971
mosaicml/mpt-30b-instruct,hendrycksTest-nutrition,5-shot,accuracy,0.5163398692810458,0.0286146247528054
mosaicml/mpt-30b-instruct,hendrycksTest-nutrition,5-shot,acc_norm,0.5163398692810458,0.0286146247528054
mosaicml/mpt-30b-instruct,hendrycksTest-philosophy,5-shot,accuracy,0.5659163987138264,0.0281502322445355
mosaicml/mpt-30b-instruct,hendrycksTest-philosophy,5-shot,acc_norm,0.5659163987138264,0.0281502322445355
mosaicml/mpt-30b-instruct,hendrycksTest-prehistory,5-shot,accuracy,0.5771604938271605,0.0274874729808715
mosaicml/mpt-30b-instruct,hendrycksTest-prehistory,5-shot,acc_norm,0.5771604938271605,0.0274874729808715
mosaicml/mpt-30b-instruct,hendrycksTest-professional_accounting,5-shot,accuracy,0.3900709219858156,0.0290976755994639
mosaicml/mpt-30b-instruct,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3900709219858156,0.0290976755994639
mosaicml/mpt-30b-instruct,hendrycksTest-professional_law,5-shot,accuracy,0.3702737940026075,0.0123329307812567
mosaicml/mpt-30b-instruct,hendrycksTest-professional_law,5-shot,acc_norm,0.3702737940026075,0.0123329307812567
mosaicml/mpt-30b-instruct,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.0302114796091215
mosaicml/mpt-30b-instruct,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.0302114796091215
mosaicml/mpt-30b-instruct,hendrycksTest-professional_psychology,5-shot,accuracy,0.4901960784313725,0.0202239460050743
mosaicml/mpt-30b-instruct,hendrycksTest-professional_psychology,5-shot,acc_norm,0.4901960784313725,0.0202239460050743
mosaicml/mpt-30b-instruct,hendrycksTest-public_relations,5-shot,accuracy,0.5181818181818182,0.0478596401079491
mosaicml/mpt-30b-instruct,hendrycksTest-public_relations,5-shot,acc_norm,0.5181818181818182,0.0478596401079491
mosaicml/mpt-30b-instruct,hendrycksTest-security_studies,5-shot,accuracy,0.563265306122449,0.0317519523758332
mosaicml/mpt-30b-instruct,hendrycksTest-security_studies,5-shot,acc_norm,0.563265306122449,0.0317519523758332
mosaicml/mpt-30b-instruct,hendrycksTest-sociology,5-shot,accuracy,0.5522388059701493,0.0351618477295216
mosaicml/mpt-30b-instruct,hendrycksTest-sociology,5-shot,acc_norm,0.5522388059701493,0.0351618477295216
mosaicml/mpt-30b-instruct,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.69,0.0464823198711731
mosaicml/mpt-30b-instruct,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.69,0.0464823198711731
mosaicml/mpt-30b-instruct,hendrycksTest-virology,5-shot,accuracy,0.463855421686747,0.0388231085089059
mosaicml/mpt-30b-instruct,hendrycksTest-virology,5-shot,acc_norm,0.463855421686747,0.0388231085089059
mosaicml/mpt-30b-instruct,hendrycksTest-world_religions,5-shot,accuracy,0.6666666666666666,0.0361550763031093
mosaicml/mpt-30b-instruct,hendrycksTest-world_religions,5-shot,acc_norm,0.6666666666666666,0.0361550763031093
mosaicml/mpt-30b-instruct,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.0153218216884762
mosaicml/mpt-30b-instruct,truthfulqa:mc,0-shot,mc2,0.3804643219400445,0.0152165202662831
mosaicml/mpt-30b-instruct,drop,3-shot,accuracy,0.3308515100671141,0.004818562129043
mosaicml/mpt-30b-instruct,drop,3-shot,f1,0.3828376677852355,0.0047214052505206
mosaicml/mpt-30b-instruct,gsm8k,5-shot,accuracy,0.1531463229719484,0.0099197281527914
mosaicml/mpt-30b-instruct,winogrande,5-shot,accuracy,0.7513812154696132,0.0121473147134031
facebook/xglm-564M,arc:challenge,25-shot,accuracy,0.1979522184300341,0.0116439909715733
facebook/xglm-564M,arc:challenge,25-shot,acc_norm,0.2457337883959044,0.0125810334537301
facebook/xglm-564M,hellaswag,10-shot,accuracy,0.3048197570205138,0.0045939026019793
facebook/xglm-564M,hellaswag,10-shot,acc_norm,0.3493328022306313,0.0047578490234119
facebook/xglm-564M,hendrycksTest-abstract_algebra,5-shot,accuracy,0.2,0.0402015126103684
facebook/xglm-564M,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.2,0.0402015126103684
facebook/xglm-564M,hendrycksTest-anatomy,5-shot,accuracy,0.3111111111111111,0.0399926287661772
facebook/xglm-564M,hendrycksTest-anatomy,5-shot,acc_norm,0.3111111111111111,0.0399926287661772
facebook/xglm-564M,hendrycksTest-astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
facebook/xglm-564M,hendrycksTest-astronomy,5-shot,acc_norm,0.1776315789473684,0.0311031823831233
facebook/xglm-564M,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.0446196043338473
facebook/xglm-564M,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.0446196043338473
facebook/xglm-564M,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2037735849056604,0.0247907845017754
facebook/xglm-564M,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2037735849056604,0.0247907845017754
facebook/xglm-564M,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.0347659010430413
facebook/xglm-564M,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.0347659010430413
facebook/xglm-564M,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
facebook/xglm-564M,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
facebook/xglm-564M,hendrycksTest-college_computer_science,5-shot,accuracy,0.15,0.0358870281282637
facebook/xglm-564M,hendrycksTest-college_computer_science,5-shot,acc_norm,0.15,0.0358870281282637
facebook/xglm-564M,hendrycksTest-college_mathematics,5-shot,accuracy,0.23,0.042295258468165
facebook/xglm-564M,hendrycksTest-college_mathematics,5-shot,acc_norm,0.23,0.042295258468165
facebook/xglm-564M,hendrycksTest-college_medicine,5-shot,accuracy,0.2138728323699422,0.0312651120617304
facebook/xglm-564M,hendrycksTest-college_medicine,5-shot,acc_norm,0.2138728323699422,0.0312651120617304
facebook/xglm-564M,hendrycksTest-college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
facebook/xglm-564M,hendrycksTest-college_physics,5-shot,acc_norm,0.2254901960784313,0.0415830753308328
facebook/xglm-564M,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.042295258468165
facebook/xglm-564M,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.042295258468165
facebook/xglm-564M,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.0303635821972381
facebook/xglm-564M,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.0303635821972381
facebook/xglm-564M,hendrycksTest-econometrics,5-shot,accuracy,0.2719298245614035,0.0418577442402205
facebook/xglm-564M,hendrycksTest-econometrics,5-shot,acc_norm,0.2719298245614035,0.0418577442402205
facebook/xglm-564M,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.0366466633722525
facebook/xglm-564M,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.0366466633722525
facebook/xglm-564M,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2619047619047619,0.0226442126155252
facebook/xglm-564M,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2619047619047619,0.0226442126155252
facebook/xglm-564M,hendrycksTest-formal_logic,5-shot,accuracy,0.1428571428571428,0.031298431857438
facebook/xglm-564M,hendrycksTest-formal_logic,5-shot,acc_norm,0.1428571428571428,0.031298431857438
facebook/xglm-564M,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.0386122919665369
facebook/xglm-564M,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.0386122919665369
facebook/xglm-564M,hendrycksTest-high_school_biology,5-shot,accuracy,0.2709677419354839,0.0252844161149001
facebook/xglm-564M,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2709677419354839,0.0252844161149001
facebook/xglm-564M,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2955665024630542,0.0321049443375145
facebook/xglm-564M,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2955665024630542,0.0321049443375145
facebook/xglm-564M,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.0476095228569523
facebook/xglm-564M,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.0476095228569523
facebook/xglm-564M,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2666666666666666,0.0345313180188541
facebook/xglm-564M,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2666666666666666,0.0345313180188541
facebook/xglm-564M,hendrycksTest-high_school_geography,5-shot,accuracy,0.1969696969696969,0.0283356097324633
facebook/xglm-564M,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1969696969696969,0.0283356097324633
facebook/xglm-564M,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2694300518134715,0.0320186712287779
facebook/xglm-564M,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2694300518134715,0.0320186712287779
facebook/xglm-564M,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2128205128205128,0.020752423722128
facebook/xglm-564M,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2128205128205128,0.020752423722128
facebook/xglm-564M,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2481481481481481,0.0263357394040558
facebook/xglm-564M,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2481481481481481,0.0263357394040558
facebook/xglm-564M,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2394957983193277,0.0277220654933612
facebook/xglm-564M,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2394957983193277,0.0277220654933612
facebook/xglm-564M,hendrycksTest-high_school_physics,5-shot,accuracy,0.1986754966887417,0.0325784738443677
facebook/xglm-564M,hendrycksTest-high_school_physics,5-shot,acc_norm,0.1986754966887417,0.0325784738443677
facebook/xglm-564M,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1871559633027523,0.0167226845262001
facebook/xglm-564M,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1871559633027523,0.0167226845262001
facebook/xglm-564M,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4537037037037037,0.0339532272637579
facebook/xglm-564M,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4537037037037037,0.0339532272637579
facebook/xglm-564M,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2892156862745098,0.0318223186764755
facebook/xglm-564M,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2892156862745098,0.0318223186764755
facebook/xglm-564M,hendrycksTest-high_school_world_history,5-shot,accuracy,0.240506329113924,0.0278207819811496
facebook/xglm-564M,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.240506329113924,0.0278207819811496
facebook/xglm-564M,hendrycksTest-human_aging,5-shot,accuracy,0.3273542600896861,0.0314938467099413
facebook/xglm-564M,hendrycksTest-human_aging,5-shot,acc_norm,0.3273542600896861,0.0314938467099413
facebook/xglm-564M,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
facebook/xglm-564M,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
facebook/xglm-564M,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.0429434084521209
facebook/xglm-564M,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.0429434084521209
facebook/xglm-564M,hendrycksTest-jurisprudence,5-shot,accuracy,0.2592592592592592,0.0423651125809463
facebook/xglm-564M,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2592592592592592,0.0423651125809463
facebook/xglm-564M,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2760736196319018,0.0351238528370505
facebook/xglm-564M,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2760736196319018,0.0351238528370505
facebook/xglm-564M,hendrycksTest-machine_learning,5-shot,accuracy,0.3303571428571428,0.0446428571428571
facebook/xglm-564M,hendrycksTest-machine_learning,5-shot,acc_norm,0.3303571428571428,0.0446428571428571
facebook/xglm-564M,hendrycksTest-management,5-shot,accuracy,0.1844660194174757,0.0384042362728827
facebook/xglm-564M,hendrycksTest-management,5-shot,acc_norm,0.1844660194174757,0.0384042362728827
facebook/xglm-564M,hendrycksTest-marketing,5-shot,accuracy,0.2905982905982906,0.029745048572674
facebook/xglm-564M,hendrycksTest-marketing,5-shot,acc_norm,0.2905982905982906,0.029745048572674
facebook/xglm-564M,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.0460566186471838
facebook/xglm-564M,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.0460566186471838
facebook/xglm-564M,hendrycksTest-miscellaneous,5-shot,accuracy,0.2375478927203065,0.0152187330461501
facebook/xglm-564M,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2375478927203065,0.0152187330461501
facebook/xglm-564M,hendrycksTest-moral_disputes,5-shot,accuracy,0.2485549132947976,0.0232675284321001
facebook/xglm-564M,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2485549132947976,0.0232675284321001
facebook/xglm-564M,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/xglm-564M,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
facebook/xglm-564M,hendrycksTest-nutrition,5-shot,accuracy,0.218954248366013,0.0236790898618077
facebook/xglm-564M,hendrycksTest-nutrition,5-shot,acc_norm,0.218954248366013,0.0236790898618077
facebook/xglm-564M,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.0221224397724807
facebook/xglm-564M,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.0221224397724807
facebook/xglm-564M,hendrycksTest-prehistory,5-shot,accuracy,0.2160493827160493,0.0228991629184458
facebook/xglm-564M,hendrycksTest-prehistory,5-shot,acc_norm,0.2160493827160493,0.0228991629184458
facebook/xglm-564M,hendrycksTest-professional_accounting,5-shot,accuracy,0.2765957446808511,0.0266845643404609
facebook/xglm-564M,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2765957446808511,0.0266845643404609
facebook/xglm-564M,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.0109961566351426
facebook/xglm-564M,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.0109961566351426
facebook/xglm-564M,hendrycksTest-professional_medicine,5-shot,accuracy,0.4448529411764705,0.0301875320603293
facebook/xglm-564M,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4448529411764705,0.0301875320603293
facebook/xglm-564M,hendrycksTest-professional_psychology,5-shot,accuracy,0.25,0.0175178188450144
facebook/xglm-564M,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25,0.0175178188450144
facebook/xglm-564M,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
facebook/xglm-564M,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
facebook/xglm-564M,hendrycksTest-security_studies,5-shot,accuracy,0.2408163265306122,0.0273729422017881
facebook/xglm-564M,hendrycksTest-security_studies,5-shot,acc_norm,0.2408163265306122,0.0273729422017881
facebook/xglm-564M,hendrycksTest-sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
facebook/xglm-564M,hendrycksTest-sociology,5-shot,acc_norm,0.2437810945273631,0.0303604901540146
facebook/xglm-564M,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.23,0.042295258468165
facebook/xglm-564M,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.23,0.042295258468165
facebook/xglm-564M,hendrycksTest-virology,5-shot,accuracy,0.2831325301204819,0.0350729543137051
facebook/xglm-564M,hendrycksTest-virology,5-shot,acc_norm,0.2831325301204819,0.0350729543137051
facebook/xglm-564M,hendrycksTest-world_religions,5-shot,accuracy,0.2339181286549707,0.0324672176511782
facebook/xglm-564M,hendrycksTest-world_religions,5-shot,acc_norm,0.2339181286549707,0.0324672176511782
facebook/xglm-564M,truthfulqa:mc,0-shot,mc1,0.2325581395348837,0.0147891575310805
facebook/xglm-564M,truthfulqa:mc,0-shot,mc2,0.4043205877039627,0.0148733921466742
facebook/xglm-564M,drop,3-shot,accuracy,0.0134228187919463,0.0011784931108563
facebook/xglm-564M,drop,3-shot,f1,0.0603596895973155,0.0017160396766447
facebook/xglm-564M,gsm8k,5-shot,accuracy,0.0166793025018953,0.0035275958887224
facebook/xglm-564M,winogrande,5-shot,accuracy,0.5130228887134964,0.0140477183939976
EleutherAI/gpt-neo-2.7B,minerva_math_precalc,5-shot,accuracy,0.0293040293040293,0.0072244873054596
EleutherAI/gpt-neo-2.7B,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458,0.0034284443646836
EleutherAI/gpt-neo-2.7B,minerva_math_num_theory,5-shot,accuracy,0.0259259259259259,0.0068449258444466
EleutherAI/gpt-neo-2.7B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0199335548172757,0.0046538980883161
EleutherAI/gpt-neo-2.7B,minerva_math_geometry,5-shot,accuracy,0.0167014613778705,0.005861462425818
EleutherAI/gpt-neo-2.7B,minerva_math_counting_and_prob,5-shot,accuracy,0.0126582278481012,0.0051403138895788
EleutherAI/gpt-neo-2.7B,minerva_math_algebra,5-shot,accuracy,0.0160067396798652,0.0036442247924417
EleutherAI/gpt-neo-2.7B,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
EleutherAI/gpt-neo-2.7B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
EleutherAI/gpt-neo-2.7B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
EleutherAI/gpt-neo-2.7B,arithmetic_2ds,5-shot,accuracy,0.012,0.0024353573624298
EleutherAI/gpt-neo-2.7B,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339
EleutherAI/gpt-neo-2.7B,arithmetic_4ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_2dm,5-shot,accuracy,0.02,0.003131278085898
EleutherAI/gpt-neo-2.7B,arithmetic_2da,5-shot,accuracy,0.0065,0.0017973564602277
EleutherAI/gpt-neo-2.7B,gsm8k_cot,5-shot,accuracy,0.0181956027293404,0.0036816118940738
EleutherAI/gpt-neo-2.7B,anli_r2,0-shot,brier_score,0.8345390393015258,
EleutherAI/gpt-neo-2.7B,anli_r3,0-shot,brier_score,0.7689959499686069,
EleutherAI/gpt-neo-2.7B,anli_r1,0-shot,brier_score,0.8666257668901235,
EleutherAI/gpt-neo-2.7B,xnli_eu,0-shot,brier_score,1.1095833790701086,
EleutherAI/gpt-neo-2.7B,xnli_vi,0-shot,brier_score,0.7461864238715018,
EleutherAI/gpt-neo-2.7B,xnli_ru,0-shot,brier_score,0.7999015977427434,
EleutherAI/gpt-neo-2.7B,xnli_zh,0-shot,brier_score,0.9806971205870976,
EleutherAI/gpt-neo-2.7B,xnli_tr,0-shot,brier_score,0.8960590433909379,
EleutherAI/gpt-neo-2.7B,xnli_fr,0-shot,brier_score,0.8146332923707491,
EleutherAI/gpt-neo-2.7B,xnli_en,0-shot,brier_score,0.6628158350379858,
EleutherAI/gpt-neo-2.7B,xnli_ur,0-shot,brier_score,1.0717135016627397,
EleutherAI/gpt-neo-2.7B,xnli_ar,0-shot,brier_score,1.273910719834552,
EleutherAI/gpt-neo-2.7B,xnli_de,0-shot,brier_score,0.8583968193026806,
EleutherAI/gpt-neo-2.7B,xnli_hi,0-shot,brier_score,0.8196953013215778,
EleutherAI/gpt-neo-2.7B,xnli_es,0-shot,brier_score,0.8843937740416182,
EleutherAI/gpt-neo-2.7B,xnli_bg,0-shot,brier_score,0.8937202436821879,
EleutherAI/gpt-neo-2.7B,xnli_sw,0-shot,brier_score,0.7992106097885577,
EleutherAI/gpt-neo-2.7B,xnli_el,0-shot,brier_score,0.9270116356402288,
EleutherAI/gpt-neo-2.7B,xnli_th,0-shot,brier_score,0.7847320508164434,
EleutherAI/gpt-neo-2.7B,logiqa2,0-shot,brier_score,1.1416601553118555,
EleutherAI/gpt-neo-2.7B,mathqa,0-shot,brier_score,0.98729092626056,
EleutherAI/gpt-neo-2.7B,lambada_standard,0-shot,perplexity,9.47299988177036,0.2657462810211015
EleutherAI/gpt-neo-2.7B,lambada_standard,0-shot,accuracy,0.5175625849019988,0.0069616790974791
EleutherAI/gpt-neo-2.7B,lambada_openai,0-shot,perplexity,5.625735040282958,0.1385185112911551
EleutherAI/gpt-neo-2.7B,lambada_openai,0-shot,accuracy,0.622355909179119,0.0067541830765266
EleutherAI/gpt-neo-2.7B,mmlu_world_religions,0-shot,accuracy,0.2748538011695906,0.0342404292469158
EleutherAI/gpt-neo-2.7B,mmlu_formal_logic,0-shot,accuracy,0.1825396825396825,0.0345507101910214
EleutherAI/gpt-neo-2.7B,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.0257731111696304
EleutherAI/gpt-neo-2.7B,mmlu_moral_scenarios,0-shot,accuracy,0.2703910614525139,0.01485499393801
EleutherAI/gpt-neo-2.7B,mmlu_high_school_world_history,0-shot,accuracy,0.2236286919831223,0.0271232982052299
EleutherAI/gpt-neo-2.7B,mmlu_moral_disputes,0-shot,accuracy,0.245664739884393,0.023176298203992
EleutherAI/gpt-neo-2.7B,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.0109961566351426
EleutherAI/gpt-neo-2.7B,mmlu_logical_fallacies,0-shot,accuracy,0.2576687116564417,0.0343615082784691
EleutherAI/gpt-neo-2.7B,mmlu_high_school_us_history,0-shot,accuracy,0.196078431372549,0.0278659422866393
EleutherAI/gpt-neo-2.7B,mmlu_philosophy,0-shot,accuracy,0.3247588424437299,0.026596782287697
EleutherAI/gpt-neo-2.7B,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.0441434366685493
EleutherAI/gpt-neo-2.7B,mmlu_international_law,0-shot,accuracy,0.2314049586776859,0.0384985609879408
EleutherAI/gpt-neo-2.7B,mmlu_high_school_european_history,0-shot,accuracy,0.2303030303030303,0.0328766675860349
EleutherAI/gpt-neo-2.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2642487046632124,0.0318215505091664
EleutherAI/gpt-neo-2.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678638
EleutherAI/gpt-neo-2.7B,mmlu_high_school_geography,0-shot,accuracy,0.3282828282828283,0.0334567842275677
EleutherAI/gpt-neo-2.7B,mmlu_high_school_psychology,0-shot,accuracy,0.3064220183486238,0.0197655172204585
EleutherAI/gpt-neo-2.7B,mmlu_public_relations,0-shot,accuracy,0.1818181818181818,0.036942843353378
EleutherAI/gpt-neo-2.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-neo-2.7B,mmlu_sociology,0-shot,accuracy,0.2288557213930348,0.0297052840567724
EleutherAI/gpt-neo-2.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3487179487179487,0.0241627802840177
EleutherAI/gpt-neo-2.7B,mmlu_security_studies,0-shot,accuracy,0.2816326530612245,0.0287951855742912
EleutherAI/gpt-neo-2.7B,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.0179179740695947
EleutherAI/gpt-neo-2.7B,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.0391534540884783
EleutherAI/gpt-neo-2.7B,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neo-2.7B,mmlu_miscellaneous,0-shot,accuracy,0.2388250319284802,0.0152468031973986
EleutherAI/gpt-neo-2.7B,mmlu_marketing,0-shot,accuracy,0.2606837606837607,0.0287603489565234
EleutherAI/gpt-neo-2.7B,mmlu_management,0-shot,accuracy,0.2718446601941747,0.0440526802414092
EleutherAI/gpt-neo-2.7B,mmlu_nutrition,0-shot,accuracy,0.3104575163398693,0.0264930332251458
EleutherAI/gpt-neo-2.7B,mmlu_medical_genetics,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-neo-2.7B,mmlu_human_aging,0-shot,accuracy,0.1838565022421524,0.0259983790923565
EleutherAI/gpt-neo-2.7B,mmlu_professional_medicine,0-shot,accuracy,0.4301470588235294,0.0300749719173028
EleutherAI/gpt-neo-2.7B,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
EleutherAI/gpt-neo-2.7B,mmlu_business_ethics,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-neo-2.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.2641509433962264,0.0271342916287417
EleutherAI/gpt-neo-2.7B,mmlu_global_facts,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/gpt-neo-2.7B,mmlu_virology,0-shot,accuracy,0.3253012048192771,0.0364716852368322
EleutherAI/gpt-neo-2.7B,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902
EleutherAI/gpt-neo-2.7B,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/gpt-neo-2.7B,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649,0.0347918557259966
EleutherAI/gpt-neo-2.7B,mmlu_high_school_biology,0-shot,accuracy,0.2483870967741935,0.024580028921481
EleutherAI/gpt-neo-2.7B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
EleutherAI/gpt-neo-2.7B,mmlu_anatomy,0-shot,accuracy,0.2,0.0345547370232543
EleutherAI/gpt-neo-2.7B,mmlu_college_chemistry,0-shot,accuracy,0.22,0.0416333199893226
EleutherAI/gpt-neo-2.7B,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,mmlu_college_computer_science,0-shot,accuracy,0.25,0.0435194139889244
EleutherAI/gpt-neo-2.7B,mmlu_astronomy,0-shot,accuracy,0.1907894736842105,0.031975658210325
EleutherAI/gpt-neo-2.7B,mmlu_college_mathematics,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-2.7B,mmlu_conceptual_physics,0-shot,accuracy,0.2765957446808511,0.0292418838696288
EleutherAI/gpt-neo-2.7B,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.0429234695990928
EleutherAI/gpt-neo-2.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.35,0.0479372485441102
EleutherAI/gpt-neo-2.7B,mmlu_machine_learning,0-shot,accuracy,0.1696428571428571,0.0356236785009539
EleutherAI/gpt-neo-2.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.2413793103448276,0.0301083307180116
EleutherAI/gpt-neo-2.7B,mmlu_high_school_statistics,0-shot,accuracy,0.4074074074074074,0.0335099160469604
EleutherAI/gpt-neo-2.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.2936507936507936,0.023456037383982
EleutherAI/gpt-neo-2.7B,mmlu_electrical_engineering,0-shot,accuracy,0.2551724137931034,0.0363298405270784
EleutherAI/gpt-neo-2.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.2481481481481481,0.0263357394040558
EleutherAI/gpt-neo-2.7B,arc_challenge,25-shot,accuracy,0.3199658703071672,0.0136313458070161
EleutherAI/gpt-neo-2.7B,arc_challenge,25-shot,acc_norm,0.348976109215017,0.0139289334613825
EleutherAI/gpt-neo-2.7B,truthfulqa_mc2,0-shot,accuracy,0.3986263191841971,0.0140420659068149
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,bleu_max,24.047333287791275,0.7526012282674005
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,bleu_acc,0.3586291309669522,0.016789289499502
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,bleu_diff,-2.089423502619666,0.8400708361193482
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge1_max,49.67982165948041,0.8665227117461761
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge1_acc,0.3414932680538555,0.0166006886199508
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge1_diff,-1.963312198645994,1.0594843696396452
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge2_max,32.68414904349889,1.029790643453569
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge2_acc,0.2619339045287637,0.015392118805015
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge2_diff,-3.882375844220804,1.1699264928139463
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rougeL_max,46.88523724078316,0.885979367776478
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rougeL_acc,0.3427172582619339,0.016614949385347
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rougeL_diff,-2.1065494987322304,1.0736368489730006
EleutherAI/gpt-neo-2.7B,truthfulqa_mc1,0-shot,accuracy,0.2386780905752754,0.0149226296954564
jisukim8873/falcon-7B-case-2,arc:challenge,25-shot,accuracy,0.4334470989761092,0.0144813762245588
jisukim8873/falcon-7B-case-2,arc:challenge,25-shot,acc_norm,0.4718430034129693,0.0145882041051022
jisukim8873/falcon-7B-case-2,hellaswag,10-shot,accuracy,0.5975901214897431,0.0048938148902083
jisukim8873/falcon-7B-case-2,hellaswag,10-shot,acc_norm,0.7847042421828321,0.0041018734073546
jisukim8873/falcon-7B-case-2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338473
jisukim8873/falcon-7B-case-2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338473
jisukim8873/falcon-7B-case-2,hendrycksTest-anatomy,5-shot,accuracy,0.3037037037037037,0.0397255288478513
jisukim8873/falcon-7B-case-2,hendrycksTest-anatomy,5-shot,acc_norm,0.3037037037037037,0.0397255288478513
jisukim8873/falcon-7B-case-2,hendrycksTest-astronomy,5-shot,accuracy,0.2039473684210526,0.0327900040631005
jisukim8873/falcon-7B-case-2,hendrycksTest-astronomy,5-shot,acc_norm,0.2039473684210526,0.0327900040631005
jisukim8873/falcon-7B-case-2,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.0440844002276808
jisukim8873/falcon-7B-case-2,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.0440844002276808
jisukim8873/falcon-7B-case-2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3245283018867924,0.0288156157134321
jisukim8873/falcon-7B-case-2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3245283018867924,0.0288156157134321
jisukim8873/falcon-7B-case-2,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.0371617743756601
jisukim8873/falcon-7B-case-2,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.0371617743756601
jisukim8873/falcon-7B-case-2,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-2,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.0446196043338473
jisukim8873/falcon-7B-case-2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.0446196043338473
jisukim8873/falcon-7B-case-2,hendrycksTest-college_mathematics,5-shot,accuracy,0.24,0.0429234695990928
jisukim8873/falcon-7B-case-2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.24,0.0429234695990928
jisukim8873/falcon-7B-case-2,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225943
jisukim8873/falcon-7B-case-2,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.0336876293225943
jisukim8873/falcon-7B-case-2,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.0433643270799317
jisukim8873/falcon-7B-case-2,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.0433643270799317
jisukim8873/falcon-7B-case-2,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.0487831731214563
jisukim8873/falcon-7B-case-2,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.0487831731214563
jisukim8873/falcon-7B-case-2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3191489361702128,0.03047297336338
jisukim8873/falcon-7B-case-2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3191489361702128,0.03047297336338
jisukim8873/falcon-7B-case-2,hendrycksTest-econometrics,5-shot,accuracy,0.3070175438596491,0.0433913832257986
jisukim8873/falcon-7B-case-2,hendrycksTest-econometrics,5-shot,acc_norm,0.3070175438596491,0.0433913832257986
jisukim8873/falcon-7B-case-2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3103448275862069,0.0385528961637894
jisukim8873/falcon-7B-case-2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3103448275862069,0.0385528961637894
jisukim8873/falcon-7B-case-2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2645502645502645,0.0227174678977086
jisukim8873/falcon-7B-case-2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2645502645502645,0.0227174678977086
jisukim8873/falcon-7B-case-2,hendrycksTest-formal_logic,5-shot,accuracy,0.1746031746031746,0.033954900208561
jisukim8873/falcon-7B-case-2,hendrycksTest-formal_logic,5-shot,acc_norm,0.1746031746031746,0.033954900208561
jisukim8873/falcon-7B-case-2,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.0394277244403662
jisukim8873/falcon-7B-case-2,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.0394277244403662
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_biology,5-shot,accuracy,0.3258064516129032,0.0266620105785671
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3258064516129032,0.0266620105785671
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3103448275862069,0.032550867699701
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3103448275862069,0.032550867699701
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3393939393939394,0.0369744220503159
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3393939393939394,0.0369744220503159
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_geography,5-shot,accuracy,0.303030303030303,0.0327428791402686
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.303030303030303,0.0327428791402686
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2435233160621761,0.0309754363868454
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2435233160621761,0.0309754363868454
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2564102564102564,0.0221390811039715
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2564102564102564,0.0221390811039715
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2925925925925925,0.027738969632176
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2925925925925925,0.027738969632176
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2773109243697479,0.02907937453948
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2773109243697479,0.02907937453948
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_physics,5-shot,accuracy,0.2450331125827814,0.0351180757180472
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2450331125827814,0.0351180757180472
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2844036697247706,0.0193420365877025
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2844036697247706,0.0193420365877025
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.199074074074074,0.0272322984626902
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.199074074074074,0.0272322984626902
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2892156862745098,0.0318223186764755
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2892156862745098,0.0318223186764755
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3080168776371308,0.0300523893356057
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3080168776371308,0.0300523893356057
jisukim8873/falcon-7B-case-2,hendrycksTest-human_aging,5-shot,accuracy,0.3991031390134529,0.0328674531256796
jisukim8873/falcon-7B-case-2,hendrycksTest-human_aging,5-shot,acc_norm,0.3991031390134529,0.0328674531256796
jisukim8873/falcon-7B-case-2,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
jisukim8873/falcon-7B-case-2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
jisukim8873/falcon-7B-case-2,hendrycksTest-international_law,5-shot,accuracy,0.3471074380165289,0.0434572457029253
jisukim8873/falcon-7B-case-2,hendrycksTest-international_law,5-shot,acc_norm,0.3471074380165289,0.0434572457029253
jisukim8873/falcon-7B-case-2,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
jisukim8873/falcon-7B-case-2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
jisukim8873/falcon-7B-case-2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.294478527607362,0.0358116579047408
jisukim8873/falcon-7B-case-2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.294478527607362,0.0358116579047408
jisukim8873/falcon-7B-case-2,hendrycksTest-machine_learning,5-shot,accuracy,0.3571428571428571,0.0454796099976437
jisukim8873/falcon-7B-case-2,hendrycksTest-machine_learning,5-shot,acc_norm,0.3571428571428571,0.0454796099976437
jisukim8873/falcon-7B-case-2,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.0430125039969087
jisukim8873/falcon-7B-case-2,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.0430125039969087
jisukim8873/falcon-7B-case-2,hendrycksTest-marketing,5-shot,accuracy,0.3675213675213675,0.0315853915774563
jisukim8873/falcon-7B-case-2,hendrycksTest-marketing,5-shot,acc_norm,0.3675213675213675,0.0315853915774563
jisukim8873/falcon-7B-case-2,hendrycksTest-medical_genetics,5-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.33,0.047258156262526
jisukim8873/falcon-7B-case-2,hendrycksTest-miscellaneous,5-shot,accuracy,0.3601532567049808,0.0171663624713692
jisukim8873/falcon-7B-case-2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3601532567049808,0.0171663624713692
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_disputes,5-shot,accuracy,0.3294797687861271,0.0253052581318797
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3294797687861271,0.0253052581318797
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2871508379888268,0.0151316088499637
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2871508379888268,0.0151316088499637
jisukim8873/falcon-7B-case-2,hendrycksTest-nutrition,5-shot,accuracy,0.326797385620915,0.0268572946632814
jisukim8873/falcon-7B-case-2,hendrycksTest-nutrition,5-shot,acc_norm,0.326797385620915,0.0268572946632814
jisukim8873/falcon-7B-case-2,hendrycksTest-philosophy,5-shot,accuracy,0.3118971061093247,0.0263118580718541
jisukim8873/falcon-7B-case-2,hendrycksTest-philosophy,5-shot,acc_norm,0.3118971061093247,0.0263118580718541
jisukim8873/falcon-7B-case-2,hendrycksTest-prehistory,5-shot,accuracy,0.2469135802469135,0.0239935017090421
jisukim8873/falcon-7B-case-2,hendrycksTest-prehistory,5-shot,acc_norm,0.2469135802469135,0.0239935017090421
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.0267891723511402
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_law,5-shot,accuracy,0.257496740547588,0.0111677060149041
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_law,5-shot,acc_norm,0.257496740547588,0.0111677060149041
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_medicine,5-shot,accuracy,0.2058823529411764,0.0245622043141423
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2058823529411764,0.0245622043141423
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_psychology,5-shot,accuracy,0.2647058823529412,0.0178480895749132
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2647058823529412,0.0178480895749132
jisukim8873/falcon-7B-case-2,hendrycksTest-public_relations,5-shot,accuracy,0.3272727272727272,0.0449429086625208
jisukim8873/falcon-7B-case-2,hendrycksTest-public_relations,5-shot,acc_norm,0.3272727272727272,0.0449429086625208
jisukim8873/falcon-7B-case-2,hendrycksTest-security_studies,5-shot,accuracy,0.2163265306122449,0.026358916334904
jisukim8873/falcon-7B-case-2,hendrycksTest-security_studies,5-shot,acc_norm,0.2163265306122449,0.026358916334904
jisukim8873/falcon-7B-case-2,hendrycksTest-sociology,5-shot,accuracy,0.2835820895522388,0.0318718753791979
jisukim8873/falcon-7B-case-2,hendrycksTest-sociology,5-shot,acc_norm,0.2835820895522388,0.0318718753791979
jisukim8873/falcon-7B-case-2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.41,0.049431107042371
jisukim8873/falcon-7B-case-2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.41,0.049431107042371
jisukim8873/falcon-7B-case-2,hendrycksTest-virology,5-shot,accuracy,0.3795180722891566,0.0377779882274801
jisukim8873/falcon-7B-case-2,hendrycksTest-virology,5-shot,acc_norm,0.3795180722891566,0.0377779882274801
jisukim8873/falcon-7B-case-2,hendrycksTest-world_religions,5-shot,accuracy,0.3391812865497076,0.036310534964889
jisukim8873/falcon-7B-case-2,hendrycksTest-world_religions,5-shot,acc_norm,0.3391812865497076,0.036310534964889
jisukim8873/falcon-7B-case-2,truthfulqa:mc,0-shot,mc1,0.2619339045287637,0.015392118805015
jisukim8873/falcon-7B-case-2,truthfulqa:mc,0-shot,mc2,0.3862844409155128,0.0144390732569955
jisukim8873/falcon-7B-case-2,winogrande,5-shot,accuracy,0.7040252565114443,0.012829348226339
jisukim8873/falcon-7B-case-2,gsm8k,5-shot,accuracy,0.0697498104624715,0.0070163895710138
jisukim8873/falcon-7B-case-2,minerva_math_precalc,5-shot,accuracy,0.0091575091575091,0.004080306065049
jisukim8873/falcon-7B-case-2,minerva_math_prealgebra,5-shot,accuracy,0.0252583237657864,0.005319703220303
jisukim8873/falcon-7B-case-2,minerva_math_num_theory,5-shot,accuracy,0.0092592592592592,0.0041254730154902
jisukim8873/falcon-7B-case-2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0121816168327796,0.0036524791938863
jisukim8873/falcon-7B-case-2,minerva_math_geometry,5-shot,accuracy,0.022964509394572,0.0068512498787692
jisukim8873/falcon-7B-case-2,minerva_math_counting_and_prob,5-shot,accuracy,0.0084388185654008,0.004206007207713
jisukim8873/falcon-7B-case-2,minerva_math_algebra,5-shot,accuracy,0.0160067396798652,0.0036442247924417
jisukim8873/falcon-7B-case-2,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-2,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-2,arithmetic_3da,5-shot,accuracy,0.033,0.0039954326099773
jisukim8873/falcon-7B-case-2,arithmetic_3ds,5-shot,accuracy,0.0555,0.0051208384560778
jisukim8873/falcon-7B-case-2,arithmetic_4da,5-shot,accuracy,0.002,0.0009992493430694
jisukim8873/falcon-7B-case-2,arithmetic_2ds,5-shot,accuracy,0.3135,0.0103760641070291
jisukim8873/falcon-7B-case-2,arithmetic_5ds,5-shot,accuracy,0.0025,0.0011169148353275
jisukim8873/falcon-7B-case-2,arithmetic_5da,5-shot,accuracy,0.0005,0.0005
jisukim8873/falcon-7B-case-2,arithmetic_1dc,5-shot,accuracy,0.092,0.0064644330337025
jisukim8873/falcon-7B-case-2,arithmetic_4ds,5-shot,accuracy,0.004,0.0014117352790977
jisukim8873/falcon-7B-case-2,arithmetic_2dm,5-shot,accuracy,0.1035,0.0068130084061133
jisukim8873/falcon-7B-case-2,arithmetic_2da,5-shot,accuracy,0.147,0.0079200292569988
jisukim8873/falcon-7B-case-2,gsm8k_cot,5-shot,accuracy,0.1046247156937073,0.0084306680820292
jisukim8873/falcon-7B-case-2,anli_r2,0-shot,brier_score,0.9517210089818692,
jisukim8873/falcon-7B-case-2,anli_r3,0-shot,brier_score,0.9091491237138922,
jisukim8873/falcon-7B-case-2,anli_r1,0-shot,brier_score,0.9740803307547984,
jisukim8873/falcon-7B-case-2,xnli_eu,0-shot,brier_score,1.039626455615514,
jisukim8873/falcon-7B-case-2,xnli_vi,0-shot,brier_score,0.9944426502362462,
jisukim8873/falcon-7B-case-2,xnli_ru,0-shot,brier_score,0.8204429465406277,
jisukim8873/falcon-7B-case-2,xnli_zh,0-shot,brier_score,1.0121312596962848,
jisukim8873/falcon-7B-case-2,xnli_tr,0-shot,brier_score,0.951139244903774,
jisukim8873/falcon-7B-case-2,xnli_fr,0-shot,brier_score,0.7485518412292533,
jisukim8873/falcon-7B-case-2,xnli_en,0-shot,brier_score,0.6574942705526986,
jisukim8873/falcon-7B-case-2,xnli_ur,0-shot,brier_score,1.3224866657294507,
jisukim8873/falcon-7B-case-2,xnli_ar,0-shot,brier_score,1.2550232606263445,
jisukim8873/falcon-7B-case-2,xnli_de,0-shot,brier_score,0.8394366588628737,
jisukim8873/falcon-7B-case-2,xnli_hi,0-shot,brier_score,1.1395961000527088,
jisukim8873/falcon-7B-case-2,xnli_es,0-shot,brier_score,0.8186718184732203,
jisukim8873/falcon-7B-case-2,xnli_bg,0-shot,brier_score,0.9250478368056084,
jisukim8873/falcon-7B-case-2,xnli_sw,0-shot,brier_score,1.106399353671628,
jisukim8873/falcon-7B-case-2,xnli_el,0-shot,brier_score,0.99014564774069,
jisukim8873/falcon-7B-case-2,xnli_th,0-shot,brier_score,0.9661701648978412,
jisukim8873/falcon-7B-case-2,logiqa2,0-shot,brier_score,1.0772771409734918,
jisukim8873/falcon-7B-case-2,mathqa,0-shot,brier_score,0.9470815194657676,
jisukim8873/falcon-7B-case-2,lambada_standard,0-shot,perplexity,4.151489270057361,0.0898434559924617
jisukim8873/falcon-7B-case-2,lambada_standard,0-shot,accuracy,0.6600038812342325,0.0065996711696681
jisukim8873/falcon-7B-case-2,lambada_openai,0-shot,perplexity,3.3084712739581,0.0686933064689936
jisukim8873/falcon-7B-case-2,lambada_openai,0-shot,accuracy,0.7345235784979623,0.0061521642395864
llama2_220M_nl_40_code_60,minerva_math_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_geometry,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_num_theory,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_prealgebra,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_precalc,5-shot,accuracy,0.0054945054945054,0.0031664282264934
llama2_220M_nl_40_code_60,gsm8k,5-shot,accuracy,0.0174374526156178,0.0036054868679982
llama2_220M_nl_40_code_60,gsm8k_cot,5-shot,accuracy,0.0189537528430629,0.0037560783410314
llama2_220M_nl_40_code_60,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_2ds,5-shot,accuracy,0.0245,0.0034577236625362
llama2_220M_nl_40_code_60,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
llama2_220M_nl_40_code_60,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
llama2_220M_nl_40_code_60,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_2dm,5-shot,accuracy,0.0225,0.0033169829948455
llama2_220M_nl_40_code_60,arithmetic_1dc,5-shot,accuracy,0.03,0.0038154001938617
llama2_220M_nl_40_code_60,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646
llama2_220M_nl_40_code_60,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
llama2_220M_nl_40_code_60,xnli_ar,0-shot,brier_score,0.8526082321262393,
llama2_220M_nl_40_code_60,xnli_bg,0-shot,brier_score,1.109604686794776,
llama2_220M_nl_40_code_60,xnli_de,0-shot,brier_score,0.967558512731451,
llama2_220M_nl_40_code_60,xnli_el,0-shot,brier_score,1.2587459676977044,
llama2_220M_nl_40_code_60,xnli_en,0-shot,brier_score,0.8104212740211147,
llama2_220M_nl_40_code_60,xnli_es,0-shot,brier_score,1.1051919631368405,
llama2_220M_nl_40_code_60,xnli_fr,0-shot,brier_score,1.0247885002669208,
llama2_220M_nl_40_code_60,xnli_hi,0-shot,brier_score,0.8712003091786603,
llama2_220M_nl_40_code_60,xnli_ru,0-shot,brier_score,0.9112442124737259,
llama2_220M_nl_40_code_60,xnli_sw,0-shot,brier_score,0.8868661517904973,
llama2_220M_nl_40_code_60,xnli_th,0-shot,brier_score,1.006804235357959,
llama2_220M_nl_40_code_60,xnli_tr,0-shot,brier_score,1.0693218975013594,
llama2_220M_nl_40_code_60,xnli_ur,0-shot,brier_score,1.3194230679835433,
llama2_220M_nl_40_code_60,xnli_vi,0-shot,brier_score,1.1831272811203084,
llama2_220M_nl_40_code_60,xnli_zh,0-shot,brier_score,1.0879824785397614,
llama2_220M_nl_40_code_60,anli_r1,0-shot,brier_score,1.0380057146075188,
llama2_220M_nl_40_code_60,anli_r3,0-shot,brier_score,0.986869985169431,
llama2_220M_nl_40_code_60,anli_r2,0-shot,brier_score,1.0434997483202808,
llama2_220M_nl_40_code_60,logiqa2,0-shot,brier_score,1.1791336836141835,
llama2_220M_nl_40_code_60,lambada_standard,0-shot,perplexity,131.9624436400779,5.489409719204202
llama2_220M_nl_40_code_60,lambada_standard,0-shot,accuracy,0.2268581408888026,0.005834704296179
llama2_220M_nl_40_code_60,lambada_openai,0-shot,perplexity,55.5328377598677,2.1332535891947404
llama2_220M_nl_40_code_60,lambada_openai,0-shot,accuracy,0.2965262953619251,0.0063630833623284
EleutherAI/pythia-1b-deduped,drop,3-shot,accuracy,0.0016778523489932,0.0004191330178826
EleutherAI/pythia-1b-deduped,drop,3-shot,f1,0.0471403104026847,0.0012275087763983
EleutherAI/pythia-1b-deduped,gsm8k,5-shot,accuracy,0.0113722517058377,0.0029206661987887
EleutherAI/pythia-1b-deduped,winogrande,5-shot,accuracy,0.5359116022099447,0.0140161934339583
EleutherAI/pythia-1b-deduped,arc:challenge,25-shot,accuracy,0.2645051194539249,0.0128892729493133
EleutherAI/pythia-1b-deduped,arc:challenge,25-shot,acc_norm,0.2909556313993174,0.0132730778659075
EleutherAI/pythia-1b-deduped,hellaswag,10-shot,accuracy,0.3872734515036845,0.0048613146132868
EleutherAI/pythia-1b-deduped,hellaswag,10-shot,acc_norm,0.4965146385182235,0.0049896601807921
EleutherAI/pythia-1b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-1b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/pythia-1b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.237037037037037,0.036737316839695
EleutherAI/pythia-1b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.237037037037037,0.036737316839695
EleutherAI/pythia-1b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.1447368421052631,0.0286319518459303
EleutherAI/pythia-1b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.1447368421052631,0.0286319518459303
EleutherAI/pythia-1b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-1b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.18,0.0386122919665369
EleutherAI/pythia-1b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.0282542003444386
EleutherAI/pythia-1b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.0282542003444386
EleutherAI/pythia-1b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008034
EleutherAI/pythia-1b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2430555555555555,0.0358687928008034
EleutherAI/pythia-1b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-1b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-1b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/pythia-1b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
EleutherAI/pythia-1b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-1b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-1b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2601156069364161,0.0334503691678899
EleutherAI/pythia-1b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2601156069364161,0.0334503691678899
EleutherAI/pythia-1b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.1862745098039215,0.0387395871414935
EleutherAI/pythia-1b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.1862745098039215,0.0387395871414935
EleutherAI/pythia-1b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-1b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-1b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.0292418838696288
EleutherAI/pythia-1b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.0292418838696288
EleutherAI/pythia-1b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.1842105263157894,0.0364675887507556
EleutherAI/pythia-1b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.1842105263157894,0.0364675887507556
EleutherAI/pythia-1b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.193103448275862,0.0328944552212739
EleutherAI/pythia-1b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.193103448275862,0.0328944552212739
EleutherAI/pythia-1b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2433862433862433,0.0221011287874154
EleutherAI/pythia-1b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2433862433862433,0.0221011287874154
EleutherAI/pythia-1b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.246031746031746,0.0385227336492431
EleutherAI/pythia-1b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.246031746031746,0.0385227336492431
EleutherAI/pythia-1b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-1b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.25,0.0435194139889244
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.267741935483871,0.0251890066602123
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.267741935483871,0.0251890066602123
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2364532019704433,0.0298961142917335
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2364532019704433,0.0298961142917335
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2363636363636363,0.0331750593000918
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2171717171717171,0.0293766164849456
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2171717171717171,0.0293766164849456
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2124352331606217,0.0295192826168172
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2124352331606217,0.0295192826168172
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2051282051282051,0.0204732331735519
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2051282051282051,0.0204732331735519
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.1890756302521008,0.0254351194381053
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.1890756302521008,0.0254351194381053
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.2317880794701986,0.0344540627198705
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2317880794701986,0.0344540627198705
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2935779816513761,0.0195251511226396
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2935779816513761,0.0195251511226396
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.25,0.0295312211609309
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.25,0.0295312211609309
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.0303915336927415
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.0303915336927415
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.0289007219062934
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.0289007219062934
EleutherAI/pythia-1b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.336322869955157,0.031708824268455
EleutherAI/pythia-1b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.336322869955157,0.031708824268455
EleutherAI/pythia-1b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-1b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-1b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.0398497965330287
EleutherAI/pythia-1b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.0398497965330287
EleutherAI/pythia-1b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
EleutherAI/pythia-1b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
EleutherAI/pythia-1b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2269938650306748,0.0329109957861576
EleutherAI/pythia-1b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2269938650306748,0.0329109957861576
EleutherAI/pythia-1b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.3035714285714285,0.0436422615584104
EleutherAI/pythia-1b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.3035714285714285,0.0436422615584104
EleutherAI/pythia-1b-deduped,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.0391666776282258
EleutherAI/pythia-1b-deduped,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.0391666776282258
EleutherAI/pythia-1b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2777777777777778,0.0293431147980944
EleutherAI/pythia-1b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2777777777777778,0.0293431147980944
EleutherAI/pythia-1b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-1b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/pythia-1b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2758620689655172,0.0159828147746956
EleutherAI/pythia-1b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2758620689655172,0.0159828147746956
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.241340782122905,0.0143109995479614
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.241340782122905,0.0143109995479614
EleutherAI/pythia-1b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2320261437908496,0.024170840879341
EleutherAI/pythia-1b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2320261437908496,0.024170840879341
EleutherAI/pythia-1b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2668810289389067,0.0251226376088166
EleutherAI/pythia-1b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2668810289389067,0.0251226376088166
EleutherAI/pythia-1b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2623456790123457,0.0244772228561351
EleutherAI/pythia-1b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2623456790123457,0.0244772228561351
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2411347517730496,0.0255187310495377
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2411347517730496,0.0255187310495377
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2385919165580182,0.0108859297420022
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2385919165580182,0.0108859297420022
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.2610294117647059,0.0266792522701031
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2610294117647059,0.0266792522701031
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.284313725490196,0.0182490244112076
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.284313725490196,0.0182490244112076
EleutherAI/pythia-1b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2727272727272727,0.0426579211094058
EleutherAI/pythia-1b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2727272727272727,0.0426579211094058
EleutherAI/pythia-1b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.1469387755102041,0.0226654004172176
EleutherAI/pythia-1b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.1469387755102041,0.0226654004172176
EleutherAI/pythia-1b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-1b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-1b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-1b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/pythia-1b-deduped,hendrycksTest-virology,5-shot,accuracy,0.2771084337349397,0.0348433159268058
EleutherAI/pythia-1b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.2771084337349397,0.0348433159268058
EleutherAI/pythia-1b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.2456140350877192,0.0330140594698725
EleutherAI/pythia-1b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.2456140350877192,0.0330140594698725
EleutherAI/pythia-1b-deduped,truthfulqa:mc,0-shot,mc1,0.226438188494492,0.0146513373246025
EleutherAI/pythia-1b-deduped,truthfulqa:mc,0-shot,mc2,0.389393782096148,0.0143149177463835
mosaicml/mpt-7b,minerva_math_precalc,5-shot,accuracy,0.0293040293040293,0.0072244873054596
mosaicml/mpt-7b,minerva_math_prealgebra,5-shot,accuracy,0.0665901262916188,0.0084524281604161
mosaicml/mpt-7b,minerva_math_num_theory,5-shot,accuracy,0.0407407407407407,0.0085150671637201
mosaicml/mpt-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0254706533776301,0.0052458302725593
mosaicml/mpt-7b,minerva_math_geometry,5-shot,accuracy,0.0334029227557411,0.0082186602033359
mosaicml/mpt-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.0464135021097046,0.0096732329328615
mosaicml/mpt-7b,minerva_math_algebra,5-shot,accuracy,0.0362257792754844,0.0054256800066016
mosaicml/mpt-7b,fld_default,0-shot,accuracy,0.0,
mosaicml/mpt-7b,fld_star,0-shot,accuracy,0.0,
mosaicml/mpt-7b,arithmetic_3da,5-shot,accuracy,0.0405,0.0044090355858621
mosaicml/mpt-7b,arithmetic_3ds,5-shot,accuracy,0.017,0.0028913110935905
mosaicml/mpt-7b,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521
mosaicml/mpt-7b,arithmetic_2ds,5-shot,accuracy,0.217,0.0092194359371657
mosaicml/mpt-7b,arithmetic_5ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b,arithmetic_5da,5-shot,accuracy,0.001,0.0007069298939339
mosaicml/mpt-7b,arithmetic_1dc,5-shot,accuracy,0.017,0.0028913110935905
mosaicml/mpt-7b,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005
mosaicml/mpt-7b,arithmetic_2dm,5-shot,accuracy,0.0485,0.0048047286821271
mosaicml/mpt-7b,arithmetic_2da,5-shot,accuracy,0.199,0.0089296903465262
mosaicml/mpt-7b,gsm8k_cot,5-shot,accuracy,0.0856709628506444,0.0077092188558827
mosaicml/mpt-7b,gsm8k,5-shot,accuracy,0.0401819560272934,0.0054094397369705
mosaicml/mpt-7b,anli_r2,0-shot,brier_score,0.7871039264444815,
mosaicml/mpt-7b,anli_r3,0-shot,brier_score,0.7735706458763796,
mosaicml/mpt-7b,anli_r1,0-shot,brier_score,0.8064532070514647,
mosaicml/mpt-7b,xnli_eu,0-shot,brier_score,0.9836769306459834,
mosaicml/mpt-7b,xnli_vi,0-shot,brier_score,0.9706863969461264,
mosaicml/mpt-7b,xnli_ru,0-shot,brier_score,0.7263265977856224,
mosaicml/mpt-7b,xnli_zh,0-shot,brier_score,0.9844979242445993,
mosaicml/mpt-7b,xnli_tr,0-shot,brier_score,0.9073384888674126,
mosaicml/mpt-7b,xnli_fr,0-shot,brier_score,0.7455615780108278,
mosaicml/mpt-7b,xnli_en,0-shot,brier_score,0.6521561020686549,
mosaicml/mpt-7b,xnli_ur,0-shot,brier_score,1.3114352706806065,
mosaicml/mpt-7b,xnli_ar,0-shot,brier_score,1.1879643155024169,
mosaicml/mpt-7b,xnli_de,0-shot,brier_score,0.8140312095660267,
mosaicml/mpt-7b,xnli_hi,0-shot,brier_score,0.9094996182021012,
mosaicml/mpt-7b,xnli_es,0-shot,brier_score,0.9037442585372436,
mosaicml/mpt-7b,xnli_bg,0-shot,brier_score,0.825572748604095,
mosaicml/mpt-7b,xnli_sw,0-shot,brier_score,1.1722319061632498,
mosaicml/mpt-7b,xnli_el,0-shot,brier_score,1.0056819587703962,
mosaicml/mpt-7b,xnli_th,0-shot,brier_score,0.8582353033497166,
mosaicml/mpt-7b,logiqa2,0-shot,brier_score,0.9764895261294206,
mosaicml/mpt-7b,mathqa,0-shot,brier_score,0.9492149034082438,
mosaicml/mpt-7b,lambada_standard,0-shot,perplexity,4.924753971769574,0.107963992307772
mosaicml/mpt-7b,lambada_standard,0-shot,accuracy,0.6194449835047545,0.0067642892220288
mosaicml/mpt-7b,lambada_openai,0-shot,perplexity,3.8685392219566808,0.080952534294837
mosaicml/mpt-7b,lambada_openai,0-shot,accuracy,0.685231903745391,0.0064703267662255
mosaicml/mpt-7b,mmlu_world_religions,0-shot,accuracy,0.3333333333333333,0.0361550763031093
mosaicml/mpt-7b,mmlu_formal_logic,0-shot,accuracy,0.1904761904761904,0.0351220741230205
mosaicml/mpt-7b,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.0257731111696304
mosaicml/mpt-7b,mmlu_moral_scenarios,0-shot,accuracy,0.2513966480446927,0.0145089794535539
mosaicml/mpt-7b,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
mosaicml/mpt-7b,mmlu_moral_disputes,0-shot,accuracy,0.2861271676300578,0.0243321467791341
mosaicml/mpt-7b,mmlu_professional_law,0-shot,accuracy,0.2633637548891786,0.0112495064036052
mosaicml/mpt-7b,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.0332201579577674
mosaicml/mpt-7b,mmlu_high_school_us_history,0-shot,accuracy,0.2696078431372549,0.0311455706594867
mosaicml/mpt-7b,mmlu_philosophy,0-shot,accuracy,0.3086816720257235,0.0262369658811532
mosaicml/mpt-7b,mmlu_jurisprudence,0-shot,accuracy,0.3425925925925926,0.0458790474130181
mosaicml/mpt-7b,mmlu_international_law,0-shot,accuracy,0.2314049586776859,0.0384985609879408
mosaicml/mpt-7b,mmlu_high_school_european_history,0-shot,accuracy,0.2424242424242424,0.0334640988105595
mosaicml/mpt-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.310880829015544,0.0334036190627658
mosaicml/mpt-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2689075630252101,0.0288013921936312
mosaicml/mpt-7b,mmlu_high_school_geography,0-shot,accuracy,0.2272727272727272,0.0298575156733864
mosaicml/mpt-7b,mmlu_high_school_psychology,0-shot,accuracy,0.2623853211009174,0.0188618850215347
mosaicml/mpt-7b,mmlu_public_relations,0-shot,accuracy,0.3090909090909091,0.0442629464820009
mosaicml/mpt-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.46,0.0500908265962033
mosaicml/mpt-7b,mmlu_sociology,0-shot,accuracy,0.2039800995024875,0.028493176245326
mosaicml/mpt-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2743589743589744,0.0226227657674932
mosaicml/mpt-7b,mmlu_security_studies,0-shot,accuracy,0.3224489795918367,0.0299231005636839
mosaicml/mpt-7b,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.0179179740695947
mosaicml/mpt-7b,mmlu_human_sexuality,0-shot,accuracy,0.3053435114503817,0.0403931497872456
mosaicml/mpt-7b,mmlu_econometrics,0-shot,accuracy,0.2894736842105263,0.0426633944315939
mosaicml/mpt-7b,mmlu_miscellaneous,0-shot,accuracy,0.2975734355044699,0.0163491119129094
mosaicml/mpt-7b,mmlu_marketing,0-shot,accuracy,0.3333333333333333,0.0308827369741386
mosaicml/mpt-7b,mmlu_management,0-shot,accuracy,0.2621359223300971,0.0435463107726059
mosaicml/mpt-7b,mmlu_nutrition,0-shot,accuracy,0.2712418300653594,0.0254577566966678
mosaicml/mpt-7b,mmlu_medical_genetics,0-shot,accuracy,0.35,0.0479372485441102
mosaicml/mpt-7b,mmlu_human_aging,0-shot,accuracy,0.3542600896860987,0.0321006215413498
mosaicml/mpt-7b,mmlu_professional_medicine,0-shot,accuracy,0.2058823529411764,0.0245622043141423
mosaicml/mpt-7b,mmlu_college_medicine,0-shot,accuracy,0.2427745664739884,0.0326926380614177
mosaicml/mpt-7b,mmlu_business_ethics,0-shot,accuracy,0.28,0.0451260859854212
mosaicml/mpt-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.2566037735849056,0.0268806478890519
mosaicml/mpt-7b,mmlu_global_facts,0-shot,accuracy,0.22,0.0416333199893226
mosaicml/mpt-7b,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.0361080501803102
mosaicml/mpt-7b,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539,0.0258921511567094
mosaicml/mpt-7b,mmlu_college_physics,0-shot,accuracy,0.2058823529411764,0.0402338227361774
mosaicml/mpt-7b,mmlu_high_school_physics,0-shot,accuracy,0.2516556291390728,0.0354330423438998
mosaicml/mpt-7b,mmlu_high_school_biology,0-shot,accuracy,0.2580645161290322,0.0248924691724628
mosaicml/mpt-7b,mmlu_college_biology,0-shot,accuracy,0.2986111111111111,0.0382705235795075
mosaicml/mpt-7b,mmlu_anatomy,0-shot,accuracy,0.2592592592592592,0.0378571446506665
mosaicml/mpt-7b,mmlu_college_chemistry,0-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b,mmlu_computer_security,0-shot,accuracy,0.27,0.0446196043338473
mosaicml/mpt-7b,mmlu_college_computer_science,0-shot,accuracy,0.35,0.0479372485441102
mosaicml/mpt-7b,mmlu_astronomy,0-shot,accuracy,0.2697368421052631,0.0361178056028489
mosaicml/mpt-7b,mmlu_college_mathematics,0-shot,accuracy,0.35,0.0479372485441101
mosaicml/mpt-7b,mmlu_conceptual_physics,0-shot,accuracy,0.3234042553191489,0.0305794427736103
mosaicml/mpt-7b,mmlu_abstract_algebra,0-shot,accuracy,0.19,0.0394277244403662
mosaicml/mpt-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.042295258468165
mosaicml/mpt-7b,mmlu_machine_learning,0-shot,accuracy,0.3571428571428571,0.0454796099976437
mosaicml/mpt-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.167487684729064,0.0262730860475354
mosaicml/mpt-7b,mmlu_high_school_statistics,0-shot,accuracy,0.2731481481481481,0.0303880513016781
mosaicml/mpt-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.2486772486772486,0.0222618176924001
mosaicml/mpt-7b,mmlu_electrical_engineering,0-shot,accuracy,0.3448275862068966,0.039609335494512
mosaicml/mpt-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2925925925925925,0.027738969632176
mosaicml/mpt-7b,arc_challenge,25-shot,accuracy,0.4351535836177474,0.014487986197186
mosaicml/mpt-7b,arc_challenge,25-shot,acc_norm,0.470136518771331,0.0145853058400071
mosaicml/mpt-7b,hellaswag,10-shot,accuracy,0.5730930093606851,0.0049361767846319
mosaicml/mpt-7b,hellaswag,10-shot,acc_norm,0.7753435570603465,0.0041650291643616
mosaicml/mpt-7b,truthfulqa_mc2,0-shot,accuracy,0.3343157635845394,0.0130891453812285
mosaicml/mpt-7b,truthfulqa_gen,0-shot,bleu_max,24.602330933425307,0.7657159724776746
mosaicml/mpt-7b,truthfulqa_gen,0-shot,bleu_acc,0.3072215422276622,0.016150201321323
mosaicml/mpt-7b,truthfulqa_gen,0-shot,bleu_diff,-9.827181119106774,0.8506396518002634
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge1_max,48.4307380348122,0.9034301197408506
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge1_acc,0.2729498164014688,0.0155947536320065
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge1_diff,-12.051963649063818,0.9400681010531302
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge2_max,32.229261537796205,1.018069487091937
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge2_acc,0.2093023255813953,0.0142412194347858
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge2_diff,-14.473968970783242,1.0917907802458022
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rougeL_max,45.768993552153226,0.907054587417782
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rougeL_acc,0.2680538555691554,0.0155062047228345
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rougeL_diff,-12.622302003402249,0.9501788414558512
mosaicml/mpt-7b,truthfulqa_mc1,0-shot,accuracy,0.204406364749082,0.0141171743374326
mosaicml/mpt-7b,winogrande,5-shot,accuracy,0.7213891081294396,0.0125998966494938
EleutherAI/pythia-160m,arc:challenge,25-shot,accuracy,0.1902730375426621,0.0114704241792256
EleutherAI/pythia-160m,arc:challenge,25-shot,acc_norm,0.227815699658703,0.0122567086023269
EleutherAI/pythia-160m,hellaswag,10-shot,accuracy,0.2874925313682533,0.004516681953879
EleutherAI/pythia-160m,hellaswag,10-shot,acc_norm,0.3034256124278032,0.0045879786255824
EleutherAI/pythia-160m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-160m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/pythia-160m,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.0374985070917402
EleutherAI/pythia-160m,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.0374985070917402
EleutherAI/pythia-160m,hendrycksTest-astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/pythia-160m,hendrycksTest-astronomy,5-shot,acc_norm,0.1776315789473684,0.0311031823831233
EleutherAI/pythia-160m,hendrycksTest-business_ethics,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-160m,hendrycksTest-business_ethics,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-160m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2037735849056604,0.0247907845017754
EleutherAI/pythia-160m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2037735849056604,0.0247907845017754
EleutherAI/pythia-160m,hendrycksTest-college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008034
EleutherAI/pythia-160m,hendrycksTest-college_biology,5-shot,acc_norm,0.2430555555555555,0.0358687928008034
EleutherAI/pythia-160m,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-160m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-160m,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-160m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/pythia-160m,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-160m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.0440844002276807
EleutherAI/pythia-160m,hendrycksTest-college_medicine,5-shot,accuracy,0.2369942196531791,0.0324241475748309
EleutherAI/pythia-160m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2369942196531791,0.0324241475748309
EleutherAI/pythia-160m,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-160m,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-160m,hendrycksTest-computer_security,5-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-160m,hendrycksTest-computer_security,5-shot,acc_norm,0.18,0.0386122919665369
EleutherAI/pythia-160m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.0289573427883423
EleutherAI/pythia-160m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.0289573427883423
EleutherAI/pythia-160m,hendrycksTest-econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
EleutherAI/pythia-160m,hendrycksTest-econometrics,5-shot,acc_norm,0.2456140350877192,0.0404933929774814
EleutherAI/pythia-160m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
EleutherAI/pythia-160m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
EleutherAI/pythia-160m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2513227513227513,0.0223404823396438
EleutherAI/pythia-160m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2513227513227513,0.0223404823396438
EleutherAI/pythia-160m,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.0356701667527686
EleutherAI/pythia-160m,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.0356701667527686
EleutherAI/pythia-160m,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-160m,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.0386122919665369
EleutherAI/pythia-160m,hendrycksTest-high_school_biology,5-shot,accuracy,0.3129032258064516,0.0263775670286458
EleutherAI/pythia-160m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3129032258064516,0.0263775670286458
EleutherAI/pythia-160m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.0312709071329769
EleutherAI/pythia-160m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.0312709071329769
EleutherAI/pythia-160m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-160m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-160m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2484848484848484,0.033744026441394
EleutherAI/pythia-160m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2484848484848484,0.033744026441394
EleutherAI/pythia-160m,hendrycksTest-high_school_geography,5-shot,accuracy,0.2323232323232323,0.0300886294902174
EleutherAI/pythia-160m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2323232323232323,0.0300886294902174
EleutherAI/pythia-160m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2435233160621761,0.0309754363868454
EleutherAI/pythia-160m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2435233160621761,0.0309754363868454
EleutherAI/pythia-160m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2461538461538461,0.021840866990423
EleutherAI/pythia-160m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2461538461538461,0.021840866990423
EleutherAI/pythia-160m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-160m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-160m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2731092436974789,0.0289420040409981
EleutherAI/pythia-160m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2731092436974789,0.0289420040409981
EleutherAI/pythia-160m,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
EleutherAI/pythia-160m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.0363132980396965
EleutherAI/pythia-160m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2238532110091743,0.0178712177677902
EleutherAI/pythia-160m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2238532110091743,0.0178712177677902
EleutherAI/pythia-160m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4768518518518518,0.034063153607115
EleutherAI/pythia-160m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4768518518518518,0.034063153607115
EleutherAI/pythia-160m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
EleutherAI/pythia-160m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2598039215686274,0.0307785546786932
EleutherAI/pythia-160m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2953586497890295,0.0296963387134228
EleutherAI/pythia-160m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2953586497890295,0.0296963387134228
EleutherAI/pythia-160m,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.0310244117405722
EleutherAI/pythia-160m,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.0310244117405722
EleutherAI/pythia-160m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-160m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
EleutherAI/pythia-160m,hendrycksTest-international_law,5-shot,accuracy,0.2231404958677686,0.0380075447522873
EleutherAI/pythia-160m,hendrycksTest-international_law,5-shot,acc_norm,0.2231404958677686,0.0380075447522873
EleutherAI/pythia-160m,hendrycksTest-jurisprudence,5-shot,accuracy,0.2777777777777778,0.0433004374965074
EleutherAI/pythia-160m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2777777777777778,0.0433004374965074
EleutherAI/pythia-160m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2085889570552147,0.0319219344893472
EleutherAI/pythia-160m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2085889570552147,0.0319219344893472
EleutherAI/pythia-160m,hendrycksTest-machine_learning,5-shot,accuracy,0.1875,0.0370468111477387
EleutherAI/pythia-160m,hendrycksTest-machine_learning,5-shot,acc_norm,0.1875,0.0370468111477387
EleutherAI/pythia-160m,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.0391666776282258
EleutherAI/pythia-160m,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.0391666776282258
EleutherAI/pythia-160m,hendrycksTest-marketing,5-shot,accuracy,0.1837606837606837,0.0253721396717229
EleutherAI/pythia-160m,hendrycksTest-marketing,5-shot,acc_norm,0.1837606837606837,0.0253721396717229
EleutherAI/pythia-160m,hendrycksTest-medical_genetics,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/pythia-160m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.32,0.046882617226215
EleutherAI/pythia-160m,hendrycksTest-miscellaneous,5-shot,accuracy,0.2528735632183908,0.0155433773137196
EleutherAI/pythia-160m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2528735632183908,0.0155433773137196
EleutherAI/pythia-160m,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
EleutherAI/pythia-160m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
EleutherAI/pythia-160m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2770949720670391,0.0149687724358121
EleutherAI/pythia-160m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2770949720670391,0.0149687724358121
EleutherAI/pythia-160m,hendrycksTest-nutrition,5-shot,accuracy,0.2352941176470588,0.0242886194660461
EleutherAI/pythia-160m,hendrycksTest-nutrition,5-shot,acc_norm,0.2352941176470588,0.0242886194660461
EleutherAI/pythia-160m,hendrycksTest-philosophy,5-shot,accuracy,0.1897106109324758,0.0222681962587832
EleutherAI/pythia-160m,hendrycksTest-philosophy,5-shot,acc_norm,0.1897106109324758,0.0222681962587832
EleutherAI/pythia-160m,hendrycksTest-prehistory,5-shot,accuracy,0.2592592592592592,0.0243836655310354
EleutherAI/pythia-160m,hendrycksTest-prehistory,5-shot,acc_norm,0.2592592592592592,0.0243836655310354
EleutherAI/pythia-160m,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
EleutherAI/pythia-160m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.0267891723511402
EleutherAI/pythia-160m,hendrycksTest-professional_law,5-shot,accuracy,0.2372881355932203,0.0108654366907802
EleutherAI/pythia-160m,hendrycksTest-professional_law,5-shot,acc_norm,0.2372881355932203,0.0108654366907802
EleutherAI/pythia-160m,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.0302114796091215
EleutherAI/pythia-160m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.0302114796091215
EleutherAI/pythia-160m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2516339869281045,0.0175558180913222
EleutherAI/pythia-160m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2516339869281045,0.0175558180913222
EleutherAI/pythia-160m,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
EleutherAI/pythia-160m,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.0417234303870538
EleutherAI/pythia-160m,hendrycksTest-security_studies,5-shot,accuracy,0.2448979591836734,0.0275296374401749
EleutherAI/pythia-160m,hendrycksTest-security_studies,5-shot,acc_norm,0.2448979591836734,0.0275296374401749
EleutherAI/pythia-160m,hendrycksTest-sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/pythia-160m,hendrycksTest-sociology,5-shot,acc_norm,0.2437810945273631,0.0303604901540146
EleutherAI/pythia-160m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-160m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.0440844002276807
EleutherAI/pythia-160m,hendrycksTest-virology,5-shot,accuracy,0.180722891566265,0.0299557378558101
EleutherAI/pythia-160m,hendrycksTest-virology,5-shot,acc_norm,0.180722891566265,0.0299557378558101
EleutherAI/pythia-160m,hendrycksTest-world_religions,5-shot,accuracy,0.2046783625730994,0.0309444597785332
EleutherAI/pythia-160m,hendrycksTest-world_religions,5-shot,acc_norm,0.2046783625730994,0.0309444597785332
EleutherAI/pythia-160m,truthfulqa:mc,0-shot,mc1,0.2509179926560587,0.0151769850277076
EleutherAI/pythia-160m,truthfulqa:mc,0-shot,mc2,0.4426308210078121,0.0149536433842408
EleutherAI/pythia-160m,drop,3-shot,accuracy,0.0012583892617449,0.0003630560893119
EleutherAI/pythia-160m,drop,3-shot,f1,0.0344987416107383,0.0010696643616809
EleutherAI/pythia-160m,gsm8k,5-shot,accuracy,0.0022744503411675,0.0013121578148674
EleutherAI/pythia-160m,winogrande,5-shot,accuracy,0.5153906866614049,0.0140458267897836
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_precalc,5-shot,accuracy,0.0164835164835164,0.0054540297647667
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_prealgebra,5-shot,accuracy,0.1297359357060849,0.0113918968323855
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_num_theory,5-shot,accuracy,0.0277777777777777,0.0070784332144114
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0287929125138427,0.0055679516014758
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_geometry,5-shot,accuracy,0.0375782881002087,0.0086983575090329
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_counting_and_prob,5-shot,accuracy,0.050632911392405,0.0100809849342132
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_algebra,5-shot,accuracy,0.0657118786857624,0.0071948212875878
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,fld_default,0-shot,accuracy,0.0,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,fld_star,0-shot,accuracy,0.0,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_3da,5-shot,accuracy,0.141,0.0077839446874607
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_3ds,5-shot,accuracy,0.3925,0.010921607746018
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_4da,5-shot,accuracy,0.1515,0.0080191039408407
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_2ds,5-shot,accuracy,0.3235,0.0104632028704004
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_5ds,5-shot,accuracy,0.105,0.0068564572122015
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_5da,5-shot,accuracy,0.053,0.0050107937521926
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_1dc,5-shot,accuracy,0.0,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_4ds,5-shot,accuracy,0.2895,0.0101437824878878
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_2dm,5-shot,accuracy,0.35,0.0106680318452715
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_2da,5-shot,accuracy,0.2235,0.0093175792801466
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,gsm8k_cot,5-shot,accuracy,0.3419257012888552,0.0130660896251828
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,gsm8k,5-shot,accuracy,0.2774829416224412,0.0123334475810475
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,anli_r2,0-shot,brier_score,0.8329852734143958,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,anli_r3,0-shot,brier_score,0.7793826620488095,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,anli_r1,0-shot,brier_score,0.8351897730779554,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_eu,0-shot,brier_score,1.089858260901451,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_vi,0-shot,brier_score,1.0476274173469688,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_ru,0-shot,brier_score,0.910098981607232,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_zh,0-shot,brier_score,1.147378120240311,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_tr,0-shot,brier_score,1.012168006747192,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_fr,0-shot,brier_score,0.880532625822361,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_en,0-shot,brier_score,0.9798329124068352,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_ur,0-shot,brier_score,1.3026970239585185,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_ar,0-shot,brier_score,1.1205455462723757,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_de,0-shot,brier_score,0.9866185162056196,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_hi,0-shot,brier_score,1.049863204263663,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_es,0-shot,brier_score,0.944072774960891,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_bg,0-shot,brier_score,1.116276803181401,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_sw,0-shot,brier_score,1.0313388576056377,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_el,0-shot,brier_score,0.9209651581499028,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_th,0-shot,brier_score,1.1454058334629815,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,logiqa2,0-shot,brier_score,1.0434407612473977,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,mathqa,0-shot,brier_score,0.9620259901451524,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_standard,0-shot,perplexity,4.372930344427915,0.1373038297230332
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_standard,0-shot,accuracy,0.6532117213273821,0.0066308814343385
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_openai,0-shot,perplexity,3.642807427442857,0.1201146598857889
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_openai,0-shot,accuracy,0.6984281001358432,0.0063939371193314
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_world_religions,0-shot,accuracy,0.2631578947368421,0.0337731025220919
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_formal_logic,0-shot,accuracy,0.1666666666666666,0.0333333333333333
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_prehistory,0-shot,accuracy,0.2345679012345679,0.0235768817440057
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_world_history,0-shot,accuracy,0.2320675105485232,0.0274797445508085
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_moral_disputes,0-shot,accuracy,0.2052023121387283,0.0217425198352762
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_professional_law,0-shot,accuracy,0.2392438070404172,0.0108961236526766
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_logical_fallacies,0-shot,accuracy,0.2576687116564417,0.0343615082784691
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_us_history,0-shot,accuracy,0.3039215686274509,0.0322821038703789
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_philosophy,0-shot,accuracy,0.2733118971061093,0.0253117659754261
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_jurisprudence,0-shot,accuracy,0.2037037037037037,0.0389354251882484
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_international_law,0-shot,accuracy,0.3553719008264462,0.0436923632657398
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_european_history,0-shot,accuracy,0.2848484848484848,0.0352439084451178
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.0346971379170437
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_microeconomics,0-shot,accuracy,0.2184873949579832,0.0268415143229589
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_geography,0-shot,accuracy,0.3333333333333333,0.0335861814573252
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_psychology,0-shot,accuracy,0.3192660550458716,0.01998782906975
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.0426579211094058
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_sociology,0-shot,accuracy,0.2487562189054726,0.0305676759389167
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_macroeconomics,0-shot,accuracy,0.358974358974359,0.0243217384846023
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_security_studies,0-shot,accuracy,0.4,0.0313625024093589
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_professional_psychology,0-shot,accuracy,0.2320261437908496,0.017077373377857
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_human_sexuality,0-shot,accuracy,0.2900763358778626,0.0398006624646776
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_econometrics,0-shot,accuracy,0.2105263157894736,0.0383515395439942
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_miscellaneous,0-shot,accuracy,0.2452107279693486,0.0153843522845439
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_marketing,0-shot,accuracy,0.188034188034188,0.0255981936866522
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_management,0-shot,accuracy,0.2621359223300971,0.0435463107726059
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_nutrition,0-shot,accuracy,0.2549019607843137,0.0249541843248799
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_medical_genetics,0-shot,accuracy,0.29,0.0456048021572068
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_human_aging,0-shot,accuracy,0.1434977578475336,0.0235293712696181
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_professional_medicine,0-shot,accuracy,0.4448529411764705,0.0301875320603293
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_college_medicine,0-shot,accuracy,0.2658959537572254,0.0336876293225942
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_business_ethics,0-shot,accuracy,0.15,0.0358870281282637
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_clinical_knowledge,0-shot,accuracy,0.1962264150943396,0.0244423881311008
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_global_facts,0-shot,accuracy,0.15,0.0358870281282636
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_virology,0-shot,accuracy,0.2108433734939759,0.0317555478662991
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_professional_accounting,0-shot,accuracy,0.2801418439716312,0.0267891723511402
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_college_physics,0-shot,accuracy,0.1764705882352941,0.0379328118530781
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_physics,0-shot,accuracy,0.2317880794701986,0.0344540627198705
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_biology,0-shot,accuracy,0.2806451612903226,0.0255606047210228
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_college_biology,0-shot,accuracy,0.2430555555555555,0.0358687928008034
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_anatomy,0-shot,accuracy,0.3481481481481481,0.0411532461033695
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_college_chemistry,0-shot,accuracy,0.23,0.042295258468165
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_computer_security,0-shot,accuracy,0.2,0.0402015126103684
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_astronomy,0-shot,accuracy,0.1578947368421052,0.0296741675201014
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_college_mathematics,0-shot,accuracy,0.24,0.0429234695990928
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_conceptual_physics,0-shot,accuracy,0.2382978723404255,0.0278512529738897
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.0429234695990928
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_computer_science,0-shot,accuracy,0.28,0.0451260859854212
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_machine_learning,0-shot,accuracy,0.25,0.0410997468263393
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_chemistry,0-shot,accuracy,0.2758620689655172,0.0314471258167824
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_elementary_mathematics,0-shot,accuracy,0.2592592592592592,0.0225698970749184
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_electrical_engineering,0-shot,accuracy,0.2275862068965517,0.0349395038013118
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,mmlu_high_school_mathematics,0-shot,accuracy,0.2814814814814815,0.0274200193509452
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,arc_challenge,25-shot,accuracy,0.1800341296928327,0.01122785672905
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,arc_challenge,25-shot,acc_norm,0.2107508532423208,0.0119182717548521
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,hellaswag,10-shot,accuracy,0.2699661422027484,0.0044303462346503
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,hellaswag,10-shot,acc_norm,0.2791276638119896,0.0044765365690565
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_mc2,0-shot,accuracy,0.4784520063855078,0.0158604878240123
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,bleu_max,14.3040447069211,0.5579122226826115
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,bleu_acc,0.3292533659730722,0.0164512644400682
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,bleu_diff,0.4373309410207555,0.4644469788436445
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge1_max,33.591622547180364,0.8000229732541625
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge1_acc,0.3219094247246022,0.0163555676119603
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge1_diff,-2.421367164569093,0.7119318106950187
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge2_max,17.86815023737885,0.8222315318558643
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge2_acc,0.200734394124847,0.0140220457174821
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge2_diff,-0.7390323263152964,0.6781232524313207
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rougeL_max,30.97952552658297,0.7772749173461312
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rougeL_acc,0.3219094247246022,0.0163555676119604
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rougeL_diff,-1.8241322231169368,0.6918870034374448
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,truthfulqa_mc1,0-shot,accuracy,0.2656058751529987,0.0154610276272535
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,winogrande,5-shot,accuracy,0.4853985793212312,0.0140464923832758
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_0_code_100,gsm8k,5-shot,accuracy,0.0136467020470053,0.0031957470754807
HuggingFaceTB/SmolLM-1.7B,mmlu_world_religions,0-shot,accuracy,0.3040935672514619,0.0352821125824523
HuggingFaceTB/SmolLM-1.7B,mmlu_formal_logic,0-shot,accuracy,0.2142857142857142,0.0367006645104718
HuggingFaceTB/SmolLM-1.7B,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.0257731111696304
HuggingFaceTB/SmolLM-1.7B,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_world_history,0-shot,accuracy,0.3248945147679324,0.0304860393891052
HuggingFaceTB/SmolLM-1.7B,mmlu_moral_disputes,0-shot,accuracy,0.3294797687861271,0.0253052581318797
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_law,0-shot,accuracy,0.2666232073011734,0.0112938360316121
HuggingFaceTB/SmolLM-1.7B,mmlu_logical_fallacies,0-shot,accuracy,0.2576687116564417,0.0343615082784691
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_us_history,0-shot,accuracy,0.3039215686274509,0.0322821038703789
HuggingFaceTB/SmolLM-1.7B,mmlu_philosophy,0-shot,accuracy,0.3344051446945337,0.0267954223278939
HuggingFaceTB/SmolLM-1.7B,mmlu_jurisprudence,0-shot,accuracy,0.324074074074074,0.0452459600703004
HuggingFaceTB/SmolLM-1.7B,mmlu_international_law,0-shot,accuracy,0.4214876033057851,0.0450773227877509
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_european_history,0-shot,accuracy,0.303030303030303,0.035886248000917
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2538860103626943,0.0314102478056531
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678638
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_geography,0-shot,accuracy,0.2676767676767677,0.0315444988827028
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_psychology,0-shot,accuracy,0.2568807339449541,0.0187324929283424
HuggingFaceTB/SmolLM-1.7B,mmlu_public_relations,0-shot,accuracy,0.3090909090909091,0.0442629464820009
HuggingFaceTB/SmolLM-1.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.0464823198711731
HuggingFaceTB/SmolLM-1.7B,mmlu_sociology,0-shot,accuracy,0.2885572139303483,0.0320384104021332
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2615384615384615,0.0222821412042044
HuggingFaceTB/SmolLM-1.7B,mmlu_security_studies,0-shot,accuracy,0.2489795918367346,0.0276829795229602
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_psychology,0-shot,accuracy,0.2712418300653594,0.0179866153040303
HuggingFaceTB/SmolLM-1.7B,mmlu_human_sexuality,0-shot,accuracy,0.2977099236641221,0.040103589424622
HuggingFaceTB/SmolLM-1.7B,mmlu_econometrics,0-shot,accuracy,0.2105263157894736,0.0383515395439941
HuggingFaceTB/SmolLM-1.7B,mmlu_miscellaneous,0-shot,accuracy,0.3141762452107279,0.0165992917358849
HuggingFaceTB/SmolLM-1.7B,mmlu_marketing,0-shot,accuracy,0.3162393162393162,0.0304636567473402
HuggingFaceTB/SmolLM-1.7B,mmlu_management,0-shot,accuracy,0.2815533980582524,0.0445325483632646
HuggingFaceTB/SmolLM-1.7B,mmlu_nutrition,0-shot,accuracy,0.2941176470588235,0.026090162504279
HuggingFaceTB/SmolLM-1.7B,mmlu_medical_genetics,0-shot,accuracy,0.28,0.0451260859854212
HuggingFaceTB/SmolLM-1.7B,mmlu_human_aging,0-shot,accuracy,0.2869955156950672,0.0303603797102919
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_medicine,0-shot,accuracy,0.2536764705882353,0.0264313298707895
HuggingFaceTB/SmolLM-1.7B,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
HuggingFaceTB/SmolLM-1.7B,mmlu_business_ethics,0-shot,accuracy,0.28,0.0451260859854212
HuggingFaceTB/SmolLM-1.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.2641509433962264,0.0271342916287417
HuggingFaceTB/SmolLM-1.7B,mmlu_global_facts,0-shot,accuracy,0.36,0.0482418151324421
HuggingFaceTB/SmolLM-1.7B,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.0352948680151111
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902
HuggingFaceTB/SmolLM-1.7B,mmlu_college_physics,0-shot,accuracy,0.196078431372549,0.0395058186117996
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.0368488152138902
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_biology,0-shot,accuracy,0.2903225806451613,0.0258221061194158
HuggingFaceTB/SmolLM-1.7B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
HuggingFaceTB/SmolLM-1.7B,mmlu_anatomy,0-shot,accuracy,0.3259259259259259,0.040491220417025
HuggingFaceTB/SmolLM-1.7B,mmlu_college_chemistry,0-shot,accuracy,0.25,0.0435194139889244
HuggingFaceTB/SmolLM-1.7B,mmlu_computer_security,0-shot,accuracy,0.27,0.0446196043338474
HuggingFaceTB/SmolLM-1.7B,mmlu_college_computer_science,0-shot,accuracy,0.2,0.0402015126103684
HuggingFaceTB/SmolLM-1.7B,mmlu_astronomy,0-shot,accuracy,0.3223684210526316,0.0380351024835158
HuggingFaceTB/SmolLM-1.7B,mmlu_college_mathematics,0-shot,accuracy,0.25,0.0435194139889244
HuggingFaceTB/SmolLM-1.7B,mmlu_conceptual_physics,0-shot,accuracy,0.2936170212765957,0.0297716427124912
HuggingFaceTB/SmolLM-1.7B,mmlu_abstract_algebra,0-shot,accuracy,0.27,0.0446196043338474
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.32,0.046882617226215
HuggingFaceTB/SmolLM-1.7B,mmlu_machine_learning,0-shot,accuracy,0.25,0.0410997468263393
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.0310898260029375
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_statistics,0-shot,accuracy,0.1759259259259259,0.0259674209582585
HuggingFaceTB/SmolLM-1.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.2724867724867725,0.0229309730716333
HuggingFaceTB/SmolLM-1.7B,mmlu_electrical_engineering,0-shot,accuracy,0.2689655172413793,0.0369518331165023
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
HuggingFaceTB/SmolLM-1.7B,arc_challenge,25-shot,accuracy,0.4616040955631399,0.0145682455502963
HuggingFaceTB/SmolLM-1.7B,arc_challenge,25-shot,acc_norm,0.4897610921501706,0.014608326906285
HuggingFaceTB/SmolLM-1.7B,hellaswag,10-shot,accuracy,0.4985062736506672,0.0049897591448122
HuggingFaceTB/SmolLM-1.7B,hellaswag,10-shot,acc_norm,0.673770165305716,0.0046787435637666
HuggingFaceTB/SmolLM-1.7B,truthfulqa_mc2,0-shot,accuracy,0.3850766766083665,0.0141318041269147
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,bleu_max,25.637781985326768,0.7862985780149876
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,bleu_acc,0.3243574051407589,0.0163879767796479
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,bleu_diff,-7.032993037032112,0.8260513065264542
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge1_max,50.02333005649277,0.888601487283363
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge1_acc,0.2974296205630355,0.016002651487361
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge1_diff,-9.43923743956951,0.9261892834631156
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge2_max,33.70014727089389,1.0124553331073003
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge2_acc,0.2545899632802937,0.0152501170791564
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge2_diff,-11.30035982468476,1.0978598755923092
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rougeL_max,47.21056335206833,0.8970416486465508
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rougeL_acc,0.2839657282741738,0.0157853708583967
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rougeL_diff,-9.830444388716176,0.9254935688315172
HuggingFaceTB/SmolLM-1.7B,truthfulqa_mc1,0-shot,accuracy,0.244798041615667,0.015051869486715
HuggingFaceTB/SmolLM-1.7B,winogrande,5-shot,accuracy,0.611681136543015,0.0136974566584572
HuggingFaceTB/SmolLM-1.7B,gsm8k,5-shot,accuracy,0.0523123578468536,0.0061330577089592
mistralai/Mixtral-8x7B-v0.1,minerva_math_precalc,5-shot,accuracy,0.1355311355311355,0.0146620928473954
mistralai/Mixtral-8x7B-v0.1,minerva_math_prealgebra,5-shot,accuracy,0.4890929965556831,0.0169475538965277
mistralai/Mixtral-8x7B-v0.1,minerva_math_num_theory,5-shot,accuracy,0.1703703703703703,0.0161936511111117
mistralai/Mixtral-8x7B-v0.1,minerva_math_intermediate_algebra,5-shot,accuracy,0.1173864894795127,0.0107174403304311
mistralai/Mixtral-8x7B-v0.1,minerva_math_geometry,5-shot,accuracy,0.2129436325678496,0.0187249772732632
mistralai/Mixtral-8x7B-v0.1,minerva_math_counting_and_prob,5-shot,accuracy,0.1962025316455696,0.0182597593715657
mistralai/Mixtral-8x7B-v0.1,minerva_math_algebra,5-shot,accuracy,0.4018534119629318,0.0142362399840764
mistralai/Mixtral-8x7B-v0.1,fld_default,0-shot,accuracy,0.0,
mistralai/Mixtral-8x7B-v0.1,fld_star,0-shot,accuracy,0.0,
mistralai/Mixtral-8x7B-v0.1,arithmetic_3da,5-shot,accuracy,0.9945,0.0016541593398342
mistralai/Mixtral-8x7B-v0.1,arithmetic_3ds,5-shot,accuracy,0.9995,0.0005
mistralai/Mixtral-8x7B-v0.1,arithmetic_4da,5-shot,accuracy,0.983,0.0028913110935905
mistralai/Mixtral-8x7B-v0.1,arithmetic_2ds,5-shot,accuracy,1.0,
mistralai/Mixtral-8x7B-v0.1,arithmetic_5ds,5-shot,accuracy,0.982,0.0029736208922129
mistralai/Mixtral-8x7B-v0.1,arithmetic_5da,5-shot,accuracy,0.9755,0.0034577236625362
mistralai/Mixtral-8x7B-v0.1,arithmetic_1dc,5-shot,accuracy,0.935,0.0055138644661141
mistralai/Mixtral-8x7B-v0.1,arithmetic_4ds,5-shot,accuracy,0.995,0.0015775754727385
mistralai/Mixtral-8x7B-v0.1,arithmetic_2dm,5-shot,accuracy,0.9525,0.0047574354011167
mistralai/Mixtral-8x7B-v0.1,arithmetic_2da,5-shot,accuracy,1.0,
mistralai/Mixtral-8x7B-v0.1,gsm8k_cot,5-shot,accuracy,0.6072782410917361,0.0134517453495865
mistralai/Mixtral-8x7B-v0.1,anli_r2,0-shot,brier_score,0.7204381849274958,
mistralai/Mixtral-8x7B-v0.1,anli_r3,0-shot,brier_score,0.7096885290045314,
mistralai/Mixtral-8x7B-v0.1,anli_r1,0-shot,brier_score,0.7298870357934575,
mistralai/Mixtral-8x7B-v0.1,xnli_eu,0-shot,brier_score,0.89697426333893,
mistralai/Mixtral-8x7B-v0.1,xnli_vi,0-shot,brier_score,0.852492855484468,
mistralai/Mixtral-8x7B-v0.1,xnli_ru,0-shot,brier_score,0.8140281538289218,
mistralai/Mixtral-8x7B-v0.1,xnli_zh,0-shot,brier_score,0.9735386004468622,
mistralai/Mixtral-8x7B-v0.1,xnli_tr,0-shot,brier_score,0.85461427730411,
mistralai/Mixtral-8x7B-v0.1,xnli_fr,0-shot,brier_score,0.7191029684114558,
mistralai/Mixtral-8x7B-v0.1,xnli_en,0-shot,brier_score,0.66142672854583,
mistralai/Mixtral-8x7B-v0.1,xnli_ur,0-shot,brier_score,0.9375055471719588,
mistralai/Mixtral-8x7B-v0.1,xnli_ar,0-shot,brier_score,1.170226749895146,
mistralai/Mixtral-8x7B-v0.1,xnli_de,0-shot,brier_score,0.7939831042563315,
mistralai/Mixtral-8x7B-v0.1,xnli_hi,0-shot,brier_score,0.8257619603057463,
mistralai/Mixtral-8x7B-v0.1,xnli_es,0-shot,brier_score,0.7897584112368303,
mistralai/Mixtral-8x7B-v0.1,xnli_bg,0-shot,brier_score,0.8047571756917079,
mistralai/Mixtral-8x7B-v0.1,xnli_sw,0-shot,brier_score,0.7954490376583093,
mistralai/Mixtral-8x7B-v0.1,xnli_el,0-shot,brier_score,0.8321932402650458,
mistralai/Mixtral-8x7B-v0.1,xnli_th,0-shot,brier_score,0.79821941139044,
mistralai/Mixtral-8x7B-v0.1,logiqa2,0-shot,brier_score,0.8376936123486322,
mistralai/Mixtral-8x7B-v0.1,mathqa,0-shot,brier_score,0.7425151563861571,
mistralai/Mixtral-8x7B-v0.1,lambada_standard,0-shot,perplexity,3.317198599562467,0.0596352894432158
mistralai/Mixtral-8x7B-v0.1,lambada_standard,0-shot,accuracy,0.7316126528235979,0.0061735318849105
mistralai/Mixtral-8x7B-v0.1,lambada_openai,0-shot,perplexity,2.788140791153981,0.0471405038198365
mistralai/Mixtral-8x7B-v0.1,lambada_openai,0-shot,accuracy,0.7822627595575393,0.0057498267352873
mistralai/Mixtral-8x7B-v0.1,mmlu_world_religions,0-shot,accuracy,0.8713450292397661,0.0256793427232769
mistralai/Mixtral-8x7B-v0.1,mmlu_formal_logic,0-shot,accuracy,0.5714285714285714,0.0442626668137991
mistralai/Mixtral-8x7B-v0.1,mmlu_prehistory,0-shot,accuracy,0.8425925925925926,0.0202637649963857
mistralai/Mixtral-8x7B-v0.1,mmlu_moral_scenarios,0-shot,accuracy,0.4100558659217877,0.016449708209026
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_world_history,0-shot,accuracy,0.8860759493670886,0.0206817451358845
mistralai/Mixtral-8x7B-v0.1,mmlu_moral_disputes,0-shot,accuracy,0.8005780346820809,0.0215119006542525
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_law,0-shot,accuracy,0.5423728813559322,0.0127242965509801
mistralai/Mixtral-8x7B-v0.1,mmlu_logical_fallacies,0-shot,accuracy,0.7852760736196319,0.0322621937728677
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_us_history,0-shot,accuracy,0.8627450980392157,0.0241522259628015
mistralai/Mixtral-8x7B-v0.1,mmlu_philosophy,0-shot,accuracy,0.7877813504823151,0.0232227567974351
mistralai/Mixtral-8x7B-v0.1,mmlu_jurisprudence,0-shot,accuracy,0.8333333333333334,0.0360281417639264
mistralai/Mixtral-8x7B-v0.1,mmlu_international_law,0-shot,accuracy,0.859504132231405,0.0317223342600215
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_european_history,0-shot,accuracy,0.806060606060606,0.030874145136562
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9430051813471504,0.0167310852936075
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_microeconomics,0-shot,accuracy,0.7815126050420168,0.0268415143229589
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_geography,0-shot,accuracy,0.8636363636363636,0.0244501559731898
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_psychology,0-shot,accuracy,0.8844036697247707,0.0137087495341726
mistralai/Mixtral-8x7B-v0.1,mmlu_public_relations,0-shot,accuracy,0.6909090909090909,0.0442629464820009
mistralai/Mixtral-8x7B-v0.1,mmlu_us_foreign_policy,0-shot,accuracy,0.93,0.0256432399976242
mistralai/Mixtral-8x7B-v0.1,mmlu_sociology,0-shot,accuracy,0.8905472636815921,0.0220763261018246
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7128205128205128,0.0229399254185306
mistralai/Mixtral-8x7B-v0.1,mmlu_security_studies,0-shot,accuracy,0.7877551020408163,0.0261769671978667
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_psychology,0-shot,accuracy,0.7794117647058824,0.0167746723654685
mistralai/Mixtral-8x7B-v0.1,mmlu_human_sexuality,0-shot,accuracy,0.816793893129771,0.0339277092649473
mistralai/Mixtral-8x7B-v0.1,mmlu_econometrics,0-shot,accuracy,0.6140350877192983,0.0457963942207043
mistralai/Mixtral-8x7B-v0.1,mmlu_miscellaneous,0-shot,accuracy,0.879948914431673,0.0116227366920412
mistralai/Mixtral-8x7B-v0.1,mmlu_marketing,0-shot,accuracy,0.9145299145299144,0.0183158916856258
mistralai/Mixtral-8x7B-v0.1,mmlu_management,0-shot,accuracy,0.8737864077669902,0.0328818027880862
mistralai/Mixtral-8x7B-v0.1,mmlu_nutrition,0-shot,accuracy,0.8300653594771242,0.0215053831212313
mistralai/Mixtral-8x7B-v0.1,mmlu_medical_genetics,0-shot,accuracy,0.75,0.0435194139889244
mistralai/Mixtral-8x7B-v0.1,mmlu_human_aging,0-shot,accuracy,0.7847533632286996,0.0275840666022082
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_medicine,0-shot,accuracy,0.8014705882352942,0.0242310133705411
mistralai/Mixtral-8x7B-v0.1,mmlu_college_medicine,0-shot,accuracy,0.7167630057803468,0.0343556805604787
mistralai/Mixtral-8x7B-v0.1,mmlu_business_ethics,0-shot,accuracy,0.75,0.0435194139889244
mistralai/Mixtral-8x7B-v0.1,mmlu_clinical_knowledge,0-shot,accuracy,0.7849056603773585,0.0252883945028913
mistralai/Mixtral-8x7B-v0.1,mmlu_global_facts,0-shot,accuracy,0.47,0.0501613558046592
mistralai/Mixtral-8x7B-v0.1,mmlu_virology,0-shot,accuracy,0.5120481927710844,0.0389136449583581
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_accounting,0-shot,accuracy,0.524822695035461,0.0297907192438297
mistralai/Mixtral-8x7B-v0.1,mmlu_college_physics,0-shot,accuracy,0.4705882352941176,0.0496657090397852
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_physics,0-shot,accuracy,0.4834437086092715,0.0408024418562897
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_biology,0-shot,accuracy,0.8451612903225807,0.0205792873265832
mistralai/Mixtral-8x7B-v0.1,mmlu_college_biology,0-shot,accuracy,0.8541666666666666,0.0295142459642917
mistralai/Mixtral-8x7B-v0.1,mmlu_anatomy,0-shot,accuracy,0.6962962962962963,0.0397255288478513
mistralai/Mixtral-8x7B-v0.1,mmlu_college_chemistry,0-shot,accuracy,0.53,0.0501613558046592
mistralai/Mixtral-8x7B-v0.1,mmlu_computer_security,0-shot,accuracy,0.82,0.0386122919665369
mistralai/Mixtral-8x7B-v0.1,mmlu_college_computer_science,0-shot,accuracy,0.6,0.049236596391733
mistralai/Mixtral-8x7B-v0.1,mmlu_astronomy,0-shot,accuracy,0.8157894736842105,0.0315469804508223
mistralai/Mixtral-8x7B-v0.1,mmlu_college_mathematics,0-shot,accuracy,0.5,0.0502518907629606
mistralai/Mixtral-8x7B-v0.1,mmlu_conceptual_physics,0-shot,accuracy,0.6680851063829787,0.0307837367577456
mistralai/Mixtral-8x7B-v0.1,mmlu_abstract_algebra,0-shot,accuracy,0.34,0.0476095228569523
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_computer_science,0-shot,accuracy,0.71,0.0456048021572068
mistralai/Mixtral-8x7B-v0.1,mmlu_machine_learning,0-shot,accuracy,0.5714285714285714,0.0469711392301021
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_chemistry,0-shot,accuracy,0.6305418719211823,0.0339597038199857
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_statistics,0-shot,accuracy,0.6527777777777778,0.0324688724363764
mistralai/Mixtral-8x7B-v0.1,mmlu_elementary_mathematics,0-shot,accuracy,0.4788359788359788,0.0257282309521307
mistralai/Mixtral-8x7B-v0.1,mmlu_electrical_engineering,0-shot,accuracy,0.6827586206896552,0.0387835237213862
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_mathematics,0-shot,accuracy,0.3518518518518518,0.029116617606083
mistralai/Mixtral-8x7B-v0.1,arc_challenge,25-shot,accuracy,0.6348122866894198,0.0140702655192688
mistralai/Mixtral-8x7B-v0.1,arc_challenge,25-shot,acc_norm,0.6655290102389079,0.0137874603224413
mistralai/Mixtral-8x7B-v0.1,truthfulqa_mc2,0-shot,accuracy,0.4861151486669348,0.0145762172924326
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,bleu_max,27.87574335360594,0.8116531950644869
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,bleu_acc,0.4430844553243574,0.0173897303468771
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,bleu_diff,-2.501118977515409,0.8046394649945218
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge1_max,53.64568184726008,0.863385829484222
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge1_acc,0.4430844553243574,0.0173897303468771
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge1_diff,-3.586087198522764,0.9509186358018956
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge2_max,38.03594067433759,1.016784051264887
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge2_acc,0.3757649938800489,0.0169545840602142
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge2_diff,-4.7518222494468025,1.101614085994352
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rougeL_max,50.9726724729725,0.8717408425200354
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rougeL_acc,0.430844553243574,0.0173352724753323
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rougeL_diff,-3.686864596715992,0.9550700976719012
mistralai/Mixtral-8x7B-v0.1,truthfulqa_mc1,0-shot,accuracy,0.3451652386780905,0.0166431033192749
EleutherAI/pythia-12b,arc:challenge,25-shot,accuracy,0.3737201365187713,0.014137708601759
EleutherAI/pythia-12b,arc:challenge,25-shot,acc_norm,0.3959044368600682,0.0142912283935365
EleutherAI/pythia-12b,hellaswag,10-shot,accuracy,0.502688707428799,0.004989709267191
EleutherAI/pythia-12b,hellaswag,10-shot,acc_norm,0.688209520015933,0.0046227805752091
EleutherAI/pythia-12b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-12b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/pythia-12b,hendrycksTest-anatomy,5-shot,accuracy,0.3333333333333333,0.0407231481187683
EleutherAI/pythia-12b,hendrycksTest-anatomy,5-shot,acc_norm,0.3333333333333333,0.0407231481187683
EleutherAI/pythia-12b,hendrycksTest-astronomy,5-shot,accuracy,0.2368421052631578,0.0345977760681053
EleutherAI/pythia-12b,hendrycksTest-astronomy,5-shot,acc_norm,0.2368421052631578,0.0345977760681053
EleutherAI/pythia-12b,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-12b,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.0409360180740332
EleutherAI/pythia-12b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.260377358490566,0.027008766090708
EleutherAI/pythia-12b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.260377358490566,0.027008766090708
EleutherAI/pythia-12b,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
EleutherAI/pythia-12b,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.0368565109589753
EleutherAI/pythia-12b,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-12b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-12b,hendrycksTest-college_computer_science,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-12b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-12b,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-12b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/pythia-12b,hendrycksTest-college_medicine,5-shot,accuracy,0.2312138728323699,0.0321473730202947
EleutherAI/pythia-12b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2312138728323699,0.0321473730202947
EleutherAI/pythia-12b,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-12b,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-12b,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.0487831731214563
EleutherAI/pythia-12b,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.0487831731214563
EleutherAI/pythia-12b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.0275017529444124
EleutherAI/pythia-12b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.0275017529444124
EleutherAI/pythia-12b,hendrycksTest-econometrics,5-shot,accuracy,0.2017543859649122,0.0377520501358363
EleutherAI/pythia-12b,hendrycksTest-econometrics,5-shot,acc_norm,0.2017543859649122,0.0377520501358363
EleutherAI/pythia-12b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2758620689655172,0.0372456361977463
EleutherAI/pythia-12b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2758620689655172,0.0372456361977463
EleutherAI/pythia-12b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2698412698412698,0.022860838309232
EleutherAI/pythia-12b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2698412698412698,0.022860838309232
EleutherAI/pythia-12b,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.0356701667527686
EleutherAI/pythia-12b,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.0356701667527686
EleutherAI/pythia-12b,hendrycksTest-global_facts,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-12b,hendrycksTest-global_facts,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-12b,hendrycksTest-high_school_biology,5-shot,accuracy,0.3161290322580645,0.0264508744890427
EleutherAI/pythia-12b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3161290322580645,0.0264508744890427
EleutherAI/pythia-12b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.0312709071329769
EleutherAI/pythia-12b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.0312709071329769
EleutherAI/pythia-12b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-12b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/pythia-12b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.1818181818181818,0.0301176889295035
EleutherAI/pythia-12b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.1818181818181818,0.0301176889295035
EleutherAI/pythia-12b,hendrycksTest-high_school_geography,5-shot,accuracy,0.3282828282828283,0.0334567842275677
EleutherAI/pythia-12b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3282828282828283,0.0334567842275677
EleutherAI/pythia-12b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.0325771407770966
EleutherAI/pythia-12b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.0325771407770966
EleutherAI/pythia-12b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2641025641025641,0.0223521937374532
EleutherAI/pythia-12b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2641025641025641,0.0223521937374532
EleutherAI/pythia-12b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2703703703703703,0.0270803728151456
EleutherAI/pythia-12b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2703703703703703,0.0270803728151456
EleutherAI/pythia-12b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2352941176470588,0.0275536144678637
EleutherAI/pythia-12b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2352941176470588,0.0275536144678637
EleutherAI/pythia-12b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
EleutherAI/pythia-12b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
EleutherAI/pythia-12b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2678899082568807,0.0189874622579786
EleutherAI/pythia-12b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2678899082568807,0.0189874622579786
EleutherAI/pythia-12b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3657407407407407,0.032847388576472
EleutherAI/pythia-12b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3657407407407407,0.032847388576472
EleutherAI/pythia-12b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2745098039215686,0.0313217980308329
EleutherAI/pythia-12b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2745098039215686,0.0313217980308329
EleutherAI/pythia-12b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2531645569620253,0.0283046579430353
EleutherAI/pythia-12b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2531645569620253,0.0283046579430353
EleutherAI/pythia-12b,hendrycksTest-human_aging,5-shot,accuracy,0.2869955156950672,0.0303603797102919
EleutherAI/pythia-12b,hendrycksTest-human_aging,5-shot,acc_norm,0.2869955156950672,0.0303603797102919
EleutherAI/pythia-12b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
EleutherAI/pythia-12b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2442748091603053,0.0376833595972874
EleutherAI/pythia-12b,hendrycksTest-international_law,5-shot,accuracy,0.2975206611570248,0.0417334914808349
EleutherAI/pythia-12b,hendrycksTest-international_law,5-shot,acc_norm,0.2975206611570248,0.0417334914808349
EleutherAI/pythia-12b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2685185185185185,0.0428446796805219
EleutherAI/pythia-12b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2685185185185185,0.0428446796805219
EleutherAI/pythia-12b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.294478527607362,0.0358116579047408
EleutherAI/pythia-12b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.294478527607362,0.0358116579047408
EleutherAI/pythia-12b,hendrycksTest-machine_learning,5-shot,accuracy,0.3303571428571428,0.0446428571428571
EleutherAI/pythia-12b,hendrycksTest-machine_learning,5-shot,acc_norm,0.3303571428571428,0.0446428571428571
EleutherAI/pythia-12b,hendrycksTest-management,5-shot,accuracy,0.2135922330097087,0.0405804201564603
EleutherAI/pythia-12b,hendrycksTest-management,5-shot,acc_norm,0.2135922330097087,0.0405804201564603
EleutherAI/pythia-12b,hendrycksTest-marketing,5-shot,accuracy,0.2649572649572649,0.0289112088027494
EleutherAI/pythia-12b,hendrycksTest-marketing,5-shot,acc_norm,0.2649572649572649,0.0289112088027494
EleutherAI/pythia-12b,hendrycksTest-medical_genetics,5-shot,accuracy,0.19,0.0394277244403662
EleutherAI/pythia-12b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.19,0.0394277244403662
EleutherAI/pythia-12b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2541507024265645,0.0155692546920457
EleutherAI/pythia-12b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2541507024265645,0.0155692546920457
EleutherAI/pythia-12b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2890173410404624,0.0244051739357832
EleutherAI/pythia-12b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2890173410404624,0.0244051739357832
EleutherAI/pythia-12b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
EleutherAI/pythia-12b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
EleutherAI/pythia-12b,hendrycksTest-nutrition,5-shot,accuracy,0.2679738562091503,0.0253606037962425
EleutherAI/pythia-12b,hendrycksTest-nutrition,5-shot,acc_norm,0.2679738562091503,0.0253606037962425
EleutherAI/pythia-12b,hendrycksTest-philosophy,5-shot,accuracy,0.3022508038585209,0.0260827006953996
EleutherAI/pythia-12b,hendrycksTest-philosophy,5-shot,acc_norm,0.3022508038585209,0.0260827006953996
EleutherAI/pythia-12b,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.0245692236004608
EleutherAI/pythia-12b,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.0245692236004608
EleutherAI/pythia-12b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2659574468085106,0.0263580656988805
EleutherAI/pythia-12b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2659574468085106,0.0263580656988805
EleutherAI/pythia-12b,hendrycksTest-professional_law,5-shot,accuracy,0.2555410691003911,0.0111398578335985
EleutherAI/pythia-12b,hendrycksTest-professional_law,5-shot,acc_norm,0.2555410691003911,0.0111398578335985
EleutherAI/pythia-12b,hendrycksTest-professional_medicine,5-shot,accuracy,0.2022058823529411,0.0243981929866549
EleutherAI/pythia-12b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2022058823529411,0.0243981929866549
EleutherAI/pythia-12b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
EleutherAI/pythia-12b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2598039215686274,0.0177408995091777
EleutherAI/pythia-12b,hendrycksTest-public_relations,5-shot,accuracy,0.2363636363636363,0.0406930631972137
EleutherAI/pythia-12b,hendrycksTest-public_relations,5-shot,acc_norm,0.2363636363636363,0.0406930631972137
EleutherAI/pythia-12b,hendrycksTest-security_studies,5-shot,accuracy,0.2571428571428571,0.0279798235387445
EleutherAI/pythia-12b,hendrycksTest-security_studies,5-shot,acc_norm,0.2571428571428571,0.0279798235387445
EleutherAI/pythia-12b,hendrycksTest-sociology,5-shot,accuracy,0.3034825870646766,0.0325100681645861
EleutherAI/pythia-12b,hendrycksTest-sociology,5-shot,acc_norm,0.3034825870646766,0.0325100681645861
EleutherAI/pythia-12b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-12b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/pythia-12b,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.0362933532994785
EleutherAI/pythia-12b,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.0362933532994785
EleutherAI/pythia-12b,hendrycksTest-world_religions,5-shot,accuracy,0.3040935672514619,0.0352821125824523
EleutherAI/pythia-12b,hendrycksTest-world_religions,5-shot,acc_norm,0.3040935672514619,0.0352821125824523
EleutherAI/pythia-12b,truthfulqa:mc,0-shot,mc1,0.2056303549571603,0.0141484822194609
EleutherAI/pythia-12b,truthfulqa:mc,0-shot,mc2,0.3185371350132154,0.0131033560031411
EleutherAI/pythia-12b,drop,3-shot,accuracy,0.0006291946308724,0.0002568002749723
EleutherAI/pythia-12b,drop,3-shot,f1,0.0444798657718121,0.0010992181181045
EleutherAI/pythia-12b,gsm8k,5-shot,accuracy,0.0174374526156178,0.0036054868679982
EleutherAI/pythia-12b,winogrande,5-shot,accuracy,0.6416732438831886,0.0134765811725675
EleutherAI/gpt-j-6b,arc:challenge,25-shot,accuracy,0.3686006825938566,0.0140978106780421
EleutherAI/gpt-j-6b,arc:challenge,25-shot,acc_norm,0.4138225255972696,0.014392730009221
EleutherAI/gpt-j-6b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-j-6b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/gpt-j-6b,hendrycksTest-anatomy,5-shot,accuracy,0.2666666666666666,0.038201699145179
EleutherAI/gpt-j-6b,hendrycksTest-anatomy,5-shot,acc_norm,0.2666666666666666,0.038201699145179
EleutherAI/gpt-j-6b,hendrycksTest-astronomy,5-shot,accuracy,0.2697368421052631,0.0361178056028489
EleutherAI/gpt-j-6b,hendrycksTest-astronomy,5-shot,acc_norm,0.2697368421052631,0.0361178056028489
EleutherAI/gpt-j-6b,hendrycksTest-business_ethics,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-j-6b,hendrycksTest-business_ethics,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/gpt-j-6b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.260377358490566,0.0270087660907081
EleutherAI/gpt-j-6b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.260377358490566,0.0270087660907081
EleutherAI/gpt-j-6b,hendrycksTest-college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008033
EleutherAI/gpt-j-6b,hendrycksTest-college_biology,5-shot,acc_norm,0.2430555555555555,0.0358687928008033
EleutherAI/gpt-j-6b,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/gpt-j-6b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/gpt-j-6b,hendrycksTest-college_computer_science,5-shot,accuracy,0.23,0.042295258468165
EleutherAI/gpt-j-6b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.23,0.042295258468165
EleutherAI/gpt-j-6b,hendrycksTest-college_mathematics,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/gpt-j-6b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.32,0.046882617226215
EleutherAI/gpt-j-6b,hendrycksTest-college_medicine,5-shot,accuracy,0.2832369942196532,0.0343556805604787
EleutherAI/gpt-j-6b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2832369942196532,0.0343556805604787
EleutherAI/gpt-j-6b,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/gpt-j-6b,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
EleutherAI/gpt-j-6b,hendrycksTest-computer_security,5-shot,accuracy,0.41,0.049431107042371
EleutherAI/gpt-j-6b,hendrycksTest-computer_security,5-shot,acc_norm,0.41,0.049431107042371
EleutherAI/gpt-j-6b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3404255319148936,0.0309766929985344
EleutherAI/gpt-j-6b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3404255319148936,0.0309766929985344
EleutherAI/gpt-j-6b,hendrycksTest-econometrics,5-shot,accuracy,0.2807017543859649,0.042270544512322
EleutherAI/gpt-j-6b,hendrycksTest-econometrics,5-shot,acc_norm,0.2807017543859649,0.042270544512322
EleutherAI/gpt-j-6b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2896551724137931,0.0378001923043801
EleutherAI/gpt-j-6b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2896551724137931,0.0378001923043801
EleutherAI/gpt-j-6b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2301587301587301,0.0216792196636931
EleutherAI/gpt-j-6b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2301587301587301,0.0216792196636931
EleutherAI/gpt-j-6b,hendrycksTest-formal_logic,5-shot,accuracy,0.1825396825396825,0.0345507101910214
EleutherAI/gpt-j-6b,hendrycksTest-formal_logic,5-shot,acc_norm,0.1825396825396825,0.0345507101910214
EleutherAI/gpt-j-6b,hendrycksTest-global_facts,5-shot,accuracy,0.21,0.0409360180740332
EleutherAI/gpt-j-6b,hendrycksTest-global_facts,5-shot,acc_norm,0.21,0.0409360180740332
EleutherAI/gpt-j-6b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2032258064516129,0.0228916879845549
EleutherAI/gpt-j-6b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2032258064516129,0.0228916879845549
EleutherAI/gpt-j-6b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2463054187192118,0.0303150992856177
EleutherAI/gpt-j-6b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2463054187192118,0.0303150992856177
EleutherAI/gpt-j-6b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.15,0.0358870281282636
EleutherAI/gpt-j-6b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.15,0.0358870281282636
EleutherAI/gpt-j-6b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2848484848484848,0.0352439084451178
EleutherAI/gpt-j-6b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2848484848484848,0.0352439084451178
EleutherAI/gpt-j-6b,hendrycksTest-high_school_geography,5-shot,accuracy,0.2323232323232323,0.0300886294902174
EleutherAI/gpt-j-6b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2323232323232323,0.0300886294902174
EleutherAI/gpt-j-6b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2227979274611398,0.0300311479776415
EleutherAI/gpt-j-6b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2227979274611398,0.0300311479776415
EleutherAI/gpt-j-6b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2230769230769231,0.0211077301272439
EleutherAI/gpt-j-6b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2230769230769231,0.0211077301272439
EleutherAI/gpt-j-6b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2740740740740741,0.0271959348040856
EleutherAI/gpt-j-6b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2740740740740741,0.0271959348040856
EleutherAI/gpt-j-6b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2563025210084033,0.0283596208705339
EleutherAI/gpt-j-6b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2563025210084033,0.0283596208705339
EleutherAI/gpt-j-6b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2516556291390728,0.0354330423438998
EleutherAI/gpt-j-6b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2516556291390728,0.0354330423438998
EleutherAI/gpt-j-6b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2256880733944954,0.017923087667803
EleutherAI/gpt-j-6b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2256880733944954,0.017923087667803
EleutherAI/gpt-j-6b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.162037037037037,0.0251304536522684
EleutherAI/gpt-j-6b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.162037037037037,0.0251304536522684
EleutherAI/gpt-j-6b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2892156862745098,0.0318223186764755
EleutherAI/gpt-j-6b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2892156862745098,0.0318223186764755
EleutherAI/gpt-j-6b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2869198312236287,0.0294437730225946
EleutherAI/gpt-j-6b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2869198312236287,0.0294437730225946
EleutherAI/gpt-j-6b,hendrycksTest-human_aging,5-shot,accuracy,0.336322869955157,0.031708824268455
EleutherAI/gpt-j-6b,hendrycksTest-human_aging,5-shot,acc_norm,0.336322869955157,0.031708824268455
EleutherAI/gpt-j-6b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2137404580152671,0.0359546161177469
EleutherAI/gpt-j-6b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2137404580152671,0.0359546161177469
EleutherAI/gpt-j-6b,hendrycksTest-international_law,5-shot,accuracy,0.2479338842975206,0.039418975265163
EleutherAI/gpt-j-6b,hendrycksTest-international_law,5-shot,acc_norm,0.2479338842975206,0.039418975265163
EleutherAI/gpt-j-6b,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
EleutherAI/gpt-j-6b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
EleutherAI/gpt-j-6b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2515337423312883,0.0340899788685752
EleutherAI/gpt-j-6b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2515337423312883,0.0340899788685752
EleutherAI/gpt-j-6b,hendrycksTest-machine_learning,5-shot,accuracy,0.3839285714285714,0.0461614307502854
EleutherAI/gpt-j-6b,hendrycksTest-machine_learning,5-shot,acc_norm,0.3839285714285714,0.0461614307502854
EleutherAI/gpt-j-6b,hendrycksTest-management,5-shot,accuracy,0.2135922330097087,0.0405804201564603
EleutherAI/gpt-j-6b,hendrycksTest-management,5-shot,acc_norm,0.2135922330097087,0.0405804201564603
EleutherAI/gpt-j-6b,hendrycksTest-marketing,5-shot,accuracy,0.2649572649572649,0.0289112088027494
EleutherAI/gpt-j-6b,hendrycksTest-marketing,5-shot,acc_norm,0.2649572649572649,0.0289112088027494
EleutherAI/gpt-j-6b,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,hendrycksTest-miscellaneous,5-shot,accuracy,0.3141762452107279,0.0165992917358849
EleutherAI/gpt-j-6b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3141762452107279,0.0165992917358849
EleutherAI/gpt-j-6b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2716763005780346,0.0239485129054683
EleutherAI/gpt-j-6b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2716763005780346,0.0239485129054683
EleutherAI/gpt-j-6b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.0143559119647678
EleutherAI/gpt-j-6b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.0143559119647678
EleutherAI/gpt-j-6b,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.0252616912197294
EleutherAI/gpt-j-6b,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.0252616912197294
EleutherAI/gpt-j-6b,hendrycksTest-philosophy,5-shot,accuracy,0.2604501607717042,0.0249267232248455
EleutherAI/gpt-j-6b,hendrycksTest-philosophy,5-shot,acc_norm,0.2604501607717042,0.0249267232248455
EleutherAI/gpt-j-6b,hendrycksTest-prehistory,5-shot,accuracy,0.3117283950617284,0.0257731111696304
EleutherAI/gpt-j-6b,hendrycksTest-prehistory,5-shot,acc_norm,0.3117283950617284,0.0257731111696304
EleutherAI/gpt-j-6b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2836879432624113,0.0268917094283439
EleutherAI/gpt-j-6b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2836879432624113,0.0268917094283439
EleutherAI/gpt-j-6b,hendrycksTest-professional_law,5-shot,accuracy,0.2894393741851369,0.0115826597022102
EleutherAI/gpt-j-6b,hendrycksTest-professional_law,5-shot,acc_norm,0.2894393741851369,0.0115826597022102
EleutherAI/gpt-j-6b,hendrycksTest-professional_medicine,5-shot,accuracy,0.2426470588235294,0.0260406624742012
EleutherAI/gpt-j-6b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2426470588235294,0.0260406624742012
EleutherAI/gpt-j-6b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2794117647058823,0.0181528710515388
EleutherAI/gpt-j-6b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2794117647058823,0.0181528710515388
EleutherAI/gpt-j-6b,hendrycksTest-public_relations,5-shot,accuracy,0.3454545454545454,0.0455461961754105
EleutherAI/gpt-j-6b,hendrycksTest-public_relations,5-shot,acc_norm,0.3454545454545454,0.0455461961754105
EleutherAI/gpt-j-6b,hendrycksTest-security_studies,5-shot,accuracy,0.3591836734693877,0.0307135604551084
EleutherAI/gpt-j-6b,hendrycksTest-security_studies,5-shot,acc_norm,0.3591836734693877,0.0307135604551084
EleutherAI/gpt-j-6b,hendrycksTest-sociology,5-shot,accuracy,0.2786069651741293,0.031700561834973
EleutherAI/gpt-j-6b,hendrycksTest-sociology,5-shot,acc_norm,0.2786069651741293,0.031700561834973
EleutherAI/gpt-j-6b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/gpt-j-6b,hendrycksTest-virology,5-shot,accuracy,0.3373493975903614,0.0368078369072758
EleutherAI/gpt-j-6b,hendrycksTest-virology,5-shot,acc_norm,0.3373493975903614,0.0368078369072758
EleutherAI/gpt-j-6b,hendrycksTest-world_religions,5-shot,accuracy,0.3450292397660818,0.036459813773888
EleutherAI/gpt-j-6b,hendrycksTest-world_religions,5-shot,acc_norm,0.3450292397660818,0.036459813773888
EleutherAI/gpt-j-6b,truthfulqa:mc,0-shot,mc1,0.2019583843329253,0.0140539574415123
EleutherAI/gpt-j-6b,truthfulqa:mc,0-shot,mc2,0.359624729495078,0.0134620195200081
EleutherAI/gpt-j-6b,drop,3-shot,accuracy,0.0009437919463087,0.0003144653119413
EleutherAI/gpt-j-6b,drop,3-shot,f1,0.0461545721476511,0.0011697500055632
gpt2,anli,0-shot,accuracy,0.345,0.0147
gpt2,xnli,0-shot,accuracy,0.3506,0.0249
gpt2,mathqa,0-shot,accuracy,0.2111,0.0075
gpt2,logiqa2,0-shot,accuracy,0.2392,0.0108
gpt2,mmlu_world_religions,0-shot,accuracy,0.2105263157894736,0.0312678171466317
gpt2,mmlu_formal_logic,0-shot,accuracy,0.1428571428571428,0.031298431857438
gpt2,mmlu_prehistory,0-shot,accuracy,0.2253086419753086,0.0232462026478197
gpt2,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
gpt2,mmlu_high_school_world_history,0-shot,accuracy,0.2489451476793249,0.0281469705994226
gpt2,mmlu_moral_disputes,0-shot,accuracy,0.2427745664739884,0.0230836585869842
gpt2,mmlu_professional_law,0-shot,accuracy,0.2464146023468057,0.0110059713999272
gpt2,mmlu_logical_fallacies,0-shot,accuracy,0.2576687116564417,0.0343615082784691
gpt2,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.0303915336927415
gpt2,mmlu_philosophy,0-shot,accuracy,0.2540192926045016,0.0247238615047716
gpt2,mmlu_jurisprudence,0-shot,accuracy,0.2129629629629629,0.0395783547198097
gpt2,mmlu_international_law,0-shot,accuracy,0.3223140495867768,0.0426641636335216
gpt2,mmlu_high_school_european_history,0-shot,accuracy,0.2121212121212121,0.0319227156954829
gpt2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3678756476683937,0.0348017566846603
gpt2,mmlu_high_school_microeconomics,0-shot,accuracy,0.2899159663865546,0.029472485833136
gpt2,mmlu_high_school_geography,0-shot,accuracy,0.3535353535353535,0.0340608672354715
gpt2,mmlu_high_school_psychology,0-shot,accuracy,0.3486238532110092,0.0204312540907143
gpt2,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
gpt2,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.0446196043338473
gpt2,mmlu_sociology,0-shot,accuracy,0.2288557213930348,0.0297052840567724
gpt2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2717948717948718,0.0225565510101323
gpt2,mmlu_security_studies,0-shot,accuracy,0.4,0.0313625024093589
gpt2,mmlu_professional_psychology,0-shot,accuracy,0.2630718954248366,0.0178126765423206
gpt2,mmlu_human_sexuality,0-shot,accuracy,0.2671755725190839,0.0388084830108239
gpt2,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.0409698513984367
gpt2,mmlu_miscellaneous,0-shot,accuracy,0.2183908045977011,0.0147743583199344
gpt2,mmlu_marketing,0-shot,accuracy,0.1794871794871795,0.0251409359503354
gpt2,mmlu_management,0-shot,accuracy,0.3300970873786408,0.0465614711001234
gpt2,mmlu_nutrition,0-shot,accuracy,0.218954248366013,0.0236790898618077
gpt2,mmlu_medical_genetics,0-shot,accuracy,0.27,0.0446196043338474
gpt2,mmlu_human_aging,0-shot,accuracy,0.2690582959641255,0.0297637794068749
gpt2,mmlu_professional_medicine,0-shot,accuracy,0.4448529411764705,0.0301875320603293
gpt2,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
gpt2,mmlu_business_ethics,0-shot,accuracy,0.18,0.0386122919665369
gpt2,mmlu_clinical_knowledge,0-shot,accuracy,0.2339622641509434,0.0260552969011529
gpt2,mmlu_global_facts,0-shot,accuracy,0.15,0.0358870281282637
gpt2,mmlu_virology,0-shot,accuracy,0.1927710843373494,0.0307098240505652
gpt2,mmlu_professional_accounting,0-shot,accuracy,0.2659574468085106,0.0263580656988805
gpt2,mmlu_college_physics,0-shot,accuracy,0.2549019607843137,0.0433643270799317
gpt2,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.0365860326276374
gpt2,mmlu_high_school_biology,0-shot,accuracy,0.3032258064516129,0.0261486859306717
gpt2,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.0347659010430413
gpt2,mmlu_anatomy,0-shot,accuracy,0.237037037037037,0.036737316839695
gpt2,mmlu_college_chemistry,0-shot,accuracy,0.2,0.0402015126103684
gpt2,mmlu_computer_security,0-shot,accuracy,0.16,0.036845294917747
gpt2,mmlu_college_computer_science,0-shot,accuracy,0.28,0.0451260859854212
gpt2,mmlu_astronomy,0-shot,accuracy,0.1644736842105263,0.0301675334686327
gpt2,mmlu_college_mathematics,0-shot,accuracy,0.28,0.0451260859854212
gpt2,mmlu_conceptual_physics,0-shot,accuracy,0.2680851063829787,0.0289573427883423
gpt2,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.0409360180740332
gpt2,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276807
gpt2,mmlu_machine_learning,0-shot,accuracy,0.2321428571428571,0.040073418097558
gpt2,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.0312709071329769
gpt2,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
gpt2,mmlu_elementary_mathematics,0-shot,accuracy,0.2539682539682539,0.0224180428911139
gpt2,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.035659981741353
gpt2,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
gpt2,arc_challenge,25-shot,accuracy,0.1919795221843003,0.0115095989065981
gpt2,arc_challenge,25-shot,acc_norm,0.2184300341296928,0.0120742916057009
gpt2,hellaswag,10-shot,accuracy,0.291575383389763,0.0045355897592026
gpt2,hellaswag,10-shot,acc_norm,0.3151762597092212,0.0046363655348197
gpt2,truthfulqa_mc2,0-shot,accuracy,0.4069358340982317,0.014921949568995
gpt2,truthfulqa_gen,0-shot,bleu_max,0.7189915567640092,0.1847502597101306
gpt2,truthfulqa_gen,0-shot,bleu_acc,0.0097919216646266,0.0034470859984644
gpt2,truthfulqa_gen,0-shot,bleu_diff,-0.1684158934139238,0.114479692250227
gpt2,truthfulqa_gen,0-shot,rouge1_max,1.4549180041272496,0.3157873316508274
gpt2,truthfulqa_gen,0-shot,rouge1_acc,0.0085679314565483,0.003226445945631
gpt2,truthfulqa_gen,0-shot,rouge1_diff,-0.3752635907645988,0.182185700437024
gpt2,truthfulqa_gen,0-shot,rouge2_max,0.8607031116173555,0.2368200600632776
gpt2,truthfulqa_gen,0-shot,rouge2_acc,0.003671970624235,0.0021174135790319
gpt2,truthfulqa_gen,0-shot,rouge2_diff,-0.3291615551523294,0.1489951175018515
gpt2,truthfulqa_gen,0-shot,rougeL_max,1.3855912860137218,0.3061171037786049
gpt2,truthfulqa_gen,0-shot,rougeL_acc,0.0097919216646266,0.0034470859984644
gpt2,truthfulqa_gen,0-shot,rougeL_diff,-0.3495058742576715,0.1620974613383816
gpt2,truthfulqa_mc1,0-shot,accuracy,0.2276621787025703,0.014679255032111
gpt2,winogrande,5-shot,accuracy,0.5114443567482242,0.0140488041998593
gpt2,gsm8k,5-shot,accuracy,0.0068233510235026,0.0022675371022544
openai-community/gpt2,arc:challenge,25-shot,accuracy,0.197098976109215,0.0116250476698806
openai-community/gpt2,arc:challenge,25-shot,acc_norm,0.220136518771331,0.0121081248834609
openai-community/gpt2,hellaswag,10-shot,accuracy,0.292670782712607,0.0045405869832299
openai-community/gpt2,hellaswag,10-shot,acc_norm,0.3152758414658435,0.0046367607625228
openai-community/gpt2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.21,0.0409360180740332
openai-community/gpt2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.21,0.0409360180740332
openai-community/gpt2,hendrycksTest-anatomy,5-shot,accuracy,0.2296296296296296,0.0363338441407346
openai-community/gpt2,hendrycksTest-anatomy,5-shot,acc_norm,0.2296296296296296,0.0363338441407346
openai-community/gpt2,hendrycksTest-astronomy,5-shot,accuracy,0.1644736842105263,0.0301675334686327
openai-community/gpt2,hendrycksTest-astronomy,5-shot,acc_norm,0.1644736842105263,0.0301675334686327
openai-community/gpt2,hendrycksTest-business_ethics,5-shot,accuracy,0.17,0.0377525168068637
openai-community/gpt2,hendrycksTest-business_ethics,5-shot,acc_norm,0.17,0.0377525168068637
openai-community/gpt2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2415094339622641,0.0263414803711183
openai-community/gpt2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2415094339622641,0.0263414803711183
openai-community/gpt2,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.0347659010430413
openai-community/gpt2,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.0347659010430413
openai-community/gpt2,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
openai-community/gpt2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
openai-community/gpt2,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
openai-community/gpt2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
openai-community/gpt2,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
openai-community/gpt2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
openai-community/gpt2,hendrycksTest-college_medicine,5-shot,accuracy,0.2427745664739884,0.0326926380614177
openai-community/gpt2,hendrycksTest-college_medicine,5-shot,acc_norm,0.2427745664739884,0.0326926380614177
openai-community/gpt2,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.0433643270799317
openai-community/gpt2,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.0433643270799317
openai-community/gpt2,hendrycksTest-computer_security,5-shot,accuracy,0.16,0.036845294917747
openai-community/gpt2,hendrycksTest-computer_security,5-shot,acc_norm,0.16,0.036845294917747
openai-community/gpt2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2723404255319149,0.0291012906983866
openai-community/gpt2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2723404255319149,0.0291012906983866
openai-community/gpt2,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
openai-community/gpt2,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
openai-community/gpt2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.035659981741353
openai-community/gpt2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.035659981741353
openai-community/gpt2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2539682539682539,0.0224180428911139
openai-community/gpt2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2539682539682539,0.0224180428911139
openai-community/gpt2,hendrycksTest-formal_logic,5-shot,accuracy,0.1428571428571428,0.0312984318574381
openai-community/gpt2,hendrycksTest-formal_logic,5-shot,acc_norm,0.1428571428571428,0.0312984318574381
openai-community/gpt2,hendrycksTest-global_facts,5-shot,accuracy,0.15,0.0358870281282636
openai-community/gpt2,hendrycksTest-global_facts,5-shot,acc_norm,0.15,0.0358870281282636
openai-community/gpt2,hendrycksTest-high_school_biology,5-shot,accuracy,0.2967741935483871,0.0259885007924118
openai-community/gpt2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2967741935483871,0.0259885007924118
openai-community/gpt2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.0312709071329769
openai-community/gpt2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.0312709071329769
openai-community/gpt2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.26,0.0440844002276807
openai-community/gpt2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.26,0.0440844002276807
openai-community/gpt2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2181818181818181,0.0322507810830628
openai-community/gpt2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2181818181818181,0.0322507810830628
openai-community/gpt2,hendrycksTest-high_school_geography,5-shot,accuracy,0.3535353535353535,0.0340608672354715
openai-community/gpt2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3535353535353535,0.0340608672354715
openai-community/gpt2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3678756476683937,0.0348017566846603
openai-community/gpt2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3678756476683937,0.0348017566846603
openai-community/gpt2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2717948717948718,0.0225565510101323
openai-community/gpt2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2717948717948718,0.0225565510101323
openai-community/gpt2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
openai-community/gpt2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
openai-community/gpt2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2899159663865546,0.029472485833136
openai-community/gpt2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2899159663865546,0.029472485833136
openai-community/gpt2,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
openai-community/gpt2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.0363132980396965
openai-community/gpt2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3486238532110092,0.0204312540907143
openai-community/gpt2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3486238532110092,0.0204312540907143
openai-community/gpt2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
openai-community/gpt2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
openai-community/gpt2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.0303915336927415
openai-community/gpt2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.0303915336927415
openai-community/gpt2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2447257383966244,0.0279856993870364
openai-community/gpt2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2447257383966244,0.0279856993870364
openai-community/gpt2,hendrycksTest-human_aging,5-shot,accuracy,0.2914798206278027,0.0305002831765459
openai-community/gpt2,hendrycksTest-human_aging,5-shot,acc_norm,0.2914798206278027,0.0305002831765459
openai-community/gpt2,hendrycksTest-human_sexuality,5-shot,accuracy,0.2671755725190839,0.0388084830108239
openai-community/gpt2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2671755725190839,0.0388084830108239
openai-community/gpt2,hendrycksTest-international_law,5-shot,accuracy,0.3223140495867768,0.0426641636335216
openai-community/gpt2,hendrycksTest-international_law,5-shot,acc_norm,0.3223140495867768,0.0426641636335216
openai-community/gpt2,hendrycksTest-jurisprudence,5-shot,accuracy,0.2129629629629629,0.0395783547198098
openai-community/gpt2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2129629629629629,0.0395783547198098
openai-community/gpt2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
openai-community/gpt2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.263803680981595,0.0346241993161562
openai-community/gpt2,hendrycksTest-machine_learning,5-shot,accuracy,0.2589285714285714,0.0415775153986562
openai-community/gpt2,hendrycksTest-machine_learning,5-shot,acc_norm,0.2589285714285714,0.0415775153986562
openai-community/gpt2,hendrycksTest-management,5-shot,accuracy,0.3495145631067961,0.0472118850609717
openai-community/gpt2,hendrycksTest-management,5-shot,acc_norm,0.3495145631067961,0.0472118850609717
openai-community/gpt2,hendrycksTest-marketing,5-shot,accuracy,0.1794871794871795,0.0251409359503354
openai-community/gpt2,hendrycksTest-marketing,5-shot,acc_norm,0.1794871794871795,0.0251409359503354
openai-community/gpt2,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.0446196043338473
openai-community/gpt2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.0446196043338473
openai-community/gpt2,hendrycksTest-miscellaneous,5-shot,accuracy,0.2158365261813537,0.0147116843861399
openai-community/gpt2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2158365261813537,0.0147116843861399
openai-community/gpt2,hendrycksTest-moral_disputes,5-shot,accuracy,0.2427745664739884,0.0230836585869842
openai-community/gpt2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2427745664739884,0.0230836585869842
openai-community/gpt2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
openai-community/gpt2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
openai-community/gpt2,hendrycksTest-nutrition,5-shot,accuracy,0.218954248366013,0.0236790898618077
openai-community/gpt2,hendrycksTest-nutrition,5-shot,acc_norm,0.218954248366013,0.0236790898618077
openai-community/gpt2,hendrycksTest-philosophy,5-shot,accuracy,0.247588424437299,0.0245138799736219
openai-community/gpt2,hendrycksTest-philosophy,5-shot,acc_norm,0.247588424437299,0.0245138799736219
openai-community/gpt2,hendrycksTest-prehistory,5-shot,accuracy,0.2253086419753086,0.0232462026478197
openai-community/gpt2,hendrycksTest-prehistory,5-shot,acc_norm,0.2253086419753086,0.0232462026478197
openai-community/gpt2,hendrycksTest-professional_accounting,5-shot,accuracy,0.2659574468085106,0.0263580656988805
openai-community/gpt2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2659574468085106,0.0263580656988805
openai-community/gpt2,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.0109961566351426
openai-community/gpt2,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.0109961566351426
openai-community/gpt2,hendrycksTest-professional_medicine,5-shot,accuracy,0.4448529411764705,0.0301875320603293
openai-community/gpt2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4448529411764705,0.0301875320603293
openai-community/gpt2,hendrycksTest-professional_psychology,5-shot,accuracy,0.261437908496732,0.017776947157528
openai-community/gpt2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.261437908496732,0.017776947157528
openai-community/gpt2,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
openai-community/gpt2,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
openai-community/gpt2,hendrycksTest-security_studies,5-shot,accuracy,0.4,0.0313625024093589
openai-community/gpt2,hendrycksTest-security_studies,5-shot,acc_norm,0.4,0.0313625024093589
openai-community/gpt2,hendrycksTest-sociology,5-shot,accuracy,0.2288557213930348,0.0297052840567724
openai-community/gpt2,hendrycksTest-sociology,5-shot,acc_norm,0.2288557213930348,0.0297052840567724
openai-community/gpt2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.27,0.0446196043338473
openai-community/gpt2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.27,0.0446196043338473
openai-community/gpt2,hendrycksTest-virology,5-shot,accuracy,0.1927710843373494,0.0307098240505652
openai-community/gpt2,hendrycksTest-virology,5-shot,acc_norm,0.1927710843373494,0.0307098240505652
openai-community/gpt2,hendrycksTest-world_religions,5-shot,accuracy,0.2105263157894736,0.0312678171466318
openai-community/gpt2,hendrycksTest-world_religions,5-shot,acc_norm,0.2105263157894736,0.0312678171466318
openai-community/gpt2,truthfulqa:mc,0-shot,mc1,0.2276621787025703,0.014679255032111
openai-community/gpt2,truthfulqa:mc,0-shot,mc2,0.4069116400376613,0.0149342501223465
openai-community/gpt2,winogrande,5-shot,accuracy,0.5043409629044988,0.0140519560640768
openai-community/gpt2,gsm8k,5-shot,accuracy,0.0166793025018953,0.0035275958887224
openai-community/gpt2,minerva_math_precalc,5-shot,accuracy,0.0073260073260073,0.003652908089383
openai-community/gpt2,minerva_math_prealgebra,5-shot,accuracy,0.0172215843857634,0.0044106716741614
openai-community/gpt2,minerva_math_num_theory,5-shot,accuracy,0.0037037037037037,0.0026164834572311
openai-community/gpt2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0055370985603543,0.0024707690436948
openai-community/gpt2,minerva_math_geometry,5-shot,accuracy,0.0041753653444676,0.0029493392170756
openai-community/gpt2,minerva_math_counting_and_prob,5-shot,accuracy,0.0126582278481012,0.0051403138895788
openai-community/gpt2,minerva_math_algebra,5-shot,accuracy,0.016849199663016,0.0037372948497597
openai-community/gpt2,fld_default,0-shot,accuracy,0.0,
openai-community/gpt2,fld_star,0-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
openai-community/gpt2,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
openai-community/gpt2,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
openai-community/gpt2,arithmetic_2ds,5-shot,accuracy,0.0075,0.0019296986470519
openai-community/gpt2,arithmetic_5ds,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_5da,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_1dc,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_4ds,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_2dm,5-shot,accuracy,0.0125,0.0024849471787626
openai-community/gpt2,arithmetic_2da,5-shot,accuracy,0.0035,0.0013208888574315
openai-community/gpt2,gsm8k_cot,5-shot,accuracy,0.0212282031842304,0.0039704491298486
openai-community/gpt2,anli_r2,0-shot,brier_score,0.8430078263586929,
openai-community/gpt2,anli_r3,0-shot,brier_score,0.8013382623470885,
openai-community/gpt2,anli_r1,0-shot,brier_score,0.8498025055736901,
openai-community/gpt2,xnli_eu,0-shot,brier_score,0.9535989536207444,
openai-community/gpt2,xnli_vi,0-shot,brier_score,1.0211886040569071,
openai-community/gpt2,xnli_ru,0-shot,brier_score,0.7731589865372022,
openai-community/gpt2,xnli_zh,0-shot,brier_score,0.9057661094866232,
openai-community/gpt2,xnli_tr,0-shot,brier_score,1.2240303777002093,
openai-community/gpt2,xnli_fr,0-shot,brier_score,1.0840384510822554,
openai-community/gpt2,xnli_en,0-shot,brier_score,0.7392875401126219,
openai-community/gpt2,xnli_ur,0-shot,brier_score,1.230167174519266,
openai-community/gpt2,xnli_ar,0-shot,brier_score,0.8911067783116383,
openai-community/gpt2,xnli_de,0-shot,brier_score,0.9368314062777842,
openai-community/gpt2,xnli_hi,0-shot,brier_score,1.019823037820243,
openai-community/gpt2,xnli_es,0-shot,brier_score,1.190947162499289,
openai-community/gpt2,xnli_bg,0-shot,brier_score,0.8735272166946944,
openai-community/gpt2,xnli_sw,0-shot,brier_score,1.0233152940648171,
openai-community/gpt2,xnli_el,0-shot,brier_score,1.26573102048963,
openai-community/gpt2,xnli_th,0-shot,brier_score,1.0943639845839197,
openai-community/gpt2,logiqa2,0-shot,brier_score,1.1036405901271378,
openai-community/gpt2,mathqa,0-shot,brier_score,1.03887777935678,
openai-community/gpt2,lambada_standard,0-shot,perplexity,93.73017204246436,3.812055170694076
openai-community/gpt2,lambada_standard,0-shot,accuracy,0.2596545701533087,0.0061083970427305
openai-community/gpt2,lambada_openai,0-shot,perplexity,40.05542696796472,1.4840536659360848
openai-community/gpt2,lambada_openai,0-shot,accuracy,0.3256355521055695,0.0065286789578354
playdev7/theseed-v0.3,arc:challenge,25-shot,accuracy,0.2261092150170648,0.0122242020970632
playdev7/theseed-v0.3,arc:challenge,25-shot,acc_norm,0.2593856655290102,0.0128082735739271
playdev7/theseed-v0.3,hellaswag,10-shot,accuracy,0.2561242780322645,0.0043559920900309
playdev7/theseed-v0.3,hellaswag,10-shot,acc_norm,0.2605058753236407,0.0043801364685439
playdev7/theseed-v0.3,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.0451260859854212
playdev7/theseed-v0.3,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.0451260859854212
playdev7/theseed-v0.3,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.0374985070917402
playdev7/theseed-v0.3,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.0374985070917402
playdev7/theseed-v0.3,hendrycksTest-astronomy,5-shot,accuracy,0.2697368421052631,0.0361178056028489
playdev7/theseed-v0.3,hendrycksTest-astronomy,5-shot,acc_norm,0.2697368421052631,0.0361178056028489
playdev7/theseed-v0.3,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.0440844002276807
playdev7/theseed-v0.3,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.0440844002276807
playdev7/theseed-v0.3,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.1698113207547169,0.0231083937998413
playdev7/theseed-v0.3,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.1698113207547169,0.0231083937998413
playdev7/theseed-v0.3,hendrycksTest-college_biology,5-shot,accuracy,0.1944444444444444,0.03309615177059
playdev7/theseed-v0.3,hendrycksTest-college_biology,5-shot,acc_norm,0.1944444444444444,0.03309615177059
playdev7/theseed-v0.3,hendrycksTest-college_chemistry,5-shot,accuracy,0.16,0.036845294917747
playdev7/theseed-v0.3,hendrycksTest-college_chemistry,5-shot,acc_norm,0.16,0.036845294917747
playdev7/theseed-v0.3,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
playdev7/theseed-v0.3,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
playdev7/theseed-v0.3,hendrycksTest-college_mathematics,5-shot,accuracy,0.19,0.0394277244403662
playdev7/theseed-v0.3,hendrycksTest-college_mathematics,5-shot,acc_norm,0.19,0.0394277244403662
playdev7/theseed-v0.3,hendrycksTest-college_medicine,5-shot,accuracy,0.2138728323699422,0.0312651120617304
playdev7/theseed-v0.3,hendrycksTest-college_medicine,5-shot,acc_norm,0.2138728323699422,0.0312651120617304
playdev7/theseed-v0.3,hendrycksTest-college_physics,5-shot,accuracy,0.2745098039215686,0.0444052190617932
playdev7/theseed-v0.3,hendrycksTest-college_physics,5-shot,acc_norm,0.2745098039215686,0.0444052190617932
playdev7/theseed-v0.3,hendrycksTest-computer_security,5-shot,accuracy,0.25,0.0435194139889244
playdev7/theseed-v0.3,hendrycksTest-computer_security,5-shot,acc_norm,0.25,0.0435194139889244
playdev7/theseed-v0.3,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2723404255319149,0.0291012906983867
playdev7/theseed-v0.3,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2723404255319149,0.0291012906983867
playdev7/theseed-v0.3,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
playdev7/theseed-v0.3,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
playdev7/theseed-v0.3,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.0380614268730999
playdev7/theseed-v0.3,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.0380614268730999
playdev7/theseed-v0.3,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
playdev7/theseed-v0.3,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
playdev7/theseed-v0.3,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.038095238095238
playdev7/theseed-v0.3,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.038095238095238
playdev7/theseed-v0.3,hendrycksTest-global_facts,5-shot,accuracy,0.26,0.0440844002276808
playdev7/theseed-v0.3,hendrycksTest-global_facts,5-shot,acc_norm,0.26,0.0440844002276808
playdev7/theseed-v0.3,hendrycksTest-high_school_biology,5-shot,accuracy,0.2129032258064516,0.0232876651272685
playdev7/theseed-v0.3,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2129032258064516,0.0232876651272685
playdev7/theseed-v0.3,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.0312709071329769
playdev7/theseed-v0.3,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.0312709071329769
playdev7/theseed-v0.3,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.0394277244403662
playdev7/theseed-v0.3,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.0394277244403662
playdev7/theseed-v0.3,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3151515151515151,0.0362773057502241
playdev7/theseed-v0.3,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3151515151515151,0.0362773057502241
playdev7/theseed-v0.3,hendrycksTest-high_school_geography,5-shot,accuracy,0.2171717171717171,0.0293766164849456
playdev7/theseed-v0.3,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2171717171717171,0.0293766164849456
playdev7/theseed-v0.3,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2590673575129533,0.0316187791793541
playdev7/theseed-v0.3,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2590673575129533,0.0316187791793541
playdev7/theseed-v0.3,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2128205128205128,0.020752423722128
playdev7/theseed-v0.3,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2128205128205128,0.020752423722128
playdev7/theseed-v0.3,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2333333333333333,0.0257878742209593
playdev7/theseed-v0.3,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2333333333333333,0.0257878742209593
playdev7/theseed-v0.3,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.0288013921936312
playdev7/theseed-v0.3,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.0288013921936312
playdev7/theseed-v0.3,hendrycksTest-high_school_physics,5-shot,accuracy,0.2582781456953642,0.0357370531476345
playdev7/theseed-v0.3,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2582781456953642,0.0357370531476345
playdev7/theseed-v0.3,hendrycksTest-high_school_psychology,5-shot,accuracy,0.218348623853211,0.0177126005287227
playdev7/theseed-v0.3,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.218348623853211,0.0177126005287227
playdev7/theseed-v0.3,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2592592592592592,0.0298869105476269
playdev7/theseed-v0.3,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2592592592592592,0.0298869105476269
playdev7/theseed-v0.3,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2794117647058823,0.0314932810450795
playdev7/theseed-v0.3,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2794117647058823,0.0314932810450795
playdev7/theseed-v0.3,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2067510548523206,0.0263616516683891
playdev7/theseed-v0.3,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2067510548523206,0.0263616516683891
playdev7/theseed-v0.3,hendrycksTest-human_aging,5-shot,accuracy,0.2825112107623318,0.0302168310115087
playdev7/theseed-v0.3,hendrycksTest-human_aging,5-shot,acc_norm,0.2825112107623318,0.0302168310115087
playdev7/theseed-v0.3,hendrycksTest-human_sexuality,5-shot,accuracy,0.2671755725190839,0.0388084830108239
playdev7/theseed-v0.3,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2671755725190839,0.0388084830108239
playdev7/theseed-v0.3,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.0398497965330287
playdev7/theseed-v0.3,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.0398497965330287
playdev7/theseed-v0.3,hendrycksTest-jurisprudence,5-shot,accuracy,0.3333333333333333,0.0455723951349775
playdev7/theseed-v0.3,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3333333333333333,0.0455723951349775
playdev7/theseed-v0.3,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2208588957055214,0.0325917739274217
playdev7/theseed-v0.3,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2208588957055214,0.0325917739274217
playdev7/theseed-v0.3,hendrycksTest-machine_learning,5-shot,accuracy,0.2410714285714285,0.0405986724695268
playdev7/theseed-v0.3,hendrycksTest-machine_learning,5-shot,acc_norm,0.2410714285714285,0.0405986724695268
playdev7/theseed-v0.3,hendrycksTest-management,5-shot,accuracy,0.174757281553398,0.0376017800602662
playdev7/theseed-v0.3,hendrycksTest-management,5-shot,acc_norm,0.174757281553398,0.0376017800602662
playdev7/theseed-v0.3,hendrycksTest-marketing,5-shot,accuracy,0.235042735042735,0.0277788359049354
playdev7/theseed-v0.3,hendrycksTest-marketing,5-shot,acc_norm,0.235042735042735,0.0277788359049354
playdev7/theseed-v0.3,hendrycksTest-medical_genetics,5-shot,accuracy,0.19,0.0394277244403662
playdev7/theseed-v0.3,hendrycksTest-medical_genetics,5-shot,acc_norm,0.19,0.0394277244403662
playdev7/theseed-v0.3,hendrycksTest-miscellaneous,5-shot,accuracy,0.2822477650063857,0.0160953029698785
playdev7/theseed-v0.3,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2822477650063857,0.0160953029698785
playdev7/theseed-v0.3,hendrycksTest-moral_disputes,5-shot,accuracy,0.2601156069364161,0.0236186783100693
playdev7/theseed-v0.3,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2601156069364161,0.0236186783100693
playdev7/theseed-v0.3,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2335195530726257,0.0141495753489762
playdev7/theseed-v0.3,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2335195530726257,0.0141495753489762
playdev7/theseed-v0.3,hendrycksTest-nutrition,5-shot,accuracy,0.2679738562091503,0.0253606037962425
playdev7/theseed-v0.3,hendrycksTest-nutrition,5-shot,acc_norm,0.2679738562091503,0.0253606037962425
playdev7/theseed-v0.3,hendrycksTest-philosophy,5-shot,accuracy,0.22508038585209,0.023720088516179
playdev7/theseed-v0.3,hendrycksTest-philosophy,5-shot,acc_norm,0.22508038585209,0.023720088516179
playdev7/theseed-v0.3,hendrycksTest-prehistory,5-shot,accuracy,0.25,0.0240934712326213
playdev7/theseed-v0.3,hendrycksTest-prehistory,5-shot,acc_norm,0.25,0.0240934712326213
playdev7/theseed-v0.3,hendrycksTest-professional_accounting,5-shot,accuracy,0.2446808510638297,0.0256455536222667
playdev7/theseed-v0.3,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2446808510638297,0.0256455536222667
playdev7/theseed-v0.3,hendrycksTest-professional_law,5-shot,accuracy,0.2398956975228161,0.0109062826179816
playdev7/theseed-v0.3,hendrycksTest-professional_law,5-shot,acc_norm,0.2398956975228161,0.0109062826179816
playdev7/theseed-v0.3,hendrycksTest-professional_medicine,5-shot,accuracy,0.2426470588235294,0.0260406624742012
playdev7/theseed-v0.3,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2426470588235294,0.0260406624742012
playdev7/theseed-v0.3,hendrycksTest-professional_psychology,5-shot,accuracy,0.2434640522875817,0.0173624737621466
playdev7/theseed-v0.3,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2434640522875817,0.0173624737621466
playdev7/theseed-v0.3,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.0430911870994645
playdev7/theseed-v0.3,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.0430911870994645
playdev7/theseed-v0.3,hendrycksTest-security_studies,5-shot,accuracy,0.2571428571428571,0.0279798235387445
playdev7/theseed-v0.3,hendrycksTest-security_studies,5-shot,acc_norm,0.2571428571428571,0.0279798235387445
playdev7/theseed-v0.3,hendrycksTest-sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
playdev7/theseed-v0.3,hendrycksTest-sociology,5-shot,acc_norm,0.2437810945273631,0.0303604901540146
playdev7/theseed-v0.3,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.18,0.0386122919665369
playdev7/theseed-v0.3,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.18,0.0386122919665369
playdev7/theseed-v0.3,hendrycksTest-virology,5-shot,accuracy,0.2650602409638554,0.0343602403794496
playdev7/theseed-v0.3,hendrycksTest-virology,5-shot,acc_norm,0.2650602409638554,0.0343602403794496
playdev7/theseed-v0.3,hendrycksTest-world_religions,5-shot,accuracy,0.2514619883040935,0.0332750442384684
playdev7/theseed-v0.3,hendrycksTest-world_religions,5-shot,acc_norm,0.2514619883040935,0.0332750442384684
playdev7/theseed-v0.3,truthfulqa:mc,0-shot,mc1,0.2215422276621787,0.0145378676013011
playdev7/theseed-v0.3,truthfulqa:mc,0-shot,mc2,0.4632631517417194,0.0170552638898116
playdev7/theseed-v0.3,winogrande,5-shot,accuracy,0.5256511444356748,0.0140339809561085
playdev7/theseed-v0.3,gsm8k,5-shot,accuracy,0.0,
Dampish/StellarX-4B-V0,arc:challenge,25-shot,accuracy,0.3233788395904436,0.0136694216300121
Dampish/StellarX-4B-V0,arc:challenge,25-shot,acc_norm,0.3694539249146757,0.0141045783664919
Dampish/StellarX-4B-V0,hellaswag,10-shot,accuracy,0.4584744074885481,0.0049725431277678
Dampish/StellarX-4B-V0,hellaswag,10-shot,acc_norm,0.6190001991635132,0.0048464003255852
Dampish/StellarX-4B-V0,hendrycksTest-abstract_algebra,5-shot,accuracy,0.34,0.0476095228569523
Dampish/StellarX-4B-V0,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.34,0.0476095228569523
Dampish/StellarX-4B-V0,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.0374985070917402
Dampish/StellarX-4B-V0,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.0374985070917402
Dampish/StellarX-4B-V0,hendrycksTest-astronomy,5-shot,accuracy,0.3421052631578947,0.0386073159931609
Dampish/StellarX-4B-V0,hendrycksTest-astronomy,5-shot,acc_norm,0.3421052631578947,0.0386073159931609
Dampish/StellarX-4B-V0,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.042295258468165
Dampish/StellarX-4B-V0,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.042295258468165
Dampish/StellarX-4B-V0,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2830188679245283,0.0277242364927009
Dampish/StellarX-4B-V0,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2830188679245283,0.0277242364927009
Dampish/StellarX-4B-V0,hendrycksTest-college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008034
Dampish/StellarX-4B-V0,hendrycksTest-college_biology,5-shot,acc_norm,0.2430555555555555,0.0358687928008034
Dampish/StellarX-4B-V0,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.0440844002276808
Dampish/StellarX-4B-V0,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.0440844002276808
Dampish/StellarX-4B-V0,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.047258156262526
Dampish/StellarX-4B-V0,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.047258156262526
Dampish/StellarX-4B-V0,hendrycksTest-college_mathematics,5-shot,accuracy,0.36,0.0482418151324421
Dampish/StellarX-4B-V0,hendrycksTest-college_mathematics,5-shot,acc_norm,0.36,0.0482418151324421
Dampish/StellarX-4B-V0,hendrycksTest-college_medicine,5-shot,accuracy,0.2601156069364161,0.0334503691678899
Dampish/StellarX-4B-V0,hendrycksTest-college_medicine,5-shot,acc_norm,0.2601156069364161,0.0334503691678899
Dampish/StellarX-4B-V0,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
Dampish/StellarX-4B-V0,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
Dampish/StellarX-4B-V0,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
Dampish/StellarX-4B-V0,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
Dampish/StellarX-4B-V0,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2851063829787234,0.0295131966255393
Dampish/StellarX-4B-V0,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2851063829787234,0.0295131966255393
Dampish/StellarX-4B-V0,hendrycksTest-econometrics,5-shot,accuracy,0.2719298245614035,0.0418577442402205
Dampish/StellarX-4B-V0,hendrycksTest-econometrics,5-shot,acc_norm,0.2719298245614035,0.0418577442402205
Dampish/StellarX-4B-V0,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.0380614268730999
Dampish/StellarX-4B-V0,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.0380614268730999
Dampish/StellarX-4B-V0,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2645502645502645,0.0227174678977086
Dampish/StellarX-4B-V0,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2645502645502645,0.0227174678977086
Dampish/StellarX-4B-V0,hendrycksTest-formal_logic,5-shot,accuracy,0.1507936507936507,0.0320068649728739
Dampish/StellarX-4B-V0,hendrycksTest-formal_logic,5-shot,acc_norm,0.1507936507936507,0.0320068649728739
Dampish/StellarX-4B-V0,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526
Dampish/StellarX-4B-V0,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526
Dampish/StellarX-4B-V0,hendrycksTest-high_school_biology,5-shot,accuracy,0.2290322580645161,0.0239049143117826
Dampish/StellarX-4B-V0,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2290322580645161,0.0239049143117826
Dampish/StellarX-4B-V0,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
Dampish/StellarX-4B-V0,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.0305165307326944
Dampish/StellarX-4B-V0,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.0429234695990928
Dampish/StellarX-4B-V0,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.0429234695990928
Dampish/StellarX-4B-V0,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2303030303030303,0.0328766675860348
Dampish/StellarX-4B-V0,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2303030303030303,0.0328766675860348
Dampish/StellarX-4B-V0,hendrycksTest-high_school_geography,5-shot,accuracy,0.3535353535353535,0.0340608672354715
Dampish/StellarX-4B-V0,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3535353535353535,0.0340608672354715
Dampish/StellarX-4B-V0,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2642487046632124,0.0318215505091664
Dampish/StellarX-4B-V0,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2642487046632124,0.0318215505091664
Dampish/StellarX-4B-V0,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2512820512820513,0.0219920166623705
Dampish/StellarX-4B-V0,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2512820512820513,0.0219920166623705
Dampish/StellarX-4B-V0,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2222222222222222,0.0253480974680978
Dampish/StellarX-4B-V0,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2222222222222222,0.0253480974680978
Dampish/StellarX-4B-V0,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.1932773109243697,0.0256494702658892
Dampish/StellarX-4B-V0,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.1932773109243697,0.0256494702658892
Dampish/StellarX-4B-V0,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
Dampish/StellarX-4B-V0,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.0363132980396965
Dampish/StellarX-4B-V0,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3284403669724771,0.0201359027972983
Dampish/StellarX-4B-V0,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3284403669724771,0.0201359027972983
Dampish/StellarX-4B-V0,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3287037037037037,0.0320361408467005
Dampish/StellarX-4B-V0,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3287037037037037,0.0320361408467005
Dampish/StellarX-4B-V0,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2401960784313725,0.0299837330559136
Dampish/StellarX-4B-V0,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2401960784313725,0.0299837330559136
Dampish/StellarX-4B-V0,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.0289007219062934
Dampish/StellarX-4B-V0,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.0289007219062934
Dampish/StellarX-4B-V0,hendrycksTest-human_aging,5-shot,accuracy,0.116591928251121,0.0215396398162444
Dampish/StellarX-4B-V0,hendrycksTest-human_aging,5-shot,acc_norm,0.116591928251121,0.0215396398162444
Dampish/StellarX-4B-V0,hendrycksTest-human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
Dampish/StellarX-4B-V0,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2442748091603053,0.0376833595972874
Dampish/StellarX-4B-V0,hendrycksTest-international_law,5-shot,accuracy,0.4049586776859504,0.0448113775594246
Dampish/StellarX-4B-V0,hendrycksTest-international_law,5-shot,acc_norm,0.4049586776859504,0.0448113775594246
Dampish/StellarX-4B-V0,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.041860917913946
Dampish/StellarX-4B-V0,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.041860917913946
Dampish/StellarX-4B-V0,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2453987730061349,0.0338093981394335
Dampish/StellarX-4B-V0,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2453987730061349,0.0338093981394335
Dampish/StellarX-4B-V0,hendrycksTest-machine_learning,5-shot,accuracy,0.2767857142857143,0.0424662433669762
Dampish/StellarX-4B-V0,hendrycksTest-machine_learning,5-shot,acc_norm,0.2767857142857143,0.0424662433669762
Dampish/StellarX-4B-V0,hendrycksTest-management,5-shot,accuracy,0.3009708737864077,0.0454160944650394
Dampish/StellarX-4B-V0,hendrycksTest-management,5-shot,acc_norm,0.3009708737864077,0.0454160944650394
Dampish/StellarX-4B-V0,hendrycksTest-marketing,5-shot,accuracy,0.2991452991452991,0.0299969518583494
Dampish/StellarX-4B-V0,hendrycksTest-marketing,5-shot,acc_norm,0.2991452991452991,0.0299969518583494
Dampish/StellarX-4B-V0,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.0446196043338473
Dampish/StellarX-4B-V0,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.0446196043338473
Dampish/StellarX-4B-V0,hendrycksTest-miscellaneous,5-shot,accuracy,0.2873563218390804,0.0161824107306827
Dampish/StellarX-4B-V0,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2873563218390804,0.0161824107306827
Dampish/StellarX-4B-V0,hendrycksTest-moral_disputes,5-shot,accuracy,0.2485549132947976,0.0232675284321001
Dampish/StellarX-4B-V0,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2485549132947976,0.0232675284321001
Dampish/StellarX-4B-V0,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2726256983240223,0.0148933917352495
Dampish/StellarX-4B-V0,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2726256983240223,0.0148933917352495
Dampish/StellarX-4B-V0,hendrycksTest-nutrition,5-shot,accuracy,0.2549019607843137,0.0249541843248799
Dampish/StellarX-4B-V0,hendrycksTest-nutrition,5-shot,acc_norm,0.2549019607843137,0.0249541843248799
Dampish/StellarX-4B-V0,hendrycksTest-philosophy,5-shot,accuracy,0.2893890675241157,0.0257558659226329
Dampish/StellarX-4B-V0,hendrycksTest-philosophy,5-shot,acc_norm,0.2893890675241157,0.0257558659226329
Dampish/StellarX-4B-V0,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.0245692236004608
Dampish/StellarX-4B-V0,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.0245692236004608
Dampish/StellarX-4B-V0,hendrycksTest-professional_accounting,5-shot,accuracy,0.2695035460992908,0.0264690368185906
Dampish/StellarX-4B-V0,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2695035460992908,0.0264690368185906
Dampish/StellarX-4B-V0,hendrycksTest-professional_law,5-shot,accuracy,0.2633637548891786,0.0112495064036052
Dampish/StellarX-4B-V0,hendrycksTest-professional_law,5-shot,acc_norm,0.2633637548891786,0.0112495064036052
Dampish/StellarX-4B-V0,hendrycksTest-professional_medicine,5-shot,accuracy,0.2426470588235294,0.0260406624742012
Dampish/StellarX-4B-V0,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2426470588235294,0.0260406624742012
Dampish/StellarX-4B-V0,hendrycksTest-professional_psychology,5-shot,accuracy,0.2401960784313725,0.0172827606951674
Dampish/StellarX-4B-V0,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2401960784313725,0.0172827606951674
Dampish/StellarX-4B-V0,hendrycksTest-public_relations,5-shot,accuracy,0.3363636363636363,0.045253935963025
Dampish/StellarX-4B-V0,hendrycksTest-public_relations,5-shot,acc_norm,0.3363636363636363,0.045253935963025
Dampish/StellarX-4B-V0,hendrycksTest-security_studies,5-shot,accuracy,0.236734693877551,0.0272128358840731
Dampish/StellarX-4B-V0,hendrycksTest-security_studies,5-shot,acc_norm,0.236734693877551,0.0272128358840731
Dampish/StellarX-4B-V0,hendrycksTest-sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
Dampish/StellarX-4B-V0,hendrycksTest-sociology,5-shot,acc_norm,0.2437810945273631,0.0303604901540146
Dampish/StellarX-4B-V0,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276807
Dampish/StellarX-4B-V0,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.0440844002276807
Dampish/StellarX-4B-V0,hendrycksTest-virology,5-shot,accuracy,0.2349397590361445,0.0330053318612892
Dampish/StellarX-4B-V0,hendrycksTest-virology,5-shot,acc_norm,0.2349397590361445,0.0330053318612892
Dampish/StellarX-4B-V0,hendrycksTest-world_religions,5-shot,accuracy,0.2923976608187134,0.0348864771345792
Dampish/StellarX-4B-V0,hendrycksTest-world_religions,5-shot,acc_norm,0.2923976608187134,0.0348864771345792
Dampish/StellarX-4B-V0,truthfulqa:mc,0-shot,mc1,0.2068543451652386,0.0141795914967283
Dampish/StellarX-4B-V0,truthfulqa:mc,0-shot,mc2,0.3429682257173366,0.0136280271638659
Dampish/StellarX-4B-V0,drop,3-shot,accuracy,0.0269505033557047,0.0016584048452624
Dampish/StellarX-4B-V0,drop,3-shot,f1,0.1094840604026844,0.0022806732636903
Dampish/StellarX-4B-V0,gsm8k,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,winogrande,5-shot,accuracy,0.6385161799526441,0.0135024796707912
Qwen/Qwen1.5-14B,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.0283809195961458
Qwen/Qwen1.5-14B,mmlu_formal_logic,0-shot,accuracy,0.5476190476190477,0.0445180795905532
Qwen/Qwen1.5-14B,mmlu_prehistory,0-shot,accuracy,0.7129629629629629,0.0251710419153096
Qwen/Qwen1.5-14B,mmlu_moral_scenarios,0-shot,accuracy,0.4424581005586592,0.0166113936872685
Qwen/Qwen1.5-14B,mmlu_high_school_world_history,0-shot,accuracy,0.8312236286919831,0.0243814068325862
Qwen/Qwen1.5-14B,mmlu_moral_disputes,0-shot,accuracy,0.7456647398843931,0.0234458262765455
Qwen/Qwen1.5-14B,mmlu_professional_law,0-shot,accuracy,0.485006518904824,0.0127644932021932
Qwen/Qwen1.5-14B,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
Qwen/Qwen1.5-14B,mmlu_high_school_us_history,0-shot,accuracy,0.8137254901960784,0.0273254709667163
Qwen/Qwen1.5-14B,mmlu_philosophy,0-shot,accuracy,0.7202572347266881,0.0254942593506949
Qwen/Qwen1.5-14B,mmlu_jurisprudence,0-shot,accuracy,0.75,0.041860917913946
Qwen/Qwen1.5-14B,mmlu_international_law,0-shot,accuracy,0.8429752066115702,0.0332124484254713
Qwen/Qwen1.5-14B,mmlu_high_school_european_history,0-shot,accuracy,0.8424242424242424,0.0284503888052843
Qwen/Qwen1.5-14B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8963730569948186,0.0219953119636442
Qwen/Qwen1.5-14B,mmlu_high_school_microeconomics,0-shot,accuracy,0.7605042016806722,0.0277220654933612
Qwen/Qwen1.5-14B,mmlu_high_school_geography,0-shot,accuracy,0.8686868686868687,0.0240631564168225
Qwen/Qwen1.5-14B,mmlu_high_school_psychology,0-shot,accuracy,0.8642201834862385,0.01468690755634
Qwen/Qwen1.5-14B,mmlu_public_relations,0-shot,accuracy,0.6454545454545455,0.0458200484150541
Qwen/Qwen1.5-14B,mmlu_us_foreign_policy,0-shot,accuracy,0.88,0.032659863237109
Qwen/Qwen1.5-14B,mmlu_sociology,0-shot,accuracy,0.8407960199004975,0.0258706467661691
Qwen/Qwen1.5-14B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7333333333333333,0.0224212736129237
Qwen/Qwen1.5-14B,mmlu_security_studies,0-shot,accuracy,0.8040816326530612,0.0254093019532256
Qwen/Qwen1.5-14B,mmlu_professional_psychology,0-shot,accuracy,0.704248366013072,0.0184631541326328
Qwen/Qwen1.5-14B,mmlu_human_sexuality,0-shot,accuracy,0.7633587786259542,0.0372767357559691
Qwen/Qwen1.5-14B,mmlu_econometrics,0-shot,accuracy,0.5526315789473685,0.0467747300449119
Qwen/Qwen1.5-14B,mmlu_miscellaneous,0-shot,accuracy,0.8378033205619413,0.0131822226167208
Qwen/Qwen1.5-14B,mmlu_marketing,0-shot,accuracy,0.8675213675213675,0.0222093090731656
Qwen/Qwen1.5-14B,mmlu_management,0-shot,accuracy,0.8058252427184466,0.0391666776282258
Qwen/Qwen1.5-14B,mmlu_nutrition,0-shot,accuracy,0.7483660130718954,0.0248480182638752
Qwen/Qwen1.5-14B,mmlu_medical_genetics,0-shot,accuracy,0.79,0.0409360180740332
Qwen/Qwen1.5-14B,mmlu_human_aging,0-shot,accuracy,0.7309417040358744,0.0297637794068749
Qwen/Qwen1.5-14B,mmlu_professional_medicine,0-shot,accuracy,0.7205882352941176,0.0272572026061149
Qwen/Qwen1.5-14B,mmlu_college_medicine,0-shot,accuracy,0.6936416184971098,0.0351494255126743
Qwen/Qwen1.5-14B,mmlu_business_ethics,0-shot,accuracy,0.76,0.0429234695990928
Qwen/Qwen1.5-14B,mmlu_clinical_knowledge,0-shot,accuracy,0.7471698113207547,0.0267498997712412
Qwen/Qwen1.5-14B,mmlu_global_facts,0-shot,accuracy,0.5,0.0502518907629606
Qwen/Qwen1.5-14B,mmlu_virology,0-shot,accuracy,0.4578313253012048,0.0387862677100236
Qwen/Qwen1.5-14B,mmlu_professional_accounting,0-shot,accuracy,0.5,0.0298274993135946
Qwen/Qwen1.5-14B,mmlu_college_physics,0-shot,accuracy,0.5,0.0497518595104994
Qwen/Qwen1.5-14B,mmlu_high_school_physics,0-shot,accuracy,0.4834437086092715,0.0408024418562897
Qwen/Qwen1.5-14B,mmlu_high_school_biology,0-shot,accuracy,0.8419354838709677,0.0207528315118752
Qwen/Qwen1.5-14B,mmlu_college_biology,0-shot,accuracy,0.7638888888888888,0.0355144661081082
Qwen/Qwen1.5-14B,mmlu_anatomy,0-shot,accuracy,0.6370370370370371,0.041539484047424
Qwen/Qwen1.5-14B,mmlu_college_chemistry,0-shot,accuracy,0.5,0.0502518907629606
Qwen/Qwen1.5-14B,mmlu_computer_security,0-shot,accuracy,0.8,0.0402015126103684
Qwen/Qwen1.5-14B,mmlu_college_computer_science,0-shot,accuracy,0.59,0.049431107042371
Qwen/Qwen1.5-14B,mmlu_astronomy,0-shot,accuracy,0.7302631578947368,0.0361178056028489
Qwen/Qwen1.5-14B,mmlu_college_mathematics,0-shot,accuracy,0.47,0.0501613558046591
Qwen/Qwen1.5-14B,mmlu_conceptual_physics,0-shot,accuracy,0.7021276595744681,0.0298961456820954
Qwen/Qwen1.5-14B,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.0482418151324421
Qwen/Qwen1.5-14B,mmlu_high_school_computer_science,0-shot,accuracy,0.76,0.0429234695990928
Qwen/Qwen1.5-14B,mmlu_machine_learning,0-shot,accuracy,0.5446428571428571,0.0472683555371909
Qwen/Qwen1.5-14B,mmlu_high_school_chemistry,0-shot,accuracy,0.5911330049261084,0.0345905881588323
Qwen/Qwen1.5-14B,mmlu_high_school_statistics,0-shot,accuracy,0.6296296296296297,0.0329337713941519
Qwen/Qwen1.5-14B,mmlu_elementary_mathematics,0-shot,accuracy,0.58994708994709,0.0253312024389444
Qwen/Qwen1.5-14B,mmlu_electrical_engineering,0-shot,accuracy,0.7103448275862069,0.0378001923043801
Qwen/Qwen1.5-14B,mmlu_high_school_mathematics,0-shot,accuracy,0.4444444444444444,0.0302967712860673
Qwen/Qwen1.5-14B,arc_challenge,25-shot,accuracy,0.5307167235494881,0.014583792546304
Qwen/Qwen1.5-14B,arc_challenge,25-shot,acc_norm,0.5691126279863481,0.0144711333926424
Qwen/Qwen1.5-14B,hellaswag,10-shot,accuracy,0.6127265484963155,0.0048613146132868
Qwen/Qwen1.5-14B,hellaswag,10-shot,acc_norm,0.810794662417845,0.0039087117912434
Qwen/Qwen1.5-14B,truthfulqa_mc2,0-shot,accuracy,0.5193716235903669,0.0149302021707268
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,bleu_max,8.957020382370416,0.587628866982265
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,bleu_acc,0.1701346389228886,0.0131539174233469
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,bleu_diff,-0.9550174536574028,0.4535834391233462
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge1_max,19.1470779828392,0.933106339842253
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge1_acc,0.1701346389228886,0.0131539174233469
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge1_diff,-0.931315508168068,0.5903759003425647
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge2_max,13.61098601109907,0.8079627333695129
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge2_acc,0.1505507955936352,0.0125188707332561
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge2_diff,-1.4228955265303969,0.6795788734397342
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rougeL_max,18.241818721109897,0.9100007129718392
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rougeL_acc,0.1738066095471236,0.0132656618509665
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rougeL_diff,-0.95605323900642,0.6021888793558169
Qwen/Qwen1.5-14B,truthfulqa_mc1,0-shot,accuracy,0.3574051407588739,0.0167765996767293
Qwen/Qwen1.5-14B,winogrande,5-shot,accuracy,0.7348066298342542,0.0124065494661928
Qwen/Qwen1.5-14B,gsm8k,5-shot,accuracy,0.6762699014404853,0.0128882473973711
microsoft/phi-1_5,mmlu_world_religions,0-shot,accuracy,0.4502923976608187,0.0381582736591323
microsoft/phi-1_5,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.0397015827323517
microsoft/phi-1_5,mmlu_prehistory,0-shot,accuracy,0.4228395061728395,0.0274874729808716
microsoft/phi-1_5,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
microsoft/phi-1_5,mmlu_high_school_world_history,0-shot,accuracy,0.510548523206751,0.0325399837916628
microsoft/phi-1_5,mmlu_moral_disputes,0-shot,accuracy,0.5491329479768786,0.0267888119315627
microsoft/phi-1_5,mmlu_professional_law,0-shot,accuracy,0.3402868318122555,0.0121012176102237
microsoft/phi-1_5,mmlu_logical_fallacies,0-shot,accuracy,0.5276073619631901,0.0392237829061099
microsoft/phi-1_5,mmlu_high_school_us_history,0-shot,accuracy,0.4607843137254901,0.0349850164936952
microsoft/phi-1_5,mmlu_philosophy,0-shot,accuracy,0.4790996784565916,0.0283732709610694
microsoft/phi-1_5,mmlu_jurisprudence,0-shot,accuracy,0.5185185185185185,0.0483036602463533
microsoft/phi-1_5,mmlu_international_law,0-shot,accuracy,0.6033057851239669,0.04465869780531
microsoft/phi-1_5,mmlu_high_school_european_history,0-shot,accuracy,0.4848484848484848,0.0390255100737444
microsoft/phi-1_5,mmlu_high_school_government_and_politics,0-shot,accuracy,0.538860103626943,0.0359752441173457
microsoft/phi-1_5,mmlu_high_school_microeconomics,0-shot,accuracy,0.453781512605042,0.0323394346818208
microsoft/phi-1_5,mmlu_high_school_geography,0-shot,accuracy,0.5202020202020202,0.0355944356556391
microsoft/phi-1_5,mmlu_high_school_psychology,0-shot,accuracy,0.5724770642201835,0.0212109102043004
microsoft/phi-1_5,mmlu_public_relations,0-shot,accuracy,0.5181818181818182,0.0478596401079491
microsoft/phi-1_5,mmlu_us_foreign_policy,0-shot,accuracy,0.65,0.0479372485441101
microsoft/phi-1_5,mmlu_sociology,0-shot,accuracy,0.6417910447761194,0.0339039304226881
microsoft/phi-1_5,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4282051282051282,0.0250883014546948
microsoft/phi-1_5,mmlu_security_studies,0-shot,accuracy,0.5061224489795918,0.032006820201639
microsoft/phi-1_5,mmlu_professional_psychology,0-shot,accuracy,0.4019607843137255,0.0198351764843753
microsoft/phi-1_5,mmlu_human_sexuality,0-shot,accuracy,0.4732824427480916,0.0437902493655389
microsoft/phi-1_5,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
microsoft/phi-1_5,mmlu_miscellaneous,0-shot,accuracy,0.508301404853129,0.017877498991072
microsoft/phi-1_5,mmlu_marketing,0-shot,accuracy,0.7008547008547008,0.0299969518583494
microsoft/phi-1_5,mmlu_management,0-shot,accuracy,0.5825242718446602,0.0488284054821223
microsoft/phi-1_5,mmlu_nutrition,0-shot,accuracy,0.5032679738562091,0.0286293051940035
microsoft/phi-1_5,mmlu_medical_genetics,0-shot,accuracy,0.5,0.0502518907629606
microsoft/phi-1_5,mmlu_human_aging,0-shot,accuracy,0.484304932735426,0.0335412657542081
microsoft/phi-1_5,mmlu_professional_medicine,0-shot,accuracy,0.3345588235294117,0.0286619962023353
microsoft/phi-1_5,mmlu_college_medicine,0-shot,accuracy,0.4161849710982659,0.0375851777540494
microsoft/phi-1_5,mmlu_business_ethics,0-shot,accuracy,0.51,0.0502418393795691
microsoft/phi-1_5,mmlu_clinical_knowledge,0-shot,accuracy,0.4943396226415094,0.0307709007638513
microsoft/phi-1_5,mmlu_global_facts,0-shot,accuracy,0.32,0.046882617226215
microsoft/phi-1_5,mmlu_virology,0-shot,accuracy,0.4156626506024096,0.0383672217659805
microsoft/phi-1_5,mmlu_professional_accounting,0-shot,accuracy,0.3049645390070922,0.0274647084420221
microsoft/phi-1_5,mmlu_college_physics,0-shot,accuracy,0.2941176470588235,0.0453383819592977
microsoft/phi-1_5,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.0365860326276374
microsoft/phi-1_5,mmlu_high_school_biology,0-shot,accuracy,0.4483870967741935,0.0282920568301127
microsoft/phi-1_5,mmlu_college_biology,0-shot,accuracy,0.375,0.0404843922269559
microsoft/phi-1_5,mmlu_anatomy,0-shot,accuracy,0.4666666666666667,0.0430973290103635
microsoft/phi-1_5,mmlu_college_chemistry,0-shot,accuracy,0.27,0.0446196043338473
microsoft/phi-1_5,mmlu_computer_security,0-shot,accuracy,0.53,0.0501613558046591
microsoft/phi-1_5,mmlu_college_computer_science,0-shot,accuracy,0.46,0.0500908265962033
microsoft/phi-1_5,mmlu_astronomy,0-shot,accuracy,0.4144736842105263,0.040089737857792
microsoft/phi-1_5,mmlu_college_mathematics,0-shot,accuracy,0.41,0.049431107042371
microsoft/phi-1_5,mmlu_conceptual_physics,0-shot,accuracy,0.3744680851063829,0.0316391066536729
microsoft/phi-1_5,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.0482418151324421
microsoft/phi-1_5,mmlu_high_school_computer_science,0-shot,accuracy,0.46,0.0500908265962033
microsoft/phi-1_5,mmlu_machine_learning,0-shot,accuracy,0.3928571428571428,0.0463555013560997
microsoft/phi-1_5,mmlu_high_school_chemistry,0-shot,accuracy,0.3448275862068966,0.0334428374428045
microsoft/phi-1_5,mmlu_high_school_statistics,0-shot,accuracy,0.287037037037037,0.030851992993257
microsoft/phi-1_5,mmlu_elementary_mathematics,0-shot,accuracy,0.3042328042328042,0.023695415009463
microsoft/phi-1_5,mmlu_electrical_engineering,0-shot,accuracy,0.4689655172413793,0.0415863276209782
microsoft/phi-1_5,mmlu_high_school_mathematics,0-shot,accuracy,0.2333333333333333,0.0257878742209593
microsoft/phi-1_5,arc_challenge,25-shot,accuracy,0.5008532423208191,0.0146113695298132
microsoft/phi-1_5,arc_challenge,25-shot,acc_norm,0.5315699658703071,0.0145822364608669
microsoft/phi-1_5,hellaswag,10-shot,accuracy,0.4832702648874726,0.0049869875089287
microsoft/phi-1_5,hellaswag,10-shot,acc_norm,0.6379207329217288,0.00479619358493
microsoft/phi-1_5,truthfulqa_mc2,0-shot,accuracy,0.4085779509507484,0.0148402691104113
microsoft/phi-1_5,truthfulqa_gen,0-shot,bleu_max,27.56766835566137,0.7836996753793046
microsoft/phi-1_5,truthfulqa_gen,0-shot,bleu_acc,0.3194614443084455,0.0163226441829605
microsoft/phi-1_5,truthfulqa_gen,0-shot,bleu_diff,-4.901645580186722,0.877157693134709
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge1_max,52.930362387533215,0.8338583348805911
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge1_acc,0.3280293757649938,0.016435632932815
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge1_diff,-5.293623582303511,1.0149100288222572
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge2_max,38.05023260311765,0.9792456124605255
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge2_acc,0.2815177478580171,0.015744027248256
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge2_diff,-6.717662093135759,1.1866608093259197
microsoft/phi-1_5,truthfulqa_gen,0-shot,rougeL_max,50.36955243236768,0.8580936496769774
microsoft/phi-1_5,truthfulqa_gen,0-shot,rougeL_acc,0.3157894736842105,0.0162722879579168
microsoft/phi-1_5,truthfulqa_gen,0-shot,rougeL_diff,-5.513065735562071,1.0278544546573318
microsoft/phi-1_5,truthfulqa_mc1,0-shot,accuracy,0.2705018359853121,0.0155507783328429
microsoft/phi-1_5,winogrande,5-shot,accuracy,0.7221783741120757,0.0125889181838716
microsoft/phi-1_5,gsm8k,5-shot,accuracy,0.1243366186504928,0.0090888809620284
cerebras/Cerebras-GPT-2.7B,drop,3-shot,accuracy,0.0010486577181208,0.0003314581465219
cerebras/Cerebras-GPT-2.7B,drop,3-shot,f1,0.045849412751678,0.0011802883893565
cerebras/Cerebras-GPT-2.7B,arc:challenge,25-shot,accuracy,0.2696245733788396,0.0129680406868691
cerebras/Cerebras-GPT-2.7B,arc:challenge,25-shot,acc_norm,0.2909556313993174,0.0132730778659075
cerebras/Cerebras-GPT-2.7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.042295258468165
cerebras/Cerebras-GPT-2.7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.042295258468165
cerebras/Cerebras-GPT-2.7B,hendrycksTest-anatomy,5-shot,accuracy,0.2666666666666666,0.038201699145179
cerebras/Cerebras-GPT-2.7B,hendrycksTest-anatomy,5-shot,acc_norm,0.2666666666666666,0.038201699145179
cerebras/Cerebras-GPT-2.7B,hendrycksTest-astronomy,5-shot,accuracy,0.1907894736842105,0.0319756582103249
cerebras/Cerebras-GPT-2.7B,hendrycksTest-astronomy,5-shot,acc_norm,0.1907894736842105,0.0319756582103249
cerebras/Cerebras-GPT-2.7B,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.0416333199893226
cerebras/Cerebras-GPT-2.7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.0416333199893226
cerebras/Cerebras-GPT-2.7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2150943396226415,0.0252883945028913
cerebras/Cerebras-GPT-2.7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2150943396226415,0.0252883945028913
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.0371617743756601
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.0371617743756601
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.37,0.048523658709391
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.37,0.048523658709391
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.25,0.0435194139889244
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.25,0.0435194139889244
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_medicine,5-shot,accuracy,0.2312138728323699,0.0321473730202946
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2312138728323699,0.0321473730202946
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_physics,5-shot,accuracy,0.1764705882352941,0.0379328118530781
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_physics,5-shot,acc_norm,0.1764705882352941,0.0379328118530781
cerebras/Cerebras-GPT-2.7B,hendrycksTest-computer_security,5-shot,accuracy,0.26,0.0440844002276807
cerebras/Cerebras-GPT-2.7B,hendrycksTest-computer_security,5-shot,acc_norm,0.26,0.0440844002276807
cerebras/Cerebras-GPT-2.7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2553191489361702,0.0285048564705141
cerebras/Cerebras-GPT-2.7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2553191489361702,0.0285048564705141
cerebras/Cerebras-GPT-2.7B,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
cerebras/Cerebras-GPT-2.7B,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
cerebras/Cerebras-GPT-2.7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.035659981741353
cerebras/Cerebras-GPT-2.7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.035659981741353
cerebras/Cerebras-GPT-2.7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.201058201058201,0.0206418107823701
cerebras/Cerebras-GPT-2.7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.201058201058201,0.0206418107823701
cerebras/Cerebras-GPT-2.7B,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.0371848900681811
cerebras/Cerebras-GPT-2.7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.0371848900681811
cerebras/Cerebras-GPT-2.7B,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.0386122919665369
cerebras/Cerebras-GPT-2.7B,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.0386122919665369
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2451612903225806,0.0244722438408955
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2451612903225806,0.0244722438408955
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.0312709071329769
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.0312709071329769
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.39,0.0490207130000197
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.39,0.0490207130000197
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2303030303030303,0.0328766675860348
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2303030303030303,0.0328766675860348
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.0296202278747904
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.0296202278747904
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2176165803108808,0.0297786630377529
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2176165803108808,0.0297786630377529
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2384615384615384,0.0216062944946477
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2384615384615384,0.0216062944946477
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2703703703703703,0.0270803728151456
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2703703703703703,0.0270803728151456
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2310924369747899,0.0273814069278689
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2310924369747899,0.0273814069278689
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2781456953642384,0.0365860326276374
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2781456953642384,0.0365860326276374
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2330275229357798,0.0181256691808615
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2330275229357798,0.0181256691808615
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.287037037037037,0.030851992993257
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.287037037037037,0.030851992993257
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2696078431372549,0.0311455706594867
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2696078431372549,0.0311455706594867
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2573839662447257,0.0284588209914603
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2573839662447257,0.0284588209914603
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_aging,5-shot,accuracy,0.242152466367713,0.0287513923986947
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_aging,5-shot,acc_norm,0.242152466367713,0.0287513923986947
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559691
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559691
cerebras/Cerebras-GPT-2.7B,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.0389687898507041
cerebras/Cerebras-GPT-2.7B,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.0389687898507041
cerebras/Cerebras-GPT-2.7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.1851851851851851,0.0375526586503718
cerebras/Cerebras-GPT-2.7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.1851851851851851,0.0375526586503718
cerebras/Cerebras-GPT-2.7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3190184049079754,0.0366199755107383
cerebras/Cerebras-GPT-2.7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3190184049079754,0.0366199755107383
cerebras/Cerebras-GPT-2.7B,hendrycksTest-machine_learning,5-shot,accuracy,0.3392857142857143,0.0449394906861353
cerebras/Cerebras-GPT-2.7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.3392857142857143,0.0449394906861353
cerebras/Cerebras-GPT-2.7B,hendrycksTest-management,5-shot,accuracy,0.2135922330097087,0.0405804201564603
cerebras/Cerebras-GPT-2.7B,hendrycksTest-management,5-shot,acc_norm,0.2135922330097087,0.0405804201564603
cerebras/Cerebras-GPT-2.7B,hendrycksTest-marketing,5-shot,accuracy,0.2564102564102564,0.0286059537020042
cerebras/Cerebras-GPT-2.7B,hendrycksTest-marketing,5-shot,acc_norm,0.2564102564102564,0.0286059537020042
cerebras/Cerebras-GPT-2.7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.22,0.0416333199893226
cerebras/Cerebras-GPT-2.7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.22,0.0416333199893226
cerebras/Cerebras-GPT-2.7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2669220945083014,0.0158184508947775
cerebras/Cerebras-GPT-2.7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2669220945083014,0.0158184508947775
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2658959537572254,0.0237862032555082
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2658959537572254,0.0237862032555082
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2715083798882681,0.0148742521680952
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2715083798882681,0.0148742521680952
cerebras/Cerebras-GPT-2.7B,hendrycksTest-nutrition,5-shot,accuracy,0.2712418300653594,0.0254577566966678
cerebras/Cerebras-GPT-2.7B,hendrycksTest-nutrition,5-shot,acc_norm,0.2712418300653594,0.0254577566966678
cerebras/Cerebras-GPT-2.7B,hendrycksTest-philosophy,5-shot,accuracy,0.2668810289389067,0.0251226376088166
cerebras/Cerebras-GPT-2.7B,hendrycksTest-philosophy,5-shot,acc_norm,0.2668810289389067,0.0251226376088166
cerebras/Cerebras-GPT-2.7B,hendrycksTest-prehistory,5-shot,accuracy,0.2314814814814814,0.0234684298324511
cerebras/Cerebras-GPT-2.7B,hendrycksTest-prehistory,5-shot,acc_norm,0.2314814814814814,0.0234684298324511
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2659574468085106,0.0263580656988805
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2659574468085106,0.0263580656988805
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_law,5-shot,accuracy,0.2640156453715775,0.0112584355377238
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_law,5-shot,acc_norm,0.2640156453715775,0.0112584355377238
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.2536764705882353,0.0264313298707895
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2536764705882353,0.0264313298707895
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2630718954248366,0.0178126765423206
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2630718954248366,0.0178126765423206
cerebras/Cerebras-GPT-2.7B,hendrycksTest-public_relations,5-shot,accuracy,0.1727272727272727,0.0362069183392921
cerebras/Cerebras-GPT-2.7B,hendrycksTest-public_relations,5-shot,acc_norm,0.1727272727272727,0.0362069183392921
cerebras/Cerebras-GPT-2.7B,hendrycksTest-security_studies,5-shot,accuracy,0.3183673469387755,0.029822533793982
cerebras/Cerebras-GPT-2.7B,hendrycksTest-security_studies,5-shot,acc_norm,0.3183673469387755,0.029822533793982
cerebras/Cerebras-GPT-2.7B,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.030769444967296
cerebras/Cerebras-GPT-2.7B,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.030769444967296
cerebras/Cerebras-GPT-2.7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.0456048021572068
cerebras/Cerebras-GPT-2.7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.0456048021572068
cerebras/Cerebras-GPT-2.7B,hendrycksTest-virology,5-shot,accuracy,0.2831325301204819,0.0350729543137051
cerebras/Cerebras-GPT-2.7B,hendrycksTest-virology,5-shot,acc_norm,0.2831325301204819,0.0350729543137051
cerebras/Cerebras-GPT-2.7B,hendrycksTest-world_religions,5-shot,accuracy,0.2690058479532163,0.0340105262010409
cerebras/Cerebras-GPT-2.7B,hendrycksTest-world_religions,5-shot,acc_norm,0.2690058479532163,0.0340105262010409
cerebras/Cerebras-GPT-2.7B,truthfulqa:mc,0-shot,mc1,0.2460220318237454,0.0150772192006625
cerebras/Cerebras-GPT-2.7B,truthfulqa:mc,0-shot,mc2,0.4136763359861922,0.0144394227554888
cerebras/Cerebras-GPT-6.7B,drop,3-shot,accuracy,0.0008389261744966,0.0002964962989801
cerebras/Cerebras-GPT-6.7B,drop,3-shot,f1,0.0473458473154363,0.001163677644884
cerebras/Cerebras-GPT-6.7B,arc:challenge,25-shot,accuracy,0.3088737201365187,0.013501770929344
cerebras/Cerebras-GPT-6.7B,arc:challenge,25-shot,acc_norm,0.3506825938566553,0.013944635930726
cerebras/Cerebras-GPT-6.7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.0451260859854212
cerebras/Cerebras-GPT-6.7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.0451260859854212
cerebras/Cerebras-GPT-6.7B,hendrycksTest-anatomy,5-shot,accuracy,0.237037037037037,0.036737316839695
cerebras/Cerebras-GPT-6.7B,hendrycksTest-anatomy,5-shot,acc_norm,0.237037037037037,0.036737316839695
cerebras/Cerebras-GPT-6.7B,hendrycksTest-astronomy,5-shot,accuracy,0.2434210526315789,0.0349234966888423
cerebras/Cerebras-GPT-6.7B,hendrycksTest-astronomy,5-shot,acc_norm,0.2434210526315789,0.0349234966888423
cerebras/Cerebras-GPT-6.7B,hendrycksTest-business_ethics,5-shot,accuracy,0.13,0.033799766898963
cerebras/Cerebras-GPT-6.7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.13,0.033799766898963
cerebras/Cerebras-GPT-6.7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2226415094339622,0.025604233470899
cerebras/Cerebras-GPT-6.7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2226415094339622,0.025604233470899
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_biology,5-shot,accuracy,0.2152777777777778,0.0343707934410613
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_biology,5-shot,acc_norm,0.2152777777777778,0.0343707934410613
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.0464823198711731
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.0464823198711731
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.4,0.049236596391733
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.4,0.049236596391733
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_medicine,5-shot,accuracy,0.2427745664739884,0.0326926380614177
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2427745664739884,0.0326926380614177
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_physics,5-shot,accuracy,0.2450980392156862,0.0428010583736439
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_physics,5-shot,acc_norm,0.2450980392156862,0.0428010583736439
cerebras/Cerebras-GPT-6.7B,hendrycksTest-computer_security,5-shot,accuracy,0.22,0.0416333199893226
cerebras/Cerebras-GPT-6.7B,hendrycksTest-computer_security,5-shot,acc_norm,0.22,0.0416333199893226
cerebras/Cerebras-GPT-6.7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3021276595744681,0.0300175544718805
cerebras/Cerebras-GPT-6.7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3021276595744681,0.0300175544718805
cerebras/Cerebras-GPT-6.7B,hendrycksTest-econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
cerebras/Cerebras-GPT-6.7B,hendrycksTest-econometrics,5-shot,acc_norm,0.2456140350877192,0.0404933929774814
cerebras/Cerebras-GPT-6.7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2551724137931034,0.0363298405270784
cerebras/Cerebras-GPT-6.7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2551724137931034,0.0363298405270784
cerebras/Cerebras-GPT-6.7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2433862433862433,0.0221011287874154
cerebras/Cerebras-GPT-6.7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2433862433862433,0.0221011287874154
cerebras/Cerebras-GPT-6.7B,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.0380952380952381
cerebras/Cerebras-GPT-6.7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.0380952380952381
cerebras/Cerebras-GPT-6.7B,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.0394277244403662
cerebras/Cerebras-GPT-6.7B,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.0394277244403662
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2193548387096774,0.0235407993587232
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2193548387096774,0.0235407993587232
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2068965517241379,0.0285013781678939
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2068965517241379,0.0285013781678939
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.37,0.0485236587093909
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.37,0.0485236587093909
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2848484848484848,0.0352439084451178
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2848484848484848,0.0352439084451178
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.1868686868686868,0.0277725333342189
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1868686868686868,0.0277725333342189
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2176165803108808,0.0297786630377529
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2176165803108808,0.0297786630377529
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2487179487179487,0.0219169577092137
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2487179487179487,0.0219169577092137
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2478991596638655,0.0280479672241768
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2478991596638655,0.0280479672241768
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.0363132980396965
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3045871559633027,0.019732299420354
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3045871559633027,0.019732299420354
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3425925925925926,0.0323658525260215
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3425925925925926,0.0323658525260215
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2401960784313725,0.0299837330559136
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2401960784313725,0.0299837330559136
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2236286919831223,0.0271232982052299
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2236286919831223,0.0271232982052299
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_aging,5-shot,accuracy,0.2959641255605381,0.0306365913486997
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_aging,5-shot,acc_norm,0.2959641255605381,0.0306365913486997
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
cerebras/Cerebras-GPT-6.7B,hendrycksTest-international_law,5-shot,accuracy,0.2644628099173554,0.040261875275912
cerebras/Cerebras-GPT-6.7B,hendrycksTest-international_law,5-shot,acc_norm,0.2644628099173554,0.040261875275912
cerebras/Cerebras-GPT-6.7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.2222222222222222,0.0401910747255735
cerebras/Cerebras-GPT-6.7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2222222222222222,0.0401910747255735
cerebras/Cerebras-GPT-6.7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
cerebras/Cerebras-GPT-6.7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.0355903953161734
cerebras/Cerebras-GPT-6.7B,hendrycksTest-machine_learning,5-shot,accuracy,0.2321428571428571,0.040073418097558
cerebras/Cerebras-GPT-6.7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.2321428571428571,0.040073418097558
cerebras/Cerebras-GPT-6.7B,hendrycksTest-management,5-shot,accuracy,0.2718446601941747,0.0440526802414092
cerebras/Cerebras-GPT-6.7B,hendrycksTest-management,5-shot,acc_norm,0.2718446601941747,0.0440526802414092
cerebras/Cerebras-GPT-6.7B,hendrycksTest-marketing,5-shot,accuracy,0.2393162393162393,0.0279518268089243
cerebras/Cerebras-GPT-6.7B,hendrycksTest-marketing,5-shot,acc_norm,0.2393162393162393,0.0279518268089243
cerebras/Cerebras-GPT-6.7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.26,0.0440844002276808
cerebras/Cerebras-GPT-6.7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.26,0.0440844002276808
cerebras/Cerebras-GPT-6.7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2605363984674329,0.015696008563807
cerebras/Cerebras-GPT-6.7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2605363984674329,0.015696008563807
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2832369942196532,0.0242579017053233
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2832369942196532,0.0242579017053233
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2726256983240223,0.0148933917352495
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2726256983240223,0.0148933917352495
cerebras/Cerebras-GPT-6.7B,hendrycksTest-nutrition,5-shot,accuracy,0.2352941176470588,0.0242886194660461
cerebras/Cerebras-GPT-6.7B,hendrycksTest-nutrition,5-shot,acc_norm,0.2352941176470588,0.0242886194660461
cerebras/Cerebras-GPT-6.7B,hendrycksTest-philosophy,5-shot,accuracy,0.2604501607717042,0.0249267232248455
cerebras/Cerebras-GPT-6.7B,hendrycksTest-philosophy,5-shot,acc_norm,0.2604501607717042,0.0249267232248455
cerebras/Cerebras-GPT-6.7B,hendrycksTest-prehistory,5-shot,accuracy,0.2407407407407407,0.0237885835516585
cerebras/Cerebras-GPT-6.7B,hendrycksTest-prehistory,5-shot,acc_norm,0.2407407407407407,0.0237885835516585
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2482269503546099,0.0257700156442903
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_law,5-shot,accuracy,0.2698826597131681,0.0113373810842504
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_law,5-shot,acc_norm,0.2698826597131681,0.0113373810842504
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.4117647058823529,0.0298961630331254
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4117647058823529,0.0298961630331254
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2483660130718954,0.0174794870013647
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2483660130718954,0.0174794870013647
cerebras/Cerebras-GPT-6.7B,hendrycksTest-public_relations,5-shot,accuracy,0.2454545454545454,0.0412206650287828
cerebras/Cerebras-GPT-6.7B,hendrycksTest-public_relations,5-shot,acc_norm,0.2454545454545454,0.0412206650287828
cerebras/Cerebras-GPT-6.7B,hendrycksTest-security_studies,5-shot,accuracy,0.2,0.0256073759865791
cerebras/Cerebras-GPT-6.7B,hendrycksTest-security_studies,5-shot,acc_norm,0.2,0.0256073759865791
cerebras/Cerebras-GPT-6.7B,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.030769444967296
cerebras/Cerebras-GPT-6.7B,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.030769444967296
cerebras/Cerebras-GPT-6.7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.0451260859854212
cerebras/Cerebras-GPT-6.7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.0451260859854212
cerebras/Cerebras-GPT-6.7B,hendrycksTest-virology,5-shot,accuracy,0.2650602409638554,0.0343602403794496
cerebras/Cerebras-GPT-6.7B,hendrycksTest-virology,5-shot,acc_norm,0.2650602409638554,0.0343602403794496
cerebras/Cerebras-GPT-6.7B,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.0358252944257312
cerebras/Cerebras-GPT-6.7B,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.0358252944257312
cerebras/Cerebras-GPT-6.7B,truthfulqa:mc,0-shot,mc1,0.2435740514075887,0.0150263548249107
cerebras/Cerebras-GPT-6.7B,truthfulqa:mc,0-shot,mc2,0.3802394598585255,0.0139258420270789
NinedayWang/PolyCoder-2.7B,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.0356507967070831
NinedayWang/PolyCoder-2.7B,mmlu_formal_logic,0-shot,accuracy,0.1825396825396825,0.0345507101910214
NinedayWang/PolyCoder-2.7B,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.0230167056402621
NinedayWang/PolyCoder-2.7B,mmlu_moral_scenarios,0-shot,accuracy,0.2480446927374301,0.0144441578082614
NinedayWang/PolyCoder-2.7B,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.029041333510598
NinedayWang/PolyCoder-2.7B,mmlu_moral_disputes,0-shot,accuracy,0.2369942196531791,0.0228940824899259
NinedayWang/PolyCoder-2.7B,mmlu_professional_law,0-shot,accuracy,0.2477183833116036,0.0110254992914437
NinedayWang/PolyCoder-2.7B,mmlu_logical_fallacies,0-shot,accuracy,0.2760736196319018,0.0351238528370505
NinedayWang/PolyCoder-2.7B,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.0309645179269234
NinedayWang/PolyCoder-2.7B,mmlu_philosophy,0-shot,accuracy,0.2636655948553054,0.0250255385005323
NinedayWang/PolyCoder-2.7B,mmlu_jurisprudence,0-shot,accuracy,0.2129629629629629,0.0395783547198097
NinedayWang/PolyCoder-2.7B,mmlu_international_law,0-shot,accuracy,0.3140495867768595,0.0423696475304101
NinedayWang/PolyCoder-2.7B,mmlu_high_school_european_history,0-shot,accuracy,0.2606060606060606,0.0342774317581652
NinedayWang/PolyCoder-2.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.310880829015544,0.0334036190627658
NinedayWang/PolyCoder-2.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678638
NinedayWang/PolyCoder-2.7B,mmlu_high_school_geography,0-shot,accuracy,0.2727272727272727,0.0317307123907172
NinedayWang/PolyCoder-2.7B,mmlu_high_school_psychology,0-shot,accuracy,0.2073394495412844,0.0173814155636086
NinedayWang/PolyCoder-2.7B,mmlu_public_relations,0-shot,accuracy,0.2272727272727272,0.0401396455407277
NinedayWang/PolyCoder-2.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.0429234695990928
NinedayWang/PolyCoder-2.7B,mmlu_sociology,0-shot,accuracy,0.2238805970149253,0.0294752502360171
NinedayWang/PolyCoder-2.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3461538461538461,0.0241211254169411
NinedayWang/PolyCoder-2.7B,mmlu_security_studies,0-shot,accuracy,0.1836734693877551,0.0247890713320076
NinedayWang/PolyCoder-2.7B,mmlu_professional_psychology,0-shot,accuracy,0.2450980392156862,0.0174018167114276
NinedayWang/PolyCoder-2.7B,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
NinedayWang/PolyCoder-2.7B,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
NinedayWang/PolyCoder-2.7B,mmlu_miscellaneous,0-shot,accuracy,0.2528735632183908,0.0155433773137196
NinedayWang/PolyCoder-2.7B,mmlu_marketing,0-shot,accuracy,0.1794871794871795,0.0251409359503354
NinedayWang/PolyCoder-2.7B,mmlu_management,0-shot,accuracy,0.1359223300970873,0.0339329572976101
NinedayWang/PolyCoder-2.7B,mmlu_nutrition,0-shot,accuracy,0.2352941176470588,0.0242886194660461
NinedayWang/PolyCoder-2.7B,mmlu_medical_genetics,0-shot,accuracy,0.31,0.0464823198711731
NinedayWang/PolyCoder-2.7B,mmlu_human_aging,0-shot,accuracy,0.3139013452914798,0.0311467964829724
NinedayWang/PolyCoder-2.7B,mmlu_professional_medicine,0-shot,accuracy,0.4411764705882353,0.0301619119307671
NinedayWang/PolyCoder-2.7B,mmlu_college_medicine,0-shot,accuracy,0.2312138728323699,0.0321473730202946
NinedayWang/PolyCoder-2.7B,mmlu_business_ethics,0-shot,accuracy,0.22,0.0416333199893226
NinedayWang/PolyCoder-2.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.2641509433962264,0.0271342916287417
NinedayWang/PolyCoder-2.7B,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
NinedayWang/PolyCoder-2.7B,mmlu_virology,0-shot,accuracy,0.1746987951807229,0.0295603262112568
NinedayWang/PolyCoder-2.7B,mmlu_professional_accounting,0-shot,accuracy,0.2659574468085106,0.0263580656988805
NinedayWang/PolyCoder-2.7B,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
NinedayWang/PolyCoder-2.7B,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.0365860326276374
NinedayWang/PolyCoder-2.7B,mmlu_high_school_biology,0-shot,accuracy,0.3032258064516129,0.0261486859306717
NinedayWang/PolyCoder-2.7B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
NinedayWang/PolyCoder-2.7B,mmlu_anatomy,0-shot,accuracy,0.3333333333333333,0.0407231481187683
NinedayWang/PolyCoder-2.7B,mmlu_college_chemistry,0-shot,accuracy,0.19,0.0394277244403662
NinedayWang/PolyCoder-2.7B,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
NinedayWang/PolyCoder-2.7B,mmlu_college_computer_science,0-shot,accuracy,0.34,0.0476095228569523
NinedayWang/PolyCoder-2.7B,mmlu_astronomy,0-shot,accuracy,0.1907894736842105,0.031975658210325
NinedayWang/PolyCoder-2.7B,mmlu_college_mathematics,0-shot,accuracy,0.27,0.0446196043338473
NinedayWang/PolyCoder-2.7B,mmlu_conceptual_physics,0-shot,accuracy,0.2170212765957447,0.0269474831214962
NinedayWang/PolyCoder-2.7B,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.0429234695990928
NinedayWang/PolyCoder-2.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.0460566186471838
NinedayWang/PolyCoder-2.7B,mmlu_machine_learning,0-shot,accuracy,0.2589285714285714,0.0415775153986562
NinedayWang/PolyCoder-2.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.0312709071329769
NinedayWang/PolyCoder-2.7B,mmlu_high_school_statistics,0-shot,accuracy,0.3842592592592592,0.0331735451431074
NinedayWang/PolyCoder-2.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.2222222222222222,0.0214116843936941
NinedayWang/PolyCoder-2.7B,mmlu_electrical_engineering,0-shot,accuracy,0.2275862068965517,0.0349395038013118
NinedayWang/PolyCoder-2.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
NinedayWang/PolyCoder-2.7B,arc_challenge,25-shot,accuracy,0.181740614334471,0.0112691989488802
NinedayWang/PolyCoder-2.7B,arc_challenge,25-shot,acc_norm,0.2192832764505119,0.0120912457876157
NinedayWang/PolyCoder-2.7B,hellaswag,10-shot,accuracy,0.2773351921927903,0.0044676841327724
NinedayWang/PolyCoder-2.7B,hellaswag,10-shot,acc_norm,0.2975502887870942,0.0045624626655052
NinedayWang/PolyCoder-2.7B,truthfulqa_mc2,0-shot,accuracy,0.4900198282743296,0.0159247761084381
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,bleu_max,19.109929609778494,0.6347291486261849
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,bleu_acc,0.2913096695226438,0.0159059870481848
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,bleu_diff,-2.73808807451127,0.4927649447592839
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge1_max,43.64910481680449,0.7663193889003078
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge1_acc,0.2974296205630355,0.0160026514873609
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge1_diff,-5.224813168517498,0.5828636449526232
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge2_max,27.07899530604482,0.8474168667870206
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge2_acc,0.2472460220318237,0.0151024047973596
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge2_diff,-4.815471980544821,0.6322835005066595
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rougeL_max,40.60861307441461,0.765726782060013
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rougeL_acc,0.2802937576499388,0.0157231395246087
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rougeL_diff,-4.909635032740114,0.5671337300796637
NinedayWang/PolyCoder-2.7B,truthfulqa_mc1,0-shot,accuracy,0.2876376988984088,0.0158463151013948
NinedayWang/PolyCoder-2.7B,winogrande,5-shot,accuracy,0.5209155485398579,0.0140401854942129
NinedayWang/PolyCoder-2.7B,gsm8k,5-shot,accuracy,0.0212282031842304,0.0039704491298486
EleutherAI/gpt-neo-1.3B,minerva_math_precalc,5-shot,accuracy,0.0128205128205128,0.0048189509824876
EleutherAI/gpt-neo-1.3B,minerva_math_prealgebra,5-shot,accuracy,0.0195177956371986,0.0046900299352845
EleutherAI/gpt-neo-1.3B,minerva_math_num_theory,5-shot,accuracy,0.0037037037037037,0.0026164834572311
EleutherAI/gpt-neo-1.3B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0166112956810631,0.0042556028721946
EleutherAI/gpt-neo-1.3B,minerva_math_geometry,5-shot,accuracy,0.0208768267223382,0.0065393857958139
EleutherAI/gpt-neo-1.3B,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.0062753625139896
EleutherAI/gpt-neo-1.3B,minerva_math_algebra,5-shot,accuracy,0.0176916596461668,0.0038279464976423
EleutherAI/gpt-neo-1.3B,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
EleutherAI/gpt-neo-1.3B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
EleutherAI/gpt-neo-1.3B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
EleutherAI/gpt-neo-1.3B,arithmetic_2ds,5-shot,accuracy,0.0125,0.0024849471787626
EleutherAI/gpt-neo-1.3B,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_1dc,5-shot,accuracy,0.0055,0.0016541593398342
EleutherAI/gpt-neo-1.3B,arithmetic_4ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_2dm,5-shot,accuracy,0.022,0.0032807593162018
EleutherAI/gpt-neo-1.3B,arithmetic_2da,5-shot,accuracy,0.0075,0.0019296986470519
EleutherAI/gpt-neo-1.3B,gsm8k_cot,5-shot,accuracy,0.0144048521607278,0.0032820559171369
EleutherAI/gpt-neo-1.3B,anli_r2,0-shot,brier_score,0.8796036508016054,
EleutherAI/gpt-neo-1.3B,anli_r3,0-shot,brier_score,0.8324430010683331,
EleutherAI/gpt-neo-1.3B,anli_r1,0-shot,brier_score,0.917864024592765,
EleutherAI/gpt-neo-1.3B,xnli_eu,0-shot,brier_score,1.2200574234266282,
EleutherAI/gpt-neo-1.3B,xnli_vi,0-shot,brier_score,0.7903674559535868,
EleutherAI/gpt-neo-1.3B,xnli_ru,0-shot,brier_score,0.7566305493567674,
EleutherAI/gpt-neo-1.3B,xnli_zh,0-shot,brier_score,0.9856404379148058,
EleutherAI/gpt-neo-1.3B,xnli_tr,0-shot,brier_score,0.9169373031276504,
EleutherAI/gpt-neo-1.3B,xnli_fr,0-shot,brier_score,0.856853328987711,
EleutherAI/gpt-neo-1.3B,xnli_en,0-shot,brier_score,0.6629460290225301,
EleutherAI/gpt-neo-1.3B,xnli_ur,0-shot,brier_score,0.944165257849468,
EleutherAI/gpt-neo-1.3B,xnli_ar,0-shot,brier_score,1.2124809306829618,
EleutherAI/gpt-neo-1.3B,xnli_de,0-shot,brier_score,0.8893252434461513,
EleutherAI/gpt-neo-1.3B,xnli_hi,0-shot,brier_score,0.8044741628382988,
EleutherAI/gpt-neo-1.3B,xnli_es,0-shot,brier_score,0.9012945391331052,
EleutherAI/gpt-neo-1.3B,xnli_bg,0-shot,brier_score,0.8686133483895749,
EleutherAI/gpt-neo-1.3B,xnli_sw,0-shot,brier_score,0.9058789893603032,
EleutherAI/gpt-neo-1.3B,xnli_el,0-shot,brier_score,1.0604737322480473,
EleutherAI/gpt-neo-1.3B,xnli_th,0-shot,brier_score,0.7762030553378716,
EleutherAI/gpt-neo-1.3B,logiqa2,0-shot,brier_score,1.225701255044906,
EleutherAI/gpt-neo-1.3B,mathqa,0-shot,brier_score,0.988187749870061,
EleutherAI/gpt-neo-1.3B,lambada_standard,0-shot,perplexity,14.582486374632817,0.4675269331972355
EleutherAI/gpt-neo-1.3B,lambada_standard,0-shot,accuracy,0.452940034931108,0.0069350547518701
EleutherAI/gpt-neo-1.3B,lambada_openai,0-shot,perplexity,7.497819400340795,0.1995827409351639
EleutherAI/gpt-neo-1.3B,lambada_openai,0-shot,accuracy,0.5720939258684261,0.0068931855169307
EleutherAI/gpt-neo-1.3B,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.0358252944257312
EleutherAI/gpt-neo-1.3B,mmlu_formal_logic,0-shot,accuracy,0.2539682539682539,0.0389325961060467
EleutherAI/gpt-neo-1.3B,mmlu_prehistory,0-shot,accuracy,0.2314814814814814,0.0234684298324511
EleutherAI/gpt-neo-1.3B,mmlu_moral_scenarios,0-shot,accuracy,0.2391061452513966,0.0142655541923311
EleutherAI/gpt-neo-1.3B,mmlu_high_school_world_history,0-shot,accuracy,0.2827004219409282,0.0293128141539559
EleutherAI/gpt-neo-1.3B,mmlu_moral_disputes,0-shot,accuracy,0.2514450867052023,0.023357365785874
EleutherAI/gpt-neo-1.3B,mmlu_professional_law,0-shot,accuracy,0.2516297262059974,0.0110832762804419
EleutherAI/gpt-neo-1.3B,mmlu_logical_fallacies,0-shot,accuracy,0.2392638036809816,0.0335195387952126
EleutherAI/gpt-neo-1.3B,mmlu_high_school_us_history,0-shot,accuracy,0.230392156862745,0.029554292605695
EleutherAI/gpt-neo-1.3B,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/gpt-neo-1.3B,mmlu_jurisprudence,0-shot,accuracy,0.25,0.041860917913946
EleutherAI/gpt-neo-1.3B,mmlu_international_law,0-shot,accuracy,0.3057851239669421,0.0420595393388412
EleutherAI/gpt-neo-1.3B,mmlu_high_school_european_history,0-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/gpt-neo-1.3B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2072538860103626,0.0292528232918036
EleutherAI/gpt-neo-1.3B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2310924369747899,0.0273814069278689
EleutherAI/gpt-neo-1.3B,mmlu_high_school_geography,0-shot,accuracy,0.1767676767676767,0.0271787526390449
EleutherAI/gpt-neo-1.3B,mmlu_high_school_psychology,0-shot,accuracy,0.1944954128440367,0.016970289090458
EleutherAI/gpt-neo-1.3B,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/gpt-neo-1.3B,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-neo-1.3B,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/gpt-neo-1.3B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3128205128205128,0.0235075790206453
EleutherAI/gpt-neo-1.3B,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904
EleutherAI/gpt-neo-1.3B,mmlu_professional_psychology,0-shot,accuracy,0.2483660130718954,0.0174794870013647
EleutherAI/gpt-neo-1.3B,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/gpt-neo-1.3B,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
EleutherAI/gpt-neo-1.3B,mmlu_miscellaneous,0-shot,accuracy,0.2234993614303959,0.0148972352294507
EleutherAI/gpt-neo-1.3B,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.0298725777088911
EleutherAI/gpt-neo-1.3B,mmlu_management,0-shot,accuracy,0.174757281553398,0.0376017800602662
EleutherAI/gpt-neo-1.3B,mmlu_nutrition,0-shot,accuracy,0.2287581699346405,0.0240510297399122
EleutherAI/gpt-neo-1.3B,mmlu_medical_genetics,0-shot,accuracy,0.31,0.0464823198711731
EleutherAI/gpt-neo-1.3B,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.0310244117405722
EleutherAI/gpt-neo-1.3B,mmlu_professional_medicine,0-shot,accuracy,0.1654411764705882,0.0225717710254947
EleutherAI/gpt-neo-1.3B,mmlu_college_medicine,0-shot,accuracy,0.2023121387283237,0.0306311455391988
EleutherAI/gpt-neo-1.3B,mmlu_business_ethics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/gpt-neo-1.3B,mmlu_clinical_knowledge,0-shot,accuracy,0.2679245283018868,0.0272572603224948
EleutherAI/gpt-neo-1.3B,mmlu_global_facts,0-shot,accuracy,0.19,0.0394277244403662
EleutherAI/gpt-neo-1.3B,mmlu_virology,0-shot,accuracy,0.2771084337349397,0.0348433159268058
EleutherAI/gpt-neo-1.3B,mmlu_professional_accounting,0-shot,accuracy,0.2375886524822695,0.0253895125527299
EleutherAI/gpt-neo-1.3B,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/gpt-neo-1.3B,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.0357370531476345
EleutherAI/gpt-neo-1.3B,mmlu_high_school_biology,0-shot,accuracy,0.2,0.0227552049595429
EleutherAI/gpt-neo-1.3B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
EleutherAI/gpt-neo-1.3B,mmlu_anatomy,0-shot,accuracy,0.1481481481481481,0.0306886476103526
EleutherAI/gpt-neo-1.3B,mmlu_college_chemistry,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/gpt-neo-1.3B,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,mmlu_college_computer_science,0-shot,accuracy,0.31,0.0464823198711731
EleutherAI/gpt-neo-1.3B,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/gpt-neo-1.3B,mmlu_college_mathematics,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/gpt-neo-1.3B,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.0295131966255393
EleutherAI/gpt-neo-1.3B,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/gpt-neo-1.3B,mmlu_high_school_computer_science,0-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-neo-1.3B,mmlu_machine_learning,0-shot,accuracy,0.3035714285714285,0.0436422615584104
EleutherAI/gpt-neo-1.3B,mmlu_high_school_chemistry,0-shot,accuracy,0.1970443349753694,0.0279867246667362
EleutherAI/gpt-neo-1.3B,mmlu_high_school_statistics,0-shot,accuracy,0.4027777777777778,0.0334488738299786
EleutherAI/gpt-neo-1.3B,mmlu_elementary_mathematics,0-shot,accuracy,0.2539682539682539,0.0224180428911139
EleutherAI/gpt-neo-1.3B,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.0375283395800333
EleutherAI/gpt-neo-1.3B,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.0264661175389599
EleutherAI/gpt-neo-1.3B,arc_challenge,25-shot,accuracy,0.2755972696245733,0.0130571696557618
EleutherAI/gpt-neo-1.3B,arc_challenge,25-shot,acc_norm,0.3088737201365187,0.013501770929344
EleutherAI/gpt-neo-1.3B,truthfulqa_mc2,0-shot,accuracy,0.3961377966394332,0.0142665112380166
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,bleu_max,22.08797851450456,0.710438739697618
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,bleu_acc,0.3231334149326805,0.0163718362864546
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,bleu_diff,-3.357699260688641,0.7725285332901728
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge1_max,46.248363981939576,0.870416787403499
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge1_acc,0.2925336597307221,0.0159255974452861
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge1_diff,-4.926670357978892,0.9736757701753777
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge2_max,29.89454979027788,0.991318388653234
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge2_acc,0.2423500611995104,0.0150006743735703
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge2_diff,-5.599665381566807,1.084275200365597
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rougeL_max,43.635205520666176,0.8764442787347267
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rougeL_acc,0.2839657282741738,0.0157853708583967
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rougeL_diff,-4.913623343185734,0.9748055918882316
EleutherAI/gpt-neo-1.3B,truthfulqa_mc1,0-shot,accuracy,0.2313341493268053,0.0147619451748626
EleutherAI/gpt-neox-20b,arc:challenge,25-shot,accuracy,0.4266211604095563,0.0144531855929202
EleutherAI/gpt-neox-20b,arc:challenge,25-shot,acc_norm,0.4573378839590443,0.014558106543924
EleutherAI/gpt-neox-20b,hellaswag,10-shot,accuracy,0.5416251742680741,0.0049724602068423
EleutherAI/gpt-neox-20b,hellaswag,10-shot,acc_norm,0.73451503684525,0.0044068861006858
EleutherAI/gpt-neox-20b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-anatomy,5-shot,accuracy,0.237037037037037,0.036737316839695
EleutherAI/gpt-neox-20b,hendrycksTest-anatomy,5-shot,acc_norm,0.237037037037037,0.036737316839695
EleutherAI/gpt-neox-20b,hendrycksTest-astronomy,5-shot,accuracy,0.2302631578947368,0.0342605942440316
EleutherAI/gpt-neox-20b,hendrycksTest-astronomy,5-shot,acc_norm,0.2302631578947368,0.0342605942440316
EleutherAI/gpt-neox-20b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.047258156262526
EleutherAI/gpt-neox-20b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.047258156262526
EleutherAI/gpt-neox-20b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2226415094339622,0.025604233470899
EleutherAI/gpt-neox-20b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2226415094339622,0.025604233470899
EleutherAI/gpt-neox-20b,hendrycksTest-college_biology,5-shot,accuracy,0.25,0.036210341218895
EleutherAI/gpt-neox-20b,hendrycksTest-college_biology,5-shot,acc_norm,0.25,0.036210341218895
EleutherAI/gpt-neox-20b,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/gpt-neox-20b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
EleutherAI/gpt-neox-20b,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-college_medicine,5-shot,accuracy,0.2196531791907514,0.0315680936270317
EleutherAI/gpt-neox-20b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2196531791907514,0.0315680936270317
EleutherAI/gpt-neox-20b,hendrycksTest-college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
EleutherAI/gpt-neox-20b,hendrycksTest-college_physics,5-shot,acc_norm,0.2254901960784313,0.0415830753308328
EleutherAI/gpt-neox-20b,hendrycksTest-computer_security,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-computer_security,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3617021276595745,0.0314108219759624
EleutherAI/gpt-neox-20b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3617021276595745,0.0314108219759624
EleutherAI/gpt-neox-20b,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neox-20b,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neox-20b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2068965517241379,0.0337567244956055
EleutherAI/gpt-neox-20b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2068965517241379,0.0337567244956055
EleutherAI/gpt-neox-20b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
EleutherAI/gpt-neox-20b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
EleutherAI/gpt-neox-20b,hendrycksTest-formal_logic,5-shot,accuracy,0.2301587301587301,0.037649508797906
EleutherAI/gpt-neox-20b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2301587301587301,0.037649508797906
EleutherAI/gpt-neox-20b,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/gpt-neox-20b,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2064516129032258,0.0230258996171887
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2064516129032258,0.0230258996171887
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.1477832512315271,0.0249696213335212
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.1477832512315271,0.0249696213335212
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2121212121212121,0.031922715695483
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2121212121212121,0.031922715695483
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_geography,5-shot,accuracy,0.1868686868686868,0.0277725333342189
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1868686868686868,0.0277725333342189
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2279792746113989,0.0302769099451782
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2279792746113989,0.0302769099451782
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2,0.0202808050625357
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2,0.0202808050625357
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2296296296296296,0.0256441086392676
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2296296296296296,0.0256441086392676
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.0268415143229589
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2184873949579832,0.0268415143229589
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2450331125827814,0.0351180757180472
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2450331125827814,0.0351180757180472
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2146788990825688,0.0176043041492564
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2146788990825688,0.0176043041492564
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1759259259259259,0.0259674209582585
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1759259259259259,0.0259674209582585
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2941176470588235,0.0319800166011507
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2941176470588235,0.0319800166011507
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2658227848101265,0.0287567996296583
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2658227848101265,0.0287567996296583
EleutherAI/gpt-neox-20b,hendrycksTest-human_aging,5-shot,accuracy,0.3497757847533632,0.032007367194845
EleutherAI/gpt-neox-20b,hendrycksTest-human_aging,5-shot,acc_norm,0.3497757847533632,0.032007367194845
EleutherAI/gpt-neox-20b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2519083969465648,0.0380738711630608
EleutherAI/gpt-neox-20b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2519083969465648,0.0380738711630608
EleutherAI/gpt-neox-20b,hendrycksTest-international_law,5-shot,accuracy,0.2727272727272727,0.040655781409087
EleutherAI/gpt-neox-20b,hendrycksTest-international_law,5-shot,acc_norm,0.2727272727272727,0.040655781409087
EleutherAI/gpt-neox-20b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2777777777777778,0.0433004374965074
EleutherAI/gpt-neox-20b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2777777777777778,0.0433004374965074
EleutherAI/gpt-neox-20b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2392638036809816,0.0335195387952126
EleutherAI/gpt-neox-20b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2392638036809816,0.0335195387952126
EleutherAI/gpt-neox-20b,hendrycksTest-machine_learning,5-shot,accuracy,0.25,0.0410997468263393
EleutherAI/gpt-neox-20b,hendrycksTest-machine_learning,5-shot,acc_norm,0.25,0.0410997468263393
EleutherAI/gpt-neox-20b,hendrycksTest-management,5-shot,accuracy,0.2427184466019417,0.0424502248638449
EleutherAI/gpt-neox-20b,hendrycksTest-management,5-shot,acc_norm,0.2427184466019417,0.0424502248638449
EleutherAI/gpt-neox-20b,hendrycksTest-marketing,5-shot,accuracy,0.2863247863247863,0.0296143236904566
EleutherAI/gpt-neox-20b,hendrycksTest-marketing,5-shot,acc_norm,0.2863247863247863,0.0296143236904566
EleutherAI/gpt-neox-20b,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/gpt-neox-20b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.0464823198711731
EleutherAI/gpt-neox-20b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2656449553001277,0.0157943024878887
EleutherAI/gpt-neox-20b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2656449553001277,0.0157943024878887
EleutherAI/gpt-neox-20b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
EleutherAI/gpt-neox-20b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
EleutherAI/gpt-neox-20b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2446927374301676,0.0143781698840984
EleutherAI/gpt-neox-20b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2446927374301676,0.0143781698840984
EleutherAI/gpt-neox-20b,hendrycksTest-nutrition,5-shot,accuracy,0.2745098039215686,0.0255531699918265
EleutherAI/gpt-neox-20b,hendrycksTest-nutrition,5-shot,acc_norm,0.2745098039215686,0.0255531699918265
EleutherAI/gpt-neox-20b,hendrycksTest-philosophy,5-shot,accuracy,0.2765273311897106,0.0254038329781796
EleutherAI/gpt-neox-20b,hendrycksTest-philosophy,5-shot,acc_norm,0.2765273311897106,0.0254038329781796
EleutherAI/gpt-neox-20b,hendrycksTest-prehistory,5-shot,accuracy,0.2592592592592592,0.0243836655310354
EleutherAI/gpt-neox-20b,hendrycksTest-prehistory,5-shot,acc_norm,0.2592592592592592,0.0243836655310354
EleutherAI/gpt-neox-20b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2588652482269503,0.0261295725271808
EleutherAI/gpt-neox-20b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2588652482269503,0.0261295725271808
EleutherAI/gpt-neox-20b,hendrycksTest-professional_law,5-shot,accuracy,0.2385919165580182,0.0108859297420022
EleutherAI/gpt-neox-20b,hendrycksTest-professional_law,5-shot,acc_norm,0.2385919165580182,0.0108859297420022
EleutherAI/gpt-neox-20b,hendrycksTest-professional_medicine,5-shot,accuracy,0.1911764705882352,0.0238868819224403
EleutherAI/gpt-neox-20b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1911764705882352,0.0238868819224403
EleutherAI/gpt-neox-20b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
EleutherAI/gpt-neox-20b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2598039215686274,0.0177408995091777
EleutherAI/gpt-neox-20b,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
EleutherAI/gpt-neox-20b,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.0417234303870538
EleutherAI/gpt-neox-20b,hendrycksTest-security_studies,5-shot,accuracy,0.2040816326530612,0.0258012834750904
EleutherAI/gpt-neox-20b,hendrycksTest-security_studies,5-shot,acc_norm,0.2040816326530612,0.0258012834750904
EleutherAI/gpt-neox-20b,hendrycksTest-sociology,5-shot,accuracy,0.2885572139303483,0.0320384104021332
EleutherAI/gpt-neox-20b,hendrycksTest-sociology,5-shot,acc_norm,0.2885572139303483,0.0320384104021332
EleutherAI/gpt-neox-20b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.22,0.0416333199893226
EleutherAI/gpt-neox-20b,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.0359156679782466
EleutherAI/gpt-neox-20b,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.0359156679782466
EleutherAI/gpt-neox-20b,hendrycksTest-world_religions,5-shot,accuracy,0.3333333333333333,0.0361550763031093
EleutherAI/gpt-neox-20b,hendrycksTest-world_religions,5-shot,acc_norm,0.3333333333333333,0.0361550763031093
EleutherAI/gpt-neox-20b,truthfulqa:mc,0-shot,mc1,0.193390452876377,0.013826240752599
EleutherAI/gpt-neox-20b,truthfulqa:mc,0-shot,mc2,0.3161314596733849,0.0130227567191774
EleutherAI/gpt-neox-20b,drop,3-shot,accuracy,0.001363255033557,0.000377860919646
EleutherAI/gpt-neox-20b,drop,3-shot,f1,0.0504289010067115,0.0012240402281522
EleutherAI/gpt-neox-20b,gsm8k,5-shot,accuracy,0.0545868081880212,0.0062574440379125
EleutherAI/gpt-neox-20b,winogrande,5-shot,accuracy,0.6890292028413575,0.013009534736286
allenai/OLMo-1.7-7B-hf,minerva_math_precalc,5-shot,accuracy,0.0238095238095238,0.0065304692197614
allenai/OLMo-1.7-7B-hf,minerva_math_prealgebra,5-shot,accuracy,0.1067738231917336,0.0104701641718158
allenai/OLMo-1.7-7B-hf,minerva_math_num_theory,5-shot,accuracy,0.0222222222222222,0.0063492063492063
allenai/OLMo-1.7-7B-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.0299003322259136,0.0056707815287762
allenai/OLMo-1.7-7B-hf,minerva_math_geometry,5-shot,accuracy,0.0417536534446764,0.0091489631610343
allenai/OLMo-1.7-7B-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.050632911392405,0.0100809849342132
allenai/OLMo-1.7-7B-hf,minerva_math_algebra,5-shot,accuracy,0.0850884582982308,0.0081018189910327
allenai/OLMo-1.7-7B-hf,fld_default,0-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,fld_star,0-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_3da,5-shot,accuracy,0.005,0.0015775754727385
allenai/OLMo-1.7-7B-hf,arithmetic_3ds,5-shot,accuracy,0.012,0.0024353573624298
allenai/OLMo-1.7-7B-hf,arithmetic_4da,5-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_2ds,5-shot,accuracy,0.054,0.0050551733292434
allenai/OLMo-1.7-7B-hf,arithmetic_5ds,5-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_5da,5-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_1dc,5-shot,accuracy,0.0275,0.0036576719757437
allenai/OLMo-1.7-7B-hf,arithmetic_4ds,5-shot,accuracy,0.0015,0.0008655920660521
allenai/OLMo-1.7-7B-hf,arithmetic_2dm,5-shot,accuracy,0.0525,0.0049884183022857
allenai/OLMo-1.7-7B-hf,arithmetic_2da,5-shot,accuracy,0.0665,0.0055726476832024
allenai/OLMo-1.7-7B-hf,gsm8k_cot,5-shot,accuracy,0.2615617892342683,0.0121056057333824
allenai/OLMo-1.7-7B-hf,gsm8k,5-shot,accuracy,0.266868840030326,0.0121837805518879
allenai/OLMo-1.7-7B-hf,anli_r2,0-shot,brier_score,0.8391596666130678,
allenai/OLMo-1.7-7B-hf,anli_r3,0-shot,brier_score,0.7378078013226562,
allenai/OLMo-1.7-7B-hf,anli_r1,0-shot,brier_score,0.8642801317460914,
allenai/OLMo-1.7-7B-hf,xnli_eu,0-shot,brier_score,0.9129364330761796,
allenai/OLMo-1.7-7B-hf,xnli_vi,0-shot,brier_score,0.967681110248311,
allenai/OLMo-1.7-7B-hf,xnli_ru,0-shot,brier_score,0.7775890640925086,
allenai/OLMo-1.7-7B-hf,xnli_zh,0-shot,brier_score,0.9694985754970612,
allenai/OLMo-1.7-7B-hf,xnli_tr,0-shot,brier_score,0.8886549764770375,
allenai/OLMo-1.7-7B-hf,xnli_fr,0-shot,brier_score,0.819053260383407,
allenai/OLMo-1.7-7B-hf,xnli_en,0-shot,brier_score,0.634008528496921,
allenai/OLMo-1.7-7B-hf,xnli_ur,0-shot,brier_score,1.289747157627863,
allenai/OLMo-1.7-7B-hf,xnli_ar,0-shot,brier_score,1.1791061584321425,
allenai/OLMo-1.7-7B-hf,xnli_de,0-shot,brier_score,0.8058792999721985,
allenai/OLMo-1.7-7B-hf,xnli_hi,0-shot,brier_score,1.0788350632986858,
allenai/OLMo-1.7-7B-hf,xnli_es,0-shot,brier_score,0.870575516923212,
allenai/OLMo-1.7-7B-hf,xnli_bg,0-shot,brier_score,0.8898152315502774,
allenai/OLMo-1.7-7B-hf,xnli_sw,0-shot,brier_score,0.9207528263844112,
allenai/OLMo-1.7-7B-hf,xnli_el,0-shot,brier_score,1.1774979653735478,
allenai/OLMo-1.7-7B-hf,xnli_th,0-shot,brier_score,0.9694268391093578,
allenai/OLMo-1.7-7B-hf,logiqa2,0-shot,brier_score,1.0623218884422152,
allenai/OLMo-1.7-7B-hf,mathqa,0-shot,brier_score,0.8861806154897796,
allenai/OLMo-1.7-7B-hf,lambada_standard,0-shot,perplexity,4.885471880639927,0.1074730896480909
allenai/OLMo-1.7-7B-hf,lambada_standard,0-shot,accuracy,0.6382689695323113,0.0066943254346452
allenai/OLMo-1.7-7B-hf,lambada_openai,0-shot,perplexity,3.8600315637206415,0.0794640972125437
allenai/OLMo-1.7-7B-hf,lambada_openai,0-shot,accuracy,0.7075490005821852,0.0063374841865443
allenai/OLMo-1.7-7B-hf,mmlu_world_religions,0-shot,accuracy,0.7076023391812866,0.0348864771345792
allenai/OLMo-1.7-7B-hf,mmlu_formal_logic,0-shot,accuracy,0.2539682539682539,0.0389325961060467
allenai/OLMo-1.7-7B-hf,mmlu_prehistory,0-shot,accuracy,0.5740740740740741,0.0275137472843794
allenai/OLMo-1.7-7B-hf,mmlu_moral_scenarios,0-shot,accuracy,0.2402234636871508,0.0142883438039253
allenai/OLMo-1.7-7B-hf,mmlu_high_school_world_history,0-shot,accuracy,0.6962025316455697,0.0299366963871386
allenai/OLMo-1.7-7B-hf,mmlu_moral_disputes,0-shot,accuracy,0.5751445086705202,0.0266133508402617
allenai/OLMo-1.7-7B-hf,mmlu_professional_law,0-shot,accuracy,0.3800521512385919,0.0123973282051378
allenai/OLMo-1.7-7B-hf,mmlu_logical_fallacies,0-shot,accuracy,0.588957055214724,0.0386569785378536
allenai/OLMo-1.7-7B-hf,mmlu_high_school_us_history,0-shot,accuracy,0.6225490196078431,0.034022720443407
allenai/OLMo-1.7-7B-hf,mmlu_philosophy,0-shot,accuracy,0.594855305466238,0.0278823837913259
allenai/OLMo-1.7-7B-hf,mmlu_jurisprudence,0-shot,accuracy,0.5833333333333334,0.0476607516535646
allenai/OLMo-1.7-7B-hf,mmlu_international_law,0-shot,accuracy,0.6776859504132231,0.0426641636335216
allenai/OLMo-1.7-7B-hf,mmlu_high_school_european_history,0-shot,accuracy,0.6787878787878788,0.0364620496325381
allenai/OLMo-1.7-7B-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.7357512953367875,0.0318215505091664
allenai/OLMo-1.7-7B-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.5588235294117647,0.0322529423239964
allenai/OLMo-1.7-7B-hf,mmlu_high_school_geography,0-shot,accuracy,0.7272727272727273,0.0317307123907172
allenai/OLMo-1.7-7B-hf,mmlu_high_school_psychology,0-shot,accuracy,0.7394495412844037,0.01881918203485
allenai/OLMo-1.7-7B-hf,mmlu_public_relations,0-shot,accuracy,0.6,0.0469237132203465
allenai/OLMo-1.7-7B-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.79,0.0409360180740332
allenai/OLMo-1.7-7B-hf,mmlu_sociology,0-shot,accuracy,0.746268656716418,0.030769444967296
allenai/OLMo-1.7-7B-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.5282051282051282,0.0253106392549339
allenai/OLMo-1.7-7B-hf,mmlu_security_studies,0-shot,accuracy,0.5836734693877551,0.0315578281655616
allenai/OLMo-1.7-7B-hf,mmlu_professional_psychology,0-shot,accuracy,0.5081699346405228,0.0202251343430572
allenai/OLMo-1.7-7B-hf,mmlu_human_sexuality,0-shot,accuracy,0.6030534351145038,0.0429113567100922
allenai/OLMo-1.7-7B-hf,mmlu_econometrics,0-shot,accuracy,0.3596491228070175,0.0451449613287363
allenai/OLMo-1.7-7B-hf,mmlu_miscellaneous,0-shot,accuracy,0.7113665389527458,0.0162037927031977
allenai/OLMo-1.7-7B-hf,mmlu_marketing,0-shot,accuracy,0.8034188034188035,0.0260353860989512
allenai/OLMo-1.7-7B-hf,mmlu_management,0-shot,accuracy,0.6893203883495146,0.0458212416016155
allenai/OLMo-1.7-7B-hf,mmlu_nutrition,0-shot,accuracy,0.6111111111111112,0.027914055510468
allenai/OLMo-1.7-7B-hf,mmlu_medical_genetics,0-shot,accuracy,0.57,0.0497569851956242
allenai/OLMo-1.7-7B-hf,mmlu_human_aging,0-shot,accuracy,0.6278026905829597,0.0324430528300873
allenai/OLMo-1.7-7B-hf,mmlu_professional_medicine,0-shot,accuracy,0.4852941176470588,0.0303596970790461
allenai/OLMo-1.7-7B-hf,mmlu_college_medicine,0-shot,accuracy,0.5086705202312138,0.0381189098894041
allenai/OLMo-1.7-7B-hf,mmlu_business_ethics,0-shot,accuracy,0.59,0.049431107042371
allenai/OLMo-1.7-7B-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.5773584905660377,0.0304023314457695
allenai/OLMo-1.7-7B-hf,mmlu_global_facts,0-shot,accuracy,0.35,0.0479372485441102
allenai/OLMo-1.7-7B-hf,mmlu_virology,0-shot,accuracy,0.4457831325301205,0.038695433234721
allenai/OLMo-1.7-7B-hf,mmlu_professional_accounting,0-shot,accuracy,0.4326241134751773,0.0295554542367788
allenai/OLMo-1.7-7B-hf,mmlu_college_physics,0-shot,accuracy,0.3039215686274509,0.0457666540320776
allenai/OLMo-1.7-7B-hf,mmlu_high_school_physics,0-shot,accuracy,0.3509933774834437,0.0389698196425737
allenai/OLMo-1.7-7B-hf,mmlu_high_school_biology,0-shot,accuracy,0.6548387096774193,0.0270457465735343
allenai/OLMo-1.7-7B-hf,mmlu_college_biology,0-shot,accuracy,0.5347222222222222,0.0417111585818161
allenai/OLMo-1.7-7B-hf,mmlu_anatomy,0-shot,accuracy,0.4888888888888889,0.0431827549197797
allenai/OLMo-1.7-7B-hf,mmlu_college_chemistry,0-shot,accuracy,0.43,0.0497569851956242
allenai/OLMo-1.7-7B-hf,mmlu_computer_security,0-shot,accuracy,0.62,0.0487831731214563
allenai/OLMo-1.7-7B-hf,mmlu_college_computer_science,0-shot,accuracy,0.48,0.0502116731568677
allenai/OLMo-1.7-7B-hf,mmlu_astronomy,0-shot,accuracy,0.5131578947368421,0.0406753313630917
allenai/OLMo-1.7-7B-hf,mmlu_college_mathematics,0-shot,accuracy,0.34,0.0476095228569523
allenai/OLMo-1.7-7B-hf,mmlu_conceptual_physics,0-shot,accuracy,0.425531914893617,0.0323214691622446
allenai/OLMo-1.7-7B-hf,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.0482418151324421
allenai/OLMo-1.7-7B-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.57,0.0497569851956242
allenai/OLMo-1.7-7B-hf,mmlu_machine_learning,0-shot,accuracy,0.3482142857142857,0.0452182990283358
allenai/OLMo-1.7-7B-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.4285714285714285,0.034819048444388
allenai/OLMo-1.7-7B-hf,mmlu_high_school_statistics,0-shot,accuracy,0.4444444444444444,0.0338885711850232
allenai/OLMo-1.7-7B-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.3015873015873015,0.0236369759961018
allenai/OLMo-1.7-7B-hf,mmlu_electrical_engineering,0-shot,accuracy,0.496551724137931,0.0416656757710158
allenai/OLMo-1.7-7B-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.3259259259259259,0.028578348365473
allenai/OLMo-1.7-7B-hf,arc_challenge,25-shot,accuracy,0.4598976109215017,0.0145643188569248
allenai/OLMo-1.7-7B-hf,arc_challenge,25-shot,acc_norm,0.4974402730375427,0.0146111993298437
allenai/OLMo-1.7-7B-hf,hellaswag,10-shot,accuracy,0.5886277633937462,0.0049107675408674
allenai/OLMo-1.7-7B-hf,hellaswag,10-shot,acc_norm,0.788886675960964,0.0040726458749922
allenai/OLMo-1.7-7B-hf,truthfulqa_mc2,0-shot,accuracy,0.3590781045200021,0.0134726910046502
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,bleu_max,25.135570028092875,0.7937442609696079
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,bleu_acc,0.3121175030599755,0.0162207567695209
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,bleu_diff,-8.039782636719476,0.8039064432980269
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge1_max,50.26135196252015,0.8621379304595767
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge1_acc,0.2900856793145654,0.0158862368742095
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge1_diff,-11.903242447741343,0.9071414792248184
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge2_max,33.597620557273906,1.0023202672623095
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge2_acc,0.2472460220318237,0.0151024047973596
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge2_diff,-12.949554590188171,1.025590144258018
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rougeL_max,47.52109438620076,0.8690433934810369
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rougeL_acc,0.2815177478580171,0.015744027248256
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rougeL_diff,-11.977711482592492,0.9105847605364588
allenai/OLMo-1.7-7B-hf,truthfulqa_mc1,0-shot,accuracy,0.2386780905752754,0.0149226296954564
allenai/OLMo-1.7-7B-hf,winogrande,5-shot,accuracy,0.7253354380426204,0.0125445160051171
microsoft/phi-2,mmlu_world_religions,0-shot,accuracy,0.6900584795321637,0.0354697695939316
microsoft/phi-2,mmlu_formal_logic,0-shot,accuracy,0.3412698412698413,0.0424079932757492
microsoft/phi-2,mmlu_prehistory,0-shot,accuracy,0.6049382716049383,0.0272011176669256
microsoft/phi-2,mmlu_moral_scenarios,0-shot,accuracy,0.2335195530726257,0.0141495753489762
microsoft/phi-2,mmlu_high_school_world_history,0-shot,accuracy,0.7552742616033755,0.0279856993870364
microsoft/phi-2,mmlu_moral_disputes,0-shot,accuracy,0.653179190751445,0.0256247239940304
microsoft/phi-2,mmlu_professional_law,0-shot,accuracy,0.4022164276401564,0.0125236468561801
microsoft/phi-2,mmlu_logical_fallacies,0-shot,accuracy,0.7484662576687117,0.0340899788685752
microsoft/phi-2,mmlu_high_school_us_history,0-shot,accuracy,0.6764705882352942,0.0328347205610856
microsoft/phi-2,mmlu_philosophy,0-shot,accuracy,0.5755627009646302,0.0280719282479462
microsoft/phi-2,mmlu_jurisprudence,0-shot,accuracy,0.7129629629629629,0.0437331304091476
microsoft/phi-2,mmlu_international_law,0-shot,accuracy,0.7603305785123967,0.0389687898507041
microsoft/phi-2,mmlu_high_school_european_history,0-shot,accuracy,0.6909090909090909,0.0360854101157396
microsoft/phi-2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.7564766839378239,0.0309754363868454
microsoft/phi-2,mmlu_high_school_microeconomics,0-shot,accuracy,0.592436974789916,0.0319186337447846
microsoft/phi-2,mmlu_high_school_geography,0-shot,accuracy,0.7323232323232324,0.0315444988827028
microsoft/phi-2,mmlu_high_school_psychology,0-shot,accuracy,0.7669724770642202,0.0181256691808615
microsoft/phi-2,mmlu_public_relations,0-shot,accuracy,0.6454545454545455,0.0458200484150541
microsoft/phi-2,mmlu_us_foreign_policy,0-shot,accuracy,0.72,0.0451260859854212
microsoft/phi-2,mmlu_sociology,0-shot,accuracy,0.7810945273631841,0.029239174636647
microsoft/phi-2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.5769230769230769,0.0250491978760423
microsoft/phi-2,mmlu_security_studies,0-shot,accuracy,0.6571428571428571,0.0303872629195477
microsoft/phi-2,mmlu_professional_psychology,0-shot,accuracy,0.5392156862745098,0.0201655233139079
microsoft/phi-2,mmlu_human_sexuality,0-shot,accuracy,0.648854961832061,0.0418644516301375
microsoft/phi-2,mmlu_econometrics,0-shot,accuracy,0.3070175438596491,0.0433913832257986
microsoft/phi-2,mmlu_miscellaneous,0-shot,accuracy,0.6896551724137931,0.0165437850260483
microsoft/phi-2,mmlu_marketing,0-shot,accuracy,0.7948717948717948,0.0264535080540403
microsoft/phi-2,mmlu_management,0-shot,accuracy,0.7378640776699029,0.0435463107726059
microsoft/phi-2,mmlu_nutrition,0-shot,accuracy,0.6176470588235294,0.0278261093072836
microsoft/phi-2,mmlu_medical_genetics,0-shot,accuracy,0.63,0.048523658709391
microsoft/phi-2,mmlu_human_aging,0-shot,accuracy,0.6367713004484304,0.032277904428505
microsoft/phi-2,mmlu_professional_medicine,0-shot,accuracy,0.4411764705882353,0.0301619119307671
microsoft/phi-2,mmlu_college_medicine,0-shot,accuracy,0.5260115606936416,0.0380730172650451
microsoft/phi-2,mmlu_business_ethics,0-shot,accuracy,0.58,0.0496044963748858
microsoft/phi-2,mmlu_clinical_knowledge,0-shot,accuracy,0.6264150943396226,0.0297730827133198
microsoft/phi-2,mmlu_global_facts,0-shot,accuracy,0.37,0.0485236587093909
microsoft/phi-2,mmlu_virology,0-shot,accuracy,0.4819277108433735,0.0388995125282721
microsoft/phi-2,mmlu_professional_accounting,0-shot,accuracy,0.4326241134751773,0.0295554542367788
microsoft/phi-2,mmlu_college_physics,0-shot,accuracy,0.3039215686274509,0.0457666540320776
microsoft/phi-2,mmlu_high_school_physics,0-shot,accuracy,0.390728476821192,0.039837983066598
microsoft/phi-2,mmlu_high_school_biology,0-shot,accuracy,0.6838709677419355,0.0264508744890427
microsoft/phi-2,mmlu_college_biology,0-shot,accuracy,0.6111111111111112,0.0407666325391856
microsoft/phi-2,mmlu_anatomy,0-shot,accuracy,0.4518518518518518,0.0429926890548086
microsoft/phi-2,mmlu_college_chemistry,0-shot,accuracy,0.36,0.0482418151324421
microsoft/phi-2,mmlu_computer_security,0-shot,accuracy,0.64,0.0482418151324421
microsoft/phi-2,mmlu_college_computer_science,0-shot,accuracy,0.46,0.0500908265962033
microsoft/phi-2,mmlu_astronomy,0-shot,accuracy,0.5592105263157895,0.0404031106249043
microsoft/phi-2,mmlu_college_mathematics,0-shot,accuracy,0.44,0.0498887651569858
microsoft/phi-2,mmlu_conceptual_physics,0-shot,accuracy,0.4723404255319149,0.0326359711840976
microsoft/phi-2,mmlu_abstract_algebra,0-shot,accuracy,0.31,0.0464823198711731
microsoft/phi-2,mmlu_high_school_computer_science,0-shot,accuracy,0.71,0.0456048021572068
microsoft/phi-2,mmlu_machine_learning,0-shot,accuracy,0.4910714285714285,0.0474503325548912
microsoft/phi-2,mmlu_high_school_chemistry,0-shot,accuracy,0.4778325123152709,0.03514528562175
microsoft/phi-2,mmlu_high_school_statistics,0-shot,accuracy,0.4351851851851852,0.0338120000564352
microsoft/phi-2,mmlu_elementary_mathematics,0-shot,accuracy,0.3862433862433862,0.0250759817676016
microsoft/phi-2,mmlu_electrical_engineering,0-shot,accuracy,0.5448275862068965,0.0414988694219211
microsoft/phi-2,mmlu_high_school_mathematics,0-shot,accuracy,0.2888888888888888,0.0276349072641785
microsoft/phi-2,arc_challenge,25-shot,accuracy,0.575938566552901,0.0144418896274643
microsoft/phi-2,arc_challenge,25-shot,acc_norm,0.6083617747440273,0.0142641221249382
microsoft/phi-2,hellaswag,10-shot,accuracy,0.5655247958573989,0.0049467486082713
microsoft/phi-2,hellaswag,10-shot,acc_norm,0.7542322246564429,0.0042966158627866
microsoft/phi-2,truthfulqa_mc2,0-shot,accuracy,0.4450329796896821,0.01512663339943
microsoft/phi-2,truthfulqa_gen,0-shot,bleu_max,30.636426608129604,0.8253318984766498
microsoft/phi-2,truthfulqa_gen,0-shot,bleu_acc,0.3843329253365973,0.0170287073012452
microsoft/phi-2,truthfulqa_gen,0-shot,bleu_diff,-1.6737273900429643,0.93776816748223
microsoft/phi-2,truthfulqa_gen,0-shot,rouge1_max,56.49815681439475,0.8385465502514413
microsoft/phi-2,truthfulqa_gen,0-shot,rouge1_acc,0.3929008567931457,0.017097248285233
microsoft/phi-2,truthfulqa_gen,0-shot,rouge1_diff,-1.2755753639684837,1.1278344789587786
microsoft/phi-2,truthfulqa_gen,0-shot,rouge2_max,42.30652845433004,1.0170277948205164
microsoft/phi-2,truthfulqa_gen,0-shot,rouge2_acc,0.3549571603427172,0.0167508623813759
microsoft/phi-2,truthfulqa_gen,0-shot,rouge2_diff,-2.1576932295313727,1.2989464511092814
microsoft/phi-2,truthfulqa_gen,0-shot,rougeL_max,53.81199070019868,0.8662590848816515
microsoft/phi-2,truthfulqa_gen,0-shot,rougeL_acc,0.3769889840881273,0.0169655175789303
microsoft/phi-2,truthfulqa_gen,0-shot,rougeL_diff,-1.3616416414143386,1.1477177478773908
microsoft/phi-2,truthfulqa_mc1,0-shot,accuracy,0.3096695226438188,0.0161857443551449
microsoft/phi-2,winogrande,5-shot,accuracy,0.734017363851618,0.012418323153051
microsoft/phi-2,gsm8k,5-shot,accuracy,0.576194086429113,0.0136116320088103
EleutherAI/pythia-160m-deduped,arc:challenge,25-shot,accuracy,0.2056313993174061,0.0118107452607425
EleutherAI/pythia-160m-deduped,arc:challenge,25-shot,acc_norm,0.2406143344709897,0.0124914685323905
EleutherAI/pythia-160m-deduped,hellaswag,10-shot,accuracy,0.2864967138020314,0.0045120024597579
EleutherAI/pythia-160m-deduped,hellaswag,10-shot,acc_norm,0.3138816968731328,0.0046312050996849
EleutherAI/pythia-160m-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-160m-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.0435194139889244
EleutherAI/pythia-160m-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.2222222222222222,0.0359144408419696
EleutherAI/pythia-160m-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.2222222222222222,0.0359144408419696
EleutherAI/pythia-160m-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/pythia-160m-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.1776315789473684,0.0311031823831233
EleutherAI/pythia-160m-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-160m-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/pythia-160m-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2075471698113207,0.0249599180289112
EleutherAI/pythia-160m-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2075471698113207,0.0249599180289112
EleutherAI/pythia-160m-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2291666666666666,0.0351469746786238
EleutherAI/pythia-160m-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2291666666666666,0.0351469746786238
EleutherAI/pythia-160m-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-160m-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
EleutherAI/pythia-160m-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-160m-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-160m-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-160m-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.25,0.0435194139889244
EleutherAI/pythia-160m-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177498
EleutherAI/pythia-160m-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2080924855491329,0.0309528902177498
EleutherAI/pythia-160m-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.0433643270799317
EleutherAI/pythia-160m-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.0433643270799317
EleutherAI/pythia-160m-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-160m-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
EleutherAI/pythia-160m-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2340425531914893,0.0276784525782123
EleutherAI/pythia-160m-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2340425531914893,0.0276784525782123
EleutherAI/pythia-160m-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.219298245614035,0.0389243110651875
EleutherAI/pythia-160m-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.219298245614035,0.0389243110651875
EleutherAI/pythia-160m-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2827586206896552,0.0375283395800333
EleutherAI/pythia-160m-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2827586206896552,0.0375283395800333
EleutherAI/pythia-160m-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2539682539682539,0.0224180428911139
EleutherAI/pythia-160m-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2539682539682539,0.0224180428911139
EleutherAI/pythia-160m-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.2063492063492063,0.0361960452412425
EleutherAI/pythia-160m-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.2063492063492063,0.0361960452412425
EleutherAI/pythia-160m-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.16,0.036845294917747
EleutherAI/pythia-160m-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.16,0.036845294917747
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.2548387096774193,0.0247901184593322
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2548387096774193,0.0247901184593322
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3103448275862069,0.032550867699701
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3103448275862069,0.032550867699701
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2121212121212121,0.0319227156954829
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2121212121212121,0.0319227156954829
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2474747474747475,0.0307463007421245
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2474747474747475,0.0307463007421245
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2590673575129533,0.031618779179354
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2590673575129533,0.031618779179354
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2153846153846154,0.0208430345574628
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2153846153846154,0.0208430345574628
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2555555555555555,0.026593939101844
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2555555555555555,0.026593939101844
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.226890756302521,0.0272053715382794
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.226890756302521,0.0272053715382794
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.3245033112582781,0.0382274693765875
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3245033112582781,0.0382274693765875
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2422018348623853,0.0183681763065986
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2422018348623853,0.0183681763065986
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2009803921568627,0.0281259722656543
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2009803921568627,0.0281259722656543
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2784810126582278,0.0291786823048425
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2784810126582278,0.0291786823048425
EleutherAI/pythia-160m-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.3677130044843049,0.0323619835092827
EleutherAI/pythia-160m-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.3677130044843049,0.0323619835092827
EleutherAI/pythia-160m-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2671755725190839,0.0388084830108239
EleutherAI/pythia-160m-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2671755725190839,0.0388084830108239
EleutherAI/pythia-160m-deduped,hendrycksTest-international_law,5-shot,accuracy,0.3057851239669421,0.0420595393388412
EleutherAI/pythia-160m-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.3057851239669421,0.0420595393388412
EleutherAI/pythia-160m-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2129629629629629,0.0395783547198098
EleutherAI/pythia-160m-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2129629629629629,0.0395783547198098
EleutherAI/pythia-160m-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.294478527607362,0.0358116579047408
EleutherAI/pythia-160m-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.294478527607362,0.0358116579047408
EleutherAI/pythia-160m-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.1785714285714285,0.036352091215778
EleutherAI/pythia-160m-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.1785714285714285,0.036352091215778
EleutherAI/pythia-160m-deduped,hendrycksTest-management,5-shot,accuracy,0.1553398058252427,0.0358659473857397
EleutherAI/pythia-160m-deduped,hendrycksTest-management,5-shot,acc_norm,0.1553398058252427,0.0358659473857397
EleutherAI/pythia-160m-deduped,hendrycksTest-marketing,5-shot,accuracy,0.1752136752136752,0.0249044390989182
EleutherAI/pythia-160m-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.1752136752136752,0.0249044390989182
EleutherAI/pythia-160m-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/pythia-160m-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.32,0.046882617226215
EleutherAI/pythia-160m-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2413793103448276,0.015302380123542
EleutherAI/pythia-160m-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2413793103448276,0.015302380123542
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.245664739884393,0.023176298203992
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.245664739884393,0.023176298203992
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
EleutherAI/pythia-160m-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2483660130718954,0.0247399813551135
EleutherAI/pythia-160m-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2483660130718954,0.0247399813551135
EleutherAI/pythia-160m-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2057877813504823,0.0229613399067642
EleutherAI/pythia-160m-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2057877813504823,0.0229613399067642
EleutherAI/pythia-160m-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2407407407407407,0.0237885835516585
EleutherAI/pythia-160m-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2407407407407407,0.0237885835516585
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2340425531914893,0.0252578613594324
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2340425531914893,0.0252578613594324
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2372881355932203,0.0108654366907802
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2372881355932203,0.0108654366907802
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.4375,0.0301346149544039
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4375,0.0301346149544039
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2532679738562091,0.0175934868953668
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2532679738562091,0.0175934868953668
EleutherAI/pythia-160m-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2,0.038313051408846
EleutherAI/pythia-160m-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2,0.038313051408846
EleutherAI/pythia-160m-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.3061224489795918,0.0295048964545959
EleutherAI/pythia-160m-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.3061224489795918,0.0295048964545959
EleutherAI/pythia-160m-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-160m-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-160m-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-160m-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.0456048021572068
EleutherAI/pythia-160m-deduped,hendrycksTest-virology,5-shot,accuracy,0.2469879518072289,0.0335735198206453
EleutherAI/pythia-160m-deduped,hendrycksTest-virology,5-shot,acc_norm,0.2469879518072289,0.0335735198206453
EleutherAI/pythia-160m-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.1988304093567251,0.0306111165574325
EleutherAI/pythia-160m-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.1988304093567251,0.0306111165574325
EleutherAI/pythia-160m-deduped,truthfulqa:mc,0-shot,mc1,0.241126070991432,0.0149748272797523
EleutherAI/pythia-160m-deduped,truthfulqa:mc,0-shot,mc2,0.443404463764025,0.0152071772528862
EleutherAI/pythia-160m-deduped,drop,3-shot,accuracy,0.0031459731543624,0.0005734993648436
EleutherAI/pythia-160m-deduped,drop,3-shot,f1,0.0338317953020134,0.0011064778180343
EleutherAI/pythia-160m-deduped,gsm8k,5-shot,accuracy,0.0022744503411675,0.0013121578148674
EleutherAI/pythia-160m-deduped,winogrande,5-shot,accuracy,0.5138121546961326,0.0140471229164404
cerebras/btlm-3b-8k-base,minerva_math_precalc,5-shot,accuracy,0.0146520146520146,0.0051468941589821
cerebras/btlm-3b-8k-base,minerva_math_prealgebra,5-shot,accuracy,0.0344431687715269,0.0061827380104873
cerebras/btlm-3b-8k-base,minerva_math_num_theory,5-shot,accuracy,0.0148148148148148,0.0052037049875126
cerebras/btlm-3b-8k-base,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.0049000930886157
cerebras/btlm-3b-8k-base,minerva_math_geometry,5-shot,accuracy,0.0271398747390396,0.0074321620907708
cerebras/btlm-3b-8k-base,minerva_math_counting_and_prob,5-shot,accuracy,0.0274261603375527,0.0075095381303842
cerebras/btlm-3b-8k-base,minerva_math_algebra,5-shot,accuracy,0.0193765796124684,0.0040026474981053
cerebras/btlm-3b-8k-base,fld_default,0-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,fld_star,0-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,arithmetic_3da,5-shot,accuracy,0.0215,0.0032440926417928
cerebras/btlm-3b-8k-base,arithmetic_3ds,5-shot,accuracy,0.043,0.0045371569177678
cerebras/btlm-3b-8k-base,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521
cerebras/btlm-3b-8k-base,arithmetic_2ds,5-shot,accuracy,0.45,0.0111270798484137
cerebras/btlm-3b-8k-base,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,arithmetic_1dc,5-shot,accuracy,0.0005,0.0005
cerebras/btlm-3b-8k-base,arithmetic_4ds,5-shot,accuracy,0.001,0.0007069298939339
cerebras/btlm-3b-8k-base,arithmetic_2dm,5-shot,accuracy,0.0335,0.004024546370306
cerebras/btlm-3b-8k-base,arithmetic_2da,5-shot,accuracy,0.207,0.0090618187070332
cerebras/btlm-3b-8k-base,gsm8k_cot,5-shot,accuracy,0.0416982562547384,0.0055062050581757
cerebras/btlm-3b-8k-base,gsm8k,5-shot,accuracy,0.0523123578468536,0.0061330577089592
cerebras/btlm-3b-8k-base,anli_r2,0-shot,brier_score,0.7462669818323581,
cerebras/btlm-3b-8k-base,anli_r3,0-shot,brier_score,0.7470389244949329,
cerebras/btlm-3b-8k-base,anli_r1,0-shot,brier_score,0.7739522354279014,
cerebras/btlm-3b-8k-base,xnli_eu,0-shot,brier_score,1.26760834525426,
cerebras/btlm-3b-8k-base,xnli_vi,0-shot,brier_score,1.0776254851454548,
cerebras/btlm-3b-8k-base,xnli_ru,0-shot,brier_score,0.8572693761958452,
cerebras/btlm-3b-8k-base,xnli_zh,0-shot,brier_score,1.02506106359857,
cerebras/btlm-3b-8k-base,xnli_tr,0-shot,brier_score,0.9690088326153724,
cerebras/btlm-3b-8k-base,xnli_fr,0-shot,brier_score,0.7572273331665541,
cerebras/btlm-3b-8k-base,xnli_en,0-shot,brier_score,0.6442514069003313,
cerebras/btlm-3b-8k-base,xnli_ur,0-shot,brier_score,1.306062980613888,
cerebras/btlm-3b-8k-base,xnli_ar,0-shot,brier_score,0.9938749206769896,
cerebras/btlm-3b-8k-base,xnli_de,0-shot,brier_score,0.9222235658348612,
cerebras/btlm-3b-8k-base,xnli_hi,0-shot,brier_score,0.894747482032871,
cerebras/btlm-3b-8k-base,xnli_es,0-shot,brier_score,0.925763882691409,
cerebras/btlm-3b-8k-base,xnli_bg,0-shot,brier_score,0.8952905602595471,
cerebras/btlm-3b-8k-base,xnli_sw,0-shot,brier_score,0.9874132082553372,
cerebras/btlm-3b-8k-base,xnli_el,0-shot,brier_score,1.057528087497967,
cerebras/btlm-3b-8k-base,xnli_th,0-shot,brier_score,1.2107325943420113,
cerebras/btlm-3b-8k-base,logiqa2,0-shot,brier_score,1.0390121306040254,
cerebras/btlm-3b-8k-base,mathqa,0-shot,brier_score,0.950267938981366,
cerebras/btlm-3b-8k-base,lambada_standard,0-shot,perplexity,5.9215811991829925,0.1434135352840909
cerebras/btlm-3b-8k-base,lambada_standard,0-shot,accuracy,0.6079953425189211,0.0068015487080569
cerebras/btlm-3b-8k-base,lambada_openai,0-shot,perplexity,4.721463260829105,0.1100767588281849
cerebras/btlm-3b-8k-base,lambada_openai,0-shot,accuracy,0.6615563749272269,0.0065923259327411
cerebras/btlm-3b-8k-base,mmlu_world_religions,0-shot,accuracy,0.3040935672514619,0.0352821125824523
cerebras/btlm-3b-8k-base,mmlu_formal_logic,0-shot,accuracy,0.2142857142857142,0.0367006645104718
cerebras/btlm-3b-8k-base,mmlu_prehistory,0-shot,accuracy,0.2808641975308642,0.0250064697557992
cerebras/btlm-3b-8k-base,mmlu_moral_scenarios,0-shot,accuracy,0.2458100558659217,0.0144002964292256
cerebras/btlm-3b-8k-base,mmlu_high_school_world_history,0-shot,accuracy,0.3375527426160337,0.0307815491020262
cerebras/btlm-3b-8k-base,mmlu_moral_disputes,0-shot,accuracy,0.2861271676300578,0.0243321467791341
cerebras/btlm-3b-8k-base,mmlu_professional_law,0-shot,accuracy,0.2529335071707953,0.0111022687138399
cerebras/btlm-3b-8k-base,mmlu_logical_fallacies,0-shot,accuracy,0.2147239263803681,0.0322621937728677
cerebras/btlm-3b-8k-base,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.0305875913516042
cerebras/btlm-3b-8k-base,mmlu_philosophy,0-shot,accuracy,0.2765273311897106,0.0254038329781796
cerebras/btlm-3b-8k-base,mmlu_jurisprudence,0-shot,accuracy,0.2685185185185185,0.0428446796805219
cerebras/btlm-3b-8k-base,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.040261875275912
cerebras/btlm-3b-8k-base,mmlu_high_school_european_history,0-shot,accuracy,0.296969696969697,0.0356796977226804
cerebras/btlm-3b-8k-base,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2746113989637305,0.0322102450804115
cerebras/btlm-3b-8k-base,mmlu_high_school_microeconomics,0-shot,accuracy,0.2563025210084033,0.0283596208705339
cerebras/btlm-3b-8k-base,mmlu_high_school_geography,0-shot,accuracy,0.2272727272727272,0.0298575156733864
cerebras/btlm-3b-8k-base,mmlu_high_school_psychology,0-shot,accuracy,0.2623853211009174,0.0188618850215347
cerebras/btlm-3b-8k-base,mmlu_public_relations,0-shot,accuracy,0.2818181818181818,0.0430911870994645
cerebras/btlm-3b-8k-base,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.0456048021572068
cerebras/btlm-3b-8k-base,mmlu_sociology,0-shot,accuracy,0.3034825870646766,0.0325100681645861
cerebras/btlm-3b-8k-base,mmlu_high_school_macroeconomics,0-shot,accuracy,0.241025641025641,0.0216855466653331
cerebras/btlm-3b-8k-base,mmlu_security_studies,0-shot,accuracy,0.3510204081632653,0.0305553167555736
cerebras/btlm-3b-8k-base,mmlu_professional_psychology,0-shot,accuracy,0.2712418300653594,0.0179866153040303
cerebras/btlm-3b-8k-base,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
cerebras/btlm-3b-8k-base,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.0409698513984367
cerebras/btlm-3b-8k-base,mmlu_miscellaneous,0-shot,accuracy,0.3192848020434227,0.0166712617495387
cerebras/btlm-3b-8k-base,mmlu_marketing,0-shot,accuracy,0.282051282051282,0.0294803605495411
cerebras/btlm-3b-8k-base,mmlu_management,0-shot,accuracy,0.203883495145631,0.0398913985953177
cerebras/btlm-3b-8k-base,mmlu_nutrition,0-shot,accuracy,0.2516339869281045,0.0248480182638752
cerebras/btlm-3b-8k-base,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
cerebras/btlm-3b-8k-base,mmlu_human_aging,0-shot,accuracy,0.3901345291479821,0.0327376672545915
cerebras/btlm-3b-8k-base,mmlu_professional_medicine,0-shot,accuracy,0.2720588235294117,0.0270330411516814
cerebras/btlm-3b-8k-base,mmlu_college_medicine,0-shot,accuracy,0.2427745664739884,0.0326926380614177
cerebras/btlm-3b-8k-base,mmlu_business_ethics,0-shot,accuracy,0.31,0.0464823198711731
cerebras/btlm-3b-8k-base,mmlu_clinical_knowledge,0-shot,accuracy,0.2339622641509434,0.0260552969011529
cerebras/btlm-3b-8k-base,mmlu_global_facts,0-shot,accuracy,0.32,0.046882617226215
cerebras/btlm-3b-8k-base,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.0361080501803102
cerebras/btlm-3b-8k-base,mmlu_professional_accounting,0-shot,accuracy,0.2446808510638297,0.0256455536222667
cerebras/btlm-3b-8k-base,mmlu_college_physics,0-shot,accuracy,0.2549019607843137,0.0433643270799317
cerebras/btlm-3b-8k-base,mmlu_high_school_physics,0-shot,accuracy,0.2317880794701986,0.0344540627198705
cerebras/btlm-3b-8k-base,mmlu_high_school_biology,0-shot,accuracy,0.2387096774193548,0.0242510712622088
cerebras/btlm-3b-8k-base,mmlu_college_biology,0-shot,accuracy,0.3055555555555556,0.0385208469600853
cerebras/btlm-3b-8k-base,mmlu_anatomy,0-shot,accuracy,0.2074074074074074,0.0350255317067831
cerebras/btlm-3b-8k-base,mmlu_college_chemistry,0-shot,accuracy,0.27,0.0446196043338474
cerebras/btlm-3b-8k-base,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276807
cerebras/btlm-3b-8k-base,mmlu_college_computer_science,0-shot,accuracy,0.3,0.0460566186471838
cerebras/btlm-3b-8k-base,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.0358349617636106
cerebras/btlm-3b-8k-base,mmlu_college_mathematics,0-shot,accuracy,0.3,0.0460566186471838
cerebras/btlm-3b-8k-base,mmlu_conceptual_physics,0-shot,accuracy,0.2978723404255319,0.0298961456820954
cerebras/btlm-3b-8k-base,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
cerebras/btlm-3b-8k-base,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.042295258468165
cerebras/btlm-3b-8k-base,mmlu_machine_learning,0-shot,accuracy,0.2589285714285714,0.0415775153986562
cerebras/btlm-3b-8k-base,mmlu_high_school_chemistry,0-shot,accuracy,0.2266009852216748,0.0294548638352929
cerebras/btlm-3b-8k-base,mmlu_high_school_statistics,0-shot,accuracy,0.2546296296296296,0.0297112758600053
cerebras/btlm-3b-8k-base,mmlu_elementary_mathematics,0-shot,accuracy,0.2619047619047619,0.0226442126155252
cerebras/btlm-3b-8k-base,mmlu_electrical_engineering,0-shot,accuracy,0.2137931034482758,0.0341652044774754
cerebras/btlm-3b-8k-base,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
cerebras/btlm-3b-8k-base,arc_challenge,25-shot,accuracy,0.3831058020477815,0.0142064726616728
cerebras/btlm-3b-8k-base,arc_challenge,25-shot,acc_norm,0.4112627986348123,0.014379441068522
cerebras/btlm-3b-8k-base,hellaswag,10-shot,accuracy,0.5226050587532364,0.0049846793593756
cerebras/btlm-3b-8k-base,hellaswag,10-shot,acc_norm,0.7099183429595698,0.0045287239518782
cerebras/btlm-3b-8k-base,truthfulqa_mc2,0-shot,accuracy,0.3596457019833742,0.0135822905895899
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,bleu_max,24.382575479594944,0.7680990447091552
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,bleu_acc,0.3243574051407589,0.0163879767796479
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,bleu_diff,-7.56470475629629,0.8511362909045714
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge1_max,48.69810706635801,0.8872210250095384
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge1_acc,0.2778457772337821,0.0156809293640246
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge1_diff,-9.685772374361353,1.0120488242587284
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge2_max,32.46140782430542,1.0148035582417252
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge2_acc,0.233782129742962,0.0148161959919315
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge2_diff,-11.23682928612394,1.121260025022326
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rougeL_max,46.12917282015782,0.9023627784282652
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rougeL_acc,0.2656058751529987,0.0154610276272535
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rougeL_diff,-9.989538296930288,1.0154544227366404
cerebras/btlm-3b-8k-base,truthfulqa_mc1,0-shot,accuracy,0.222766217870257,0.0145665069613967
cerebras/btlm-3b-8k-base,winogrande,5-shot,accuracy,0.6614048934490924,0.0133001698658424
CohereForAI/c4ai-command-r-v01,arc:challenge,25-shot,accuracy,0.64419795221843,0.0139905711379187
CohereForAI/c4ai-command-r-v01,arc:challenge,25-shot,acc_norm,0.6552901023890785,0.0138888162867821
CohereForAI/c4ai-command-r-v01,hellaswag,10-shot,accuracy,0.6742680740888269,0.0046768988619789
CohereForAI/c4ai-command-r-v01,hellaswag,10-shot,acc_norm,0.8700458076080462,0.0033556582385714
CohereForAI/c4ai-command-r-v01,hendrycksTest-abstract_algebra,5-shot,accuracy,0.39,0.0490207130000197
CohereForAI/c4ai-command-r-v01,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.39,0.0490207130000197
CohereForAI/c4ai-command-r-v01,hendrycksTest-anatomy,5-shot,accuracy,0.6296296296296297,0.0417165416135454
CohereForAI/c4ai-command-r-v01,hendrycksTest-anatomy,5-shot,acc_norm,0.6296296296296297,0.0417165416135454
CohereForAI/c4ai-command-r-v01,hendrycksTest-astronomy,5-shot,accuracy,0.7631578947368421,0.0345977760681053
CohereForAI/c4ai-command-r-v01,hendrycksTest-astronomy,5-shot,acc_norm,0.7631578947368421,0.0345977760681053
CohereForAI/c4ai-command-r-v01,hendrycksTest-business_ethics,5-shot,accuracy,0.75,0.0435194139889244
CohereForAI/c4ai-command-r-v01,hendrycksTest-business_ethics,5-shot,acc_norm,0.75,0.0435194139889244
CohereForAI/c4ai-command-r-v01,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7509433962264151,0.0266164829805017
CohereForAI/c4ai-command-r-v01,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7509433962264151,0.0266164829805017
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_biology,5-shot,accuracy,0.7777777777777778,0.0347659010430413
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_biology,5-shot,acc_norm,0.7777777777777778,0.0347659010430413
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_chemistry,5-shot,accuracy,0.48,0.0502116731568677
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_chemistry,5-shot,acc_norm,0.48,0.0502116731568677
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_computer_science,5-shot,accuracy,0.63,0.0485236587093909
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_computer_science,5-shot,acc_norm,0.63,0.0485236587093909
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_medicine,5-shot,accuracy,0.6589595375722543,0.0361466542418082
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_medicine,5-shot,acc_norm,0.6589595375722543,0.0361466542418082
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_physics,5-shot,accuracy,0.4019607843137255,0.0487860871446699
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_physics,5-shot,acc_norm,0.4019607843137255,0.0487860871446699
CohereForAI/c4ai-command-r-v01,hendrycksTest-computer_security,5-shot,accuracy,0.78,0.0416333199893226
CohereForAI/c4ai-command-r-v01,hendrycksTest-computer_security,5-shot,acc_norm,0.78,0.0416333199893226
CohereForAI/c4ai-command-r-v01,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6042553191489362,0.0319675869783536
CohereForAI/c4ai-command-r-v01,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6042553191489362,0.0319675869783536
CohereForAI/c4ai-command-r-v01,hendrycksTest-econometrics,5-shot,accuracy,0.5614035087719298,0.0466800073851045
CohereForAI/c4ai-command-r-v01,hendrycksTest-econometrics,5-shot,acc_norm,0.5614035087719298,0.0466800073851045
CohereForAI/c4ai-command-r-v01,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5793103448275863,0.0411391498118926
CohereForAI/c4ai-command-r-v01,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5793103448275863,0.0411391498118926
CohereForAI/c4ai-command-r-v01,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4656084656084656,0.0256903217624938
CohereForAI/c4ai-command-r-v01,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4656084656084656,0.0256903217624938
CohereForAI/c4ai-command-r-v01,hendrycksTest-formal_logic,5-shot,accuracy,0.4603174603174603,0.0445802912547097
CohereForAI/c4ai-command-r-v01,hendrycksTest-formal_logic,5-shot,acc_norm,0.4603174603174603,0.0445802912547097
CohereForAI/c4ai-command-r-v01,hendrycksTest-global_facts,5-shot,accuracy,0.46,0.0500908265962033
CohereForAI/c4ai-command-r-v01,hendrycksTest-global_facts,5-shot,acc_norm,0.46,0.0500908265962033
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_biology,5-shot,accuracy,0.7741935483870968,0.023785577884181
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7741935483870968,0.023785577884181
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.035128190778761
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.035128190778761
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.73,0.0446196043338473
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.73,0.0446196043338473
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8121212121212121,0.0305019340594291
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8121212121212121,0.0305019340594291
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_geography,5-shot,accuracy,0.8181818181818182,0.0274796030105387
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8181818181818182,0.0274796030105387
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.911917098445596,0.020453746601601
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.911917098445596,0.020453746601601
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6615384615384615,0.023991500500313
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6615384615384615,0.023991500500313
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3185185185185185,0.0284065330906084
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3185185185185185,0.0284065330906084
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7226890756302521,0.02907937453948
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7226890756302521,0.02907937453948
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_physics,5-shot,accuracy,0.3841059602649007,0.0397130181471919
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3841059602649007,0.0397130181471919
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8440366972477065,0.0155558027135901
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8440366972477065,0.0155558027135901
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5694444444444444,0.0337692215125233
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5694444444444444,0.0337692215125233
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8382352941176471,0.0258450179869269
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8382352941176471,0.0258450179869269
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_world_history,5-shot,accuracy,0.869198312236287,0.0219487660594707
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.869198312236287,0.0219487660594707
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_aging,5-shot,accuracy,0.7757847533632287,0.0279915342585195
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_aging,5-shot,acc_norm,0.7757847533632287,0.0279915342585195
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_sexuality,5-shot,accuracy,0.8244274809160306,0.0333682033847607
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8244274809160306,0.0333682033847607
CohereForAI/c4ai-command-r-v01,hendrycksTest-international_law,5-shot,accuracy,0.859504132231405,0.0317223342600215
CohereForAI/c4ai-command-r-v01,hendrycksTest-international_law,5-shot,acc_norm,0.859504132231405,0.0317223342600215
CohereForAI/c4ai-command-r-v01,hendrycksTest-jurisprudence,5-shot,accuracy,0.8518518518518519,0.03434300243631
CohereForAI/c4ai-command-r-v01,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8518518518518519,0.03434300243631
CohereForAI/c4ai-command-r-v01,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8404907975460123,0.0287674817259838
CohereForAI/c4ai-command-r-v01,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8404907975460123,0.0287674817259838
CohereForAI/c4ai-command-r-v01,hendrycksTest-machine_learning,5-shot,accuracy,0.4732142857142857,0.0473897511927415
CohereForAI/c4ai-command-r-v01,hendrycksTest-machine_learning,5-shot,acc_norm,0.4732142857142857,0.0473897511927415
CohereForAI/c4ai-command-r-v01,hendrycksTest-management,5-shot,accuracy,0.7961165048543689,0.0398913985953177
CohereForAI/c4ai-command-r-v01,hendrycksTest-management,5-shot,acc_norm,0.7961165048543689,0.0398913985953177
CohereForAI/c4ai-command-r-v01,hendrycksTest-marketing,5-shot,accuracy,0.905982905982906,0.0191198927989249
CohereForAI/c4ai-command-r-v01,hendrycksTest-marketing,5-shot,acc_norm,0.905982905982906,0.0191198927989249
CohereForAI/c4ai-command-r-v01,hendrycksTest-medical_genetics,5-shot,accuracy,0.79,0.0409360180740332
CohereForAI/c4ai-command-r-v01,hendrycksTest-medical_genetics,5-shot,acc_norm,0.79,0.0409360180740332
CohereForAI/c4ai-command-r-v01,hendrycksTest-miscellaneous,5-shot,accuracy,0.859514687100894,0.0124262113530934
CohereForAI/c4ai-command-r-v01,hendrycksTest-miscellaneous,5-shot,acc_norm,0.859514687100894,0.0124262113530934
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_disputes,5-shot,accuracy,0.7196531791907514,0.0241824274965776
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7196531791907514,0.0241824274965776
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_scenarios,5-shot,accuracy,0.5720670391061452,0.0165478879974161
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.5720670391061452,0.0165478879974161
CohereForAI/c4ai-command-r-v01,hendrycksTest-nutrition,5-shot,accuracy,0.7222222222222222,0.0256468630971379
CohereForAI/c4ai-command-r-v01,hendrycksTest-nutrition,5-shot,acc_norm,0.7222222222222222,0.0256468630971379
CohereForAI/c4ai-command-r-v01,hendrycksTest-philosophy,5-shot,accuracy,0.7491961414790996,0.0246197719566971
CohereForAI/c4ai-command-r-v01,hendrycksTest-philosophy,5-shot,acc_norm,0.7491961414790996,0.0246197719566971
CohereForAI/c4ai-command-r-v01,hendrycksTest-prehistory,5-shot,accuracy,0.7808641975308642,0.0230167056402621
CohereForAI/c4ai-command-r-v01,hendrycksTest-prehistory,5-shot,acc_norm,0.7808641975308642,0.0230167056402621
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_accounting,5-shot,accuracy,0.5390070921985816,0.0297365925264244
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5390070921985816,0.0297365925264244
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_law,5-shot,accuracy,0.5508474576271186,0.0127040305188514
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_law,5-shot,acc_norm,0.5508474576271186,0.0127040305188514
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_medicine,5-shot,accuracy,0.6838235294117647,0.0282456873914629
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_medicine,5-shot,acc_norm,0.6838235294117647,0.0282456873914629
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_psychology,5-shot,accuracy,0.7238562091503268,0.0180872769356631
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7238562091503268,0.0180872769356631
CohereForAI/c4ai-command-r-v01,hendrycksTest-public_relations,5-shot,accuracy,0.7363636363636363,0.0422022469297198
CohereForAI/c4ai-command-r-v01,hendrycksTest-public_relations,5-shot,acc_norm,0.7363636363636363,0.0422022469297198
CohereForAI/c4ai-command-r-v01,hendrycksTest-security_studies,5-shot,accuracy,0.763265306122449,0.0272128358840731
CohereForAI/c4ai-command-r-v01,hendrycksTest-security_studies,5-shot,acc_norm,0.763265306122449,0.0272128358840731
CohereForAI/c4ai-command-r-v01,hendrycksTest-sociology,5-shot,accuracy,0.8805970149253731,0.0229287932772197
CohereForAI/c4ai-command-r-v01,hendrycksTest-sociology,5-shot,acc_norm,0.8805970149253731,0.0229287932772197
CohereForAI/c4ai-command-r-v01,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.0348735088019776
CohereForAI/c4ai-command-r-v01,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.0348735088019776
CohereForAI/c4ai-command-r-v01,hendrycksTest-virology,5-shot,accuracy,0.5301204819277109,0.0388542542086676
CohereForAI/c4ai-command-r-v01,hendrycksTest-virology,5-shot,acc_norm,0.5301204819277109,0.0388542542086676
CohereForAI/c4ai-command-r-v01,hendrycksTest-world_religions,5-shot,accuracy,0.847953216374269,0.0275391228890614
CohereForAI/c4ai-command-r-v01,hendrycksTest-world_religions,5-shot,acc_norm,0.847953216374269,0.0275391228890614
CohereForAI/c4ai-command-r-v01,truthfulqa:mc,0-shot,mc1,0.3635250917992656,0.0168388628839658
CohereForAI/c4ai-command-r-v01,truthfulqa:mc,0-shot,mc2,0.5231991183271746,0.0154930189638173
CohereForAI/c4ai-command-r-v01,winogrande,5-shot,accuracy,0.8153117600631413,0.0109059781121568
CohereForAI/c4ai-command-r-v01,gsm8k,5-shot,accuracy,0.5663381349507203,0.0136507280470646
facebook/xglm-564M,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_prealgebra,5-shot,accuracy,0.0011481056257175,0.0011481056257175
facebook/xglm-564M,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_algebra,5-shot,accuracy,0.0008424599831508,0.0008424599831507
facebook/xglm-564M,fld_default,0-shot,accuracy,0.0,
facebook/xglm-564M,fld_star,0-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
facebook/xglm-564M,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005
facebook/xglm-564M,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
facebook/xglm-564M,arithmetic_2ds,5-shot,accuracy,0.008,0.0019924821184884
facebook/xglm-564M,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339
facebook/xglm-564M,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_2dm,5-shot,accuracy,0.0185,0.0030138707185866
facebook/xglm-564M,arithmetic_2da,5-shot,accuracy,0.0065,0.0017973564602277
facebook/xglm-564M,gsm8k_cot,5-shot,accuracy,0.0212282031842304,0.0039704491298486
facebook/xglm-564M,anli_r2,0-shot,brier_score,0.8949833517502548,
facebook/xglm-564M,anli_r3,0-shot,brier_score,0.8360668742052519,
facebook/xglm-564M,anli_r1,0-shot,brier_score,0.8884687642270159,
facebook/xglm-564M,xnli_eu,0-shot,brier_score,0.7371713039069413,
facebook/xglm-564M,xnli_vi,0-shot,brier_score,0.8654369273137161,
facebook/xglm-564M,xnli_ru,0-shot,brier_score,0.7606713225425753,
facebook/xglm-564M,xnli_zh,0-shot,brier_score,1.0486949265688723,
facebook/xglm-564M,xnli_tr,0-shot,brier_score,0.8214109452628465,
facebook/xglm-564M,xnli_fr,0-shot,brier_score,0.7815638841837665,
facebook/xglm-564M,xnli_en,0-shot,brier_score,0.7112428085311449,
facebook/xglm-564M,xnli_ur,0-shot,brier_score,1.062454393810576,
facebook/xglm-564M,xnli_ar,0-shot,brier_score,1.296895724941598,
facebook/xglm-564M,xnli_de,0-shot,brier_score,0.8351648045281843,
facebook/xglm-564M,xnli_hi,0-shot,brier_score,0.8394046981053793,
facebook/xglm-564M,xnli_es,0-shot,brier_score,0.8566611033916195,
facebook/xglm-564M,xnli_bg,0-shot,brier_score,0.7676392908318753,
facebook/xglm-564M,xnli_sw,0-shot,brier_score,0.8344826636346988,
facebook/xglm-564M,xnli_el,0-shot,brier_score,0.8773823206071238,
facebook/xglm-564M,xnli_th,0-shot,brier_score,0.8581311641212404,
facebook/xglm-564M,logiqa2,0-shot,brier_score,1.1939318152210765,
facebook/xglm-564M,mathqa,0-shot,brier_score,1.0334681136329835,
facebook/xglm-564M,lambada_standard,0-shot,perplexity,42.939212075145306,1.641006871749824
facebook/xglm-564M,lambada_standard,0-shot,accuracy,0.3291286629148069,0.0065465809755531
facebook/xglm-564M,lambada_openai,0-shot,perplexity,28.56367062584693,1.0264515146295097
facebook/xglm-564M,lambada_openai,0-shot,accuracy,0.3584319813700757,0.0066809281736803
facebook/xglm-564M,mmlu_world_religions,0-shot,accuracy,0.2280701754385964,0.0321809379560235
facebook/xglm-564M,mmlu_formal_logic,0-shot,accuracy,0.1428571428571428,0.031298431857438
facebook/xglm-564M,mmlu_prehistory,0-shot,accuracy,0.2160493827160493,0.0228991629184458
facebook/xglm-564M,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/xglm-564M,mmlu_high_school_world_history,0-shot,accuracy,0.2489451476793249,0.0281469705994226
facebook/xglm-564M,mmlu_moral_disputes,0-shot,accuracy,0.2485549132947976,0.0232675284321001
facebook/xglm-564M,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.0109961566351426
facebook/xglm-564M,mmlu_logical_fallacies,0-shot,accuracy,0.263803680981595,0.0346241993161562
facebook/xglm-564M,mmlu_high_school_us_history,0-shot,accuracy,0.2794117647058823,0.0314932810450795
facebook/xglm-564M,mmlu_philosophy,0-shot,accuracy,0.1832797427652733,0.0219741988482658
facebook/xglm-564M,mmlu_jurisprudence,0-shot,accuracy,0.2592592592592592,0.0423651125809463
facebook/xglm-564M,mmlu_international_law,0-shot,accuracy,0.3223140495867768,0.0426641636335216
facebook/xglm-564M,mmlu_high_school_european_history,0-shot,accuracy,0.2666666666666666,0.0345313180188541
facebook/xglm-564M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2694300518134715,0.0320186712287779
facebook/xglm-564M,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678637
facebook/xglm-564M,mmlu_high_school_geography,0-shot,accuracy,0.1868686868686868,0.0277725333342189
facebook/xglm-564M,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936
facebook/xglm-564M,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
facebook/xglm-564M,mmlu_us_foreign_policy,0-shot,accuracy,0.23,0.042295258468165
facebook/xglm-564M,mmlu_sociology,0-shot,accuracy,0.2288557213930348,0.0297052840567724
facebook/xglm-564M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2076923076923077,0.0205675395672468
facebook/xglm-564M,mmlu_security_studies,0-shot,accuracy,0.2612244897959184,0.0281234293351427
facebook/xglm-564M,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
facebook/xglm-564M,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
facebook/xglm-564M,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.0418577442402205
facebook/xglm-564M,mmlu_miscellaneous,0-shot,accuracy,0.2375478927203065,0.0152187330461501
facebook/xglm-564M,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.0298725777088911
facebook/xglm-564M,mmlu_management,0-shot,accuracy,0.1844660194174757,0.0384042362728827
facebook/xglm-564M,mmlu_nutrition,0-shot,accuracy,0.218954248366013,0.0236790898618077
facebook/xglm-564M,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
facebook/xglm-564M,mmlu_human_aging,0-shot,accuracy,0.3318385650224215,0.0316029514377667
facebook/xglm-564M,mmlu_professional_medicine,0-shot,accuracy,0.4448529411764705,0.0301875320603293
facebook/xglm-564M,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.0312651120617304
facebook/xglm-564M,mmlu_business_ethics,0-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-564M,mmlu_clinical_knowledge,0-shot,accuracy,0.2037735849056604,0.0247907845017754
facebook/xglm-564M,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
facebook/xglm-564M,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
facebook/xglm-564M,mmlu_professional_accounting,0-shot,accuracy,0.2801418439716312,0.0267891723511402
facebook/xglm-564M,mmlu_college_physics,0-shot,accuracy,0.2352941176470588,0.0422077365917145
facebook/xglm-564M,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.0329798664847383
facebook/xglm-564M,mmlu_high_school_biology,0-shot,accuracy,0.2709677419354839,0.0252844161149001
facebook/xglm-564M,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.0347659010430413
facebook/xglm-564M,mmlu_anatomy,0-shot,accuracy,0.2962962962962963,0.0394462416250111
facebook/xglm-564M,mmlu_college_chemistry,0-shot,accuracy,0.2,0.0402015126103684
facebook/xglm-564M,mmlu_computer_security,0-shot,accuracy,0.25,0.0435194139889244
facebook/xglm-564M,mmlu_college_computer_science,0-shot,accuracy,0.16,0.036845294917747
facebook/xglm-564M,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
facebook/xglm-564M,mmlu_college_mathematics,0-shot,accuracy,0.24,0.0429234695990928
facebook/xglm-564M,mmlu_conceptual_physics,0-shot,accuracy,0.3063829787234042,0.0301359064785175
facebook/xglm-564M,mmlu_abstract_algebra,0-shot,accuracy,0.2,0.0402015126103684
facebook/xglm-564M,mmlu_high_school_computer_science,0-shot,accuracy,0.31,0.0464823198711731
facebook/xglm-564M,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.0449394906861353
facebook/xglm-564M,mmlu_high_school_chemistry,0-shot,accuracy,0.2955665024630542,0.0321049443375145
facebook/xglm-564M,mmlu_high_school_statistics,0-shot,accuracy,0.4583333333333333,0.0339811089029463
facebook/xglm-564M,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.0224945107675031
facebook/xglm-564M,mmlu_electrical_engineering,0-shot,accuracy,0.2551724137931034,0.0363298405270784
facebook/xglm-564M,mmlu_high_school_mathematics,0-shot,accuracy,0.237037037037037,0.0259288761327661
facebook/xglm-564M,arc_challenge,25-shot,accuracy,0.2064846416382252,0.0118288656190023
facebook/xglm-564M,arc_challenge,25-shot,acc_norm,0.2542662116040955,0.0127249999451577
facebook/xglm-564M,truthfulqa_mc2,0-shot,accuracy,0.4037316135738844,0.0148657189622228
facebook/xglm-564M,truthfulqa_gen,0-shot,bleu_max,1.3466045659305554,0.0699003960606437
facebook/xglm-564M,truthfulqa_gen,0-shot,bleu_acc,0.3280293757649938,0.016435632932815
facebook/xglm-564M,truthfulqa_gen,0-shot,bleu_diff,-0.2413219814434506,0.0422639869522222
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge1_max,6.215723953891085,0.1887917253922373
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge1_acc,0.3561811505507956,0.0167637907284463
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge1_diff,-0.5530378301879756,0.1049215506089369
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge2_max,3.0775068077414733,0.1443089347191126
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge2_acc,0.2399020807833537,0.0149488126790621
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge2_diff,-0.6499181850303296,0.0934320138734598
facebook/xglm-564M,truthfulqa_gen,0-shot,rougeL_max,5.696483921597016,0.1816196852303411
facebook/xglm-564M,truthfulqa_gen,0-shot,rougeL_acc,0.3463892288861689,0.0166569971091251
facebook/xglm-564M,truthfulqa_gen,0-shot,rougeL_diff,-0.5754493671860949,0.100363161107059
facebook/xglm-564M,truthfulqa_mc1,0-shot,accuracy,0.2288861689106487,0.014706994909055
Salesforce/codegen-350M-multi,minerva_math_precalc,5-shot,accuracy,0.0109890109890109,0.0044656184273314
Salesforce/codegen-350M-multi,minerva_math_prealgebra,5-shot,accuracy,0.0195177956371986,0.0046900299352845
Salesforce/codegen-350M-multi,minerva_math_num_theory,5-shot,accuracy,0.0055555555555555,0.0032015451273209
Salesforce/codegen-350M-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.0121816168327796,0.0036524791938863
Salesforce/codegen-350M-multi,minerva_math_geometry,5-shot,accuracy,0.0020876826722338,0.0020876826722338
Salesforce/codegen-350M-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.0084388185654008,0.004206007207713
Salesforce/codegen-350M-multi,minerva_math_algebra,5-shot,accuracy,0.0143218197135636,0.003450041570937
Salesforce/codegen-350M-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521
Salesforce/codegen-350M-multi,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339
Salesforce/codegen-350M-multi,arithmetic_4da,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_2ds,5-shot,accuracy,0.0015,0.0008655920660521
Salesforce/codegen-350M-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_1dc,5-shot,accuracy,0.029,0.0037532044004605
Salesforce/codegen-350M-multi,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_2dm,5-shot,accuracy,0.016,0.0028064101569415
Salesforce/codegen-350M-multi,arithmetic_2da,5-shot,accuracy,0.004,0.0014117352790977
Salesforce/codegen-350M-multi,gsm8k_cot,5-shot,accuracy,0.0227445034116755,0.0041066206377497
Salesforce/codegen-350M-multi,gsm8k,5-shot,accuracy,0.0204700530705079,0.0039004133859157
Salesforce/codegen-350M-multi,anli_r2,0-shot,brier_score,0.985719346196084,
Salesforce/codegen-350M-multi,anli_r3,0-shot,brier_score,0.9732009467249916,
Salesforce/codegen-350M-multi,anli_r1,0-shot,brier_score,0.9985649407847308,
Salesforce/codegen-350M-multi,xnli_eu,0-shot,brier_score,1.2461840807929438,
Salesforce/codegen-350M-multi,xnli_vi,0-shot,brier_score,1.1678074729947867,
Salesforce/codegen-350M-multi,xnli_ru,0-shot,brier_score,0.9554534962904668,
Salesforce/codegen-350M-multi,xnli_zh,0-shot,brier_score,1.0339034216345016,
Salesforce/codegen-350M-multi,xnli_tr,0-shot,brier_score,1.255509308223099,
Salesforce/codegen-350M-multi,xnli_fr,0-shot,brier_score,1.2357496951451048,
Salesforce/codegen-350M-multi,xnli_en,0-shot,brier_score,0.8204070568367904,
Salesforce/codegen-350M-multi,xnli_ur,0-shot,brier_score,1.2449382078214195,
Salesforce/codegen-350M-multi,xnli_ar,0-shot,brier_score,1.026144943391084,
Salesforce/codegen-350M-multi,xnli_de,0-shot,brier_score,0.972227313977324,
Salesforce/codegen-350M-multi,xnli_hi,0-shot,brier_score,1.001365390726176,
Salesforce/codegen-350M-multi,xnli_es,0-shot,brier_score,1.1864389653417815,
Salesforce/codegen-350M-multi,xnli_bg,0-shot,brier_score,0.9590675242887678,
Salesforce/codegen-350M-multi,xnli_sw,0-shot,brier_score,1.087228705558193,
Salesforce/codegen-350M-multi,xnli_el,0-shot,brier_score,1.107009491211521,
Salesforce/codegen-350M-multi,xnli_th,0-shot,brier_score,1.0205217980811283,
Salesforce/codegen-350M-multi,logiqa2,0-shot,brier_score,1.1839026222832,
Salesforce/codegen-350M-multi,mathqa,0-shot,brier_score,1.0109975167317484,
Salesforce/codegen-350M-multi,lambada_standard,0-shot,perplexity,929.843244445648,44.08843957528335
Salesforce/codegen-350M-multi,lambada_standard,0-shot,accuracy,0.1201241994954395,0.0045293722269201
Salesforce/codegen-350M-multi,lambada_openai,0-shot,perplexity,1118.2514080440103,58.16346545779122
Salesforce/codegen-350M-multi,lambada_openai,0-shot,accuracy,0.1265282359790413,0.0046315913556623
Salesforce/codegen-350M-multi,mmlu_world_religions,0-shot,accuracy,0.2514619883040935,0.0332750442384684
Salesforce/codegen-350M-multi,mmlu_formal_logic,0-shot,accuracy,0.1825396825396825,0.0345507101910214
Salesforce/codegen-350M-multi,mmlu_prehistory,0-shot,accuracy,0.228395061728395,0.0233582118406262
Salesforce/codegen-350M-multi,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
Salesforce/codegen-350M-multi,mmlu_high_school_world_history,0-shot,accuracy,0.2911392405063291,0.0295716010657533
Salesforce/codegen-350M-multi,mmlu_moral_disputes,0-shot,accuracy,0.2341040462427745,0.0227971102780711
Salesforce/codegen-350M-multi,mmlu_professional_law,0-shot,accuracy,0.2470664928292047,0.0110157522552793
Salesforce/codegen-350M-multi,mmlu_logical_fallacies,0-shot,accuracy,0.2883435582822086,0.0355903953161734
Salesforce/codegen-350M-multi,mmlu_high_school_us_history,0-shot,accuracy,0.2745098039215686,0.0313217980308328
Salesforce/codegen-350M-multi,mmlu_philosophy,0-shot,accuracy,0.2572347266881029,0.0248261712892508
Salesforce/codegen-350M-multi,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
Salesforce/codegen-350M-multi,mmlu_international_law,0-shot,accuracy,0.2809917355371901,0.0410320383051451
Salesforce/codegen-350M-multi,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.0354656301962433
Salesforce/codegen-350M-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3678756476683937,0.0348017566846603
Salesforce/codegen-350M-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.2142857142857142,0.0266535315967154
Salesforce/codegen-350M-multi,mmlu_high_school_geography,0-shot,accuracy,0.3484848484848485,0.033948539651564
Salesforce/codegen-350M-multi,mmlu_high_school_psychology,0-shot,accuracy,0.3064220183486238,0.0197655172204585
Salesforce/codegen-350M-multi,mmlu_public_relations,0-shot,accuracy,0.1454545454545454,0.0337689831983308
Salesforce/codegen-350M-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-350M-multi,mmlu_sociology,0-shot,accuracy,0.2338308457711442,0.0299294154083483
Salesforce/codegen-350M-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3256410256410256,0.0237596657674122
Salesforce/codegen-350M-multi,mmlu_security_studies,0-shot,accuracy,0.3061224489795918,0.0295048964545959
Salesforce/codegen-350M-multi,mmlu_professional_psychology,0-shot,accuracy,0.2549019607843137,0.0176308273751483
Salesforce/codegen-350M-multi,mmlu_human_sexuality,0-shot,accuracy,0.2442748091603053,0.0376833595972874
Salesforce/codegen-350M-multi,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
Salesforce/codegen-350M-multi,mmlu_miscellaneous,0-shot,accuracy,0.2452107279693486,0.0153843522845439
Salesforce/codegen-350M-multi,mmlu_marketing,0-shot,accuracy,0.1965811965811965,0.0260353860989512
Salesforce/codegen-350M-multi,mmlu_management,0-shot,accuracy,0.2621359223300971,0.0435463107726059
Salesforce/codegen-350M-multi,mmlu_nutrition,0-shot,accuracy,0.261437908496732,0.0251609982142924
Salesforce/codegen-350M-multi,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-350M-multi,mmlu_human_aging,0-shot,accuracy,0.2825112107623318,0.0302168310115087
Salesforce/codegen-350M-multi,mmlu_professional_medicine,0-shot,accuracy,0.4522058823529412,0.0302337585515964
Salesforce/codegen-350M-multi,mmlu_college_medicine,0-shot,accuracy,0.2716763005780346,0.0339175032232165
Salesforce/codegen-350M-multi,mmlu_business_ethics,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-350M-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.2830188679245283,0.0277242364927009
Salesforce/codegen-350M-multi,mmlu_global_facts,0-shot,accuracy,0.16,0.036845294917747
Salesforce/codegen-350M-multi,mmlu_virology,0-shot,accuracy,0.1686746987951807,0.0291520096278565
Salesforce/codegen-350M-multi,mmlu_professional_accounting,0-shot,accuracy,0.226950354609929,0.0249871063656429
Salesforce/codegen-350M-multi,mmlu_college_physics,0-shot,accuracy,0.2254901960784313,0.0415830753308328
Salesforce/codegen-350M-multi,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.0365860326276374
Salesforce/codegen-350M-multi,mmlu_high_school_biology,0-shot,accuracy,0.2741935483870967,0.0253781399708852
Salesforce/codegen-350M-multi,mmlu_college_biology,0-shot,accuracy,0.2083333333333333,0.0339611620584533
Salesforce/codegen-350M-multi,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.0374985070917402
Salesforce/codegen-350M-multi,mmlu_college_chemistry,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-multi,mmlu_computer_security,0-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-350M-multi,mmlu_college_computer_science,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-multi,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
Salesforce/codegen-350M-multi,mmlu_college_mathematics,0-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-350M-multi,mmlu_conceptual_physics,0-shot,accuracy,0.2425531914893617,0.0280202262712002
Salesforce/codegen-350M-multi,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-350M-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.19,0.0394277244403662
Salesforce/codegen-350M-multi,mmlu_machine_learning,0-shot,accuracy,0.3303571428571428,0.0446428571428571
Salesforce/codegen-350M-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.2167487684729064,0.0289903312525162
Salesforce/codegen-350M-multi,mmlu_high_school_statistics,0-shot,accuracy,0.4629629629629629,0.0340060362553827
Salesforce/codegen-350M-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.0224945107675031
Salesforce/codegen-350M-multi,mmlu_electrical_engineering,0-shot,accuracy,0.2758620689655172,0.0372456361977463
Salesforce/codegen-350M-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.2814814814814815,0.0274200193509452
Salesforce/codegen-350M-multi,arc_challenge,25-shot,accuracy,0.1732081911262798,0.0110586941832803
Salesforce/codegen-350M-multi,arc_challenge,25-shot,acc_norm,0.2192832764505119,0.0120912457876157
Salesforce/codegen-350M-multi,hellaswag,10-shot,accuracy,0.274546903007369,0.0044537359009478
Salesforce/codegen-350M-multi,hellaswag,10-shot,acc_norm,0.2935670185222067,0.0045446519760401
Salesforce/codegen-350M-multi,truthfulqa_mc2,0-shot,accuracy,0.468786844189529,0.0159197578083543
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,bleu_max,14.843134516314572,0.5728559782867888
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,bleu_acc,0.2864137086903305,0.0158261424395023
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,bleu_diff,-2.2717050031248,0.4333116828119593
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge1_max,35.03124298423832,0.7866389432856384
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge1_acc,0.2888616891064871,0.0158663464013843
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge1_diff,-4.893306750479266,0.5725461641851453
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge2_max,19.758755174632952,0.79627641363227
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge2_acc,0.2068543451652386,0.0141795914967283
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge2_diff,-4.274872146257501,0.5919097348223242
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rougeL_max,32.14833428429696,0.7758150933271327
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rougeL_acc,0.2668298653610771,0.0154836919392372
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rougeL_diff,-4.921493483348142,0.5510430423849421
Salesforce/codegen-350M-multi,truthfulqa_mc1,0-shot,accuracy,0.2753977968176254,0.0156381356677755
Salesforce/codegen-350M-multi,winogrande,5-shot,accuracy,0.516179952644041,0.0140451261309786
mistral-community/Mixtral-8x22B-v0.1,arc:challenge,25-shot,accuracy,0.6646757679180887,0.0137961829477855
mistral-community/Mixtral-8x22B-v0.1,arc:challenge,25-shot,acc_norm,0.7047781569965871,0.0133297502933823
mistral-community/Mixtral-8x22B-v0.1,hellaswag,10-shot,accuracy,0.7050388368850826,0.0045509331425287
mistral-community/Mixtral-8x22B-v0.1,hellaswag,10-shot,acc_norm,0.8872734515036845,0.0031561189647529
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.5,0.0502518907629606
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.5,0.0502518907629606
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.762962962962963,0.036737316839695
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.762962962962963,0.036737316839695
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.881578947368421,0.0262939958554749
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.881578947368421,0.0262939958554749
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.73,0.0446196043338473
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.73,0.0446196043338473
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8264150943396227,0.0233105830260062
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8264150943396227,0.0233105830260062
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.8819444444444444,0.0269833465033093
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.8819444444444444,0.0269833465033093
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.6,0.049236596391733
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.6,0.049236596391733
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.72,0.0451260859854212
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.72,0.0451260859854212
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.48,0.0502116731568677
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.48,0.0502116731568677
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.7861271676300579,0.0312651120617304
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.7861271676300579,0.0312651120617304
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.5392156862745098,0.0495985996638418
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.5392156862745098,0.0495985996638418
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.82,0.0386122919665369
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.82,0.0386122919665369
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.8085106382978723,0.0257221499926377
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.8085106382978723,0.0257221499926377
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.6842105263157895,0.04372748290278
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.6842105263157895,0.04372748290278
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7724137931034483,0.0349395038013118
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7724137931034483,0.0349395038013118
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.6190476190476191,0.0250107491161375
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.6190476190476191,0.0250107491161375
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.6031746031746031,0.0437588849272705
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.6031746031746031,0.0437588849272705
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.55,0.05
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.55,0.05
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.9129032258064516,0.0160411007416966
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.9129032258064516,0.0160411007416966
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6699507389162561,0.0330853042622825
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6699507389162561,0.0330853042622825
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.87,0.033799766898963
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.87,0.033799766898963
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8545454545454545,0.0275301963550665
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8545454545454545,0.0275301963550665
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.914141414141414,0.0199602255631728
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.914141414141414,0.0199602255631728
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9689119170984456,0.012525310625527
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9689119170984456,0.012525310625527
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8051282051282052,0.0200831675951813
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8051282051282052,0.0200831675951813
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4518518518518518,0.0303438629985126
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4518518518518518,0.0303438629985126
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8739495798319328,0.0215596231212139
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8739495798319328,0.0215596231212139
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.5562913907284768,0.0405652790228173
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5562913907284768,0.0405652790228173
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9247706422018348,0.0113086625375717
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9247706422018348,0.0113086625375717
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6898148148148148,0.0315469628565662
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6898148148148148,0.0315469628565662
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8970588235294118,0.0213283375708043
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8970588235294118,0.0213283375708043
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9029535864978904,0.0192693230256402
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9029535864978904,0.0192693230256402
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.7982062780269058,0.0269361119128022
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.7982062780269058,0.0269361119128022
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.900763358778626,0.0262222351714773
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.900763358778626,0.0262222351714773
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.9173553719008264,0.0251353823566042
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.9173553719008264,0.0251353823566042
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.0352070399051796
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.0352070399051796
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8773006134969326,0.0257773284269789
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8773006134969326,0.0257773284269789
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.6428571428571429,0.0454796099976437
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.6428571428571429,0.0454796099976437
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-management,5-shot,accuracy,0.8737864077669902,0.0328818027880862
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-management,5-shot,acc_norm,0.8737864077669902,0.0328818027880862
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.9230769230769232,0.0174569878724361
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.9230769230769232,0.0174569878724361
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.84,0.036845294917747
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.84,0.036845294917747
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.9029374201787996,0.0105864747120182
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9029374201787996,0.0105864747120182
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.8294797687861272,0.0202479615693037
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8294797687861272,0.0202479615693037
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6547486033519553,0.0159014326089303
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6547486033519553,0.0159014326089303
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.8660130718954249,0.0195048906184648
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.8660130718954249,0.0195048906184648
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.842443729903537,0.0206922372735839
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.842443729903537,0.0206922372735839
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.8703703703703703,0.018689725721062
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.8703703703703703,0.018689725721062
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.6205673758865248,0.0289473388516141
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6205673758865248,0.0289473388516141
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.6114732724902217,0.0124488178382923
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.6114732724902217,0.0124488178382923
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.875,0.0200897433029359
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.875,0.0200897433029359
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.8349673202614379,0.0150175507992473
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8349673202614379,0.0150175507992473
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.7727272727272727,0.0401396455407277
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.7727272727272727,0.0401396455407277
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.8653061224489796,0.0218556588408116
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.8653061224489796,0.0218556588408116
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.9203980099502488,0.0191396856335038
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.9203980099502488,0.0191396856335038
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.94,0.0238683256575941
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.94,0.0238683256575941
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5783132530120482,0.0384445318177091
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5783132530120482,0.0384445318177091
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8947368421052632,0.0235375576578925
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8947368421052632,0.0235375576578925
mistral-community/Mixtral-8x22B-v0.1,truthfulqa:mc,0-shot,mc1,0.3341493268053855,0.0165125306771505
mistral-community/Mixtral-8x22B-v0.1,truthfulqa:mc,0-shot,mc2,0.5108062819806165,0.0145609437130532
mistral-community/Mixtral-8x22B-v0.1,winogrande,5-shot,accuracy,0.8453038674033149,0.0101631726504335
mistral-community/Mixtral-8x22B-v0.1,gsm8k,5-shot,accuracy,0.7414708112206216,0.0120599113725161
meta-llama/Llama-2-7b-chat-hf,drop,3-shot,accuracy,0.0676384228187919,0.0025717489509556
meta-llama/Llama-2-7b-chat-hf,drop,3-shot,f1,0.1308557046979862,0.0028825856446422
meta-llama/Llama-2-7b-chat-hf,gsm8k,5-shot,accuracy,0.224412433661865,0.0114916177566305
meta-llama/Llama-2-7b-chat-hf,winogrande,5-shot,accuracy,0.7237569060773481,0.0125668150156981
meta-llama/Llama-2-7b-chat-hf,mmlu_world_religions,0-shot,accuracy,0.7251461988304093,0.0342404292469158
meta-llama/Llama-2-7b-chat-hf,mmlu_formal_logic,0-shot,accuracy,0.2539682539682539,0.0389325961060467
meta-llama/Llama-2-7b-chat-hf,mmlu_prehistory,0-shot,accuracy,0.5679012345679012,0.0275630109716066
meta-llama/Llama-2-7b-chat-hf,mmlu_moral_scenarios,0-shot,accuracy,0.2312849162011173,0.0141022236231525
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_world_history,0-shot,accuracy,0.6708860759493671,0.0305873262947023
meta-llama/Llama-2-7b-chat-hf,mmlu_moral_disputes,0-shot,accuracy,0.523121387283237,0.0268902978813031
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_law,0-shot,accuracy,0.3500651890482399,0.0121825523132151
meta-llama/Llama-2-7b-chat-hf,mmlu_logical_fallacies,0-shot,accuracy,0.5398773006134969,0.0391585729143697
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_us_history,0-shot,accuracy,0.6764705882352942,0.0328347205610856
meta-llama/Llama-2-7b-chat-hf,mmlu_philosophy,0-shot,accuracy,0.5594855305466238,0.0281964005741974
meta-llama/Llama-2-7b-chat-hf,mmlu_jurisprudence,0-shot,accuracy,0.5925925925925926,0.0475007734119998
meta-llama/Llama-2-7b-chat-hf,mmlu_international_law,0-shot,accuracy,0.6363636363636364,0.0439132628672407
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_european_history,0-shot,accuracy,0.5818181818181818,0.0385171631939839
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6994818652849741,0.0330881859441574
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.4243697478991596,0.0321047905101577
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_geography,0-shot,accuracy,0.5909090909090909,0.03502975799413
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_psychology,0-shot,accuracy,0.6752293577981652,0.0200777291093103
meta-llama/Llama-2-7b-chat-hf,mmlu_public_relations,0-shot,accuracy,0.5272727272727272,0.0478200179138006
meta-llama/Llama-2-7b-chat-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.71,0.0456048021572068
meta-llama/Llama-2-7b-chat-hf,mmlu_sociology,0-shot,accuracy,0.6467661691542289,0.0337979061179677
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4153846153846154,0.0249853549231023
meta-llama/Llama-2-7b-chat-hf,mmlu_security_studies,0-shot,accuracy,0.5265306122448979,0.0319641273452327
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_psychology,0-shot,accuracy,0.4787581699346405,0.0202095723886002
meta-llama/Llama-2-7b-chat-hf,mmlu_human_sexuality,0-shot,accuracy,0.5725190839694656,0.043389203057924
meta-llama/Llama-2-7b-chat-hf,mmlu_econometrics,0-shot,accuracy,0.3684210526315789,0.0453781535493939
meta-llama/Llama-2-7b-chat-hf,mmlu_miscellaneous,0-shot,accuracy,0.6781609195402298,0.0167063814150579
meta-llama/Llama-2-7b-chat-hf,mmlu_marketing,0-shot,accuracy,0.7094017094017094,0.029745048572674
meta-llama/Llama-2-7b-chat-hf,mmlu_management,0-shot,accuracy,0.6699029126213593,0.0465614711001235
meta-llama/Llama-2-7b-chat-hf,mmlu_nutrition,0-shot,accuracy,0.5196078431372549,0.028607893699576
meta-llama/Llama-2-7b-chat-hf,mmlu_medical_genetics,0-shot,accuracy,0.48,0.0502116731568677
meta-llama/Llama-2-7b-chat-hf,mmlu_human_aging,0-shot,accuracy,0.5739910313901345,0.0331883328621728
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_medicine,0-shot,accuracy,0.4558823529411764,0.0302543725739766
meta-llama/Llama-2-7b-chat-hf,mmlu_college_medicine,0-shot,accuracy,0.4046242774566474,0.0374246119388724
meta-llama/Llama-2-7b-chat-hf,mmlu_business_ethics,0-shot,accuracy,0.52,0.0502116731568677
meta-llama/Llama-2-7b-chat-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.5433962264150943,0.0306567486967394
meta-llama/Llama-2-7b-chat-hf,mmlu_global_facts,0-shot,accuracy,0.36,0.0482418151324421
meta-llama/Llama-2-7b-chat-hf,mmlu_virology,0-shot,accuracy,0.4277108433734939,0.0385159768371853
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_accounting,0-shot,accuracy,0.3617021276595745,0.0286638201471994
meta-llama/Llama-2-7b-chat-hf,mmlu_college_physics,0-shot,accuracy,0.2254901960784313,0.0415830753308328
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.0368488152138902
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_biology,0-shot,accuracy,0.5161290322580645,0.0284292031767245
meta-llama/Llama-2-7b-chat-hf,mmlu_college_biology,0-shot,accuracy,0.5138888888888888,0.04179596617581
meta-llama/Llama-2-7b-chat-hf,mmlu_anatomy,0-shot,accuracy,0.4222222222222222,0.0426676340409958
meta-llama/Llama-2-7b-chat-hf,mmlu_college_chemistry,0-shot,accuracy,0.28,0.0451260859854212
meta-llama/Llama-2-7b-chat-hf,mmlu_computer_security,0-shot,accuracy,0.58,0.0496044963748858
meta-llama/Llama-2-7b-chat-hf,mmlu_college_computer_science,0-shot,accuracy,0.38,0.0487831731214563
meta-llama/Llama-2-7b-chat-hf,mmlu_astronomy,0-shot,accuracy,0.4802631578947368,0.040657710025626
meta-llama/Llama-2-7b-chat-hf,mmlu_college_mathematics,0-shot,accuracy,0.35,0.0479372485441101
meta-llama/Llama-2-7b-chat-hf,mmlu_conceptual_physics,0-shot,accuracy,0.4170212765957446,0.0322327626671171
meta-llama/Llama-2-7b-chat-hf,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.0451260859854212
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.41,0.049431107042371
meta-llama/Llama-2-7b-chat-hf,mmlu_machine_learning,0-shot,accuracy,0.3035714285714285,0.0436422615584104
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.3596059113300492,0.0337645824650956
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_statistics,0-shot,accuracy,0.3379629629629629,0.0322594135263129
meta-llama/Llama-2-7b-chat-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.2989417989417989,0.0235776047916558
meta-llama/Llama-2-7b-chat-hf,mmlu_electrical_engineering,0-shot,accuracy,0.503448275862069,0.0416656757710157
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
meta-llama/Llama-2-7b-chat-hf,arc_challenge,25-shot,accuracy,0.4965870307167235,0.014611050403244
meta-llama/Llama-2-7b-chat-hf,arc_challenge,25-shot,acc_norm,0.5315699658703071,0.0145822364608669
meta-llama/Llama-2-7b-chat-hf,hellaswag,10-shot,accuracy,0.5947022505476997,0.0048994621118323
meta-llama/Llama-2-7b-chat-hf,hellaswag,10-shot,acc_norm,0.7894841665006971,0.0040684184172756
meta-llama/Llama-2-7b-chat-hf,truthfulqa_mc2,0-shot,accuracy,0.4531582138681175,0.0156390413533169
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,bleu_max,20.472111648556485,0.6992083497906967
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,bleu_acc,0.449204406364749,0.0174129419861153
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,bleu_diff,-1.7469763977464083,0.6106131740110677
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge1_max,45.30661702215245,0.7994928909675085
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge1_acc,0.4455324357405141,0.0173993352801403
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge1_diff,-1.823653233932142,0.7513119253259004
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge2_max,30.17172248739628,0.8894034989138645
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge2_acc,0.3818849449204406,0.0170081019391634
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge2_diff,-3.252024210254133,0.8763263995994484
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rougeL_max,42.04839858066761,0.8019864423352677
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rougeL_acc,0.4467564259485924,0.0174039775225571
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rougeL_diff,-2.098732465700866,0.7482280753043922
meta-llama/Llama-2-7b-chat-hf,truthfulqa_mc1,0-shot,accuracy,0.3035495716034271,0.0160958841553868
bigscience/bloom-1b7,minerva_math_precalc,5-shot,accuracy,0.0073260073260073,0.003652908089383
bigscience/bloom-1b7,minerva_math_prealgebra,5-shot,accuracy,0.0045924225028702,0.0022922488477038
bigscience/bloom-1b7,minerva_math_num_theory,5-shot,accuracy,0.0018518518518518,0.0018518518518518
bigscience/bloom-1b7,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417,0.001565259593407
bigscience/bloom-1b7,minerva_math_geometry,5-shot,accuracy,0.0020876826722338,0.0020876826722338
bigscience/bloom-1b7,minerva_math_counting_and_prob,5-shot,accuracy,0.0042194092827004,0.002980417365102
bigscience/bloom-1b7,minerva_math_algebra,5-shot,accuracy,0.0016849199663016,0.0011909159437168
bigscience/bloom-1b7,fld_default,0-shot,accuracy,0.0,
bigscience/bloom-1b7,fld_star,0-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
bigscience/bloom-1b7,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005
bigscience/bloom-1b7,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
bigscience/bloom-1b7,arithmetic_2ds,5-shot,accuracy,0.0125,0.0024849471787626
bigscience/bloom-1b7,arithmetic_5ds,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_5da,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_1dc,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_4ds,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_2dm,5-shot,accuracy,0.0215,0.0032440926417928
bigscience/bloom-1b7,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
bigscience/bloom-1b7,gsm8k_cot,5-shot,accuracy,0.0197119029567854,0.0038289829787357
bigscience/bloom-1b7,anli_r2,0-shot,brier_score,1.0330875069706045,
bigscience/bloom-1b7,anli_r3,0-shot,brier_score,0.972528258500966,
bigscience/bloom-1b7,anli_r1,0-shot,brier_score,1.060858355860431,
bigscience/bloom-1b7,xnli_eu,0-shot,brier_score,0.7072047602264854,
bigscience/bloom-1b7,xnli_vi,0-shot,brier_score,0.7456517590436983,
bigscience/bloom-1b7,xnli_ru,0-shot,brier_score,0.8780666443369725,
bigscience/bloom-1b7,xnli_zh,0-shot,brier_score,1.0595421588854064,
bigscience/bloom-1b7,xnli_tr,0-shot,brier_score,1.0231158229219457,
bigscience/bloom-1b7,xnli_fr,0-shot,brier_score,0.7775316951866434,
bigscience/bloom-1b7,xnli_en,0-shot,brier_score,0.6711576322549884,
bigscience/bloom-1b7,xnli_ur,0-shot,brier_score,0.923647671686833,
bigscience/bloom-1b7,xnli_ar,0-shot,brier_score,1.1627421056647778,
bigscience/bloom-1b7,xnli_de,0-shot,brier_score,0.8945053213391374,
bigscience/bloom-1b7,xnli_hi,0-shot,brier_score,0.7454041920812018,
bigscience/bloom-1b7,xnli_es,0-shot,brier_score,0.8212945296045504,
bigscience/bloom-1b7,xnli_bg,0-shot,brier_score,0.9352486767969872,
bigscience/bloom-1b7,xnli_sw,0-shot,brier_score,1.033987465428587,
bigscience/bloom-1b7,xnli_el,0-shot,brier_score,0.9576070804353392,
bigscience/bloom-1b7,xnli_th,0-shot,brier_score,1.311056215973815,
bigscience/bloom-1b7,logiqa2,0-shot,brier_score,1.1991580009166556,
bigscience/bloom-1b7,mathqa,0-shot,brier_score,0.9845015446410308,
bigscience/bloom-1b7,lambada_standard,0-shot,perplexity,16.696442854077176,0.5544057536741888
bigscience/bloom-1b7,lambada_standard,0-shot,accuracy,0.4449835047545119,0.006923679791679
bigscience/bloom-1b7,lambada_openai,0-shot,perplexity,12.585182126996132,0.4008910742689631
bigscience/bloom-1b7,lambada_openai,0-shot,accuracy,0.4622549970890743,0.0069461006470815
bigscience/bloom-1b7,mmlu_world_religions,0-shot,accuracy,0.2690058479532163,0.0340105262010409
bigscience/bloom-1b7,mmlu_formal_logic,0-shot,accuracy,0.3571428571428571,0.0428571428571428
bigscience/bloom-1b7,mmlu_prehistory,0-shot,accuracy,0.2253086419753086,0.0232462026478197
bigscience/bloom-1b7,mmlu_moral_scenarios,0-shot,accuracy,0.2726256983240223,0.0148933917352496
bigscience/bloom-1b7,mmlu_high_school_world_history,0-shot,accuracy,0.2447257383966244,0.0279856993870364
bigscience/bloom-1b7,mmlu_moral_disputes,0-shot,accuracy,0.2716763005780346,0.0239485129054683
bigscience/bloom-1b7,mmlu_professional_law,0-shot,accuracy,0.2646675358539765,0.0112673329928455
bigscience/bloom-1b7,mmlu_logical_fallacies,0-shot,accuracy,0.2699386503067484,0.0348782516849789
bigscience/bloom-1b7,mmlu_high_school_us_history,0-shot,accuracy,0.2450980392156862,0.0301902824535019
bigscience/bloom-1b7,mmlu_philosophy,0-shot,accuracy,0.2797427652733119,0.0254942593506949
bigscience/bloom-1b7,mmlu_jurisprudence,0-shot,accuracy,0.2407407407407407,0.0413311944024383
bigscience/bloom-1b7,mmlu_international_law,0-shot,accuracy,0.2231404958677686,0.0380075447522873
bigscience/bloom-1b7,mmlu_high_school_european_history,0-shot,accuracy,0.2787878787878788,0.0350143870629678
bigscience/bloom-1b7,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3782383419689119,0.0349980727619333
bigscience/bloom-1b7,mmlu_high_school_microeconomics,0-shot,accuracy,0.2941176470588235,0.029597329730978
bigscience/bloom-1b7,mmlu_high_school_geography,0-shot,accuracy,0.3636363636363636,0.0342730865299993
bigscience/bloom-1b7,mmlu_high_school_psychology,0-shot,accuracy,0.3486238532110092,0.0204312540907143
bigscience/bloom-1b7,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.0426579211094058
bigscience/bloom-1b7,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
bigscience/bloom-1b7,mmlu_sociology,0-shot,accuracy,0.2686567164179104,0.0313432835820895
bigscience/bloom-1b7,mmlu_high_school_macroeconomics,0-shot,accuracy,0.358974358974359,0.0243217384846023
bigscience/bloom-1b7,mmlu_security_studies,0-shot,accuracy,0.4,0.0313625024093589
bigscience/bloom-1b7,mmlu_professional_psychology,0-shot,accuracy,0.2908496732026144,0.0183731169159039
bigscience/bloom-1b7,mmlu_human_sexuality,0-shot,accuracy,0.2290076335877862,0.0368534663171185
bigscience/bloom-1b7,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
bigscience/bloom-1b7,mmlu_miscellaneous,0-shot,accuracy,0.2043422733077905,0.0144191239809319
bigscience/bloom-1b7,mmlu_marketing,0-shot,accuracy,0.2393162393162393,0.0279518268089243
bigscience/bloom-1b7,mmlu_management,0-shot,accuracy,0.3883495145631068,0.0482572933735639
bigscience/bloom-1b7,mmlu_nutrition,0-shot,accuracy,0.2777777777777778,0.0256468630971379
bigscience/bloom-1b7,mmlu_medical_genetics,0-shot,accuracy,0.25,0.0435194139889244
bigscience/bloom-1b7,mmlu_human_aging,0-shot,accuracy,0.1345291479820627,0.0229011837615755
bigscience/bloom-1b7,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091215
bigscience/bloom-1b7,mmlu_college_medicine,0-shot,accuracy,0.2427745664739884,0.0326926380614177
bigscience/bloom-1b7,mmlu_business_ethics,0-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-1b7,mmlu_clinical_knowledge,0-shot,accuracy,0.2981132075471698,0.0281528379424938
bigscience/bloom-1b7,mmlu_global_facts,0-shot,accuracy,0.31,0.0464823198711731
bigscience/bloom-1b7,mmlu_virology,0-shot,accuracy,0.1987951807228915,0.0310693902607894
bigscience/bloom-1b7,mmlu_professional_accounting,0-shot,accuracy,0.2730496453900709,0.0265778609433078
bigscience/bloom-1b7,mmlu_college_physics,0-shot,accuracy,0.2647058823529412,0.0438986995680877
bigscience/bloom-1b7,mmlu_high_school_physics,0-shot,accuracy,0.3311258278145695,0.0384258171865986
bigscience/bloom-1b7,mmlu_high_school_biology,0-shot,accuracy,0.2967741935483871,0.0259885007924119
bigscience/bloom-1b7,mmlu_college_biology,0-shot,accuracy,0.2777777777777778,0.0374555479146245
bigscience/bloom-1b7,mmlu_anatomy,0-shot,accuracy,0.2444444444444444,0.0371253783361486
bigscience/bloom-1b7,mmlu_college_chemistry,0-shot,accuracy,0.22,0.0416333199893226
bigscience/bloom-1b7,mmlu_computer_security,0-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-1b7,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
bigscience/bloom-1b7,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.0358349617636106
bigscience/bloom-1b7,mmlu_college_mathematics,0-shot,accuracy,0.31,0.0464823198711731
bigscience/bloom-1b7,mmlu_conceptual_physics,0-shot,accuracy,0.225531914893617,0.0273210784173875
bigscience/bloom-1b7,mmlu_abstract_algebra,0-shot,accuracy,0.18,0.0386122919665369
bigscience/bloom-1b7,mmlu_high_school_computer_science,0-shot,accuracy,0.22,0.0416333199893226
bigscience/bloom-1b7,mmlu_machine_learning,0-shot,accuracy,0.1785714285714285,0.036352091215778
bigscience/bloom-1b7,mmlu_high_school_chemistry,0-shot,accuracy,0.2807881773399014,0.0316185633535861
bigscience/bloom-1b7,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
bigscience/bloom-1b7,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.0217659616721545
bigscience/bloom-1b7,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.0360010569272777
bigscience/bloom-1b7,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
bigscience/bloom-1b7,arc_challenge,25-shot,accuracy,0.2627986348122867,0.0128625231753513
bigscience/bloom-1b7,arc_challenge,25-shot,acc_norm,0.2977815699658703,0.0133630801072444
bigscience/bloom-1b7,truthfulqa_mc2,0-shot,accuracy,0.4132695309257005,0.0144349387872041
bigscience/bloom-1b7,truthfulqa_gen,0-shot,bleu_max,6.25057775012128,0.3528698043271998
bigscience/bloom-1b7,truthfulqa_gen,0-shot,bleu_acc,0.2472460220318237,0.0151024047973596
bigscience/bloom-1b7,truthfulqa_gen,0-shot,bleu_diff,-0.4783944349233914,0.2910323308258888
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge1_max,18.178185223654555,0.7404994679390557
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge1_acc,0.2521419828641371,0.0152015222462999
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge1_diff,-1.0202739853119616,0.4615520291209216
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge2_max,10.60641799597891,0.5882439588833563
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge2_acc,0.1872705018359853,0.013657229868067
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge2_diff,-1.1219916084328412,0.4934334962891775
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rougeL_max,17.075940690862858,0.7066741788370577
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rougeL_acc,0.2509179926560587,0.0151769850277076
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rougeL_diff,-0.81444520512165,0.4536773949245845
bigscience/bloom-1b7,truthfulqa_mc1,0-shot,accuracy,0.244798041615667,0.0150518694867149
mosaicml/mpt-7b-instruct,drop,3-shot,accuracy,0.2429739932885906,0.0043921275795198
mosaicml/mpt-7b-instruct,drop,3-shot,f1,0.2939712667785233,0.0043826840891421
mosaicml/mpt-7b-instruct,arc:challenge,25-shot,accuracy,0.4436860068259385,0.0145184218256704
mosaicml/mpt-7b-instruct,arc:challenge,25-shot,acc_norm,0.5034129692832765,0.014611050403244
mosaicml/mpt-7b-instruct,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.0464823198711731
mosaicml/mpt-7b-instruct,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.0464823198711731
mosaicml/mpt-7b-instruct,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.0388500424580025
mosaicml/mpt-7b-instruct,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.0388500424580025
mosaicml/mpt-7b-instruct,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.0373852067611966
mosaicml/mpt-7b-instruct,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.0373852067611966
mosaicml/mpt-7b-instruct,hendrycksTest-business_ethics,5-shot,accuracy,0.28,0.0451260859854212
mosaicml/mpt-7b-instruct,hendrycksTest-business_ethics,5-shot,acc_norm,0.28,0.0451260859854212
mosaicml/mpt-7b-instruct,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.350943396226415,0.0293736462532346
mosaicml/mpt-7b-instruct,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.350943396226415,0.0293736462532346
mosaicml/mpt-7b-instruct,hendrycksTest-college_biology,5-shot,accuracy,0.3194444444444444,0.0389907368735733
mosaicml/mpt-7b-instruct,hendrycksTest-college_biology,5-shot,acc_norm,0.3194444444444444,0.0389907368735733
mosaicml/mpt-7b-instruct,hendrycksTest-college_chemistry,5-shot,accuracy,0.35,0.0479372485441101
mosaicml/mpt-7b-instruct,hendrycksTest-college_chemistry,5-shot,acc_norm,0.35,0.0479372485441101
mosaicml/mpt-7b-instruct,hendrycksTest-college_computer_science,5-shot,accuracy,0.38,0.0487831731214563
mosaicml/mpt-7b-instruct,hendrycksTest-college_computer_science,5-shot,acc_norm,0.38,0.0487831731214563
mosaicml/mpt-7b-instruct,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
mosaicml/mpt-7b-instruct,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
mosaicml/mpt-7b-instruct,hendrycksTest-college_medicine,5-shot,accuracy,0.3063583815028902,0.0351494255126743
mosaicml/mpt-7b-instruct,hendrycksTest-college_medicine,5-shot,acc_norm,0.3063583815028902,0.0351494255126743
mosaicml/mpt-7b-instruct,hendrycksTest-college_physics,5-shot,accuracy,0.2352941176470588,0.0422077365917145
mosaicml/mpt-7b-instruct,hendrycksTest-college_physics,5-shot,acc_norm,0.2352941176470588,0.0422077365917145
mosaicml/mpt-7b-instruct,hendrycksTest-computer_security,5-shot,accuracy,0.37,0.048523658709391
mosaicml/mpt-7b-instruct,hendrycksTest-computer_security,5-shot,acc_norm,0.37,0.048523658709391
mosaicml/mpt-7b-instruct,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3489361702127659,0.0311585221313577
mosaicml/mpt-7b-instruct,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3489361702127659,0.0311585221313577
mosaicml/mpt-7b-instruct,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984367
mosaicml/mpt-7b-instruct,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.0409698513984367
mosaicml/mpt-7b-instruct,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3586206896551724,0.0399662957487671
mosaicml/mpt-7b-instruct,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3586206896551724,0.0399662957487671
mosaicml/mpt-7b-instruct,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2645502645502645,0.0227174678977086
mosaicml/mpt-7b-instruct,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2645502645502645,0.0227174678977086
mosaicml/mpt-7b-instruct,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.0356701667527686
mosaicml/mpt-7b-instruct,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.0356701667527686
mosaicml/mpt-7b-instruct,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.0482418151324421
mosaicml/mpt-7b-instruct,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.0482418151324421
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_biology,5-shot,accuracy,0.3806451612903225,0.027621717832907
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3806451612903225,0.027621717832907
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2955665024630542,0.0321049443375145
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2955665024630542,0.0321049443375145
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2909090909090909,0.0354656301962433
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2909090909090909,0.0354656301962433
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_geography,5-shot,accuracy,0.4444444444444444,0.0354029437709536
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_geography,5-shot,acc_norm,0.4444444444444444,0.0354029437709536
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3782383419689119,0.0349980727619333
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3782383419689119,0.0349980727619333
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3692307692307692,0.0244686152414789
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3692307692307692,0.0244686152414789
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2555555555555555,0.026593939101844
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2555555555555555,0.026593939101844
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.361344537815126,0.03120469122515
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.361344537815126,0.03120469122515
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_physics,5-shot,accuracy,0.3576158940397351,0.0391345343117725
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3576158940397351,0.0391345343117725
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3357798165137615,0.0202480813967529
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3357798165137615,0.0202480813967529
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3981481481481481,0.033384734032074
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3981481481481481,0.033384734032074
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2598039215686274,0.0307785546786932
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2573839662447257,0.0284588209914603
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2573839662447257,0.0284588209914603
mosaicml/mpt-7b-instruct,hendrycksTest-human_aging,5-shot,accuracy,0.358744394618834,0.0321907920041999
mosaicml/mpt-7b-instruct,hendrycksTest-human_aging,5-shot,acc_norm,0.358744394618834,0.0321907920041999
mosaicml/mpt-7b-instruct,hendrycksTest-human_sexuality,5-shot,accuracy,0.3893129770992366,0.0427648654281459
mosaicml/mpt-7b-instruct,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3893129770992366,0.0427648654281459
mosaicml/mpt-7b-instruct,hendrycksTest-international_law,5-shot,accuracy,0.3140495867768595,0.0423696475304101
mosaicml/mpt-7b-instruct,hendrycksTest-international_law,5-shot,acc_norm,0.3140495867768595,0.0423696475304101
mosaicml/mpt-7b-instruct,hendrycksTest-jurisprudence,5-shot,accuracy,0.3703703703703703,0.0466840803302493
mosaicml/mpt-7b-instruct,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3703703703703703,0.0466840803302493
mosaicml/mpt-7b-instruct,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3558282208588957,0.0376152138004673
mosaicml/mpt-7b-instruct,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3558282208588957,0.0376152138004673
mosaicml/mpt-7b-instruct,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.0428785875134045
mosaicml/mpt-7b-instruct,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.0428785875134045
mosaicml/mpt-7b-instruct,hendrycksTest-management,5-shot,accuracy,0.3689320388349514,0.0477761518115674
mosaicml/mpt-7b-instruct,hendrycksTest-management,5-shot,acc_norm,0.3689320388349514,0.0477761518115674
mosaicml/mpt-7b-instruct,hendrycksTest-marketing,5-shot,accuracy,0.3247863247863248,0.0306790227654988
mosaicml/mpt-7b-instruct,hendrycksTest-marketing,5-shot,acc_norm,0.3247863247863248,0.0306790227654988
mosaicml/mpt-7b-instruct,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.0451260859854212
mosaicml/mpt-7b-instruct,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.0451260859854212
mosaicml/mpt-7b-instruct,hendrycksTest-miscellaneous,5-shot,accuracy,0.3499361430395913,0.0170556797971504
mosaicml/mpt-7b-instruct,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3499361430395913,0.0170556797971504
mosaicml/mpt-7b-instruct,hendrycksTest-moral_disputes,5-shot,accuracy,0.3121387283236994,0.0249467922252723
mosaicml/mpt-7b-instruct,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3121387283236994,0.0249467922252723
mosaicml/mpt-7b-instruct,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
mosaicml/mpt-7b-instruct,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
mosaicml/mpt-7b-instruct,hendrycksTest-nutrition,5-shot,accuracy,0.3137254901960784,0.0265689210154571
mosaicml/mpt-7b-instruct,hendrycksTest-nutrition,5-shot,acc_norm,0.3137254901960784,0.0265689210154571
mosaicml/mpt-7b-instruct,hendrycksTest-philosophy,5-shot,accuracy,0.3279742765273312,0.0266644108869376
mosaicml/mpt-7b-instruct,hendrycksTest-philosophy,5-shot,acc_norm,0.3279742765273312,0.0266644108869376
mosaicml/mpt-7b-instruct,hendrycksTest-prehistory,5-shot,accuracy,0.3148148148148148,0.0258422487009021
mosaicml/mpt-7b-instruct,hendrycksTest-prehistory,5-shot,acc_norm,0.3148148148148148,0.0258422487009021
mosaicml/mpt-7b-instruct,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
mosaicml/mpt-7b-instruct,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
mosaicml/mpt-7b-instruct,hendrycksTest-professional_law,5-shot,accuracy,0.2783572359843546,0.0114469901973809
mosaicml/mpt-7b-instruct,hendrycksTest-professional_law,5-shot,acc_norm,0.2783572359843546,0.0114469901973809
mosaicml/mpt-7b-instruct,hendrycksTest-professional_medicine,5-shot,accuracy,0.2573529411764705,0.0265565194700415
mosaicml/mpt-7b-instruct,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2573529411764705,0.0265565194700415
mosaicml/mpt-7b-instruct,hendrycksTest-professional_psychology,5-shot,accuracy,0.3137254901960784,0.0187716838935281
mosaicml/mpt-7b-instruct,hendrycksTest-professional_psychology,5-shot,acc_norm,0.3137254901960784,0.0187716838935281
mosaicml/mpt-7b-instruct,hendrycksTest-public_relations,5-shot,accuracy,0.3363636363636363,0.045253935963025
mosaicml/mpt-7b-instruct,hendrycksTest-public_relations,5-shot,acc_norm,0.3363636363636363,0.045253935963025
mosaicml/mpt-7b-instruct,hendrycksTest-security_studies,5-shot,accuracy,0.4163265306122449,0.0315578281655616
mosaicml/mpt-7b-instruct,hendrycksTest-security_studies,5-shot,acc_norm,0.4163265306122449,0.0315578281655616
mosaicml/mpt-7b-instruct,hendrycksTest-sociology,5-shot,accuracy,0.3383084577114428,0.0334556307033919
mosaicml/mpt-7b-instruct,hendrycksTest-sociology,5-shot,acc_norm,0.3383084577114428,0.0334556307033919
mosaicml/mpt-7b-instruct,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.42,0.0496044963748858
mosaicml/mpt-7b-instruct,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.42,0.0496044963748858
mosaicml/mpt-7b-instruct,hendrycksTest-virology,5-shot,accuracy,0.3614457831325301,0.0374005938202932
mosaicml/mpt-7b-instruct,hendrycksTest-virology,5-shot,acc_norm,0.3614457831325301,0.0374005938202932
mosaicml/mpt-7b-instruct,hendrycksTest-world_religions,5-shot,accuracy,0.2573099415204678,0.0335279984416186
mosaicml/mpt-7b-instruct,hendrycksTest-world_religions,5-shot,acc_norm,0.2573099415204678,0.0335279984416186
mosaicml/mpt-7b-instruct,truthfulqa:mc,0-shot,mc1,0.2288861689106487,0.014706994909055
mosaicml/mpt-7b-instruct,truthfulqa:mc,0-shot,mc2,0.3508407855782673,0.0137711221713866
gpt2-medium,arc:challenge,25-shot,accuracy,0.2209897610921501,0.0121249292068182
gpt2-medium,arc:challenge,25-shot,acc_norm,0.2704778156996587,0.0129809545476595
gpt2-medium,hellaswag,10-shot,accuracy,0.3306114319856602,0.0046947189182257
gpt2-medium,hellaswag,10-shot,acc_norm,0.4017128062139016,0.0048924253563757
gpt2-medium,hendrycksTest-abstract_algebra,5-shot,accuracy,0.26,0.0440844002276808
gpt2-medium,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.26,0.0440844002276808
gpt2-medium,hendrycksTest-anatomy,5-shot,accuracy,0.2444444444444444,0.0371253783361486
gpt2-medium,hendrycksTest-anatomy,5-shot,acc_norm,0.2444444444444444,0.0371253783361486
gpt2-medium,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.0373852067611966
gpt2-medium,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.0373852067611966
gpt2-medium,hendrycksTest-business_ethics,5-shot,accuracy,0.15,0.0358870281282636
gpt2-medium,hendrycksTest-business_ethics,5-shot,acc_norm,0.15,0.0358870281282636
gpt2-medium,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.0282542003444386
gpt2-medium,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.0282542003444386
gpt2-medium,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.0371617743756601
gpt2-medium,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.0371617743756601
gpt2-medium,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.0464823198711731
gpt2-medium,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.0464823198711731
gpt2-medium,hendrycksTest-college_computer_science,5-shot,accuracy,0.36,0.0482418151324421
gpt2-medium,hendrycksTest-college_computer_science,5-shot,acc_norm,0.36,0.0482418151324421
gpt2-medium,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.0416333199893226
gpt2-medium,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.0416333199893226
gpt2-medium,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225943
gpt2-medium,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.0336876293225943
gpt2-medium,hendrycksTest-college_physics,5-shot,accuracy,0.1568627450980392,0.0361866481993624
gpt2-medium,hendrycksTest-college_physics,5-shot,acc_norm,0.1568627450980392,0.0361866481993624
gpt2-medium,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.042295258468165
gpt2-medium,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.042295258468165
gpt2-medium,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2936170212765957,0.0297716427124912
gpt2-medium,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2936170212765957,0.0297716427124912
gpt2-medium,hendrycksTest-econometrics,5-shot,accuracy,0.3070175438596491,0.0433913832257986
gpt2-medium,hendrycksTest-econometrics,5-shot,acc_norm,0.3070175438596491,0.0433913832257986
gpt2-medium,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
gpt2-medium,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
gpt2-medium,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
gpt2-medium,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
gpt2-medium,hendrycksTest-formal_logic,5-shot,accuracy,0.1587301587301587,0.0326845401301174
gpt2-medium,hendrycksTest-formal_logic,5-shot,acc_norm,0.1587301587301587,0.0326845401301174
gpt2-medium,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.0464823198711731
gpt2-medium,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.0464823198711731
gpt2-medium,hendrycksTest-high_school_biology,5-shot,accuracy,0.2419354838709677,0.0243625996930311
gpt2-medium,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2419354838709677,0.0243625996930311
gpt2-medium,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3054187192118227,0.032406615658684
gpt2-medium,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3054187192118227,0.032406615658684
gpt2-medium,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.0451260859854212
gpt2-medium,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
gpt2-medium,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2666666666666666,0.0345313180188541
gpt2-medium,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2666666666666666,0.0345313180188541
gpt2-medium,hendrycksTest-high_school_geography,5-shot,accuracy,0.3585858585858585,0.0341690364039152
gpt2-medium,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3585858585858585,0.0341690364039152
gpt2-medium,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3160621761658031,0.0335539736968617
gpt2-medium,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3160621761658031,0.0335539736968617
gpt2-medium,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3076923076923077,0.0234009289183105
gpt2-medium,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3076923076923077,0.0234009289183105
gpt2-medium,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
gpt2-medium,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2629629629629629,0.0268420578738337
gpt2-medium,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3025210084033613,0.0298379623882919
gpt2-medium,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3025210084033613,0.0298379623882919
gpt2-medium,hendrycksTest-high_school_physics,5-shot,accuracy,0.3443708609271523,0.0387968702407332
gpt2-medium,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3443708609271523,0.0387968702407332
gpt2-medium,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3357798165137615,0.0202480813967529
gpt2-medium,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3357798165137615,0.0202480813967529
gpt2-medium,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4074074074074074,0.0335099160469604
gpt2-medium,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4074074074074074,0.0335099160469604
gpt2-medium,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2254901960784313,0.0293311622942517
gpt2-medium,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2254901960784313,0.0293311622942517
gpt2-medium,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2109704641350211,0.0265583725026619
gpt2-medium,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2109704641350211,0.0265583725026619
gpt2-medium,hendrycksTest-human_aging,5-shot,accuracy,0.2062780269058296,0.0271571504795638
gpt2-medium,hendrycksTest-human_aging,5-shot,acc_norm,0.2062780269058296,0.0271571504795638
gpt2-medium,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
gpt2-medium,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
gpt2-medium,hendrycksTest-international_law,5-shot,accuracy,0.1652892561983471,0.0339078061297277
gpt2-medium,hendrycksTest-international_law,5-shot,acc_norm,0.1652892561983471,0.0339078061297277
gpt2-medium,hendrycksTest-jurisprudence,5-shot,accuracy,0.2037037037037037,0.0389354251882484
gpt2-medium,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2037037037037037,0.0389354251882484
gpt2-medium,hendrycksTest-logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
gpt2-medium,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.263803680981595,0.0346241993161562
gpt2-medium,hendrycksTest-machine_learning,5-shot,accuracy,0.1964285714285714,0.0377097004934701
gpt2-medium,hendrycksTest-machine_learning,5-shot,acc_norm,0.1964285714285714,0.0377097004934701
gpt2-medium,hendrycksTest-management,5-shot,accuracy,0.3592233009708738,0.0475045839904169
gpt2-medium,hendrycksTest-management,5-shot,acc_norm,0.3592233009708738,0.0475045839904169
gpt2-medium,hendrycksTest-marketing,5-shot,accuracy,0.2051282051282051,0.0264535080540403
gpt2-medium,hendrycksTest-marketing,5-shot,acc_norm,0.2051282051282051,0.0264535080540403
gpt2-medium,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.0451260859854212
gpt2-medium,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.0451260859854212
gpt2-medium,hendrycksTest-miscellaneous,5-shot,accuracy,0.2401021711366538,0.0152746852137341
gpt2-medium,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2401021711366538,0.0152746852137341
gpt2-medium,hendrycksTest-moral_disputes,5-shot,accuracy,0.2427745664739884,0.0230836585869842
gpt2-medium,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2427745664739884,0.0230836585869842
gpt2-medium,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
gpt2-medium,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
gpt2-medium,hendrycksTest-nutrition,5-shot,accuracy,0.2679738562091503,0.0253606037962425
gpt2-medium,hendrycksTest-nutrition,5-shot,acc_norm,0.2679738562091503,0.0253606037962425
gpt2-medium,hendrycksTest-philosophy,5-shot,accuracy,0.2411575562700964,0.0242965940347634
gpt2-medium,hendrycksTest-philosophy,5-shot,acc_norm,0.2411575562700964,0.0242965940347634
gpt2-medium,hendrycksTest-prehistory,5-shot,accuracy,0.2191358024691358,0.0230167056402621
gpt2-medium,hendrycksTest-prehistory,5-shot,acc_norm,0.2191358024691358,0.0230167056402621
gpt2-medium,hendrycksTest-professional_accounting,5-shot,accuracy,0.2588652482269503,0.0261295725271808
gpt2-medium,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2588652482269503,0.0261295725271808
gpt2-medium,hendrycksTest-professional_law,5-shot,accuracy,0.2451108213820078,0.0109863078700455
gpt2-medium,hendrycksTest-professional_law,5-shot,acc_norm,0.2451108213820078,0.0109863078700455
gpt2-medium,hendrycksTest-professional_medicine,5-shot,accuracy,0.4522058823529412,0.0302337585515964
gpt2-medium,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4522058823529412,0.0302337585515964
gpt2-medium,hendrycksTest-professional_psychology,5-shot,accuracy,0.2352941176470588,0.0171605872350463
gpt2-medium,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2352941176470588,0.0171605872350463
gpt2-medium,hendrycksTest-public_relations,5-shot,accuracy,0.2636363636363636,0.0422022469297198
gpt2-medium,hendrycksTest-public_relations,5-shot,acc_norm,0.2636363636363636,0.0422022469297198
gpt2-medium,hendrycksTest-security_studies,5-shot,accuracy,0.3346938775510204,0.0302092352262423
gpt2-medium,hendrycksTest-security_studies,5-shot,acc_norm,0.3346938775510204,0.0302092352262423
gpt2-medium,hendrycksTest-sociology,5-shot,accuracy,0.2288557213930348,0.0297052840567724
gpt2-medium,hendrycksTest-sociology,5-shot,acc_norm,0.2288557213930348,0.0297052840567724
gpt2-medium,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.0451260859854212
gpt2-medium,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.0451260859854212
gpt2-medium,hendrycksTest-virology,5-shot,accuracy,0.2048192771084337,0.0314178429166392
gpt2-medium,hendrycksTest-virology,5-shot,acc_norm,0.2048192771084337,0.0314178429166392
gpt2-medium,hendrycksTest-world_religions,5-shot,accuracy,0.2514619883040935,0.0332750442384684
gpt2-medium,hendrycksTest-world_religions,5-shot,acc_norm,0.2514619883040935,0.0332750442384684
gpt2-medium,truthfulqa:mc,0-shot,mc1,0.2252141982864137,0.0146232407680235
gpt2-medium,truthfulqa:mc,0-shot,mc2,0.4075602335796246,0.0145967631587624
EleutherAI/pythia-410M,mmlu_world_religions,0-shot,accuracy,0.2748538011695906,0.0342404292469158
EleutherAI/pythia-410M,mmlu_formal_logic,0-shot,accuracy,0.2936507936507936,0.0407352432214712
EleutherAI/pythia-410M,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.0230167056402621
EleutherAI/pythia-410M,mmlu_moral_scenarios,0-shot,accuracy,0.2525139664804469,0.0145303302014686
EleutherAI/pythia-410M,mmlu_high_school_world_history,0-shot,accuracy,0.2109704641350211,0.0265583725026619
EleutherAI/pythia-410M,mmlu_moral_disputes,0-shot,accuracy,0.2630057803468208,0.0237030995252581
EleutherAI/pythia-410M,mmlu_professional_law,0-shot,accuracy,0.241851368970013,0.010936550813827
EleutherAI/pythia-410M,mmlu_logical_fallacies,0-shot,accuracy,0.2883435582822086,0.0355903953161734
EleutherAI/pythia-410M,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.0309645179269234
EleutherAI/pythia-410M,mmlu_philosophy,0-shot,accuracy,0.2411575562700964,0.0242965940347634
EleutherAI/pythia-410M,mmlu_jurisprudence,0-shot,accuracy,0.2037037037037037,0.0389354251882484
EleutherAI/pythia-410M,mmlu_international_law,0-shot,accuracy,0.4049586776859504,0.0448113775594246
EleutherAI/pythia-410M,mmlu_high_school_european_history,0-shot,accuracy,0.2424242424242424,0.0334640988105595
EleutherAI/pythia-410M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2279792746113989,0.0302769099451782
EleutherAI/pythia-410M,mmlu_high_school_microeconomics,0-shot,accuracy,0.2394957983193277,0.0277220654933612
EleutherAI/pythia-410M,mmlu_high_school_geography,0-shot,accuracy,0.303030303030303,0.0327428791402686
EleutherAI/pythia-410M,mmlu_high_school_psychology,0-shot,accuracy,0.3211009174311927,0.0200181497727337
EleutherAI/pythia-410M,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-410M,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-410M,mmlu_sociology,0-shot,accuracy,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-410M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3487179487179487,0.0241627802840177
EleutherAI/pythia-410M,mmlu_security_studies,0-shot,accuracy,0.4040816326530612,0.0314147080258658
EleutherAI/pythia-410M,mmlu_professional_psychology,0-shot,accuracy,0.2745098039215686,0.0180540274588151
EleutherAI/pythia-410M,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
EleutherAI/pythia-410M,mmlu_econometrics,0-shot,accuracy,0.2280701754385964,0.0394715278266941
EleutherAI/pythia-410M,mmlu_miscellaneous,0-shot,accuracy,0.2592592592592592,0.0156710060093395
EleutherAI/pythia-410M,mmlu_marketing,0-shot,accuracy,0.1923076923076923,0.0258192332564837
EleutherAI/pythia-410M,mmlu_management,0-shot,accuracy,0.2621359223300971,0.0435463107726059
EleutherAI/pythia-410M,mmlu_nutrition,0-shot,accuracy,0.261437908496732,0.0251609982142924
EleutherAI/pythia-410M,mmlu_medical_genetics,0-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-410M,mmlu_human_aging,0-shot,accuracy,0.2286995515695067,0.0281882400469291
EleutherAI/pythia-410M,mmlu_professional_medicine,0-shot,accuracy,0.4448529411764705,0.0301875320603293
EleutherAI/pythia-410M,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.0312651120617304
EleutherAI/pythia-410M,mmlu_business_ethics,0-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-410M,mmlu_clinical_knowledge,0-shot,accuracy,0.2566037735849056,0.0268806478890519
EleutherAI/pythia-410M,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-410M,mmlu_virology,0-shot,accuracy,0.2650602409638554,0.0343602403794496
EleutherAI/pythia-410M,mmlu_professional_accounting,0-shot,accuracy,0.2375886524822695,0.0253895125527299
EleutherAI/pythia-410M,mmlu_college_physics,0-shot,accuracy,0.2450980392156862,0.0428010583736439
EleutherAI/pythia-410M,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.0371018572611999
EleutherAI/pythia-410M,mmlu_high_school_biology,0-shot,accuracy,0.2903225806451613,0.0258221061194159
EleutherAI/pythia-410M,mmlu_college_biology,0-shot,accuracy,0.2777777777777778,0.0374555479146245
EleutherAI/pythia-410M,mmlu_anatomy,0-shot,accuracy,0.2888888888888888,0.0391545063041425
EleutherAI/pythia-410M,mmlu_college_chemistry,0-shot,accuracy,0.42,0.0496044963748858
EleutherAI/pythia-410M,mmlu_computer_security,0-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-410M,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
EleutherAI/pythia-410M,mmlu_astronomy,0-shot,accuracy,0.2171052631578947,0.0335504530488292
EleutherAI/pythia-410M,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215
EleutherAI/pythia-410M,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.0295131966255393
EleutherAI/pythia-410M,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-410M,mmlu_high_school_computer_science,0-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-410M,mmlu_machine_learning,0-shot,accuracy,0.2142857142857142,0.0389464112004479
EleutherAI/pythia-410M,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.0310898260029375
EleutherAI/pythia-410M,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410M,mmlu_elementary_mathematics,0-shot,accuracy,0.2513227513227513,0.0223404823396438
EleutherAI/pythia-410M,mmlu_electrical_engineering,0-shot,accuracy,0.2206896551724138,0.0345593020192481
EleutherAI/pythia-410M,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
EleutherAI/pythia-410M,arc_challenge,25-shot,accuracy,0.2269624573378839,0.0122404915361328
EleutherAI/pythia-410M,arc_challenge,25-shot,acc_norm,0.2704778156996587,0.0129809545476595
EleutherAI/pythia-410M,hellaswag,10-shot,acc_norm,0.4110734913363871,0.0049102296432627
EleutherAI/pythia-410M,truthfulqa_mc2,0-shot,accuracy,0.4123790335693685,0.0145643736691822
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,bleu_max,16.26090055559814,0.5405929938913795
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,bleu_acc,0.3855569155446756,0.0170388390105916
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,bleu_diff,-3.0508334325257387,0.5347158599372487
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge1_max,36.94365940903368,0.8170526245474131
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge1_acc,0.2949816401468788,0.0159644009655896
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge1_diff,-7.001303766637389,0.7647828158374674
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge2_max,19.221965303272068,0.8332853887183771
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge2_acc,0.1811505507955936,0.0134826971878178
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge2_diff,-7.513009385966499,0.7649345167230245
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rougeL_max,34.22730634914542,0.7952582275069422
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rougeL_acc,0.2925336597307221,0.0159255974452861
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rougeL_diff,-6.85285056456493,0.7573700129593819
EleutherAI/pythia-410M,truthfulqa_mc1,0-shot,accuracy,0.2362301101591187,0.014869755015871
EleutherAI/pythia-410M,winogrande,5-shot,accuracy,0.5232833464877664,0.0140372413095736
EleutherAI/pythia-410M,gsm8k,5-shot,accuracy,0.0181956027293404,0.0036816118940738
Salesforce/codegen-2B-mono,minerva_math_precalc,5-shot,accuracy,0.0146520146520146,0.0051468941589821
Salesforce/codegen-2B-mono,minerva_math_prealgebra,5-shot,accuracy,0.0321469575200918,0.0059801905403747
Salesforce/codegen-2B-mono,minerva_math_num_theory,5-shot,accuracy,0.0314814814814814,0.0075212004387168
Salesforce/codegen-2B-mono,minerva_math_intermediate_algebra,5-shot,accuracy,0.0166112956810631,0.0042556028721946
Salesforce/codegen-2B-mono,minerva_math_geometry,5-shot,accuracy,0.0083507306889352,0.0041622421102958
Salesforce/codegen-2B-mono,minerva_math_counting_and_prob,5-shot,accuracy,0.010548523206751,0.0046974537353761
Salesforce/codegen-2B-mono,minerva_math_algebra,5-shot,accuracy,0.0151642796967144,0.0035485460431325
Salesforce/codegen-2B-mono,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_3da,5-shot,accuracy,0.0085,0.0020532859010609
Salesforce/codegen-2B-mono,arithmetic_3ds,5-shot,accuracy,0.0075,0.0019296986470519
Salesforce/codegen-2B-mono,arithmetic_4da,5-shot,accuracy,0.001,0.0007069298939339
Salesforce/codegen-2B-mono,arithmetic_2ds,5-shot,accuracy,0.108,0.0069420527258169
Salesforce/codegen-2B-mono,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_1dc,5-shot,accuracy,0.0775,0.0059803643182242
Salesforce/codegen-2B-mono,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_2dm,5-shot,accuracy,0.054,0.0050551733292434
Salesforce/codegen-2B-mono,arithmetic_2da,5-shot,accuracy,0.104,0.0068275403809738
Salesforce/codegen-2B-mono,gsm8k_cot,5-shot,accuracy,0.0242608036391205,0.0042380079000013
Salesforce/codegen-2B-mono,gsm8k,5-shot,accuracy,0.0257771038665655,0.0043650429536218
Salesforce/codegen-2B-mono,anli_r2,0-shot,brier_score,0.7685841435173599,
Salesforce/codegen-2B-mono,anli_r3,0-shot,brier_score,0.8015102484190165,
Salesforce/codegen-2B-mono,anli_r1,0-shot,brier_score,0.8031169120598974,
Salesforce/codegen-2B-mono,xnli_eu,0-shot,brier_score,1.1017671299956897,
Salesforce/codegen-2B-mono,xnli_vi,0-shot,brier_score,1.131535014847705,
Salesforce/codegen-2B-mono,xnli_ru,0-shot,brier_score,0.8173338590693098,
Salesforce/codegen-2B-mono,xnli_zh,0-shot,brier_score,1.069665725041166,
Salesforce/codegen-2B-mono,xnli_tr,0-shot,brier_score,0.903173033726133,
Salesforce/codegen-2B-mono,xnli_fr,0-shot,brier_score,1.073901212241402,
Salesforce/codegen-2B-mono,xnli_en,0-shot,brier_score,0.7006196663679488,
Salesforce/codegen-2B-mono,xnli_ur,0-shot,brier_score,1.2038380352221547,
Salesforce/codegen-2B-mono,xnli_ar,0-shot,brier_score,0.9165665499376836,
Salesforce/codegen-2B-mono,xnli_de,0-shot,brier_score,1.0937779753771633,
Salesforce/codegen-2B-mono,xnli_hi,0-shot,brier_score,1.0356865346007569,
Salesforce/codegen-2B-mono,xnli_es,0-shot,brier_score,0.952199600899833,
Salesforce/codegen-2B-mono,xnli_bg,0-shot,brier_score,0.9337331322076656,
Salesforce/codegen-2B-mono,xnli_sw,0-shot,brier_score,0.8983199952247677,
Salesforce/codegen-2B-mono,xnli_el,0-shot,brier_score,0.831761035039731,
Salesforce/codegen-2B-mono,xnli_th,0-shot,brier_score,1.1558354492975338,
Salesforce/codegen-2B-mono,logiqa2,0-shot,brier_score,1.1424943532533816,
Salesforce/codegen-2B-mono,mathqa,0-shot,brier_score,0.983335118929985,
Salesforce/codegen-2B-mono,lambada_standard,0-shot,perplexity,127.0654873504989,5.294900704959425
Salesforce/codegen-2B-mono,lambada_standard,0-shot,accuracy,0.208810401707743,0.0056627634693618
Salesforce/codegen-2B-mono,lambada_openai,0-shot,perplexity,120.31307761952348,5.287438923428367
Salesforce/codegen-2B-mono,lambada_openai,0-shot,accuracy,0.2109450805356103,0.0056839518407047
Salesforce/codegen-2B-mono,mmlu_world_religions,0-shot,accuracy,0.2631578947368421,0.0337731025220919
Salesforce/codegen-2B-mono,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.0397015827323517
Salesforce/codegen-2B-mono,mmlu_prehistory,0-shot,accuracy,0.2037037037037037,0.0224096745473041
Salesforce/codegen-2B-mono,mmlu_moral_scenarios,0-shot,accuracy,0.2737430167597765,0.0149124130963724
Salesforce/codegen-2B-mono,mmlu_high_school_world_history,0-shot,accuracy,0.240506329113924,0.0278207819811496
Salesforce/codegen-2B-mono,mmlu_moral_disputes,0-shot,accuracy,0.2572254335260115,0.0235329254310442
Salesforce/codegen-2B-mono,mmlu_professional_law,0-shot,accuracy,0.2464146023468057,0.0110059713999272
Salesforce/codegen-2B-mono,mmlu_logical_fallacies,0-shot,accuracy,0.2269938650306748,0.0329109957861576
Salesforce/codegen-2B-mono,mmlu_high_school_us_history,0-shot,accuracy,0.2401960784313725,0.0299837330559136
Salesforce/codegen-2B-mono,mmlu_philosophy,0-shot,accuracy,0.1929260450160771,0.0224115167809113
Salesforce/codegen-2B-mono,mmlu_jurisprudence,0-shot,accuracy,0.2314814814814814,0.0407749470925262
Salesforce/codegen-2B-mono,mmlu_international_law,0-shot,accuracy,0.2479338842975206,0.039418975265163
Salesforce/codegen-2B-mono,mmlu_high_school_european_history,0-shot,accuracy,0.2484848484848484,0.033744026441394
Salesforce/codegen-2B-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2072538860103626,0.0292528232918036
Salesforce/codegen-2B-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.2142857142857142,0.0266535315967154
Salesforce/codegen-2B-mono,mmlu_high_school_geography,0-shot,accuracy,0.1919191919191919,0.028057791672989
Salesforce/codegen-2B-mono,mmlu_high_school_psychology,0-shot,accuracy,0.2110091743119266,0.0174939224041126
Salesforce/codegen-2B-mono,mmlu_public_relations,0-shot,accuracy,0.2,0.038313051408846
Salesforce/codegen-2B-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-2B-mono,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
Salesforce/codegen-2B-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2205128205128205,0.0210206726808279
Salesforce/codegen-2B-mono,mmlu_security_studies,0-shot,accuracy,0.1877551020408163,0.0250002560395462
Salesforce/codegen-2B-mono,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
Salesforce/codegen-2B-mono,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.0391534540884783
Salesforce/codegen-2B-mono,mmlu_econometrics,0-shot,accuracy,0.2105263157894736,0.0383515395439941
Salesforce/codegen-2B-mono,mmlu_miscellaneous,0-shot,accuracy,0.2222222222222222,0.0148668216647095
Salesforce/codegen-2B-mono,mmlu_marketing,0-shot,accuracy,0.2991452991452991,0.0299969518583494
Salesforce/codegen-2B-mono,mmlu_management,0-shot,accuracy,0.2233009708737864,0.0412355318989143
Salesforce/codegen-2B-mono,mmlu_nutrition,0-shot,accuracy,0.2287581699346405,0.0240510297399122
Salesforce/codegen-2B-mono,mmlu_medical_genetics,0-shot,accuracy,0.32,0.046882617226215
Salesforce/codegen-2B-mono,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.0314938467099413
Salesforce/codegen-2B-mono,mmlu_professional_medicine,0-shot,accuracy,0.2463235294117647,0.02617343857052
Salesforce/codegen-2B-mono,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
Salesforce/codegen-2B-mono,mmlu_business_ethics,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-2B-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.2,0.0246182981958665
Salesforce/codegen-2B-mono,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
Salesforce/codegen-2B-mono,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.0352948680151111
Salesforce/codegen-2B-mono,mmlu_professional_accounting,0-shot,accuracy,0.202127659574468,0.0239566682378502
Salesforce/codegen-2B-mono,mmlu_college_physics,0-shot,accuracy,0.2450980392156862,0.0428010583736439
Salesforce/codegen-2B-mono,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.0368488152138902
Salesforce/codegen-2B-mono,mmlu_high_school_biology,0-shot,accuracy,0.2193548387096774,0.0235407993587232
Salesforce/codegen-2B-mono,mmlu_college_biology,0-shot,accuracy,0.2361111111111111,0.0355144661081082
Salesforce/codegen-2B-mono,mmlu_anatomy,0-shot,accuracy,0.2074074074074074,0.0350255317067831
Salesforce/codegen-2B-mono,mmlu_college_chemistry,0-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-2B-mono,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-2B-mono,mmlu_college_computer_science,0-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-2B-mono,mmlu_astronomy,0-shot,accuracy,0.2302631578947368,0.0342605942440316
Salesforce/codegen-2B-mono,mmlu_college_mathematics,0-shot,accuracy,0.19,0.0394277244403662
Salesforce/codegen-2B-mono,mmlu_conceptual_physics,0-shot,accuracy,0.251063829787234,0.0283469637771624
Salesforce/codegen-2B-mono,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
Salesforce/codegen-2B-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-2B-mono,mmlu_machine_learning,0-shot,accuracy,0.2678571428571428,0.0420327729146776
Salesforce/codegen-2B-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.1724137931034483,0.0265776721830365
Salesforce/codegen-2B-mono,mmlu_high_school_statistics,0-shot,accuracy,0.1944444444444444,0.0269914545020367
Salesforce/codegen-2B-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.2275132275132275,0.0215912694078237
Salesforce/codegen-2B-mono,mmlu_electrical_engineering,0-shot,accuracy,0.2620689655172414,0.0366466633722525
Salesforce/codegen-2B-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.2259259259259259,0.0254975326396095
Salesforce/codegen-2B-mono,arc_challenge,25-shot,accuracy,0.1919795221843003,0.0115095989065981
Salesforce/codegen-2B-mono,arc_challenge,25-shot,acc_norm,0.2329351535836177,0.0123525070426173
Salesforce/codegen-2B-mono,hellaswag,10-shot,accuracy,0.3044214299940251,0.0045922151182952
Salesforce/codegen-2B-mono,hellaswag,10-shot,acc_norm,0.3430591515634336,0.0047376083401634
Salesforce/codegen-2B-mono,truthfulqa_mc2,0-shot,accuracy,0.4489096881754911,0.0154241287931977
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,bleu_max,16.04577191970097,0.5552162891689132
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,bleu_acc,0.3720930232558139,0.016921090118814
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,bleu_diff,-2.0090839666050755,0.52202727913542
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge1_max,34.93723491168912,0.8615146874033828
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge1_acc,0.2962056303549572,0.0159835951018113
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge1_diff,-5.533004957393886,0.7883741299337192
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge2_max,18.30765799283543,0.8421305079189312
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge2_acc,0.1652386780905752,0.0130014543564992
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge2_diff,-5.205693990711149,0.7125780222110801
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rougeL_max,32.32955226819275,0.8369096403695482
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rougeL_acc,0.3047735618115055,0.0161141241568824
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rougeL_diff,-5.442950391491437,0.7788716099947031
Salesforce/codegen-2B-mono,truthfulqa_mc1,0-shot,accuracy,0.2582619339045288,0.0153218216884761
Salesforce/codegen-2B-mono,winogrande,5-shot,accuracy,0.5122336227308603,0.0140482788204056
facebook/xglm-7.5B,drop,3-shot,accuracy,0.1390520134228187,0.0035433720039612
facebook/xglm-7.5B,drop,3-shot,f1,0.1858085151006711,0.0037071149655913
facebook/xglm-7.5B,arc:challenge,25-shot,accuracy,0.3071672354948805,0.0134810340549809
facebook/xglm-7.5B,arc:challenge,25-shot,acc_norm,0.3412969283276451,0.0138558312874977
facebook/xglm-7.5B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
facebook/xglm-7.5B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
facebook/xglm-7.5B,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.0374985070917402
facebook/xglm-7.5B,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.0374985070917402
facebook/xglm-7.5B,hendrycksTest-astronomy,5-shot,accuracy,0.2236842105263158,0.033911609343436
facebook/xglm-7.5B,hendrycksTest-astronomy,5-shot,acc_norm,0.2236842105263158,0.033911609343436
facebook/xglm-7.5B,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.0440844002276807
facebook/xglm-7.5B,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.0440844002276807
facebook/xglm-7.5B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.0282542003444386
facebook/xglm-7.5B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.0282542003444386
facebook/xglm-7.5B,hendrycksTest-college_biology,5-shot,accuracy,0.2986111111111111,0.0382705235795075
facebook/xglm-7.5B,hendrycksTest-college_biology,5-shot,acc_norm,0.2986111111111111,0.0382705235795075
facebook/xglm-7.5B,hendrycksTest-college_chemistry,5-shot,accuracy,0.27,0.0446196043338474
facebook/xglm-7.5B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.27,0.0446196043338474
facebook/xglm-7.5B,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-7.5B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
facebook/xglm-7.5B,hendrycksTest-college_mathematics,5-shot,accuracy,0.25,0.0435194139889244
facebook/xglm-7.5B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.25,0.0435194139889244
facebook/xglm-7.5B,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/xglm-7.5B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
facebook/xglm-7.5B,hendrycksTest-college_physics,5-shot,accuracy,0.196078431372549,0.0395058186117996
facebook/xglm-7.5B,hendrycksTest-college_physics,5-shot,acc_norm,0.196078431372549,0.0395058186117996
facebook/xglm-7.5B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-7.5B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
facebook/xglm-7.5B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2893617021276595,0.0296440065770096
facebook/xglm-7.5B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2893617021276595,0.0296440065770096
facebook/xglm-7.5B,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984366
facebook/xglm-7.5B,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.0409698513984366
facebook/xglm-7.5B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2068965517241379,0.0337567244956055
facebook/xglm-7.5B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2068965517241379,0.0337567244956055
facebook/xglm-7.5B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2671957671957672,0.0227896731457765
facebook/xglm-7.5B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2671957671957672,0.0227896731457765
facebook/xglm-7.5B,hendrycksTest-formal_logic,5-shot,accuracy,0.3174603174603174,0.0416345303130285
facebook/xglm-7.5B,hendrycksTest-formal_logic,5-shot,acc_norm,0.3174603174603174,0.0416345303130285
facebook/xglm-7.5B,hendrycksTest-global_facts,5-shot,accuracy,0.29,0.0456048021572068
facebook/xglm-7.5B,hendrycksTest-global_facts,5-shot,acc_norm,0.29,0.0456048021572068
facebook/xglm-7.5B,hendrycksTest-high_school_biology,5-shot,accuracy,0.332258064516129,0.0267955608481227
facebook/xglm-7.5B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.332258064516129,0.0267955608481227
facebook/xglm-7.5B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3103448275862069,0.032550867699701
facebook/xglm-7.5B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3103448275862069,0.032550867699701
facebook/xglm-7.5B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.0451260859854212
facebook/xglm-7.5B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
facebook/xglm-7.5B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2787878787878788,0.0350143870629678
facebook/xglm-7.5B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2787878787878788,0.0350143870629678
facebook/xglm-7.5B,hendrycksTest-high_school_geography,5-shot,accuracy,0.3585858585858585,0.0341690364039152
facebook/xglm-7.5B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3585858585858585,0.0341690364039152
facebook/xglm-7.5B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3626943005181347,0.0346971379170437
facebook/xglm-7.5B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3626943005181347,0.0346971379170437
facebook/xglm-7.5B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3564102564102564,0.0242831405294672
facebook/xglm-7.5B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3564102564102564,0.0242831405294672
facebook/xglm-7.5B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2481481481481481,0.0263357394040558
facebook/xglm-7.5B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2481481481481481,0.0263357394040558
facebook/xglm-7.5B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3319327731092437,0.0305886970137836
facebook/xglm-7.5B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3319327731092437,0.0305886970137836
facebook/xglm-7.5B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2980132450331126,0.0373453567678719
facebook/xglm-7.5B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2980132450331126,0.0373453567678719
facebook/xglm-7.5B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2770642201834862,0.0191884825901695
facebook/xglm-7.5B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2770642201834862,0.0191884825901695
facebook/xglm-7.5B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4212962962962963,0.0336746213889607
facebook/xglm-7.5B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4212962962962963,0.0336746213889607
facebook/xglm-7.5B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2352941176470588,0.0297717752281456
facebook/xglm-7.5B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2352941176470588,0.0297717752281456
facebook/xglm-7.5B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2573839662447257,0.0284588209914602
facebook/xglm-7.5B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2573839662447257,0.0284588209914602
facebook/xglm-7.5B,hendrycksTest-human_aging,5-shot,accuracy,0.3452914798206278,0.0319110019283579
facebook/xglm-7.5B,hendrycksTest-human_aging,5-shot,acc_norm,0.3452914798206278,0.0319110019283579
facebook/xglm-7.5B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559692
facebook/xglm-7.5B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559692
facebook/xglm-7.5B,hendrycksTest-international_law,5-shot,accuracy,0.3636363636363636,0.0439132628672407
facebook/xglm-7.5B,hendrycksTest-international_law,5-shot,acc_norm,0.3636363636363636,0.0439132628672407
facebook/xglm-7.5B,hendrycksTest-jurisprudence,5-shot,accuracy,0.2129629629629629,0.0395783547198098
facebook/xglm-7.5B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2129629629629629,0.0395783547198098
facebook/xglm-7.5B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2576687116564417,0.0343615082784691
facebook/xglm-7.5B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2576687116564417,0.0343615082784691
facebook/xglm-7.5B,hendrycksTest-machine_learning,5-shot,accuracy,0.2232142857142857,0.0395230196770251
facebook/xglm-7.5B,hendrycksTest-machine_learning,5-shot,acc_norm,0.2232142857142857,0.0395230196770251
facebook/xglm-7.5B,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.0435463107726059
facebook/xglm-7.5B,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.0435463107726059
facebook/xglm-7.5B,hendrycksTest-marketing,5-shot,accuracy,0.1794871794871795,0.0251409359503354
facebook/xglm-7.5B,hendrycksTest-marketing,5-shot,acc_norm,0.1794871794871795,0.0251409359503354
facebook/xglm-7.5B,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.0464823198711731
facebook/xglm-7.5B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.0464823198711731
facebook/xglm-7.5B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2554278416347382,0.0155949553844557
facebook/xglm-7.5B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2554278416347382,0.0155949553844557
facebook/xglm-7.5B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2312138728323699,0.0226986571678557
facebook/xglm-7.5B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2312138728323699,0.0226986571678557
facebook/xglm-7.5B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/xglm-7.5B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
facebook/xglm-7.5B,hendrycksTest-nutrition,5-shot,accuracy,0.2777777777777778,0.0256468630971378
facebook/xglm-7.5B,hendrycksTest-nutrition,5-shot,acc_norm,0.2777777777777778,0.0256468630971378
facebook/xglm-7.5B,hendrycksTest-philosophy,5-shot,accuracy,0.2443729903536977,0.0244061620946688
facebook/xglm-7.5B,hendrycksTest-philosophy,5-shot,acc_norm,0.2443729903536977,0.0244061620946688
facebook/xglm-7.5B,hendrycksTest-prehistory,5-shot,accuracy,0.2746913580246913,0.0248360578682946
facebook/xglm-7.5B,hendrycksTest-prehistory,5-shot,acc_norm,0.2746913580246913,0.0248360578682946
facebook/xglm-7.5B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2624113475177305,0.026244920349843
facebook/xglm-7.5B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2624113475177305,0.026244920349843
facebook/xglm-7.5B,hendrycksTest-professional_law,5-shot,accuracy,0.2444589308996088,0.0109764250131139
facebook/xglm-7.5B,hendrycksTest-professional_law,5-shot,acc_norm,0.2444589308996088,0.0109764250131139
facebook/xglm-7.5B,hendrycksTest-professional_medicine,5-shot,accuracy,0.4522058823529412,0.0302337585515964
facebook/xglm-7.5B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4522058823529412,0.0302337585515964
facebook/xglm-7.5B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2696078431372549,0.0179524491969878
facebook/xglm-7.5B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2696078431372549,0.0179524491969878
facebook/xglm-7.5B,hendrycksTest-public_relations,5-shot,accuracy,0.2454545454545454,0.0412206650287828
facebook/xglm-7.5B,hendrycksTest-public_relations,5-shot,acc_norm,0.2454545454545454,0.0412206650287828
facebook/xglm-7.5B,hendrycksTest-security_studies,5-shot,accuracy,0.2897959183673469,0.0290430886833043
facebook/xglm-7.5B,hendrycksTest-security_studies,5-shot,acc_norm,0.2897959183673469,0.0290430886833043
facebook/xglm-7.5B,hendrycksTest-sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
facebook/xglm-7.5B,hendrycksTest-sociology,5-shot,acc_norm,0.2388059701492537,0.0301477759354092
facebook/xglm-7.5B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.24,0.0429234695990928
facebook/xglm-7.5B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.24,0.0429234695990928
facebook/xglm-7.5B,hendrycksTest-virology,5-shot,accuracy,0.2891566265060241,0.0352948680151111
facebook/xglm-7.5B,hendrycksTest-virology,5-shot,acc_norm,0.2891566265060241,0.0352948680151111
facebook/xglm-7.5B,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.0344629621708842
facebook/xglm-7.5B,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.0344629621708842
facebook/xglm-7.5B,truthfulqa:mc,0-shot,mc1,0.2093023255813953,0.0142412194347858
facebook/xglm-7.5B,truthfulqa:mc,0-shot,mc2,0.3666152388235458,0.0136668752670778
AbacusResearch/RasGulla1-7b,arc:challenge,25-shot,accuracy,0.659556313993174,0.0138474605188929
AbacusResearch/RasGulla1-7b,arc:challenge,25-shot,acc_norm,0.697098976109215,0.0134282415731853
AbacusResearch/RasGulla1-7b,hellaswag,10-shot,accuracy,0.6983668591913962,0.0045802887281959
AbacusResearch/RasGulla1-7b,hellaswag,10-shot,acc_norm,0.8740290778729337,0.0033113844981586
AbacusResearch/RasGulla1-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.37,0.0485236587093909
AbacusResearch/RasGulla1-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.37,0.0485236587093909
AbacusResearch/RasGulla1-7b,hendrycksTest-anatomy,5-shot,accuracy,0.6518518518518519,0.0411532461033695
AbacusResearch/RasGulla1-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.6518518518518519,0.0411532461033695
AbacusResearch/RasGulla1-7b,hendrycksTest-astronomy,5-shot,accuracy,0.6907894736842105,0.0376107086986748
AbacusResearch/RasGulla1-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.6907894736842105,0.0376107086986748
AbacusResearch/RasGulla1-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.61,0.0490207130000197
AbacusResearch/RasGulla1-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.61,0.0490207130000197
AbacusResearch/RasGulla1-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7132075471698113,0.027834912527544
AbacusResearch/RasGulla1-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7132075471698113,0.027834912527544
AbacusResearch/RasGulla1-7b,hendrycksTest-college_biology,5-shot,accuracy,0.7708333333333334,0.0351469746786238
AbacusResearch/RasGulla1-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.7708333333333334,0.0351469746786238
AbacusResearch/RasGulla1-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.47,0.0501613558046592
AbacusResearch/RasGulla1-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.47,0.0501613558046592
AbacusResearch/RasGulla1-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.57,0.0497569851956242
AbacusResearch/RasGulla1-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.57,0.0497569851956242
AbacusResearch/RasGulla1-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
AbacusResearch/RasGulla1-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.0464823198711731
AbacusResearch/RasGulla1-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.6878612716763006,0.0353313338932365
AbacusResearch/RasGulla1-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.6878612716763006,0.0353313338932365
AbacusResearch/RasGulla1-7b,hendrycksTest-college_physics,5-shot,accuracy,0.4117647058823529,0.0489710495272636
AbacusResearch/RasGulla1-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.4117647058823529,0.0489710495272636
AbacusResearch/RasGulla1-7b,hendrycksTest-computer_security,5-shot,accuracy,0.77,0.042295258468165
AbacusResearch/RasGulla1-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.77,0.042295258468165
AbacusResearch/RasGulla1-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.5829787234042553,0.0322327626671171
AbacusResearch/RasGulla1-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.5829787234042553,0.0322327626671171
AbacusResearch/RasGulla1-7b,hendrycksTest-econometrics,5-shot,accuracy,0.4736842105263157,0.0469708513664786
AbacusResearch/RasGulla1-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.4736842105263157,0.0469708513664786
AbacusResearch/RasGulla1-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5517241379310345,0.0414431181087815
AbacusResearch/RasGulla1-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5517241379310345,0.0414431181087815
AbacusResearch/RasGulla1-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4021164021164021,0.0252530325549976
AbacusResearch/RasGulla1-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4021164021164021,0.0252530325549976
AbacusResearch/RasGulla1-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.492063492063492,0.0447157253629434
AbacusResearch/RasGulla1-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.492063492063492,0.0447157253629434
AbacusResearch/RasGulla1-7b,hendrycksTest-global_facts,5-shot,accuracy,0.32,0.046882617226215
AbacusResearch/RasGulla1-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.32,0.046882617226215
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.7774193548387097,0.0236642166716425
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7774193548387097,0.0236642166716425
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.4827586206896552,0.0351589555116569
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.4827586206896552,0.0351589555116569
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.69,0.0464823198711731
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.69,0.0464823198711731
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7818181818181819,0.0322507810830628
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7818181818181819,0.0322507810830628
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.8080808080808081,0.028057791672989
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8080808080808081,0.028057791672989
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9015544041450776,0.0215002495760334
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9015544041450776,0.0215002495760334
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6666666666666666,0.0239011579794025
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6666666666666666,0.0239011579794025
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3481481481481481,0.0290456002906162
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3481481481481481,0.0290456002906162
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.6680672268907563,0.0305886970137836
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.6680672268907563,0.0305886970137836
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.3443708609271523,0.0387968702407332
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3443708609271523,0.0387968702407332
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8477064220183487,0.015405084393157
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8477064220183487,0.015405084393157
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5046296296296297,0.0340982551916357
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5046296296296297,0.0340982551916357
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8529411764705882,0.0248574780802504
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8529411764705882,0.0248574780802504
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8143459915611815,0.0253104953769448
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8143459915611815,0.0253104953769448
AbacusResearch/RasGulla1-7b,hendrycksTest-human_aging,5-shot,accuracy,0.695067264573991,0.0308986108824775
AbacusResearch/RasGulla1-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.695067264573991,0.0308986108824775
AbacusResearch/RasGulla1-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.7938931297709924,0.0354777100415946
AbacusResearch/RasGulla1-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7938931297709924,0.0354777100415946
AbacusResearch/RasGulla1-7b,hendrycksTest-international_law,5-shot,accuracy,0.7933884297520661,0.0369598012809882
AbacusResearch/RasGulla1-7b,hendrycksTest-international_law,5-shot,acc_norm,0.7933884297520661,0.0369598012809882
AbacusResearch/RasGulla1-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.8055555555555556,0.0382607632488486
AbacusResearch/RasGulla1-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8055555555555556,0.0382607632488486
AbacusResearch/RasGulla1-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7668711656441718,0.0332201579577674
AbacusResearch/RasGulla1-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7668711656441718,0.0332201579577674
AbacusResearch/RasGulla1-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.4285714285714285,0.0469711392301021
AbacusResearch/RasGulla1-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.4285714285714285,0.0469711392301021
AbacusResearch/RasGulla1-7b,hendrycksTest-management,5-shot,accuracy,0.7864077669902912,0.0405804201564603
AbacusResearch/RasGulla1-7b,hendrycksTest-management,5-shot,acc_norm,0.7864077669902912,0.0405804201564603
AbacusResearch/RasGulla1-7b,hendrycksTest-marketing,5-shot,accuracy,0.8803418803418803,0.0212627194004069
AbacusResearch/RasGulla1-7b,hendrycksTest-marketing,5-shot,acc_norm,0.8803418803418803,0.0212627194004069
AbacusResearch/RasGulla1-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.71,0.0456048021572068
AbacusResearch/RasGulla1-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.71,0.0456048021572068
AbacusResearch/RasGulla1-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.8237547892720306,0.0136255569079934
AbacusResearch/RasGulla1-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8237547892720306,0.0136255569079934
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.7485549132947977,0.023357365785874
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7485549132947977,0.023357365785874
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4581005586592179,0.0166636832950205
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4581005586592179,0.0166636832950205
AbacusResearch/RasGulla1-7b,hendrycksTest-nutrition,5-shot,accuracy,0.7320261437908496,0.0253606037962425
AbacusResearch/RasGulla1-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.7320261437908496,0.0253606037962425
AbacusResearch/RasGulla1-7b,hendrycksTest-philosophy,5-shot,accuracy,0.7138263665594855,0.0256702592421889
AbacusResearch/RasGulla1-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.7138263665594855,0.0256702592421889
AbacusResearch/RasGulla1-7b,hendrycksTest-prehistory,5-shot,accuracy,0.7314814814814815,0.0246596851859672
AbacusResearch/RasGulla1-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.7314814814814815,0.0246596851859672
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.4645390070921986,0.029752389657427
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4645390070921986,0.029752389657427
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_law,5-shot,accuracy,0.46870925684485,0.0127452046260831
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.46870925684485,0.0127452046260831
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.6838235294117647,0.0282456873914629
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.6838235294117647,0.0282456873914629
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.6797385620915033,0.0188756829380694
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.6797385620915033,0.0188756829380694
AbacusResearch/RasGulla1-7b,hendrycksTest-public_relations,5-shot,accuracy,0.6727272727272727,0.0449429086625209
AbacusResearch/RasGulla1-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.6727272727272727,0.0449429086625209
AbacusResearch/RasGulla1-7b,hendrycksTest-security_studies,5-shot,accuracy,0.7346938775510204,0.0282638899437845
AbacusResearch/RasGulla1-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.7346938775510204,0.0282638899437845
AbacusResearch/RasGulla1-7b,hendrycksTest-sociology,5-shot,accuracy,0.8557213930348259,0.024845753212306
AbacusResearch/RasGulla1-7b,hendrycksTest-sociology,5-shot,acc_norm,0.8557213930348259,0.024845753212306
AbacusResearch/RasGulla1-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.0348735088019777
AbacusResearch/RasGulla1-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.0348735088019777
AbacusResearch/RasGulla1-7b,hendrycksTest-virology,5-shot,accuracy,0.5481927710843374,0.0387437155658795
AbacusResearch/RasGulla1-7b,hendrycksTest-virology,5-shot,acc_norm,0.5481927710843374,0.0387437155658795
AbacusResearch/RasGulla1-7b,hendrycksTest-world_religions,5-shot,accuracy,0.8421052631578947,0.0279667858591608
AbacusResearch/RasGulla1-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.8421052631578947,0.0279667858591608
AbacusResearch/RasGulla1-7b,truthfulqa:mc,0-shot,mc1,0.4589963280293758,0.0174445444476611
AbacusResearch/RasGulla1-7b,truthfulqa:mc,0-shot,mc2,0.633141301315501,0.0151193912525376
AbacusResearch/RasGulla1-7b,winogrande,5-shot,accuracy,0.8089976322020521,0.0110478087615104
AbacusResearch/RasGulla1-7b,gsm8k,5-shot,accuracy,0.7172100075815011,0.0124050204178736
bigscience/bloom-7b1,drop,3-shot,accuracy,0.0009437919463087,0.0003144653119413
bigscience/bloom-7b1,drop,3-shot,f1,0.0479645553691276,0.0011701293326885
bigscience/bloom-7b1,arc:challenge,25-shot,accuracy,0.3643344709897611,0.0140632602798824
bigscience/bloom-7b1,arc:challenge,25-shot,acc_norm,0.4112627986348123,0.014379441068522
bigscience/bloom-7b1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.26,0.0440844002276808
bigscience/bloom-7b1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.26,0.0440844002276808
bigscience/bloom-7b1,hendrycksTest-anatomy,5-shot,accuracy,0.2444444444444444,0.0371253783361486
bigscience/bloom-7b1,hendrycksTest-anatomy,5-shot,acc_norm,0.2444444444444444,0.0371253783361486
bigscience/bloom-7b1,hendrycksTest-astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
bigscience/bloom-7b1,hendrycksTest-astronomy,5-shot,acc_norm,0.1776315789473684,0.0311031823831233
bigscience/bloom-7b1,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.0440844002276807
bigscience/bloom-7b1,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.0440844002276807
bigscience/bloom-7b1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2754716981132075,0.027495663683724
bigscience/bloom-7b1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2754716981132075,0.027495663683724
bigscience/bloom-7b1,hendrycksTest-college_biology,5-shot,accuracy,0.1944444444444444,0.03309615177059
bigscience/bloom-7b1,hendrycksTest-college_biology,5-shot,acc_norm,0.1944444444444444,0.03309615177059
bigscience/bloom-7b1,hendrycksTest-college_chemistry,5-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-7b1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.21,0.0409360180740332
bigscience/bloom-7b1,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
bigscience/bloom-7b1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
bigscience/bloom-7b1,hendrycksTest-college_mathematics,5-shot,accuracy,0.32,0.046882617226215
bigscience/bloom-7b1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.32,0.046882617226215
bigscience/bloom-7b1,hendrycksTest-college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177499
bigscience/bloom-7b1,hendrycksTest-college_medicine,5-shot,acc_norm,0.2080924855491329,0.0309528902177499
bigscience/bloom-7b1,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
bigscience/bloom-7b1,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
bigscience/bloom-7b1,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.042295258468165
bigscience/bloom-7b1,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.042295258468165
bigscience/bloom-7b1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3234042553191489,0.0305794427736103
bigscience/bloom-7b1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3234042553191489,0.0305794427736103
bigscience/bloom-7b1,hendrycksTest-econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
bigscience/bloom-7b1,hendrycksTest-econometrics,5-shot,acc_norm,0.2368421052631578,0.0399942387928133
bigscience/bloom-7b1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
bigscience/bloom-7b1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
bigscience/bloom-7b1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2751322751322751,0.0230000868590686
bigscience/bloom-7b1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2751322751322751,0.0230000868590686
bigscience/bloom-7b1,hendrycksTest-formal_logic,5-shot,accuracy,0.1666666666666666,0.0333333333333333
bigscience/bloom-7b1,hendrycksTest-formal_logic,5-shot,acc_norm,0.1666666666666666,0.0333333333333333
bigscience/bloom-7b1,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526
bigscience/bloom-7b1,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526
bigscience/bloom-7b1,hendrycksTest-high_school_biology,5-shot,accuracy,0.2548387096774193,0.0247901184593322
bigscience/bloom-7b1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2548387096774193,0.0247901184593322
bigscience/bloom-7b1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.0312709071329769
bigscience/bloom-7b1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.0312709071329769
bigscience/bloom-7b1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.35,0.0479372485441102
bigscience/bloom-7b1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
bigscience/bloom-7b1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2666666666666666,0.0345313180188541
bigscience/bloom-7b1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2666666666666666,0.0345313180188541
bigscience/bloom-7b1,hendrycksTest-high_school_geography,5-shot,accuracy,0.2272727272727272,0.0298575156733864
bigscience/bloom-7b1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2272727272727272,0.0298575156733864
bigscience/bloom-7b1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2072538860103626,0.0292528232918036
bigscience/bloom-7b1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2072538860103626,0.0292528232918036
bigscience/bloom-7b1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2307692307692307,0.0213620277252227
bigscience/bloom-7b1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2307692307692307,0.0213620277252227
bigscience/bloom-7b1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2666666666666666,0.0269624243250738
bigscience/bloom-7b1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2666666666666666,0.0269624243250738
bigscience/bloom-7b1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.226890756302521,0.0272053715382794
bigscience/bloom-7b1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.226890756302521,0.0272053715382794
bigscience/bloom-7b1,hendrycksTest-high_school_physics,5-shot,accuracy,0.23841059602649,0.0347918557259965
bigscience/bloom-7b1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23841059602649,0.0347918557259965
bigscience/bloom-7b1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2477064220183486,0.0185081436025478
bigscience/bloom-7b1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2477064220183486,0.0185081436025478
bigscience/bloom-7b1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3842592592592592,0.0331735451431074
bigscience/bloom-7b1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3842592592592592,0.0331735451431074
bigscience/bloom-7b1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2647058823529412,0.0309645179269234
bigscience/bloom-7b1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2647058823529412,0.0309645179269234
bigscience/bloom-7b1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2658227848101265,0.0287567996296583
bigscience/bloom-7b1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2658227848101265,0.0287567996296583
bigscience/bloom-7b1,hendrycksTest-human_aging,5-shot,accuracy,0.3632286995515695,0.0322779044285049
bigscience/bloom-7b1,hendrycksTest-human_aging,5-shot,acc_norm,0.3632286995515695,0.0322779044285049
bigscience/bloom-7b1,hendrycksTest-human_sexuality,5-shot,accuracy,0.2290076335877862,0.0368534663171185
bigscience/bloom-7b1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2290076335877862,0.0368534663171185
bigscience/bloom-7b1,hendrycksTest-international_law,5-shot,accuracy,0.3636363636363636,0.0439132628672407
bigscience/bloom-7b1,hendrycksTest-international_law,5-shot,acc_norm,0.3636363636363636,0.0439132628672407
bigscience/bloom-7b1,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
bigscience/bloom-7b1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
bigscience/bloom-7b1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2576687116564417,0.0343615082784691
bigscience/bloom-7b1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2576687116564417,0.0343615082784691
bigscience/bloom-7b1,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257873
bigscience/bloom-7b1,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257873
bigscience/bloom-7b1,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.0430125039969087
bigscience/bloom-7b1,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.0430125039969087
bigscience/bloom-7b1,hendrycksTest-marketing,5-shot,accuracy,0.2521367521367521,0.028447965476231
bigscience/bloom-7b1,hendrycksTest-marketing,5-shot,acc_norm,0.2521367521367521,0.028447965476231
bigscience/bloom-7b1,hendrycksTest-medical_genetics,5-shot,accuracy,0.26,0.0440844002276807
bigscience/bloom-7b1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.26,0.0440844002276807
bigscience/bloom-7b1,hendrycksTest-miscellaneous,5-shot,accuracy,0.2886334610472541,0.0162037927031978
bigscience/bloom-7b1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2886334610472541,0.0162037927031978
bigscience/bloom-7b1,hendrycksTest-moral_disputes,5-shot,accuracy,0.245664739884393,0.023176298203992
bigscience/bloom-7b1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.245664739884393,0.023176298203992
bigscience/bloom-7b1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
bigscience/bloom-7b1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.0143335220592178
bigscience/bloom-7b1,hendrycksTest-nutrition,5-shot,accuracy,0.2320261437908496,0.024170840879341
bigscience/bloom-7b1,hendrycksTest-nutrition,5-shot,acc_norm,0.2320261437908496,0.024170840879341
bigscience/bloom-7b1,hendrycksTest-philosophy,5-shot,accuracy,0.2733118971061093,0.0253117659754261
bigscience/bloom-7b1,hendrycksTest-philosophy,5-shot,acc_norm,0.2733118971061093,0.0253117659754261
bigscience/bloom-7b1,hendrycksTest-prehistory,5-shot,accuracy,0.2623456790123457,0.0244772228561351
bigscience/bloom-7b1,hendrycksTest-prehistory,5-shot,acc_norm,0.2623456790123457,0.0244772228561351
bigscience/bloom-7b1,hendrycksTest-professional_accounting,5-shot,accuracy,0.2659574468085106,0.0263580656988805
bigscience/bloom-7b1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2659574468085106,0.0263580656988805
bigscience/bloom-7b1,hendrycksTest-professional_law,5-shot,accuracy,0.2529335071707953,0.0111022687138399
bigscience/bloom-7b1,hendrycksTest-professional_law,5-shot,acc_norm,0.2529335071707953,0.0111022687138399
bigscience/bloom-7b1,hendrycksTest-professional_medicine,5-shot,accuracy,0.2095588235294117,0.024723110407677
bigscience/bloom-7b1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2095588235294117,0.024723110407677
bigscience/bloom-7b1,hendrycksTest-professional_psychology,5-shot,accuracy,0.261437908496732,0.017776947157528
bigscience/bloom-7b1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.261437908496732,0.017776947157528
bigscience/bloom-7b1,hendrycksTest-public_relations,5-shot,accuracy,0.3272727272727272,0.0449429086625209
bigscience/bloom-7b1,hendrycksTest-public_relations,5-shot,acc_norm,0.3272727272727272,0.0449429086625209
bigscience/bloom-7b1,hendrycksTest-security_studies,5-shot,accuracy,0.3020408163265306,0.0293936093198798
bigscience/bloom-7b1,hendrycksTest-security_studies,5-shot,acc_norm,0.3020408163265306,0.0293936093198798
bigscience/bloom-7b1,hendrycksTest-sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
bigscience/bloom-7b1,hendrycksTest-sociology,5-shot,acc_norm,0.2388059701492537,0.0301477759354092
bigscience/bloom-7b1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.0409360180740332
bigscience/bloom-7b1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.0409360180740332
bigscience/bloom-7b1,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.0359156679782466
bigscience/bloom-7b1,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.0359156679782466
bigscience/bloom-7b1,hendrycksTest-world_religions,5-shot,accuracy,0.2923976608187134,0.0348864771345792
bigscience/bloom-7b1,hendrycksTest-world_religions,5-shot,acc_norm,0.2923976608187134,0.0348864771345792
bigscience/bloom-7b1,truthfulqa:mc,0-shot,mc1,0.2239902080783353,0.0145949643294742
bigscience/bloom-7b1,truthfulqa:mc,0-shot,mc2,0.3889784219035787,0.0140157534820364
mosaicml/mpt-7b,drop,3-shot,accuracy,0.0006291946308724,0.0002568002749724
mosaicml/mpt-7b,drop,3-shot,f1,0.0554834312080538,0.001289672637018
mosaicml/mpt-7b,arc:challenge,25-shot,accuracy,0.4291808873720136,0.0144640858948706
mosaicml/mpt-7b,arc:challenge,25-shot,acc_norm,0.4769624573378839,0.0145958732053582
mosaicml/mpt-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.19,0.0394277244403662
mosaicml/mpt-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.19,0.0394277244403662
mosaicml/mpt-7b,hendrycksTest-anatomy,5-shot,accuracy,0.2222222222222222,0.0359144408419696
mosaicml/mpt-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.2222222222222222,0.0359144408419696
mosaicml/mpt-7b,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.0358349617636106
mosaicml/mpt-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.0358349617636106
mosaicml/mpt-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.047258156262526
mosaicml/mpt-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.047258156262526
mosaicml/mpt-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2867924528301886,0.027834912527544
mosaicml/mpt-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2867924528301886,0.027834912527544
mosaicml/mpt-7b,hendrycksTest-college_biology,5-shot,accuracy,0.2847222222222222,0.0377380999068693
mosaicml/mpt-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.2847222222222222,0.0377380999068693
mosaicml/mpt-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.25,0.0435194139889244
mosaicml/mpt-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.25,0.0435194139889244
mosaicml/mpt-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215
mosaicml/mpt-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215
mosaicml/mpt-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
mosaicml/mpt-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
mosaicml/mpt-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225942
mosaicml/mpt-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.0336876293225942
mosaicml/mpt-7b,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
mosaicml/mpt-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
mosaicml/mpt-7b,hendrycksTest-computer_security,5-shot,accuracy,0.29,0.0456048021572068
mosaicml/mpt-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.29,0.0456048021572068
mosaicml/mpt-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3404255319148936,0.0309766929985344
mosaicml/mpt-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3404255319148936,0.0309766929985344
mosaicml/mpt-7b,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.0426633944315939
mosaicml/mpt-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.0426633944315939
mosaicml/mpt-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.0366466633722525
mosaicml/mpt-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.0366466633722525
mosaicml/mpt-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.238095238095238,0.0219358780811847
mosaicml/mpt-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.238095238095238,0.0219358780811847
mosaicml/mpt-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.2301587301587301,0.037649508797906
mosaicml/mpt-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2301587301587301,0.037649508797906
mosaicml/mpt-7b,hendrycksTest-global_facts,5-shot,accuracy,0.23,0.042295258468165
mosaicml/mpt-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.23,0.042295258468165
mosaicml/mpt-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2516129032258064,0.0246859792862399
mosaicml/mpt-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2516129032258064,0.0246859792862399
mosaicml/mpt-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2068965517241379,0.0285013781678939
mosaicml/mpt-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2068965517241379,0.0285013781678939
mosaicml/mpt-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
mosaicml/mpt-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526
mosaicml/mpt-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2424242424242424,0.0334640988105595
mosaicml/mpt-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2424242424242424,0.0334640988105595
mosaicml/mpt-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.0296202278747904
mosaicml/mpt-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.0296202278747904
mosaicml/mpt-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3575129533678756,0.03458816042181
mosaicml/mpt-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3575129533678756,0.03458816042181
mosaicml/mpt-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3205128205128205,0.0236612963939642
mosaicml/mpt-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3205128205128205,0.0236612963939642
mosaicml/mpt-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2851851851851852,0.0275285992103404
mosaicml/mpt-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2851851851851852,0.0275285992103404
mosaicml/mpt-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2899159663865546,0.029472485833136
mosaicml/mpt-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2899159663865546,0.029472485833136
mosaicml/mpt-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
mosaicml/mpt-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
mosaicml/mpt-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2605504587155963,0.01881918203485
mosaicml/mpt-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2605504587155963,0.01881918203485
mosaicml/mpt-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3055555555555556,0.0314155462940254
mosaicml/mpt-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3055555555555556,0.0314155462940254
mosaicml/mpt-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.0305875913516042
mosaicml/mpt-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.0305875913516042
mosaicml/mpt-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.0289007219062934
mosaicml/mpt-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.0289007219062934
mosaicml/mpt-7b,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.0310244117405722
mosaicml/mpt-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.0310244117405722
mosaicml/mpt-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2977099236641221,0.040103589424622
mosaicml/mpt-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2977099236641221,0.040103589424622
mosaicml/mpt-7b,hendrycksTest-international_law,5-shot,accuracy,0.2975206611570248,0.0417334914808349
mosaicml/mpt-7b,hendrycksTest-international_law,5-shot,acc_norm,0.2975206611570248,0.0417334914808349
mosaicml/mpt-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.0441434366685493
mosaicml/mpt-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.0441434366685493
mosaicml/mpt-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2515337423312883,0.0340899788685752
mosaicml/mpt-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2515337423312883,0.0340899788685752
mosaicml/mpt-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.3660714285714285,0.0457237235873743
mosaicml/mpt-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.3660714285714285,0.0457237235873743
mosaicml/mpt-7b,hendrycksTest-management,5-shot,accuracy,0.233009708737864,0.0418583259892831
mosaicml/mpt-7b,hendrycksTest-management,5-shot,acc_norm,0.233009708737864,0.0418583259892831
mosaicml/mpt-7b,hendrycksTest-marketing,5-shot,accuracy,0.3205128205128205,0.0305728113102996
mosaicml/mpt-7b,hendrycksTest-marketing,5-shot,acc_norm,0.3205128205128205,0.0305728113102996
mosaicml/mpt-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.0479372485441101
mosaicml/mpt-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.0479372485441101
mosaicml/mpt-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.3001277139208174,0.0163892496913174
mosaicml/mpt-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3001277139208174,0.0163892496913174
mosaicml/mpt-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
mosaicml/mpt-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
mosaicml/mpt-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2446927374301676,0.0143781698840984
mosaicml/mpt-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2446927374301676,0.0143781698840984
mosaicml/mpt-7b,hendrycksTest-nutrition,5-shot,accuracy,0.2810457516339869,0.0257388547978187
mosaicml/mpt-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.2810457516339869,0.0257388547978187
mosaicml/mpt-7b,hendrycksTest-philosophy,5-shot,accuracy,0.2990353697749196,0.0260033011178851
mosaicml/mpt-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.2990353697749196,0.0260033011178851
mosaicml/mpt-7b,hendrycksTest-prehistory,5-shot,accuracy,0.3209876543209876,0.0259765660108627
mosaicml/mpt-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.3209876543209876,0.0259765660108627
mosaicml/mpt-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
mosaicml/mpt-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2482269503546099,0.0257700156442903
mosaicml/mpt-7b,hendrycksTest-professional_law,5-shot,accuracy,0.2607561929595828,0.0112134715596023
mosaicml/mpt-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2607561929595828,0.0112134715596023
mosaicml/mpt-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.1948529411764706,0.0240605994234874
mosaicml/mpt-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1948529411764706,0.0240605994234874
mosaicml/mpt-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
mosaicml/mpt-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2598039215686274,0.0177408995091777
mosaicml/mpt-7b,hendrycksTest-public_relations,5-shot,accuracy,0.3363636363636363,0.045253935963025
mosaicml/mpt-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.3363636363636363,0.045253935963025
mosaicml/mpt-7b,hendrycksTest-security_studies,5-shot,accuracy,0.3020408163265306,0.0293936093198798
mosaicml/mpt-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.3020408163265306,0.0293936093198798
mosaicml/mpt-7b,hendrycksTest-sociology,5-shot,accuracy,0.2338308457711442,0.0299294154083483
mosaicml/mpt-7b,hendrycksTest-sociology,5-shot,acc_norm,0.2338308457711442,0.0299294154083483
mosaicml/mpt-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.4,0.049236596391733
mosaicml/mpt-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.4,0.049236596391733
mosaicml/mpt-7b,hendrycksTest-virology,5-shot,accuracy,0.3493975903614458,0.0371172519074074
mosaicml/mpt-7b,hendrycksTest-virology,5-shot,acc_norm,0.3493975903614458,0.0371172519074074
mosaicml/mpt-7b,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.0358252944257312
mosaicml/mpt-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.0358252944257312
mosaicml/mpt-7b,truthfulqa:mc,0-shot,mc1,0.2056303549571603,0.0141484822194609
mosaicml/mpt-7b,truthfulqa:mc,0-shot,mc2,0.3354506043570123,0.0131103233135939
facebook/opt-6.7b,minerva_math_precalc,5-shot,accuracy,0.0036630036630036,0.0025877573681934
facebook/opt-6.7b,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458,0.0034284443646836
facebook/opt-6.7b,minerva_math_num_theory,5-shot,accuracy,0.0055555555555555,0.0032015451273208
facebook/opt-6.7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0044296788482834,0.0022111531423787
facebook/opt-6.7b,minerva_math_geometry,5-shot,accuracy,0.0104384133611691,0.0046486271171846
facebook/opt-6.7b,minerva_math_counting_and_prob,5-shot,accuracy,0.0084388185654008,0.004206007207713
facebook/opt-6.7b,minerva_math_algebra,5-shot,accuracy,0.0092670598146588,0.0027823191184888
facebook/opt-6.7b,fld_default,0-shot,accuracy,0.0,
facebook/opt-6.7b,fld_star,0-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521
facebook/opt-6.7b,arithmetic_3ds,5-shot,accuracy,0.002,0.0009992493430694
facebook/opt-6.7b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
facebook/opt-6.7b,arithmetic_2ds,5-shot,accuracy,0.012,0.0024353573624298
facebook/opt-6.7b,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_1dc,5-shot,accuracy,0.0205,0.0031693686198869
facebook/opt-6.7b,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_2dm,5-shot,accuracy,0.036,0.0041666149738331
facebook/opt-6.7b,arithmetic_2da,5-shot,accuracy,0.0055,0.0016541593398342
facebook/opt-6.7b,gsm8k_cot,5-shot,accuracy,0.0288097043214556,0.0046074842837674
facebook/opt-6.7b,mmlu_world_religions,0-shot,accuracy,0.2222222222222222,0.0318857801768639
facebook/opt-6.7b,mmlu_formal_logic,0-shot,accuracy,0.1666666666666666,0.0333333333333333
facebook/opt-6.7b,mmlu_prehistory,0-shot,accuracy,0.2746913580246913,0.0248360578682946
facebook/opt-6.7b,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/opt-6.7b,mmlu_high_school_world_history,0-shot,accuracy,0.2573839662447257,0.0284588209914603
facebook/opt-6.7b,mmlu_moral_disputes,0-shot,accuracy,0.2369942196531791,0.0228940824899259
facebook/opt-6.7b,mmlu_professional_law,0-shot,accuracy,0.2353324641460234,0.0108344325439122
facebook/opt-6.7b,mmlu_logical_fallacies,0-shot,accuracy,0.2453987730061349,0.0338093981394335
facebook/opt-6.7b,mmlu_high_school_us_history,0-shot,accuracy,0.2401960784313725,0.0299837330559136
facebook/opt-6.7b,mmlu_philosophy,0-shot,accuracy,0.2282958199356913,0.0238393033113982
facebook/opt-6.7b,mmlu_jurisprudence,0-shot,accuracy,0.25,0.041860917913946
facebook/opt-6.7b,mmlu_international_law,0-shot,accuracy,0.256198347107438,0.0398497965330287
facebook/opt-6.7b,mmlu_high_school_european_history,0-shot,accuracy,0.2424242424242424,0.0334640988105595
facebook/opt-6.7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.233160621761658,0.030516111371476
facebook/opt-6.7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.1722689075630252,0.0245286649713054
facebook/opt-6.7b,mmlu_high_school_geography,0-shot,accuracy,0.1969696969696969,0.0283356097324633
facebook/opt-6.7b,mmlu_high_school_psychology,0-shot,accuracy,0.2697247706422018,0.0190284867111154
facebook/opt-6.7b,mmlu_public_relations,0-shot,accuracy,0.3272727272727272,0.0449429086625209
facebook/opt-6.7b,mmlu_us_foreign_policy,0-shot,accuracy,0.22,0.0416333199893226
facebook/opt-6.7b,mmlu_sociology,0-shot,accuracy,0.2338308457711442,0.0299294154083483
facebook/opt-6.7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2435897435897435,0.0217637336841739
facebook/opt-6.7b,mmlu_security_studies,0-shot,accuracy,0.1918367346938775,0.0252069631542253
facebook/opt-6.7b,mmlu_professional_psychology,0-shot,accuracy,0.2516339869281045,0.0175558180913222
facebook/opt-6.7b,mmlu_human_sexuality,0-shot,accuracy,0.2213740458015267,0.0364129708131373
facebook/opt-6.7b,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
facebook/opt-6.7b,mmlu_miscellaneous,0-shot,accuracy,0.2835249042145594,0.0161173181668322
facebook/opt-6.7b,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.0298725777088911
facebook/opt-6.7b,mmlu_management,0-shot,accuracy,0.2427184466019417,0.0424502248638449
facebook/opt-6.7b,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.0238051865248881
facebook/opt-6.7b,mmlu_medical_genetics,0-shot,accuracy,0.32,0.046882617226215
facebook/opt-6.7b,mmlu_human_aging,0-shot,accuracy,0.3542600896860987,0.0321006215413498
facebook/opt-6.7b,mmlu_professional_medicine,0-shot,accuracy,0.2647058823529412,0.0267995620248876
facebook/opt-6.7b,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/opt-6.7b,mmlu_business_ethics,0-shot,accuracy,0.2,0.0402015126103684
facebook/opt-6.7b,mmlu_clinical_knowledge,0-shot,accuracy,0.1886792452830188,0.0240799951300622
facebook/opt-6.7b,mmlu_global_facts,0-shot,accuracy,0.29,0.0456048021572068
facebook/opt-6.7b,mmlu_virology,0-shot,accuracy,0.3313253012048193,0.0366431477728808
facebook/opt-6.7b,mmlu_professional_accounting,0-shot,accuracy,0.301418439716312,0.0273741288826311
facebook/opt-6.7b,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
facebook/opt-6.7b,mmlu_high_school_physics,0-shot,accuracy,0.1655629139072847,0.0303481834103036
facebook/opt-6.7b,mmlu_high_school_biology,0-shot,accuracy,0.232258064516129,0.0240222561303082
facebook/opt-6.7b,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.0347659010430413
facebook/opt-6.7b,mmlu_anatomy,0-shot,accuracy,0.3333333333333333,0.0407231481187683
facebook/opt-6.7b,mmlu_college_chemistry,0-shot,accuracy,0.24,0.0429234695990928
facebook/opt-6.7b,mmlu_computer_security,0-shot,accuracy,0.24,0.0429234695990928
facebook/opt-6.7b,mmlu_college_computer_science,0-shot,accuracy,0.3,0.0460566186471838
facebook/opt-6.7b,mmlu_astronomy,0-shot,accuracy,0.1578947368421052,0.0296741675201014
facebook/opt-6.7b,mmlu_college_mathematics,0-shot,accuracy,0.3,0.0460566186471838
facebook/opt-6.7b,mmlu_conceptual_physics,0-shot,accuracy,0.2553191489361702,0.0285048564705141
facebook/opt-6.7b,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.0429234695990928
facebook/opt-6.7b,mmlu_high_school_computer_science,0-shot,accuracy,0.24,0.0429234695990928
facebook/opt-6.7b,mmlu_machine_learning,0-shot,accuracy,0.2946428571428571,0.0432704093257872
facebook/opt-6.7b,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.0305165307326944
facebook/opt-6.7b,mmlu_high_school_statistics,0-shot,accuracy,0.1759259259259259,0.0259674209582585
facebook/opt-6.7b,mmlu_elementary_mathematics,0-shot,accuracy,0.2407407407407407,0.0220190800122178
facebook/opt-6.7b,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.0375283395800333
facebook/opt-6.7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2740740740740741,0.0271959348040856
facebook/opt-6.7b,arc_challenge,25-shot,accuracy,0.356655290102389,0.0139980569026201
facebook/opt-6.7b,arc_challenge,25-shot,acc_norm,0.3856655290102389,0.0142242509732571
facebook/opt-6.7b,truthfulqa_mc2,0-shot,accuracy,0.3509559624657817,0.0135682195026171
facebook/opt-6.7b,truthfulqa_gen,0-shot,bleu_max,23.707553429599244,0.7450122059703083
facebook/opt-6.7b,truthfulqa_gen,0-shot,bleu_acc,0.2876376988984088,0.0158463151013947
facebook/opt-6.7b,truthfulqa_gen,0-shot,bleu_diff,-8.638098070838828,0.8005088298726831
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge1_max,48.60974313039175,0.858316293921586
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge1_acc,0.2594859241126071,0.015345409485558
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge1_diff,-11.045582261019916,0.8882795627003046
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge2_max,31.52878460707801,0.9802016635484708
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge2_acc,0.2019583843329253,0.0140539574415123
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge2_diff,-13.46757758334418,1.035259887092465
facebook/opt-6.7b,truthfulqa_gen,0-shot,rougeL_max,45.74410864668455,0.8676424195118835
facebook/opt-6.7b,truthfulqa_gen,0-shot,rougeL_acc,0.2594859241126071,0.015345409485558
facebook/opt-6.7b,truthfulqa_gen,0-shot,rougeL_diff,-11.266372071748116,0.8999068306781938
facebook/opt-6.7b,truthfulqa_mc1,0-shot,accuracy,0.2178702570379437,0.0144508467141238
AbacusResearch/haLLAwa2,minerva_math_precalc,5-shot,accuracy,0.0604395604395604,0.0102076262166469
AbacusResearch/haLLAwa2,minerva_math_prealgebra,5-shot,accuracy,0.3099885189437428,0.0156798295303162
AbacusResearch/haLLAwa2,minerva_math_num_theory,5-shot,accuracy,0.0888888888888888,0.0122578704655672
AbacusResearch/haLLAwa2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0764119601328903,0.0088453811516459
AbacusResearch/haLLAwa2,minerva_math_geometry,5-shot,accuracy,0.1336116910229645,0.0155619699953403
AbacusResearch/haLLAwa2,minerva_math_counting_and_prob,5-shot,accuracy,0.1392405063291139,0.0159181699553674
AbacusResearch/haLLAwa2,minerva_math_algebra,5-shot,accuracy,0.2266217354675652,0.0121563841921995
AbacusResearch/haLLAwa2,fld_default,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa2,fld_star,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa2,arithmetic_3da,5-shot,accuracy,0.9685,0.0039065977208917
AbacusResearch/haLLAwa2,arithmetic_3ds,5-shot,accuracy,0.97,0.0038154001938617
AbacusResearch/haLLAwa2,arithmetic_4da,5-shot,accuracy,0.9135,0.0062871805540846
AbacusResearch/haLLAwa2,arithmetic_2ds,5-shot,accuracy,0.9955,0.0014969954902233
AbacusResearch/haLLAwa2,arithmetic_5ds,5-shot,accuracy,0.8495,0.0079973028845175
AbacusResearch/haLLAwa2,arithmetic_5da,5-shot,accuracy,0.8895,0.007012093819243
AbacusResearch/haLLAwa2,arithmetic_1dc,5-shot,accuracy,0.6425,0.010719343597608
AbacusResearch/haLLAwa2,arithmetic_4ds,5-shot,accuracy,0.9125,0.0063199561646391
AbacusResearch/haLLAwa2,arithmetic_2dm,5-shot,accuracy,0.617,0.0108726541057669
AbacusResearch/haLLAwa2,arithmetic_2da,5-shot,accuracy,0.986,0.0026278228110667
AbacusResearch/haLLAwa2,gsm8k_cot,5-shot,accuracy,0.5701288855193328,0.0136363440173937
AbacusResearch/haLLAwa2,gsm8k,5-shot,accuracy,0.5276724791508719,0.0137513755388013
AbacusResearch/haLLAwa2,anli_r2,0-shot,brier_score,1.0924707676244785,
AbacusResearch/haLLAwa2,anli_r3,0-shot,brier_score,1.0236139706947502,
AbacusResearch/haLLAwa2,anli_r1,0-shot,brier_score,1.0837828290354423,
AbacusResearch/haLLAwa2,xnli_eu,0-shot,brier_score,1.1439712925050591,
AbacusResearch/haLLAwa2,xnli_vi,0-shot,brier_score,1.0246823649930676,
AbacusResearch/haLLAwa2,xnli_ru,0-shot,brier_score,0.9703116231053286,
AbacusResearch/haLLAwa2,xnli_zh,0-shot,brier_score,1.2234280607922878,
AbacusResearch/haLLAwa2,xnli_tr,0-shot,brier_score,1.0282731990983742,
AbacusResearch/haLLAwa2,xnli_fr,0-shot,brier_score,0.9242344176415858,
AbacusResearch/haLLAwa2,xnli_en,0-shot,brier_score,0.6881897992605256,
AbacusResearch/haLLAwa2,xnli_ur,0-shot,brier_score,1.2513617048946404,
AbacusResearch/haLLAwa2,xnli_ar,0-shot,brier_score,1.305527995518514,
AbacusResearch/haLLAwa2,xnli_de,0-shot,brier_score,1.018285594258703,
AbacusResearch/haLLAwa2,xnli_hi,0-shot,brier_score,1.023238689228211,
AbacusResearch/haLLAwa2,xnli_es,0-shot,brier_score,0.9770376302469204,
AbacusResearch/haLLAwa2,xnli_bg,0-shot,brier_score,0.9690174071509364,
AbacusResearch/haLLAwa2,xnli_sw,0-shot,brier_score,0.998198662122768,
AbacusResearch/haLLAwa2,xnli_el,0-shot,brier_score,0.9569683975606962,
AbacusResearch/haLLAwa2,xnli_th,0-shot,brier_score,1.0281918906994378,
AbacusResearch/haLLAwa2,logiqa2,0-shot,brier_score,0.9982039832917936,
AbacusResearch/haLLAwa2,mathqa,0-shot,brier_score,0.9685303356773712,
AbacusResearch/haLLAwa2,lambada_standard,0-shot,perplexity,3.706296285581885,0.1109393542709144
AbacusResearch/haLLAwa2,lambada_standard,0-shot,accuracy,0.6743644478944304,0.0065286789578354
AbacusResearch/haLLAwa2,lambada_openai,0-shot,perplexity,2.805125379000774,0.0700237977491076
AbacusResearch/haLLAwa2,lambada_openai,0-shot,accuracy,0.74345041723268,0.0060844837271676
AbacusResearch/haLLAwa2,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.0283809195961458
AbacusResearch/haLLAwa2,mmlu_formal_logic,0-shot,accuracy,0.4047619047619047,0.0439025926537756
AbacusResearch/haLLAwa2,mmlu_prehistory,0-shot,accuracy,0.7160493827160493,0.0250894785237651
AbacusResearch/haLLAwa2,mmlu_moral_scenarios,0-shot,accuracy,0.4223463687150838,0.0165195942752971
AbacusResearch/haLLAwa2,mmlu_high_school_world_history,0-shot,accuracy,0.810126582278481,0.0255301004602334
AbacusResearch/haLLAwa2,mmlu_moral_disputes,0-shot,accuracy,0.7369942196531792,0.0237030995252581
AbacusResearch/haLLAwa2,mmlu_professional_law,0-shot,accuracy,0.4582790091264667,0.0127257016569536
AbacusResearch/haLLAwa2,mmlu_logical_fallacies,0-shot,accuracy,0.7607361963190185,0.0335195387952127
AbacusResearch/haLLAwa2,mmlu_high_school_us_history,0-shot,accuracy,0.8137254901960784,0.0273254709667163
AbacusResearch/haLLAwa2,mmlu_philosophy,0-shot,accuracy,0.7266881028938906,0.0253117659754261
AbacusResearch/haLLAwa2,mmlu_jurisprudence,0-shot,accuracy,0.8055555555555556,0.0382607632488486
AbacusResearch/haLLAwa2,mmlu_international_law,0-shot,accuracy,0.8181818181818182,0.0352089395109765
AbacusResearch/haLLAwa2,mmlu_high_school_european_history,0-shot,accuracy,0.7636363636363637,0.0331750593000918
AbacusResearch/haLLAwa2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8704663212435233,0.0242335322977587
AbacusResearch/haLLAwa2,mmlu_high_school_microeconomics,0-shot,accuracy,0.6512605042016807,0.0309566363285665
AbacusResearch/haLLAwa2,mmlu_high_school_geography,0-shot,accuracy,0.7777777777777778,0.0296202278747904
AbacusResearch/haLLAwa2,mmlu_high_school_psychology,0-shot,accuracy,0.8256880733944955,0.0162656756320103
AbacusResearch/haLLAwa2,mmlu_public_relations,0-shot,accuracy,0.6454545454545455,0.0458200484150541
AbacusResearch/haLLAwa2,mmlu_us_foreign_policy,0-shot,accuracy,0.89,0.031446603773522
AbacusResearch/haLLAwa2,mmlu_sociology,0-shot,accuracy,0.8606965174129353,0.0244844871629139
AbacusResearch/haLLAwa2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6128205128205129,0.0246972169308789
AbacusResearch/haLLAwa2,mmlu_security_studies,0-shot,accuracy,0.726530612244898,0.0285355603371284
AbacusResearch/haLLAwa2,mmlu_professional_psychology,0-shot,accuracy,0.6519607843137255,0.0192709987082239
AbacusResearch/haLLAwa2,mmlu_human_sexuality,0-shot,accuracy,0.7557251908396947,0.0376833595972874
AbacusResearch/haLLAwa2,mmlu_econometrics,0-shot,accuracy,0.4824561403508772,0.0470070803355103
AbacusResearch/haLLAwa2,mmlu_miscellaneous,0-shot,accuracy,0.8084291187739464,0.0140728593104519
AbacusResearch/haLLAwa2,mmlu_marketing,0-shot,accuracy,0.8803418803418803,0.0212627194004069
AbacusResearch/haLLAwa2,mmlu_management,0-shot,accuracy,0.7961165048543689,0.0398913985953177
AbacusResearch/haLLAwa2,mmlu_nutrition,0-shot,accuracy,0.7516339869281046,0.0247399813551135
AbacusResearch/haLLAwa2,mmlu_medical_genetics,0-shot,accuracy,0.67,0.047258156262526
AbacusResearch/haLLAwa2,mmlu_human_aging,0-shot,accuracy,0.6995515695067265,0.0307693520082291
AbacusResearch/haLLAwa2,mmlu_professional_medicine,0-shot,accuracy,0.6948529411764706,0.0279715413701706
AbacusResearch/haLLAwa2,mmlu_college_medicine,0-shot,accuracy,0.6011560693641619,0.037336266553835
AbacusResearch/haLLAwa2,mmlu_business_ethics,0-shot,accuracy,0.56,0.0498887651569858
AbacusResearch/haLLAwa2,mmlu_clinical_knowledge,0-shot,accuracy,0.6981132075471698,0.0282542003444386
AbacusResearch/haLLAwa2,mmlu_global_facts,0-shot,accuracy,0.4,0.049236596391733
AbacusResearch/haLLAwa2,mmlu_virology,0-shot,accuracy,0.5421686746987951,0.0387862677100235
AbacusResearch/haLLAwa2,mmlu_professional_accounting,0-shot,accuracy,0.4468085106382978,0.0296582350976669
AbacusResearch/haLLAwa2,mmlu_college_physics,0-shot,accuracy,0.4411764705882353,0.0494063563060565
AbacusResearch/haLLAwa2,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.0368488152138902
AbacusResearch/haLLAwa2,mmlu_high_school_biology,0-shot,accuracy,0.7419354838709677,0.0248924691724628
AbacusResearch/haLLAwa2,mmlu_college_biology,0-shot,accuracy,0.7222222222222222,0.0374555479146245
AbacusResearch/haLLAwa2,mmlu_anatomy,0-shot,accuracy,0.6222222222222222,0.0418830753759585
AbacusResearch/haLLAwa2,mmlu_college_chemistry,0-shot,accuracy,0.46,0.0500908265962033
AbacusResearch/haLLAwa2,mmlu_computer_security,0-shot,accuracy,0.77,0.042295258468165
AbacusResearch/haLLAwa2,mmlu_college_computer_science,0-shot,accuracy,0.52,0.0502116731568677
AbacusResearch/haLLAwa2,mmlu_astronomy,0-shot,accuracy,0.6710526315789473,0.038234289699266
AbacusResearch/haLLAwa2,mmlu_college_mathematics,0-shot,accuracy,0.36,0.0482418151324421
AbacusResearch/haLLAwa2,mmlu_conceptual_physics,0-shot,accuracy,0.5872340425531914,0.0321847114140035
AbacusResearch/haLLAwa2,mmlu_abstract_algebra,0-shot,accuracy,0.35,0.0479372485441101
AbacusResearch/haLLAwa2,mmlu_high_school_computer_science,0-shot,accuracy,0.67,0.047258156262526
AbacusResearch/haLLAwa2,mmlu_machine_learning,0-shot,accuracy,0.4732142857142857,0.0473897511927415
AbacusResearch/haLLAwa2,mmlu_high_school_chemistry,0-shot,accuracy,0.4876847290640394,0.0351692044422089
AbacusResearch/haLLAwa2,mmlu_high_school_statistics,0-shot,accuracy,0.4814814814814814,0.0340763209385405
AbacusResearch/haLLAwa2,mmlu_elementary_mathematics,0-shot,accuracy,0.3835978835978836,0.0250437573185201
AbacusResearch/haLLAwa2,mmlu_electrical_engineering,0-shot,accuracy,0.5310344827586206,0.0415863276209782
AbacusResearch/haLLAwa2,mmlu_high_school_mathematics,0-shot,accuracy,0.3814814814814815,0.0296167189274975
AbacusResearch/haLLAwa2,arc_challenge,25-shot,accuracy,0.6023890784982935,0.0143017522232795
AbacusResearch/haLLAwa2,arc_challenge,25-shot,acc_norm,0.6407849829351536,0.0140202241558391
AbacusResearch/haLLAwa2,hellaswag,10-shot,accuracy,0.68123879705238,0.0046504387817452
AbacusResearch/haLLAwa2,hellaswag,10-shot,acc_norm,0.8464449312885879,0.003597849139815
AbacusResearch/haLLAwa2,truthfulqa_mc2,0-shot,accuracy,0.476571257537057,0.0156330980157801
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,bleu_max,26.979448196778783,0.7893178897204924
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,bleu_acc,0.3929008567931457,0.017097248285233
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,bleu_diff,-2.604511700508371,0.8015257472068844
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge1_max,52.66916062467333,0.8511635282334032
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge1_acc,0.3953488372093023,0.0171158156324182
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge1_diff,-4.016130326745816,0.974035197973408
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge2_max,37.88807807215924,0.989325527207374
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge2_acc,0.3549571603427172,0.0167508623813759
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge2_diff,-4.403291370670143,1.1060638545857444
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rougeL_max,49.48769052738361,0.8679658445507986
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rougeL_acc,0.3708690330477356,0.0169096935802488
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rougeL_diff,-4.579117328573877,0.9886743725581556
AbacusResearch/haLLAwa2,truthfulqa_mc1,0-shot,accuracy,0.3341493268053855,0.0165125306771505
AbacusResearch/haLLAwa2,winogrande,5-shot,accuracy,0.760852407261247,0.0119885418448439
Qwen/Qwen2.5-32B,mmlu_world_religions,0-shot,accuracy,0.9064327485380116,0.0223359932311632
Qwen/Qwen2.5-32B,mmlu_formal_logic,0-shot,accuracy,0.7222222222222222,0.0400616808384887
Qwen/Qwen2.5-32B,mmlu_prehistory,0-shot,accuracy,0.9135802469135802,0.0156343057106935
Qwen/Qwen2.5-32B,mmlu_moral_scenarios,0-shot,accuracy,0.7541899441340782,0.0144002964292256
Qwen/Qwen2.5-32B,mmlu_high_school_world_history,0-shot,accuracy,0.9240506329113924,0.0172446332510657
Qwen/Qwen2.5-32B,mmlu_moral_disputes,0-shot,accuracy,0.8439306358381503,0.019539014685374
Qwen/Qwen2.5-32B,mmlu_professional_law,0-shot,accuracy,0.6492829204693612,0.0121877733707415
Qwen/Qwen2.5-32B,mmlu_logical_fallacies,0-shot,accuracy,0.8895705521472392,0.0246249377889413
Qwen/Qwen2.5-32B,mmlu_high_school_us_history,0-shot,accuracy,0.9362745098039216,0.0171439216555249
Qwen/Qwen2.5-32B,mmlu_philosophy,0-shot,accuracy,0.8778135048231511,0.0186008112529679
Qwen/Qwen2.5-32B,mmlu_jurisprudence,0-shot,accuracy,0.8425925925925926,0.0352070399051796
Qwen/Qwen2.5-32B,mmlu_international_law,0-shot,accuracy,0.9338842975206612,0.0226834036917233
Qwen/Qwen2.5-32B,mmlu_high_school_european_history,0-shot,accuracy,0.8787878787878788,0.0254854983733432
Qwen/Qwen2.5-32B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9740932642487048,0.0114645233569531
Qwen/Qwen2.5-32B,mmlu_high_school_microeconomics,0-shot,accuracy,0.9453781512605042,0.0147608648934984
Qwen/Qwen2.5-32B,mmlu_high_school_geography,0-shot,accuracy,0.9444444444444444,0.0163199507007673
Qwen/Qwen2.5-32B,mmlu_high_school_psychology,0-shot,accuracy,0.9339449541284404,0.0106491314878589
Qwen/Qwen2.5-32B,mmlu_public_relations,0-shot,accuracy,0.7636363636363637,0.0406930631972137
Qwen/Qwen2.5-32B,mmlu_us_foreign_policy,0-shot,accuracy,0.96,0.0196946385566932
Qwen/Qwen2.5-32B,mmlu_sociology,0-shot,accuracy,0.9104477611940298,0.0201906705350279
Qwen/Qwen2.5-32B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.8769230769230769,0.0166569032820682
Qwen/Qwen2.5-32B,mmlu_security_studies,0-shot,accuracy,0.8489795918367347,0.0229230040947368
Qwen/Qwen2.5-32B,mmlu_professional_psychology,0-shot,accuracy,0.8709150326797386,0.0135645416340441
Qwen/Qwen2.5-32B,mmlu_human_sexuality,0-shot,accuracy,0.9236641221374046,0.0232889395361737
Qwen/Qwen2.5-32B,mmlu_econometrics,0-shot,accuracy,0.7807017543859649,0.0389243110651875
Qwen/Qwen2.5-32B,mmlu_miscellaneous,0-shot,accuracy,0.9195402298850576,0.0097268313161418
Qwen/Qwen2.5-32B,mmlu_marketing,0-shot,accuracy,0.9145299145299144,0.0183158916856258
Qwen/Qwen2.5-32B,mmlu_management,0-shot,accuracy,0.883495145631068,0.031766839486404
Qwen/Qwen2.5-32B,mmlu_nutrition,0-shot,accuracy,0.869281045751634,0.0193018736242152
Qwen/Qwen2.5-32B,mmlu_medical_genetics,0-shot,accuracy,0.91,0.0287623491264661
Qwen/Qwen2.5-32B,mmlu_human_aging,0-shot,accuracy,0.8161434977578476,0.0259983790923565
Qwen/Qwen2.5-32B,mmlu_professional_medicine,0-shot,accuracy,0.8970588235294118,0.0184595189553886
Qwen/Qwen2.5-32B,mmlu_college_medicine,0-shot,accuracy,0.8265895953757225,0.0288681078749706
Qwen/Qwen2.5-32B,mmlu_business_ethics,0-shot,accuracy,0.82,0.0386122919665369
Qwen/Qwen2.5-32B,mmlu_clinical_knowledge,0-shot,accuracy,0.8754716981132076,0.0203213766306962
Qwen/Qwen2.5-32B,mmlu_global_facts,0-shot,accuracy,0.71,0.0456048021572068
Qwen/Qwen2.5-32B,mmlu_virology,0-shot,accuracy,0.572289156626506,0.0385159768371853
Qwen/Qwen2.5-32B,mmlu_professional_accounting,0-shot,accuracy,0.6843971631205674,0.0277249894495093
Qwen/Qwen2.5-32B,mmlu_college_physics,0-shot,accuracy,0.7450980392156863,0.0433643270799318
Qwen/Qwen2.5-32B,mmlu_high_school_physics,0-shot,accuracy,0.7615894039735099,0.0347918557259966
Qwen/Qwen2.5-32B,mmlu_high_school_biology,0-shot,accuracy,0.9516129032258064,0.0122071899922936
Qwen/Qwen2.5-32B,mmlu_college_biology,0-shot,accuracy,0.9375,0.0202421961134779
Qwen/Qwen2.5-32B,mmlu_anatomy,0-shot,accuracy,0.7851851851851852,0.0354785419856082
Qwen/Qwen2.5-32B,mmlu_college_chemistry,0-shot,accuracy,0.62,0.0487831731214563
Qwen/Qwen2.5-32B,mmlu_computer_security,0-shot,accuracy,0.86,0.0348735088019777
Qwen/Qwen2.5-32B,mmlu_college_computer_science,0-shot,accuracy,0.79,0.0409360180740332
Qwen/Qwen2.5-32B,mmlu_astronomy,0-shot,accuracy,0.9407894736842104,0.0192068971968003
Qwen/Qwen2.5-32B,mmlu_college_mathematics,0-shot,accuracy,0.74,0.0440844002276807
Qwen/Qwen2.5-32B,mmlu_conceptual_physics,0-shot,accuracy,0.902127659574468,0.0194247777055733
Qwen/Qwen2.5-32B,mmlu_abstract_algebra,0-shot,accuracy,0.72,0.0451260859854212
Qwen/Qwen2.5-32B,mmlu_high_school_computer_science,0-shot,accuracy,0.94,0.0238683256575941
Qwen/Qwen2.5-32B,mmlu_machine_learning,0-shot,accuracy,0.7946428571428571,0.0383424102141907
Qwen/Qwen2.5-32B,mmlu_high_school_chemistry,0-shot,accuracy,0.7684729064039408,0.0296783331414444
Qwen/Qwen2.5-32B,mmlu_high_school_statistics,0-shot,accuracy,0.8518518518518519,0.0242276292737283
Qwen/Qwen2.5-32B,mmlu_elementary_mathematics,0-shot,accuracy,0.902116402116402,0.0153043742250914
Qwen/Qwen2.5-32B,mmlu_electrical_engineering,0-shot,accuracy,0.8206896551724138,0.0319676643337318
Qwen/Qwen2.5-32B,mmlu_high_school_mathematics,0-shot,accuracy,0.6518518518518519,0.0290456002906162
Qwen/Qwen2.5-32B,arc_challenge,25-shot,accuracy,0.6621160409556314,0.0138220479222835
Qwen/Qwen2.5-32B,arc_challenge,25-shot,acc_norm,0.6911262798634812,0.013501770929344
Qwen/Qwen2.5-32B,hellaswag,10-shot,accuracy,0.6571400119498108,0.0047369508106178
Qwen/Qwen2.5-32B,hellaswag,10-shot,acc_norm,0.8515236008763195,0.003548449054286
Qwen/Qwen2.5-32B,truthfulqa_mc2,0-shot,accuracy,0.5781834572377078,0.014797004312646
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,bleu_max,6.969336348202127,0.5870541151739272
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,bleu_acc,0.1309669522643818,0.0118101095817125
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,bleu_diff,0.6563244381841533,0.422393788389518
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge1_max,14.003241923004378,0.9380420509133744
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge1_acc,0.1395348837209302,0.0121300600895814
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge1_diff,1.0093134943305602,0.5297814254859047
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge2_max,10.188591447651278,0.7985636803484863
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge2_acc,0.1126070991432068,0.0110661303373993
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge2_diff,0.6138278691566204,0.608593814594194
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rougeL_max,13.156036427349836,0.8996246678278078
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rougeL_acc,0.1395348837209302,0.0121300600895814
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rougeL_diff,0.973211591517098,0.5279186710795332
Qwen/Qwen2.5-32B,truthfulqa_mc1,0-shot,accuracy,0.4002447980416156,0.0171516055557491
Qwen/Qwen2.5-32B,winogrande,5-shot,accuracy,0.8184688239936859,0.0108332765150075
Qwen/Qwen2.5-32B,gsm8k,5-shot,accuracy,0.8976497346474602,0.0083491109962088
rinna/bilingual-gpt-neox-4b,arc:challenge,25-shot,accuracy,0.2389078498293515,0.0124610713763166
rinna/bilingual-gpt-neox-4b,arc:challenge,25-shot,acc_norm,0.2918088737201365,0.0132845252924035
rinna/bilingual-gpt-neox-4b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.0416333199893226
rinna/bilingual-gpt-neox-4b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.0416333199893226
rinna/bilingual-gpt-neox-4b,hendrycksTest-anatomy,5-shot,accuracy,0.1851851851851851,0.0335567721631314
rinna/bilingual-gpt-neox-4b,hendrycksTest-anatomy,5-shot,acc_norm,0.1851851851851851,0.0335567721631314
rinna/bilingual-gpt-neox-4b,hendrycksTest-astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
rinna/bilingual-gpt-neox-4b,hendrycksTest-astronomy,5-shot,acc_norm,0.1776315789473684,0.0311031823831233
rinna/bilingual-gpt-neox-4b,hendrycksTest-business_ethics,5-shot,accuracy,0.3,0.0460566186471838
rinna/bilingual-gpt-neox-4b,hendrycksTest-business_ethics,5-shot,acc_norm,0.3,0.0460566186471838
rinna/bilingual-gpt-neox-4b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2150943396226415,0.0252883945028913
rinna/bilingual-gpt-neox-4b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2150943396226415,0.0252883945028913
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.0365394696944209
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.0365394696944209
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.0402015126103684
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.0440844002276808
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.0440844002276808
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_mathematics,5-shot,accuracy,0.2,0.0402015126103684
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.2,0.0402015126103684
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177498
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2080924855491329,0.0309528902177498
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
rinna/bilingual-gpt-neox-4b,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.0451260859854212
rinna/bilingual-gpt-neox-4b,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.0451260859854212
rinna/bilingual-gpt-neox-4b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2638297872340425,0.0288099898541029
rinna/bilingual-gpt-neox-4b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2638297872340425,0.0288099898541029
rinna/bilingual-gpt-neox-4b,hendrycksTest-econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
rinna/bilingual-gpt-neox-4b,hendrycksTest-econometrics,5-shot,acc_norm,0.2368421052631578,0.0399942387928133
rinna/bilingual-gpt-neox-4b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.035659981741353
rinna/bilingual-gpt-neox-4b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.035659981741353
rinna/bilingual-gpt-neox-4b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2089947089947089,0.0209404815653348
rinna/bilingual-gpt-neox-4b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2089947089947089,0.0209404815653348
rinna/bilingual-gpt-neox-4b,hendrycksTest-formal_logic,5-shot,accuracy,0.2857142857142857,0.0404061017820884
rinna/bilingual-gpt-neox-4b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2857142857142857,0.0404061017820884
rinna/bilingual-gpt-neox-4b,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.0386122919665369
rinna/bilingual-gpt-neox-4b,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.0386122919665369
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_biology,5-shot,accuracy,0.1774193548387097,0.0217325406893292
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.1774193548387097,0.0217325406893292
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.1527093596059113,0.0253089045393806
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.1527093596059113,0.0253089045393806
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.25,0.0435194139889244
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.25,0.0435194139889244
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2181818181818181,0.0322507810830628
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2181818181818181,0.0322507810830628
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_geography,5-shot,accuracy,0.1767676767676767,0.0271787526390449
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1767676767676767,0.0271787526390449
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.1968911917098445,0.0286978739718606
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.1968911917098445,0.0286978739718606
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2025641025641025,0.0203776609703713
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2025641025641025,0.0203776609703713
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2111111111111111,0.024882116857655
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2111111111111111,0.024882116857655
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2100840336134453,0.0264613987174718
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2100840336134453,0.0264613987174718
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_physics,5-shot,accuracy,0.1986754966887417,0.0325784738443677
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.1986754966887417,0.0325784738443677
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1926605504587156,0.016909276884936
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1926605504587156,0.016909276884936
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1527777777777778,0.0245363260261342
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1527777777777778,0.0245363260261342
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.0303915336927415
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.0303915336927415
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.0289007219062934
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.0289007219062934
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_aging,5-shot,accuracy,0.3139013452914798,0.0311467964829724
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_aging,5-shot,acc_norm,0.3139013452914798,0.0311467964829724
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.0384487613978527
rinna/bilingual-gpt-neox-4b,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.0389687898507041
rinna/bilingual-gpt-neox-4b,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.0389687898507041
rinna/bilingual-gpt-neox-4b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2592592592592592,0.0423651125809463
rinna/bilingual-gpt-neox-4b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2592592592592592,0.0423651125809463
rinna/bilingual-gpt-neox-4b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2208588957055214,0.0325917739274217
rinna/bilingual-gpt-neox-4b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2208588957055214,0.0325917739274217
rinna/bilingual-gpt-neox-4b,hendrycksTest-machine_learning,5-shot,accuracy,0.3125,0.0439946505757152
rinna/bilingual-gpt-neox-4b,hendrycksTest-machine_learning,5-shot,acc_norm,0.3125,0.0439946505757152
rinna/bilingual-gpt-neox-4b,hendrycksTest-management,5-shot,accuracy,0.174757281553398,0.0376017800602662
rinna/bilingual-gpt-neox-4b,hendrycksTest-management,5-shot,acc_norm,0.174757281553398,0.0376017800602662
rinna/bilingual-gpt-neox-4b,hendrycksTest-marketing,5-shot,accuracy,0.2905982905982906,0.029745048572674
rinna/bilingual-gpt-neox-4b,hendrycksTest-marketing,5-shot,acc_norm,0.2905982905982906,0.029745048572674
rinna/bilingual-gpt-neox-4b,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.0460566186471838
rinna/bilingual-gpt-neox-4b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.0460566186471838
rinna/bilingual-gpt-neox-4b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2375478927203065,0.0152187330461501
rinna/bilingual-gpt-neox-4b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2375478927203065,0.0152187330461501
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2485549132947976,0.0232675284321001
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2485549132947976,0.0232675284321001
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2379888268156424,0.0142426300705749
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2379888268156424,0.0142426300705749
rinna/bilingual-gpt-neox-4b,hendrycksTest-nutrition,5-shot,accuracy,0.2254901960784313,0.0239291555173512
rinna/bilingual-gpt-neox-4b,hendrycksTest-nutrition,5-shot,acc_norm,0.2254901960784313,0.0239291555173512
rinna/bilingual-gpt-neox-4b,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.0221224397724807
rinna/bilingual-gpt-neox-4b,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.0221224397724807
rinna/bilingual-gpt-neox-4b,hendrycksTest-prehistory,5-shot,accuracy,0.2160493827160493,0.0228991629184458
rinna/bilingual-gpt-neox-4b,hendrycksTest-prehistory,5-shot,acc_norm,0.2160493827160493,0.0228991629184458
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2340425531914893,0.0252578613594324
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2340425531914893,0.0252578613594324
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.0109961566351426
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.0109961566351426
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_medicine,5-shot,accuracy,0.1838235294117647,0.0235292421851931
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1838235294117647,0.0235292421851931
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_psychology,5-shot,accuracy,0.25,0.0175178188450144
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25,0.0175178188450144
rinna/bilingual-gpt-neox-4b,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
rinna/bilingual-gpt-neox-4b,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
rinna/bilingual-gpt-neox-4b,hendrycksTest-security_studies,5-shot,accuracy,0.1877551020408163,0.0250002560395462
rinna/bilingual-gpt-neox-4b,hendrycksTest-security_studies,5-shot,acc_norm,0.1877551020408163,0.0250002560395462
rinna/bilingual-gpt-neox-4b,hendrycksTest-sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
rinna/bilingual-gpt-neox-4b,hendrycksTest-sociology,5-shot,acc_norm,0.2437810945273631,0.0303604901540146
rinna/bilingual-gpt-neox-4b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.0451260859854212
rinna/bilingual-gpt-neox-4b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.0451260859854212
rinna/bilingual-gpt-neox-4b,hendrycksTest-virology,5-shot,accuracy,0.2831325301204819,0.0350729543137051
rinna/bilingual-gpt-neox-4b,hendrycksTest-virology,5-shot,acc_norm,0.2831325301204819,0.0350729543137051
rinna/bilingual-gpt-neox-4b,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.0358252944257312
rinna/bilingual-gpt-neox-4b,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.0358252944257312
rinna/bilingual-gpt-neox-4b,truthfulqa:mc,0-shot,mc1,0.2460220318237454,0.0150772192006625
rinna/bilingual-gpt-neox-4b,truthfulqa:mc,0-shot,mc2,0.4500298950094902,0.0149974128395491
rinna/bilingual-gpt-neox-4b,drop,3-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,drop,3-shot,f1,0.0019494546979865,0.0001656985868155
facebook/opt-2.7b,minerva_math_precalc,5-shot,accuracy,0.0238095238095238,0.0065304692197614
facebook/opt-2.7b,minerva_math_prealgebra,5-shot,accuracy,0.0149253731343283,0.0041109059285056
facebook/opt-2.7b,minerva_math_num_theory,5-shot,accuracy,0.0148148148148148,0.0052037049875126
facebook/opt-2.7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.0049000930886157
facebook/opt-2.7b,minerva_math_geometry,5-shot,accuracy,0.0125260960334029,0.0050869413896779
facebook/opt-2.7b,minerva_math_counting_and_prob,5-shot,accuracy,0.010548523206751,0.0046974537353761
facebook/opt-2.7b,minerva_math_algebra,5-shot,accuracy,0.0151642796967144,0.0035485460431325
facebook/opt-2.7b,fld_default,0-shot,accuracy,0.0,
facebook/opt-2.7b,fld_star,0-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521
facebook/opt-2.7b,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339
facebook/opt-2.7b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
facebook/opt-2.7b,arithmetic_2ds,5-shot,accuracy,0.0135,0.0025811249685073
facebook/opt-2.7b,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339
facebook/opt-2.7b,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_2dm,5-shot,accuracy,0.0215,0.0032440926417928
facebook/opt-2.7b,arithmetic_2da,5-shot,accuracy,0.011,0.0023328568559933
facebook/opt-2.7b,gsm8k_cot,5-shot,accuracy,0.0212282031842304,0.0039704491298486
facebook/opt-2.7b,anli_r2,0-shot,brier_score,0.9806575981542668,
facebook/opt-2.7b,anli_r3,0-shot,brier_score,0.8989579823804109,
facebook/opt-2.7b,anli_r1,0-shot,brier_score,0.9835816196646928,
facebook/opt-2.7b,xnli_eu,0-shot,brier_score,1.0610127557895537,
facebook/opt-2.7b,xnli_vi,0-shot,brier_score,0.9078406353013024,
facebook/opt-2.7b,xnli_ru,0-shot,brier_score,0.8321734481428278,
facebook/opt-2.7b,xnli_zh,0-shot,brier_score,1.1784647036176363,
facebook/opt-2.7b,xnli_tr,0-shot,brier_score,0.8894841423618519,
facebook/opt-2.7b,xnli_fr,0-shot,brier_score,0.8313152226484499,
facebook/opt-2.7b,xnli_en,0-shot,brier_score,0.6116601747186954,
facebook/opt-2.7b,xnli_ur,0-shot,brier_score,1.322704035007194,
facebook/opt-2.7b,xnli_ar,0-shot,brier_score,0.9110827039215548,
facebook/opt-2.7b,xnli_de,0-shot,brier_score,0.8552219959870653,
facebook/opt-2.7b,xnli_hi,0-shot,brier_score,0.9737996476054792,
facebook/opt-2.7b,xnli_es,0-shot,brier_score,0.8705426231888244,
facebook/opt-2.7b,xnli_bg,0-shot,brier_score,0.9735045902629904,
facebook/opt-2.7b,xnli_sw,0-shot,brier_score,0.8732903791230823,
facebook/opt-2.7b,xnli_el,0-shot,brier_score,1.1872540651461676,
facebook/opt-2.7b,xnli_th,0-shot,brier_score,1.2687537528892334,
facebook/opt-2.7b,logiqa2,0-shot,brier_score,1.2134396565953744,
facebook/opt-2.7b,mathqa,0-shot,brier_score,0.9929214264531075,
facebook/opt-2.7b,lambada_standard,0-shot,perplexity,7.415927761731489,0.1967566897252187
facebook/opt-2.7b,lambada_standard,0-shot,accuracy,0.5592858529012226,0.0069168361138352
facebook/opt-2.7b,lambada_openai,0-shot,perplexity,5.119225735809997,0.119744800718221
facebook/opt-2.7b,lambada_openai,0-shot,accuracy,0.6353580438579468,0.0067058627120832
facebook/opt-2.7b,mmlu_world_religions,0-shot,accuracy,0.2222222222222222,0.0318857801768639
facebook/opt-2.7b,mmlu_formal_logic,0-shot,accuracy,0.1904761904761904,0.0351220741230205
facebook/opt-2.7b,mmlu_prehistory,0-shot,accuracy,0.2530864197530864,0.024191808600713
facebook/opt-2.7b,mmlu_moral_scenarios,0-shot,accuracy,0.2346368715083799,0.0141730440983036
facebook/opt-2.7b,mmlu_high_school_world_history,0-shot,accuracy,0.2447257383966244,0.0279856993870364
facebook/opt-2.7b,mmlu_moral_disputes,0-shot,accuracy,0.2543352601156069,0.0234458262765455
facebook/opt-2.7b,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.0110641510271654
facebook/opt-2.7b,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.0332201579577674
facebook/opt-2.7b,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.0305875913516042
facebook/opt-2.7b,mmlu_philosophy,0-shot,accuracy,0.3247588424437299,0.026596782287697
facebook/opt-2.7b,mmlu_jurisprudence,0-shot,accuracy,0.25,0.041860917913946
facebook/opt-2.7b,mmlu_international_law,0-shot,accuracy,0.2479338842975206,0.039418975265163
facebook/opt-2.7b,mmlu_high_school_european_history,0-shot,accuracy,0.2303030303030303,0.0328766675860348
facebook/opt-2.7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3523316062176165,0.0344747828641435
facebook/opt-2.7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2184873949579832,0.0268415143229589
facebook/opt-2.7b,mmlu_high_school_geography,0-shot,accuracy,0.2121212121212121,0.0291265228345868
facebook/opt-2.7b,mmlu_high_school_psychology,0-shot,accuracy,0.3155963302752294,0.0199261175138696
facebook/opt-2.7b,mmlu_public_relations,0-shot,accuracy,0.2272727272727272,0.0401396455407277
facebook/opt-2.7b,mmlu_us_foreign_policy,0-shot,accuracy,0.25,0.0435194139889244
facebook/opt-2.7b,mmlu_sociology,0-shot,accuracy,0.263681592039801,0.0311571508693555
facebook/opt-2.7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.358974358974359,0.0243217384846023
facebook/opt-2.7b,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904
facebook/opt-2.7b,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.0179179740695947
facebook/opt-2.7b,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.0372767357559691
facebook/opt-2.7b,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
facebook/opt-2.7b,mmlu_miscellaneous,0-shot,accuracy,0.2337164750957854,0.0151333832789888
facebook/opt-2.7b,mmlu_marketing,0-shot,accuracy,0.2307692307692307,0.0276019213814176
facebook/opt-2.7b,mmlu_management,0-shot,accuracy,0.3883495145631068,0.0482572933735639
facebook/opt-2.7b,mmlu_nutrition,0-shot,accuracy,0.2320261437908496,0.024170840879341
facebook/opt-2.7b,mmlu_medical_genetics,0-shot,accuracy,0.33,0.047258156262526
facebook/opt-2.7b,mmlu_human_aging,0-shot,accuracy,0.2017937219730941,0.0269361119128022
facebook/opt-2.7b,mmlu_professional_medicine,0-shot,accuracy,0.3345588235294117,0.0286619962023353
facebook/opt-2.7b,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
facebook/opt-2.7b,mmlu_business_ethics,0-shot,accuracy,0.19,0.0394277244403662
facebook/opt-2.7b,mmlu_clinical_knowledge,0-shot,accuracy,0.2113207547169811,0.0251257664848278
facebook/opt-2.7b,mmlu_global_facts,0-shot,accuracy,0.36,0.0482418151324421
facebook/opt-2.7b,mmlu_virology,0-shot,accuracy,0.216867469879518,0.0320828445035636
facebook/opt-2.7b,mmlu_professional_accounting,0-shot,accuracy,0.2872340425531915,0.0269921991730643
facebook/opt-2.7b,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
facebook/opt-2.7b,mmlu_high_school_physics,0-shot,accuracy,0.3112582781456953,0.0378044585052673
facebook/opt-2.7b,mmlu_high_school_biology,0-shot,accuracy,0.2161290322580645,0.0234152934335685
facebook/opt-2.7b,mmlu_college_biology,0-shot,accuracy,0.2152777777777778,0.0343707934410613
facebook/opt-2.7b,mmlu_anatomy,0-shot,accuracy,0.2814814814814815,0.0388500424580025
facebook/opt-2.7b,mmlu_college_chemistry,0-shot,accuracy,0.21,0.0409360180740332
facebook/opt-2.7b,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276808
facebook/opt-2.7b,mmlu_college_computer_science,0-shot,accuracy,0.3,0.0460566186471838
facebook/opt-2.7b,mmlu_astronomy,0-shot,accuracy,0.1907894736842105,0.031975658210325
facebook/opt-2.7b,mmlu_college_mathematics,0-shot,accuracy,0.23,0.042295258468165
facebook/opt-2.7b,mmlu_conceptual_physics,0-shot,accuracy,0.2382978723404255,0.0278512529738898
facebook/opt-2.7b,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.0409360180740332
facebook/opt-2.7b,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.0476095228569523
facebook/opt-2.7b,mmlu_machine_learning,0-shot,accuracy,0.2589285714285714,0.0415775153986562
facebook/opt-2.7b,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.0312709071329769
facebook/opt-2.7b,mmlu_high_school_statistics,0-shot,accuracy,0.3981481481481481,0.033384734032074
facebook/opt-2.7b,mmlu_elementary_mathematics,0-shot,accuracy,0.2407407407407407,0.0220190800122178
facebook/opt-2.7b,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.0375283395800333
facebook/opt-2.7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2851851851851852,0.0275285992103404
facebook/opt-2.7b,arc_challenge,25-shot,accuracy,0.3139931740614334,0.0135626912247262
facebook/opt-2.7b,arc_challenge,25-shot,acc_norm,0.3455631399317406,0.0138969384611456
facebook/opt-2.7b,truthfulqa_mc2,0-shot,accuracy,0.3763787086134506,0.0138095147427225
facebook/opt-2.7b,truthfulqa_gen,0-shot,bleu_max,21.936813599784077,0.6915463833520875
facebook/opt-2.7b,truthfulqa_gen,0-shot,bleu_acc,0.2827417380660955,0.0157647708367773
facebook/opt-2.7b,truthfulqa_gen,0-shot,bleu_diff,-7.93706461416589,0.6942322145010175
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge1_max,46.56471474176105,0.8452800285319477
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge1_acc,0.2619339045287637,0.015392118805015
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge1_diff,-10.43421160381809,0.7865219397875223
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge2_max,29.59058433337149,0.9351396896455382
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge2_acc,0.1921664626682986,0.0137928704806289
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge2_diff,-12.224344822664076,0.8785968206293998
facebook/opt-2.7b,truthfulqa_gen,0-shot,rougeL_max,43.69031715340036,0.8461881443382573
facebook/opt-2.7b,truthfulqa_gen,0-shot,rougeL_acc,0.2472460220318237,0.0151024047973596
facebook/opt-2.7b,truthfulqa_gen,0-shot,rougeL_diff,-10.79728624233618,0.7775003821253601
facebook/opt-2.7b,truthfulqa_mc1,0-shot,accuracy,0.2239902080783353,0.0145949643294742
Salesforce/codegen-16B-mono,anli_r2,0-shot,brier_score,0.8253824795424403,
Salesforce/codegen-16B-mono,anli_r3,0-shot,brier_score,0.7940354639454789,
Salesforce/codegen-16B-mono,anli_r1,0-shot,brier_score,0.8429583560477731,
Salesforce/codegen-16B-mono,xnli_eu,0-shot,brier_score,1.0481738865590222,
Salesforce/codegen-16B-mono,xnli_vi,0-shot,brier_score,1.0622085188319774,
Salesforce/codegen-16B-mono,xnli_ru,0-shot,brier_score,0.8720664022610117,
Salesforce/codegen-16B-mono,xnli_zh,0-shot,brier_score,0.9990022713619396,
Salesforce/codegen-16B-mono,xnli_tr,0-shot,brier_score,0.9070160533682528,
Salesforce/codegen-16B-mono,xnli_fr,0-shot,brier_score,0.938508643895241,
Salesforce/codegen-16B-mono,xnli_en,0-shot,brier_score,0.7700595581688592,
Salesforce/codegen-16B-mono,xnli_ur,0-shot,brier_score,1.3209693093229593,
Salesforce/codegen-16B-mono,xnli_ar,0-shot,brier_score,1.082490658523419,
Salesforce/codegen-16B-mono,xnli_de,0-shot,brier_score,1.043589609239351,
Salesforce/codegen-16B-mono,xnli_hi,0-shot,brier_score,0.99796012320528,
Salesforce/codegen-16B-mono,xnli_es,0-shot,brier_score,0.9189766913317708,
Salesforce/codegen-16B-mono,xnli_bg,0-shot,brier_score,0.998109809205014,
Salesforce/codegen-16B-mono,xnli_sw,0-shot,brier_score,0.9858592239662022,
Salesforce/codegen-16B-mono,xnli_el,0-shot,brier_score,0.8942569399409489,
Salesforce/codegen-16B-mono,xnli_th,0-shot,brier_score,0.8555323255757882,
Salesforce/codegen-16B-mono,logiqa2,0-shot,brier_score,1.0900452349971228,
Salesforce/codegen-16B-mono,mathqa,0-shot,brier_score,0.963194292535674,
Salesforce/codegen-16B-mono,lambada_standard,0-shot,perplexity,40.81985306298772,1.485975941096083
Salesforce/codegen-16B-mono,lambada_standard,0-shot,accuracy,0.3052590723850184,0.0064159032309226
Salesforce/codegen-16B-mono,lambada_openai,0-shot,perplexity,27.0009985482394,0.9699976694126962
Salesforce/codegen-16B-mono,lambada_openai,0-shot,accuracy,0.3592082282165729,0.0066841113199758
Salesforce/codegen-16B-mono,mmlu_world_religions,0-shot,accuracy,0.2748538011695906,0.0342404292469158
Salesforce/codegen-16B-mono,mmlu_formal_logic,0-shot,accuracy,0.2063492063492063,0.0361960452412425
Salesforce/codegen-16B-mono,mmlu_prehistory,0-shot,accuracy,0.2839506172839506,0.0250894785237651
Salesforce/codegen-16B-mono,mmlu_moral_scenarios,0-shot,accuracy,0.235754189944134,0.0141963756862908
Salesforce/codegen-16B-mono,mmlu_high_school_world_history,0-shot,accuracy,0.2531645569620253,0.0283046579430352
Salesforce/codegen-16B-mono,mmlu_moral_disputes,0-shot,accuracy,0.2601156069364161,0.0236186783100693
Salesforce/codegen-16B-mono,mmlu_professional_law,0-shot,accuracy,0.2535853976531942,0.0111117153361011
Salesforce/codegen-16B-mono,mmlu_logical_fallacies,0-shot,accuracy,0.2208588957055214,0.0325917739274217
Salesforce/codegen-16B-mono,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.0303915336927415
Salesforce/codegen-16B-mono,mmlu_philosophy,0-shot,accuracy,0.270096463022508,0.0252180403734106
Salesforce/codegen-16B-mono,mmlu_jurisprudence,0-shot,accuracy,0.2685185185185185,0.0428446796805219
Salesforce/codegen-16B-mono,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.040261875275912
Salesforce/codegen-16B-mono,mmlu_high_school_european_history,0-shot,accuracy,0.2242424242424242,0.032568666616811
Salesforce/codegen-16B-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2072538860103626,0.0292528232918036
Salesforce/codegen-16B-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.2352941176470588,0.0275536144678637
Salesforce/codegen-16B-mono,mmlu_high_school_geography,0-shot,accuracy,0.1919191919191919,0.028057791672989
Salesforce/codegen-16B-mono,mmlu_high_school_psychology,0-shot,accuracy,0.2330275229357798,0.0181256691808615
Salesforce/codegen-16B-mono,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.0426579211094058
Salesforce/codegen-16B-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-16B-mono,mmlu_sociology,0-shot,accuracy,0.2985074626865671,0.0323574378935504
Salesforce/codegen-16B-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2256410256410256,0.0211936325251485
Salesforce/codegen-16B-mono,mmlu_security_studies,0-shot,accuracy,0.2204081632653061,0.0265370453121452
Salesforce/codegen-16B-mono,mmlu_professional_psychology,0-shot,accuracy,0.2598039215686274,0.0177408995091777
Salesforce/codegen-16B-mono,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.0391534540884783
Salesforce/codegen-16B-mono,mmlu_econometrics,0-shot,accuracy,0.2456140350877192,0.0404933929774814
Salesforce/codegen-16B-mono,mmlu_miscellaneous,0-shot,accuracy,0.2656449553001277,0.0157943024878887
Salesforce/codegen-16B-mono,mmlu_marketing,0-shot,accuracy,0.2863247863247863,0.0296143236904566
Salesforce/codegen-16B-mono,mmlu_management,0-shot,accuracy,0.2233009708737864,0.0412355318989143
Salesforce/codegen-16B-mono,mmlu_nutrition,0-shot,accuracy,0.2549019607843137,0.0249541843248799
Salesforce/codegen-16B-mono,mmlu_medical_genetics,0-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-16B-mono,mmlu_human_aging,0-shot,accuracy,0.3542600896860987,0.0321006215413498
Salesforce/codegen-16B-mono,mmlu_professional_medicine,0-shot,accuracy,0.2058823529411764,0.0245622043141423
Salesforce/codegen-16B-mono,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.0318620985164114
Salesforce/codegen-16B-mono,mmlu_business_ethics,0-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-16B-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.2188679245283019,0.0254478638251086
Salesforce/codegen-16B-mono,mmlu_global_facts,0-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-16B-mono,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
Salesforce/codegen-16B-mono,mmlu_professional_accounting,0-shot,accuracy,0.2659574468085106,0.0263580656988805
Salesforce/codegen-16B-mono,mmlu_college_physics,0-shot,accuracy,0.1862745098039215,0.0387395871414935
Salesforce/codegen-16B-mono,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.0368488152138902
Salesforce/codegen-16B-mono,mmlu_high_school_biology,0-shot,accuracy,0.1903225806451612,0.022331707611823
Salesforce/codegen-16B-mono,mmlu_college_biology,0-shot,accuracy,0.25,0.036210341218895
Salesforce/codegen-16B-mono,mmlu_anatomy,0-shot,accuracy,0.2222222222222222,0.0359144408419696
Salesforce/codegen-16B-mono,mmlu_college_chemistry,0-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-16B-mono,mmlu_computer_security,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-16B-mono,mmlu_college_computer_science,0-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-16B-mono,mmlu_astronomy,0-shot,accuracy,0.2039473684210526,0.0327900040631005
Salesforce/codegen-16B-mono,mmlu_college_mathematics,0-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-16B-mono,mmlu_conceptual_physics,0-shot,accuracy,0.2978723404255319,0.0298961456820954
Salesforce/codegen-16B-mono,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-16B-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-16B-mono,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.0428785875134045
Salesforce/codegen-16B-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.2019704433497537,0.0282473501221802
Salesforce/codegen-16B-mono,mmlu_high_school_statistics,0-shot,accuracy,0.2037037037037037,0.0274674018040579
Salesforce/codegen-16B-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.2354497354497354,0.0218515098220317
Salesforce/codegen-16B-mono,mmlu_electrical_engineering,0-shot,accuracy,0.2896551724137931,0.0378001923043801
Salesforce/codegen-16B-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.2074074074074074,0.0247207131939521
Salesforce/codegen-16B-mono,arc_challenge,25-shot,accuracy,0.2696245733788396,0.0129680406868691
Salesforce/codegen-16B-mono,arc_challenge,25-shot,acc_norm,0.295221843003413,0.0133297502933823
Salesforce/codegen-16B-mono,hellaswag,10-shot,accuracy,0.3579964150567616,0.0047843129724953
Salesforce/codegen-16B-mono,hellaswag,10-shot,acc_norm,0.4473212507468632,0.0049620103382263
Salesforce/codegen-16B-mono,truthfulqa_mc2,0-shot,accuracy,0.4061093580338541,0.0151364377735347
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,bleu_max,19.57158267860521,0.672039191352111
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,bleu_acc,0.3023255813953488,0.016077509266133
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,bleu_diff,-5.301786631798468,0.658779186406353
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge1_max,43.18409379824872,0.8482447875377391
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge1_acc,0.2802937576499388,0.0157231395246087
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge1_diff,-7.480158262440118,0.8228031808206397
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge2_max,26.21877195377042,0.9378604257610168
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge2_acc,0.2056303549571603,0.0141484822194609
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge2_diff,-8.570941233813052,0.9197989857896314
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rougeL_max,40.08679096043078,0.8489879245723853
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rougeL_acc,0.2864137086903305,0.0158261424395023
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rougeL_diff,-7.518785257278961,0.8301172921913628
Salesforce/codegen-16B-mono,truthfulqa_mc1,0-shot,accuracy,0.2435740514075887,0.0150263548249107
Salesforce/codegen-16B-mono,winogrande,5-shot,accuracy,0.5674822415153907,0.0139239115786238
Salesforce/codegen-16B-mono,gsm8k,5-shot,accuracy,0.026535253980288,0.0044270459872651
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_precalc,5-shot,accuracy,0.1117216117216117,0.0134941300997326
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_prealgebra,5-shot,accuracy,0.4764638346727899,0.016932796474939
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_num_theory,5-shot,accuracy,0.2037037037037037,0.0173477209637619
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_intermediate_algebra,5-shot,accuracy,0.1207087486157253,0.010847570493593
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_geometry,5-shot,accuracy,0.1962421711899791,0.0181653943288506
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_counting_and_prob,5-shot,accuracy,0.2447257383966244,0.0197679482693528
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_algebra,5-shot,accuracy,0.3782645324347093,0.0140818037640229
meta-llama/Meta-Llama-3-8B-Instruct,fld_default,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B-Instruct,fld_star,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_3da,5-shot,accuracy,0.9995,0.0005
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_3ds,5-shot,accuracy,0.8855,0.007121814032784
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_4da,5-shot,accuracy,0.8945,0.0068708426877363
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_2ds,5-shot,accuracy,0.997,0.0012232122154647
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_5ds,5-shot,accuracy,0.7225,0.0100148401640644
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_5da,5-shot,accuracy,0.795,0.009029300312431
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_1dc,5-shot,accuracy,0.7925,0.0090698956169987
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_4ds,5-shot,accuracy,0.8155,0.0086756849155773
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_2dm,5-shot,accuracy,0.67,0.0105169055644389
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_2da,5-shot,accuracy,1.0,
meta-llama/Meta-Llama-3-8B-Instruct,gsm8k_cot,5-shot,accuracy,0.775587566338135,0.0114916177566305
meta-llama/Meta-Llama-3-8B-Instruct,anli_r2,0-shot,brier_score,0.7985456079098852,
meta-llama/Meta-Llama-3-8B-Instruct,anli_r3,0-shot,brier_score,0.8097953259921752,
meta-llama/Meta-Llama-3-8B-Instruct,anli_r1,0-shot,brier_score,0.7717221698804513,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_eu,0-shot,brier_score,0.8196432026111591,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_vi,0-shot,brier_score,0.7618000692371746,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_ru,0-shot,brier_score,0.8013434537191864,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_zh,0-shot,brier_score,0.9930200253772428,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_tr,0-shot,brier_score,0.8615816405837291,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_fr,0-shot,brier_score,0.6960972021301648,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_en,0-shot,brier_score,0.6514935237404581,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_ur,0-shot,brier_score,1.2113932574417663,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_ar,0-shot,brier_score,1.2690322872660167,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_de,0-shot,brier_score,0.8339566525731148,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_hi,0-shot,brier_score,0.8549027587669167,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_es,0-shot,brier_score,0.8028087096573583,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_bg,0-shot,brier_score,0.8658614845860327,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_sw,0-shot,brier_score,1.0179283740266234,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_el,0-shot,brier_score,0.8840107296653167,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_th,0-shot,brier_score,0.8332666686094854,
meta-llama/Meta-Llama-3-8B-Instruct,logiqa2,0-shot,brier_score,0.9980174455769028,
meta-llama/Meta-Llama-3-8B-Instruct,mathqa,0-shot,brier_score,0.8366526946512588,
meta-llama/Meta-Llama-3-8B-Instruct,lambada_standard,0-shot,perplexity,4.010364094889167,0.1026959868237609
meta-llama/Meta-Llama-3-8B-Instruct,lambada_standard,0-shot,accuracy,0.6501067339413934,0.0066446513367465
meta-llama/Meta-Llama-3-8B-Instruct,lambada_openai,0-shot,perplexity,3.104536403718183,0.0766234707086666
meta-llama/Meta-Llama-3-8B-Instruct,lambada_openai,0-shot,accuracy,0.7189986415680186,0.0062622487891643
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_world_religions,0-shot,accuracy,0.783625730994152,0.0315814953933873
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_formal_logic,0-shot,accuracy,0.492063492063492,0.0447157253629434
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_prehistory,0-shot,accuracy,0.7407407407407407,0.0243836655310354
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_moral_scenarios,0-shot,accuracy,0.4346368715083799,0.0165789974354967
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_world_history,0-shot,accuracy,0.8438818565400844,0.0236271594603186
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_moral_disputes,0-shot,accuracy,0.7456647398843931,0.0234458262765455
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_law,0-shot,accuracy,0.4784876140808344,0.0127584109410389
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_us_history,0-shot,accuracy,0.8529411764705882,0.0248574780802504
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_philosophy,0-shot,accuracy,0.7202572347266881,0.0254942593506949
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_jurisprudence,0-shot,accuracy,0.7777777777777778,0.0401910747255735
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_international_law,0-shot,accuracy,0.8181818181818182,0.0352089395109765
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_european_history,0-shot,accuracy,0.7515151515151515,0.033744026441394
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_government_and_politics,0-shot,accuracy,0.911917098445596,0.020453746601601
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_microeconomics,0-shot,accuracy,0.7647058823529411,0.0275536144678638
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_geography,0-shot,accuracy,0.8434343434343434,0.0258905203581414
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_psychology,0-shot,accuracy,0.8311926605504587,0.0160600562685303
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_public_relations,0-shot,accuracy,0.6363636363636364,0.0460758209071997
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_us_foreign_policy,0-shot,accuracy,0.85,0.0358870281282637
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_sociology,0-shot,accuracy,0.8706467661691543,0.0237298308810185
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_macroeconomics,0-shot,accuracy,0.658974358974359,0.024035489676335
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_security_studies,0-shot,accuracy,0.7428571428571429,0.0279798235387445
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_psychology,0-shot,accuracy,0.7091503267973857,0.0183731169159039
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_human_sexuality,0-shot,accuracy,0.7786259541984732,0.0364129708131372
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_econometrics,0-shot,accuracy,0.6140350877192983,0.0457963942207043
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_miscellaneous,0-shot,accuracy,0.7994891443167306,0.0143176537085942
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_marketing,0-shot,accuracy,0.905982905982906,0.0191198927989249
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_management,0-shot,accuracy,0.7766990291262136,0.0412355318989143
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_nutrition,0-shot,accuracy,0.7516339869281046,0.0247399813551135
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_medical_genetics,0-shot,accuracy,0.8,0.0402015126103684
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_human_aging,0-shot,accuracy,0.7309417040358744,0.0297637794068749
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_medicine,0-shot,accuracy,0.7242647058823529,0.0271462719366251
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_medicine,0-shot,accuracy,0.6416184971098265,0.0365634365335316
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_business_ethics,0-shot,accuracy,0.69,0.0464823198711731
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_clinical_knowledge,0-shot,accuracy,0.7471698113207547,0.0267498997712412
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_global_facts,0-shot,accuracy,0.4,0.049236596391733
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_virology,0-shot,accuracy,0.5060240963855421,0.0389221219533304
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_accounting,0-shot,accuracy,0.5390070921985816,0.0297365925264244
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_physics,0-shot,accuracy,0.5098039215686274,0.0497422946042281
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_physics,0-shot,accuracy,0.4437086092715231,0.0405652790228173
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_biology,0-shot,accuracy,0.7806451612903226,0.0235407993587233
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_biology,0-shot,accuracy,0.7916666666666666,0.0339611620584533
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_anatomy,0-shot,accuracy,0.6370370370370371,0.041539484047424
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_chemistry,0-shot,accuracy,0.47,0.0501613558046591
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_computer_security,0-shot,accuracy,0.77,0.042295258468165
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_computer_science,0-shot,accuracy,0.58,0.0496044963748858
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_astronomy,0-shot,accuracy,0.6973684210526315,0.0373852067611966
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_mathematics,0-shot,accuracy,0.38,0.0487831731214563
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_conceptual_physics,0-shot,accuracy,0.6042553191489362,0.0319675869783536
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_abstract_algebra,0-shot,accuracy,0.32,0.046882617226215
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_computer_science,0-shot,accuracy,0.75,0.0435194139889244
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_machine_learning,0-shot,accuracy,0.5446428571428571,0.0472683555371909
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_chemistry,0-shot,accuracy,0.5024630541871922,0.0351794503869106
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_statistics,0-shot,accuracy,0.5370370370370371,0.0340060362553827
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_elementary_mathematics,0-shot,accuracy,0.4470899470899471,0.025606723995777
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_electrical_engineering,0-shot,accuracy,0.6275862068965518,0.0402873153294755
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_mathematics,0-shot,accuracy,0.3925925925925926,0.0297738470125329
meta-llama/Meta-Llama-3-8B-Instruct,arc_challenge,25-shot,accuracy,0.5767918088737202,0.014438036220848
meta-llama/Meta-Llama-3-8B-Instruct,arc_challenge,25-shot,acc_norm,0.6186006825938567,0.0141943890866852
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_mc2,0-shot,accuracy,0.5165834108450839,0.0151884805301017
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,bleu_max,20.38284601416647,0.7259585799496531
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,bleu_acc,0.4749082007343941,0.017481446804104
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,bleu_diff,-0.3982597967274656,0.6399548657857169
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge1_max,43.70636561691526,0.8683149503828073
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge1_acc,0.4969400244798042,0.0175031732609606
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge1_diff,-0.267087450778999,0.8748649880343018
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge2_max,27.58382289527688,0.9560493672304742
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge2_acc,0.3647490820073439,0.0168509610617201
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge2_diff,-1.7302074043155249,0.928098862686909
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rougeL_max,40.83179541506895,0.8672456167019427
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rougeL_acc,0.4847001223990208,0.0174953044731879
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rougeL_diff,-0.6070224090190008,0.8794043276943261
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_mc1,0-shot,accuracy,0.3647490820073439,0.0168509610617201
mosaicml/mpt-7b-storywriter,minerva_math_precalc,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_prealgebra,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_num_theory,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_geometry,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_algebra,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,fld_default,0-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,fld_star,0-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
mosaicml/mpt-7b-storywriter,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
mosaicml/mpt-7b-storywriter,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
mosaicml/mpt-7b-storywriter,arithmetic_2ds,5-shot,accuracy,0.0125,0.0024849471787626
mosaicml/mpt-7b-storywriter,arithmetic_5ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_5da,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_1dc,5-shot,accuracy,0.0055,0.0016541593398342
mosaicml/mpt-7b-storywriter,arithmetic_4ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_2dm,5-shot,accuracy,0.0265,0.0035923985947876
mosaicml/mpt-7b-storywriter,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237
mosaicml/mpt-7b-storywriter,gsm8k_cot,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,anli_r2,0-shot,brier_score,0.8579585824630889,
mosaicml/mpt-7b-storywriter,anli_r3,0-shot,brier_score,0.8532457528157426,
mosaicml/mpt-7b-storywriter,anli_r1,0-shot,brier_score,0.8446244791915563,
mosaicml/mpt-7b-storywriter,xnli_eu,0-shot,brier_score,1.2168588145018897,
mosaicml/mpt-7b-storywriter,xnli_vi,0-shot,brier_score,1.2991601321455832,
mosaicml/mpt-7b-storywriter,xnli_ru,0-shot,brier_score,1.2753960909711015,
mosaicml/mpt-7b-storywriter,xnli_zh,0-shot,brier_score,1.0743836903664954,
mosaicml/mpt-7b-storywriter,xnli_tr,0-shot,brier_score,1.1927998634673966,
mosaicml/mpt-7b-storywriter,xnli_fr,0-shot,brier_score,1.290251935664798,
mosaicml/mpt-7b-storywriter,xnli_en,0-shot,brier_score,0.8772257385108837,
mosaicml/mpt-7b-storywriter,xnli_ur,0-shot,brier_score,1.2785277549537262,
mosaicml/mpt-7b-storywriter,xnli_ar,0-shot,brier_score,0.9907536655140208,
mosaicml/mpt-7b-storywriter,xnli_de,0-shot,brier_score,1.2274392301166228,
mosaicml/mpt-7b-storywriter,xnli_hi,0-shot,brier_score,1.1088702072239738,
mosaicml/mpt-7b-storywriter,xnli_es,0-shot,brier_score,1.2244126501923114,
mosaicml/mpt-7b-storywriter,xnli_bg,0-shot,brier_score,1.0445857377325605,
mosaicml/mpt-7b-storywriter,xnli_sw,0-shot,brier_score,1.084639887151233,
mosaicml/mpt-7b-storywriter,xnli_el,0-shot,brier_score,1.1457153222036922,
mosaicml/mpt-7b-storywriter,xnli_th,0-shot,brier_score,1.301506120005153,
mosaicml/mpt-7b-storywriter,logiqa2,0-shot,brier_score,1.4689876462916187,
mosaicml/mpt-7b-storywriter,mathqa,0-shot,brier_score,1.0079977099532107,
mosaicml/mpt-7b-storywriter,lambada_standard,0-shot,perplexity,202.229010197755,9.46813064432611
mosaicml/mpt-7b-storywriter,lambada_standard,0-shot,accuracy,0.1144964098583349,0.0044361188251183
mosaicml/mpt-7b-storywriter,lambada_openai,0-shot,perplexity,176.61298095244155,9.78176772928432
mosaicml/mpt-7b-storywriter,lambada_openai,0-shot,accuracy,0.1765961575781098,0.0053126247648258
mosaicml/mpt-7b-storywriter,mmlu_world_religions,0-shot,accuracy,0.239766081871345,0.0327448521194695
mosaicml/mpt-7b-storywriter,mmlu_formal_logic,0-shot,accuracy,0.2142857142857142,0.0367006645104718
mosaicml/mpt-7b-storywriter,mmlu_prehistory,0-shot,accuracy,0.2685185185185185,0.0246596851859672
mosaicml/mpt-7b-storywriter,mmlu_moral_scenarios,0-shot,accuracy,0.2491620111731843,0.0144658938298599
mosaicml/mpt-7b-storywriter,mmlu_high_school_world_history,0-shot,accuracy,0.3333333333333333,0.0306858205966108
mosaicml/mpt-7b-storywriter,mmlu_moral_disputes,0-shot,accuracy,0.291907514450867,0.0244769940762473
mosaicml/mpt-7b-storywriter,mmlu_professional_law,0-shot,accuracy,0.256844850065189,0.0111584558530988
mosaicml/mpt-7b-storywriter,mmlu_logical_fallacies,0-shot,accuracy,0.2699386503067484,0.0348782516849789
mosaicml/mpt-7b-storywriter,mmlu_high_school_us_history,0-shot,accuracy,0.3480392156862745,0.0334331124048841
mosaicml/mpt-7b-storywriter,mmlu_philosophy,0-shot,accuracy,0.2636655948553054,0.0250255385005323
mosaicml/mpt-7b-storywriter,mmlu_jurisprudence,0-shot,accuracy,0.2592592592592592,0.0423651125809463
mosaicml/mpt-7b-storywriter,mmlu_international_law,0-shot,accuracy,0.2479338842975206,0.039418975265163
mosaicml/mpt-7b-storywriter,mmlu_high_school_european_history,0-shot,accuracy,0.2484848484848484,0.033744026441394
mosaicml/mpt-7b-storywriter,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2694300518134715,0.0320186712287779
mosaicml/mpt-7b-storywriter,mmlu_high_school_microeconomics,0-shot,accuracy,0.2394957983193277,0.0277220654933612
mosaicml/mpt-7b-storywriter,mmlu_high_school_geography,0-shot,accuracy,0.2474747474747475,0.0307463007421245
mosaicml/mpt-7b-storywriter,mmlu_high_school_psychology,0-shot,accuracy,0.236697247706422,0.018224078117299
mosaicml/mpt-7b-storywriter,mmlu_public_relations,0-shot,accuracy,0.3818181818181818,0.046534298079135
mosaicml/mpt-7b-storywriter,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
mosaicml/mpt-7b-storywriter,mmlu_sociology,0-shot,accuracy,0.2835820895522388,0.0318718753791979
mosaicml/mpt-7b-storywriter,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2538461538461538,0.0220660543787262
mosaicml/mpt-7b-storywriter,mmlu_security_studies,0-shot,accuracy,0.2816326530612245,0.0287951855742912
mosaicml/mpt-7b-storywriter,mmlu_professional_psychology,0-shot,accuracy,0.2745098039215686,0.0180540274588152
mosaicml/mpt-7b-storywriter,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.0391534540884783
mosaicml/mpt-7b-storywriter,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.0414243971948935
mosaicml/mpt-7b-storywriter,mmlu_miscellaneous,0-shot,accuracy,0.3039591315453384,0.016448321686769
mosaicml/mpt-7b-storywriter,mmlu_marketing,0-shot,accuracy,0.2991452991452991,0.0299969518583494
mosaicml/mpt-7b-storywriter,mmlu_management,0-shot,accuracy,0.2427184466019417,0.0424502248638449
mosaicml/mpt-7b-storywriter,mmlu_nutrition,0-shot,accuracy,0.3039215686274509,0.0263366134690466
mosaicml/mpt-7b-storywriter,mmlu_medical_genetics,0-shot,accuracy,0.26,0.0440844002276808
mosaicml/mpt-7b-storywriter,mmlu_human_aging,0-shot,accuracy,0.3228699551569506,0.0313814763757549
mosaicml/mpt-7b-storywriter,mmlu_professional_medicine,0-shot,accuracy,0.2169117647058823,0.0250358452277112
mosaicml/mpt-7b-storywriter,mmlu_college_medicine,0-shot,accuracy,0.2369942196531791,0.0324241475748309
mosaicml/mpt-7b-storywriter,mmlu_business_ethics,0-shot,accuracy,0.28,0.0451260859854212
mosaicml/mpt-7b-storywriter,mmlu_clinical_knowledge,0-shot,accuracy,0.260377358490566,0.0270087660907081
mosaicml/mpt-7b-storywriter,mmlu_global_facts,0-shot,accuracy,0.27,0.0446196043338473
mosaicml/mpt-7b-storywriter,mmlu_virology,0-shot,accuracy,0.3373493975903614,0.0368078369072758
mosaicml/mpt-7b-storywriter,mmlu_professional_accounting,0-shot,accuracy,0.2801418439716312,0.0267891723511402
mosaicml/mpt-7b-storywriter,mmlu_college_physics,0-shot,accuracy,0.2058823529411764,0.0402338227361774
mosaicml/mpt-7b-storywriter,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.0357370531476345
mosaicml/mpt-7b-storywriter,mmlu_high_school_biology,0-shot,accuracy,0.2548387096774193,0.0247901184593322
mosaicml/mpt-7b-storywriter,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
mosaicml/mpt-7b-storywriter,mmlu_anatomy,0-shot,accuracy,0.2814814814814815,0.0388500424580025
mosaicml/mpt-7b-storywriter,mmlu_college_chemistry,0-shot,accuracy,0.18,0.0386122919665369
mosaicml/mpt-7b-storywriter,mmlu_computer_security,0-shot,accuracy,0.31,0.0464823198711731
mosaicml/mpt-7b-storywriter,mmlu_college_computer_science,0-shot,accuracy,0.23,0.042295258468165
mosaicml/mpt-7b-storywriter,mmlu_astronomy,0-shot,accuracy,0.2236842105263158,0.033911609343436
mosaicml/mpt-7b-storywriter,mmlu_college_mathematics,0-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.03047297336338
mosaicml/mpt-7b-storywriter,mmlu_abstract_algebra,0-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276807
mosaicml/mpt-7b-storywriter,mmlu_machine_learning,0-shot,accuracy,0.2589285714285714,0.0415775153986562
mosaicml/mpt-7b-storywriter,mmlu_high_school_chemistry,0-shot,accuracy,0.2463054187192118,0.0303150992856177
mosaicml/mpt-7b-storywriter,mmlu_high_school_statistics,0-shot,accuracy,0.2916666666666667,0.0309986663045605
mosaicml/mpt-7b-storywriter,mmlu_elementary_mathematics,0-shot,accuracy,0.2751322751322751,0.0230000868590686
mosaicml/mpt-7b-storywriter,mmlu_electrical_engineering,0-shot,accuracy,0.2551724137931034,0.0363298405270784
mosaicml/mpt-7b-storywriter,mmlu_high_school_mathematics,0-shot,accuracy,0.2592592592592592,0.0267192407837121
mosaicml/mpt-7b-storywriter,arc_challenge,25-shot,accuracy,0.4274744027303754,0.0144568629446506
mosaicml/mpt-7b-storywriter,arc_challenge,25-shot,acc_norm,0.4607508532423208,0.0145663036766365
mosaicml/mpt-7b-storywriter,truthfulqa_mc2,0-shot,accuracy,0.3596941297527655,0.0135133065111755
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,bleu_max,17.977525977108503,0.7695541132431905
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,bleu_acc,0.2350061199510404,0.0148430615077316
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,bleu_diff,-5.0966333287389975,0.721249793575583
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge1_max,35.04312245719028,1.0996506075148464
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge1_acc,0.1921664626682986,0.0137928704806289
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge1_diff,-6.663656074389997,0.8025282580401039
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge2_max,24.121071934633303,1.0180145363654554
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge2_acc,0.1713586291309669,0.0131914099237393
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge2_diff,-7.719271942054364,0.953796284275681
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rougeL_max,33.244148594030946,1.075159041310315
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rougeL_acc,0.1921664626682986,0.0137928704806289
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rougeL_diff,-6.790992534457851,0.8170761181979462
mosaicml/mpt-7b-storywriter,truthfulqa_mc1,0-shot,accuracy,0.215422276621787,0.0143919026524276
Qwen/Qwen2.5-3B,mmlu_world_religions,0-shot,accuracy,0.8304093567251462,0.0287821081054017
Qwen/Qwen2.5-3B,mmlu_formal_logic,0-shot,accuracy,0.4841269841269841,0.044698818540726
Qwen/Qwen2.5-3B,mmlu_prehistory,0-shot,accuracy,0.7253086419753086,0.0248360578682946
Qwen/Qwen2.5-3B,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.0143559119647678
Qwen/Qwen2.5-3B,mmlu_high_school_world_history,0-shot,accuracy,0.8270042194092827,0.0246215628667684
Qwen/Qwen2.5-3B,mmlu_moral_disputes,0-shot,accuracy,0.684971098265896,0.0250093137900697
Qwen/Qwen2.5-3B,mmlu_professional_law,0-shot,accuracy,0.484354628422425,0.0127639828381209
Qwen/Qwen2.5-3B,mmlu_logical_fallacies,0-shot,accuracy,0.7975460122699386,0.0315706507891189
Qwen/Qwen2.5-3B,mmlu_high_school_us_history,0-shot,accuracy,0.8333333333333334,0.026156867523931
Qwen/Qwen2.5-3B,mmlu_philosophy,0-shot,accuracy,0.7138263665594855,0.0256702592421889
Qwen/Qwen2.5-3B,mmlu_jurisprudence,0-shot,accuracy,0.8333333333333334,0.0360281417639264
Qwen/Qwen2.5-3B,mmlu_international_law,0-shot,accuracy,0.8264462809917356,0.0345727283691766
Qwen/Qwen2.5-3B,mmlu_high_school_european_history,0-shot,accuracy,0.7696969696969697,0.0328766675860349
Qwen/Qwen2.5-3B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8601036269430051,0.0250338705830151
Qwen/Qwen2.5-3B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8025210084033614,0.0258591641220514
Qwen/Qwen2.5-3B,mmlu_high_school_geography,0-shot,accuracy,0.7929292929292929,0.028869778460267
Qwen/Qwen2.5-3B,mmlu_high_school_psychology,0-shot,accuracy,0.8532110091743119,0.0151731418451262
Qwen/Qwen2.5-3B,mmlu_public_relations,0-shot,accuracy,0.7,0.0438931145464428
Qwen/Qwen2.5-3B,mmlu_us_foreign_policy,0-shot,accuracy,0.84,0.036845294917747
Qwen/Qwen2.5-3B,mmlu_sociology,0-shot,accuracy,0.8308457711442786,0.0265085906562332
Qwen/Qwen2.5-3B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6820512820512821,0.0236108843089278
Qwen/Qwen2.5-3B,mmlu_security_studies,0-shot,accuracy,0.7387755102040816,0.0281234293351427
Qwen/Qwen2.5-3B,mmlu_professional_psychology,0-shot,accuracy,0.704248366013072,0.0184631541326328
Qwen/Qwen2.5-3B,mmlu_human_sexuality,0-shot,accuracy,0.7480916030534351,0.0380738711630608
Qwen/Qwen2.5-3B,mmlu_econometrics,0-shot,accuracy,0.5263157894736842,0.0469708513664786
Qwen/Qwen2.5-3B,mmlu_miscellaneous,0-shot,accuracy,0.7994891443167306,0.0143176537085942
Qwen/Qwen2.5-3B,mmlu_marketing,0-shot,accuracy,0.8846153846153846,0.0209301931851793
Qwen/Qwen2.5-3B,mmlu_management,0-shot,accuracy,0.8155339805825242,0.0384042362728827
Qwen/Qwen2.5-3B,mmlu_nutrition,0-shot,accuracy,0.761437908496732,0.0244043949280878
Qwen/Qwen2.5-3B,mmlu_medical_genetics,0-shot,accuracy,0.73,0.0446196043338474
Qwen/Qwen2.5-3B,mmlu_human_aging,0-shot,accuracy,0.7399103139013453,0.0294424955858574
Qwen/Qwen2.5-3B,mmlu_professional_medicine,0-shot,accuracy,0.6544117647058824,0.0288881931039886
Qwen/Qwen2.5-3B,mmlu_college_medicine,0-shot,accuracy,0.6589595375722543,0.0361466542418082
Qwen/Qwen2.5-3B,mmlu_business_ethics,0-shot,accuracy,0.72,0.0451260859854212
Qwen/Qwen2.5-3B,mmlu_clinical_knowledge,0-shot,accuracy,0.7283018867924528,0.0273777066246707
Qwen/Qwen2.5-3B,mmlu_global_facts,0-shot,accuracy,0.37,0.0485236587093909
Qwen/Qwen2.5-3B,mmlu_virology,0-shot,accuracy,0.5060240963855421,0.0389221219533304
Qwen/Qwen2.5-3B,mmlu_professional_accounting,0-shot,accuracy,0.5035460992907801,0.0298267491532809
Qwen/Qwen2.5-3B,mmlu_college_physics,0-shot,accuracy,0.5490196078431373,0.0495121825239626
Qwen/Qwen2.5-3B,mmlu_high_school_physics,0-shot,accuracy,0.423841059602649,0.0403484667860339
Qwen/Qwen2.5-3B,mmlu_high_school_biology,0-shot,accuracy,0.8096774193548387,0.022331707611823
Qwen/Qwen2.5-3B,mmlu_college_biology,0-shot,accuracy,0.7152777777777778,0.0377380999068693
Qwen/Qwen2.5-3B,mmlu_anatomy,0-shot,accuracy,0.6222222222222222,0.0418830753759585
Qwen/Qwen2.5-3B,mmlu_college_chemistry,0-shot,accuracy,0.5,0.0502518907629606
Qwen/Qwen2.5-3B,mmlu_computer_security,0-shot,accuracy,0.77,0.042295258468165
Qwen/Qwen2.5-3B,mmlu_college_computer_science,0-shot,accuracy,0.59,0.049431107042371
Qwen/Qwen2.5-3B,mmlu_astronomy,0-shot,accuracy,0.6842105263157895,0.0378272898086546
Qwen/Qwen2.5-3B,mmlu_college_mathematics,0-shot,accuracy,0.39,0.0490207130000197
Qwen/Qwen2.5-3B,mmlu_conceptual_physics,0-shot,accuracy,0.676595744680851,0.0305794427736103
Qwen/Qwen2.5-3B,mmlu_abstract_algebra,0-shot,accuracy,0.49,0.0502418393795691
Qwen/Qwen2.5-3B,mmlu_high_school_computer_science,0-shot,accuracy,0.74,0.0440844002276807
Qwen/Qwen2.5-3B,mmlu_machine_learning,0-shot,accuracy,0.4642857142857143,0.0473366789005375
Qwen/Qwen2.5-3B,mmlu_high_school_chemistry,0-shot,accuracy,0.5960591133004927,0.0345245390382203
Qwen/Qwen2.5-3B,mmlu_high_school_statistics,0-shot,accuracy,0.6018518518518519,0.033384734032074
Qwen/Qwen2.5-3B,mmlu_elementary_mathematics,0-shot,accuracy,0.5793650793650794,0.025424835086924
Qwen/Qwen2.5-3B,mmlu_electrical_engineering,0-shot,accuracy,0.6482758620689655,0.0397923663749741
Qwen/Qwen2.5-3B,mmlu_high_school_mathematics,0-shot,accuracy,0.5074074074074074,0.0304821923951915
Qwen/Qwen2.5-3B,arc_challenge,25-shot,accuracy,0.5332764505119454,0.0145789958596057
Qwen/Qwen2.5-3B,arc_challenge,25-shot,acc_norm,0.5750853242320819,0.0144456989685207
Qwen/Qwen2.5-3B,hellaswag,10-shot,accuracy,0.5538737303326031,0.0049607323822552
Qwen/Qwen2.5-3B,hellaswag,10-shot,acc_norm,0.7460665206134236,0.00434370451238
Qwen/Qwen2.5-3B,truthfulqa_mc2,0-shot,accuracy,0.4893490828530172,0.014899604966354
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,bleu_max,17.723506876309774,0.6221890551789969
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,bleu_acc,0.3647490820073439,0.0168509610617201
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,bleu_diff,-2.7917053834970758,0.5719675325277486
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge1_max,42.124187039438695,0.7609882906246943
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge1_acc,0.3720930232558139,0.016921090118814
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge1_diff,-4.20704637231704,0.7335338436606067
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge2_max,28.977364786765644,0.8202368269256133
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge2_acc,0.3231334149326805,0.0163718362864546
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge2_diff,-5.148974861232573,0.8363241355197695
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rougeL_max,39.37523397372098,0.7596826090671275
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rougeL_acc,0.3537331701346389,0.0167378143588461
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rougeL_diff,-4.5716762036035945,0.7325385085991697
Qwen/Qwen2.5-3B,truthfulqa_mc1,0-shot,accuracy,0.3170134638922888,0.0162892033744033
Qwen/Qwen2.5-3B,winogrande,5-shot,accuracy,0.7087608524072613,0.0127690293053707
Qwen/Qwen2.5-3B,gsm8k,5-shot,accuracy,0.7467778620166793,0.0119781251942996
EleutherAI/pythia-1.4b-deduped,arc:challenge,25-shot,accuracy,0.295221843003413,0.0133297502933823
EleutherAI/pythia-1.4b-deduped,arc:challenge,25-shot,acc_norm,0.3267918088737201,0.0137066659755873
EleutherAI/pythia-1.4b-deduped,hellaswag,10-shot,accuracy,0.4178450507866958,0.004921964133874
EleutherAI/pythia-1.4b-deduped,hellaswag,10-shot,acc_norm,0.5495917147978491,0.0049651776330499
EleutherAI/pythia-1.4b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-1.4b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-1.4b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.1925925925925926,0.0340654205850265
EleutherAI/pythia-1.4b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.1925925925925926,0.0340654205850265
EleutherAI/pythia-1.4b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.1578947368421052,0.0296741675201014
EleutherAI/pythia-1.4b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.1578947368421052,0.0296741675201014
EleutherAI/pythia-1.4b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.34,0.0476095228569523
EleutherAI/pythia-1.4b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.34,0.0476095228569523
EleutherAI/pythia-1.4b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2490566037735849,0.0266164829805017
EleutherAI/pythia-1.4b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2490566037735849,0.0266164829805017
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2986111111111111,0.0382705235795075
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2986111111111111,0.0382705235795075
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.0429234695990928
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441102
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177498
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2080924855491329,0.0309528902177498
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-1.4b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-1.4b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-1.4b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.0292418838696288
EleutherAI/pythia-1.4b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.0292418838696288
EleutherAI/pythia-1.4b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.219298245614035,0.0389243110651875
EleutherAI/pythia-1.4b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.219298245614035,0.0389243110651875
EleutherAI/pythia-1.4b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3241379310344827,0.0390043206918555
EleutherAI/pythia-1.4b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3241379310344827,0.0390043206918555
EleutherAI/pythia-1.4b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
EleutherAI/pythia-1.4b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2592592592592592,0.0225698970749184
EleutherAI/pythia-1.4b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.0380952380952381
EleutherAI/pythia-1.4b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.0380952380952381
EleutherAI/pythia-1.4b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.34,0.0476095228569523
EleutherAI/pythia-1.4b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.34,0.0476095228569523
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.2419354838709677,0.024362599693031
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2419354838709677,0.024362599693031
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2364532019704433,0.0298961142917335
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2364532019704433,0.0298961142917335
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2424242424242424,0.0334640988105595
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2424242424242424,0.0334640988105595
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.1666666666666666,0.0265522078282152
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1666666666666666,0.0265522078282152
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2072538860103626,0.0292528232918036
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2072538860103626,0.0292528232918036
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2384615384615384,0.0216062944946477
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2384615384615384,0.0216062944946477
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.0264661175389599
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2058823529411764,0.0262650246082758
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2058823529411764,0.0262650246082758
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.0363132980396965
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2055045871559633,0.017324352325016
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2055045871559633,0.017324352325016
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4166666666666667,0.0336227743660804
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4166666666666667,0.0336227743660804
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2450980392156862,0.0301902824535019
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2450980392156862,0.0301902824535019
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2489451476793249,0.0281469705994226
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2489451476793249,0.0281469705994226
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.3183856502242152,0.0312658052251371
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.3183856502242152,0.0312658052251371
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2290076335877862,0.0368534663171185
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2290076335877862,0.0368534663171185
EleutherAI/pythia-1.4b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.0398497965330287
EleutherAI/pythia-1.4b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.0398497965330287
EleutherAI/pythia-1.4b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2592592592592592,0.0423651125809463
EleutherAI/pythia-1.4b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2592592592592592,0.0423651125809463
EleutherAI/pythia-1.4b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2453987730061349,0.0338093981394335
EleutherAI/pythia-1.4b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2453987730061349,0.0338093981394335
EleutherAI/pythia-1.4b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.2589285714285714,0.0415775153986562
EleutherAI/pythia-1.4b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.2589285714285714,0.0415775153986562
EleutherAI/pythia-1.4b-deduped,hendrycksTest-management,5-shot,accuracy,0.174757281553398,0.0376017800602662
EleutherAI/pythia-1.4b-deduped,hendrycksTest-management,5-shot,acc_norm,0.174757281553398,0.0376017800602662
EleutherAI/pythia-1.4b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2692307692307692,0.0290585883037488
EleutherAI/pythia-1.4b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2692307692307692,0.0290585883037488
EleutherAI/pythia-1.4b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-1.4b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.26,0.0440844002276808
EleutherAI/pythia-1.4b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2349936143039591,0.0151620241522784
EleutherAI/pythia-1.4b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2349936143039591,0.0151620241522784
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.26878612716763,0.0238680032625001
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.26878612716763,0.0238680032625001
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2379888268156424,0.0142426300705749
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2379888268156424,0.0142426300705749
EleutherAI/pythia-1.4b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.261437908496732,0.0251609982142924
EleutherAI/pythia-1.4b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.261437908496732,0.0251609982142924
EleutherAI/pythia-1.4b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2411575562700964,0.0242965940347634
EleutherAI/pythia-1.4b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2411575562700964,0.0242965940347634
EleutherAI/pythia-1.4b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2407407407407407,0.0237885835516585
EleutherAI/pythia-1.4b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2407407407407407,0.0237885835516585
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2872340425531915,0.0269921991730643
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2872340425531915,0.0269921991730643
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2496740547588005,0.0110545383778323
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2496740547588005,0.0110545383778323
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.2205882352941176,0.0251877866602272
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2205882352941176,0.0251877866602272
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2549019607843137,0.0176308273751483
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2549019607843137,0.0176308273751483
EleutherAI/pythia-1.4b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-1.4b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-1.4b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.2081632653061224,0.0259911176728133
EleutherAI/pythia-1.4b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.2081632653061224,0.0259911176728133
EleutherAI/pythia-1.4b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-1.4b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2388059701492537,0.0301477759354092
EleutherAI/pythia-1.4b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-1.4b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.3,0.0460566186471838
EleutherAI/pythia-1.4b-deduped,hendrycksTest-virology,5-shot,accuracy,0.2590361445783132,0.0341064661407185
EleutherAI/pythia-1.4b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.2590361445783132,0.0341064661407185
EleutherAI/pythia-1.4b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.3157894736842105,0.0356507967070831
EleutherAI/pythia-1.4b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.3157894736842105,0.0356507967070831
EleutherAI/pythia-1.4b-deduped,truthfulqa:mc,0-shot,mc1,0.230110159118727,0.0147345579598077
EleutherAI/pythia-1.4b-deduped,truthfulqa:mc,0-shot,mc2,0.3865846445222244,0.0139993827663357
EleutherAI/pythia-1.4b-deduped,drop,3-shot,accuracy,0.0014681208053691,0.0003921042190298
EleutherAI/pythia-1.4b-deduped,drop,3-shot,f1,0.0433053691275169,0.0011661836886516
EleutherAI/pythia-1.4b-deduped,gsm8k,5-shot,accuracy,0.0083396512509476,0.0025049422268605
EleutherAI/pythia-1.4b-deduped,winogrande,5-shot,accuracy,0.5730071033938438,0.013901878072575
facebook/opt-125m,minerva_math_precalc,5-shot,accuracy,0.0018315018315018,0.0018315018315018
facebook/opt-125m,minerva_math_prealgebra,5-shot,accuracy,0.0045924225028702,0.0022922488477037
facebook/opt-125m,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/opt-125m,minerva_math_intermediate_algebra,5-shot,accuracy,0.0011074197120708,0.0011074197120708
facebook/opt-125m,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/opt-125m,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/opt-125m,minerva_math_algebra,5-shot,accuracy,0.0067396798652064,0.0023757942810498
facebook/opt-125m,fld_default,0-shot,accuracy,0.0,
facebook/opt-125m,fld_star,0-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339
facebook/opt-125m,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521
facebook/opt-125m,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
facebook/opt-125m,arithmetic_2ds,5-shot,accuracy,0.012,0.0024353573624298
facebook/opt-125m,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_1dc,5-shot,accuracy,0.003,0.0012232122154646
facebook/opt-125m,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_2dm,5-shot,accuracy,0.023,0.003352778036238
facebook/opt-125m,arithmetic_2da,5-shot,accuracy,0.008,0.0019924821184884
facebook/opt-125m,gsm8k_cot,5-shot,accuracy,0.0159211523881728,0.0034478192723889
facebook/opt-125m,anli_r2,0-shot,brier_score,0.7830871364529988,
facebook/opt-125m,anli_r3,0-shot,brier_score,0.7953205099615661,
facebook/opt-125m,anli_r1,0-shot,brier_score,0.8170221338251071,
facebook/opt-125m,xnli_eu,0-shot,brier_score,1.2285116904787872,
facebook/opt-125m,xnli_vi,0-shot,brier_score,1.0784582678662726,
facebook/opt-125m,xnli_ru,0-shot,brier_score,0.9507820309163364,
facebook/opt-125m,xnli_zh,0-shot,brier_score,0.8250929559751068,
facebook/opt-125m,xnli_tr,0-shot,brier_score,1.0901246908488005,
facebook/opt-125m,xnli_fr,0-shot,brier_score,0.8265299752835056,
facebook/opt-125m,xnli_en,0-shot,brier_score,0.7095494061851583,
facebook/opt-125m,xnli_ur,0-shot,brier_score,1.2931860597684186,
facebook/opt-125m,xnli_ar,0-shot,brier_score,0.8195340201469077,
facebook/opt-125m,xnli_de,0-shot,brier_score,0.8832758677383753,
facebook/opt-125m,xnli_hi,0-shot,brier_score,1.201948609364907,
facebook/opt-125m,xnli_es,0-shot,brier_score,1.014068172955923,
facebook/opt-125m,xnli_bg,0-shot,brier_score,1.298722997987076,
facebook/opt-125m,xnli_sw,0-shot,brier_score,0.959071321033876,
facebook/opt-125m,xnli_el,0-shot,brier_score,0.9979876427435008,
facebook/opt-125m,xnli_th,0-shot,brier_score,1.2628707586880557,
facebook/opt-125m,logiqa2,0-shot,brier_score,1.1225169562376798,
facebook/opt-125m,mathqa,0-shot,brier_score,1.033042750508362,
facebook/opt-125m,lambada_standard,0-shot,perplexity,73.09771501293442,3.093022125006431
facebook/opt-125m,lambada_standard,0-shot,accuracy,0.289928197166699,0.0063213295768572
facebook/opt-125m,lambada_openai,0-shot,perplexity,26.021596320893057,0.9407497764446304
facebook/opt-125m,lambada_openai,0-shot,accuracy,0.3784203376673782,0.0067569033267737
facebook/opt-125m,mmlu_world_religions,0-shot,accuracy,0.175438596491228,0.0291708855007276
facebook/opt-125m,mmlu_formal_logic,0-shot,accuracy,0.1349206349206349,0.0305571015894175
facebook/opt-125m,mmlu_prehistory,0-shot,accuracy,0.2962962962962963,0.0254071977988901
facebook/opt-125m,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.0143559119647678
facebook/opt-125m,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
facebook/opt-125m,mmlu_moral_disputes,0-shot,accuracy,0.2369942196531791,0.0228940824899259
facebook/opt-125m,mmlu_professional_law,0-shot,accuracy,0.2548891786179921,0.0111305098126629
facebook/opt-125m,mmlu_logical_fallacies,0-shot,accuracy,0.2208588957055214,0.0325917739274217
facebook/opt-125m,mmlu_high_school_us_history,0-shot,accuracy,0.2598039215686274,0.0307785546786932
facebook/opt-125m,mmlu_philosophy,0-shot,accuracy,0.2411575562700964,0.0242965940347634
facebook/opt-125m,mmlu_jurisprudence,0-shot,accuracy,0.2129629629629629,0.0395783547198098
facebook/opt-125m,mmlu_international_law,0-shot,accuracy,0.3801652892561983,0.0443132450196843
facebook/opt-125m,mmlu_high_school_european_history,0-shot,accuracy,0.2121212121212121,0.031922715695483
facebook/opt-125m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3678756476683937,0.0348017566846603
facebook/opt-125m,mmlu_high_school_microeconomics,0-shot,accuracy,0.3445378151260504,0.0308686826041216
facebook/opt-125m,mmlu_high_school_geography,0-shot,accuracy,0.2828282828282828,0.0320877955878675
facebook/opt-125m,mmlu_high_school_psychology,0-shot,accuracy,0.2275229357798165,0.0179744635787765
facebook/opt-125m,mmlu_public_relations,0-shot,accuracy,0.2272727272727272,0.0401396455407277
facebook/opt-125m,mmlu_us_foreign_policy,0-shot,accuracy,0.3,0.0460566186471838
facebook/opt-125m,mmlu_sociology,0-shot,accuracy,0.2338308457711442,0.0299294154083483
facebook/opt-125m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.358974358974359,0.0243217384846023
facebook/opt-125m,mmlu_security_studies,0-shot,accuracy,0.2489795918367346,0.0276829795229602
facebook/opt-125m,mmlu_professional_psychology,0-shot,accuracy,0.2222222222222222,0.0168190283757363
facebook/opt-125m,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
facebook/opt-125m,mmlu_econometrics,0-shot,accuracy,0.2280701754385964,0.0394715278266941
facebook/opt-125m,mmlu_miscellaneous,0-shot,accuracy,0.2426564495530012,0.0153298889408998
facebook/opt-125m,mmlu_marketing,0-shot,accuracy,0.1965811965811965,0.0260353860989512
facebook/opt-125m,mmlu_management,0-shot,accuracy,0.1844660194174757,0.0384042362728827
facebook/opt-125m,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.0252616912197294
facebook/opt-125m,mmlu_medical_genetics,0-shot,accuracy,0.34,0.0476095228569523
facebook/opt-125m,mmlu_human_aging,0-shot,accuracy,0.2017937219730941,0.0269361119128022
facebook/opt-125m,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091215
facebook/opt-125m,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.0312651120617304
facebook/opt-125m,mmlu_business_ethics,0-shot,accuracy,0.21,0.0409360180740332
facebook/opt-125m,mmlu_clinical_knowledge,0-shot,accuracy,0.2150943396226415,0.0252883945028913
facebook/opt-125m,mmlu_global_facts,0-shot,accuracy,0.19,0.0394277244403662
facebook/opt-125m,mmlu_virology,0-shot,accuracy,0.2108433734939759,0.0317555478662991
facebook/opt-125m,mmlu_professional_accounting,0-shot,accuracy,0.2588652482269503,0.0261295725271808
facebook/opt-125m,mmlu_college_physics,0-shot,accuracy,0.3725490196078431,0.0481084014808263
facebook/opt-125m,mmlu_high_school_physics,0-shot,accuracy,0.3178807947019867,0.038020397601079
facebook/opt-125m,mmlu_high_school_biology,0-shot,accuracy,0.3161290322580645,0.0264508744890427
facebook/opt-125m,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.0347659010430413
facebook/opt-125m,mmlu_anatomy,0-shot,accuracy,0.2444444444444444,0.0371253783361486
facebook/opt-125m,mmlu_college_chemistry,0-shot,accuracy,0.29,0.0456048021572068
facebook/opt-125m,mmlu_computer_security,0-shot,accuracy,0.18,0.0386122919665369
facebook/opt-125m,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
facebook/opt-125m,mmlu_astronomy,0-shot,accuracy,0.2565789473684211,0.0355418036802569
facebook/opt-125m,mmlu_college_mathematics,0-shot,accuracy,0.28,0.0451260859854212
facebook/opt-125m,mmlu_conceptual_physics,0-shot,accuracy,0.3021276595744681,0.0300175544718805
facebook/opt-125m,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.0456048021572068
facebook/opt-125m,mmlu_high_school_computer_science,0-shot,accuracy,0.19,0.0394277244403662
facebook/opt-125m,mmlu_machine_learning,0-shot,accuracy,0.1696428571428571,0.0356236785009539
facebook/opt-125m,mmlu_high_school_chemistry,0-shot,accuracy,0.3004926108374384,0.0322579947623348
facebook/opt-125m,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-125m,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.0224945107675031
facebook/opt-125m,mmlu_electrical_engineering,0-shot,accuracy,0.2551724137931034,0.0363298405270784
facebook/opt-125m,mmlu_high_school_mathematics,0-shot,accuracy,0.2629629629629629,0.0268420578738337
facebook/opt-125m,arc_challenge,25-shot,accuracy,0.2073378839590443,0.0118469057829713
facebook/opt-125m,arc_challenge,25-shot,acc_norm,0.2226962457337884,0.0121583147748299
facebook/opt-125m,truthfulqa_mc2,0-shot,accuracy,0.428871922722071,0.0150698218067162
facebook/opt-125m,truthfulqa_gen,0-shot,bleu_max,15.720420721623515,0.5472384436992891
facebook/opt-125m,truthfulqa_gen,0-shot,bleu_acc,0.412484700122399,0.0172332993995712
facebook/opt-125m,truthfulqa_gen,0-shot,bleu_diff,-0.8644446662587123,0.5171860112672116
facebook/opt-125m,truthfulqa_gen,0-shot,rouge1_max,36.57421967061916,0.7883288595416582
facebook/opt-125m,truthfulqa_gen,0-shot,rouge1_acc,0.3488372093023256,0.0166844198599869
facebook/opt-125m,truthfulqa_gen,0-shot,rouge1_diff,-3.1802654121876426,0.7798761989232565
facebook/opt-125m,truthfulqa_gen,0-shot,rouge2_max,18.635541322636527,0.8253607772770336
facebook/opt-125m,truthfulqa_gen,0-shot,rouge2_acc,0.208078335373317,0.0142105034735766
facebook/opt-125m,truthfulqa_gen,0-shot,rouge2_diff,-3.48758369439353,0.7563641375616611
facebook/opt-125m,truthfulqa_gen,0-shot,rougeL_max,34.10946249460258,0.7777451900951593
facebook/opt-125m,truthfulqa_gen,0-shot,rougeL_acc,0.3525091799265606,0.0167246463807565
facebook/opt-125m,truthfulqa_gen,0-shot,rougeL_diff,-2.817674709105612,0.7655327681953442
facebook/opt-125m,truthfulqa_mc1,0-shot,accuracy,0.241126070991432,0.0149748272797523
Devio/test-3b,minerva_math_precalc,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_prealgebra,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_num_theory,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_geometry,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_algebra,5-shot,accuracy,0.0,
Devio/test-3b,fld_default,0-shot,accuracy,0.0,
Devio/test-3b,fld_star,0-shot,accuracy,0.0,
Devio/test-3b,arithmetic_3da,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_3ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_4da,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_2ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_5ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_5da,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_1dc,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_4ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_2dm,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_2da,5-shot,accuracy,0.0,
Devio/test-3b,gsm8k_cot,5-shot,accuracy,0.0,
Devio/test-3b,gsm8k,5-shot,accuracy,0.0,
Devio/test-3b,anli_r2,0-shot,brier_score,1.2297922636861298,
Devio/test-3b,anli_r3,0-shot,brier_score,1.2333997188823227,
Devio/test-3b,anli_r1,0-shot,brier_score,1.2298489673650648,
Devio/test-3b,xnli_eu,0-shot,brier_score,1.0534244785984144,
Devio/test-3b,xnli_vi,0-shot,brier_score,1.3218456507070504,
Devio/test-3b,xnli_ru,0-shot,brier_score,0.8656071346235457,
Devio/test-3b,xnli_zh,0-shot,brier_score,0.9476036633633778,
Devio/test-3b,xnli_tr,0-shot,brier_score,0.9925860351979764,
Devio/test-3b,xnli_fr,0-shot,brier_score,0.957790185963517,
Devio/test-3b,xnli_en,0-shot,brier_score,0.7251422341685971,
Devio/test-3b,xnli_ur,0-shot,brier_score,1.0854404485149611,
Devio/test-3b,xnli_ar,0-shot,brier_score,0.8967637727978277,
Devio/test-3b,xnli_de,0-shot,brier_score,1.0362908584253308,
Devio/test-3b,xnli_hi,0-shot,brier_score,1.0781294620168924,
Devio/test-3b,xnli_es,0-shot,brier_score,1.2906893281594574,
Devio/test-3b,xnli_bg,0-shot,brier_score,0.9965475745381036,
Devio/test-3b,xnli_sw,0-shot,brier_score,1.115504637558115,
Devio/test-3b,xnli_el,0-shot,brier_score,1.155706973028575,
Devio/test-3b,xnli_th,0-shot,brier_score,1.1482107613489194,
Devio/test-3b,logiqa2,0-shot,brier_score,1.5177031880020304,
Devio/test-3b,mathqa,0-shot,brier_score,1.014650380983031,
Devio/test-3b,lambada_standard,0-shot,perplexity,538584815.1171707,54349236.53521809
Devio/test-3b,lambada_standard,0-shot,accuracy,0.0001940617116242,0.0001940617116243
Devio/test-3b,lambada_openai,0-shot,perplexity,254132266.18287277,26030411.523864448
Devio/test-3b,lambada_openai,0-shot,accuracy,0.0,
Devio/test-3b,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.0358252944257312
Devio/test-3b,mmlu_formal_logic,0-shot,accuracy,0.365079365079365,0.0430624125912715
Devio/test-3b,mmlu_prehistory,0-shot,accuracy,0.2407407407407407,0.0237885835516585
Devio/test-3b,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
Devio/test-3b,mmlu_high_school_world_history,0-shot,accuracy,0.2573839662447257,0.0284588209914602
Devio/test-3b,mmlu_moral_disputes,0-shot,accuracy,0.2196531791907514,0.0222896388526178
Devio/test-3b,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.0110641510271654
Devio/test-3b,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.0332201579577674
Devio/test-3b,mmlu_high_school_us_history,0-shot,accuracy,0.2598039215686274,0.0307785546786932
Devio/test-3b,mmlu_philosophy,0-shot,accuracy,0.2186495176848874,0.0234755814178611
Devio/test-3b,mmlu_jurisprudence,0-shot,accuracy,0.2129629629629629,0.0395783547198097
Devio/test-3b,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.0389687898507041
Devio/test-3b,mmlu_high_school_european_history,0-shot,accuracy,0.2121212121212121,0.0319227156954829
Devio/test-3b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3419689119170984,0.0342346510010428
Devio/test-3b,mmlu_high_school_microeconomics,0-shot,accuracy,0.3445378151260504,0.0308686826041216
Devio/test-3b,mmlu_high_school_geography,0-shot,accuracy,0.2525252525252525,0.0309540554703659
Devio/test-3b,mmlu_high_school_psychology,0-shot,accuracy,0.3467889908256881,0.020406097104093
Devio/test-3b,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
Devio/test-3b,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
Devio/test-3b,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296
Devio/test-3b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3641025641025641,0.0243966729850947
Devio/test-3b,mmlu_security_studies,0-shot,accuracy,0.2448979591836734,0.0275296374401749
Devio/test-3b,mmlu_professional_psychology,0-shot,accuracy,0.2173202614379085,0.0166848209291485
Devio/test-3b,mmlu_human_sexuality,0-shot,accuracy,0.2671755725190839,0.0388084830108239
Devio/test-3b,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.0418577442402205
Devio/test-3b,mmlu_miscellaneous,0-shot,accuracy,0.2362707535121328,0.0151904737170374
Devio/test-3b,mmlu_marketing,0-shot,accuracy,0.1965811965811965,0.0260353860989512
Devio/test-3b,mmlu_management,0-shot,accuracy,0.3203883495145631,0.0462028408228004
Devio/test-3b,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.0252616912197294
Devio/test-3b,mmlu_medical_genetics,0-shot,accuracy,0.23,0.042295258468165
Devio/test-3b,mmlu_human_aging,0-shot,accuracy,0.2780269058295964,0.030069584874494
Devio/test-3b,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091216
Devio/test-3b,mmlu_college_medicine,0-shot,accuracy,0.3526011560693641,0.0364303716895854
Devio/test-3b,mmlu_business_ethics,0-shot,accuracy,0.21,0.0409360180740332
Devio/test-3b,mmlu_clinical_knowledge,0-shot,accuracy,0.3018867924528302,0.0282542003444386
Devio/test-3b,mmlu_global_facts,0-shot,accuracy,0.2,0.0402015126103684
Devio/test-3b,mmlu_virology,0-shot,accuracy,0.2289156626506024,0.0327074527735247
Devio/test-3b,mmlu_professional_accounting,0-shot,accuracy,0.2375886524822695,0.0253895125527299
Devio/test-3b,mmlu_college_physics,0-shot,accuracy,0.2352941176470588,0.0422077365917145
Devio/test-3b,mmlu_high_school_physics,0-shot,accuracy,0.3377483443708609,0.0386155754625517
Devio/test-3b,mmlu_high_school_biology,0-shot,accuracy,0.3161290322580645,0.0264508744890427
Devio/test-3b,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.0371617743756601
Devio/test-3b,mmlu_anatomy,0-shot,accuracy,0.2296296296296296,0.0363338441407346
Devio/test-3b,mmlu_college_chemistry,0-shot,accuracy,0.4,0.049236596391733
Devio/test-3b,mmlu_computer_security,0-shot,accuracy,0.2,0.0402015126103684
Devio/test-3b,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526
Devio/test-3b,mmlu_astronomy,0-shot,accuracy,0.3289473684210526,0.038234289699266
Devio/test-3b,mmlu_college_mathematics,0-shot,accuracy,0.31,0.0464823198711731
Devio/test-3b,mmlu_conceptual_physics,0-shot,accuracy,0.2085106382978723,0.0265569821178387
Devio/test-3b,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276807
Devio/test-3b,mmlu_high_school_computer_science,0-shot,accuracy,0.2,0.0402015126103684
Devio/test-3b,mmlu_machine_learning,0-shot,accuracy,0.1607142857142857,0.0348594609647574
Devio/test-3b,mmlu_high_school_chemistry,0-shot,accuracy,0.2610837438423645,0.0309037969521144
Devio/test-3b,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
Devio/test-3b,mmlu_elementary_mathematics,0-shot,accuracy,0.2222222222222222,0.0214116843936941
Devio/test-3b,mmlu_electrical_engineering,0-shot,accuracy,0.2551724137931034,0.0363298405270784
Devio/test-3b,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
Devio/test-3b,arc_challenge,25-shot,accuracy,0.204778156996587,0.0117925443385134
Devio/test-3b,arc_challenge,25-shot,acc_norm,0.2619453924914676,0.0128490548268581
Devio/test-3b,hellaswag,10-shot,accuracy,0.2595100577574188,0.0043746991892848
Devio/test-3b,hellaswag,10-shot,acc_norm,0.2632941645090619,0.004395205528158
Devio/test-3b,truthfulqa_mc2,0-shot,accuracy,0.4854291073268079,0.0166475222160003
Devio/test-3b,truthfulqa_gen,0-shot,bleu_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,bleu_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,bleu_diff,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge1_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge1_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge1_diff,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge2_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge2_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge2_diff,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rougeL_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rougeL_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rougeL_diff,0.0,
Devio/test-3b,truthfulqa_mc1,0-shot,accuracy,0.2386780905752754,0.0149226296954564
Devio/test-3b,winogrande,5-shot,accuracy,0.489344909234412,0.0140492945362903
LLM360/Amber,arc:challenge,25-shot,accuracy,0.3976109215017064,0.0143017522232795
LLM360/Amber,arc:challenge,25-shot,acc_norm,0.409556313993174,0.0143703586324724
LLM360/Amber,hellaswag,10-shot,accuracy,0.5478988249352719,0.004966832553245
LLM360/Amber,hellaswag,10-shot,acc_norm,0.7379008165704043,0.0043887752982101
LLM360/Amber,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.047258156262526
LLM360/Amber,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.047258156262526
LLM360/Amber,hendrycksTest-anatomy,5-shot,accuracy,0.2222222222222222,0.0359144408419696
LLM360/Amber,hendrycksTest-anatomy,5-shot,acc_norm,0.2222222222222222,0.0359144408419696
LLM360/Amber,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.0358349617636106
LLM360/Amber,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.0358349617636106
LLM360/Amber,hendrycksTest-business_ethics,5-shot,accuracy,0.31,0.0464823198711731
LLM360/Amber,hendrycksTest-business_ethics,5-shot,acc_norm,0.31,0.0464823198711731
LLM360/Amber,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2113207547169811,0.0251257664848278
LLM360/Amber,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2113207547169811,0.0251257664848278
LLM360/Amber,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.0371617743756601
LLM360/Amber,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.0371617743756601
LLM360/Amber,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.0416333199893226
LLM360/Amber,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.0416333199893226
LLM360/Amber,hendrycksTest-college_computer_science,5-shot,accuracy,0.4,0.049236596391733
LLM360/Amber,hendrycksTest-college_computer_science,5-shot,acc_norm,0.4,0.049236596391733
LLM360/Amber,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.0416333199893226
LLM360/Amber,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.0416333199893226
LLM360/Amber,hendrycksTest-college_medicine,5-shot,accuracy,0.2485549132947976,0.0329530469681831
LLM360/Amber,hendrycksTest-college_medicine,5-shot,acc_norm,0.2485549132947976,0.0329530469681831
LLM360/Amber,hendrycksTest-college_physics,5-shot,accuracy,0.1764705882352941,0.037932811853078
LLM360/Amber,hendrycksTest-college_physics,5-shot,acc_norm,0.1764705882352941,0.037932811853078
LLM360/Amber,hendrycksTest-computer_security,5-shot,accuracy,0.3,0.0460566186471838
LLM360/Amber,hendrycksTest-computer_security,5-shot,acc_norm,0.3,0.0460566186471838
LLM360/Amber,hendrycksTest-conceptual_physics,5-shot,accuracy,0.251063829787234,0.0283469637771624
LLM360/Amber,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.251063829787234,0.0283469637771624
LLM360/Amber,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
LLM360/Amber,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
LLM360/Amber,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3103448275862069,0.0385528961637894
LLM360/Amber,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3103448275862069,0.0385528961637894
LLM360/Amber,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2777777777777778,0.0230681888482611
LLM360/Amber,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2777777777777778,0.0230681888482611
LLM360/Amber,hendrycksTest-formal_logic,5-shot,accuracy,0.2936507936507936,0.0407352432214712
LLM360/Amber,hendrycksTest-formal_logic,5-shot,acc_norm,0.2936507936507936,0.0407352432214712
LLM360/Amber,hendrycksTest-global_facts,5-shot,accuracy,0.29,0.0456048021572068
LLM360/Amber,hendrycksTest-global_facts,5-shot,acc_norm,0.29,0.0456048021572068
LLM360/Amber,hendrycksTest-high_school_biology,5-shot,accuracy,0.2387096774193548,0.0242510712622088
LLM360/Amber,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2387096774193548,0.0242510712622088
LLM360/Amber,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.167487684729064,0.0262730860475354
LLM360/Amber,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.167487684729064,0.0262730860475354
LLM360/Amber,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.32,0.046882617226215
LLM360/Amber,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.32,0.046882617226215
LLM360/Amber,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2848484848484848,0.0352439084451178
LLM360/Amber,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2848484848484848,0.0352439084451178
LLM360/Amber,hendrycksTest-high_school_geography,5-shot,accuracy,0.1919191919191919,0.028057791672989
LLM360/Amber,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1919191919191919,0.028057791672989
LLM360/Amber,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.1968911917098445,0.0286978739718606
LLM360/Amber,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.1968911917098445,0.0286978739718606
LLM360/Amber,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2256410256410256,0.0211936325251485
LLM360/Amber,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2256410256410256,0.0211936325251485
LLM360/Amber,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2074074074074074,0.0247207131939521
LLM360/Amber,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2074074074074074,0.0247207131939521
LLM360/Amber,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2478991596638655,0.0280479672241768
LLM360/Amber,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2478991596638655,0.0280479672241768
LLM360/Amber,hendrycksTest-high_school_physics,5-shot,accuracy,0.2582781456953642,0.0357370531476345
LLM360/Amber,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2582781456953642,0.0357370531476345
LLM360/Amber,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2018348623853211,0.0172085793577875
LLM360/Amber,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2018348623853211,0.0172085793577875
LLM360/Amber,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2777777777777778,0.0305467452649532
LLM360/Amber,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2777777777777778,0.0305467452649532
LLM360/Amber,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3235294117647059,0.0328347205610856
LLM360/Amber,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3235294117647059,0.0328347205610856
LLM360/Amber,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2995780590717299,0.029818024749753
LLM360/Amber,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2995780590717299,0.029818024749753
LLM360/Amber,hendrycksTest-human_aging,5-shot,accuracy,0.3004484304932735,0.0307693520082291
LLM360/Amber,hendrycksTest-human_aging,5-shot,acc_norm,0.3004484304932735,0.0307693520082291
LLM360/Amber,hendrycksTest-human_sexuality,5-shot,accuracy,0.3511450381679389,0.0418644516301375
LLM360/Amber,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3511450381679389,0.0418644516301375
LLM360/Amber,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.0398497965330287
LLM360/Amber,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.0398497965330287
LLM360/Amber,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.041860917913946
LLM360/Amber,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.041860917913946
LLM360/Amber,hendrycksTest-logical_fallacies,5-shot,accuracy,0.1963190184049079,0.0312079703947092
LLM360/Amber,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.1963190184049079,0.0312079703947092
LLM360/Amber,hendrycksTest-machine_learning,5-shot,accuracy,0.2946428571428571,0.0432704093257872
LLM360/Amber,hendrycksTest-machine_learning,5-shot,acc_norm,0.2946428571428571,0.0432704093257872
LLM360/Amber,hendrycksTest-management,5-shot,accuracy,0.174757281553398,0.0376017800602662
LLM360/Amber,hendrycksTest-management,5-shot,acc_norm,0.174757281553398,0.0376017800602662
LLM360/Amber,hendrycksTest-marketing,5-shot,accuracy,0.3119658119658119,0.0303515273233449
LLM360/Amber,hendrycksTest-marketing,5-shot,acc_norm,0.3119658119658119,0.0303515273233449
LLM360/Amber,hendrycksTest-medical_genetics,5-shot,accuracy,0.4,0.049236596391733
LLM360/Amber,hendrycksTest-medical_genetics,5-shot,acc_norm,0.4,0.049236596391733
LLM360/Amber,hendrycksTest-miscellaneous,5-shot,accuracy,0.2796934865900383,0.0160507921480365
LLM360/Amber,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2796934865900383,0.0160507921480365
LLM360/Amber,hendrycksTest-moral_disputes,5-shot,accuracy,0.3265895953757225,0.0252482647742428
LLM360/Amber,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3265895953757225,0.0252482647742428
LLM360/Amber,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2368715083798882,0.0142195707881039
LLM360/Amber,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2368715083798882,0.0142195707881039
LLM360/Amber,hendrycksTest-nutrition,5-shot,accuracy,0.2712418300653594,0.0254577566966678
LLM360/Amber,hendrycksTest-nutrition,5-shot,acc_norm,0.2712418300653594,0.0254577566966678
LLM360/Amber,hendrycksTest-philosophy,5-shot,accuracy,0.3183279742765273,0.026457225067811
LLM360/Amber,hendrycksTest-philosophy,5-shot,acc_norm,0.3183279742765273,0.026457225067811
LLM360/Amber,hendrycksTest-prehistory,5-shot,accuracy,0.2530864197530864,0.024191808600713
LLM360/Amber,hendrycksTest-prehistory,5-shot,acc_norm,0.2530864197530864,0.024191808600713
LLM360/Amber,hendrycksTest-professional_accounting,5-shot,accuracy,0.301418439716312,0.0273741288826311
LLM360/Amber,hendrycksTest-professional_accounting,5-shot,acc_norm,0.301418439716312,0.0273741288826311
LLM360/Amber,hendrycksTest-professional_law,5-shot,accuracy,0.2790091264667536,0.0114552088328035
LLM360/Amber,hendrycksTest-professional_law,5-shot,acc_norm,0.2790091264667536,0.0114552088328035
LLM360/Amber,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.0237097882538117
LLM360/Amber,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.0237097882538117
LLM360/Amber,hendrycksTest-professional_psychology,5-shot,accuracy,0.3071895424836601,0.0186633596714636
LLM360/Amber,hendrycksTest-professional_psychology,5-shot,acc_norm,0.3071895424836601,0.0186633596714636
LLM360/Amber,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.0430911870994645
LLM360/Amber,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.0430911870994645
LLM360/Amber,hendrycksTest-security_studies,5-shot,accuracy,0.1836734693877551,0.0247890713320076
LLM360/Amber,hendrycksTest-security_studies,5-shot,acc_norm,0.1836734693877551,0.0247890713320076
LLM360/Amber,hendrycksTest-sociology,5-shot,accuracy,0.2885572139303483,0.0320384104021332
LLM360/Amber,hendrycksTest-sociology,5-shot,acc_norm,0.2885572139303483,0.0320384104021332
LLM360/Amber,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.3,0.0460566186471838
LLM360/Amber,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.3,0.0460566186471838
LLM360/Amber,hendrycksTest-virology,5-shot,accuracy,0.2771084337349397,0.0348433159268058
LLM360/Amber,hendrycksTest-virology,5-shot,acc_norm,0.2771084337349397,0.0348433159268058
LLM360/Amber,hendrycksTest-world_religions,5-shot,accuracy,0.3684210526315789,0.0369965801765687
LLM360/Amber,hendrycksTest-world_religions,5-shot,acc_norm,0.3684210526315789,0.0369965801765687
LLM360/Amber,truthfulqa:mc,0-shot,mc1,0.2141982864137087,0.0143621481556904
LLM360/Amber,truthfulqa:mc,0-shot,mc2,0.3355637385526089,0.0130682822251643
LLM360/Amber,winogrande,5-shot,accuracy,0.6787687450670876,0.0131235993245583
LLM360/Amber,gsm8k,5-shot,accuracy,0.0280515542077331,0.0045482295338363
facebook/xglm-2.9B,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_prealgebra,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_algebra,5-shot,accuracy,0.0,
facebook/xglm-2.9B,fld_default,0-shot,accuracy,0.0,
facebook/xglm-2.9B,fld_star,0-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_3da,5-shot,accuracy,0.0005,0.0005
facebook/xglm-2.9B,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005
facebook/xglm-2.9B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
facebook/xglm-2.9B,arithmetic_2ds,5-shot,accuracy,0.0125,0.0024849471787626
facebook/xglm-2.9B,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_1dc,5-shot,accuracy,0.0025,0.0011169148353275
facebook/xglm-2.9B,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_2dm,5-shot,accuracy,0.01,0.0022254159696827
facebook/xglm-2.9B,arithmetic_2da,5-shot,accuracy,0.0005,0.0005
facebook/xglm-2.9B,gsm8k_cot,5-shot,accuracy,0.0242608036391205,0.0042380079000013
facebook/xglm-2.9B,gsm8k,5-shot,accuracy,0.0151630022744503,0.0033660229497263
facebook/xglm-2.9B,anli_r2,0-shot,brier_score,0.8903282853468429,
facebook/xglm-2.9B,anli_r3,0-shot,brier_score,0.8828546319214271,
facebook/xglm-2.9B,anli_r1,0-shot,brier_score,0.9090076784540244,
facebook/xglm-2.9B,xnli_eu,0-shot,brier_score,0.6945318464480982,
facebook/xglm-2.9B,xnli_vi,0-shot,brier_score,0.7198974352337865,
facebook/xglm-2.9B,xnli_ru,0-shot,brier_score,0.7940689223362921,
facebook/xglm-2.9B,xnli_zh,0-shot,brier_score,0.9662319296220724,
facebook/xglm-2.9B,xnli_tr,0-shot,brier_score,0.7985713408191671,
facebook/xglm-2.9B,xnli_fr,0-shot,brier_score,0.8777604442850733,
facebook/xglm-2.9B,xnli_en,0-shot,brier_score,0.6546699998277438,
facebook/xglm-2.9B,xnli_ur,0-shot,brier_score,0.9005216252993709,
facebook/xglm-2.9B,xnli_ar,0-shot,brier_score,1.221392600244141,
facebook/xglm-2.9B,xnli_de,0-shot,brier_score,0.8098229929420445,
facebook/xglm-2.9B,xnli_hi,0-shot,brier_score,0.7612249719321225,
facebook/xglm-2.9B,xnli_es,0-shot,brier_score,0.8391711908916202,
facebook/xglm-2.9B,xnli_bg,0-shot,brier_score,0.7784193725622509,
facebook/xglm-2.9B,xnli_sw,0-shot,brier_score,0.752512116394542,
facebook/xglm-2.9B,xnli_el,0-shot,brier_score,0.8014402208550904,
facebook/xglm-2.9B,xnli_th,0-shot,brier_score,0.8209309976197092,
facebook/xglm-2.9B,logiqa2,0-shot,brier_score,1.0440698590170625,
facebook/xglm-2.9B,mathqa,0-shot,brier_score,1.0015941907404673,
facebook/xglm-2.9B,lambada_standard,0-shot,perplexity,10.724228161680852,0.3113611480259543
facebook/xglm-2.9B,lambada_standard,0-shot,accuracy,0.5022317096836794,0.006965908268351
facebook/xglm-2.9B,lambada_openai,0-shot,perplexity,9.850349597657685,0.2859716030715756
facebook/xglm-2.9B,lambada_openai,0-shot,accuracy,0.4940811177954589,0.0069654895595805
facebook/xglm-2.9B,mmlu_world_religions,0-shot,accuracy,0.2573099415204678,0.0335279984416186
facebook/xglm-2.9B,mmlu_formal_logic,0-shot,accuracy,0.2936507936507936,0.0407352432214712
facebook/xglm-2.9B,mmlu_prehistory,0-shot,accuracy,0.2098765432098765,0.0226583440859813
facebook/xglm-2.9B,mmlu_moral_scenarios,0-shot,accuracy,0.2234636871508379,0.0139320686385797
facebook/xglm-2.9B,mmlu_high_school_world_history,0-shot,accuracy,0.2911392405063291,0.0295716010657533
facebook/xglm-2.9B,mmlu_moral_disputes,0-shot,accuracy,0.2341040462427745,0.0227971102780711
facebook/xglm-2.9B,mmlu_professional_law,0-shot,accuracy,0.2483702737940026,0.0110352125980344
facebook/xglm-2.9B,mmlu_logical_fallacies,0-shot,accuracy,0.2515337423312883,0.0340899788685752
facebook/xglm-2.9B,mmlu_high_school_us_history,0-shot,accuracy,0.2352941176470588,0.0297717752281456
facebook/xglm-2.9B,mmlu_philosophy,0-shot,accuracy,0.2186495176848874,0.0234755814178611
facebook/xglm-2.9B,mmlu_jurisprudence,0-shot,accuracy,0.2037037037037037,0.0389354251882484
facebook/xglm-2.9B,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.040261875275912
facebook/xglm-2.9B,mmlu_high_school_european_history,0-shot,accuracy,0.2303030303030303,0.0328766675860348
facebook/xglm-2.9B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.1968911917098445,0.0286978739718606
facebook/xglm-2.9B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2184873949579832,0.0268415143229589
facebook/xglm-2.9B,mmlu_high_school_geography,0-shot,accuracy,0.1565656565656565,0.0258905203581414
facebook/xglm-2.9B,mmlu_high_school_psychology,0-shot,accuracy,0.2275229357798165,0.0179744635787765
facebook/xglm-2.9B,mmlu_public_relations,0-shot,accuracy,0.3,0.0438931145464428
facebook/xglm-2.9B,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.0446196043338474
facebook/xglm-2.9B,mmlu_sociology,0-shot,accuracy,0.2338308457711442,0.0299294154083484
facebook/xglm-2.9B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2333333333333333,0.0214445473015604
facebook/xglm-2.9B,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904
facebook/xglm-2.9B,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
facebook/xglm-2.9B,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.0372767357559691
facebook/xglm-2.9B,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.0414243971948936
facebook/xglm-2.9B,mmlu_miscellaneous,0-shot,accuracy,0.2541507024265645,0.0155692546920457
facebook/xglm-2.9B,mmlu_marketing,0-shot,accuracy,0.2735042735042735,0.0292025401534311
facebook/xglm-2.9B,mmlu_management,0-shot,accuracy,0.2524271844660194,0.0430125039969087
facebook/xglm-2.9B,mmlu_nutrition,0-shot,accuracy,0.2352941176470588,0.0242886194660461
facebook/xglm-2.9B,mmlu_medical_genetics,0-shot,accuracy,0.31,0.0464823198711731
facebook/xglm-2.9B,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.0310244117405722
facebook/xglm-2.9B,mmlu_professional_medicine,0-shot,accuracy,0.1580882352941176,0.0221614626080685
facebook/xglm-2.9B,mmlu_college_medicine,0-shot,accuracy,0.1849710982658959,0.0296056239817712
facebook/xglm-2.9B,mmlu_business_ethics,0-shot,accuracy,0.31,0.0464823198711731
facebook/xglm-2.9B,mmlu_clinical_knowledge,0-shot,accuracy,0.2075471698113207,0.0249599180289112
facebook/xglm-2.9B,mmlu_global_facts,0-shot,accuracy,0.22,0.0416333199893226
facebook/xglm-2.9B,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.0352948680151111
facebook/xglm-2.9B,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.0266845643404609
facebook/xglm-2.9B,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
facebook/xglm-2.9B,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.0329798664847383
facebook/xglm-2.9B,mmlu_high_school_biology,0-shot,accuracy,0.2161290322580645,0.0234152934335685
facebook/xglm-2.9B,mmlu_college_biology,0-shot,accuracy,0.2430555555555555,0.0358687928008034
facebook/xglm-2.9B,mmlu_anatomy,0-shot,accuracy,0.1925925925925926,0.0340654205850265
facebook/xglm-2.9B,mmlu_college_chemistry,0-shot,accuracy,0.17,0.0377525168068637
facebook/xglm-2.9B,mmlu_computer_security,0-shot,accuracy,0.29,0.0456048021572068
facebook/xglm-2.9B,mmlu_college_computer_science,0-shot,accuracy,0.22,0.0416333199893226
facebook/xglm-2.9B,mmlu_astronomy,0-shot,accuracy,0.1842105263157894,0.0315469804508223
facebook/xglm-2.9B,mmlu_college_mathematics,0-shot,accuracy,0.22,0.0416333199893227
facebook/xglm-2.9B,mmlu_conceptual_physics,0-shot,accuracy,0.2553191489361702,0.0285048564705142
facebook/xglm-2.9B,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
facebook/xglm-2.9B,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.0460566186471838
facebook/xglm-2.9B,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.0449394906861353
facebook/xglm-2.9B,mmlu_high_school_chemistry,0-shot,accuracy,0.1970443349753694,0.0279867246667362
facebook/xglm-2.9B,mmlu_high_school_statistics,0-shot,accuracy,0.1712962962962963,0.0256953416438246
facebook/xglm-2.9B,mmlu_elementary_mathematics,0-shot,accuracy,0.2486772486772486,0.0222618176924001
facebook/xglm-2.9B,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.035659981741353
facebook/xglm-2.9B,mmlu_high_school_mathematics,0-shot,accuracy,0.237037037037037,0.0259288761327661
facebook/xglm-2.9B,arc_challenge,25-shot,accuracy,0.2764505119453925,0.0130696624742524
facebook/xglm-2.9B,arc_challenge,25-shot,acc_norm,0.3011945392491467,0.0134067417678476
facebook/xglm-2.9B,hellaswag,10-shot,accuracy,0.4106751643098984,0.0049095095385251
facebook/xglm-2.9B,hellaswag,10-shot,acc_norm,0.5415255925114519,0.0049725431277678
facebook/xglm-2.9B,truthfulqa_mc2,0-shot,accuracy,0.3582356473001071,0.0137546749710961
facebook/xglm-2.9B,truthfulqa_gen,0-shot,bleu_max,3.6087558287053865,0.1867024966847327
facebook/xglm-2.9B,truthfulqa_gen,0-shot,bleu_acc,0.2631578947368421,0.015415241740237
facebook/xglm-2.9B,truthfulqa_gen,0-shot,bleu_diff,-1.3731184892287656,0.143713740467178
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge1_max,13.419021251081578,0.3823807582971274
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge1_acc,0.2949816401468788,0.0159644009655896
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge1_diff,-2.159339412861333,0.2209385179967685
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge2_max,7.604441562464007,0.3269185048983413
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge2_acc,0.1995104039167686,0.0139899299675596
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge2_diff,-2.736876590067832,0.2457329314578265
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rougeL_max,12.14073166236158,0.3609507196982488
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rougeL_acc,0.2900856793145654,0.0158862368742095
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rougeL_diff,-2.3125797772891734,0.2207937223844452
facebook/xglm-2.9B,truthfulqa_mc1,0-shot,accuracy,0.2141982864137087,0.0143621481556904
facebook/xglm-2.9B,winogrande,5-shot,accuracy,0.5588003157063931,0.0139549750728347
openlm-research/open_llama_7b_v2,minerva_math_precalc,5-shot,accuracy,0.0219780219780219,0.0062801549282525
openlm-research/open_llama_7b_v2,minerva_math_prealgebra,5-shot,accuracy,0.0493685419058553,0.0073446586249585
openlm-research/open_llama_7b_v2,minerva_math_num_theory,5-shot,accuracy,0.0111111111111111,0.0045150037076946
openlm-research/open_llama_7b_v2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0310077519379844,0.0057715441621136
openlm-research/open_llama_7b_v2,minerva_math_geometry,5-shot,accuracy,0.0250521920668058,0.0071482478380138
openlm-research/open_llama_7b_v2,minerva_math_counting_and_prob,5-shot,accuracy,0.050632911392405,0.0100809849342132
openlm-research/open_llama_7b_v2,minerva_math_algebra,5-shot,accuracy,0.0303285593934288,0.0049796159429974
openlm-research/open_llama_7b_v2,fld_default,0-shot,accuracy,0.0,
openlm-research/open_llama_7b_v2,fld_star,0-shot,accuracy,0.0,
openlm-research/open_llama_7b_v2,arithmetic_3da,5-shot,accuracy,0.71,0.0101489655014869
openlm-research/open_llama_7b_v2,arithmetic_3ds,5-shot,accuracy,0.3825,0.0108699564385737
openlm-research/open_llama_7b_v2,arithmetic_4da,5-shot,accuracy,0.3935,0.010926507643554
openlm-research/open_llama_7b_v2,arithmetic_2ds,5-shot,accuracy,0.4185,0.011033573531383
openlm-research/open_llama_7b_v2,arithmetic_5ds,5-shot,accuracy,0.1685,0.0083719125329717
openlm-research/open_llama_7b_v2,arithmetic_5da,5-shot,accuracy,0.1345,0.0076311199699649
openlm-research/open_llama_7b_v2,arithmetic_1dc,5-shot,accuracy,0.2,0.0089465088168516
openlm-research/open_llama_7b_v2,arithmetic_4ds,5-shot,accuracy,0.3295,0.0105128557046854
openlm-research/open_llama_7b_v2,arithmetic_2dm,5-shot,accuracy,0.291,0.0101592866655476
openlm-research/open_llama_7b_v2,arithmetic_2da,5-shot,accuracy,0.801,0.0089296903465262
openlm-research/open_llama_7b_v2,gsm8k_cot,5-shot,accuracy,0.0780894617134192,0.0073906544811082
openlm-research/open_llama_7b_v2,gsm8k,5-shot,accuracy,0.0348749052312357,0.0050534807650222
openlm-research/open_llama_7b_v2,anli_r2,0-shot,brier_score,0.750784987299882,
openlm-research/open_llama_7b_v2,anli_r3,0-shot,brier_score,0.6964174783567271,
openlm-research/open_llama_7b_v2,anli_r1,0-shot,brier_score,0.757434803319279,
openlm-research/open_llama_7b_v2,xnli_eu,0-shot,brier_score,0.988272712740076,
openlm-research/open_llama_7b_v2,xnli_vi,0-shot,brier_score,0.9074524363165406,
openlm-research/open_llama_7b_v2,xnli_ru,0-shot,brier_score,0.7900843853265546,
openlm-research/open_llama_7b_v2,xnli_zh,0-shot,brier_score,0.8965790535014185,
openlm-research/open_llama_7b_v2,xnli_tr,0-shot,brier_score,0.883110444362914,
openlm-research/open_llama_7b_v2,xnli_fr,0-shot,brier_score,0.7477033299175156,
openlm-research/open_llama_7b_v2,xnli_en,0-shot,brier_score,0.6792402837627817,
openlm-research/open_llama_7b_v2,xnli_ur,0-shot,brier_score,1.3278735930168233,
openlm-research/open_llama_7b_v2,xnli_ar,0-shot,brier_score,1.0709475690301518,
openlm-research/open_llama_7b_v2,xnli_de,0-shot,brier_score,0.7830298946319253,
openlm-research/open_llama_7b_v2,xnli_hi,0-shot,brier_score,0.9790361891860578,
openlm-research/open_llama_7b_v2,xnli_es,0-shot,brier_score,0.9468302931488434,
openlm-research/open_llama_7b_v2,xnli_bg,0-shot,brier_score,0.7759774458295697,
openlm-research/open_llama_7b_v2,xnli_sw,0-shot,brier_score,0.9755466790498818,
openlm-research/open_llama_7b_v2,xnli_el,0-shot,brier_score,1.056880149085425,
openlm-research/open_llama_7b_v2,xnli_th,0-shot,brier_score,0.9438295823457749,
openlm-research/open_llama_7b_v2,logiqa2,0-shot,brier_score,1.0170481725501903,
openlm-research/open_llama_7b_v2,mathqa,0-shot,brier_score,0.9136828407266988,
openlm-research/open_llama_7b_v2,lambada_standard,0-shot,perplexity,4.773750468568448,0.1009673907917097
openlm-research/open_llama_7b_v2,lambada_standard,0-shot,accuracy,0.6413739569183,0.0066817257434126
openlm-research/open_llama_7b_v2,lambada_openai,0-shot,perplexity,3.822540094776356,0.0781882615831751
openlm-research/open_llama_7b_v2,lambada_openai,0-shot,accuracy,0.7151174073355328,0.0062883065382526
openlm-research/open_llama_7b_v2,mmlu_world_religions,0-shot,accuracy,0.52046783625731,0.0383161053282193
openlm-research/open_llama_7b_v2,mmlu_formal_logic,0-shot,accuracy,0.3253968253968254,0.0419059643887113
openlm-research/open_llama_7b_v2,mmlu_prehistory,0-shot,accuracy,0.4351851851851852,0.0275860062216076
openlm-research/open_llama_7b_v2,mmlu_moral_scenarios,0-shot,accuracy,0.2614525139664804,0.0146965996503645
openlm-research/open_llama_7b_v2,mmlu_high_school_world_history,0-shot,accuracy,0.4852320675105485,0.0325330280787773
openlm-research/open_llama_7b_v2,mmlu_moral_disputes,0-shot,accuracy,0.4335260115606936,0.0266801347616792
openlm-research/open_llama_7b_v2,mmlu_professional_law,0-shot,accuracy,0.3357235984354628,0.0120613041576646
openlm-research/open_llama_7b_v2,mmlu_logical_fallacies,0-shot,accuracy,0.3926380368098159,0.0383674090783102
openlm-research/open_llama_7b_v2,mmlu_high_school_us_history,0-shot,accuracy,0.4411764705882353,0.0348494151442923
openlm-research/open_llama_7b_v2,mmlu_philosophy,0-shot,accuracy,0.405144694533762,0.0278823837913259
openlm-research/open_llama_7b_v2,mmlu_jurisprudence,0-shot,accuracy,0.4907407407407407,0.0483285355343705
openlm-research/open_llama_7b_v2,mmlu_international_law,0-shot,accuracy,0.5041322314049587,0.0456419876743275
openlm-research/open_llama_7b_v2,mmlu_high_school_european_history,0-shot,accuracy,0.4363636363636363,0.0387259298352475
openlm-research/open_llama_7b_v2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.5803108808290155,0.0356158732768588
openlm-research/open_llama_7b_v2,mmlu_high_school_microeconomics,0-shot,accuracy,0.3781512605042016,0.031499305777849
openlm-research/open_llama_7b_v2,mmlu_high_school_geography,0-shot,accuracy,0.4646464646464646,0.0355343636882806
openlm-research/open_llama_7b_v2,mmlu_high_school_psychology,0-shot,accuracy,0.5211009174311927,0.0214182247542646
openlm-research/open_llama_7b_v2,mmlu_public_relations,0-shot,accuracy,0.4545454545454545,0.0476930056897274
openlm-research/open_llama_7b_v2,mmlu_us_foreign_policy,0-shot,accuracy,0.55,0.05
openlm-research/open_llama_7b_v2,mmlu_sociology,0-shot,accuracy,0.5572139303482587,0.0351231096412393
openlm-research/open_llama_7b_v2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4102564102564102,0.0249393139069407
openlm-research/open_llama_7b_v2,mmlu_security_studies,0-shot,accuracy,0.4571428571428571,0.0318914183242139
openlm-research/open_llama_7b_v2,mmlu_professional_psychology,0-shot,accuracy,0.3839869281045752,0.0196758081352815
openlm-research/open_llama_7b_v2,mmlu_human_sexuality,0-shot,accuracy,0.4961832061068702,0.0438516232560155
openlm-research/open_llama_7b_v2,mmlu_econometrics,0-shot,accuracy,0.2982456140350877,0.0430368403353731
openlm-research/open_llama_7b_v2,mmlu_miscellaneous,0-shot,accuracy,0.5696040868454662,0.0177058687762923
openlm-research/open_llama_7b_v2,mmlu_marketing,0-shot,accuracy,0.6068376068376068,0.0319995792465104
openlm-research/open_llama_7b_v2,mmlu_management,0-shot,accuracy,0.5436893203883495,0.0493180199422041
openlm-research/open_llama_7b_v2,mmlu_nutrition,0-shot,accuracy,0.4215686274509804,0.0282754901567914
openlm-research/open_llama_7b_v2,mmlu_medical_genetics,0-shot,accuracy,0.53,0.0501613558046591
openlm-research/open_llama_7b_v2,mmlu_human_aging,0-shot,accuracy,0.3991031390134529,0.0328674531256796
openlm-research/open_llama_7b_v2,mmlu_professional_medicine,0-shot,accuracy,0.4375,0.0301346149544039
openlm-research/open_llama_7b_v2,mmlu_college_medicine,0-shot,accuracy,0.4046242774566474,0.0374246119388724
openlm-research/open_llama_7b_v2,mmlu_business_ethics,0-shot,accuracy,0.4,0.049236596391733
openlm-research/open_llama_7b_v2,mmlu_clinical_knowledge,0-shot,accuracy,0.4566037735849056,0.0306567486967394
openlm-research/open_llama_7b_v2,mmlu_global_facts,0-shot,accuracy,0.33,0.047258156262526
openlm-research/open_llama_7b_v2,mmlu_virology,0-shot,accuracy,0.4156626506024096,0.0383672217659805
openlm-research/open_llama_7b_v2,mmlu_professional_accounting,0-shot,accuracy,0.3120567375886525,0.0276401205451699
openlm-research/open_llama_7b_v2,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
openlm-research/open_llama_7b_v2,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.0371018572611999
openlm-research/open_llama_7b_v2,mmlu_high_school_biology,0-shot,accuracy,0.4354838709677419,0.0282062255915027
openlm-research/open_llama_7b_v2,mmlu_college_biology,0-shot,accuracy,0.4236111111111111,0.0413212501972336
openlm-research/open_llama_7b_v2,mmlu_anatomy,0-shot,accuracy,0.437037037037037,0.0428495863975339
openlm-research/open_llama_7b_v2,mmlu_college_chemistry,0-shot,accuracy,0.3,0.0460566186471838
openlm-research/open_llama_7b_v2,mmlu_computer_security,0-shot,accuracy,0.58,0.0496044963748858
openlm-research/open_llama_7b_v2,mmlu_college_computer_science,0-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,mmlu_astronomy,0-shot,accuracy,0.4276315789473684,0.0402609708329655
openlm-research/open_llama_7b_v2,mmlu_college_mathematics,0-shot,accuracy,0.33,0.047258156262526
openlm-research/open_llama_7b_v2,mmlu_conceptual_physics,0-shot,accuracy,0.3404255319148936,0.0309766929985344
openlm-research/open_llama_7b_v2,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.0429234695990928
openlm-research/open_llama_7b_v2,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.0449394906861354
openlm-research/open_llama_7b_v2,mmlu_high_school_chemistry,0-shot,accuracy,0.2463054187192118,0.0303150992856177
openlm-research/open_llama_7b_v2,mmlu_high_school_statistics,0-shot,accuracy,0.2638888888888889,0.0300582027043098
openlm-research/open_llama_7b_v2,mmlu_elementary_mathematics,0-shot,accuracy,0.2830687830687831,0.0232013929381949
openlm-research/open_llama_7b_v2,mmlu_electrical_engineering,0-shot,accuracy,0.4413793103448276,0.0413793103448275
openlm-research/open_llama_7b_v2,mmlu_high_school_mathematics,0-shot,accuracy,0.237037037037037,0.0259288761327661
openlm-research/open_llama_7b_v2,arc_challenge,25-shot,accuracy,0.4223549488054607,0.0144341387133799
openlm-research/open_llama_7b_v2,arc_challenge,25-shot,acc_norm,0.4488054607508532,0.0145345995850976
openlm-research/open_llama_7b_v2,hellaswag,10-shot,accuracy,0.5456084445329615,0.0049689792597383
openlm-research/open_llama_7b_v2,hellaswag,10-shot,acc_norm,0.7219677355108544,0.0044711373336196
openlm-research/open_llama_7b_v2,truthfulqa_mc2,0-shot,accuracy,0.3457132297166326,0.0134829264645555
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,bleu_max,26.30496139654964,0.7825243698743464
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,bleu_acc,0.2974296205630355,0.0160026514873609
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,bleu_diff,-9.780023194536044,0.8239620546869166
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge1_max,50.831523166784905,0.8615610126946267
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge1_acc,0.2729498164014688,0.0155947536320065
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge1_diff,-12.094224542000417,0.8842926117979107
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge2_max,34.74146560299905,0.9969274635318672
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge2_acc,0.2288861689106487,0.014706994909055
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge2_diff,-14.236043695877685,1.0648852083123828
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rougeL_max,47.945898558796365,0.8719715450118734
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rougeL_acc,0.2570379436964504,0.015298077509485
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rougeL_diff,-12.736671064103732,0.8845110469156796
openlm-research/open_llama_7b_v2,truthfulqa_mc1,0-shot,accuracy,0.226438188494492,0.0146513373246025
openlm-research/open_llama_7b_v2,winogrande,5-shot,accuracy,0.6937647987371744,0.0129543859728024
Monero/WizardLM-13b-OpenAssistant-Uncensored,arc:challenge,25-shot,accuracy,0.4709897610921502,0.0145867763552943
Monero/WizardLM-13b-OpenAssistant-Uncensored,arc:challenge,25-shot,acc_norm,0.4854948805460751,0.01460524108137
Monero/WizardLM-13b-OpenAssistant-Uncensored,hellaswag,10-shot,accuracy,0.569806811392153,0.0049409117792733
Monero/WizardLM-13b-OpenAssistant-Uncensored,hellaswag,10-shot,acc_norm,0.7603067118103963,0.0042602380336579
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.047258156262526
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.047258156262526
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-anatomy,5-shot,accuracy,0.4,0.0423207369515158
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-anatomy,5-shot,acc_norm,0.4,0.0423207369515158
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-astronomy,5-shot,accuracy,0.3157894736842105,0.0378272898086547
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-astronomy,5-shot,acc_norm,0.3157894736842105,0.0378272898086547
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-business_ethics,5-shot,accuracy,0.47,0.0501613558046591
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-business_ethics,5-shot,acc_norm,0.47,0.0501613558046591
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4528301886792453,0.0306356279579618
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4528301886792453,0.0306356279579618
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_biology,5-shot,accuracy,0.4097222222222222,0.0411249097467078
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_biology,5-shot,acc_norm,0.4097222222222222,0.0411249097467078
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.0440844002276807
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.0440844002276807
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_computer_science,5-shot,accuracy,0.4,0.049236596391733
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_computer_science,5-shot,acc_norm,0.4,0.049236596391733
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_mathematics,5-shot,accuracy,0.41,0.049431107042371
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_mathematics,5-shot,acc_norm,0.41,0.049431107042371
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_medicine,5-shot,accuracy,0.3815028901734104,0.0370385119309952
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_medicine,5-shot,acc_norm,0.3815028901734104,0.0370385119309952
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_physics,5-shot,accuracy,0.1862745098039215,0.0387395871414935
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_physics,5-shot,acc_norm,0.1862745098039215,0.0387395871414935
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-computer_security,5-shot,accuracy,0.55,0.0499999999999999
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-computer_security,5-shot,acc_norm,0.55,0.0499999999999999
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3787234042553191,0.0317099560604065
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3787234042553191,0.0317099560604065
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-econometrics,5-shot,accuracy,0.2807017543859649,0.042270544512322
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-econometrics,5-shot,acc_norm,0.2807017543859649,0.042270544512322
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3724137931034483,0.0402873153294755
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3724137931034483,0.0402873153294755
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3042328042328042,0.023695415009463
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3042328042328042,0.023695415009463
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-formal_logic,5-shot,accuracy,0.2698412698412698,0.0397015827323517
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-formal_logic,5-shot,acc_norm,0.2698412698412698,0.0397015827323517
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-global_facts,5-shot,accuracy,0.37,0.0485236587093909
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-global_facts,5-shot,acc_norm,0.37,0.0485236587093909
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_biology,5-shot,accuracy,0.4161290322580645,0.0280409813807615
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4161290322580645,0.0280409813807615
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2857142857142857,0.0317852971064274
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2857142857142857,0.0317852971064274
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.46,0.0500908265962033
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.46,0.0500908265962033
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_european_history,5-shot,accuracy,0.5151515151515151,0.0390255100737444
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.5151515151515151,0.0390255100737444
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_geography,5-shot,accuracy,0.5202020202020202,0.0355944356556391
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5202020202020202,0.0355944356556391
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6113989637305699,0.0351773979637313
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6113989637305699,0.0351773979637313
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3974358974358974,0.0248119200179038
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3974358974358974,0.0248119200179038
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2703703703703703,0.0270803728151456
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2703703703703703,0.0270803728151456
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.4159663865546218,0.0320165010073961
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.4159663865546218,0.0320165010073961
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.0329798664847383
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.0329798664847383
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_psychology,5-shot,accuracy,0.5394495412844037,0.021370494609995
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.5394495412844037,0.021370494609995
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2777777777777778,0.0305467452649531
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2777777777777778,0.0305467452649531
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_us_history,5-shot,accuracy,0.5784313725490197,0.0346586819638076
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.5784313725490197,0.0346586819638076
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_world_history,5-shot,accuracy,0.5738396624472574,0.0321903570313177
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.5738396624472574,0.0321903570313177
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_aging,5-shot,accuracy,0.5650224215246636,0.0332728337027134
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_aging,5-shot,acc_norm,0.5650224215246636,0.0332728337027134
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_sexuality,5-shot,accuracy,0.4198473282442748,0.0432857721526297
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_sexuality,5-shot,acc_norm,0.4198473282442748,0.0432857721526297
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-international_law,5-shot,accuracy,0.5371900826446281,0.0455171119610421
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-international_law,5-shot,acc_norm,0.5371900826446281,0.0455171119610421
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-jurisprudence,5-shot,accuracy,0.5277777777777778,0.0482621729413989
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5277777777777778,0.0482621729413989
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-logical_fallacies,5-shot,accuracy,0.4355828220858895,0.0389563246413893
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.4355828220858895,0.0389563246413893
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-machine_learning,5-shot,accuracy,0.4017857142857143,0.0465333314697364
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-machine_learning,5-shot,acc_norm,0.4017857142857143,0.0465333314697364
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-management,5-shot,accuracy,0.5728155339805825,0.0489795773778116
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-management,5-shot,acc_norm,0.5728155339805825,0.0489795773778116
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-marketing,5-shot,accuracy,0.6837606837606838,0.0304636567473402
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-marketing,5-shot,acc_norm,0.6837606837606838,0.0304636567473402
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-medical_genetics,5-shot,accuracy,0.52,0.0502116731568677
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-medical_genetics,5-shot,acc_norm,0.52,0.0502116731568677
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-miscellaneous,5-shot,accuracy,0.6091954022988506,0.0174483660670625
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6091954022988506,0.0174483660670625
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_disputes,5-shot,accuracy,0.4277456647398844,0.026636539741116
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_disputes,5-shot,acc_norm,0.4277456647398844,0.026636539741116
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2670391061452514,0.0147965026225625
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2670391061452514,0.0147965026225625
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-nutrition,5-shot,accuracy,0.4411764705882353,0.0284310954441766
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-nutrition,5-shot,acc_norm,0.4411764705882353,0.0284310954441766
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-philosophy,5-shot,accuracy,0.472668810289389,0.0283556335683281
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-philosophy,5-shot,acc_norm,0.472668810289389,0.0283556335683281
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-prehistory,5-shot,accuracy,0.4660493827160494,0.0277565352573476
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-prehistory,5-shot,acc_norm,0.4660493827160494,0.0277565352573476
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_accounting,5-shot,accuracy,0.3475177304964539,0.0284066278095909
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3475177304964539,0.0284066278095909
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_law,5-shot,accuracy,0.3376792698826597,0.0120785637771455
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_law,5-shot,acc_norm,0.3376792698826597,0.0120785637771455
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_medicine,5-shot,accuracy,0.4154411764705882,0.0299353427078777
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4154411764705882,0.0299353427078777
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_psychology,5-shot,accuracy,0.434640522875817,0.0200542692007264
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_psychology,5-shot,acc_norm,0.434640522875817,0.0200542692007264
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-public_relations,5-shot,accuracy,0.5181818181818182,0.0478596401079491
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-public_relations,5-shot,acc_norm,0.5181818181818182,0.0478596401079491
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-security_studies,5-shot,accuracy,0.4285714285714285,0.0316809116123388
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-security_studies,5-shot,acc_norm,0.4285714285714285,0.0316809116123388
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-sociology,5-shot,accuracy,0.5074626865671642,0.0353514008427671
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-sociology,5-shot,acc_norm,0.5074626865671642,0.0353514008427671
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.63,0.0485236587093909
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.63,0.0485236587093909
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-virology,5-shot,accuracy,0.3855421686746988,0.0378913442461155
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-virology,5-shot,acc_norm,0.3855421686746988,0.0378913442461155
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-world_religions,5-shot,accuracy,0.631578947368421,0.0369965801765687
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-world_religions,5-shot,acc_norm,0.631578947368421,0.0369965801765687
Monero/WizardLM-13b-OpenAssistant-Uncensored,truthfulqa:mc,0-shot,mc1,0.3329253365973072,0.016497402382012
Monero/WizardLM-13b-OpenAssistant-Uncensored,truthfulqa:mc,0-shot,mc2,0.4940407873488723,0.0159835958356283
Monero/WizardLM-13b-OpenAssistant-Uncensored,drop,3-shot,accuracy,0.0803271812080537,0.0027834767010105
Monero/WizardLM-13b-OpenAssistant-Uncensored,drop,3-shot,f1,0.1744945469798657,0.0031261159442318
Monero/WizardLM-13b-OpenAssistant-Uncensored,gsm8k,5-shot,accuracy,0.1690674753601213,0.0103241714454973
Monero/WizardLM-13b-OpenAssistant-Uncensored,winogrande,5-shot,accuracy,0.6977111286503551,0.0129072003616275
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_precalc,5-shot,accuracy,0.0128205128205128,0.0048189509824876
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_prealgebra,5-shot,accuracy,0.0482204362801377,0.0072631352121036
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_num_theory,5-shot,accuracy,0.0092592592592592,0.0041254730154902
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_intermediate_algebra,5-shot,accuracy,0.0099667774086378,0.003307493466972
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_geometry,5-shot,accuracy,0.0104384133611691,0.0046486271171846
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.0062753625139896
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_algebra,5-shot,accuracy,0.02106149957877,0.004169461854206
Monero/WizardLM-13b-OpenAssistant-Uncensored,fld_default,0-shot,accuracy,0.0,
Monero/WizardLM-13b-OpenAssistant-Uncensored,fld_star,0-shot,accuracy,0.0,
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_3da,5-shot,accuracy,0.494,0.0111823308062822
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_3ds,5-shot,accuracy,0.0925,0.0064801906943945
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_4da,5-shot,accuracy,0.143,0.0078298245878525
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_2ds,5-shot,accuracy,0.092,0.0064644330337025
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_5ds,5-shot,accuracy,0.0105,0.0022797968630709
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_5da,5-shot,accuracy,0.0185,0.0030138707185866
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_1dc,5-shot,accuracy,0.0,
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_4ds,5-shot,accuracy,0.0655,0.0055335508575005
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_2dm,5-shot,accuracy,0.1065,0.0068994692794569
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_2da,5-shot,accuracy,0.569,0.0110761383351875
Monero/WizardLM-13b-OpenAssistant-Uncensored,gsm8k_cot,5-shot,accuracy,0.1993934799090219,0.0110054380294756
Monero/WizardLM-13b-OpenAssistant-Uncensored,anli_r2,0-shot,brier_score,0.9518990734725756,
Monero/WizardLM-13b-OpenAssistant-Uncensored,anli_r3,0-shot,brier_score,0.905021916309984,
Monero/WizardLM-13b-OpenAssistant-Uncensored,anli_r1,0-shot,brier_score,0.9499203600796312,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_eu,0-shot,brier_score,1.2116651846902302,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_vi,0-shot,brier_score,1.1118470416477884,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_ru,0-shot,brier_score,0.8328410171110645,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_zh,0-shot,brier_score,1.1789746567492252,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_tr,0-shot,brier_score,1.153573017626289,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_fr,0-shot,brier_score,1.008165615172993,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_en,0-shot,brier_score,0.9233846980099404,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_ur,0-shot,brier_score,1.3155356160757106,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_ar,0-shot,brier_score,1.274329293402633,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_de,0-shot,brier_score,0.9779918916363576,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_hi,0-shot,brier_score,0.920937714318964,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_es,0-shot,brier_score,0.9890453652911488,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_bg,0-shot,brier_score,0.902415483528614,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_sw,0-shot,brier_score,0.9466655510728088,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_el,0-shot,brier_score,0.9289485390405852,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_th,0-shot,brier_score,1.0414528699854,
Monero/WizardLM-13b-OpenAssistant-Uncensored,logiqa2,0-shot,brier_score,1.150759024889456,
Monero/WizardLM-13b-OpenAssistant-Uncensored,mathqa,0-shot,brier_score,1.0865117207773218,
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_standard,0-shot,perplexity,4.612695206892958,0.162544749476321
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_standard,0-shot,accuracy,0.6541820298855036,0.0066265145588051
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_openai,0-shot,perplexity,3.3610202083230787,0.1123279051077298
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_openai,0-shot,accuracy,0.7240442460702503,0.0062275088455456
google/gemma-7b,mmlu_world_religions,0-shot,accuracy,0.8421052631578947,0.0279667858591608
google/gemma-7b,mmlu_formal_logic,0-shot,accuracy,0.5,0.0447213595499957
google/gemma-7b,mmlu_prehistory,0-shot,accuracy,0.7314814814814815,0.0246596851859672
google/gemma-7b,mmlu_moral_scenarios,0-shot,accuracy,0.3150837988826815,0.0155368508524736
google/gemma-7b,mmlu_high_school_world_history,0-shot,accuracy,0.8227848101265823,0.0248563641845032
google/gemma-7b,mmlu_moral_disputes,0-shot,accuracy,0.6676300578034682,0.0253611687496882
google/gemma-7b,mmlu_professional_law,0-shot,accuracy,0.4661016949152542,0.0127408538729498
google/gemma-7b,mmlu_logical_fallacies,0-shot,accuracy,0.7484662576687117,0.0340899788685752
google/gemma-7b,mmlu_high_school_us_history,0-shot,accuracy,0.7843137254901961,0.0288674314498493
google/gemma-7b,mmlu_philosophy,0-shot,accuracy,0.7009646302250804,0.0260033011178851
google/gemma-7b,mmlu_jurisprudence,0-shot,accuracy,0.75,0.041860917913946
google/gemma-7b,mmlu_international_law,0-shot,accuracy,0.8512396694214877,0.0324847008380719
google/gemma-7b,mmlu_high_school_european_history,0-shot,accuracy,0.7575757575757576,0.0334640988105595
google/gemma-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8860103626943006,0.0229351440539194
google/gemma-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.6470588235294118,0.0310419413040592
google/gemma-7b,mmlu_high_school_geography,0-shot,accuracy,0.803030303030303,0.0283356097324633
google/gemma-7b,mmlu_high_school_psychology,0-shot,accuracy,0.8128440366972477,0.0167226845262001
google/gemma-7b,mmlu_public_relations,0-shot,accuracy,0.6545454545454545,0.0455461961754105
google/gemma-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.87,0.033799766898963
google/gemma-7b,mmlu_sociology,0-shot,accuracy,0.835820895522388,0.0261939235444541
google/gemma-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6307692307692307,0.0244686152414789
google/gemma-7b,mmlu_security_studies,0-shot,accuracy,0.7183673469387755,0.0287951855742912
google/gemma-7b,mmlu_professional_psychology,0-shot,accuracy,0.6895424836601307,0.0187180670526232
google/gemma-7b,mmlu_human_sexuality,0-shot,accuracy,0.7251908396946565,0.0391534540884783
google/gemma-7b,mmlu_econometrics,0-shot,accuracy,0.4561403508771929,0.0468547304190778
google/gemma-7b,mmlu_miscellaneous,0-shot,accuracy,0.8416347381864623,0.0130553467535167
google/gemma-7b,mmlu_marketing,0-shot,accuracy,0.8931623931623932,0.0202371490089909
google/gemma-7b,mmlu_management,0-shot,accuracy,0.8252427184466019,0.0376017800602662
google/gemma-7b,mmlu_nutrition,0-shot,accuracy,0.7418300653594772,0.0250585033169581
google/gemma-7b,mmlu_medical_genetics,0-shot,accuracy,0.75,0.0435194139889244
google/gemma-7b,mmlu_human_aging,0-shot,accuracy,0.7085201793721974,0.0305002831765458
google/gemma-7b,mmlu_professional_medicine,0-shot,accuracy,0.5772058823529411,0.0300085628450034
google/gemma-7b,mmlu_college_medicine,0-shot,accuracy,0.6473988439306358,0.0364303716895854
google/gemma-7b,mmlu_business_ethics,0-shot,accuracy,0.64,0.0482418151324421
google/gemma-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.660377358490566,0.0291469047477983
google/gemma-7b,mmlu_global_facts,0-shot,accuracy,0.35,0.0479372485441102
google/gemma-7b,mmlu_virology,0-shot,accuracy,0.5240963855421686,0.0388797184959726
google/gemma-7b,mmlu_professional_accounting,0-shot,accuracy,0.4609929078014184,0.0297365925264244
google/gemma-7b,mmlu_college_physics,0-shot,accuracy,0.3823529411764705,0.0483550369610722
google/gemma-7b,mmlu_high_school_physics,0-shot,accuracy,0.3973509933774834,0.0399552400768168
google/gemma-7b,mmlu_high_school_biology,0-shot,accuracy,0.8,0.0227552049595429
google/gemma-7b,mmlu_college_biology,0-shot,accuracy,0.7291666666666666,0.0371617743756601
google/gemma-7b,mmlu_anatomy,0-shot,accuracy,0.5259259259259259,0.0431353169675057
google/gemma-7b,mmlu_college_chemistry,0-shot,accuracy,0.48,0.0502116731568677
google/gemma-7b,mmlu_computer_security,0-shot,accuracy,0.75,0.0435194139889244
google/gemma-7b,mmlu_college_computer_science,0-shot,accuracy,0.49,0.050241839379569
google/gemma-7b,mmlu_astronomy,0-shot,accuracy,0.6907894736842105,0.0376107086986747
google/gemma-7b,mmlu_college_mathematics,0-shot,accuracy,0.38,0.0487831731214563
google/gemma-7b,mmlu_conceptual_physics,0-shot,accuracy,0.5829787234042553,0.0322327626671171
google/gemma-7b,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.0429234695990928
google/gemma-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.65,0.0479372485441101
google/gemma-7b,mmlu_machine_learning,0-shot,accuracy,0.5535714285714286,0.0471847148521958
google/gemma-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.5024630541871922,0.0351794503869106
google/gemma-7b,mmlu_high_school_statistics,0-shot,accuracy,0.4814814814814814,0.0340763209385405
google/gemma-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.4603174603174603,0.0256700806369091
google/gemma-7b,mmlu_electrical_engineering,0-shot,accuracy,0.6344827586206897,0.0401312419542438
google/gemma-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.362962962962963,0.0293182036452068
google/gemma-7b,arc_challenge,25-shot,accuracy,0.5708191126279863,0.0144640858948706
google/gemma-7b,arc_challenge,25-shot,acc_norm,0.6049488054607508,0.0142858982929381
google/gemma-7b,hellaswag,10-shot,accuracy,0.6207926707827126,0.0048419819735153
google/gemma-7b,hellaswag,10-shot,acc_norm,0.8228440549691296,0.003810203308901
google/gemma-7b,truthfulqa_mc2,0-shot,accuracy,0.4510998237487311,0.0146903617212972
google/gemma-7b,truthfulqa_gen,0-shot,bleu_max,27.01069206803005,0.8195476991507818
google/gemma-7b,truthfulqa_gen,0-shot,bleu_acc,0.4394124847001224,0.0173745204825137
google/gemma-7b,truthfulqa_gen,0-shot,bleu_diff,0.7951375918699185,0.9114193943215876
google/gemma-7b,truthfulqa_gen,0-shot,rouge1_max,52.92972110420629,0.8888037925222474
google/gemma-7b,truthfulqa_gen,0-shot,rouge1_acc,0.4406364749082007,0.0173796975554374
google/gemma-7b,truthfulqa_gen,0-shot,rouge1_diff,1.0282214491941382,1.1490330858956697
google/gemma-7b,truthfulqa_gen,0-shot,rouge2_max,37.75976775138884,1.0663490449012212
google/gemma-7b,truthfulqa_gen,0-shot,rouge2_acc,0.3671970624235006,0.0168748050014531
google/gemma-7b,truthfulqa_gen,0-shot,rouge2_diff,-0.4520668868748621,1.3041898041246565
google/gemma-7b,truthfulqa_gen,0-shot,rougeL_max,49.90048034353921,0.9075990783414511
google/gemma-7b,truthfulqa_gen,0-shot,rougeL_acc,0.4210526315789473,0.0172839362481364
google/gemma-7b,truthfulqa_gen,0-shot,rougeL_diff,0.7823563764856337,1.1650307856683864
google/gemma-7b,truthfulqa_mc1,0-shot,accuracy,0.3108935128518972,0.0162033166735596
google/gemma-7b,winogrande,5-shot,accuracy,0.760852407261247,0.0119885418448439
google/gemma-7b,gsm8k,5-shot,accuracy,0.5382865807429871,0.0137320482270166
meta-llama/Llama-2-70b-hf,drop,3-shot,accuracy,0.0017827181208053,0.0004320097346038
meta-llama/Llama-2-70b-hf,drop,3-shot,f1,0.0661556208053691,0.0013739852117668
meta-llama/Llama-2-70b-hf,gsm8k,5-shot,accuracy,0.5405610310841547,0.0137270930104297
meta-llama/Llama-2-70b-hf,winogrande,5-shot,accuracy,0.8374112075769534,0.0103704555513433
openlm-research/open_llama_7b_v2,arc:challenge,25-shot,accuracy,0.4232081911262799,0.014438036220848
openlm-research/open_llama_7b_v2,arc:challenge,25-shot,acc_norm,0.4368600682593856,0.0144944215842565
openlm-research/open_llama_7b_v2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.24,0.0429234695990928
openlm-research/open_llama_7b_v2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.24,0.0429234695990928
openlm-research/open_llama_7b_v2,hendrycksTest-anatomy,5-shot,accuracy,0.437037037037037,0.0428495863975339
openlm-research/open_llama_7b_v2,hendrycksTest-anatomy,5-shot,acc_norm,0.437037037037037,0.0428495863975339
openlm-research/open_llama_7b_v2,hendrycksTest-astronomy,5-shot,accuracy,0.4407894736842105,0.0404031106249043
openlm-research/open_llama_7b_v2,hendrycksTest-astronomy,5-shot,acc_norm,0.4407894736842105,0.0404031106249043
openlm-research/open_llama_7b_v2,hendrycksTest-business_ethics,5-shot,accuracy,0.41,0.049431107042371
openlm-research/open_llama_7b_v2,hendrycksTest-business_ethics,5-shot,acc_norm,0.41,0.049431107042371
openlm-research/open_llama_7b_v2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4641509433962264,0.030693675018458
openlm-research/open_llama_7b_v2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4641509433962264,0.030693675018458
openlm-research/open_llama_7b_v2,hendrycksTest-college_biology,5-shot,accuracy,0.4305555555555556,0.041406856391115
openlm-research/open_llama_7b_v2,hendrycksTest-college_biology,5-shot,acc_norm,0.4305555555555556,0.041406856391115
openlm-research/open_llama_7b_v2,hendrycksTest-college_chemistry,5-shot,accuracy,0.3,0.0460566186471838
openlm-research/open_llama_7b_v2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.3,0.0460566186471838
openlm-research/open_llama_7b_v2,hendrycksTest-college_computer_science,5-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,hendrycksTest-college_mathematics,5-shot,accuracy,0.33,0.047258156262526
openlm-research/open_llama_7b_v2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.33,0.047258156262526
openlm-research/open_llama_7b_v2,hendrycksTest-college_medicine,5-shot,accuracy,0.3988439306358382,0.037336266553835
openlm-research/open_llama_7b_v2,hendrycksTest-college_medicine,5-shot,acc_norm,0.3988439306358382,0.037336266553835
openlm-research/open_llama_7b_v2,hendrycksTest-college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
openlm-research/open_llama_7b_v2,hendrycksTest-college_physics,5-shot,acc_norm,0.2254901960784313,0.0415830753308328
openlm-research/open_llama_7b_v2,hendrycksTest-computer_security,5-shot,accuracy,0.56,0.0498887651569858
openlm-research/open_llama_7b_v2,hendrycksTest-computer_security,5-shot,acc_norm,0.56,0.0498887651569858
openlm-research/open_llama_7b_v2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3446808510638298,0.0310689859631221
openlm-research/open_llama_7b_v2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3446808510638298,0.0310689859631221
openlm-research/open_llama_7b_v2,hendrycksTest-econometrics,5-shot,accuracy,0.3070175438596491,0.0433913832257986
openlm-research/open_llama_7b_v2,hendrycksTest-econometrics,5-shot,acc_norm,0.3070175438596491,0.0433913832257986
openlm-research/open_llama_7b_v2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.4344827586206896,0.0413074087955549
openlm-research/open_llama_7b_v2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.4344827586206896,0.0413074087955549
openlm-research/open_llama_7b_v2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2804232804232804,0.0231352879743256
openlm-research/open_llama_7b_v2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2804232804232804,0.0231352879743256
openlm-research/open_llama_7b_v2,hendrycksTest-formal_logic,5-shot,accuracy,0.3571428571428571,0.0428571428571428
openlm-research/open_llama_7b_v2,hendrycksTest-formal_logic,5-shot,acc_norm,0.3571428571428571,0.0428571428571428
openlm-research/open_llama_7b_v2,hendrycksTest-global_facts,5-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,hendrycksTest-global_facts,5-shot,acc_norm,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_biology,5-shot,accuracy,0.4451612903225806,0.0282724101862149
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4451612903225806,0.0282724101862149
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2660098522167488,0.0310898260029375
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2660098522167488,0.0310898260029375
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.0476095228569523
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.4242424242424242,0.0385926814207026
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.4242424242424242,0.0385926814207026
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_geography,5-shot,accuracy,0.4696969696969697,0.0355580405176392
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.4696969696969697,0.0355580405176392
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.5803108808290155,0.0356158732768588
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.5803108808290155,0.0356158732768588
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4051282051282051,0.0248904717699381
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4051282051282051,0.0248904717699381
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.237037037037037,0.0259288761327661
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.237037037037037,0.0259288761327661
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3823529411764705,0.0315666309921541
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3823529411764705,0.0315666309921541
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_physics,5-shot,accuracy,0.304635761589404,0.0375794992294334
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.304635761589404,0.0375794992294334
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.5284403669724771,0.021402615697348
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.5284403669724771,0.021402615697348
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2731481481481481,0.0303880513016781
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2731481481481481,0.0303880513016781
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.4509803921568627,0.0349240610416361
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.4509803921568627,0.0349240610416361
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.4767932489451477,0.0325121520114101
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.4767932489451477,0.0325121520114101
openlm-research/open_llama_7b_v2,hendrycksTest-human_aging,5-shot,accuracy,0.4170403587443946,0.0330926693607172
openlm-research/open_llama_7b_v2,hendrycksTest-human_aging,5-shot,acc_norm,0.4170403587443946,0.0330926693607172
openlm-research/open_llama_7b_v2,hendrycksTest-human_sexuality,5-shot,accuracy,0.5114503816793893,0.0438414002407801
openlm-research/open_llama_7b_v2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5114503816793893,0.0438414002407801
openlm-research/open_llama_7b_v2,hendrycksTest-international_law,5-shot,accuracy,0.4876033057851239,0.0456295154818076
openlm-research/open_llama_7b_v2,hendrycksTest-international_law,5-shot,acc_norm,0.4876033057851239,0.0456295154818076
openlm-research/open_llama_7b_v2,hendrycksTest-jurisprudence,5-shot,accuracy,0.5185185185185185,0.0483036602463533
openlm-research/open_llama_7b_v2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5185185185185185,0.0483036602463533
openlm-research/open_llama_7b_v2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3803680981595092,0.0381426989326183
openlm-research/open_llama_7b_v2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3803680981595092,0.0381426989326183
openlm-research/open_llama_7b_v2,hendrycksTest-machine_learning,5-shot,accuracy,0.3214285714285714,0.0443280405529152
openlm-research/open_llama_7b_v2,hendrycksTest-machine_learning,5-shot,acc_norm,0.3214285714285714,0.0443280405529152
openlm-research/open_llama_7b_v2,hendrycksTest-management,5-shot,accuracy,0.5728155339805825,0.0489795773778116
openlm-research/open_llama_7b_v2,hendrycksTest-management,5-shot,acc_norm,0.5728155339805825,0.0489795773778116
openlm-research/open_llama_7b_v2,hendrycksTest-marketing,5-shot,accuracy,0.6111111111111112,0.0319370572620029
openlm-research/open_llama_7b_v2,hendrycksTest-marketing,5-shot,acc_norm,0.6111111111111112,0.0319370572620029
openlm-research/open_llama_7b_v2,hendrycksTest-medical_genetics,5-shot,accuracy,0.53,0.0501613558046592
openlm-research/open_llama_7b_v2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.53,0.0501613558046592
openlm-research/open_llama_7b_v2,hendrycksTest-miscellaneous,5-shot,accuracy,0.5683269476372924,0.0177122289392998
openlm-research/open_llama_7b_v2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.5683269476372924,0.0177122289392998
openlm-research/open_llama_7b_v2,hendrycksTest-moral_disputes,5-shot,accuracy,0.4508670520231214,0.0267888119315627
openlm-research/open_llama_7b_v2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.4508670520231214,0.0267888119315627
openlm-research/open_llama_7b_v2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2480446927374301,0.0144441578082614
openlm-research/open_llama_7b_v2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2480446927374301,0.0144441578082614
openlm-research/open_llama_7b_v2,hendrycksTest-nutrition,5-shot,accuracy,0.4281045751633986,0.0283323974836642
openlm-research/open_llama_7b_v2,hendrycksTest-nutrition,5-shot,acc_norm,0.4281045751633986,0.0283323974836642
openlm-research/open_llama_7b_v2,hendrycksTest-philosophy,5-shot,accuracy,0.4019292604501607,0.0278464760059304
openlm-research/open_llama_7b_v2,hendrycksTest-philosophy,5-shot,acc_norm,0.4019292604501607,0.0278464760059304
openlm-research/open_llama_7b_v2,hendrycksTest-prehistory,5-shot,accuracy,0.4382716049382716,0.0276079140874004
openlm-research/open_llama_7b_v2,hendrycksTest-prehistory,5-shot,acc_norm,0.4382716049382716,0.0276079140874004
openlm-research/open_llama_7b_v2,hendrycksTest-professional_accounting,5-shot,accuracy,0.3156028368794326,0.0277249894495093
openlm-research/open_llama_7b_v2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3156028368794326,0.0277249894495093
openlm-research/open_llama_7b_v2,hendrycksTest-professional_law,5-shot,accuracy,0.3363754889178618,0.0120670830794522
openlm-research/open_llama_7b_v2,hendrycksTest-professional_law,5-shot,acc_norm,0.3363754889178618,0.0120670830794522
openlm-research/open_llama_7b_v2,hendrycksTest-professional_medicine,5-shot,accuracy,0.4448529411764705,0.0301875320603293
openlm-research/open_llama_7b_v2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4448529411764705,0.0301875320603293
openlm-research/open_llama_7b_v2,hendrycksTest-professional_psychology,5-shot,accuracy,0.3627450980392157,0.0194507684325055
openlm-research/open_llama_7b_v2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.3627450980392157,0.0194507684325055
openlm-research/open_llama_7b_v2,hendrycksTest-public_relations,5-shot,accuracy,0.4636363636363636,0.0477644916239619
openlm-research/open_llama_7b_v2,hendrycksTest-public_relations,5-shot,acc_norm,0.4636363636363636,0.0477644916239619
openlm-research/open_llama_7b_v2,hendrycksTest-security_studies,5-shot,accuracy,0.4489795918367347,0.0318421386668757
openlm-research/open_llama_7b_v2,hendrycksTest-security_studies,5-shot,acc_norm,0.4489795918367347,0.0318421386668757
openlm-research/open_llama_7b_v2,hendrycksTest-sociology,5-shot,accuracy,0.5771144278606966,0.0349323177742128
openlm-research/open_llama_7b_v2,hendrycksTest-sociology,5-shot,acc_norm,0.5771144278606966,0.0349323177742128
openlm-research/open_llama_7b_v2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.53,0.0501613558046592
openlm-research/open_llama_7b_v2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.53,0.0501613558046592
openlm-research/open_llama_7b_v2,hendrycksTest-virology,5-shot,accuracy,0.4156626506024096,0.0383672217659805
openlm-research/open_llama_7b_v2,hendrycksTest-virology,5-shot,acc_norm,0.4156626506024096,0.0383672217659805
openlm-research/open_llama_7b_v2,hendrycksTest-world_religions,5-shot,accuracy,0.5321637426900585,0.0382688241766036
openlm-research/open_llama_7b_v2,hendrycksTest-world_religions,5-shot,acc_norm,0.5321637426900585,0.0382688241766036
openlm-research/open_llama_7b_v2,truthfulqa:mc,0-shot,mc1,0.237454100367197,0.0148962774410418
openlm-research/open_llama_7b_v2,truthfulqa:mc,0-shot,mc2,0.3553921542940895,0.0136113904795235
openlm-research/open_llama_7b_v2,drop,3-shot,accuracy,0.0011535234899328,0.0003476179896857
openlm-research/open_llama_7b_v2,drop,3-shot,f1,0.0549307885906042,0.0013198629767466
jisukim8873/falcon-7B-case-3,arc:challenge,25-shot,accuracy,0.4411262798634812,0.0145097477490646
jisukim8873/falcon-7B-case-3,arc:challenge,25-shot,acc_norm,0.4778156996587031,0.0145970019270761
jisukim8873/falcon-7B-case-3,hellaswag,10-shot,accuracy,0.5955984863572994,0.0048977283707372
jisukim8873/falcon-7B-case-3,hellaswag,10-shot,acc_norm,0.783011352320255,0.0041135241598451
jisukim8873/falcon-7B-case-3,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-3,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-3,hendrycksTest-anatomy,5-shot,accuracy,0.3333333333333333,0.0407231481187683
jisukim8873/falcon-7B-case-3,hendrycksTest-anatomy,5-shot,acc_norm,0.3333333333333333,0.0407231481187683
jisukim8873/falcon-7B-case-3,hendrycksTest-astronomy,5-shot,accuracy,0.2960526315789473,0.037150621549989
jisukim8873/falcon-7B-case-3,hendrycksTest-astronomy,5-shot,acc_norm,0.2960526315789473,0.037150621549989
jisukim8873/falcon-7B-case-3,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.0416333199893227
jisukim8873/falcon-7B-case-3,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.0416333199893227
jisukim8873/falcon-7B-case-3,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.350943396226415,0.0293736462532346
jisukim8873/falcon-7B-case-3,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.350943396226415,0.0293736462532346
jisukim8873/falcon-7B-case-3,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
jisukim8873/falcon-7B-case-3,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.0368565109589753
jisukim8873/falcon-7B-case-3,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-3,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.0386122919665369
jisukim8873/falcon-7B-case-3,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-3,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.0460566186471838
jisukim8873/falcon-7B-case-3,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-3,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.0451260859854212
jisukim8873/falcon-7B-case-3,hendrycksTest-college_medicine,5-shot,accuracy,0.3526011560693641,0.0364303716895854
jisukim8873/falcon-7B-case-3,hendrycksTest-college_medicine,5-shot,acc_norm,0.3526011560693641,0.0364303716895854
jisukim8873/falcon-7B-case-3,hendrycksTest-college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
jisukim8873/falcon-7B-case-3,hendrycksTest-college_physics,5-shot,acc_norm,0.2058823529411764,0.0402338227361774
jisukim8873/falcon-7B-case-3,hendrycksTest-computer_security,5-shot,accuracy,0.42,0.0496044963748858
jisukim8873/falcon-7B-case-3,hendrycksTest-computer_security,5-shot,acc_norm,0.42,0.0496044963748858
jisukim8873/falcon-7B-case-3,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3531914893617021,0.0312453252027619
jisukim8873/falcon-7B-case-3,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3531914893617021,0.0312453252027619
jisukim8873/falcon-7B-case-3,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.0426633944315939
jisukim8873/falcon-7B-case-3,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.0426633944315939
jisukim8873/falcon-7B-case-3,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.0380614268730999
jisukim8873/falcon-7B-case-3,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.0380614268730999
jisukim8873/falcon-7B-case-3,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2407407407407407,0.0220190800122179
jisukim8873/falcon-7B-case-3,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2407407407407407,0.0220190800122179
jisukim8873/falcon-7B-case-3,hendrycksTest-formal_logic,5-shot,accuracy,0.2063492063492063,0.0361960452412425
jisukim8873/falcon-7B-case-3,hendrycksTest-formal_logic,5-shot,acc_norm,0.2063492063492063,0.0361960452412425
jisukim8873/falcon-7B-case-3,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526
jisukim8873/falcon-7B-case-3,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_biology,5-shot,accuracy,0.3354838709677419,0.0268602064447243
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3354838709677419,0.0268602064447243
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3004926108374384,0.0322579947623348
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3004926108374384,0.0322579947623348
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.36,0.0482418151324421
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_european_history,5-shot,accuracy,0.296969696969697,0.0356796977226804
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.296969696969697,0.0356796977226804
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_geography,5-shot,accuracy,0.3939393939393939,0.0348128533823296
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3939393939393939,0.0348128533823296
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3523316062176165,0.0344747828641435
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3523316062176165,0.0344747828641435
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3051282051282051,0.0233463352933258
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3051282051282051,0.0233463352933258
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2481481481481481,0.0263357394040558
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2481481481481481,0.0263357394040558
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3067226890756303,0.029953823891887
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3067226890756303,0.029953823891887
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.0368488152138902
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.0368488152138902
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3192660550458716,0.01998782906975
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3192660550458716,0.01998782906975
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1851851851851851,0.0264919147273551
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1851851851851851,0.0264919147273551
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3235294117647059,0.0328347205610856
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3235294117647059,0.0328347205610856
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3628691983122363,0.0312992082553021
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3628691983122363,0.0312992082553021
jisukim8873/falcon-7B-case-3,hendrycksTest-human_aging,5-shot,accuracy,0.3721973094170404,0.0324430528300873
jisukim8873/falcon-7B-case-3,hendrycksTest-human_aging,5-shot,acc_norm,0.3721973094170404,0.0324430528300873
jisukim8873/falcon-7B-case-3,hendrycksTest-human_sexuality,5-shot,accuracy,0.3511450381679389,0.0418644516301374
jisukim8873/falcon-7B-case-3,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3511450381679389,0.0418644516301374
jisukim8873/falcon-7B-case-3,hendrycksTest-international_law,5-shot,accuracy,0.3801652892561983,0.0443132450196843
jisukim8873/falcon-7B-case-3,hendrycksTest-international_law,5-shot,acc_norm,0.3801652892561983,0.0443132450196843
jisukim8873/falcon-7B-case-3,hendrycksTest-jurisprudence,5-shot,accuracy,0.3333333333333333,0.0455723951349775
jisukim8873/falcon-7B-case-3,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3333333333333333,0.0455723951349775
jisukim8873/falcon-7B-case-3,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3067484662576687,0.0362308991572414
jisukim8873/falcon-7B-case-3,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3067484662576687,0.0362308991572414
jisukim8873/falcon-7B-case-3,hendrycksTest-machine_learning,5-shot,accuracy,0.3571428571428571,0.0454796099976437
jisukim8873/falcon-7B-case-3,hendrycksTest-machine_learning,5-shot,acc_norm,0.3571428571428571,0.0454796099976437
jisukim8873/falcon-7B-case-3,hendrycksTest-management,5-shot,accuracy,0.3009708737864077,0.0454160944650394
jisukim8873/falcon-7B-case-3,hendrycksTest-management,5-shot,acc_norm,0.3009708737864077,0.0454160944650394
jisukim8873/falcon-7B-case-3,hendrycksTest-marketing,5-shot,accuracy,0.4017094017094017,0.0321169375105162
jisukim8873/falcon-7B-case-3,hendrycksTest-marketing,5-shot,acc_norm,0.4017094017094017,0.0321169375105162
jisukim8873/falcon-7B-case-3,hendrycksTest-medical_genetics,5-shot,accuracy,0.4,0.049236596391733
jisukim8873/falcon-7B-case-3,hendrycksTest-medical_genetics,5-shot,acc_norm,0.4,0.049236596391733
jisukim8873/falcon-7B-case-3,hendrycksTest-miscellaneous,5-shot,accuracy,0.40485312899106,0.0175532464677202
jisukim8873/falcon-7B-case-3,hendrycksTest-miscellaneous,5-shot,acc_norm,0.40485312899106,0.0175532464677202
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_disputes,5-shot,accuracy,0.338150289017341,0.0254697701494001
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_disputes,5-shot,acc_norm,0.338150289017341,0.0254697701494001
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2502793296089385,0.0144875008528504
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2502793296089385,0.0144875008528504
jisukim8873/falcon-7B-case-3,hendrycksTest-nutrition,5-shot,accuracy,0.3235294117647059,0.0267874531119065
jisukim8873/falcon-7B-case-3,hendrycksTest-nutrition,5-shot,acc_norm,0.3235294117647059,0.0267874531119065
jisukim8873/falcon-7B-case-3,hendrycksTest-philosophy,5-shot,accuracy,0.3279742765273312,0.0266644108869376
jisukim8873/falcon-7B-case-3,hendrycksTest-philosophy,5-shot,acc_norm,0.3279742765273312,0.0266644108869376
jisukim8873/falcon-7B-case-3,hendrycksTest-prehistory,5-shot,accuracy,0.3148148148148148,0.0258422487009021
jisukim8873/falcon-7B-case-3,hendrycksTest-prehistory,5-shot,acc_norm,0.3148148148148148,0.0258422487009021
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_law,5-shot,accuracy,0.2659713168187744,0.0112850331655512
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_law,5-shot,acc_norm,0.2659713168187744,0.0112850331655512
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_medicine,5-shot,accuracy,0.3970588235294117,0.02972215209928
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3970588235294117,0.02972215209928
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_psychology,5-shot,accuracy,0.2875816993464052,0.0183116530536482
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2875816993464052,0.0183116530536482
jisukim8873/falcon-7B-case-3,hendrycksTest-public_relations,5-shot,accuracy,0.3090909090909091,0.0442629464820009
jisukim8873/falcon-7B-case-3,hendrycksTest-public_relations,5-shot,acc_norm,0.3090909090909091,0.0442629464820009
jisukim8873/falcon-7B-case-3,hendrycksTest-security_studies,5-shot,accuracy,0.3591836734693877,0.0307135604551084
jisukim8873/falcon-7B-case-3,hendrycksTest-security_studies,5-shot,acc_norm,0.3591836734693877,0.0307135604551084
jisukim8873/falcon-7B-case-3,hendrycksTest-sociology,5-shot,accuracy,0.3681592039800995,0.034104105654953
jisukim8873/falcon-7B-case-3,hendrycksTest-sociology,5-shot,acc_norm,0.3681592039800995,0.034104105654953
jisukim8873/falcon-7B-case-3,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.47,0.0501613558046591
jisukim8873/falcon-7B-case-3,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.47,0.0501613558046591
jisukim8873/falcon-7B-case-3,hendrycksTest-virology,5-shot,accuracy,0.3313253012048193,0.0366431477728808
jisukim8873/falcon-7B-case-3,hendrycksTest-virology,5-shot,acc_norm,0.3313253012048193,0.0366431477728808
jisukim8873/falcon-7B-case-3,hendrycksTest-world_religions,5-shot,accuracy,0.4152046783625731,0.037792759455032
jisukim8873/falcon-7B-case-3,hendrycksTest-world_religions,5-shot,acc_norm,0.4152046783625731,0.037792759455032
jisukim8873/falcon-7B-case-3,truthfulqa:mc,0-shot,mc1,0.2472460220318237,0.0151024047973596
jisukim8873/falcon-7B-case-3,truthfulqa:mc,0-shot,mc2,0.3643319758257046,0.014190689156837
jisukim8873/falcon-7B-case-3,winogrande,5-shot,accuracy,0.7103393843725335,0.0127485508076382
jisukim8873/falcon-7B-case-3,gsm8k,5-shot,accuracy,0.0659590598938589,0.0068369511920341
jisukim8873/falcon-7B-case-3,minerva_math_precalc,5-shot,accuracy,0.0201465201465201,0.0060184178896539
jisukim8873/falcon-7B-case-3,minerva_math_prealgebra,5-shot,accuracy,0.0309988518943742,0.0058759125557452
jisukim8873/falcon-7B-case-3,minerva_math_num_theory,5-shot,accuracy,0.0185185185185185,0.0058069728079122
jisukim8873/falcon-7B-case-3,minerva_math_intermediate_algebra,5-shot,accuracy,0.0166112956810631,0.0042556028721946
jisukim8873/falcon-7B-case-3,minerva_math_geometry,5-shot,accuracy,0.0104384133611691,0.0046486271171846
jisukim8873/falcon-7B-case-3,minerva_math_counting_and_prob,5-shot,accuracy,0.0168776371308016,0.0059228268948526
jisukim8873/falcon-7B-case-3,minerva_math_algebra,5-shot,accuracy,0.012636899747262,0.0032435184443521
jisukim8873/falcon-7B-case-3,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-3,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-3,arithmetic_3da,5-shot,accuracy,0.0225,0.0033169829948455
jisukim8873/falcon-7B-case-3,arithmetic_3ds,5-shot,accuracy,0.0445,0.0046119963416212
jisukim8873/falcon-7B-case-3,arithmetic_4da,5-shot,accuracy,0.0005,0.0005
jisukim8873/falcon-7B-case-3,arithmetic_2ds,5-shot,accuracy,0.3365,0.0105683357185477
jisukim8873/falcon-7B-case-3,arithmetic_5ds,5-shot,accuracy,0.003,0.0012232122154646
jisukim8873/falcon-7B-case-3,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-3,arithmetic_1dc,5-shot,accuracy,0.074,0.00585483898752
jisukim8873/falcon-7B-case-3,arithmetic_4ds,5-shot,accuracy,0.002,0.0009992493430694
jisukim8873/falcon-7B-case-3,arithmetic_2dm,5-shot,accuracy,0.1455,0.0078864423529557
jisukim8873/falcon-7B-case-3,arithmetic_2da,5-shot,accuracy,0.28,0.0100424312401232
jisukim8873/falcon-7B-case-3,gsm8k_cot,5-shot,accuracy,0.1076573161485974,0.0085374840030233
jisukim8873/falcon-7B-case-3,anli_r2,0-shot,brier_score,0.9498963226859568,
jisukim8873/falcon-7B-case-3,anli_r3,0-shot,brier_score,0.9049030040265976,
jisukim8873/falcon-7B-case-3,anli_r1,0-shot,brier_score,0.9935104969984004,
jisukim8873/falcon-7B-case-3,xnli_eu,0-shot,brier_score,1.0601168919059505,
jisukim8873/falcon-7B-case-3,xnli_vi,0-shot,brier_score,1.0474950786006254,
jisukim8873/falcon-7B-case-3,xnli_ru,0-shot,brier_score,0.830398964603772,
jisukim8873/falcon-7B-case-3,xnli_zh,0-shot,brier_score,0.9872301490924532,
jisukim8873/falcon-7B-case-3,xnli_tr,0-shot,brier_score,0.9775824306933932,
jisukim8873/falcon-7B-case-3,xnli_fr,0-shot,brier_score,0.7460284259966345,
jisukim8873/falcon-7B-case-3,xnli_en,0-shot,brier_score,0.6732677740933464,
jisukim8873/falcon-7B-case-3,xnli_ur,0-shot,brier_score,1.296502100859824,
jisukim8873/falcon-7B-case-3,xnli_ar,0-shot,brier_score,1.2688541556577642,
jisukim8873/falcon-7B-case-3,xnli_de,0-shot,brier_score,0.8333599994517225,
jisukim8873/falcon-7B-case-3,xnli_hi,0-shot,brier_score,1.125339879413296,
jisukim8873/falcon-7B-case-3,xnli_es,0-shot,brier_score,0.8171417939353351,
jisukim8873/falcon-7B-case-3,xnli_bg,0-shot,brier_score,1.0120503254545998,
jisukim8873/falcon-7B-case-3,xnli_sw,0-shot,brier_score,1.1011185031976496,
jisukim8873/falcon-7B-case-3,xnli_el,0-shot,brier_score,0.8884252615275592,
jisukim8873/falcon-7B-case-3,xnli_th,0-shot,brier_score,0.9584810095383592,
jisukim8873/falcon-7B-case-3,logiqa2,0-shot,brier_score,1.1023888188527122,
jisukim8873/falcon-7B-case-3,mathqa,0-shot,brier_score,0.9436004260298938,
jisukim8873/falcon-7B-case-3,lambada_standard,0-shot,perplexity,4.165215660982916,0.0923976679815377
jisukim8873/falcon-7B-case-3,lambada_standard,0-shot,accuracy,0.6712594605084417,0.0065446121513527
jisukim8873/falcon-7B-case-3,lambada_openai,0-shot,perplexity,3.340843946340243,0.0699485950807469
jisukim8873/falcon-7B-case-3,lambada_openai,0-shot,accuracy,0.736270133902581,0.0061391793635698
Dampish/StellarX-4B-V0,lambada_openai,0-shot,perplexity,6.854529238400485,0.1860771773877384
Dampish/StellarX-4B-V0,lambada_openai,0-shot,accuracy,0.5984863186493304,0.0068295070925447
Dampish/StellarX-4B-V0,lambada_standard,0-shot,perplexity,11.543408264434987,0.3438668886795974
Dampish/StellarX-4B-V0,lambada_standard,0-shot,accuracy,0.4836017853677469,0.0069622303263683
microsoft/phi-1_5,lambada_openai,0-shot,perplexity,8.920757905488566,0.2983472264131881
microsoft/phi-1_5,lambada_openai,0-shot,accuracy,0.5342518921016883,0.0069496135763181
microsoft/phi-1_5,lambada_standard,0-shot,perplexity,34.79891122734685,1.3934638300150313
microsoft/phi-1_5,lambada_standard,0-shot,accuracy,0.3483407723656123,0.0066378051957728
EleutherAI/pythia-70m,lambada_openai,0-shot,perplexity,142.4856061857315,6.0383129124907775
EleutherAI/pythia-70m,lambada_openai,0-shot,accuracy,0.1837764409082088,0.0053958709461702
EleutherAI/pythia-70m,lambada_standard,0-shot,perplexity,966.2316285514788,49.66877678546758
EleutherAI/pythia-70m,lambada_standard,0-shot,accuracy,0.1340966427323889,0.004747399033321
EleutherAI/pythia-2.8b-deduped,lambada_openai,0-shot,perplexity,5.007840239567457,0.1183217894775687
EleutherAI/pythia-2.8b-deduped,lambada_openai,0-shot,accuracy,0.6508829807878905,0.0066412379537506
EleutherAI/pythia-2.8b-deduped,lambada_standard,0-shot,perplexity,8.191848267083204,0.2234078217073945
EleutherAI/pythia-2.8b-deduped,lambada_standard,0-shot,accuracy,0.5422084222782845,0.0069411127922818
EleutherAI/pythia-12b-deduped,lambada_openai,0-shot,perplexity,3.876000469136433,0.0822271044796744
EleutherAI/pythia-12b-deduped,lambada_openai,0-shot,accuracy,0.7079371240054337,0.0063350142358844
EleutherAI/pythia-12b-deduped,lambada_standard,0-shot,perplexity,5.932455454138496,0.1419329914891892
EleutherAI/pythia-12b-deduped,lambada_standard,0-shot,accuracy,0.6019794294585679,0.00681954904794
EleutherAI/pythia-1b-deduped,lambada_openai,0-shot,perplexity,7.410495936735888,0.1950451019170836
EleutherAI/pythia-1b-deduped,lambada_openai,0-shot,accuracy,0.5806326411798952,0.0068748006348331
EleutherAI/pythia-1b-deduped,lambada_standard,0-shot,perplexity,15.715150949549749,0.5007488418872335
EleutherAI/pythia-1b-deduped,lambada_standard,0-shot,accuracy,0.4393557151174073,0.0069145498587991
EleutherAI/pythia-410m,lambada_openai,0-shot,perplexity,10.840017505271842,0.3223261607992659
EleutherAI/pythia-410m,lambada_openai,0-shot,accuracy,0.5152338443625073,0.0069627437174515
EleutherAI/pythia-410m,lambada_standard,0-shot,perplexity,31.515169868976937,1.2155413705911626
EleutherAI/pythia-410m,lambada_standard,0-shot,accuracy,0.3642538327188045,0.0067043397295289
EleutherAI/pythia-70m-deduped,lambada_openai,0-shot,perplexity,138.42494788569465,5.920318316854413
EleutherAI/pythia-70m-deduped,lambada_openai,0-shot,accuracy,0.1921210945080535,0.0054887409382451
EleutherAI/pythia-70m-deduped,lambada_standard,0-shot,perplexity,774.2086234818479,38.151408392768815
EleutherAI/pythia-70m-deduped,lambada_standard,0-shot,accuracy,0.1428294197554822,0.0048747745729013
EleutherAI/pythia-160m,lambada_openai,0-shot,perplexity,38.228515298604016,1.4412698983935048
EleutherAI/pythia-160m,lambada_openai,0-shot,accuracy,0.326605860663691,0.0065336930212616
EleutherAI/pythia-160m,lambada_standard,0-shot,perplexity,187.5950672320752,8.391976328550136
EleutherAI/pythia-160m,lambada_standard,0-shot,accuracy,0.2130797593634776,0.0057049064830217
EleutherAI/pythia-410m-deduped,lambada_openai,0-shot,perplexity,10.576122278723773,0.3181426216535976
EleutherAI/pythia-410m-deduped,lambada_openai,0-shot,accuracy,0.5247428682320978,0.0069574431532924
EleutherAI/pythia-410m-deduped,lambada_standard,0-shot,perplexity,25.728901036678486,0.932443507598544
EleutherAI/pythia-410m-deduped,lambada_standard,0-shot,accuracy,0.3923927809043275,0.006802742619162
EleutherAI/pile-t5-large,lambada_openai,0-shot,perplexity,1174326494310883.8,168418333345271.2
EleutherAI/pile-t5-large,lambada_openai,0-shot,accuracy,0.0,0.0
EleutherAI/pile-t5-large,lambada_standard,0-shot,perplexity,559876661772981.0,46847058707406.46
EleutherAI/pile-t5-large,lambada_standard,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,lambada_openai,0-shot,perplexity,3.919867446026272,0.0836345919848914
EleutherAI/pythia-12b,lambada_openai,0-shot,accuracy,0.7056083834659421,0.0063497504573974
EleutherAI/pythia-12b,lambada_standard,0-shot,perplexity,5.943677224766392,0.14315402566638
EleutherAI/pythia-12b,lambada_standard,0-shot,accuracy,0.5895594799146129,0.00685331984709
EleutherAI/pythia-6.9b-deduped,lambada_openai,0-shot,perplexity,4.1543541069624,0.0905071157855457
EleutherAI/pythia-6.9b-deduped,lambada_openai,0-shot,accuracy,0.6867843974383854,0.0064616581301303
EleutherAI/pythia-6.9b-deduped,lambada_standard,0-shot,perplexity,6.635289522467756,0.1703126404610236
EleutherAI/pythia-6.9b-deduped,lambada_standard,0-shot,accuracy,0.5868426159518727,0.0068601032857639
LLM360/Amber,lambada_openai,0-shot,perplexity,4.299153889087319,0.0930843938673959
LLM360/Amber,lambada_openai,0-shot,accuracy,0.6759169415874248,0.0065205901435062
LLM360/Amber,lambada_standard,0-shot,perplexity,5.586387757032215,0.1264491844489558
LLM360/Amber,lambada_standard,0-shot,accuracy,0.6074131573840481,0.0068033378844866
google/gemma-7b,lambada_openai,0-shot,perplexity,3.439312769788616,0.0672905486284727
google/gemma-7b,lambada_openai,0-shot,accuracy,0.6757228798758005,0.0065216057169501
google/gemma-7b,lambada_standard,0-shot,perplexity,3.8872754278475408,0.0782635042751257
google/gemma-7b,lambada_standard,0-shot,accuracy,0.6444789443042888,0.0066688216494302
google/gemma-2-2b,lambada_openai,0-shot,perplexity,3.714699747527507,0.075673834178077
google/gemma-2-2b,lambada_openai,0-shot,accuracy,0.6984281001358432,0.0063939371193314
google/gemma-2-2b,lambada_standard,0-shot,perplexity,4.488273605097951,0.0967111023003739
google/gemma-2-2b,lambada_standard,0-shot,accuracy,0.6409858334950514,0.0066833173161355
google/gemma-2-9b,lambada_openai,0-shot,perplexity,2.9199585468729032,0.0527994036567571
google/gemma-2-9b,lambada_openai,0-shot,accuracy,0.7620803415486125,0.0059323616140153
google/gemma-2-9b,lambada_standard,0-shot,perplexity,3.209501475020577,0.0568413457402903
google/gemma-2-9b,lambada_standard,0-shot,accuracy,0.7230739375121289,0.0062342661958364
facebook/opt-6.7b,lambada_openai,0-shot,perplexity,4.252279766654777,0.0928006668523406
facebook/opt-6.7b,lambada_openai,0-shot,accuracy,0.6764991267222977,0.0065175357443602
facebook/opt-6.7b,lambada_standard,0-shot,perplexity,5.217868074032219,0.1221711082102531
facebook/opt-6.7b,lambada_standard,0-shot,accuracy,0.632835241606831,0.0067156475552276
HuggingFaceTB/SmolLM-360M,lambada_openai,0-shot,perplexity,16.280667347400456,0.5503547907654323
HuggingFaceTB/SmolLM-360M,lambada_openai,0-shot,accuracy,0.4554628371822239,0.0069382877697232
HuggingFaceTB/SmolLM-360M,lambada_standard,0-shot,perplexity,29.48370098333974,1.108423010837081
HuggingFaceTB/SmolLM-360M,lambada_standard,0-shot,accuracy,0.3673588201047933,0.006716392026588
HuggingFaceTB/SmolLM-1.7B,lambada_openai,0-shot,perplexity,8.504771462842546,0.2411136525578525
HuggingFaceTB/SmolLM-1.7B,lambada_openai,0-shot,accuracy,0.5557927420919853,0.0069224740040908
HuggingFaceTB/SmolLM-1.7B,lambada_standard,0-shot,perplexity,14.10811631696615,0.4597872046360158
HuggingFaceTB/SmolLM-1.7B,lambada_standard,0-shot,accuracy,0.4628371822239472,0.0069467099346343
01-ai/Yi-9B,lambada_openai,0-shot,perplexity,3.667684429380235,0.0754864949671457
01-ai/Yi-9B,lambada_openai,0-shot,accuracy,0.6856200271686397,0.0064681674770558
01-ai/Yi-9B,lambada_standard,0-shot,perplexity,3.8304913678510495,0.0800953221316198
01-ai/Yi-9B,lambada_standard,0-shot,accuracy,0.685814088880264,0.0064670858666538
Qwen/Qwen1.5-14B,lambada_openai,0-shot,perplexity,3.508657442431227,0.0745968083731285
Qwen/Qwen1.5-14B,lambada_openai,0-shot,accuracy,0.7197748884145159,0.0062569681407934
Qwen/Qwen1.5-14B,lambada_standard,0-shot,perplexity,6.2724514081006575,0.1400013505380934
Qwen/Qwen1.5-14B,lambada_standard,0-shot,accuracy,0.5299825344459538,0.0069534422140034
microsoft/phi-2,mmlu_formal_logic,5-shot,accuracy,0.3571428571428571,0.0428571428571428
microsoft/phi-2,mmlu_high_school_european_history,5-shot,accuracy,0.6424242424242425,0.0374259704380658
microsoft/phi-2,mmlu_high_school_us_history,5-shot,accuracy,0.6715686274509803,0.0329624511017222
microsoft/phi-2,mmlu_high_school_world_history,5-shot,accuracy,0.7383966244725738,0.0286095167169949
microsoft/phi-2,mmlu_international_law,5-shot,accuracy,0.743801652892562,0.0398497965330287
microsoft/phi-2,mmlu_jurisprudence,5-shot,accuracy,0.7222222222222222,0.0433004374965074
microsoft/phi-2,mmlu_logical_fallacies,5-shot,accuracy,0.7484662576687117,0.0340899788685752
microsoft/phi-2,mmlu_moral_disputes,5-shot,accuracy,0.6676300578034682,0.0253611687496882
microsoft/phi-2,mmlu_moral_scenarios,5-shot,accuracy,0.3094972067039106,0.0154611690023715
microsoft/phi-2,mmlu_philosophy,5-shot,accuracy,0.6302250803858521,0.0274179967056309
microsoft/phi-2,mmlu_prehistory,5-shot,accuracy,0.6234567901234568,0.0269593445187477
microsoft/phi-2,mmlu_professional_law,5-shot,accuracy,0.4230769230769231,0.0126182040665883
microsoft/phi-2,mmlu_world_religions,5-shot,accuracy,0.6842105263157895,0.0356507967070831
microsoft/phi-2,mmlu_business_ethics,5-shot,accuracy,0.6,0.049236596391733
microsoft/phi-2,mmlu_clinical_knowledge,5-shot,accuracy,0.6113207547169811,0.0300004854486759
microsoft/phi-2,mmlu_college_medicine,5-shot,accuracy,0.6011560693641619,0.037336266553835
microsoft/phi-2,mmlu_global_facts,5-shot,accuracy,0.34,0.0476095228569523
microsoft/phi-2,mmlu_human_aging,5-shot,accuracy,0.6502242152466368,0.032007367194845
microsoft/phi-2,mmlu_management,5-shot,accuracy,0.7378640776699029,0.0435463107726059
microsoft/phi-2,mmlu_marketing,5-shot,accuracy,0.8205128205128205,0.0251409359503354
microsoft/phi-2,mmlu_medical_genetics,5-shot,accuracy,0.63,0.0485236587093909
microsoft/phi-2,mmlu_miscellaneous,5-shot,accuracy,0.6896551724137931,0.0165437850260483
microsoft/phi-2,mmlu_nutrition,5-shot,accuracy,0.6176470588235294,0.0278261093072836
microsoft/phi-2,mmlu_professional_accounting,5-shot,accuracy,0.4432624113475177,0.029634838473766
microsoft/phi-2,mmlu_professional_medicine,5-shot,accuracy,0.4705882352941176,0.0303202432650041
microsoft/phi-2,mmlu_virology,5-shot,accuracy,0.4698795180722891,0.0388542542086676
microsoft/phi-2,mmlu_econometrics,5-shot,accuracy,0.3859649122807017,0.0457963942207043
microsoft/phi-2,mmlu_high_school_geography,5-shot,accuracy,0.7323232323232324,0.0315444988827028
microsoft/phi-2,mmlu_high_school_government_and_politics,5-shot,accuracy,0.8082901554404145,0.0284089536262452
microsoft/phi-2,mmlu_high_school_macroeconomics,5-shot,accuracy,0.5692307692307692,0.0251068206605397
microsoft/phi-2,mmlu_high_school_microeconomics,5-shot,accuracy,0.6134453781512605,0.0316314580755238
microsoft/phi-2,mmlu_high_school_psychology,5-shot,accuracy,0.7963302752293578,0.0172667420876307
microsoft/phi-2,mmlu_human_sexuality,5-shot,accuracy,0.7099236641221374,0.0398006624646776
microsoft/phi-2,mmlu_professional_psychology,5-shot,accuracy,0.5620915032679739,0.0200712578868865
microsoft/phi-2,mmlu_public_relations,5-shot,accuracy,0.6727272727272727,0.0449429086625209
microsoft/phi-2,mmlu_security_studies,5-shot,accuracy,0.726530612244898,0.0285355603371284
microsoft/phi-2,mmlu_sociology,5-shot,accuracy,0.8109452736318408,0.027686913588013
microsoft/phi-2,mmlu_us_foreign_policy,5-shot,accuracy,0.76,0.0429234695990928
microsoft/phi-2,mmlu_abstract_algebra,5-shot,accuracy,0.31,0.0464823198711731
microsoft/phi-2,mmlu_anatomy,5-shot,accuracy,0.4444444444444444,0.0429259671825698
microsoft/phi-2,mmlu_astronomy,5-shot,accuracy,0.5921052631578947,0.0399930971277747
microsoft/phi-2,mmlu_college_biology,5-shot,accuracy,0.6666666666666666,0.0394208263992721
microsoft/phi-2,mmlu_college_chemistry,5-shot,accuracy,0.41,0.049431107042371
microsoft/phi-2,mmlu_college_computer_science,5-shot,accuracy,0.43,0.0497569851956242
microsoft/phi-2,mmlu_college_mathematics,5-shot,accuracy,0.38,0.0487831731214563
microsoft/phi-2,mmlu_college_physics,5-shot,accuracy,0.3627450980392157,0.0478406070410565
microsoft/phi-2,mmlu_computer_security,5-shot,accuracy,0.74,0.0440844002276807
microsoft/phi-2,mmlu_conceptual_physics,5-shot,accuracy,0.502127659574468,0.0326857265866749
microsoft/phi-2,mmlu_electrical_engineering,5-shot,accuracy,0.5448275862068965,0.0414988694219211
microsoft/phi-2,mmlu_elementary_mathematics,5-shot,accuracy,0.4576719576719577,0.0256588688620583
microsoft/phi-2,mmlu_high_school_biology,5-shot,accuracy,0.7064516129032258,0.0259060870213192
microsoft/phi-2,mmlu_high_school_chemistry,5-shot,accuracy,0.4876847290640394,0.0351692044422089
microsoft/phi-2,mmlu_high_school_computer_science,5-shot,accuracy,0.64,0.0482418151324421
microsoft/phi-2,mmlu_high_school_mathematics,5-shot,accuracy,0.337037037037037,0.0288208846662532
microsoft/phi-2,mmlu_high_school_physics,5-shot,accuracy,0.3774834437086092,0.0395802723112157
microsoft/phi-2,mmlu_high_school_statistics,5-shot,accuracy,0.5,0.0340997169735236
microsoft/phi-2,mmlu_machine_learning,5-shot,accuracy,0.4910714285714285,0.0474503325548912
microsoft/phi-1_5,mmlu_formal_logic,5-shot,accuracy,0.2698412698412698,0.0397015827323517
microsoft/phi-1_5,mmlu_high_school_european_history,5-shot,accuracy,0.4787878787878787,0.039008289137373
microsoft/phi-1_5,mmlu_high_school_us_history,5-shot,accuracy,0.4656862745098039,0.0350103832763589
microsoft/phi-1_5,mmlu_high_school_world_history,5-shot,accuracy,0.5147679324894515,0.0325330280787773
microsoft/phi-1_5,mmlu_international_law,5-shot,accuracy,0.6033057851239669,0.04465869780531
microsoft/phi-1_5,mmlu_jurisprudence,5-shot,accuracy,0.5185185185185185,0.0483036602463533
microsoft/phi-1_5,mmlu_logical_fallacies,5-shot,accuracy,0.5276073619631901,0.0392237829061099
microsoft/phi-1_5,mmlu_moral_disputes,5-shot,accuracy,0.5520231213872833,0.0267729906533618
microsoft/phi-1_5,mmlu_moral_scenarios,5-shot,accuracy,0.2458100558659217,0.0144002964292256
microsoft/phi-1_5,mmlu_philosophy,5-shot,accuracy,0.4790996784565916,0.0283732709610694
microsoft/phi-1_5,mmlu_prehistory,5-shot,accuracy,0.419753086419753,0.0274600995570051
microsoft/phi-1_5,mmlu_professional_law,5-shot,accuracy,0.3402868318122555,0.0121012176102237
microsoft/phi-1_5,mmlu_world_religions,5-shot,accuracy,0.4502923976608187,0.0381582736591323
microsoft/phi-1_5,mmlu_business_ethics,5-shot,accuracy,0.52,0.0502116731568677
microsoft/phi-1_5,mmlu_clinical_knowledge,5-shot,accuracy,0.4981132075471698,0.0307726536420756
microsoft/phi-1_5,mmlu_college_medicine,5-shot,accuracy,0.4104046242774566,0.0375075704489553
microsoft/phi-1_5,mmlu_global_facts,5-shot,accuracy,0.32,0.046882617226215
microsoft/phi-1_5,mmlu_human_aging,5-shot,accuracy,0.484304932735426,0.0335412657542081
microsoft/phi-1_5,mmlu_management,5-shot,accuracy,0.5825242718446602,0.0488284054821223
microsoft/phi-1_5,mmlu_marketing,5-shot,accuracy,0.7008547008547008,0.0299969518583494
microsoft/phi-1_5,mmlu_medical_genetics,5-shot,accuracy,0.5,0.0502518907629606
microsoft/phi-1_5,mmlu_miscellaneous,5-shot,accuracy,0.508301404853129,0.017877498991072
microsoft/phi-1_5,mmlu_nutrition,5-shot,accuracy,0.5032679738562091,0.0286293051940035
microsoft/phi-1_5,mmlu_professional_accounting,5-shot,accuracy,0.3049645390070922,0.0274647084420221
microsoft/phi-1_5,mmlu_professional_medicine,5-shot,accuracy,0.3345588235294117,0.0286619962023353
microsoft/phi-1_5,mmlu_virology,5-shot,accuracy,0.4156626506024096,0.0383672217659805
microsoft/phi-1_5,mmlu_econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
microsoft/phi-1_5,mmlu_high_school_geography,5-shot,accuracy,0.5202020202020202,0.0355944356556391
microsoft/phi-1_5,mmlu_high_school_government_and_politics,5-shot,accuracy,0.538860103626943,0.0359752441173457
microsoft/phi-1_5,mmlu_high_school_macroeconomics,5-shot,accuracy,0.4282051282051282,0.0250883014546948
microsoft/phi-1_5,mmlu_high_school_microeconomics,5-shot,accuracy,0.4579831932773109,0.0323636111195194
microsoft/phi-1_5,mmlu_high_school_psychology,5-shot,accuracy,0.5688073394495413,0.0212333650303195
microsoft/phi-1_5,mmlu_human_sexuality,5-shot,accuracy,0.4732824427480916,0.0437902493655389
microsoft/phi-1_5,mmlu_professional_psychology,5-shot,accuracy,0.4019607843137255,0.0198351764843753
microsoft/phi-1_5,mmlu_public_relations,5-shot,accuracy,0.5181818181818182,0.0478596401079491
microsoft/phi-1_5,mmlu_security_studies,5-shot,accuracy,0.5061224489795918,0.032006820201639
microsoft/phi-1_5,mmlu_sociology,5-shot,accuracy,0.6417910447761194,0.0339039304226881
microsoft/phi-1_5,mmlu_us_foreign_policy,5-shot,accuracy,0.65,0.0479372485441101
microsoft/phi-1_5,mmlu_abstract_algebra,5-shot,accuracy,0.35,0.0479372485441101
microsoft/phi-1_5,mmlu_anatomy,5-shot,accuracy,0.4666666666666667,0.0430973290103635
microsoft/phi-1_5,mmlu_astronomy,5-shot,accuracy,0.4144736842105263,0.040089737857792
microsoft/phi-1_5,mmlu_college_biology,5-shot,accuracy,0.375,0.0404843922269559
microsoft/phi-1_5,mmlu_college_chemistry,5-shot,accuracy,0.27,0.0446196043338473
microsoft/phi-1_5,mmlu_college_computer_science,5-shot,accuracy,0.46,0.0500908265962033
microsoft/phi-1_5,mmlu_college_mathematics,5-shot,accuracy,0.41,0.049431107042371
microsoft/phi-1_5,mmlu_college_physics,5-shot,accuracy,0.2941176470588235,0.0453383819592977
microsoft/phi-1_5,mmlu_computer_security,5-shot,accuracy,0.53,0.0501613558046591
microsoft/phi-1_5,mmlu_conceptual_physics,5-shot,accuracy,0.3744680851063829,0.0316391066536729
microsoft/phi-1_5,mmlu_electrical_engineering,5-shot,accuracy,0.4689655172413793,0.0415863276209782
microsoft/phi-1_5,mmlu_elementary_mathematics,5-shot,accuracy,0.3042328042328042,0.023695415009463
microsoft/phi-1_5,mmlu_high_school_biology,5-shot,accuracy,0.4483870967741935,0.0282920568301127
microsoft/phi-1_5,mmlu_high_school_chemistry,5-shot,accuracy,0.3448275862068966,0.0334428374428045
microsoft/phi-1_5,mmlu_high_school_computer_science,5-shot,accuracy,0.46,0.0500908265962033
microsoft/phi-1_5,mmlu_high_school_mathematics,5-shot,accuracy,0.2333333333333333,0.0257878742209593
microsoft/phi-1_5,mmlu_high_school_physics,5-shot,accuracy,0.2781456953642384,0.0365860326276374
microsoft/phi-1_5,mmlu_high_school_statistics,5-shot,accuracy,0.287037037037037,0.030851992993257
microsoft/phi-1_5,mmlu_machine_learning,5-shot,accuracy,0.3928571428571428,0.0463555013560997
Qwen/Qwen1.5-14B,mmlu_formal_logic,5-shot,accuracy,0.5476190476190477,0.0445180795905532
Qwen/Qwen1.5-14B,mmlu_high_school_european_history,5-shot,accuracy,0.8484848484848485,0.0279980737987816
Qwen/Qwen1.5-14B,mmlu_high_school_us_history,5-shot,accuracy,0.8137254901960784,0.0273254709667163
Qwen/Qwen1.5-14B,mmlu_high_school_world_history,5-shot,accuracy,0.8312236286919831,0.0243814068325862
Qwen/Qwen1.5-14B,mmlu_international_law,5-shot,accuracy,0.8429752066115702,0.0332124484254712
Qwen/Qwen1.5-14B,mmlu_jurisprudence,5-shot,accuracy,0.7407407407407407,0.0423651125809463
Qwen/Qwen1.5-14B,mmlu_logical_fallacies,5-shot,accuracy,0.7607361963190185,0.0335195387952127
Qwen/Qwen1.5-14B,mmlu_moral_disputes,5-shot,accuracy,0.7456647398843931,0.0234458262765455
Qwen/Qwen1.5-14B,mmlu_moral_scenarios,5-shot,accuracy,0.4424581005586592,0.0166113936872685
Qwen/Qwen1.5-14B,mmlu_philosophy,5-shot,accuracy,0.7202572347266881,0.0254942593506948
Qwen/Qwen1.5-14B,mmlu_prehistory,5-shot,accuracy,0.7160493827160493,0.0250894785237651
Qwen/Qwen1.5-14B,mmlu_professional_law,5-shot,accuracy,0.4863102998696219,0.01276544872261
Qwen/Qwen1.5-14B,mmlu_world_religions,5-shot,accuracy,0.8304093567251462,0.0287821081054017
Qwen/Qwen1.5-14B,mmlu_business_ethics,5-shot,accuracy,0.75,0.0435194139889244
Qwen/Qwen1.5-14B,mmlu_clinical_knowledge,5-shot,accuracy,0.7471698113207547,0.0267498997712412
Qwen/Qwen1.5-14B,mmlu_college_medicine,5-shot,accuracy,0.6878612716763006,0.0353313338932365
Qwen/Qwen1.5-14B,mmlu_global_facts,5-shot,accuracy,0.51,0.0502418393795691
Qwen/Qwen1.5-14B,mmlu_human_aging,5-shot,accuracy,0.7309417040358744,0.0297637794068749
Qwen/Qwen1.5-14B,mmlu_management,5-shot,accuracy,0.8058252427184466,0.0391666776282258
Qwen/Qwen1.5-14B,mmlu_marketing,5-shot,accuracy,0.8717948717948718,0.0219019051150733
Qwen/Qwen1.5-14B,mmlu_medical_genetics,5-shot,accuracy,0.79,0.0409360180740332
Qwen/Qwen1.5-14B,mmlu_miscellaneous,5-shot,accuracy,0.8403575989782887,0.013097934513263
Qwen/Qwen1.5-14B,mmlu_nutrition,5-shot,accuracy,0.7516339869281046,0.0247399813551135
Qwen/Qwen1.5-14B,mmlu_professional_accounting,5-shot,accuracy,0.5106382978723404,0.0298207471914224
Qwen/Qwen1.5-14B,mmlu_professional_medicine,5-shot,accuracy,0.7205882352941176,0.0272572026061149
Qwen/Qwen1.5-14B,mmlu_virology,5-shot,accuracy,0.4518072289156626,0.0387437155658795
Qwen/Qwen1.5-14B,mmlu_econometrics,5-shot,accuracy,0.5526315789473685,0.046774730044912
Qwen/Qwen1.5-14B,mmlu_high_school_geography,5-shot,accuracy,0.8686868686868687,0.0240631564168225
Qwen/Qwen1.5-14B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.8963730569948186,0.0219953119636442
Qwen/Qwen1.5-14B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.7410256410256411,0.0222111068100616
Qwen/Qwen1.5-14B,mmlu_high_school_microeconomics,5-shot,accuracy,0.7647058823529411,0.0275536144678638
Qwen/Qwen1.5-14B,mmlu_high_school_psychology,5-shot,accuracy,0.8623853211009175,0.0147701058786494
Qwen/Qwen1.5-14B,mmlu_human_sexuality,5-shot,accuracy,0.7633587786259542,0.0372767357559691
Qwen/Qwen1.5-14B,mmlu_professional_psychology,5-shot,accuracy,0.7026143790849673,0.0184925965363969
Qwen/Qwen1.5-14B,mmlu_public_relations,5-shot,accuracy,0.6454545454545455,0.0458200484150541
Qwen/Qwen1.5-14B,mmlu_security_studies,5-shot,accuracy,0.8081632653061225,0.0252069631542254
Qwen/Qwen1.5-14B,mmlu_sociology,5-shot,accuracy,0.8407960199004975,0.0258706467661691
Qwen/Qwen1.5-14B,mmlu_us_foreign_policy,5-shot,accuracy,0.88,0.032659863237109
Qwen/Qwen1.5-14B,mmlu_abstract_algebra,5-shot,accuracy,0.39,0.0490207130000197
Qwen/Qwen1.5-14B,mmlu_anatomy,5-shot,accuracy,0.6296296296296297,0.0417165416135454
Qwen/Qwen1.5-14B,mmlu_astronomy,5-shot,accuracy,0.7368421052631579,0.0358349617636107
Qwen/Qwen1.5-14B,mmlu_college_biology,5-shot,accuracy,0.7708333333333334,0.0351469746786238
Qwen/Qwen1.5-14B,mmlu_college_chemistry,5-shot,accuracy,0.5,0.0502518907629606
Qwen/Qwen1.5-14B,mmlu_college_computer_science,5-shot,accuracy,0.57,0.0497569851956242
Qwen/Qwen1.5-14B,mmlu_college_mathematics,5-shot,accuracy,0.46,0.0500908265962033
Qwen/Qwen1.5-14B,mmlu_college_physics,5-shot,accuracy,0.5,0.0497518595104994
Qwen/Qwen1.5-14B,mmlu_computer_security,5-shot,accuracy,0.8,0.0402015126103684
Qwen/Qwen1.5-14B,mmlu_conceptual_physics,5-shot,accuracy,0.7021276595744681,0.0298961456820954
Qwen/Qwen1.5-14B,mmlu_electrical_engineering,5-shot,accuracy,0.7241379310344828,0.0372456361977463
Qwen/Qwen1.5-14B,mmlu_elementary_mathematics,5-shot,accuracy,0.5925925925925926,0.0253059062415906
Qwen/Qwen1.5-14B,mmlu_high_school_biology,5-shot,accuracy,0.8451612903225807,0.0205792873265832
Qwen/Qwen1.5-14B,mmlu_high_school_chemistry,5-shot,accuracy,0.5911330049261084,0.0345905881588323
Qwen/Qwen1.5-14B,mmlu_high_school_computer_science,5-shot,accuracy,0.74,0.0440844002276807
Qwen/Qwen1.5-14B,mmlu_high_school_mathematics,5-shot,accuracy,0.437037037037037,0.030242862397654
Qwen/Qwen1.5-14B,mmlu_high_school_physics,5-shot,accuracy,0.4834437086092715,0.0408024418562897
Qwen/Qwen1.5-14B,mmlu_high_school_statistics,5-shot,accuracy,0.6342592592592593,0.032847388576472
Qwen/Qwen1.5-14B,mmlu_machine_learning,5-shot,accuracy,0.5446428571428571,0.0472683555371909
Qwen/Qwen2.5-3B,mmlu_formal_logic,5-shot,accuracy,0.4841269841269841,0.044698818540726
Qwen/Qwen2.5-3B,mmlu_high_school_european_history,5-shot,accuracy,0.7818181818181819,0.0322507810830628
Qwen/Qwen2.5-3B,mmlu_high_school_us_history,5-shot,accuracy,0.8431372549019608,0.0255247223245533
Qwen/Qwen2.5-3B,mmlu_high_school_world_history,5-shot,accuracy,0.8396624472573839,0.0238843809259656
Qwen/Qwen2.5-3B,mmlu_international_law,5-shot,accuracy,0.7768595041322314,0.0380075447522873
Qwen/Qwen2.5-3B,mmlu_jurisprudence,5-shot,accuracy,0.7777777777777778,0.0401910747255734
Qwen/Qwen2.5-3B,mmlu_logical_fallacies,5-shot,accuracy,0.7791411042944786,0.0325917739274217
Qwen/Qwen2.5-3B,mmlu_moral_disputes,5-shot,accuracy,0.7196531791907514,0.0241824274965776
Qwen/Qwen2.5-3B,mmlu_moral_scenarios,5-shot,accuracy,0.264804469273743,0.0147569064832606
Qwen/Qwen2.5-3B,mmlu_philosophy,5-shot,accuracy,0.707395498392283,0.0258398983348779
Qwen/Qwen2.5-3B,mmlu_prehistory,5-shot,accuracy,0.7561728395061729,0.0238918795419596
Qwen/Qwen2.5-3B,mmlu_professional_law,5-shot,accuracy,0.4830508474576271,0.0127628968892108
Qwen/Qwen2.5-3B,mmlu_world_religions,5-shot,accuracy,0.8245614035087719,0.0291708855007276
Qwen/Qwen2.5-3B,mmlu_business_ethics,5-shot,accuracy,0.71,0.0456048021572068
Qwen/Qwen2.5-3B,mmlu_clinical_knowledge,5-shot,accuracy,0.7245283018867924,0.027495663683724
Qwen/Qwen2.5-3B,mmlu_college_medicine,5-shot,accuracy,0.6763005780346821,0.0356760379963917
Qwen/Qwen2.5-3B,mmlu_global_facts,5-shot,accuracy,0.4,0.049236596391733
Qwen/Qwen2.5-3B,mmlu_human_aging,5-shot,accuracy,0.695067264573991,0.0308986108824775
Qwen/Qwen2.5-3B,mmlu_management,5-shot,accuracy,0.7961165048543689,0.0398913985953177
Qwen/Qwen2.5-3B,mmlu_marketing,5-shot,accuracy,0.8974358974358975,0.0198756550278674
Qwen/Qwen2.5-3B,mmlu_medical_genetics,5-shot,accuracy,0.78,0.0416333199893226
Qwen/Qwen2.5-3B,mmlu_miscellaneous,5-shot,accuracy,0.7969348659003831,0.0143855250766115
Qwen/Qwen2.5-3B,mmlu_nutrition,5-shot,accuracy,0.7483660130718954,0.0248480182638751
Qwen/Qwen2.5-3B,mmlu_professional_accounting,5-shot,accuracy,0.4964539007092198,0.0298267491532809
Qwen/Qwen2.5-3B,mmlu_professional_medicine,5-shot,accuracy,0.6727941176470589,0.0285014528603965
Qwen/Qwen2.5-3B,mmlu_virology,5-shot,accuracy,0.4939759036144578,0.0389221219533304
Qwen/Qwen2.5-3B,mmlu_econometrics,5-shot,accuracy,0.5701754385964912,0.0465704726059496
Qwen/Qwen2.5-3B,mmlu_high_school_geography,5-shot,accuracy,0.8131313131313131,0.0277725333342189
Qwen/Qwen2.5-3B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.8601036269430051,0.0250338705830151
Qwen/Qwen2.5-3B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.7051282051282052,0.0231193627582322
Qwen/Qwen2.5-3B,mmlu_high_school_microeconomics,5-shot,accuracy,0.773109243697479,0.0272053715382794
Qwen/Qwen2.5-3B,mmlu_high_school_psychology,5-shot,accuracy,0.8623853211009175,0.0147701058786494
Qwen/Qwen2.5-3B,mmlu_human_sexuality,5-shot,accuracy,0.7786259541984732,0.0364129708131372
Qwen/Qwen2.5-3B,mmlu_professional_psychology,5-shot,accuracy,0.696078431372549,0.0186075521312798
Qwen/Qwen2.5-3B,mmlu_public_relations,5-shot,accuracy,0.7181818181818181,0.0430911870994645
Qwen/Qwen2.5-3B,mmlu_security_studies,5-shot,accuracy,0.7387755102040816,0.0281234293351427
Qwen/Qwen2.5-3B,mmlu_sociology,5-shot,accuracy,0.835820895522388,0.0261939235444541
Qwen/Qwen2.5-3B,mmlu_us_foreign_policy,5-shot,accuracy,0.84,0.0368452949177471
Qwen/Qwen2.5-3B,mmlu_abstract_algebra,5-shot,accuracy,0.38,0.0487831731214563
Qwen/Qwen2.5-3B,mmlu_anatomy,5-shot,accuracy,0.6296296296296297,0.0417165416135454
Qwen/Qwen2.5-3B,mmlu_astronomy,5-shot,accuracy,0.7368421052631579,0.0358349617636107
Qwen/Qwen2.5-3B,mmlu_college_biology,5-shot,accuracy,0.75,0.036210341218895
Qwen/Qwen2.5-3B,mmlu_college_chemistry,5-shot,accuracy,0.48,0.0502116731568677
Qwen/Qwen2.5-3B,mmlu_college_computer_science,5-shot,accuracy,0.61,0.0490207130000197
Qwen/Qwen2.5-3B,mmlu_college_mathematics,5-shot,accuracy,0.45,0.05
Qwen/Qwen2.5-3B,mmlu_college_physics,5-shot,accuracy,0.5098039215686274,0.0497422946042281
Qwen/Qwen2.5-3B,mmlu_computer_security,5-shot,accuracy,0.78,0.0416333199893226
Qwen/Qwen2.5-3B,mmlu_conceptual_physics,5-shot,accuracy,0.6382978723404256,0.0314108219759624
Qwen/Qwen2.5-3B,mmlu_electrical_engineering,5-shot,accuracy,0.696551724137931,0.0383122604885033
Qwen/Qwen2.5-3B,mmlu_elementary_mathematics,5-shot,accuracy,0.6005291005291006,0.0252254502840679
Qwen/Qwen2.5-3B,mmlu_high_school_biology,5-shot,accuracy,0.8225806451612904,0.0217325406893292
Qwen/Qwen2.5-3B,mmlu_high_school_chemistry,5-shot,accuracy,0.6157635467980296,0.0342239856565755
Qwen/Qwen2.5-3B,mmlu_high_school_computer_science,5-shot,accuracy,0.78,0.0416333199893226
Qwen/Qwen2.5-3B,mmlu_high_school_mathematics,5-shot,accuracy,0.5037037037037037,0.0304847016650843
Qwen/Qwen2.5-3B,mmlu_high_school_physics,5-shot,accuracy,0.4569536423841059,0.0406732517424744
Qwen/Qwen2.5-3B,mmlu_high_school_statistics,5-shot,accuracy,0.6203703703703703,0.0330968258111903
Qwen/Qwen2.5-3B,mmlu_machine_learning,5-shot,accuracy,0.4732142857142857,0.0473897511927415
Salesforce/codegen-16B-mono,mmlu_formal_logic,5-shot,accuracy,0.2063492063492063,0.0361960452412424
Salesforce/codegen-16B-mono,mmlu_high_school_european_history,5-shot,accuracy,0.2242424242424242,0.032568666616811
Salesforce/codegen-16B-mono,mmlu_high_school_us_history,5-shot,accuracy,0.2450980392156862,0.0301902824535019
Salesforce/codegen-16B-mono,mmlu_high_school_world_history,5-shot,accuracy,0.2531645569620253,0.0283046579430352
Salesforce/codegen-16B-mono,mmlu_international_law,5-shot,accuracy,0.2644628099173554,0.040261875275912
Salesforce/codegen-16B-mono,mmlu_jurisprudence,5-shot,accuracy,0.2685185185185185,0.0428446796805219
Salesforce/codegen-16B-mono,mmlu_logical_fallacies,5-shot,accuracy,0.2331288343558282,0.0332201579577674
Salesforce/codegen-16B-mono,mmlu_moral_disputes,5-shot,accuracy,0.2658959537572254,0.0237862032555082
Salesforce/codegen-16B-mono,mmlu_moral_scenarios,5-shot,accuracy,0.2346368715083799,0.0141730440983036
Salesforce/codegen-16B-mono,mmlu_philosophy,5-shot,accuracy,0.270096463022508,0.0252180403734106
Salesforce/codegen-16B-mono,mmlu_prehistory,5-shot,accuracy,0.287037037037037,0.0251710419153096
Salesforce/codegen-16B-mono,mmlu_professional_law,5-shot,accuracy,0.2535853976531942,0.0111117153361011
Salesforce/codegen-16B-mono,mmlu_world_religions,5-shot,accuracy,0.2807017543859649,0.0344629621708842
Salesforce/codegen-16B-mono,mmlu_business_ethics,5-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-16B-mono,mmlu_clinical_knowledge,5-shot,accuracy,0.2188679245283019,0.0254478638251086
Salesforce/codegen-16B-mono,mmlu_college_medicine,5-shot,accuracy,0.2254335260115607,0.0318620985164114
Salesforce/codegen-16B-mono,mmlu_global_facts,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-16B-mono,mmlu_human_aging,5-shot,accuracy,0.358744394618834,0.0321907920041999
Salesforce/codegen-16B-mono,mmlu_management,5-shot,accuracy,0.2233009708737864,0.0412355318989143
Salesforce/codegen-16B-mono,mmlu_marketing,5-shot,accuracy,0.2863247863247863,0.0296143236904566
Salesforce/codegen-16B-mono,mmlu_medical_genetics,5-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-16B-mono,mmlu_miscellaneous,5-shot,accuracy,0.264367816091954,0.0157699848406905
Salesforce/codegen-16B-mono,mmlu_nutrition,5-shot,accuracy,0.2581699346405229,0.0250585033169581
Salesforce/codegen-16B-mono,mmlu_professional_accounting,5-shot,accuracy,0.2659574468085106,0.0263580656988805
Salesforce/codegen-16B-mono,mmlu_professional_medicine,5-shot,accuracy,0.2022058823529411,0.0243981929866549
Salesforce/codegen-16B-mono,mmlu_virology,5-shot,accuracy,0.2831325301204819,0.0350729543137051
Salesforce/codegen-16B-mono,mmlu_econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
Salesforce/codegen-16B-mono,mmlu_high_school_geography,5-shot,accuracy,0.1919191919191919,0.028057791672989
Salesforce/codegen-16B-mono,mmlu_high_school_government_and_politics,5-shot,accuracy,0.2020725388601036,0.0289790897942967
Salesforce/codegen-16B-mono,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2256410256410256,0.0211936325251485
Salesforce/codegen-16B-mono,mmlu_high_school_microeconomics,5-shot,accuracy,0.2310924369747899,0.0273814069278689
Salesforce/codegen-16B-mono,mmlu_high_school_psychology,5-shot,accuracy,0.2330275229357798,0.0181256691808614
Salesforce/codegen-16B-mono,mmlu_human_sexuality,5-shot,accuracy,0.2748091603053435,0.0391534540884783
Salesforce/codegen-16B-mono,mmlu_professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
Salesforce/codegen-16B-mono,mmlu_public_relations,5-shot,accuracy,0.2727272727272727,0.0426579211094058
Salesforce/codegen-16B-mono,mmlu_security_studies,5-shot,accuracy,0.2163265306122449,0.026358916334904
Salesforce/codegen-16B-mono,mmlu_sociology,5-shot,accuracy,0.2985074626865671,0.0323574378935504
Salesforce/codegen-16B-mono,mmlu_us_foreign_policy,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-16B-mono,mmlu_abstract_algebra,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-16B-mono,mmlu_anatomy,5-shot,accuracy,0.2222222222222222,0.0359144408419697
Salesforce/codegen-16B-mono,mmlu_astronomy,5-shot,accuracy,0.2039473684210526,0.0327900040631005
Salesforce/codegen-16B-mono,mmlu_college_biology,5-shot,accuracy,0.25,0.036210341218895
Salesforce/codegen-16B-mono,mmlu_college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-16B-mono,mmlu_college_computer_science,5-shot,accuracy,0.19,0.0394277244403662
Salesforce/codegen-16B-mono,mmlu_college_mathematics,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-16B-mono,mmlu_college_physics,5-shot,accuracy,0.1862745098039215,0.0387395871414935
Salesforce/codegen-16B-mono,mmlu_computer_security,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-16B-mono,mmlu_conceptual_physics,5-shot,accuracy,0.2936170212765957,0.0297716427124912
Salesforce/codegen-16B-mono,mmlu_electrical_engineering,5-shot,accuracy,0.2896551724137931,0.0378001923043801
Salesforce/codegen-16B-mono,mmlu_elementary_mathematics,5-shot,accuracy,0.2328042328042328,0.0217659616721545
Salesforce/codegen-16B-mono,mmlu_high_school_biology,5-shot,accuracy,0.1870967741935484,0.0221857100922522
Salesforce/codegen-16B-mono,mmlu_high_school_chemistry,5-shot,accuracy,0.1970443349753694,0.0279867246667362
Salesforce/codegen-16B-mono,mmlu_high_school_computer_science,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-16B-mono,mmlu_high_school_mathematics,5-shot,accuracy,0.2,0.0243884304339876
Salesforce/codegen-16B-mono,mmlu_high_school_physics,5-shot,accuracy,0.2781456953642384,0.0365860326276374
Salesforce/codegen-16B-mono,mmlu_high_school_statistics,5-shot,accuracy,0.2129629629629629,0.0279209631479936
Salesforce/codegen-16B-mono,mmlu_machine_learning,5-shot,accuracy,0.2857142857142857,0.0428785875134045
Salesforce/codegen-6B-mono,mmlu_formal_logic,5-shot,accuracy,0.2936507936507936,0.0407352432214712
Salesforce/codegen-6B-mono,mmlu_high_school_european_history,5-shot,accuracy,0.2181818181818181,0.0322507810830628
Salesforce/codegen-6B-mono,mmlu_high_school_us_history,5-shot,accuracy,0.2205882352941176,0.029102254389674
Salesforce/codegen-6B-mono,mmlu_high_school_world_history,5-shot,accuracy,0.2658227848101265,0.0287567996296583
Salesforce/codegen-6B-mono,mmlu_international_law,5-shot,accuracy,0.2644628099173554,0.040261875275912
Salesforce/codegen-6B-mono,mmlu_jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
Salesforce/codegen-6B-mono,mmlu_logical_fallacies,5-shot,accuracy,0.2822085889570552,0.0353611788666474
Salesforce/codegen-6B-mono,mmlu_moral_disputes,5-shot,accuracy,0.2601156069364161,0.0236186783100693
Salesforce/codegen-6B-mono,mmlu_moral_scenarios,5-shot,accuracy,0.2502793296089385,0.0144875008528504
Salesforce/codegen-6B-mono,mmlu_philosophy,5-shot,accuracy,0.2282958199356913,0.0238393033113982
Salesforce/codegen-6B-mono,mmlu_prehistory,5-shot,accuracy,0.2808641975308642,0.0250064697557992
Salesforce/codegen-6B-mono,mmlu_professional_law,5-shot,accuracy,0.2405475880052151,0.0109164067354789
Salesforce/codegen-6B-mono,mmlu_world_religions,5-shot,accuracy,0.3099415204678362,0.0354697695939316
Salesforce/codegen-6B-mono,mmlu_business_ethics,5-shot,accuracy,0.35,0.0479372485441101
Salesforce/codegen-6B-mono,mmlu_clinical_knowledge,5-shot,accuracy,0.2339622641509434,0.0260552969011529
Salesforce/codegen-6B-mono,mmlu_college_medicine,5-shot,accuracy,0.2023121387283237,0.0306311455391988
Salesforce/codegen-6B-mono,mmlu_global_facts,5-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-6B-mono,mmlu_human_aging,5-shot,accuracy,0.2511210762331838,0.0291052208332246
Salesforce/codegen-6B-mono,mmlu_management,5-shot,accuracy,0.1553398058252427,0.0358659473857397
Salesforce/codegen-6B-mono,mmlu_marketing,5-shot,accuracy,0.2606837606837607,0.0287603489565234
Salesforce/codegen-6B-mono,mmlu_medical_genetics,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-6B-mono,mmlu_miscellaneous,5-shot,accuracy,0.2669220945083014,0.0158184508947775
Salesforce/codegen-6B-mono,mmlu_nutrition,5-shot,accuracy,0.2549019607843137,0.0249541843248799
Salesforce/codegen-6B-mono,mmlu_professional_accounting,5-shot,accuracy,0.2482269503546099,0.0257700156442903
Salesforce/codegen-6B-mono,mmlu_professional_medicine,5-shot,accuracy,0.375,0.0294083729322787
Salesforce/codegen-6B-mono,mmlu_virology,5-shot,accuracy,0.2710843373493976,0.0346057990755302
Salesforce/codegen-6B-mono,mmlu_econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
Salesforce/codegen-6B-mono,mmlu_high_school_geography,5-shot,accuracy,0.2676767676767677,0.0315444988827028
Salesforce/codegen-6B-mono,mmlu_high_school_government_and_politics,5-shot,accuracy,0.233160621761658,0.030516111371476
Salesforce/codegen-6B-mono,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2948717948717949,0.0231193627582322
Salesforce/codegen-6B-mono,mmlu_high_school_microeconomics,5-shot,accuracy,0.3235294117647059,0.0303883535518868
Salesforce/codegen-6B-mono,mmlu_high_school_psychology,5-shot,accuracy,0.2330275229357798,0.0181256691808614
Salesforce/codegen-6B-mono,mmlu_human_sexuality,5-shot,accuracy,0.2519083969465648,0.0380738711630608
Salesforce/codegen-6B-mono,mmlu_professional_psychology,5-shot,accuracy,0.272875816993464,0.0180204741483935
Salesforce/codegen-6B-mono,mmlu_public_relations,5-shot,accuracy,0.1909090909090909,0.0376442558598492
Salesforce/codegen-6B-mono,mmlu_security_studies,5-shot,accuracy,0.2897959183673469,0.0290430886833043
Salesforce/codegen-6B-mono,mmlu_sociology,5-shot,accuracy,0.263681592039801,0.0311571508693555
Salesforce/codegen-6B-mono,mmlu_us_foreign_policy,5-shot,accuracy,0.34,0.0476095228569523
Salesforce/codegen-6B-mono,mmlu_abstract_algebra,5-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-6B-mono,mmlu_anatomy,5-shot,accuracy,0.2,0.0345547370232543
Salesforce/codegen-6B-mono,mmlu_astronomy,5-shot,accuracy,0.1973684210526315,0.0323898160169939
Salesforce/codegen-6B-mono,mmlu_college_biology,5-shot,accuracy,0.2430555555555555,0.0358687928008034
Salesforce/codegen-6B-mono,mmlu_college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-6B-mono,mmlu_college_computer_science,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-6B-mono,mmlu_college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-6B-mono,mmlu_college_physics,5-shot,accuracy,0.1666666666666666,0.0370828466241654
Salesforce/codegen-6B-mono,mmlu_computer_security,5-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-6B-mono,mmlu_conceptual_physics,5-shot,accuracy,0.3063829787234042,0.0301359064785175
Salesforce/codegen-6B-mono,mmlu_electrical_engineering,5-shot,accuracy,0.2068965517241379,0.0337567244956055
Salesforce/codegen-6B-mono,mmlu_elementary_mathematics,5-shot,accuracy,0.2354497354497354,0.0218515098220317
Salesforce/codegen-6B-mono,mmlu_high_school_biology,5-shot,accuracy,0.2096774193548387,0.0231578793490835
Salesforce/codegen-6B-mono,mmlu_high_school_chemistry,5-shot,accuracy,0.2807881773399014,0.0316185633535861
Salesforce/codegen-6B-mono,mmlu_high_school_computer_science,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-6B-mono,mmlu_high_school_mathematics,5-shot,accuracy,0.2777777777777778,0.0273091405882301
Salesforce/codegen-6B-mono,mmlu_high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
Salesforce/codegen-6B-mono,mmlu_high_school_statistics,5-shot,accuracy,0.375,0.0330169089872108
Salesforce/codegen-6B-mono,mmlu_machine_learning,5-shot,accuracy,0.3125,0.0439946505757152
Salesforce/codegen-2B-multi,mmlu_formal_logic,5-shot,accuracy,0.1269841269841269,0.0297804175226884
Salesforce/codegen-2B-multi,mmlu_high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000918
Salesforce/codegen-2B-multi,mmlu_high_school_us_history,5-shot,accuracy,0.2450980392156862,0.0301902824535019
Salesforce/codegen-2B-multi,mmlu_high_school_world_history,5-shot,accuracy,0.2067510548523206,0.026361651668389
Salesforce/codegen-2B-multi,mmlu_international_law,5-shot,accuracy,0.3636363636363636,0.0439132628672407
Salesforce/codegen-2B-multi,mmlu_jurisprudence,5-shot,accuracy,0.2592592592592592,0.0423651125809463
Salesforce/codegen-2B-multi,mmlu_logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
Salesforce/codegen-2B-multi,mmlu_moral_disputes,5-shot,accuracy,0.2543352601156069,0.0234458262765455
Salesforce/codegen-2B-multi,mmlu_moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
Salesforce/codegen-2B-multi,mmlu_philosophy,5-shot,accuracy,0.2958199356913183,0.0259223717888187
Salesforce/codegen-2B-multi,mmlu_prehistory,5-shot,accuracy,0.2314814814814814,0.0234684298324511
Salesforce/codegen-2B-multi,mmlu_professional_law,5-shot,accuracy,0.2516297262059974,0.0110832762804419
Salesforce/codegen-2B-multi,mmlu_world_religions,5-shot,accuracy,0.239766081871345,0.0327448521194695
Salesforce/codegen-2B-multi,mmlu_business_ethics,5-shot,accuracy,0.21,0.0409360180740332
Salesforce/codegen-2B-multi,mmlu_clinical_knowledge,5-shot,accuracy,0.260377358490566,0.027008766090708
Salesforce/codegen-2B-multi,mmlu_college_medicine,5-shot,accuracy,0.2427745664739884,0.0326926380614177
Salesforce/codegen-2B-multi,mmlu_global_facts,5-shot,accuracy,0.32,0.046882617226215
Salesforce/codegen-2B-multi,mmlu_human_aging,5-shot,accuracy,0.3139013452914798,0.0311467964829724
Salesforce/codegen-2B-multi,mmlu_management,5-shot,accuracy,0.1359223300970873,0.0339329572976101
Salesforce/codegen-2B-multi,mmlu_marketing,5-shot,accuracy,0.3076923076923077,0.030236389942173
Salesforce/codegen-2B-multi,mmlu_medical_genetics,5-shot,accuracy,0.22,0.0416333199893227
Salesforce/codegen-2B-multi,mmlu_miscellaneous,5-shot,accuracy,0.2733077905491698,0.0159366810626285
Salesforce/codegen-2B-multi,mmlu_nutrition,5-shot,accuracy,0.238562091503268,0.0244043949280878
Salesforce/codegen-2B-multi,mmlu_professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
Salesforce/codegen-2B-multi,mmlu_professional_medicine,5-shot,accuracy,0.3492647058823529,0.0289597551968248
Salesforce/codegen-2B-multi,mmlu_virology,5-shot,accuracy,0.2951807228915662,0.0355092018568962
Salesforce/codegen-2B-multi,mmlu_econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984367
Salesforce/codegen-2B-multi,mmlu_high_school_geography,5-shot,accuracy,0.2323232323232323,0.0300886294902174
Salesforce/codegen-2B-multi,mmlu_high_school_government_and_politics,5-shot,accuracy,0.3419689119170984,0.0342346510010428
Salesforce/codegen-2B-multi,mmlu_high_school_macroeconomics,5-shot,accuracy,0.258974358974359,0.0222111068100616
Salesforce/codegen-2B-multi,mmlu_high_school_microeconomics,5-shot,accuracy,0.2100840336134453,0.0264613987174718
Salesforce/codegen-2B-multi,mmlu_high_school_psychology,5-shot,accuracy,0.2073394495412844,0.0173814155636086
Salesforce/codegen-2B-multi,mmlu_human_sexuality,5-shot,accuracy,0.2137404580152671,0.0359546161177469
Salesforce/codegen-2B-multi,mmlu_professional_psychology,5-shot,accuracy,0.261437908496732,0.017776947157528
Salesforce/codegen-2B-multi,mmlu_public_relations,5-shot,accuracy,0.2272727272727272,0.0401396455407277
Salesforce/codegen-2B-multi,mmlu_security_studies,5-shot,accuracy,0.2326530612244897,0.0270492579158961
Salesforce/codegen-2B-multi,mmlu_sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
Salesforce/codegen-2B-multi,mmlu_us_foreign_policy,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-2B-multi,mmlu_abstract_algebra,5-shot,accuracy,0.21,0.0409360180740332
Salesforce/codegen-2B-multi,mmlu_anatomy,5-shot,accuracy,0.3111111111111111,0.0399926287661772
Salesforce/codegen-2B-multi,mmlu_astronomy,5-shot,accuracy,0.1907894736842105,0.031975658210325
Salesforce/codegen-2B-multi,mmlu_college_biology,5-shot,accuracy,0.2638888888888889,0.0368565109589753
Salesforce/codegen-2B-multi,mmlu_college_chemistry,5-shot,accuracy,0.13,0.033799766898963
Salesforce/codegen-2B-multi,mmlu_college_computer_science,5-shot,accuracy,0.36,0.0482418151324421
Salesforce/codegen-2B-multi,mmlu_college_mathematics,5-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-2B-multi,mmlu_college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
Salesforce/codegen-2B-multi,mmlu_computer_security,5-shot,accuracy,0.22,0.0416333199893226
Salesforce/codegen-2B-multi,mmlu_conceptual_physics,5-shot,accuracy,0.2553191489361702,0.0285048564705142
Salesforce/codegen-2B-multi,mmlu_electrical_engineering,5-shot,accuracy,0.2827586206896552,0.0375283395800333
Salesforce/codegen-2B-multi,mmlu_elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.0224945107675031
Salesforce/codegen-2B-multi,mmlu_high_school_biology,5-shot,accuracy,0.3258064516129032,0.0266620105785671
Salesforce/codegen-2B-multi,mmlu_high_school_chemistry,5-shot,accuracy,0.2266009852216748,0.0294548638352929
Salesforce/codegen-2B-multi,mmlu_high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
Salesforce/codegen-2B-multi,mmlu_high_school_mathematics,5-shot,accuracy,0.2629629629629629,0.0268420578738337
Salesforce/codegen-2B-multi,mmlu_high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
Salesforce/codegen-2B-multi,mmlu_high_school_statistics,5-shot,accuracy,0.4398148148148148,0.0338517797604481
Salesforce/codegen-2B-multi,mmlu_machine_learning,5-shot,accuracy,0.2321428571428571,0.040073418097558
Salesforce/codegen-350M-multi,mmlu_formal_logic,5-shot,accuracy,0.1825396825396825,0.0345507101910214
Salesforce/codegen-350M-multi,mmlu_high_school_european_history,5-shot,accuracy,0.2909090909090909,0.0354656301962433
Salesforce/codegen-350M-multi,mmlu_high_school_us_history,5-shot,accuracy,0.2745098039215686,0.0313217980308328
Salesforce/codegen-350M-multi,mmlu_high_school_world_history,5-shot,accuracy,0.2953586497890295,0.0296963387134228
Salesforce/codegen-350M-multi,mmlu_international_law,5-shot,accuracy,0.2809917355371901,0.0410320383051451
Salesforce/codegen-350M-multi,mmlu_jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
Salesforce/codegen-350M-multi,mmlu_logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
Salesforce/codegen-350M-multi,mmlu_moral_disputes,5-shot,accuracy,0.2312138728323699,0.0226986571678557
Salesforce/codegen-350M-multi,mmlu_moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
Salesforce/codegen-350M-multi,mmlu_philosophy,5-shot,accuracy,0.2572347266881029,0.0248261712892508
Salesforce/codegen-350M-multi,mmlu_prehistory,5-shot,accuracy,0.228395061728395,0.0233582118406262
Salesforce/codegen-350M-multi,mmlu_professional_law,5-shot,accuracy,0.2477183833116036,0.0110254992914437
Salesforce/codegen-350M-multi,mmlu_world_religions,5-shot,accuracy,0.2514619883040935,0.0332750442384684
Salesforce/codegen-350M-multi,mmlu_business_ethics,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-350M-multi,mmlu_clinical_knowledge,5-shot,accuracy,0.2867924528301886,0.027834912527544
Salesforce/codegen-350M-multi,mmlu_college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225943
Salesforce/codegen-350M-multi,mmlu_global_facts,5-shot,accuracy,0.16,0.036845294917747
Salesforce/codegen-350M-multi,mmlu_human_aging,5-shot,accuracy,0.2869955156950672,0.0303603797102919
Salesforce/codegen-350M-multi,mmlu_management,5-shot,accuracy,0.2621359223300971,0.0435463107726059
Salesforce/codegen-350M-multi,mmlu_marketing,5-shot,accuracy,0.1965811965811965,0.0260353860989512
Salesforce/codegen-350M-multi,mmlu_medical_genetics,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-multi,mmlu_miscellaneous,5-shot,accuracy,0.247765006385696,0.0154380830805689
Salesforce/codegen-350M-multi,mmlu_nutrition,5-shot,accuracy,0.261437908496732,0.0251609982142924
Salesforce/codegen-350M-multi,mmlu_professional_accounting,5-shot,accuracy,0.2304964539007092,0.0251237392268724
Salesforce/codegen-350M-multi,mmlu_professional_medicine,5-shot,accuracy,0.4522058823529412,0.0302337585515964
Salesforce/codegen-350M-multi,mmlu_virology,5-shot,accuracy,0.1686746987951807,0.0291520096278565
Salesforce/codegen-350M-multi,mmlu_econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
Salesforce/codegen-350M-multi,mmlu_high_school_geography,5-shot,accuracy,0.3535353535353535,0.0340608672354715
Salesforce/codegen-350M-multi,mmlu_high_school_government_and_politics,5-shot,accuracy,0.3678756476683937,0.0348017566846603
Salesforce/codegen-350M-multi,mmlu_high_school_macroeconomics,5-shot,accuracy,0.3256410256410256,0.0237596657674122
Salesforce/codegen-350M-multi,mmlu_high_school_microeconomics,5-shot,accuracy,0.2142857142857142,0.0266535315967154
Salesforce/codegen-350M-multi,mmlu_high_school_psychology,5-shot,accuracy,0.3064220183486238,0.0197655172204585
Salesforce/codegen-350M-multi,mmlu_human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
Salesforce/codegen-350M-multi,mmlu_professional_psychology,5-shot,accuracy,0.2516339869281045,0.0175558180913222
Salesforce/codegen-350M-multi,mmlu_public_relations,5-shot,accuracy,0.1454545454545454,0.0337689831983308
Salesforce/codegen-350M-multi,mmlu_security_studies,5-shot,accuracy,0.3020408163265306,0.0293936093198798
Salesforce/codegen-350M-multi,mmlu_sociology,5-shot,accuracy,0.2388059701492537,0.0301477759354092
Salesforce/codegen-350M-multi,mmlu_us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-350M-multi,mmlu_abstract_algebra,5-shot,accuracy,0.22,0.0416333199893226
Salesforce/codegen-350M-multi,mmlu_anatomy,5-shot,accuracy,0.2666666666666666,0.038201699145179
Salesforce/codegen-350M-multi,mmlu_astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
Salesforce/codegen-350M-multi,mmlu_college_biology,5-shot,accuracy,0.2083333333333333,0.0339611620584533
Salesforce/codegen-350M-multi,mmlu_college_chemistry,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-multi,mmlu_college_computer_science,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-350M-multi,mmlu_college_mathematics,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-350M-multi,mmlu_college_physics,5-shot,accuracy,0.2254901960784313,0.0415830753308328
Salesforce/codegen-350M-multi,mmlu_computer_security,5-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-350M-multi,mmlu_conceptual_physics,5-shot,accuracy,0.2425531914893617,0.0280202262712002
Salesforce/codegen-350M-multi,mmlu_electrical_engineering,5-shot,accuracy,0.2758620689655172,0.0372456361977463
Salesforce/codegen-350M-multi,mmlu_elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
Salesforce/codegen-350M-multi,mmlu_high_school_biology,5-shot,accuracy,0.2741935483870967,0.0253781399708852
Salesforce/codegen-350M-multi,mmlu_high_school_chemistry,5-shot,accuracy,0.2216748768472906,0.0292255758924896
Salesforce/codegen-350M-multi,mmlu_high_school_computer_science,5-shot,accuracy,0.19,0.0394277244403662
Salesforce/codegen-350M-multi,mmlu_high_school_mathematics,5-shot,accuracy,0.2851851851851852,0.0275285992103404
Salesforce/codegen-350M-multi,mmlu_high_school_physics,5-shot,accuracy,0.2781456953642384,0.0365860326276374
Salesforce/codegen-350M-multi,mmlu_high_school_statistics,5-shot,accuracy,0.4629629629629629,0.0340060362553827
Salesforce/codegen-350M-multi,mmlu_machine_learning,5-shot,accuracy,0.3214285714285714,0.0443280405529151
Salesforce/codegen-2B-mono,mmlu_formal_logic,5-shot,accuracy,0.2857142857142857,0.0404061017820884
Salesforce/codegen-2B-mono,mmlu_high_school_european_history,5-shot,accuracy,0.2303030303030303,0.0328766675860349
Salesforce/codegen-2B-mono,mmlu_high_school_us_history,5-shot,accuracy,0.2058823529411764,0.0283794494515886
Salesforce/codegen-2B-mono,mmlu_high_school_world_history,5-shot,accuracy,0.2658227848101265,0.0287567996296583
Salesforce/codegen-2B-mono,mmlu_international_law,5-shot,accuracy,0.3305785123966942,0.0429434084521209
Salesforce/codegen-2B-mono,mmlu_jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
Salesforce/codegen-2B-mono,mmlu_logical_fallacies,5-shot,accuracy,0.2699386503067484,0.0348782516849789
Salesforce/codegen-2B-mono,mmlu_moral_disputes,5-shot,accuracy,0.2254335260115607,0.0224972301909675
Salesforce/codegen-2B-mono,mmlu_moral_scenarios,5-shot,accuracy,0.2446927374301676,0.0143781698840984
Salesforce/codegen-2B-mono,mmlu_philosophy,5-shot,accuracy,0.2861736334405145,0.0256702592421889
Salesforce/codegen-2B-mono,mmlu_prehistory,5-shot,accuracy,0.2314814814814814,0.0234684298324511
Salesforce/codegen-2B-mono,mmlu_professional_law,5-shot,accuracy,0.2438070404172099,0.0109665079721784
Salesforce/codegen-2B-mono,mmlu_world_religions,5-shot,accuracy,0.2573099415204678,0.0335279984416186
Salesforce/codegen-2B-mono,mmlu_business_ethics,5-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-2B-mono,mmlu_clinical_knowledge,5-shot,accuracy,0.2792452830188679,0.0276111634023997
Salesforce/codegen-2B-mono,mmlu_college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225943
Salesforce/codegen-2B-mono,mmlu_global_facts,5-shot,accuracy,0.19,0.0394277244403662
Salesforce/codegen-2B-mono,mmlu_human_aging,5-shot,accuracy,0.1748878923766816,0.0254952846264449
Salesforce/codegen-2B-mono,mmlu_management,5-shot,accuracy,0.3300970873786408,0.0465614711001235
Salesforce/codegen-2B-mono,mmlu_marketing,5-shot,accuracy,0.2948717948717949,0.0298725777088911
Salesforce/codegen-2B-mono,mmlu_medical_genetics,5-shot,accuracy,0.26,0.0440844002276807
Salesforce/codegen-2B-mono,mmlu_miscellaneous,5-shot,accuracy,0.231162196679438,0.015075523238101
Salesforce/codegen-2B-mono,mmlu_nutrition,5-shot,accuracy,0.2777777777777778,0.0256468630971379
Salesforce/codegen-2B-mono,mmlu_professional_accounting,5-shot,accuracy,0.2695035460992908,0.0264690368185906
Salesforce/codegen-2B-mono,mmlu_professional_medicine,5-shot,accuracy,0.4375,0.0301346149544039
Salesforce/codegen-2B-mono,mmlu_virology,5-shot,accuracy,0.2710843373493976,0.0346057990755302
Salesforce/codegen-2B-mono,mmlu_econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
Salesforce/codegen-2B-mono,mmlu_high_school_geography,5-shot,accuracy,0.2828282828282828,0.0320877955878675
Salesforce/codegen-2B-mono,mmlu_high_school_government_and_politics,5-shot,accuracy,0.310880829015544,0.0334036190627658
Salesforce/codegen-2B-mono,mmlu_high_school_macroeconomics,5-shot,accuracy,0.3333333333333333,0.0239011579794025
Salesforce/codegen-2B-mono,mmlu_high_school_microeconomics,5-shot,accuracy,0.2647058823529412,0.0286574912850719
Salesforce/codegen-2B-mono,mmlu_high_school_psychology,5-shot,accuracy,0.2458715596330275,0.0184619409687084
Salesforce/codegen-2B-mono,mmlu_human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
Salesforce/codegen-2B-mono,mmlu_professional_psychology,5-shot,accuracy,0.2647058823529412,0.0178480895749132
Salesforce/codegen-2B-mono,mmlu_public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
Salesforce/codegen-2B-mono,mmlu_security_studies,5-shot,accuracy,0.3551020408163265,0.0306356551503876
Salesforce/codegen-2B-mono,mmlu_sociology,5-shot,accuracy,0.2686567164179104,0.0313432835820895
Salesforce/codegen-2B-mono,mmlu_us_foreign_policy,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-2B-mono,mmlu_abstract_algebra,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-2B-mono,mmlu_anatomy,5-shot,accuracy,0.237037037037037,0.036737316839695
Salesforce/codegen-2B-mono,mmlu_astronomy,5-shot,accuracy,0.2565789473684211,0.0355418036802568
Salesforce/codegen-2B-mono,mmlu_college_biology,5-shot,accuracy,0.2777777777777778,0.0374555479146245
Salesforce/codegen-2B-mono,mmlu_college_chemistry,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-2B-mono,mmlu_college_computer_science,5-shot,accuracy,0.39,0.0490207130000197
Salesforce/codegen-2B-mono,mmlu_college_mathematics,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-2B-mono,mmlu_college_physics,5-shot,accuracy,0.1764705882352941,0.0379328118530781
Salesforce/codegen-2B-mono,mmlu_computer_security,5-shot,accuracy,0.22,0.0416333199893226
Salesforce/codegen-2B-mono,mmlu_conceptual_physics,5-shot,accuracy,0.2936170212765957,0.0297716427124912
Salesforce/codegen-2B-mono,mmlu_electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
Salesforce/codegen-2B-mono,mmlu_elementary_mathematics,5-shot,accuracy,0.246031746031746,0.0221820372029483
Salesforce/codegen-2B-mono,mmlu_high_school_biology,5-shot,accuracy,0.1838709677419354,0.0220372173402678
Salesforce/codegen-2B-mono,mmlu_high_school_chemistry,5-shot,accuracy,0.270935960591133,0.0312709071329769
Salesforce/codegen-2B-mono,mmlu_high_school_computer_science,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-2B-mono,mmlu_high_school_mathematics,5-shot,accuracy,0.2592592592592592,0.0267192407837121
Salesforce/codegen-2B-mono,mmlu_high_school_physics,5-shot,accuracy,0.3509933774834437,0.0389698196425737
Salesforce/codegen-2B-mono,mmlu_high_school_statistics,5-shot,accuracy,0.3703703703703703,0.0329337713941519
Salesforce/codegen-2B-mono,mmlu_machine_learning,5-shot,accuracy,0.1964285714285714,0.0377097004934701
Salesforce/codegen-350M-mono,mmlu_formal_logic,5-shot,accuracy,0.2063492063492063,0.0361960452412424
Salesforce/codegen-350M-mono,mmlu_high_school_european_history,5-shot,accuracy,0.2424242424242424,0.0334640988105595
Salesforce/codegen-350M-mono,mmlu_high_school_us_history,5-shot,accuracy,0.2549019607843137,0.0305875913516042
Salesforce/codegen-350M-mono,mmlu_high_school_world_history,5-shot,accuracy,0.2194092827004219,0.0269391065815539
Salesforce/codegen-350M-mono,mmlu_international_law,5-shot,accuracy,0.2479338842975206,0.039418975265163
Salesforce/codegen-350M-mono,mmlu_jurisprudence,5-shot,accuracy,0.2407407407407407,0.0413311944024383
Salesforce/codegen-350M-mono,mmlu_logical_fallacies,5-shot,accuracy,0.2576687116564417,0.0343615082784691
Salesforce/codegen-350M-mono,mmlu_moral_disputes,5-shot,accuracy,0.1820809248554913,0.0207767611025129
Salesforce/codegen-350M-mono,mmlu_moral_scenarios,5-shot,accuracy,0.2379888268156424,0.0142426300705748
Salesforce/codegen-350M-mono,mmlu_philosophy,5-shot,accuracy,0.2379421221864952,0.0241851506478187
Salesforce/codegen-350M-mono,mmlu_prehistory,5-shot,accuracy,0.2067901234567901,0.0225350067059428
Salesforce/codegen-350M-mono,mmlu_professional_law,5-shot,accuracy,0.2483702737940026,0.0110352125980344
Salesforce/codegen-350M-mono,mmlu_world_religions,5-shot,accuracy,0.2923976608187134,0.0348864771345792
Salesforce/codegen-350M-mono,mmlu_business_ethics,5-shot,accuracy,0.33,0.047258156262526
Salesforce/codegen-350M-mono,mmlu_clinical_knowledge,5-shot,accuracy,0.2792452830188679,0.0276111634023997
Salesforce/codegen-350M-mono,mmlu_college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
Salesforce/codegen-350M-mono,mmlu_global_facts,5-shot,accuracy,0.18,0.0386122919665369
Salesforce/codegen-350M-mono,mmlu_human_aging,5-shot,accuracy,0.2197309417040358,0.0277901770643836
Salesforce/codegen-350M-mono,mmlu_management,5-shot,accuracy,0.203883495145631,0.0398913985953177
Salesforce/codegen-350M-mono,mmlu_marketing,5-shot,accuracy,0.2393162393162393,0.0279518268089243
Salesforce/codegen-350M-mono,mmlu_medical_genetics,5-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-350M-mono,mmlu_miscellaneous,5-shot,accuracy,0.2515964240102171,0.0155173223655296
Salesforce/codegen-350M-mono,mmlu_nutrition,5-shot,accuracy,0.2549019607843137,0.0249541843248799
Salesforce/codegen-350M-mono,mmlu_professional_accounting,5-shot,accuracy,0.2765957446808511,0.0266845643404609
Salesforce/codegen-350M-mono,mmlu_professional_medicine,5-shot,accuracy,0.3566176470588235,0.0290972095684119
Salesforce/codegen-350M-mono,mmlu_virology,5-shot,accuracy,0.1927710843373494,0.0307098240505652
Salesforce/codegen-350M-mono,mmlu_econometrics,5-shot,accuracy,0.2105263157894736,0.0383515395439942
Salesforce/codegen-350M-mono,mmlu_high_school_geography,5-shot,accuracy,0.3535353535353535,0.0340608672354715
Salesforce/codegen-350M-mono,mmlu_high_school_government_and_politics,5-shot,accuracy,0.3626943005181347,0.0346971379170437
Salesforce/codegen-350M-mono,mmlu_high_school_macroeconomics,5-shot,accuracy,0.3384615384615385,0.023991500500313
Salesforce/codegen-350M-mono,mmlu_high_school_microeconomics,5-shot,accuracy,0.2647058823529412,0.0286574912850719
Salesforce/codegen-350M-mono,mmlu_high_school_psychology,5-shot,accuracy,0.2954128440366972,0.019560619182976
Salesforce/codegen-350M-mono,mmlu_human_sexuality,5-shot,accuracy,0.2290076335877862,0.0368534663171185
Salesforce/codegen-350M-mono,mmlu_professional_psychology,5-shot,accuracy,0.2450980392156862,0.0174018167114276
Salesforce/codegen-350M-mono,mmlu_public_relations,5-shot,accuracy,0.209090909090909,0.0389509101572413
Salesforce/codegen-350M-mono,mmlu_security_studies,5-shot,accuracy,0.2775510204081632,0.0286668577902746
Salesforce/codegen-350M-mono,mmlu_sociology,5-shot,accuracy,0.2338308457711442,0.0299294154083484
Salesforce/codegen-350M-mono,mmlu_us_foreign_policy,5-shot,accuracy,0.27,0.0446196043338474
Salesforce/codegen-350M-mono,mmlu_abstract_algebra,5-shot,accuracy,0.21,0.0409360180740332
Salesforce/codegen-350M-mono,mmlu_anatomy,5-shot,accuracy,0.237037037037037,0.036737316839695
Salesforce/codegen-350M-mono,mmlu_astronomy,5-shot,accuracy,0.1842105263157894,0.0315469804508223
Salesforce/codegen-350M-mono,mmlu_college_biology,5-shot,accuracy,0.25,0.036210341218895
Salesforce/codegen-350M-mono,mmlu_college_chemistry,5-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-350M-mono,mmlu_college_computer_science,5-shot,accuracy,0.38,0.0487831731214563
Salesforce/codegen-350M-mono,mmlu_college_mathematics,5-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-350M-mono,mmlu_college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
Salesforce/codegen-350M-mono,mmlu_computer_security,5-shot,accuracy,0.2,0.0402015126103684
Salesforce/codegen-350M-mono,mmlu_conceptual_physics,5-shot,accuracy,0.3361702127659574,0.0308816185206769
Salesforce/codegen-350M-mono,mmlu_electrical_engineering,5-shot,accuracy,0.2551724137931034,0.0363298405270784
Salesforce/codegen-350M-mono,mmlu_elementary_mathematics,5-shot,accuracy,0.246031746031746,0.0221820372029483
Salesforce/codegen-350M-mono,mmlu_high_school_biology,5-shot,accuracy,0.2967741935483871,0.0259885007924118
Salesforce/codegen-350M-mono,mmlu_high_school_chemistry,5-shot,accuracy,0.2857142857142857,0.0317852971064275
Salesforce/codegen-350M-mono,mmlu_high_school_computer_science,5-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-350M-mono,mmlu_high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.0264661175389599
Salesforce/codegen-350M-mono,mmlu_high_school_physics,5-shot,accuracy,0.2781456953642384,0.0365860326276374
Salesforce/codegen-350M-mono,mmlu_high_school_statistics,5-shot,accuracy,0.4444444444444444,0.0338885711850232
Salesforce/codegen-350M-mono,mmlu_machine_learning,5-shot,accuracy,0.2589285714285714,0.0415775153986562
cerebras/btlm-3b-8k-base,mmlu_formal_logic,5-shot,accuracy,0.1666666666666666,0.0333333333333333
cerebras/btlm-3b-8k-base,mmlu_high_school_european_history,5-shot,accuracy,0.3090909090909091,0.0360854101157396
cerebras/btlm-3b-8k-base,mmlu_high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
cerebras/btlm-3b-8k-base,mmlu_high_school_world_history,5-shot,accuracy,0.3375527426160337,0.0307815491020262
cerebras/btlm-3b-8k-base,mmlu_international_law,5-shot,accuracy,0.2892561983471074,0.0413911272763546
cerebras/btlm-3b-8k-base,mmlu_jurisprudence,5-shot,accuracy,0.2407407407407407,0.0413311944024383
cerebras/btlm-3b-8k-base,mmlu_logical_fallacies,5-shot,accuracy,0.1963190184049079,0.0312079703947092
cerebras/btlm-3b-8k-base,mmlu_moral_disputes,5-shot,accuracy,0.26878612716763,0.0238680032625001
cerebras/btlm-3b-8k-base,mmlu_moral_scenarios,5-shot,accuracy,0.2402234636871508,0.0142883438039253
cerebras/btlm-3b-8k-base,mmlu_philosophy,5-shot,accuracy,0.315112540192926,0.0263852737034644
cerebras/btlm-3b-8k-base,mmlu_prehistory,5-shot,accuracy,0.2808641975308642,0.0250064697557992
cerebras/btlm-3b-8k-base,mmlu_professional_law,5-shot,accuracy,0.2542372881355932,0.0111211290078406
cerebras/btlm-3b-8k-base,mmlu_world_religions,5-shot,accuracy,0.2923976608187134,0.0348864771345792
cerebras/btlm-3b-8k-base,mmlu_business_ethics,5-shot,accuracy,0.31,0.0464823198711731
cerebras/btlm-3b-8k-base,mmlu_clinical_knowledge,5-shot,accuracy,0.2377358490566037,0.0261998088075619
cerebras/btlm-3b-8k-base,mmlu_college_medicine,5-shot,accuracy,0.2832369942196532,0.0343556805604787
cerebras/btlm-3b-8k-base,mmlu_global_facts,5-shot,accuracy,0.34,0.0476095228569523
cerebras/btlm-3b-8k-base,mmlu_human_aging,5-shot,accuracy,0.3721973094170404,0.0324430528300873
cerebras/btlm-3b-8k-base,mmlu_management,5-shot,accuracy,0.233009708737864,0.0418583259892831
cerebras/btlm-3b-8k-base,mmlu_marketing,5-shot,accuracy,0.2948717948717949,0.0298725777088911
cerebras/btlm-3b-8k-base,mmlu_medical_genetics,5-shot,accuracy,0.28,0.0451260859854212
cerebras/btlm-3b-8k-base,mmlu_miscellaneous,5-shot,accuracy,0.3333333333333333,0.0168573912474725
cerebras/btlm-3b-8k-base,mmlu_nutrition,5-shot,accuracy,0.218954248366013,0.0236790898618077
cerebras/btlm-3b-8k-base,mmlu_professional_accounting,5-shot,accuracy,0.2304964539007092,0.0251237392268724
cerebras/btlm-3b-8k-base,mmlu_professional_medicine,5-shot,accuracy,0.2573529411764705,0.0265565194700415
cerebras/btlm-3b-8k-base,mmlu_virology,5-shot,accuracy,0.3192771084337349,0.0362933532994786
cerebras/btlm-3b-8k-base,mmlu_econometrics,5-shot,accuracy,0.2280701754385964,0.0394715278266941
cerebras/btlm-3b-8k-base,mmlu_high_school_geography,5-shot,accuracy,0.2272727272727272,0.0298575156733864
cerebras/btlm-3b-8k-base,mmlu_high_school_government_and_politics,5-shot,accuracy,0.2642487046632124,0.0318215505091664
cerebras/btlm-3b-8k-base,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2384615384615384,0.0216062944946477
cerebras/btlm-3b-8k-base,mmlu_high_school_microeconomics,5-shot,accuracy,0.2436974789915966,0.0278868280783805
cerebras/btlm-3b-8k-base,mmlu_high_school_psychology,5-shot,accuracy,0.2513761467889908,0.0185992063602874
cerebras/btlm-3b-8k-base,mmlu_human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
cerebras/btlm-3b-8k-base,mmlu_professional_psychology,5-shot,accuracy,0.272875816993464,0.0180204741483935
cerebras/btlm-3b-8k-base,mmlu_public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
cerebras/btlm-3b-8k-base,mmlu_security_studies,5-shot,accuracy,0.4,0.0313625024093589
cerebras/btlm-3b-8k-base,mmlu_sociology,5-shot,accuracy,0.2885572139303483,0.0320384104021332
cerebras/btlm-3b-8k-base,mmlu_us_foreign_policy,5-shot,accuracy,0.34,0.0476095228569523
cerebras/btlm-3b-8k-base,mmlu_abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
cerebras/btlm-3b-8k-base,mmlu_anatomy,5-shot,accuracy,0.1777777777777777,0.0330278985990171
cerebras/btlm-3b-8k-base,mmlu_astronomy,5-shot,accuracy,0.2631578947368421,0.0358349617636106
cerebras/btlm-3b-8k-base,mmlu_college_biology,5-shot,accuracy,0.2777777777777778,0.0374555479146245
cerebras/btlm-3b-8k-base,mmlu_college_chemistry,5-shot,accuracy,0.27,0.0446196043338474
cerebras/btlm-3b-8k-base,mmlu_college_computer_science,5-shot,accuracy,0.32,0.046882617226215
cerebras/btlm-3b-8k-base,mmlu_college_mathematics,5-shot,accuracy,0.29,0.0456048021572068
cerebras/btlm-3b-8k-base,mmlu_college_physics,5-shot,accuracy,0.2647058823529412,0.0438986995680878
cerebras/btlm-3b-8k-base,mmlu_computer_security,5-shot,accuracy,0.3,0.0460566186471838
cerebras/btlm-3b-8k-base,mmlu_conceptual_physics,5-shot,accuracy,0.3063829787234042,0.0301359064785175
cerebras/btlm-3b-8k-base,mmlu_electrical_engineering,5-shot,accuracy,0.2206896551724138,0.0345593020192481
cerebras/btlm-3b-8k-base,mmlu_elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.0224945107675031
cerebras/btlm-3b-8k-base,mmlu_high_school_biology,5-shot,accuracy,0.2161290322580645,0.0234152934335685
cerebras/btlm-3b-8k-base,mmlu_high_school_chemistry,5-shot,accuracy,0.2315270935960591,0.0296783331414444
cerebras/btlm-3b-8k-base,mmlu_high_school_computer_science,5-shot,accuracy,0.21,0.0409360180740332
cerebras/btlm-3b-8k-base,mmlu_high_school_mathematics,5-shot,accuracy,0.2740740740740741,0.0271959348040856
cerebras/btlm-3b-8k-base,mmlu_high_school_physics,5-shot,accuracy,0.23841059602649,0.0347918557259966
cerebras/btlm-3b-8k-base,mmlu_high_school_statistics,5-shot,accuracy,0.2638888888888889,0.0300582027043098
cerebras/btlm-3b-8k-base,mmlu_machine_learning,5-shot,accuracy,0.2142857142857142,0.0389464112004479
EleutherAI/pythia-70m,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.0393253768039287
EleutherAI/pythia-70m,mmlu_high_school_european_history,0-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/pythia-70m,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.0303915336927415
EleutherAI/pythia-70m,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
EleutherAI/pythia-70m,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.0389687898507041
EleutherAI/pythia-70m,mmlu_jurisprudence,0-shot,accuracy,0.2592592592592592,0.0423651125809463
EleutherAI/pythia-70m,mmlu_logical_fallacies,0-shot,accuracy,0.2208588957055214,0.0325917739274217
EleutherAI/pythia-70m,mmlu_moral_disputes,0-shot,accuracy,0.2514450867052023,0.023357365785874
EleutherAI/pythia-70m,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705748
EleutherAI/pythia-70m,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/pythia-70m,mmlu_prehistory,0-shot,accuracy,0.2129629629629629,0.0227797190887333
EleutherAI/pythia-70m,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.0109961566351426
EleutherAI/pythia-70m,mmlu_world_religions,0-shot,accuracy,0.327485380116959,0.0359933577145602
EleutherAI/pythia-70m,mmlu_business_ethics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-70m,mmlu_clinical_knowledge,0-shot,accuracy,0.2150943396226415,0.0252883945028913
EleutherAI/pythia-70m,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.0312651120617304
EleutherAI/pythia-70m,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-70m,mmlu_human_aging,0-shot,accuracy,0.3139013452914798,0.0311467964829724
EleutherAI/pythia-70m,mmlu_management,0-shot,accuracy,0.174757281553398,0.0376017800602662
EleutherAI/pythia-70m,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674
EleutherAI/pythia-70m,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-70m,mmlu_miscellaneous,0-shot,accuracy,0.2362707535121328,0.0151904737170375
EleutherAI/pythia-70m,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.0238051865248881
EleutherAI/pythia-70m,mmlu_professional_accounting,0-shot,accuracy,0.2340425531914893,0.0252578613594324
EleutherAI/pythia-70m,mmlu_professional_medicine,0-shot,accuracy,0.1875,0.0237097882538117
EleutherAI/pythia-70m,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
EleutherAI/pythia-70m,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
EleutherAI/pythia-70m,mmlu_high_school_geography,0-shot,accuracy,0.1767676767676767,0.0271787526390449
EleutherAI/pythia-70m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.1968911917098445,0.0286978739718606
EleutherAI/pythia-70m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2025641025641025,0.0203776609703713
EleutherAI/pythia-70m,mmlu_high_school_microeconomics,0-shot,accuracy,0.2058823529411764,0.0262650246082758
EleutherAI/pythia-70m,mmlu_high_school_psychology,0-shot,accuracy,0.1908256880733945,0.016847676400091
EleutherAI/pythia-70m,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/pythia-70m,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
EleutherAI/pythia-70m,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-70m,mmlu_security_studies,0-shot,accuracy,0.1877551020408163,0.0250002560395462
EleutherAI/pythia-70m,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/pythia-70m,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-70m,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-70m,mmlu_anatomy,0-shot,accuracy,0.2,0.0345547370232543
EleutherAI/pythia-70m,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/pythia-70m,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.0371617743756601
EleutherAI/pythia-70m,mmlu_college_chemistry,0-shot,accuracy,0.19,0.0394277244403662
EleutherAI/pythia-70m,mmlu_college_computer_science,0-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-70m,mmlu_college_mathematics,0-shot,accuracy,0.22,0.0416333199893227
EleutherAI/pythia-70m,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-70m,mmlu_computer_security,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-70m,mmlu_conceptual_physics,0-shot,accuracy,0.2638297872340425,0.0288099898541029
EleutherAI/pythia-70m,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.035659981741353
EleutherAI/pythia-70m,mmlu_elementary_mathematics,0-shot,accuracy,0.2063492063492063,0.0208422909301146
EleutherAI/pythia-70m,mmlu_high_school_biology,0-shot,accuracy,0.1806451612903225,0.0218861785671725
EleutherAI/pythia-70m,mmlu_high_school_chemistry,0-shot,accuracy,0.1527093596059113,0.0253089045393806
EleutherAI/pythia-70m,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-70m,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.024882116857655
EleutherAI/pythia-70m,mmlu_high_school_physics,0-shot,accuracy,0.1920529801324503,0.0321629842059361
EleutherAI/pythia-70m,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.0245363260261342
EleutherAI/pythia-70m,mmlu_machine_learning,0-shot,accuracy,0.3125,0.0439946505757152
EleutherAI/pythia-6.9b,mmlu_formal_logic,5-shot,accuracy,0.1587301587301587,0.0326845401301174
EleutherAI/pythia-6.9b,mmlu_high_school_european_history,5-shot,accuracy,0.2363636363636363,0.0331750593000917
EleutherAI/pythia-6.9b,mmlu_high_school_us_history,5-shot,accuracy,0.2647058823529412,0.0309645179269233
EleutherAI/pythia-6.9b,mmlu_high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598
EleutherAI/pythia-6.9b,mmlu_international_law,5-shot,accuracy,0.3801652892561983,0.0443132450196843
EleutherAI/pythia-6.9b,mmlu_jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
EleutherAI/pythia-6.9b,mmlu_logical_fallacies,5-shot,accuracy,0.2822085889570552,0.0353611788666474
EleutherAI/pythia-6.9b,mmlu_moral_disputes,5-shot,accuracy,0.2543352601156069,0.0234458262765455
EleutherAI/pythia-6.9b,mmlu_moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
EleutherAI/pythia-6.9b,mmlu_philosophy,5-shot,accuracy,0.2668810289389067,0.0251226376088166
EleutherAI/pythia-6.9b,mmlu_prehistory,5-shot,accuracy,0.25,0.0240934712326213
EleutherAI/pythia-6.9b,mmlu_professional_law,5-shot,accuracy,0.2340286831812255,0.0108135855526596
EleutherAI/pythia-6.9b,mmlu_world_religions,5-shot,accuracy,0.2982456140350877,0.0350877192982456
EleutherAI/pythia-6.9b,mmlu_business_ethics,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-6.9b,mmlu_clinical_knowledge,5-shot,accuracy,0.2528301886792453,0.0267498997712412
EleutherAI/pythia-6.9b,mmlu_college_medicine,5-shot,accuracy,0.1791907514450867,0.0292425130590632
EleutherAI/pythia-6.9b,mmlu_global_facts,5-shot,accuracy,0.32,0.046882617226215
EleutherAI/pythia-6.9b,mmlu_human_aging,5-shot,accuracy,0.3318385650224215,0.0316029514377667
EleutherAI/pythia-6.9b,mmlu_management,5-shot,accuracy,0.2718446601941747,0.0440526802414092
EleutherAI/pythia-6.9b,mmlu_marketing,5-shot,accuracy,0.2264957264957265,0.0274210072953929
EleutherAI/pythia-6.9b,mmlu_medical_genetics,5-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-6.9b,mmlu_miscellaneous,5-shot,accuracy,0.2835249042145594,0.0161173181668322
EleutherAI/pythia-6.9b,mmlu_nutrition,5-shot,accuracy,0.261437908496732,0.0251609982142924
EleutherAI/pythia-6.9b,mmlu_professional_accounting,5-shot,accuracy,0.301418439716312,0.0273741288826311
EleutherAI/pythia-6.9b,mmlu_professional_medicine,5-shot,accuracy,0.2794117647058823,0.0272572026061149
EleutherAI/pythia-6.9b,mmlu_virology,5-shot,accuracy,0.3132530120481928,0.0361080501803102
EleutherAI/pythia-6.9b,mmlu_econometrics,5-shot,accuracy,0.2719298245614035,0.0418577442402205
EleutherAI/pythia-6.9b,mmlu_high_school_geography,5-shot,accuracy,0.2525252525252525,0.0309540554703659
EleutherAI/pythia-6.9b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.238341968911917,0.0307489053639098
EleutherAI/pythia-6.9b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.3025641025641025,0.0232908880537727
EleutherAI/pythia-6.9b,mmlu_high_school_microeconomics,5-shot,accuracy,0.226890756302521,0.0272053715382794
EleutherAI/pythia-6.9b,mmlu_high_school_psychology,5-shot,accuracy,0.2513761467889908,0.0185992063602874
EleutherAI/pythia-6.9b,mmlu_human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
EleutherAI/pythia-6.9b,mmlu_professional_psychology,5-shot,accuracy,0.2647058823529412,0.0178480895749132
EleutherAI/pythia-6.9b,mmlu_public_relations,5-shot,accuracy,0.3818181818181818,0.046534298079135
EleutherAI/pythia-6.9b,mmlu_security_studies,5-shot,accuracy,0.1755102040816326,0.02435280072297
EleutherAI/pythia-6.9b,mmlu_sociology,5-shot,accuracy,0.2537313432835821,0.030769444967296
EleutherAI/pythia-6.9b,mmlu_us_foreign_policy,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-6.9b,mmlu_abstract_algebra,5-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-6.9b,mmlu_anatomy,5-shot,accuracy,0.3555555555555555,0.0413517674972038
EleutherAI/pythia-6.9b,mmlu_astronomy,5-shot,accuracy,0.2960526315789473,0.037150621549989
EleutherAI/pythia-6.9b,mmlu_college_biology,5-shot,accuracy,0.2777777777777778,0.0374555479146245
EleutherAI/pythia-6.9b,mmlu_college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-6.9b,mmlu_college_computer_science,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-6.9b,mmlu_college_mathematics,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-6.9b,mmlu_college_physics,5-shot,accuracy,0.2549019607843137,0.0433643270799317
EleutherAI/pythia-6.9b,mmlu_computer_security,5-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-6.9b,mmlu_conceptual_physics,5-shot,accuracy,0.3191489361702128,0.03047297336338
EleutherAI/pythia-6.9b,mmlu_electrical_engineering,5-shot,accuracy,0.2689655172413793,0.0369518331165023
EleutherAI/pythia-6.9b,mmlu_elementary_mathematics,5-shot,accuracy,0.291005291005291,0.0233938265004848
EleutherAI/pythia-6.9b,mmlu_high_school_biology,5-shot,accuracy,0.2548387096774193,0.0247901184593322
EleutherAI/pythia-6.9b,mmlu_high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.0305165307326944
EleutherAI/pythia-6.9b,mmlu_high_school_computer_science,5-shot,accuracy,0.31,0.0464823198711731
EleutherAI/pythia-6.9b,mmlu_high_school_mathematics,5-shot,accuracy,0.2296296296296296,0.0256441086392676
EleutherAI/pythia-6.9b,mmlu_high_school_physics,5-shot,accuracy,0.2516556291390728,0.0354330423438998
EleutherAI/pythia-6.9b,mmlu_high_school_statistics,5-shot,accuracy,0.3657407407407407,0.032847388576472
EleutherAI/pythia-6.9b,mmlu_machine_learning,5-shot,accuracy,0.2678571428571428,0.0420327729146776
EleutherAI/pythia-410m,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.0393253768039287
EleutherAI/pythia-410m,mmlu_high_school_european_history,0-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/pythia-410m,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.0305875913516042
EleutherAI/pythia-410m,mmlu_high_school_world_history,0-shot,accuracy,0.2573839662447257,0.0284588209914602
EleutherAI/pythia-410m,mmlu_international_law,0-shot,accuracy,0.2479338842975206,0.039418975265163
EleutherAI/pythia-410m,mmlu_jurisprudence,0-shot,accuracy,0.2685185185185185,0.0428446796805219
EleutherAI/pythia-410m,mmlu_logical_fallacies,0-shot,accuracy,0.2024539877300613,0.031570650789119
EleutherAI/pythia-410m,mmlu_moral_disputes,0-shot,accuracy,0.2398843930635838,0.0229895925431235
EleutherAI/pythia-410m,mmlu_moral_scenarios,0-shot,accuracy,0.2312849162011173,0.0141022236231525
EleutherAI/pythia-410m,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/pythia-410m,mmlu_prehistory,0-shot,accuracy,0.2129629629629629,0.0227797190887334
EleutherAI/pythia-410m,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.0110641510271654
EleutherAI/pythia-410m,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.0356507967070831
EleutherAI/pythia-410m,mmlu_business_ethics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-410m,mmlu_clinical_knowledge,0-shot,accuracy,0.2150943396226415,0.0252883945028913
EleutherAI/pythia-410m,mmlu_college_medicine,0-shot,accuracy,0.1965317919075144,0.0302995746647881
EleutherAI/pythia-410m,mmlu_global_facts,0-shot,accuracy,0.23,0.042295258468165
EleutherAI/pythia-410m,mmlu_human_aging,0-shot,accuracy,0.3139013452914798,0.0311467964829724
EleutherAI/pythia-410m,mmlu_management,0-shot,accuracy,0.1941747572815534,0.0391666776282258
EleutherAI/pythia-410m,mmlu_marketing,0-shot,accuracy,0.282051282051282,0.0294803605495411
EleutherAI/pythia-410m,mmlu_medical_genetics,0-shot,accuracy,0.29,0.0456048021572068
EleutherAI/pythia-410m,mmlu_miscellaneous,0-shot,accuracy,0.2426564495530012,0.0153298889408998
EleutherAI/pythia-410m,mmlu_nutrition,0-shot,accuracy,0.2254901960784313,0.0239291555173512
EleutherAI/pythia-410m,mmlu_professional_accounting,0-shot,accuracy,0.2659574468085106,0.0263580656988805
EleutherAI/pythia-410m,mmlu_professional_medicine,0-shot,accuracy,0.1875,0.0237097882538117
EleutherAI/pythia-410m,mmlu_virology,0-shot,accuracy,0.2771084337349397,0.0348433159268058
EleutherAI/pythia-410m,mmlu_econometrics,0-shot,accuracy,0.2368421052631578,0.0399942387928133
EleutherAI/pythia-410m,mmlu_high_school_geography,0-shot,accuracy,0.1919191919191919,0.028057791672989
EleutherAI/pythia-410m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.1968911917098445,0.0286978739718606
EleutherAI/pythia-410m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2102564102564102,0.0206605974850269
EleutherAI/pythia-410m,mmlu_high_school_microeconomics,0-shot,accuracy,0.2142857142857142,0.0266535315967154
EleutherAI/pythia-410m,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936
EleutherAI/pythia-410m,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
EleutherAI/pythia-410m,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.0176678416123789
EleutherAI/pythia-410m,mmlu_public_relations,0-shot,accuracy,0.209090909090909,0.0389509101572413
EleutherAI/pythia-410m,mmlu_security_studies,0-shot,accuracy,0.1877551020408163,0.0250002560395462
EleutherAI/pythia-410m,mmlu_sociology,0-shot,accuracy,0.2487562189054726,0.0305676759389167
EleutherAI/pythia-410m,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-410m,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-410m,mmlu_anatomy,0-shot,accuracy,0.1777777777777777,0.0330278985990171
EleutherAI/pythia-410m,mmlu_astronomy,0-shot,accuracy,0.1842105263157894,0.0315469804508223
EleutherAI/pythia-410m,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.0365394696944209
EleutherAI/pythia-410m,mmlu_college_chemistry,0-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-410m,mmlu_college_computer_science,0-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-410m,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-410m,mmlu_college_physics,0-shot,accuracy,0.2450980392156862,0.0428010583736439
EleutherAI/pythia-410m,mmlu_computer_security,0-shot,accuracy,0.24,0.0429234695990928
EleutherAI/pythia-410m,mmlu_conceptual_physics,0-shot,accuracy,0.2595744680851063,0.0286591793742923
EleutherAI/pythia-410m,mmlu_electrical_engineering,0-shot,accuracy,0.2275862068965517,0.0349395038013118
EleutherAI/pythia-410m,mmlu_elementary_mathematics,0-shot,accuracy,0.1851851851851851,0.0200060754945244
EleutherAI/pythia-410m,mmlu_high_school_biology,0-shot,accuracy,0.1838709677419354,0.0220372173402678
EleutherAI/pythia-410m,mmlu_high_school_chemistry,0-shot,accuracy,0.1576354679802955,0.0256390141311724
EleutherAI/pythia-410m,mmlu_high_school_computer_science,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-410m,mmlu_high_school_mathematics,0-shot,accuracy,0.2407407407407407,0.0260671592222757
EleutherAI/pythia-410m,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649,0.0347918557259966
EleutherAI/pythia-410m,mmlu_high_school_statistics,0-shot,accuracy,0.1759259259259259,0.0259674209582585
EleutherAI/pythia-410m,mmlu_machine_learning,0-shot,accuracy,0.3303571428571428,0.0446428571428571
EleutherAI/pile-t5-large,mmlu_formal_logic,5-shot,accuracy,0.2857142857142857,0.0404061017820884
EleutherAI/pile-t5-large,mmlu_high_school_european_history,5-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/pile-t5-large,mmlu_high_school_us_history,5-shot,accuracy,0.25,0.0303915336927415
EleutherAI/pile-t5-large,mmlu_high_school_world_history,5-shot,accuracy,0.270042194092827,0.0289007219062934
EleutherAI/pile-t5-large,mmlu_international_law,5-shot,accuracy,0.2396694214876033,0.0389687898507041
EleutherAI/pile-t5-large,mmlu_jurisprudence,5-shot,accuracy,0.2592592592592592,0.0423651125809463
EleutherAI/pile-t5-large,mmlu_logical_fallacies,5-shot,accuracy,0.2208588957055214,0.0325917739274217
EleutherAI/pile-t5-large,mmlu_moral_disputes,5-shot,accuracy,0.2485549132947976,0.0232675284321001
EleutherAI/pile-t5-large,mmlu_moral_scenarios,5-shot,accuracy,0.2379888268156424,0.0142426300705748
EleutherAI/pile-t5-large,mmlu_philosophy,5-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/pile-t5-large,mmlu_prehistory,5-shot,accuracy,0.2160493827160493,0.0228991629184458
EleutherAI/pile-t5-large,mmlu_professional_law,5-shot,accuracy,0.2457627118644068,0.0109961566351426
EleutherAI/pile-t5-large,mmlu_world_religions,5-shot,accuracy,0.3216374269005848,0.0358252944257312
EleutherAI/pile-t5-large,mmlu_business_ethics,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pile-t5-large,mmlu_clinical_knowledge,5-shot,accuracy,0.2150943396226415,0.0252883945028913
EleutherAI/pile-t5-large,mmlu_college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177498
EleutherAI/pile-t5-large,mmlu_global_facts,5-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pile-t5-large,mmlu_human_aging,5-shot,accuracy,0.3139013452914798,0.0311467964829724
EleutherAI/pile-t5-large,mmlu_management,5-shot,accuracy,0.174757281553398,0.0376017800602662
EleutherAI/pile-t5-large,mmlu_marketing,5-shot,accuracy,0.2905982905982906,0.029745048572674
EleutherAI/pile-t5-large,mmlu_medical_genetics,5-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pile-t5-large,mmlu_miscellaneous,5-shot,accuracy,0.2375478927203065,0.0152187330461501
EleutherAI/pile-t5-large,mmlu_nutrition,5-shot,accuracy,0.2254901960784313,0.0239291555173512
EleutherAI/pile-t5-large,mmlu_professional_accounting,5-shot,accuracy,0.2340425531914893,0.0252578613594324
EleutherAI/pile-t5-large,mmlu_professional_medicine,5-shot,accuracy,0.1838235294117647,0.0235292421851931
EleutherAI/pile-t5-large,mmlu_virology,5-shot,accuracy,0.2831325301204819,0.0350729543137051
EleutherAI/pile-t5-large,mmlu_econometrics,5-shot,accuracy,0.2368421052631578,0.0399942387928133
EleutherAI/pile-t5-large,mmlu_high_school_geography,5-shot,accuracy,0.1767676767676767,0.0271787526390449
EleutherAI/pile-t5-large,mmlu_high_school_government_and_politics,5-shot,accuracy,0.1968911917098445,0.0286978739718606
EleutherAI/pile-t5-large,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2025641025641025,0.0203776609703713
EleutherAI/pile-t5-large,mmlu_high_school_microeconomics,5-shot,accuracy,0.2100840336134453,0.0264613987174718
EleutherAI/pile-t5-large,mmlu_high_school_psychology,5-shot,accuracy,0.1926605504587156,0.016909276884936
EleutherAI/pile-t5-large,mmlu_human_sexuality,5-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/pile-t5-large,mmlu_professional_psychology,5-shot,accuracy,0.25,0.0175178188450144
EleutherAI/pile-t5-large,mmlu_public_relations,5-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pile-t5-large,mmlu_security_studies,5-shot,accuracy,0.1877551020408163,0.0250002560395462
EleutherAI/pile-t5-large,mmlu_sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/pile-t5-large,mmlu_us_foreign_policy,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pile-t5-large,mmlu_abstract_algebra,5-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pile-t5-large,mmlu_anatomy,5-shot,accuracy,0.1851851851851851,0.0335567721631314
EleutherAI/pile-t5-large,mmlu_astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/pile-t5-large,mmlu_college_biology,5-shot,accuracy,0.2569444444444444,0.0365394696944209
EleutherAI/pile-t5-large,mmlu_college_chemistry,5-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pile-t5-large,mmlu_college_computer_science,5-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pile-t5-large,mmlu_college_mathematics,5-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pile-t5-large,mmlu_college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/pile-t5-large,mmlu_computer_security,5-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pile-t5-large,mmlu_conceptual_physics,5-shot,accuracy,0.2638297872340425,0.0288099898541029
EleutherAI/pile-t5-large,mmlu_electrical_engineering,5-shot,accuracy,0.2413793103448276,0.035659981741353
EleutherAI/pile-t5-large,mmlu_elementary_mathematics,5-shot,accuracy,0.2089947089947089,0.0209404815653348
EleutherAI/pile-t5-large,mmlu_high_school_biology,5-shot,accuracy,0.1774193548387097,0.0217325406893292
EleutherAI/pile-t5-large,mmlu_high_school_chemistry,5-shot,accuracy,0.1527093596059113,0.0253089045393806
EleutherAI/pile-t5-large,mmlu_high_school_computer_science,5-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pile-t5-large,mmlu_high_school_mathematics,5-shot,accuracy,0.2111111111111111,0.024882116857655
EleutherAI/pile-t5-large,mmlu_high_school_physics,5-shot,accuracy,0.1986754966887417,0.0325784738443677
EleutherAI/pile-t5-large,mmlu_high_school_statistics,5-shot,accuracy,0.1527777777777778,0.0245363260261342
EleutherAI/pile-t5-large,mmlu_machine_learning,5-shot,accuracy,0.3125,0.0439946505757152
HuggingFaceTB/SmolLM-1.7B,mmlu_formal_logic,5-shot,accuracy,0.2222222222222222,0.0371848900681811
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_european_history,5-shot,accuracy,0.2424242424242424,0.0334640988105595
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_us_history,5-shot,accuracy,0.230392156862745,0.029554292605695
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_world_history,5-shot,accuracy,0.2658227848101265,0.0287567996296583
HuggingFaceTB/SmolLM-1.7B,mmlu_international_law,5-shot,accuracy,0.3636363636363636,0.0439132628672407
HuggingFaceTB/SmolLM-1.7B,mmlu_jurisprudence,5-shot,accuracy,0.3055555555555556,0.0445319750737498
HuggingFaceTB/SmolLM-1.7B,mmlu_logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
HuggingFaceTB/SmolLM-1.7B,mmlu_moral_disputes,5-shot,accuracy,0.3236994219653179,0.0251901813276084
HuggingFaceTB/SmolLM-1.7B,mmlu_moral_scenarios,5-shot,accuracy,0.2446927374301676,0.0143781698840984
HuggingFaceTB/SmolLM-1.7B,mmlu_philosophy,5-shot,accuracy,0.3697749196141479,0.0274179967056309
HuggingFaceTB/SmolLM-1.7B,mmlu_prehistory,5-shot,accuracy,0.3271604938271605,0.0261056738614098
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_law,5-shot,accuracy,0.2835723598435463,0.0115119007759683
HuggingFaceTB/SmolLM-1.7B,mmlu_world_religions,5-shot,accuracy,0.3450292397660818,0.036459813773888
HuggingFaceTB/SmolLM-1.7B,mmlu_business_ethics,5-shot,accuracy,0.32,0.046882617226215
HuggingFaceTB/SmolLM-1.7B,mmlu_clinical_knowledge,5-shot,accuracy,0.350943396226415,0.0293736462532346
HuggingFaceTB/SmolLM-1.7B,mmlu_college_medicine,5-shot,accuracy,0.2658959537572254,0.0336876293225943
HuggingFaceTB/SmolLM-1.7B,mmlu_global_facts,5-shot,accuracy,0.35,0.0479372485441102
HuggingFaceTB/SmolLM-1.7B,mmlu_human_aging,5-shot,accuracy,0.3318385650224215,0.0316029514377667
HuggingFaceTB/SmolLM-1.7B,mmlu_management,5-shot,accuracy,0.3009708737864077,0.0454160944650394
HuggingFaceTB/SmolLM-1.7B,mmlu_marketing,5-shot,accuracy,0.2863247863247863,0.0296143236904566
HuggingFaceTB/SmolLM-1.7B,mmlu_medical_genetics,5-shot,accuracy,0.32,0.046882617226215
HuggingFaceTB/SmolLM-1.7B,mmlu_miscellaneous,5-shot,accuracy,0.3320561941251596,0.0168411746552957
HuggingFaceTB/SmolLM-1.7B,mmlu_nutrition,5-shot,accuracy,0.3333333333333333,0.0269925443392972
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_accounting,5-shot,accuracy,0.2411347517730496,0.0255187310495377
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_medicine,5-shot,accuracy,0.3492647058823529,0.0289597551968248
HuggingFaceTB/SmolLM-1.7B,mmlu_virology,5-shot,accuracy,0.3795180722891566,0.0377779882274801
HuggingFaceTB/SmolLM-1.7B,mmlu_econometrics,5-shot,accuracy,0.2017543859649122,0.0377520501358363
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_geography,5-shot,accuracy,0.3484848484848485,0.033948539651564
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.2746113989637305,0.0322102450804115
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2897435897435897,0.0230006282436879
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_microeconomics,5-shot,accuracy,0.2605042016806723,0.0285102515123419
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_psychology,5-shot,accuracy,0.3045871559633027,0.019732299420354
HuggingFaceTB/SmolLM-1.7B,mmlu_human_sexuality,5-shot,accuracy,0.2977099236641221,0.040103589424622
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_psychology,5-shot,accuracy,0.2663398692810457,0.0178831881346672
HuggingFaceTB/SmolLM-1.7B,mmlu_public_relations,5-shot,accuracy,0.2545454545454545,0.0417234303870538
HuggingFaceTB/SmolLM-1.7B,mmlu_security_studies,5-shot,accuracy,0.363265306122449,0.0307890511390308
HuggingFaceTB/SmolLM-1.7B,mmlu_sociology,5-shot,accuracy,0.2885572139303483,0.0320384104021332
HuggingFaceTB/SmolLM-1.7B,mmlu_us_foreign_policy,5-shot,accuracy,0.39,0.0490207130000197
HuggingFaceTB/SmolLM-1.7B,mmlu_abstract_algebra,5-shot,accuracy,0.24,0.0429234695990928
HuggingFaceTB/SmolLM-1.7B,mmlu_anatomy,5-shot,accuracy,0.4148148148148148,0.042561937679014
HuggingFaceTB/SmolLM-1.7B,mmlu_astronomy,5-shot,accuracy,0.3618421052631579,0.0391052575284972
HuggingFaceTB/SmolLM-1.7B,mmlu_college_biology,5-shot,accuracy,0.3541666666666667,0.0399941113575354
HuggingFaceTB/SmolLM-1.7B,mmlu_college_chemistry,5-shot,accuracy,0.32,0.046882617226215
HuggingFaceTB/SmolLM-1.7B,mmlu_college_computer_science,5-shot,accuracy,0.34,0.0476095228569523
HuggingFaceTB/SmolLM-1.7B,mmlu_college_mathematics,5-shot,accuracy,0.27,0.0446196043338474
HuggingFaceTB/SmolLM-1.7B,mmlu_college_physics,5-shot,accuracy,0.2647058823529412,0.0438986995680878
HuggingFaceTB/SmolLM-1.7B,mmlu_computer_security,5-shot,accuracy,0.31,0.0464823198711731
HuggingFaceTB/SmolLM-1.7B,mmlu_conceptual_physics,5-shot,accuracy,0.2808510638297872,0.0293791704641248
HuggingFaceTB/SmolLM-1.7B,mmlu_electrical_engineering,5-shot,accuracy,0.2206896551724138,0.034559302019248
HuggingFaceTB/SmolLM-1.7B,mmlu_elementary_mathematics,5-shot,accuracy,0.2486772486772486,0.0222618176924001
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_biology,5-shot,accuracy,0.332258064516129,0.0267955608481227
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_chemistry,5-shot,accuracy,0.2413793103448276,0.0301083307180116
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_mathematics,5-shot,accuracy,0.2481481481481481,0.0263357394040558
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_physics,5-shot,accuracy,0.271523178807947,0.0363132980396965
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_statistics,5-shot,accuracy,0.3194444444444444,0.0317987634217685
HuggingFaceTB/SmolLM-1.7B,mmlu_machine_learning,5-shot,accuracy,0.3214285714285714,0.0443280405529151
HuggingFaceTB/SmolLM-360M,mmlu_formal_logic,5-shot,accuracy,0.1746031746031746,0.0339549002085611
HuggingFaceTB/SmolLM-360M,mmlu_high_school_european_history,5-shot,accuracy,0.2606060606060606,0.0342774317581652
HuggingFaceTB/SmolLM-360M,mmlu_high_school_us_history,5-shot,accuracy,0.196078431372549,0.0278659422866393
HuggingFaceTB/SmolLM-360M,mmlu_high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598
HuggingFaceTB/SmolLM-360M,mmlu_international_law,5-shot,accuracy,0.3553719008264462,0.0436923632657398
HuggingFaceTB/SmolLM-360M,mmlu_jurisprudence,5-shot,accuracy,0.2314814814814814,0.0407749470925262
HuggingFaceTB/SmolLM-360M,mmlu_logical_fallacies,5-shot,accuracy,0.294478527607362,0.0358116579047408
HuggingFaceTB/SmolLM-360M,mmlu_moral_disputes,5-shot,accuracy,0.2861271676300578,0.0243321467791341
HuggingFaceTB/SmolLM-360M,mmlu_moral_scenarios,5-shot,accuracy,0.2469273743016759,0.0144222922048088
HuggingFaceTB/SmolLM-360M,mmlu_philosophy,5-shot,accuracy,0.2958199356913183,0.0259223717888187
HuggingFaceTB/SmolLM-360M,mmlu_prehistory,5-shot,accuracy,0.2839506172839506,0.0250894785237651
HuggingFaceTB/SmolLM-360M,mmlu_professional_law,5-shot,accuracy,0.2385919165580182,0.0108859297420022
HuggingFaceTB/SmolLM-360M,mmlu_world_religions,5-shot,accuracy,0.2748538011695906,0.0342404292469158
HuggingFaceTB/SmolLM-360M,mmlu_business_ethics,5-shot,accuracy,0.25,0.0435194139889244
HuggingFaceTB/SmolLM-360M,mmlu_clinical_knowledge,5-shot,accuracy,0.2452830188679245,0.0264803571798956
HuggingFaceTB/SmolLM-360M,mmlu_college_medicine,5-shot,accuracy,0.2080924855491329,0.0309528902177498
HuggingFaceTB/SmolLM-360M,mmlu_global_facts,5-shot,accuracy,0.33,0.047258156262526
HuggingFaceTB/SmolLM-360M,mmlu_human_aging,5-shot,accuracy,0.3318385650224215,0.0316029514377667
HuggingFaceTB/SmolLM-360M,mmlu_management,5-shot,accuracy,0.1650485436893204,0.0367566883223318
HuggingFaceTB/SmolLM-360M,mmlu_marketing,5-shot,accuracy,0.2222222222222222,0.0272360139461967
HuggingFaceTB/SmolLM-360M,mmlu_medical_genetics,5-shot,accuracy,0.25,0.0435194139889244
HuggingFaceTB/SmolLM-360M,mmlu_miscellaneous,5-shot,accuracy,0.2784163473818646,0.0160282951889924
HuggingFaceTB/SmolLM-360M,mmlu_nutrition,5-shot,accuracy,0.2450980392156862,0.0246300489798247
HuggingFaceTB/SmolLM-360M,mmlu_professional_accounting,5-shot,accuracy,0.2588652482269503,0.0261295725271808
HuggingFaceTB/SmolLM-360M,mmlu_professional_medicine,5-shot,accuracy,0.3897058823529412,0.0296246635811596
HuggingFaceTB/SmolLM-360M,mmlu_virology,5-shot,accuracy,0.3192771084337349,0.0362933532994786
HuggingFaceTB/SmolLM-360M,mmlu_econometrics,5-shot,accuracy,0.219298245614035,0.0389243110651875
HuggingFaceTB/SmolLM-360M,mmlu_high_school_geography,5-shot,accuracy,0.2373737373737373,0.0303137105381988
HuggingFaceTB/SmolLM-360M,mmlu_high_school_government_and_politics,5-shot,accuracy,0.233160621761658,0.030516111371476
HuggingFaceTB/SmolLM-360M,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2282051282051282,0.0212783938635862
HuggingFaceTB/SmolLM-360M,mmlu_high_school_microeconomics,5-shot,accuracy,0.2352941176470588,0.0275536144678638
HuggingFaceTB/SmolLM-360M,mmlu_high_school_psychology,5-shot,accuracy,0.2385321100917431,0.0182725758102318
HuggingFaceTB/SmolLM-360M,mmlu_human_sexuality,5-shot,accuracy,0.2290076335877862,0.0368534663171185
HuggingFaceTB/SmolLM-360M,mmlu_professional_psychology,5-shot,accuracy,0.2401960784313725,0.0172827606951674
HuggingFaceTB/SmolLM-360M,mmlu_public_relations,5-shot,accuracy,0.2909090909090909,0.0435027144292324
HuggingFaceTB/SmolLM-360M,mmlu_security_studies,5-shot,accuracy,0.2163265306122449,0.026358916334904
HuggingFaceTB/SmolLM-360M,mmlu_sociology,5-shot,accuracy,0.2437810945273631,0.0303604901540146
HuggingFaceTB/SmolLM-360M,mmlu_us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276807
HuggingFaceTB/SmolLM-360M,mmlu_abstract_algebra,5-shot,accuracy,0.33,0.047258156262526
HuggingFaceTB/SmolLM-360M,mmlu_anatomy,5-shot,accuracy,0.3481481481481481,0.0411532461033695
HuggingFaceTB/SmolLM-360M,mmlu_astronomy,5-shot,accuracy,0.2171052631578947,0.0335504530488292
HuggingFaceTB/SmolLM-360M,mmlu_college_biology,5-shot,accuracy,0.2569444444444444,0.0365394696944209
HuggingFaceTB/SmolLM-360M,mmlu_college_chemistry,5-shot,accuracy,0.15,0.0358870281282636
HuggingFaceTB/SmolLM-360M,mmlu_college_computer_science,5-shot,accuracy,0.3,0.0460566186471838
HuggingFaceTB/SmolLM-360M,mmlu_college_mathematics,5-shot,accuracy,0.27,0.0446196043338474
HuggingFaceTB/SmolLM-360M,mmlu_college_physics,5-shot,accuracy,0.2058823529411764,0.0402338227361774
HuggingFaceTB/SmolLM-360M,mmlu_computer_security,5-shot,accuracy,0.24,0.0429234695990928
HuggingFaceTB/SmolLM-360M,mmlu_conceptual_physics,5-shot,accuracy,0.3276595744680851,0.0306830208432309
HuggingFaceTB/SmolLM-360M,mmlu_electrical_engineering,5-shot,accuracy,0.2827586206896552,0.0375283395800333
HuggingFaceTB/SmolLM-360M,mmlu_elementary_mathematics,5-shot,accuracy,0.2592592592592592,0.0225698970749184
HuggingFaceTB/SmolLM-360M,mmlu_high_school_biology,5-shot,accuracy,0.2129032258064516,0.0232876651272685
HuggingFaceTB/SmolLM-360M,mmlu_high_school_chemistry,5-shot,accuracy,0.2758620689655172,0.0314471258167824
HuggingFaceTB/SmolLM-360M,mmlu_high_school_computer_science,5-shot,accuracy,0.34,0.0476095228569523
HuggingFaceTB/SmolLM-360M,mmlu_high_school_mathematics,5-shot,accuracy,0.2666666666666666,0.0269624243250738
HuggingFaceTB/SmolLM-360M,mmlu_high_school_physics,5-shot,accuracy,0.2185430463576159,0.0337423555042569
HuggingFaceTB/SmolLM-360M,mmlu_high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
HuggingFaceTB/SmolLM-360M,mmlu_machine_learning,5-shot,accuracy,0.2053571428571428,0.0383424102141907
EleutherAI/pythia-410m-deduped,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.0393253768039287
EleutherAI/pythia-410m-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/pythia-410m-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.0305875913516042
EleutherAI/pythia-410m-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
EleutherAI/pythia-410m-deduped,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.0389687898507041
EleutherAI/pythia-410m-deduped,mmlu_jurisprudence,0-shot,accuracy,0.2592592592592592,0.0423651125809463
EleutherAI/pythia-410m-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.2269938650306748,0.0329109957861576
EleutherAI/pythia-410m-deduped,mmlu_moral_disputes,0-shot,accuracy,0.2485549132947976,0.0232675284321001
EleutherAI/pythia-410m-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705748
EleutherAI/pythia-410m-deduped,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/pythia-410m-deduped,mmlu_prehistory,0-shot,accuracy,0.2160493827160493,0.0228991629184458
EleutherAI/pythia-410m-deduped,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.0110641510271654
EleutherAI/pythia-410m-deduped,mmlu_world_religions,0-shot,accuracy,0.327485380116959,0.0359933577145602
EleutherAI/pythia-410m-deduped,mmlu_business_ethics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-410m-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.2226415094339622,0.025604233470899
EleutherAI/pythia-410m-deduped,mmlu_college_medicine,0-shot,accuracy,0.2312138728323699,0.0321473730202946
EleutherAI/pythia-410m-deduped,mmlu_global_facts,0-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-410m-deduped,mmlu_human_aging,0-shot,accuracy,0.3049327354260089,0.0308986108824775
EleutherAI/pythia-410m-deduped,mmlu_management,0-shot,accuracy,0.1941747572815534,0.0391666776282258
EleutherAI/pythia-410m-deduped,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674
EleutherAI/pythia-410m-deduped,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-410m-deduped,mmlu_miscellaneous,0-shot,accuracy,0.2388250319284802,0.0152468031973986
EleutherAI/pythia-410m-deduped,mmlu_nutrition,0-shot,accuracy,0.2418300653594771,0.0245181956418793
EleutherAI/pythia-410m-deduped,mmlu_professional_accounting,0-shot,accuracy,0.2446808510638297,0.0256455536222667
EleutherAI/pythia-410m-deduped,mmlu_professional_medicine,0-shot,accuracy,0.1838235294117647,0.0235292421851931
EleutherAI/pythia-410m-deduped,mmlu_virology,0-shot,accuracy,0.2710843373493976,0.0346057990755302
EleutherAI/pythia-410m-deduped,mmlu_econometrics,0-shot,accuracy,0.219298245614035,0.0389243110651875
EleutherAI/pythia-410m-deduped,mmlu_high_school_geography,0-shot,accuracy,0.1717171717171717,0.0268697161874299
EleutherAI/pythia-410m-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.1968911917098445,0.0286978739718606
EleutherAI/pythia-410m-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2,0.0202808050625357
EleutherAI/pythia-410m-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.2142857142857142,0.0266535315967154
EleutherAI/pythia-410m-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.1963302752293578,0.0170307193391543
EleutherAI/pythia-410m-deduped,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
EleutherAI/pythia-410m-deduped,mmlu_professional_psychology,0-shot,accuracy,0.25,0.0175178188450144
EleutherAI/pythia-410m-deduped,mmlu_public_relations,0-shot,accuracy,0.2,0.038313051408846
EleutherAI/pythia-410m-deduped,mmlu_security_studies,0-shot,accuracy,0.1836734693877551,0.0247890713320076
EleutherAI/pythia-410m-deduped,mmlu_sociology,0-shot,accuracy,0.2437810945273631,0.0303604901540146
EleutherAI/pythia-410m-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-410m-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-410m-deduped,mmlu_anatomy,0-shot,accuracy,0.1851851851851851,0.0335567721631314
EleutherAI/pythia-410m-deduped,mmlu_astronomy,0-shot,accuracy,0.2105263157894736,0.0331767278753315
EleutherAI/pythia-410m-deduped,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.0368565109589753
EleutherAI/pythia-410m-deduped,mmlu_college_chemistry,0-shot,accuracy,0.16,0.036845294917747
EleutherAI/pythia-410m-deduped,mmlu_college_computer_science,0-shot,accuracy,0.27,0.0446196043338473
EleutherAI/pythia-410m-deduped,mmlu_college_mathematics,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-410m-deduped,mmlu_college_physics,0-shot,accuracy,0.2745098039215686,0.0444052190617932
EleutherAI/pythia-410m-deduped,mmlu_computer_security,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-410m-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2638297872340425,0.0288099898541029
EleutherAI/pythia-410m-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.035659981741353
EleutherAI/pythia-410m-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.0217659616721545
EleutherAI/pythia-410m-deduped,mmlu_high_school_biology,0-shot,accuracy,0.1806451612903225,0.0218861785671725
EleutherAI/pythia-410m-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.1724137931034483,0.0265776721830365
EleutherAI/pythia-410m-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-410m-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.2185185185185185,0.0251957522518237
EleutherAI/pythia-410m-deduped,mmlu_high_school_physics,0-shot,accuracy,0.2317880794701986,0.0344540627198705
EleutherAI/pythia-410m-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.1712962962962963,0.0256953416438246
EleutherAI/pythia-410m-deduped,mmlu_machine_learning,0-shot,accuracy,0.3125,0.0439946505757152
EleutherAI/pythia-160m-deduped,mmlu_formal_logic,0-shot,accuracy,0.2936507936507936,0.0407352432214712
EleutherAI/pythia-160m-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.2181818181818181,0.0322507810830628
EleutherAI/pythia-160m-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.0303915336927415
EleutherAI/pythia-160m-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.0289007219062934
EleutherAI/pythia-160m-deduped,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.0389687898507041
EleutherAI/pythia-160m-deduped,mmlu_jurisprudence,0-shot,accuracy,0.2592592592592592,0.0423651125809463
EleutherAI/pythia-160m-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.2208588957055214,0.0325917739274217
EleutherAI/pythia-160m-deduped,mmlu_moral_disputes,0-shot,accuracy,0.2485549132947976,0.0232675284321001
EleutherAI/pythia-160m-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.2379888268156424,0.0142426300705748
EleutherAI/pythia-160m-deduped,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.0221224397724807
EleutherAI/pythia-160m-deduped,mmlu_prehistory,0-shot,accuracy,0.2160493827160493,0.0228991629184458
EleutherAI/pythia-160m-deduped,mmlu_professional_law,0-shot,accuracy,0.2451108213820078,0.0109863078700455
EleutherAI/pythia-160m-deduped,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.0358252944257312
EleutherAI/pythia-160m-deduped,mmlu_business_ethics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-160m-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.2150943396226415,0.0252883945028913
EleutherAI/pythia-160m-deduped,mmlu_college_medicine,0-shot,accuracy,0.2080924855491329,0.0309528902177498
EleutherAI/pythia-160m-deduped,mmlu_global_facts,0-shot,accuracy,0.18,0.0386122919665369
EleutherAI/pythia-160m-deduped,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.0310244117405722
EleutherAI/pythia-160m-deduped,mmlu_management,0-shot,accuracy,0.174757281553398,0.0376017800602662
EleutherAI/pythia-160m-deduped,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674
EleutherAI/pythia-160m-deduped,mmlu_medical_genetics,0-shot,accuracy,0.3,0.0460566186471838
EleutherAI/pythia-160m-deduped,mmlu_miscellaneous,0-shot,accuracy,0.2349936143039591,0.0151620241522784
EleutherAI/pythia-160m-deduped,mmlu_nutrition,0-shot,accuracy,0.218954248366013,0.0236790898618077
EleutherAI/pythia-160m-deduped,mmlu_professional_accounting,0-shot,accuracy,0.2340425531914893,0.0252578613594324
EleutherAI/pythia-160m-deduped,mmlu_professional_medicine,0-shot,accuracy,0.1948529411764706,0.0240605994234874
EleutherAI/pythia-160m-deduped,mmlu_virology,0-shot,accuracy,0.2831325301204819,0.0350729543137051
EleutherAI/pythia-160m-deduped,mmlu_econometrics,0-shot,accuracy,0.219298245614035,0.0389243110651875
EleutherAI/pythia-160m-deduped,mmlu_high_school_geography,0-shot,accuracy,0.1717171717171717,0.0268697161874299
EleutherAI/pythia-160m-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.1968911917098445,0.0286978739718606
EleutherAI/pythia-160m-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2025641025641025,0.0203776609703713
EleutherAI/pythia-160m-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.2100840336134453,0.0264613987174718
EleutherAI/pythia-160m-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.1908256880733945,0.016847676400091
EleutherAI/pythia-160m-deduped,mmlu_human_sexuality,0-shot,accuracy,0.2519083969465648,0.0380738711630608
EleutherAI/pythia-160m-deduped,mmlu_professional_psychology,0-shot,accuracy,0.2516339869281045,0.0175558180913222
EleutherAI/pythia-160m-deduped,mmlu_public_relations,0-shot,accuracy,0.2181818181818181,0.0395593286179583
EleutherAI/pythia-160m-deduped,mmlu_security_studies,0-shot,accuracy,0.1877551020408163,0.0250002560395462
EleutherAI/pythia-160m-deduped,mmlu_sociology,0-shot,accuracy,0.2487562189054726,0.0305676759389167
EleutherAI/pythia-160m-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.0451260859854212
EleutherAI/pythia-160m-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.0416333199893226
EleutherAI/pythia-160m-deduped,mmlu_anatomy,0-shot,accuracy,0.1925925925925926,0.0340654205850265
EleutherAI/pythia-160m-deduped,mmlu_astronomy,0-shot,accuracy,0.1776315789473684,0.0311031823831233
EleutherAI/pythia-160m-deduped,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.0368565109589753
EleutherAI/pythia-160m-deduped,mmlu_college_chemistry,0-shot,accuracy,0.2,0.0402015126103684
EleutherAI/pythia-160m-deduped,mmlu_college_computer_science,0-shot,accuracy,0.25,0.0435194139889244
EleutherAI/pythia-160m-deduped,mmlu_college_mathematics,0-shot,accuracy,0.21,0.0409360180740332
EleutherAI/pythia-160m-deduped,mmlu_college_physics,0-shot,accuracy,0.2156862745098039,0.0409256395823765
EleutherAI/pythia-160m-deduped,mmlu_computer_security,0-shot,accuracy,0.27,0.0446196043338473
EleutherAI/pythia-160m-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2638297872340425,0.0288099898541029
EleutherAI/pythia-160m-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.035659981741353
EleutherAI/pythia-160m-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.2089947089947089,0.0209404815653348
EleutherAI/pythia-160m-deduped,mmlu_high_school_biology,0-shot,accuracy,0.1741935483870967,0.0215762481845145
EleutherAI/pythia-160m-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.1625615763546798,0.0259603000646055
EleutherAI/pythia-160m-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276807
EleutherAI/pythia-160m-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.024882116857655
EleutherAI/pythia-160m-deduped,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.0325784738443677
EleutherAI/pythia-160m-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.0245363260261342
EleutherAI/pythia-160m-deduped,mmlu_machine_learning,0-shot,accuracy,0.3035714285714285,0.0436422615584104
aisingapore/sea-lion-7b,mmlu_formal_logic,5-shot,accuracy,0.1984126984126984,0.0356701667527686
aisingapore/sea-lion-7b,mmlu_high_school_european_history,5-shot,accuracy,0.2666666666666666,0.0345313180188541
aisingapore/sea-lion-7b,mmlu_high_school_us_history,5-shot,accuracy,0.2598039215686274,0.0307785546786932
aisingapore/sea-lion-7b,mmlu_high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598
aisingapore/sea-lion-7b,mmlu_international_law,5-shot,accuracy,0.2231404958677686,0.0380075447522873
aisingapore/sea-lion-7b,mmlu_jurisprudence,5-shot,accuracy,0.2777777777777778,0.0433004374965074
aisingapore/sea-lion-7b,mmlu_logical_fallacies,5-shot,accuracy,0.2515337423312883,0.0340899788685752
aisingapore/sea-lion-7b,mmlu_moral_disputes,5-shot,accuracy,0.291907514450867,0.0244769940762473
aisingapore/sea-lion-7b,mmlu_moral_scenarios,5-shot,accuracy,0.2726256983240223,0.0148933917352496
aisingapore/sea-lion-7b,mmlu_philosophy,5-shot,accuracy,0.3086816720257235,0.0262369658811532
aisingapore/sea-lion-7b,mmlu_prehistory,5-shot,accuracy,0.2716049382716049,0.0247486244905373
aisingapore/sea-lion-7b,mmlu_professional_law,5-shot,accuracy,0.2803129074315515,0.0114715559449586
aisingapore/sea-lion-7b,mmlu_world_religions,5-shot,accuracy,0.2923976608187134,0.0348864771345792
aisingapore/sea-lion-7b,mmlu_business_ethics,5-shot,accuracy,0.3,0.0460566186471838
aisingapore/sea-lion-7b,mmlu_clinical_knowledge,5-shot,accuracy,0.2943396226415094,0.0280491863156952
aisingapore/sea-lion-7b,mmlu_college_medicine,5-shot,accuracy,0.3583815028901734,0.0365634365335315
aisingapore/sea-lion-7b,mmlu_global_facts,5-shot,accuracy,0.27,0.0446196043338473
aisingapore/sea-lion-7b,mmlu_human_aging,5-shot,accuracy,0.2062780269058296,0.0271571504795638
aisingapore/sea-lion-7b,mmlu_management,5-shot,accuracy,0.233009708737864,0.0418583259892831
aisingapore/sea-lion-7b,mmlu_marketing,5-shot,accuracy,0.2606837606837607,0.0287603489565234
aisingapore/sea-lion-7b,mmlu_medical_genetics,5-shot,accuracy,0.21,0.0409360180740332
aisingapore/sea-lion-7b,mmlu_miscellaneous,5-shot,accuracy,0.2592592592592592,0.0156710060093395
aisingapore/sea-lion-7b,mmlu_nutrition,5-shot,accuracy,0.2810457516339869,0.0257388547978187
aisingapore/sea-lion-7b,mmlu_professional_accounting,5-shot,accuracy,0.2765957446808511,0.0266845643404609
aisingapore/sea-lion-7b,mmlu_professional_medicine,5-shot,accuracy,0.2058823529411764,0.0245622043141423
aisingapore/sea-lion-7b,mmlu_virology,5-shot,accuracy,0.2710843373493976,0.0346057990755302
aisingapore/sea-lion-7b,mmlu_econometrics,5-shot,accuracy,0.2982456140350877,0.0430368403353731
aisingapore/sea-lion-7b,mmlu_high_school_geography,5-shot,accuracy,0.3434343434343434,0.0338320122324444
aisingapore/sea-lion-7b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.2901554404145077,0.0327526446779151
aisingapore/sea-lion-7b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2794871794871795,0.0227523888397768
aisingapore/sea-lion-7b,mmlu_high_school_microeconomics,5-shot,accuracy,0.2352941176470588,0.0275536144678637
aisingapore/sea-lion-7b,mmlu_high_school_psychology,5-shot,accuracy,0.291743119266055,0.0194893009688765
aisingapore/sea-lion-7b,mmlu_human_sexuality,5-shot,accuracy,0.2824427480916031,0.0394840612576836
aisingapore/sea-lion-7b,mmlu_professional_psychology,5-shot,accuracy,0.2598039215686274,0.0177408995091777
aisingapore/sea-lion-7b,mmlu_public_relations,5-shot,accuracy,0.2818181818181818,0.0430911870994646
aisingapore/sea-lion-7b,mmlu_security_studies,5-shot,accuracy,0.4326530612244898,0.0317175282406266
aisingapore/sea-lion-7b,mmlu_sociology,5-shot,accuracy,0.3034825870646766,0.0325100681645861
aisingapore/sea-lion-7b,mmlu_us_foreign_policy,5-shot,accuracy,0.3,0.0460566186471838
aisingapore/sea-lion-7b,mmlu_abstract_algebra,5-shot,accuracy,0.25,0.0435194139889244
aisingapore/sea-lion-7b,mmlu_anatomy,5-shot,accuracy,0.2296296296296296,0.0363338441407346
aisingapore/sea-lion-7b,mmlu_astronomy,5-shot,accuracy,0.2697368421052631,0.0361178056028489
aisingapore/sea-lion-7b,mmlu_college_biology,5-shot,accuracy,0.3055555555555556,0.0385208469600853
aisingapore/sea-lion-7b,mmlu_college_chemistry,5-shot,accuracy,0.18,0.0386122919665369
aisingapore/sea-lion-7b,mmlu_college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
aisingapore/sea-lion-7b,mmlu_college_mathematics,5-shot,accuracy,0.24,0.0429234695990928
aisingapore/sea-lion-7b,mmlu_college_physics,5-shot,accuracy,0.1666666666666666,0.0370828466241654
aisingapore/sea-lion-7b,mmlu_computer_security,5-shot,accuracy,0.3,0.0460566186471838
aisingapore/sea-lion-7b,mmlu_conceptual_physics,5-shot,accuracy,0.2595744680851063,0.0286591793742923
aisingapore/sea-lion-7b,mmlu_electrical_engineering,5-shot,accuracy,0.296551724137931,0.0380614268730999
aisingapore/sea-lion-7b,mmlu_elementary_mathematics,5-shot,accuracy,0.246031746031746,0.0221820372029483
aisingapore/sea-lion-7b,mmlu_high_school_biology,5-shot,accuracy,0.2483870967741935,0.024580028921481
aisingapore/sea-lion-7b,mmlu_high_school_chemistry,5-shot,accuracy,0.2315270935960591,0.0296783331414444
aisingapore/sea-lion-7b,mmlu_high_school_computer_science,5-shot,accuracy,0.3,0.0460566186471838
aisingapore/sea-lion-7b,mmlu_high_school_mathematics,5-shot,accuracy,0.2333333333333333,0.0257878742209593
aisingapore/sea-lion-7b,mmlu_high_school_physics,5-shot,accuracy,0.2251655629139073,0.0341043528200893
aisingapore/sea-lion-7b,mmlu_high_school_statistics,5-shot,accuracy,0.2453703703703703,0.0293466650943729
aisingapore/sea-lion-7b,mmlu_machine_learning,5-shot,accuracy,0.2589285714285714,0.0415775153986562
facebook/xglm-2.9B,mmlu_formal_logic,5-shot,accuracy,0.1666666666666666,0.0333333333333333
facebook/xglm-2.9B,mmlu_high_school_european_history,5-shot,accuracy,0.206060606060606,0.031584153240477
facebook/xglm-2.9B,mmlu_high_school_us_history,5-shot,accuracy,0.2401960784313725,0.0299837330559136
facebook/xglm-2.9B,mmlu_high_school_world_history,5-shot,accuracy,0.2531645569620253,0.0283046579430352
facebook/xglm-2.9B,mmlu_international_law,5-shot,accuracy,0.3636363636363636,0.0439132628672407
facebook/xglm-2.9B,mmlu_jurisprudence,5-shot,accuracy,0.2222222222222222,0.0401910747255734
facebook/xglm-2.9B,mmlu_logical_fallacies,5-shot,accuracy,0.263803680981595,0.0346241993161562
facebook/xglm-2.9B,mmlu_moral_disputes,5-shot,accuracy,0.1994219653179191,0.0215119006542525
facebook/xglm-2.9B,mmlu_moral_scenarios,5-shot,accuracy,0.2424581005586592,0.0143335220592178
facebook/xglm-2.9B,mmlu_philosophy,5-shot,accuracy,0.1929260450160771,0.0224115167809113
facebook/xglm-2.9B,mmlu_prehistory,5-shot,accuracy,0.2654320987654321,0.0245692236004608
facebook/xglm-2.9B,mmlu_professional_law,5-shot,accuracy,0.2451108213820078,0.0109863078700455
facebook/xglm-2.9B,mmlu_world_religions,5-shot,accuracy,0.2690058479532163,0.0340105262010409
facebook/xglm-2.9B,mmlu_business_ethics,5-shot,accuracy,0.18,0.0386122919665369
facebook/xglm-2.9B,mmlu_clinical_knowledge,5-shot,accuracy,0.2188679245283019,0.0254478638251086
facebook/xglm-2.9B,mmlu_college_medicine,5-shot,accuracy,0.2369942196531791,0.0324241475748309
facebook/xglm-2.9B,mmlu_global_facts,5-shot,accuracy,0.18,0.0386122919665369
facebook/xglm-2.9B,mmlu_human_aging,5-shot,accuracy,0.2331838565022421,0.0283803911470947
facebook/xglm-2.9B,mmlu_management,5-shot,accuracy,0.1844660194174757,0.0384042362728827
facebook/xglm-2.9B,mmlu_marketing,5-shot,accuracy,0.1666666666666666,0.0244149473045436
facebook/xglm-2.9B,mmlu_medical_genetics,5-shot,accuracy,0.3,0.0460566186471838
facebook/xglm-2.9B,mmlu_miscellaneous,5-shot,accuracy,0.2720306513409962,0.0159133674475005
facebook/xglm-2.9B,mmlu_nutrition,5-shot,accuracy,0.2418300653594771,0.0245181956418793
facebook/xglm-2.9B,mmlu_professional_accounting,5-shot,accuracy,0.2340425531914893,0.0252578613594323
facebook/xglm-2.9B,mmlu_professional_medicine,5-shot,accuracy,0.3198529411764705,0.0283329595140312
facebook/xglm-2.9B,mmlu_virology,5-shot,accuracy,0.2409638554216867,0.0332939411907352
facebook/xglm-2.9B,mmlu_econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984366
facebook/xglm-2.9B,mmlu_high_school_geography,5-shot,accuracy,0.2525252525252525,0.0309540554703659
facebook/xglm-2.9B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.3678756476683937,0.0348017566846603
facebook/xglm-2.9B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2769230769230769,0.0226880423524249
facebook/xglm-2.9B,mmlu_high_school_microeconomics,5-shot,accuracy,0.3235294117647059,0.0303883535518868
facebook/xglm-2.9B,mmlu_high_school_psychology,5-shot,accuracy,0.2220183486238532,0.0178188495647966
facebook/xglm-2.9B,mmlu_human_sexuality,5-shot,accuracy,0.2442748091603053,0.0376833595972874
facebook/xglm-2.9B,mmlu_professional_psychology,5-shot,accuracy,0.2450980392156862,0.0174018167114276
facebook/xglm-2.9B,mmlu_public_relations,5-shot,accuracy,0.2454545454545454,0.0412206650287828
facebook/xglm-2.9B,mmlu_security_studies,5-shot,accuracy,0.2163265306122449,0.026358916334904
facebook/xglm-2.9B,mmlu_sociology,5-shot,accuracy,0.1990049751243781,0.0282313650927584
facebook/xglm-2.9B,mmlu_us_foreign_policy,5-shot,accuracy,0.27,0.0446196043338473
facebook/xglm-2.9B,mmlu_abstract_algebra,5-shot,accuracy,0.27,0.0446196043338473
facebook/xglm-2.9B,mmlu_anatomy,5-shot,accuracy,0.3185185185185185,0.0402477840197711
facebook/xglm-2.9B,mmlu_astronomy,5-shot,accuracy,0.1776315789473684,0.0311031823831233
facebook/xglm-2.9B,mmlu_college_biology,5-shot,accuracy,0.2847222222222222,0.0377380999068693
facebook/xglm-2.9B,mmlu_college_chemistry,5-shot,accuracy,0.23,0.042295258468165
facebook/xglm-2.9B,mmlu_college_computer_science,5-shot,accuracy,0.26,0.0440844002276808
facebook/xglm-2.9B,mmlu_college_mathematics,5-shot,accuracy,0.24,0.0429234695990928
facebook/xglm-2.9B,mmlu_college_physics,5-shot,accuracy,0.2450980392156862,0.0428010583736439
facebook/xglm-2.9B,mmlu_computer_security,5-shot,accuracy,0.15,0.0358870281282637
facebook/xglm-2.9B,mmlu_conceptual_physics,5-shot,accuracy,0.2297872340425532,0.0275017529444124
facebook/xglm-2.9B,mmlu_electrical_engineering,5-shot,accuracy,0.2758620689655172,0.0372456361977463
facebook/xglm-2.9B,mmlu_elementary_mathematics,5-shot,accuracy,0.2698412698412698,0.022860838309232
facebook/xglm-2.9B,mmlu_high_school_biology,5-shot,accuracy,0.3129032258064516,0.0263775670286458
facebook/xglm-2.9B,mmlu_high_school_chemistry,5-shot,accuracy,0.3103448275862069,0.032550867699701
facebook/xglm-2.9B,mmlu_high_school_computer_science,5-shot,accuracy,0.32,0.046882617226215
facebook/xglm-2.9B,mmlu_high_school_mathematics,5-shot,accuracy,0.2925925925925925,0.027738969632176
facebook/xglm-2.9B,mmlu_high_school_physics,5-shot,accuracy,0.2913907284768212,0.0371018572611999
facebook/xglm-2.9B,mmlu_high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/xglm-2.9B,mmlu_machine_learning,5-shot,accuracy,0.3035714285714285,0.0436422615584104
google/gemma-7b,mmlu_formal_logic,5-shot,accuracy,0.5238095238095238,0.0446706262840327
google/gemma-7b,mmlu_high_school_european_history,5-shot,accuracy,0.7515151515151515,0.033744026441394
google/gemma-7b,mmlu_high_school_us_history,5-shot,accuracy,0.7794117647058824,0.029102254389674
google/gemma-7b,mmlu_high_school_world_history,5-shot,accuracy,0.8016877637130801,0.0259550208416211
google/gemma-7b,mmlu_international_law,5-shot,accuracy,0.8429752066115702,0.0332124484254712
google/gemma-7b,mmlu_jurisprudence,5-shot,accuracy,0.75,0.041860917913946
google/gemma-7b,mmlu_logical_fallacies,5-shot,accuracy,0.754601226993865,0.0338093981394335
google/gemma-7b,mmlu_moral_disputes,5-shot,accuracy,0.6820809248554913,0.0250707137191531
google/gemma-7b,mmlu_moral_scenarios,5-shot,accuracy,0.3396648044692737,0.0158394004062124
google/gemma-7b,mmlu_philosophy,5-shot,accuracy,0.7170418006430869,0.0255830624899848
google/gemma-7b,mmlu_prehistory,5-shot,accuracy,0.7129629629629629,0.0251710419153096
google/gemma-7b,mmlu_professional_law,5-shot,accuracy,0.4621903520208605,0.0127336718803425
google/gemma-7b,mmlu_world_religions,5-shot,accuracy,0.8304093567251462,0.0287821081054017
google/gemma-7b,mmlu_business_ethics,5-shot,accuracy,0.63,0.0485236587093909
google/gemma-7b,mmlu_clinical_knowledge,5-shot,accuracy,0.660377358490566,0.0291469047477983
google/gemma-7b,mmlu_college_medicine,5-shot,accuracy,0.653179190751445,0.0362914667015966
google/gemma-7b,mmlu_global_facts,5-shot,accuracy,0.39,0.0490207130000197
google/gemma-7b,mmlu_human_aging,5-shot,accuracy,0.7130044843049327,0.0303603797102919
google/gemma-7b,mmlu_management,5-shot,accuracy,0.8640776699029126,0.0339329572976101
google/gemma-7b,mmlu_marketing,5-shot,accuracy,0.8974358974358975,0.0198756550278674
google/gemma-7b,mmlu_medical_genetics,5-shot,accuracy,0.74,0.0440844002276807
google/gemma-7b,mmlu_miscellaneous,5-shot,accuracy,0.8441890166028098,0.0129692692477625
google/gemma-7b,mmlu_nutrition,5-shot,accuracy,0.7516339869281046,0.0247399813551135
google/gemma-7b,mmlu_professional_accounting,5-shot,accuracy,0.475177304964539,0.0297907192438297
google/gemma-7b,mmlu_professional_medicine,5-shot,accuracy,0.5845588235294118,0.0299353427078777
google/gemma-7b,mmlu_virology,5-shot,accuracy,0.536144578313253,0.0388231085089059
google/gemma-7b,mmlu_econometrics,5-shot,accuracy,0.4824561403508772,0.0470070803355103
google/gemma-7b,mmlu_high_school_geography,5-shot,accuracy,0.803030303030303,0.0283356097324633
google/gemma-7b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.8756476683937824,0.0238144770865935
google/gemma-7b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.6358974358974359,0.0243966729850947
google/gemma-7b,mmlu_high_school_microeconomics,5-shot,accuracy,0.634453781512605,0.0312821770636846
google/gemma-7b,mmlu_high_school_psychology,5-shot,accuracy,0.8165137614678899,0.0165952597103993
google/gemma-7b,mmlu_human_sexuality,5-shot,accuracy,0.7251908396946565,0.0391534540884783
google/gemma-7b,mmlu_professional_psychology,5-shot,accuracy,0.6862745098039216,0.0187716838935281
google/gemma-7b,mmlu_public_relations,5-shot,accuracy,0.6545454545454545,0.0455461961754105
google/gemma-7b,mmlu_security_studies,5-shot,accuracy,0.7142857142857143,0.0289205832206755
google/gemma-7b,mmlu_sociology,5-shot,accuracy,0.8407960199004975,0.0258706467661691
google/gemma-7b,mmlu_us_foreign_policy,5-shot,accuracy,0.87,0.033799766898963
google/gemma-7b,mmlu_abstract_algebra,5-shot,accuracy,0.28,0.0451260859854212
google/gemma-7b,mmlu_anatomy,5-shot,accuracy,0.562962962962963,0.042849586397534
google/gemma-7b,mmlu_astronomy,5-shot,accuracy,0.7105263157894737,0.0369067798613728
google/gemma-7b,mmlu_college_biology,5-shot,accuracy,0.7430555555555556,0.0365394696944209
google/gemma-7b,mmlu_college_chemistry,5-shot,accuracy,0.47,0.0501613558046591
google/gemma-7b,mmlu_college_computer_science,5-shot,accuracy,0.53,0.0501613558046591
google/gemma-7b,mmlu_college_mathematics,5-shot,accuracy,0.33,0.047258156262526
google/gemma-7b,mmlu_college_physics,5-shot,accuracy,0.3627450980392157,0.0478406070410565
google/gemma-7b,mmlu_computer_security,5-shot,accuracy,0.75,0.0435194139889244
google/gemma-7b,mmlu_conceptual_physics,5-shot,accuracy,0.5914893617021276,0.0321341802670157
google/gemma-7b,mmlu_electrical_engineering,5-shot,accuracy,0.6482758620689655,0.0397923663749741
google/gemma-7b,mmlu_elementary_mathematics,5-shot,accuracy,0.4523809523809524,0.0256342581155549
google/gemma-7b,mmlu_high_school_biology,5-shot,accuracy,0.8,0.0227552049595429
google/gemma-7b,mmlu_high_school_chemistry,5-shot,accuracy,0.5172413793103449,0.0351589555116569
google/gemma-7b,mmlu_high_school_computer_science,5-shot,accuracy,0.69,0.0464823198711731
google/gemma-7b,mmlu_high_school_mathematics,5-shot,accuracy,0.4,0.0298696050953169
google/gemma-7b,mmlu_high_school_physics,5-shot,accuracy,0.3509933774834437,0.0389698196425737
google/gemma-7b,mmlu_high_school_statistics,5-shot,accuracy,0.4768518518518518,0.034063153607115
google/gemma-7b,mmlu_machine_learning,5-shot,accuracy,0.5178571428571429,0.0474276236124301
01-ai/Yi-9B,mmlu_formal_logic,5-shot,accuracy,0.5952380952380952,0.0439025926537756
01-ai/Yi-9B,mmlu_high_school_european_history,5-shot,accuracy,0.7696969696969697,0.0328766675860349
01-ai/Yi-9B,mmlu_high_school_us_history,5-shot,accuracy,0.8676470588235294,0.0237842975209188
01-ai/Yi-9B,mmlu_high_school_world_history,5-shot,accuracy,0.8185654008438819,0.0250859611445796
01-ai/Yi-9B,mmlu_international_law,5-shot,accuracy,0.8347107438016529,0.0339078061297277
01-ai/Yi-9B,mmlu_jurisprudence,5-shot,accuracy,0.7870370370370371,0.0395783547198097
01-ai/Yi-9B,mmlu_logical_fallacies,5-shot,accuracy,0.7668711656441718,0.0332201579577674
01-ai/Yi-9B,mmlu_moral_disputes,5-shot,accuracy,0.7572254335260116,0.0230836585869842
01-ai/Yi-9B,mmlu_moral_scenarios,5-shot,accuracy,0.3843575418994413,0.0162690886639593
01-ai/Yi-9B,mmlu_philosophy,5-shot,accuracy,0.7717041800643086,0.0238393033113982
01-ai/Yi-9B,mmlu_prehistory,5-shot,accuracy,0.7746913580246914,0.0232462026478197
01-ai/Yi-9B,mmlu_professional_law,5-shot,accuracy,0.5,0.0127702361059699
01-ai/Yi-9B,mmlu_world_religions,5-shot,accuracy,0.8421052631578947,0.0279667858591608
01-ai/Yi-9B,mmlu_business_ethics,5-shot,accuracy,0.8,0.0402015126103684
01-ai/Yi-9B,mmlu_clinical_knowledge,5-shot,accuracy,0.7283018867924528,0.0273777066246707
01-ai/Yi-9B,mmlu_college_medicine,5-shot,accuracy,0.7341040462427746,0.0336876293225943
01-ai/Yi-9B,mmlu_global_facts,5-shot,accuracy,0.34,0.0476095228569523
01-ai/Yi-9B,mmlu_human_aging,5-shot,accuracy,0.7399103139013453,0.0294424955858574
01-ai/Yi-9B,mmlu_management,5-shot,accuracy,0.8349514563106796,0.0367566883223318
01-ai/Yi-9B,mmlu_marketing,5-shot,accuracy,0.9102564102564102,0.0187243017419416
01-ai/Yi-9B,mmlu_medical_genetics,5-shot,accuracy,0.75,0.0435194139889244
01-ai/Yi-9B,mmlu_miscellaneous,5-shot,accuracy,0.8403575989782887,0.013097934513263
01-ai/Yi-9B,mmlu_nutrition,5-shot,accuracy,0.7483660130718954,0.0248480182638751
01-ai/Yi-9B,mmlu_professional_accounting,5-shot,accuracy,0.5638297872340425,0.029583452036284
01-ai/Yi-9B,mmlu_professional_medicine,5-shot,accuracy,0.7095588235294118,0.0275764686227405
01-ai/Yi-9B,mmlu_virology,5-shot,accuracy,0.5120481927710844,0.0389136449583581
01-ai/Yi-9B,mmlu_econometrics,5-shot,accuracy,0.5350877192982456,0.0469200838136891
01-ai/Yi-9B,mmlu_high_school_geography,5-shot,accuracy,0.8535353535353535,0.0251909211146039
01-ai/Yi-9B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.927461139896373,0.0187189985206781
01-ai/Yi-9B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.7564102564102564,0.0217637336841739
01-ai/Yi-9B,mmlu_high_school_microeconomics,5-shot,accuracy,0.819327731092437,0.0249919649666007
01-ai/Yi-9B,mmlu_high_school_psychology,5-shot,accuracy,0.8660550458715597,0.0146028114355926
01-ai/Yi-9B,mmlu_human_sexuality,5-shot,accuracy,0.7862595419847328,0.0359546161177469
01-ai/Yi-9B,mmlu_professional_psychology,5-shot,accuracy,0.7026143790849673,0.0184925965363969
01-ai/Yi-9B,mmlu_public_relations,5-shot,accuracy,0.7272727272727273,0.0426579211094058
01-ai/Yi-9B,mmlu_security_studies,5-shot,accuracy,0.7591836734693878,0.0273729422017881
01-ai/Yi-9B,mmlu_sociology,5-shot,accuracy,0.8606965174129353,0.0244844871629139
01-ai/Yi-9B,mmlu_us_foreign_policy,5-shot,accuracy,0.9,0.0301511344577763
01-ai/Yi-9B,mmlu_abstract_algebra,5-shot,accuracy,0.32,0.046882617226215
01-ai/Yi-9B,mmlu_anatomy,5-shot,accuracy,0.5925925925925926,0.0424463323835322
01-ai/Yi-9B,mmlu_astronomy,5-shot,accuracy,0.75,0.0352380739301204
01-ai/Yi-9B,mmlu_college_biology,5-shot,accuracy,0.8263888888888888,0.0316747338379571
01-ai/Yi-9B,mmlu_college_chemistry,5-shot,accuracy,0.54,0.0500908265962033
01-ai/Yi-9B,mmlu_college_computer_science,5-shot,accuracy,0.61,0.0490207130000197
01-ai/Yi-9B,mmlu_college_mathematics,5-shot,accuracy,0.45,0.0499999999999999
01-ai/Yi-9B,mmlu_college_physics,5-shot,accuracy,0.4313725490196078,0.0492809959728753
01-ai/Yi-9B,mmlu_computer_security,5-shot,accuracy,0.82,0.0386122919665369
01-ai/Yi-9B,mmlu_conceptual_physics,5-shot,accuracy,0.7106382978723405,0.0296440065770096
01-ai/Yi-9B,mmlu_electrical_engineering,5-shot,accuracy,0.7241379310344828,0.0372456361977463
01-ai/Yi-9B,mmlu_elementary_mathematics,5-shot,accuracy,0.6005291005291006,0.0252254502840679
01-ai/Yi-9B,mmlu_high_school_biology,5-shot,accuracy,0.8451612903225807,0.0205792873265832
01-ai/Yi-9B,mmlu_high_school_chemistry,5-shot,accuracy,0.5763546798029556,0.0347672574764903
01-ai/Yi-9B,mmlu_high_school_computer_science,5-shot,accuracy,0.83,0.0377525168068637
01-ai/Yi-9B,mmlu_high_school_mathematics,5-shot,accuracy,0.3962962962962963,0.029822619458534
01-ai/Yi-9B,mmlu_high_school_physics,5-shot,accuracy,0.423841059602649,0.0403484667860339
01-ai/Yi-9B,mmlu_high_school_statistics,5-shot,accuracy,0.6759259259259259,0.0319192344568618
01-ai/Yi-9B,mmlu_machine_learning,5-shot,accuracy,0.5982142857142857,0.0465333314697364
LLM360/Amber,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.0397015827323517
LLM360/Amber,mmlu_high_school_european_history,0-shot,accuracy,0.2848484848484848,0.0352439084451178
LLM360/Amber,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.0309645179269234
LLM360/Amber,mmlu_high_school_world_history,0-shot,accuracy,0.2616033755274262,0.0286095167169949
LLM360/Amber,mmlu_international_law,0-shot,accuracy,0.2727272727272727,0.040655781409087
LLM360/Amber,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.0441434366685493
LLM360/Amber,mmlu_logical_fallacies,0-shot,accuracy,0.2760736196319018,0.0351238528370505
LLM360/Amber,mmlu_moral_disputes,0-shot,accuracy,0.3005780346820809,0.0246853168672578
LLM360/Amber,mmlu_moral_scenarios,0-shot,accuracy,0.2469273743016759,0.0144222922048088
LLM360/Amber,mmlu_philosophy,0-shot,accuracy,0.315112540192926,0.0263852737034644
LLM360/Amber,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.0247486244905373
LLM360/Amber,mmlu_professional_law,0-shot,accuracy,0.2842242503259452,0.011519880596516
LLM360/Amber,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.0358252944257312
LLM360/Amber,mmlu_business_ethics,0-shot,accuracy,0.24,0.0429234695990928
LLM360/Amber,mmlu_clinical_knowledge,0-shot,accuracy,0.2528301886792453,0.0267498997712412
LLM360/Amber,mmlu_college_medicine,0-shot,accuracy,0.2716763005780346,0.0339175032232165
LLM360/Amber,mmlu_global_facts,0-shot,accuracy,0.33,0.047258156262526
LLM360/Amber,mmlu_human_aging,0-shot,accuracy,0.3049327354260089,0.0308986108824775
LLM360/Amber,mmlu_management,0-shot,accuracy,0.2718446601941747,0.0440526802414092
LLM360/Amber,mmlu_marketing,0-shot,accuracy,0.3333333333333333,0.0308827369741386
LLM360/Amber,mmlu_medical_genetics,0-shot,accuracy,0.23,0.042295258468165
LLM360/Amber,mmlu_miscellaneous,0-shot,accuracy,0.3141762452107279,0.0165992917358849
LLM360/Amber,mmlu_nutrition,0-shot,accuracy,0.2777777777777778,0.0256468630971379
LLM360/Amber,mmlu_professional_accounting,0-shot,accuracy,0.2943262411347517,0.0271871270115038
LLM360/Amber,mmlu_professional_medicine,0-shot,accuracy,0.1838235294117647,0.0235292421851931
LLM360/Amber,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.0361080501803102
LLM360/Amber,mmlu_econometrics,0-shot,accuracy,0.2105263157894736,0.0383515395439942
LLM360/Amber,mmlu_high_school_geography,0-shot,accuracy,0.2626262626262626,0.0313530500953308
LLM360/Amber,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2538860103626943,0.0314102478056531
LLM360/Amber,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2153846153846154,0.0208430345574628
LLM360/Amber,mmlu_high_school_microeconomics,0-shot,accuracy,0.1848739495798319,0.0252159928779542
LLM360/Amber,mmlu_high_school_psychology,0-shot,accuracy,0.2238532110091743,0.0178712177677902
LLM360/Amber,mmlu_human_sexuality,0-shot,accuracy,0.2977099236641221,0.040103589424622
LLM360/Amber,mmlu_professional_psychology,0-shot,accuracy,0.2957516339869281,0.0184631541326328
LLM360/Amber,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.0426579211094058
LLM360/Amber,mmlu_security_studies,0-shot,accuracy,0.2693877551020408,0.0284012520290229
LLM360/Amber,mmlu_sociology,0-shot,accuracy,0.3283582089552239,0.0332068588974432
LLM360/Amber,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.0464823198711731
LLM360/Amber,mmlu_abstract_algebra,0-shot,accuracy,0.31,0.0464823198711731
LLM360/Amber,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.0374985070917402
LLM360/Amber,mmlu_astronomy,0-shot,accuracy,0.2368421052631578,0.0345977760681053
LLM360/Amber,mmlu_college_biology,0-shot,accuracy,0.2152777777777778,0.0343707934410613
LLM360/Amber,mmlu_college_chemistry,0-shot,accuracy,0.23,0.042295258468165
LLM360/Amber,mmlu_college_computer_science,0-shot,accuracy,0.24,0.0429234695990928
LLM360/Amber,mmlu_college_mathematics,0-shot,accuracy,0.23,0.042295258468165
LLM360/Amber,mmlu_college_physics,0-shot,accuracy,0.1176470588235294,0.0320590773314452
LLM360/Amber,mmlu_computer_security,0-shot,accuracy,0.31,0.0464823198711731
LLM360/Amber,mmlu_conceptual_physics,0-shot,accuracy,0.2340425531914893,0.0276784525782123
LLM360/Amber,mmlu_electrical_engineering,0-shot,accuracy,0.3241379310344827,0.0390043206918555
LLM360/Amber,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.0217659616721545
LLM360/Amber,mmlu_high_school_biology,0-shot,accuracy,0.2774193548387096,0.0254701968359
LLM360/Amber,mmlu_high_school_chemistry,0-shot,accuracy,0.2758620689655172,0.0314471258167824
LLM360/Amber,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.047258156262526
LLM360/Amber,mmlu_high_school_mathematics,0-shot,accuracy,0.2555555555555555,0.026593939101844
LLM360/Amber,mmlu_high_school_physics,0-shot,accuracy,0.2317880794701986,0.0344540627198705
LLM360/Amber,mmlu_high_school_statistics,0-shot,accuracy,0.1805555555555555,0.0262328789714916
LLM360/Amber,mmlu_machine_learning,0-shot,accuracy,0.2678571428571428,0.0420327729146776
EleutherAI/pythia-160m-deduped,gsm8k_cot,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,gsm8k_cot,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,gsm8k_cot,0-shot,accuracy,0.0007581501137225,0.0007581501137225
EleutherAI/pythia-410m,gsm8k_cot,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,gsm8k_cot,0-shot,accuracy,0.0,0.0
microsoft/phi-2,gsm8k_cot,0-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-3B,gsm8k_cot,0-shot,accuracy,0.6542835481425322,0.0131004229904415
HuggingFaceTB/SmolLM-1.7B,gsm8k_cot,0-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,gsm8k_cot,0-shot,accuracy,0.0,0.0
facebook/xglm-2.9B,arc_challenge,0-shot,accuracy,0.2320819112627986,0.0123367182849488
facebook/xglm-2.9B,arc_challenge,0-shot,acc_norm,0.273037542662116,0.0130193327626357
EleutherAI/pythia-70m,arc_challenge,0-shot,accuracy,0.181740614334471,0.0112691989488802
EleutherAI/pythia-70m,arc_challenge,0-shot,acc_norm,0.2235494880546075,0.0121748966312026
EleutherAI/pythia-410m,arc_challenge,0-shot,accuracy,0.2133105802047781,0.0119709717423263
EleutherAI/pythia-410m,arc_challenge,0-shot,acc_norm,0.2397610921501706,0.0124763041274539
EleutherAI/pythia-410m-deduped,arc_challenge,0-shot,accuracy,0.2039249146757679,0.0117742624787022
EleutherAI/pythia-410m-deduped,arc_challenge,0-shot,acc_norm,0.2551194539249147,0.0127390386952021
EleutherAI/pythia-160m-deduped,arc_challenge,0-shot,accuracy,0.1945392491467577,0.0115677091746487
EleutherAI/pythia-160m-deduped,arc_challenge,0-shot,acc_norm,0.2431740614334471,0.012536554144587
Qwen/Qwen2.5-3B,arc_challenge,0-shot,accuracy,0.4453924914675768,0.014523987638344
Qwen/Qwen2.5-3B,arc_challenge,0-shot,acc_norm,0.4692832764505119,0.014583792546304
HuggingFaceTB/SmolLM-1.7B,arc_challenge,0-shot,accuracy,0.4343003412969283,0.0144847030488573
HuggingFaceTB/SmolLM-1.7B,arc_challenge,0-shot,acc_norm,0.4641638225255973,0.0145738136647357
HuggingFaceTB/SmolLM-360M,arc_challenge,0-shot,accuracy,0.3327645051194539,0.0137698630461923
HuggingFaceTB/SmolLM-360M,arc_challenge,0-shot,acc_norm,0.3592150170648464,0.0140202241558391
EleutherAI/pythia-160m-deduped,arithmetic_1dc,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_2da,0-shot,accuracy,0.007,0.001864735536023767
EleutherAI/pythia-160m-deduped,arithmetic_2dm,0-shot,accuracy,0.0255,0.003525775169416292
EleutherAI/pythia-160m-deduped,arithmetic_2ds,0-shot,accuracy,0.0125,0.002484947178762671
EleutherAI/pythia-160m-deduped,arithmetic_3da,0-shot,accuracy,0.001,0.0007069298939339296
EleutherAI/pythia-160m-deduped,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
EleutherAI/pythia-160m-deduped,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
EleutherAI/pythia-160m-deduped,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_1dc,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_2da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_2dm,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_2ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_3da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_3ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_4da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_1dc,0-shot,accuracy,0.0335,0.004024546370306088
EleutherAI/pythia-410m,arithmetic_2da,0-shot,accuracy,0.0105,0.0022797968630709933
EleutherAI/pythia-410m,arithmetic_2dm,0-shot,accuracy,0.0225,0.0033169829948455193
EleutherAI/pythia-410m,arithmetic_2ds,0-shot,accuracy,0.012,0.0024353573624298326
EleutherAI/pythia-410m,arithmetic_3da,0-shot,accuracy,0.0015,0.0008655920660521572
EleutherAI/pythia-410m,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
EleutherAI/pythia-410m,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
EleutherAI/pythia-410m,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_1dc,0-shot,accuracy,0.005,0.0015775754727385047
EleutherAI/pythia-410m-deduped,arithmetic_2da,0-shot,accuracy,0.007,0.001864735536023767
EleutherAI/pythia-410m-deduped,arithmetic_2dm,0-shot,accuracy,0.0245,0.0034577236625362453
EleutherAI/pythia-410m-deduped,arithmetic_2ds,0-shot,accuracy,0.0125,0.002484947178762671
EleutherAI/pythia-410m-deduped,arithmetic_3da,0-shot,accuracy,0.001,0.0007069298939339296
EleutherAI/pythia-410m-deduped,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
EleutherAI/pythia-410m-deduped,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
EleutherAI/pythia-410m-deduped,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_1dc,0-shot,accuracy,0.0015,0.0008655920660521522
EleutherAI/pythia-70m-deduped,arithmetic_2da,0-shot,accuracy,0.0005,0.0005000000000000006
EleutherAI/pythia-70m-deduped,arithmetic_2dm,0-shot,accuracy,0.001,0.0007069298939339532
EleutherAI/pythia-70m-deduped,arithmetic_2ds,0-shot,accuracy,0.0005,0.0005000000000000175
EleutherAI/pythia-70m-deduped,arithmetic_3da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_3ds,0-shot,accuracy,0.001,0.0007069298939339585
EleutherAI/pythia-70m-deduped,arithmetic_4da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_5ds,0-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_1dc,0-shot,accuracy,0.034,0.004053420174069573
HuggingFaceTB/SmolLM-360M,arithmetic_2da,0-shot,accuracy,0.13,0.007521854102850679
HuggingFaceTB/SmolLM-360M,arithmetic_2dm,0-shot,accuracy,0.0755,0.005909089072507989
HuggingFaceTB/SmolLM-360M,arithmetic_2ds,0-shot,accuracy,0.092,0.006464433033702516
HuggingFaceTB/SmolLM-360M,arithmetic_3da,0-shot,accuracy,0.023,0.0033527780362380454
HuggingFaceTB/SmolLM-360M,arithmetic_3ds,0-shot,accuracy,0.0095,0.002169614853910048
HuggingFaceTB/SmolLM-360M,arithmetic_4da,0-shot,accuracy,0.001,0.0007069298939339437
HuggingFaceTB/SmolLM-360M,arithmetic_4ds,0-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_5da,0-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_5ds,0-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-1.7B,arithmetic_1dc,0-shot,accuracy,0.0635,0.005454241411478458
HuggingFaceTB/SmolLM-1.7B,arithmetic_2da,0-shot,accuracy,0.46,0.011147292544180027
HuggingFaceTB/SmolLM-1.7B,arithmetic_2dm,0-shot,accuracy,0.2165,0.009211748105087025
HuggingFaceTB/SmolLM-1.7B,arithmetic_2ds,0-shot,accuracy,0.4275,0.011064948781886599
HuggingFaceTB/SmolLM-1.7B,arithmetic_3da,0-shot,accuracy,0.162,0.008240871069127832
HuggingFaceTB/SmolLM-1.7B,arithmetic_3ds,0-shot,accuracy,0.303,0.010278537063322019
HuggingFaceTB/SmolLM-1.7B,arithmetic_4da,0-shot,accuracy,0.0495,0.004851457855290583
HuggingFaceTB/SmolLM-1.7B,arithmetic_4ds,0-shot,accuracy,0.1695,0.008391667596045543
HuggingFaceTB/SmolLM-1.7B,arithmetic_5da,0-shot,accuracy,0.0125,0.002484947178762673
HuggingFaceTB/SmolLM-1.7B,arithmetic_5ds,0-shot,accuracy,0.024,0.0034231358327511275
Qwen/Qwen2.5-3B,lambada_openai,0-shot,perplexity,4.555046636024411,0.1076208480415047
Qwen/Qwen2.5-3B,lambada_openai,0-shot,accuracy,0.6685425965457016,0.0065582878844023
Qwen/Qwen2.5-3B,lambada_standard,0-shot,perplexity,5.914696831467921,0.1434764969306379
Qwen/Qwen2.5-3B,lambada_standard,0-shot,accuracy,0.5899476033378614,0.0068523331681548
EleutherAI/pythia-160m-deduped,lambada_openai,0-shot,perplexity,32.20032433651531,1.1996388136394602
EleutherAI/pythia-160m-deduped,lambada_openai,0-shot,accuracy,0.3396079953425189,0.0065978422744824
EleutherAI/pythia-160m-deduped,lambada_standard,0-shot,perplexity,177.23281292193062,7.943467582055663
EleutherAI/pythia-160m-deduped,lambada_standard,0-shot,accuracy,0.2225887832330681,0.0057954760014215
microsoft/phi-2,lambada_openai,0-shot,perplexity,5.558904423009891,0.1501573114838471
microsoft/phi-2,lambada_openai,0-shot,accuracy,0.625654958276732,0.0067424162519019
microsoft/phi-2,lambada_standard,0-shot,perplexity,11.877196935515974,0.3803929716780367
microsoft/phi-2,lambada_standard,0-shot,accuracy,0.4985445371628177,0.0069659481440645
Qwen/Qwen1.5-110B,arc:challenge,25-shot,accuracy,0.6544368600682594,0.0138969384611456
Qwen/Qwen1.5-110B,arc:challenge,25-shot,acc_norm,0.6996587030716723,0.013395909309957
Qwen/Qwen1.5-110B,hellaswag,10-shot,accuracy,0.682832105158335,0.0046442232947277
Qwen/Qwen1.5-110B,hellaswag,10-shot,acc_norm,0.8748257319259112,0.0033024011069263
Qwen/Qwen1.5-110B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.59,0.049431107042371
Qwen/Qwen1.5-110B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.59,0.049431107042371
Qwen/Qwen1.5-110B,hendrycksTest-anatomy,5-shot,accuracy,0.7333333333333333,0.038201699145179
Qwen/Qwen1.5-110B,hendrycksTest-anatomy,5-shot,acc_norm,0.7333333333333333,0.038201699145179
Qwen/Qwen1.5-110B,hendrycksTest-astronomy,5-shot,accuracy,0.9144736842105264,0.0227586771308886
Qwen/Qwen1.5-110B,hendrycksTest-astronomy,5-shot,acc_norm,0.9144736842105264,0.0227586771308886
Qwen/Qwen1.5-110B,hendrycksTest-business_ethics,5-shot,accuracy,0.82,0.0386122919665369
Qwen/Qwen1.5-110B,hendrycksTest-business_ethics,5-shot,acc_norm,0.82,0.0386122919665369
Qwen/Qwen1.5-110B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8226415094339623,0.0235087392188469
Qwen/Qwen1.5-110B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8226415094339623,0.0235087392188469
Qwen/Qwen1.5-110B,hendrycksTest-college_biology,5-shot,accuracy,0.9236111111111112,0.0222122039383459
Qwen/Qwen1.5-110B,hendrycksTest-college_biology,5-shot,acc_norm,0.9236111111111112,0.0222122039383459
Qwen/Qwen1.5-110B,hendrycksTest-college_chemistry,5-shot,accuracy,0.57,0.0497569851956242
Qwen/Qwen1.5-110B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.57,0.0497569851956242
Qwen/Qwen1.5-110B,hendrycksTest-college_computer_science,5-shot,accuracy,0.69,0.0464823198711731
Qwen/Qwen1.5-110B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.69,0.0464823198711731
Qwen/Qwen1.5-110B,hendrycksTest-college_mathematics,5-shot,accuracy,0.59,0.049431107042371
Qwen/Qwen1.5-110B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.59,0.049431107042371
Qwen/Qwen1.5-110B,hendrycksTest-college_medicine,5-shot,accuracy,0.7456647398843931,0.0332055644308557
Qwen/Qwen1.5-110B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7456647398843931,0.0332055644308557
Qwen/Qwen1.5-110B,hendrycksTest-college_physics,5-shot,accuracy,0.5294117647058824,0.0496657090397852
Qwen/Qwen1.5-110B,hendrycksTest-college_physics,5-shot,acc_norm,0.5294117647058824,0.0496657090397852
Qwen/Qwen1.5-110B,hendrycksTest-computer_security,5-shot,accuracy,0.83,0.0377525168068637
Qwen/Qwen1.5-110B,hendrycksTest-computer_security,5-shot,acc_norm,0.83,0.0377525168068637
Qwen/Qwen1.5-110B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.8127659574468085,0.0255015883418835
Qwen/Qwen1.5-110B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.8127659574468085,0.0255015883418835
Qwen/Qwen1.5-110B,hendrycksTest-econometrics,5-shot,accuracy,0.6754385964912281,0.0440455615737476
Qwen/Qwen1.5-110B,hendrycksTest-econometrics,5-shot,acc_norm,0.6754385964912281,0.0440455615737476
Qwen/Qwen1.5-110B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7724137931034483,0.0349395038013118
Qwen/Qwen1.5-110B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7724137931034483,0.0349395038013118
Qwen/Qwen1.5-110B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.8148148148148148,0.0200060754945244
Qwen/Qwen1.5-110B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.8148148148148148,0.0200060754945244
Qwen/Qwen1.5-110B,hendrycksTest-formal_logic,5-shot,accuracy,0.6190476190476191,0.0434352542894909
Qwen/Qwen1.5-110B,hendrycksTest-formal_logic,5-shot,acc_norm,0.6190476190476191,0.0434352542894909
Qwen/Qwen1.5-110B,hendrycksTest-global_facts,5-shot,accuracy,0.61,0.0490207130000197
Qwen/Qwen1.5-110B,hendrycksTest-global_facts,5-shot,acc_norm,0.61,0.0490207130000197
Qwen/Qwen1.5-110B,hendrycksTest-high_school_biology,5-shot,accuracy,0.9193548387096774,0.015490002961591
Qwen/Qwen1.5-110B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.9193548387096774,0.015490002961591
Qwen/Qwen1.5-110B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.7241379310344828,0.0314471258167824
Qwen/Qwen1.5-110B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.7241379310344828,0.0314471258167824
Qwen/Qwen1.5-110B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.88,0.032659863237109
Qwen/Qwen1.5-110B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.88,0.032659863237109
Qwen/Qwen1.5-110B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8909090909090909,0.0243438381351456
Qwen/Qwen1.5-110B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8909090909090909,0.0243438381351456
Qwen/Qwen1.5-110B,hendrycksTest-high_school_geography,5-shot,accuracy,0.9242424242424242,0.018852670234993
Qwen/Qwen1.5-110B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9242424242424242,0.018852670234993
Qwen/Qwen1.5-110B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9844559585492229,0.0089274927150843
Qwen/Qwen1.5-110B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9844559585492229,0.0089274927150843
Qwen/Qwen1.5-110B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8358974358974359,0.0187784343134237
Qwen/Qwen1.5-110B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8358974358974359,0.0187784343134237
Qwen/Qwen1.5-110B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.5888888888888889,0.0299999235087066
Qwen/Qwen1.5-110B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.5888888888888889,0.0299999235087066
Qwen/Qwen1.5-110B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8907563025210085,0.0202629874006053
Qwen/Qwen1.5-110B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8907563025210085,0.0202629874006053
Qwen/Qwen1.5-110B,hendrycksTest-high_school_physics,5-shot,accuracy,0.5827814569536424,0.0402614149763461
Qwen/Qwen1.5-110B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5827814569536424,0.0402614149763461
Qwen/Qwen1.5-110B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9394495412844036,0.0102257694828511
Qwen/Qwen1.5-110B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9394495412844036,0.0102257694828511
Qwen/Qwen1.5-110B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.75,0.0295312211609309
Qwen/Qwen1.5-110B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.75,0.0295312211609309
Qwen/Qwen1.5-110B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.946078431372549,0.0158524652811069
Qwen/Qwen1.5-110B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.946078431372549,0.0158524652811069
Qwen/Qwen1.5-110B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9240506329113924,0.0172446332510656
Qwen/Qwen1.5-110B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9240506329113924,0.0172446332510656
Qwen/Qwen1.5-110B,hendrycksTest-human_aging,5-shot,accuracy,0.8385650224215246,0.0246939578991284
Qwen/Qwen1.5-110B,hendrycksTest-human_aging,5-shot,acc_norm,0.8385650224215246,0.0246939578991284
Qwen/Qwen1.5-110B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8549618320610687,0.0308846610895153
Qwen/Qwen1.5-110B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8549618320610687,0.0308846610895153
Qwen/Qwen1.5-110B,hendrycksTest-international_law,5-shot,accuracy,0.9008264462809916,0.0272852463127589
Qwen/Qwen1.5-110B,hendrycksTest-international_law,5-shot,acc_norm,0.9008264462809916,0.0272852463127589
Qwen/Qwen1.5-110B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8611111111111112,0.0334327006286962
Qwen/Qwen1.5-110B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8611111111111112,0.0334327006286962
Qwen/Qwen1.5-110B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8957055214723927,0.024013517319439
Qwen/Qwen1.5-110B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8957055214723927,0.024013517319439
Qwen/Qwen1.5-110B,hendrycksTest-machine_learning,5-shot,accuracy,0.6517857142857143,0.0452182990283358
Qwen/Qwen1.5-110B,hendrycksTest-machine_learning,5-shot,acc_norm,0.6517857142857143,0.0452182990283358
Qwen/Qwen1.5-110B,hendrycksTest-management,5-shot,accuracy,0.8932038834951457,0.0305810889283313
Qwen/Qwen1.5-110B,hendrycksTest-management,5-shot,acc_norm,0.8932038834951457,0.0305810889283313
Qwen/Qwen1.5-110B,hendrycksTest-marketing,5-shot,accuracy,0.9487179487179488,0.0144501811768727
Qwen/Qwen1.5-110B,hendrycksTest-marketing,5-shot,acc_norm,0.9487179487179488,0.0144501811768727
Qwen/Qwen1.5-110B,hendrycksTest-medical_genetics,5-shot,accuracy,0.85,0.0358870281282637
Qwen/Qwen1.5-110B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.85,0.0358870281282637
Qwen/Qwen1.5-110B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9450830140485312,0.0081467605007523
Qwen/Qwen1.5-110B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9450830140485312,0.0081467605007523
Qwen/Qwen1.5-110B,hendrycksTest-moral_disputes,5-shot,accuracy,0.846820809248555,0.0193903701089699
Qwen/Qwen1.5-110B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.846820809248555,0.0193903701089699
Qwen/Qwen1.5-110B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.8100558659217877,0.0131190283104926
Qwen/Qwen1.5-110B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.8100558659217877,0.0131190283104926
Qwen/Qwen1.5-110B,hendrycksTest-nutrition,5-shot,accuracy,0.8954248366013072,0.0175218082941744
Qwen/Qwen1.5-110B,hendrycksTest-nutrition,5-shot,acc_norm,0.8954248366013072,0.0175218082941744
Qwen/Qwen1.5-110B,hendrycksTest-philosophy,5-shot,accuracy,0.8520900321543409,0.0201632538062841
Qwen/Qwen1.5-110B,hendrycksTest-philosophy,5-shot,acc_norm,0.8520900321543409,0.0201632538062841
Qwen/Qwen1.5-110B,hendrycksTest-prehistory,5-shot,accuracy,0.8919753086419753,0.0172717630844835
Qwen/Qwen1.5-110B,hendrycksTest-prehistory,5-shot,acc_norm,0.8919753086419753,0.0172717630844835
Qwen/Qwen1.5-110B,hendrycksTest-professional_accounting,5-shot,accuracy,0.6702127659574468,0.0280459469420424
Qwen/Qwen1.5-110B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6702127659574468,0.0280459469420424
Qwen/Qwen1.5-110B,hendrycksTest-professional_law,5-shot,accuracy,0.6473272490221643,0.0122032868460539
Qwen/Qwen1.5-110B,hendrycksTest-professional_law,5-shot,acc_norm,0.6473272490221643,0.0122032868460539
Qwen/Qwen1.5-110B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8860294117647058,0.0193034892791189
Qwen/Qwen1.5-110B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8860294117647058,0.0193034892791189
Qwen/Qwen1.5-110B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8431372549019608,0.0147125665414381
Qwen/Qwen1.5-110B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8431372549019608,0.0147125665414381
Qwen/Qwen1.5-110B,hendrycksTest-public_relations,5-shot,accuracy,0.7272727272727273,0.0426579211094058
Qwen/Qwen1.5-110B,hendrycksTest-public_relations,5-shot,acc_norm,0.7272727272727273,0.0426579211094058
Qwen/Qwen1.5-110B,hendrycksTest-security_studies,5-shot,accuracy,0.8448979591836735,0.0231747988612186
Qwen/Qwen1.5-110B,hendrycksTest-security_studies,5-shot,acc_norm,0.8448979591836735,0.0231747988612186
Qwen/Qwen1.5-110B,hendrycksTest-sociology,5-shot,accuracy,0.9303482587064676,0.0180000522538562
Qwen/Qwen1.5-110B,hendrycksTest-sociology,5-shot,acc_norm,0.9303482587064676,0.0180000522538562
Qwen/Qwen1.5-110B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.95,0.021904291355759
Qwen/Qwen1.5-110B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.95,0.021904291355759
Qwen/Qwen1.5-110B,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721
Qwen/Qwen1.5-110B,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721
Qwen/Qwen1.5-110B,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.0251729843501557
Qwen/Qwen1.5-110B,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.0251729843501557
Qwen/Qwen1.5-110B,truthfulqa:mc,0-shot,mc1,0.3365973072215422,0.0165424128094948
Qwen/Qwen1.5-110B,truthfulqa:mc,0-shot,mc2,0.4966492716220296,0.0145020912507795
Qwen/Qwen1.5-110B,winogrande,5-shot,accuracy,0.8413575374901342,0.0102679362430282
Qwen/Qwen1.5-110B,gsm8k,5-shot,accuracy,0.8104624715693708,0.0107958379318963
Qwen/Qwen1.5-14B,arc:challenge,25-shot,accuracy,0.5221843003412969,0.0145970019270761
Qwen/Qwen1.5-14B,arc:challenge,25-shot,acc_norm,0.5656996587030717,0.0144847030488573
Qwen/Qwen1.5-14B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.39,0.0490207130000197
Qwen/Qwen1.5-14B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.39,0.0490207130000197
Qwen/Qwen1.5-14B,hendrycksTest-anatomy,5-shot,accuracy,0.6296296296296297,0.0417165416135454
Qwen/Qwen1.5-14B,hendrycksTest-anatomy,5-shot,acc_norm,0.6296296296296297,0.0417165416135454
Qwen/Qwen1.5-14B,hendrycksTest-astronomy,5-shot,accuracy,0.7368421052631579,0.0358349617636107
Qwen/Qwen1.5-14B,hendrycksTest-astronomy,5-shot,acc_norm,0.7368421052631579,0.0358349617636107
Qwen/Qwen1.5-14B,hendrycksTest-business_ethics,5-shot,accuracy,0.76,0.0429234695990928
Qwen/Qwen1.5-14B,hendrycksTest-business_ethics,5-shot,acc_norm,0.76,0.0429234695990928
Qwen/Qwen1.5-14B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7433962264150943,0.0268806478890519
Qwen/Qwen1.5-14B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7433962264150943,0.0268806478890519
Qwen/Qwen1.5-14B,hendrycksTest-college_biology,5-shot,accuracy,0.7638888888888888,0.0355144661081082
Qwen/Qwen1.5-14B,hendrycksTest-college_biology,5-shot,acc_norm,0.7638888888888888,0.0355144661081082
Qwen/Qwen1.5-14B,hendrycksTest-college_chemistry,5-shot,accuracy,0.48,0.0502116731568677
Qwen/Qwen1.5-14B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.48,0.0502116731568677
Qwen/Qwen1.5-14B,hendrycksTest-college_computer_science,5-shot,accuracy,0.58,0.0496044963748858
Qwen/Qwen1.5-14B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.58,0.0496044963748858
Qwen/Qwen1.5-14B,hendrycksTest-college_mathematics,5-shot,accuracy,0.46,0.0500908265962033
Qwen/Qwen1.5-14B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.46,0.0500908265962033
Qwen/Qwen1.5-14B,hendrycksTest-college_medicine,5-shot,accuracy,0.6936416184971098,0.0351494255126743
Qwen/Qwen1.5-14B,hendrycksTest-college_medicine,5-shot,acc_norm,0.6936416184971098,0.0351494255126743
Qwen/Qwen1.5-14B,hendrycksTest-college_physics,5-shot,accuracy,0.4901960784313725,0.0497422946042281
Qwen/Qwen1.5-14B,hendrycksTest-college_physics,5-shot,acc_norm,0.4901960784313725,0.0497422946042281
Qwen/Qwen1.5-14B,hendrycksTest-computer_security,5-shot,accuracy,0.8,0.0402015126103684
Qwen/Qwen1.5-14B,hendrycksTest-computer_security,5-shot,acc_norm,0.8,0.0402015126103684
Qwen/Qwen1.5-14B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7063829787234043,0.0297716427124912
Qwen/Qwen1.5-14B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7063829787234043,0.0297716427124912
Qwen/Qwen1.5-14B,hendrycksTest-econometrics,5-shot,accuracy,0.5526315789473685,0.0467747300449119
Qwen/Qwen1.5-14B,hendrycksTest-econometrics,5-shot,acc_norm,0.5526315789473685,0.0467747300449119
Qwen/Qwen1.5-14B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7310344827586207,0.0369518331165023
Qwen/Qwen1.5-14B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7310344827586207,0.0369518331165023
Qwen/Qwen1.5-14B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.58994708994709,0.0253312024389444
Qwen/Qwen1.5-14B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.58994708994709,0.0253312024389444
Qwen/Qwen1.5-14B,hendrycksTest-formal_logic,5-shot,accuracy,0.5555555555555556,0.0444444444444444
Qwen/Qwen1.5-14B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5555555555555556,0.0444444444444444
Qwen/Qwen1.5-14B,hendrycksTest-global_facts,5-shot,accuracy,0.53,0.0501613558046591
Qwen/Qwen1.5-14B,hendrycksTest-global_facts,5-shot,acc_norm,0.53,0.0501613558046591
Qwen/Qwen1.5-14B,hendrycksTest-high_school_biology,5-shot,accuracy,0.8451612903225807,0.0205792873265832
Qwen/Qwen1.5-14B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8451612903225807,0.0205792873265832
Qwen/Qwen1.5-14B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5960591133004927,0.0345245390382203
Qwen/Qwen1.5-14B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5960591133004927,0.0345245390382203
Qwen/Qwen1.5-14B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.75,0.0435194139889244
Qwen/Qwen1.5-14B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.75,0.0435194139889244
Qwen/Qwen1.5-14B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8424242424242424,0.0284503888052843
Qwen/Qwen1.5-14B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8424242424242424,0.0284503888052843
Qwen/Qwen1.5-14B,hendrycksTest-high_school_geography,5-shot,accuracy,0.8686868686868687,0.0240631564168225
Qwen/Qwen1.5-14B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8686868686868687,0.0240631564168225
Qwen/Qwen1.5-14B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9015544041450776,0.0215002495760334
Qwen/Qwen1.5-14B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9015544041450776,0.0215002495760334
Qwen/Qwen1.5-14B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.735897435897436,0.0223521937374532
Qwen/Qwen1.5-14B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.735897435897436,0.0223521937374532
Qwen/Qwen1.5-14B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4333333333333333,0.0302133402892379
Qwen/Qwen1.5-14B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4333333333333333,0.0302133402892379
Qwen/Qwen1.5-14B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7647058823529411,0.0275536144678638
Qwen/Qwen1.5-14B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7647058823529411,0.0275536144678638
Qwen/Qwen1.5-14B,hendrycksTest-high_school_physics,5-shot,accuracy,0.4966887417218543,0.0408239337944965
Qwen/Qwen1.5-14B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4966887417218543,0.0408239337944965
Qwen/Qwen1.5-14B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8660550458715597,0.0146028114355926
Qwen/Qwen1.5-14B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8660550458715597,0.0146028114355926
Qwen/Qwen1.5-14B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6342592592592593,0.032847388576472
Qwen/Qwen1.5-14B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6342592592592593,0.032847388576472
Qwen/Qwen1.5-14B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8137254901960784,0.0273254709667163
Qwen/Qwen1.5-14B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8137254901960784,0.0273254709667163
Qwen/Qwen1.5-14B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8312236286919831,0.0243814068325862
Qwen/Qwen1.5-14B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8312236286919831,0.0243814068325862
Qwen/Qwen1.5-14B,hendrycksTest-human_aging,5-shot,accuracy,0.7309417040358744,0.0297637794068749
Qwen/Qwen1.5-14B,hendrycksTest-human_aging,5-shot,acc_norm,0.7309417040358744,0.0297637794068749
Qwen/Qwen1.5-14B,hendrycksTest-human_sexuality,5-shot,accuracy,0.7633587786259542,0.0372767357559691
Qwen/Qwen1.5-14B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7633587786259542,0.0372767357559691
Qwen/Qwen1.5-14B,hendrycksTest-international_law,5-shot,accuracy,0.8429752066115702,0.0332124484254712
Qwen/Qwen1.5-14B,hendrycksTest-international_law,5-shot,acc_norm,0.8429752066115702,0.0332124484254712
Qwen/Qwen1.5-14B,hendrycksTest-jurisprudence,5-shot,accuracy,0.75,0.041860917913946
Qwen/Qwen1.5-14B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.75,0.041860917913946
Qwen/Qwen1.5-14B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7607361963190185,0.0335195387952127
Qwen/Qwen1.5-14B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7607361963190185,0.0335195387952127
Qwen/Qwen1.5-14B,hendrycksTest-machine_learning,5-shot,accuracy,0.5267857142857143,0.0473897511927415
Qwen/Qwen1.5-14B,hendrycksTest-machine_learning,5-shot,acc_norm,0.5267857142857143,0.0473897511927415
Qwen/Qwen1.5-14B,hendrycksTest-management,5-shot,accuracy,0.8252427184466019,0.0376017800602662
Qwen/Qwen1.5-14B,hendrycksTest-management,5-shot,acc_norm,0.8252427184466019,0.0376017800602662
Qwen/Qwen1.5-14B,hendrycksTest-marketing,5-shot,accuracy,0.8717948717948718,0.0219019051150733
Qwen/Qwen1.5-14B,hendrycksTest-marketing,5-shot,acc_norm,0.8717948717948718,0.0219019051150733
Qwen/Qwen1.5-14B,hendrycksTest-medical_genetics,5-shot,accuracy,0.79,0.0409360180740332
Qwen/Qwen1.5-14B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.79,0.0409360180740332
Qwen/Qwen1.5-14B,hendrycksTest-miscellaneous,5-shot,accuracy,0.8403575989782887,0.0130979345132629
Qwen/Qwen1.5-14B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8403575989782887,0.0130979345132629
Qwen/Qwen1.5-14B,hendrycksTest-moral_disputes,5-shot,accuracy,0.7485549132947977,0.023357365785874
Qwen/Qwen1.5-14B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7485549132947977,0.023357365785874
Qwen/Qwen1.5-14B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4491620111731844,0.0166358383416319
Qwen/Qwen1.5-14B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4491620111731844,0.0166358383416319
Qwen/Qwen1.5-14B,hendrycksTest-nutrition,5-shot,accuracy,0.7516339869281046,0.0247399813551135
Qwen/Qwen1.5-14B,hendrycksTest-nutrition,5-shot,acc_norm,0.7516339869281046,0.0247399813551135
Qwen/Qwen1.5-14B,hendrycksTest-philosophy,5-shot,accuracy,0.7170418006430869,0.0255830624899848
Qwen/Qwen1.5-14B,hendrycksTest-philosophy,5-shot,acc_norm,0.7170418006430869,0.0255830624899848
Qwen/Qwen1.5-14B,hendrycksTest-prehistory,5-shot,accuracy,0.7129629629629629,0.0251710419153096
Qwen/Qwen1.5-14B,hendrycksTest-prehistory,5-shot,acc_norm,0.7129629629629629,0.0251710419153096
Qwen/Qwen1.5-14B,hendrycksTest-professional_accounting,5-shot,accuracy,0.5141843971631206,0.029815494483682
Qwen/Qwen1.5-14B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5141843971631206,0.029815494483682
Qwen/Qwen1.5-14B,hendrycksTest-professional_law,5-shot,accuracy,0.483702737940026,0.0127634507346998
Qwen/Qwen1.5-14B,hendrycksTest-professional_law,5-shot,acc_norm,0.483702737940026,0.0127634507346998
Qwen/Qwen1.5-14B,hendrycksTest-professional_medicine,5-shot,accuracy,0.7169117647058824,0.0273658611315138
Qwen/Qwen1.5-14B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.7169117647058824,0.0273658611315138
Qwen/Qwen1.5-14B,hendrycksTest-professional_psychology,5-shot,accuracy,0.6993464052287581,0.0185506345029529
Qwen/Qwen1.5-14B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.6993464052287581,0.0185506345029529
Qwen/Qwen1.5-14B,hendrycksTest-public_relations,5-shot,accuracy,0.6454545454545455,0.0458200484150541
Qwen/Qwen1.5-14B,hendrycksTest-public_relations,5-shot,acc_norm,0.6454545454545455,0.0458200484150541
Qwen/Qwen1.5-14B,hendrycksTest-security_studies,5-shot,accuracy,0.8122448979591836,0.0250002560395462
Qwen/Qwen1.5-14B,hendrycksTest-security_studies,5-shot,acc_norm,0.8122448979591836,0.0250002560395462
Qwen/Qwen1.5-14B,hendrycksTest-sociology,5-shot,accuracy,0.8407960199004975,0.0258706467661691
Qwen/Qwen1.5-14B,hendrycksTest-sociology,5-shot,acc_norm,0.8407960199004975,0.0258706467661691
Qwen/Qwen1.5-14B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.88,0.032659863237109
Qwen/Qwen1.5-14B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.88,0.032659863237109
Qwen/Qwen1.5-14B,hendrycksTest-virology,5-shot,accuracy,0.4578313253012048,0.0387862677100235
Qwen/Qwen1.5-14B,hendrycksTest-virology,5-shot,acc_norm,0.4578313253012048,0.0387862677100235
Qwen/Qwen1.5-14B,hendrycksTest-world_religions,5-shot,accuracy,0.8304093567251462,0.0287821081054017
Qwen/Qwen1.5-14B,hendrycksTest-world_religions,5-shot,acc_norm,0.8304093567251462,0.0287821081054017
Qwen/Qwen1.5-14B,truthfulqa:mc,0-shot,mc1,0.3574051407588739,0.0167765996767293
Qwen/Qwen1.5-14B,truthfulqa:mc,0-shot,mc2,0.5206092394796343,0.0149147994861834
Salesforce/codegen-16B-nl,drop,3-shot,em,0.0012583892617449,0.0003630560893119
Salesforce/codegen-16B-nl,drop,3-shot,f1,0.0501153523489933,0.00120040401033
Salesforce/codegen-16B-nl,gsm8k,5-shot,accuracy,0.026535253980288,0.0044270459872651
Salesforce/codegen-16B-nl,winogrande,5-shot,accuracy,0.6795580110497238,0.0131150854576817
Salesforce/codegen-16B-nl,arc:challenge,25-shot,accuracy,0.4180887372013652,0.014413988396996
Salesforce/codegen-16B-nl,arc:challenge,25-shot,acc_norm,0.4675767918088737,0.0145806375699954
Salesforce/codegen-16B-nl,hellaswag,10-shot,accuracy,0.5323640709022107,0.0049793175154325
Salesforce/codegen-16B-nl,hellaswag,10-shot,acc_norm,0.7186815375423222,0.0044872356579556
Salesforce/codegen-16B-nl,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-16B-nl,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.0460566186471838
Salesforce/codegen-16B-nl,hendrycksTest-anatomy,5-shot,accuracy,0.3259259259259259,0.040491220417025
Salesforce/codegen-16B-nl,hendrycksTest-anatomy,5-shot,acc_norm,0.3259259259259259,0.040491220417025
Salesforce/codegen-16B-nl,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.0373852067611966
Salesforce/codegen-16B-nl,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.0373852067611966
Salesforce/codegen-16B-nl,hendrycksTest-business_ethics,5-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-16B-nl,hendrycksTest-business_ethics,5-shot,acc_norm,0.28,0.0451260859854212
Salesforce/codegen-16B-nl,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3169811320754717,0.0286372356398009
Salesforce/codegen-16B-nl,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3169811320754717,0.0286372356398009
Salesforce/codegen-16B-nl,hendrycksTest-college_biology,5-shot,accuracy,0.3333333333333333,0.0394208263992721
Salesforce/codegen-16B-nl,hendrycksTest-college_biology,5-shot,acc_norm,0.3333333333333333,0.0394208263992721
Salesforce/codegen-16B-nl,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.0440844002276807
Salesforce/codegen-16B-nl,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.0440844002276807
Salesforce/codegen-16B-nl,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.0451260859854212
Salesforce/codegen-16B-nl,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.0451260859854212
Salesforce/codegen-16B-nl,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.0460566186471838
Salesforce/codegen-16B-nl,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.0460566186471838
Salesforce/codegen-16B-nl,hendrycksTest-college_medicine,5-shot,accuracy,0.2774566473988439,0.0341401400704403
Salesforce/codegen-16B-nl,hendrycksTest-college_medicine,5-shot,acc_norm,0.2774566473988439,0.0341401400704403
Salesforce/codegen-16B-nl,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.0433643270799317
Salesforce/codegen-16B-nl,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.0433643270799317
Salesforce/codegen-16B-nl,hendrycksTest-computer_security,5-shot,accuracy,0.41,0.049431107042371
Salesforce/codegen-16B-nl,hendrycksTest-computer_security,5-shot,acc_norm,0.41,0.049431107042371
Salesforce/codegen-16B-nl,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3404255319148936,0.0309766929985344
Salesforce/codegen-16B-nl,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3404255319148936,0.0309766929985344
Salesforce/codegen-16B-nl,hendrycksTest-econometrics,5-shot,accuracy,0.2456140350877192,0.0404933929774814
Salesforce/codegen-16B-nl,hendrycksTest-econometrics,5-shot,acc_norm,0.2456140350877192,0.0404933929774814
Salesforce/codegen-16B-nl,hendrycksTest-electrical_engineering,5-shot,accuracy,0.4275862068965517,0.0412273711137033
Salesforce/codegen-16B-nl,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.4275862068965517,0.0412273711137033
Salesforce/codegen-16B-nl,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2407407407407407,0.0220190800122178
Salesforce/codegen-16B-nl,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2407407407407407,0.0220190800122178
Salesforce/codegen-16B-nl,hendrycksTest-formal_logic,5-shot,accuracy,0.238095238095238,0.0380952380952381
Salesforce/codegen-16B-nl,hendrycksTest-formal_logic,5-shot,acc_norm,0.238095238095238,0.0380952380952381
Salesforce/codegen-16B-nl,hendrycksTest-global_facts,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-16B-nl,hendrycksTest-global_facts,5-shot,acc_norm,0.29,0.0456048021572068
Salesforce/codegen-16B-nl,hendrycksTest-high_school_biology,5-shot,accuracy,0.3161290322580645,0.0264508744890427
Salesforce/codegen-16B-nl,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3161290322580645,0.0264508744890427
Salesforce/codegen-16B-nl,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2266009852216748,0.0294548638352929
Salesforce/codegen-16B-nl,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2266009852216748,0.0294548638352929
Salesforce/codegen-16B-nl,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-16B-nl,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.0464823198711731
Salesforce/codegen-16B-nl,hendrycksTest-high_school_european_history,5-shot,accuracy,0.303030303030303,0.035886248000917
Salesforce/codegen-16B-nl,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.303030303030303,0.035886248000917
Salesforce/codegen-16B-nl,hendrycksTest-high_school_geography,5-shot,accuracy,0.3535353535353535,0.0340608672354715
Salesforce/codegen-16B-nl,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3535353535353535,0.0340608672354715
Salesforce/codegen-16B-nl,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3367875647668393,0.0341078025183618
Salesforce/codegen-16B-nl,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3367875647668393,0.0341078025183618
Salesforce/codegen-16B-nl,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2307692307692307,0.0213620277252227
Salesforce/codegen-16B-nl,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2307692307692307,0.0213620277252227
Salesforce/codegen-16B-nl,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2703703703703703,0.0270803728151456
Salesforce/codegen-16B-nl,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2703703703703703,0.0270803728151456
Salesforce/codegen-16B-nl,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2436974789915966,0.0278868280783805
Salesforce/codegen-16B-nl,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2436974789915966,0.0278868280783805
Salesforce/codegen-16B-nl,hendrycksTest-high_school_physics,5-shot,accuracy,0.2980132450331126,0.0373453567678719
Salesforce/codegen-16B-nl,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2980132450331126,0.0373453567678719
Salesforce/codegen-16B-nl,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2844036697247706,0.0193420365877025
Salesforce/codegen-16B-nl,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2844036697247706,0.0193420365877025
Salesforce/codegen-16B-nl,hendrycksTest-high_school_statistics,5-shot,accuracy,0.199074074074074,0.0272322984626902
Salesforce/codegen-16B-nl,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.199074074074074,0.0272322984626902
Salesforce/codegen-16B-nl,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3529411764705882,0.0335409243759151
Salesforce/codegen-16B-nl,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3529411764705882,0.0335409243759151
Salesforce/codegen-16B-nl,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3417721518987341,0.0308745375375536
Salesforce/codegen-16B-nl,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3417721518987341,0.0308745375375536
Salesforce/codegen-16B-nl,hendrycksTest-human_aging,5-shot,accuracy,0.3766816143497757,0.0325211348992918
Salesforce/codegen-16B-nl,hendrycksTest-human_aging,5-shot,acc_norm,0.3766816143497757,0.0325211348992918
Salesforce/codegen-16B-nl,hendrycksTest-human_sexuality,5-shot,accuracy,0.3816793893129771,0.0426073515764456
Salesforce/codegen-16B-nl,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3816793893129771,0.0426073515764456
Salesforce/codegen-16B-nl,hendrycksTest-international_law,5-shot,accuracy,0.4297520661157025,0.0451908202131977
Salesforce/codegen-16B-nl,hendrycksTest-international_law,5-shot,acc_norm,0.4297520661157025,0.0451908202131977
Salesforce/codegen-16B-nl,hendrycksTest-jurisprudence,5-shot,accuracy,0.3981481481481481,0.0473233261597881
Salesforce/codegen-16B-nl,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3981481481481481,0.0473233261597881
Salesforce/codegen-16B-nl,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.0355903953161734
Salesforce/codegen-16B-nl,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.0355903953161734
Salesforce/codegen-16B-nl,hendrycksTest-machine_learning,5-shot,accuracy,0.3303571428571428,0.0446428571428571
Salesforce/codegen-16B-nl,hendrycksTest-machine_learning,5-shot,acc_norm,0.3303571428571428,0.0446428571428571
Salesforce/codegen-16B-nl,hendrycksTest-management,5-shot,accuracy,0.2912621359223301,0.0449867632057292
Salesforce/codegen-16B-nl,hendrycksTest-management,5-shot,acc_norm,0.2912621359223301,0.0449867632057292
Salesforce/codegen-16B-nl,hendrycksTest-marketing,5-shot,accuracy,0.3675213675213675,0.0315853915774563
Salesforce/codegen-16B-nl,hendrycksTest-marketing,5-shot,acc_norm,0.3675213675213675,0.0315853915774563
Salesforce/codegen-16B-nl,hendrycksTest-medical_genetics,5-shot,accuracy,0.43,0.0497569851956242
Salesforce/codegen-16B-nl,hendrycksTest-medical_genetics,5-shot,acc_norm,0.43,0.0497569851956242
Salesforce/codegen-16B-nl,hendrycksTest-miscellaneous,5-shot,accuracy,0.3742017879948914,0.017304805072252
Salesforce/codegen-16B-nl,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3742017879948914,0.017304805072252
Salesforce/codegen-16B-nl,hendrycksTest-moral_disputes,5-shot,accuracy,0.3670520231213873,0.025950054337654
Salesforce/codegen-16B-nl,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3670520231213873,0.025950054337654
Salesforce/codegen-16B-nl,hendrycksTest-moral_scenarios,5-shot,accuracy,0.28268156424581,0.015060381730018
Salesforce/codegen-16B-nl,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.28268156424581,0.015060381730018
Salesforce/codegen-16B-nl,hendrycksTest-nutrition,5-shot,accuracy,0.3790849673202614,0.0277801412070233
Salesforce/codegen-16B-nl,hendrycksTest-nutrition,5-shot,acc_norm,0.3790849673202614,0.0277801412070233
Salesforce/codegen-16B-nl,hendrycksTest-philosophy,5-shot,accuracy,0.3344051446945337,0.0267954223278939
Salesforce/codegen-16B-nl,hendrycksTest-philosophy,5-shot,acc_norm,0.3344051446945337,0.0267954223278939
Salesforce/codegen-16B-nl,hendrycksTest-prehistory,5-shot,accuracy,0.3611111111111111,0.0267258688091007
Salesforce/codegen-16B-nl,hendrycksTest-prehistory,5-shot,acc_norm,0.3611111111111111,0.0267258688091007
Salesforce/codegen-16B-nl,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
Salesforce/codegen-16B-nl,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
Salesforce/codegen-16B-nl,hendrycksTest-professional_law,5-shot,accuracy,0.3239895697522816,0.0119528408096465
Salesforce/codegen-16B-nl,hendrycksTest-professional_law,5-shot,acc_norm,0.3239895697522816,0.0119528408096465
Salesforce/codegen-16B-nl,hendrycksTest-professional_medicine,5-shot,accuracy,0.2977941176470588,0.0277782987015454
Salesforce/codegen-16B-nl,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2977941176470588,0.0277782987015454
Salesforce/codegen-16B-nl,hendrycksTest-professional_psychology,5-shot,accuracy,0.2973856209150327,0.0184925965363969
Salesforce/codegen-16B-nl,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2973856209150327,0.0184925965363969
Salesforce/codegen-16B-nl,hendrycksTest-public_relations,5-shot,accuracy,0.3545454545454545,0.0458200484150541
Salesforce/codegen-16B-nl,hendrycksTest-public_relations,5-shot,acc_norm,0.3545454545454545,0.0458200484150541
Salesforce/codegen-16B-nl,hendrycksTest-security_studies,5-shot,accuracy,0.4285714285714285,0.0316809116123388
Salesforce/codegen-16B-nl,hendrycksTest-security_studies,5-shot,acc_norm,0.4285714285714285,0.0316809116123388
Salesforce/codegen-16B-nl,hendrycksTest-sociology,5-shot,accuracy,0.3781094527363184,0.0342886784877865
Salesforce/codegen-16B-nl,hendrycksTest-sociology,5-shot,acc_norm,0.3781094527363184,0.0342886784877865
Salesforce/codegen-16B-nl,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.5,0.0502518907629606
Salesforce/codegen-16B-nl,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.5,0.0502518907629606
Salesforce/codegen-16B-nl,hendrycksTest-virology,5-shot,accuracy,0.3313253012048193,0.0366431477728808
Salesforce/codegen-16B-nl,hendrycksTest-virology,5-shot,acc_norm,0.3313253012048193,0.0366431477728808
Salesforce/codegen-16B-nl,hendrycksTest-world_religions,5-shot,accuracy,0.391812865497076,0.037439798259264
Salesforce/codegen-16B-nl,hendrycksTest-world_religions,5-shot,acc_norm,0.391812865497076,0.037439798259264
Salesforce/codegen-16B-nl,truthfulqa:mc,0-shot,mc1,0.2068543451652386,0.0141795914967283
Salesforce/codegen-16B-nl,truthfulqa:mc,0-shot,mc2,0.3394667243139816,0.0133257175868028
Qwen/Qwen2-72B,arc:challenge,25-shot,accuracy,0.658703071672355,0.0138558312874977
Qwen/Qwen2-72B,arc:challenge,25-shot,acc_norm,0.6877133105802048,0.013542598541688
Qwen/Qwen2-72B,hellaswag,10-shot,accuracy,0.6763592909778928,0.0046690854113421
Qwen/Qwen2-72B,hellaswag,10-shot,acc_norm,0.8727345150368453,0.0033258902255298
Qwen/Qwen2-72B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.67,0.047258156262526
Qwen/Qwen2-72B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.67,0.047258156262526
Qwen/Qwen2-72B,hendrycksTest-anatomy,5-shot,accuracy,0.8,0.0345547370232543
Qwen/Qwen2-72B,hendrycksTest-anatomy,5-shot,acc_norm,0.8,0.0345547370232543
Qwen/Qwen2-72B,hendrycksTest-astronomy,5-shot,accuracy,0.9210526315789472,0.0219443428182479
Qwen/Qwen2-72B,hendrycksTest-astronomy,5-shot,acc_norm,0.9210526315789472,0.0219443428182479
Qwen/Qwen2-72B,hendrycksTest-business_ethics,5-shot,accuracy,0.8,0.0402015126103684
Qwen/Qwen2-72B,hendrycksTest-business_ethics,5-shot,acc_norm,0.8,0.0402015126103684
Qwen/Qwen2-72B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8716981132075472,0.0205824756879918
Qwen/Qwen2-72B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8716981132075472,0.0205824756879918
Qwen/Qwen2-72B,hendrycksTest-college_biology,5-shot,accuracy,0.9305555555555556,0.021257974822832
Qwen/Qwen2-72B,hendrycksTest-college_biology,5-shot,acc_norm,0.9305555555555556,0.021257974822832
Qwen/Qwen2-72B,hendrycksTest-college_chemistry,5-shot,accuracy,0.6,0.049236596391733
Qwen/Qwen2-72B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.6,0.049236596391733
Qwen/Qwen2-72B,hendrycksTest-college_computer_science,5-shot,accuracy,0.81,0.0394277244403662
Qwen/Qwen2-72B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.81,0.0394277244403662
Qwen/Qwen2-72B,hendrycksTest-college_mathematics,5-shot,accuracy,0.65,0.0479372485441102
Qwen/Qwen2-72B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.65,0.0479372485441102
Qwen/Qwen2-72B,hendrycksTest-college_medicine,5-shot,accuracy,0.838150289017341,0.0280835942795757
Qwen/Qwen2-72B,hendrycksTest-college_medicine,5-shot,acc_norm,0.838150289017341,0.0280835942795757
Qwen/Qwen2-72B,hendrycksTest-college_physics,5-shot,accuracy,0.6568627450980392,0.0472400735238388
Qwen/Qwen2-72B,hendrycksTest-college_physics,5-shot,acc_norm,0.6568627450980392,0.0472400735238388
Qwen/Qwen2-72B,hendrycksTest-computer_security,5-shot,accuracy,0.84,0.036845294917747
Qwen/Qwen2-72B,hendrycksTest-computer_security,5-shot,acc_norm,0.84,0.036845294917747
Qwen/Qwen2-72B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.8936170212765957,0.0201559773070498
Qwen/Qwen2-72B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.8936170212765957,0.0201559773070498
Qwen/Qwen2-72B,hendrycksTest-econometrics,5-shot,accuracy,0.7368421052631579,0.0414243971948936
Qwen/Qwen2-72B,hendrycksTest-econometrics,5-shot,acc_norm,0.7368421052631579,0.0414243971948936
Qwen/Qwen2-72B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.8206896551724138,0.0319676643337318
Qwen/Qwen2-72B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.8206896551724138,0.0319676643337318
Qwen/Qwen2-72B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.8862433862433863,0.0163528764804948
Qwen/Qwen2-72B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.8862433862433863,0.0163528764804948
Qwen/Qwen2-72B,hendrycksTest-formal_logic,5-shot,accuracy,0.7380952380952381,0.0393253768039287
Qwen/Qwen2-72B,hendrycksTest-formal_logic,5-shot,acc_norm,0.7380952380952381,0.0393253768039287
Qwen/Qwen2-72B,hendrycksTest-global_facts,5-shot,accuracy,0.63,0.048523658709391
Qwen/Qwen2-72B,hendrycksTest-global_facts,5-shot,acc_norm,0.63,0.048523658709391
Qwen/Qwen2-72B,hendrycksTest-high_school_biology,5-shot,accuracy,0.935483870967742,0.0139756837055894
Qwen/Qwen2-72B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.935483870967742,0.0139756837055894
Qwen/Qwen2-72B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.7783251231527094,0.0292255758924896
Qwen/Qwen2-72B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.7783251231527094,0.0292255758924896
Qwen/Qwen2-72B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.91,0.0287623491264661
Qwen/Qwen2-72B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.91,0.0287623491264661
Qwen/Qwen2-72B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8848484848484849,0.0249256997981153
Qwen/Qwen2-72B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8848484848484849,0.0249256997981153
Qwen/Qwen2-72B,hendrycksTest-high_school_geography,5-shot,accuracy,0.9393939393939394,0.0169999949274216
Qwen/Qwen2-72B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9393939393939394,0.0169999949274216
Qwen/Qwen2-72B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9896373056994818,0.0073084243867921
Qwen/Qwen2-72B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9896373056994818,0.0073084243867921
Qwen/Qwen2-72B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.882051282051282,0.0163538017783034
Qwen/Qwen2-72B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.882051282051282,0.0163538017783034
Qwen/Qwen2-72B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.6851851851851852,0.0283175334960664
Qwen/Qwen2-72B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.6851851851851852,0.0283175334960664
Qwen/Qwen2-72B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.9369747899159664,0.0157850852236709
Qwen/Qwen2-72B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.9369747899159664,0.0157850852236709
Qwen/Qwen2-72B,hendrycksTest-high_school_physics,5-shot,accuracy,0.7019867549668874,0.0373453567678719
Qwen/Qwen2-72B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.7019867549668874,0.0373453567678719
Qwen/Qwen2-72B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9357798165137616,0.0105104947132013
Qwen/Qwen2-72B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9357798165137616,0.0105104947132013
Qwen/Qwen2-72B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.7870370370370371,0.0279209631479936
Qwen/Qwen2-72B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.7870370370370371,0.0279209631479936
Qwen/Qwen2-72B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.946078431372549,0.0158524652811069
Qwen/Qwen2-72B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.946078431372549,0.0158524652811069
Qwen/Qwen2-72B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9324894514767932,0.0163324666732443
Qwen/Qwen2-72B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9324894514767932,0.0163324666732443
Qwen/Qwen2-72B,hendrycksTest-human_aging,5-shot,accuracy,0.8699551569506726,0.0225745194241748
Qwen/Qwen2-72B,hendrycksTest-human_aging,5-shot,acc_norm,0.8699551569506726,0.0225745194241748
Qwen/Qwen2-72B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8854961832061069,0.0279274737535974
Qwen/Qwen2-72B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8854961832061069,0.0279274737535974
Qwen/Qwen2-72B,hendrycksTest-international_law,5-shot,accuracy,0.8842975206611571,0.0291998024556228
Qwen/Qwen2-72B,hendrycksTest-international_law,5-shot,acc_norm,0.8842975206611571,0.0291998024556228
Qwen/Qwen2-72B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8796296296296297,0.0314570385430624
Qwen/Qwen2-72B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8796296296296297,0.0314570385430624
Qwen/Qwen2-72B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8957055214723927,0.024013517319439
Qwen/Qwen2-72B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8957055214723927,0.024013517319439
Qwen/Qwen2-72B,hendrycksTest-machine_learning,5-shot,accuracy,0.7589285714285714,0.0405986724695268
Qwen/Qwen2-72B,hendrycksTest-machine_learning,5-shot,acc_norm,0.7589285714285714,0.0405986724695268
Qwen/Qwen2-72B,hendrycksTest-management,5-shot,accuracy,0.9223300970873788,0.0265014407847627
Qwen/Qwen2-72B,hendrycksTest-management,5-shot,acc_norm,0.9223300970873788,0.0265014407847627
Qwen/Qwen2-72B,hendrycksTest-marketing,5-shot,accuracy,0.9529914529914528,0.0138661200585948
Qwen/Qwen2-72B,hendrycksTest-marketing,5-shot,acc_norm,0.9529914529914528,0.0138661200585948
Qwen/Qwen2-72B,hendrycksTest-medical_genetics,5-shot,accuracy,0.9,0.0301511344577763
Qwen/Qwen2-72B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.9,0.0301511344577763
Qwen/Qwen2-72B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9450830140485312,0.0081467605007523
Qwen/Qwen2-72B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9450830140485312,0.0081467605007523
Qwen/Qwen2-72B,hendrycksTest-moral_disputes,5-shot,accuracy,0.8728323699421965,0.0179367668651498
Qwen/Qwen2-72B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8728323699421965,0.0179367668651498
Qwen/Qwen2-72B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.8268156424581006,0.0126558090686448
Qwen/Qwen2-72B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.8268156424581006,0.0126558090686448
Qwen/Qwen2-72B,hendrycksTest-nutrition,5-shot,accuracy,0.9052287581699346,0.0167713312718364
Qwen/Qwen2-72B,hendrycksTest-nutrition,5-shot,acc_norm,0.9052287581699346,0.0167713312718364
Qwen/Qwen2-72B,hendrycksTest-philosophy,5-shot,accuracy,0.8745980707395499,0.0188094250052061
Qwen/Qwen2-72B,hendrycksTest-philosophy,5-shot,acc_norm,0.8745980707395499,0.0188094250052061
Qwen/Qwen2-72B,hendrycksTest-prehistory,5-shot,accuracy,0.9074074074074074,0.0161282787618244
Qwen/Qwen2-72B,hendrycksTest-prehistory,5-shot,acc_norm,0.9074074074074074,0.0161282787618244
Qwen/Qwen2-72B,hendrycksTest-professional_accounting,5-shot,accuracy,0.7411347517730497,0.0261295725271808
Qwen/Qwen2-72B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.7411347517730497,0.0261295725271808
Qwen/Qwen2-72B,hendrycksTest-professional_law,5-shot,accuracy,0.6864406779661016,0.0118492342914593
Qwen/Qwen2-72B,hendrycksTest-professional_law,5-shot,acc_norm,0.6864406779661016,0.0118492342914593
Qwen/Qwen2-72B,hendrycksTest-professional_medicine,5-shot,accuracy,0.9044117647058824,0.0178607905685156
Qwen/Qwen2-72B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.9044117647058824,0.0178607905685156
Qwen/Qwen2-72B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8872549019607843,0.012795357747288
Qwen/Qwen2-72B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8872549019607843,0.012795357747288
Qwen/Qwen2-72B,hendrycksTest-public_relations,5-shot,accuracy,0.7454545454545455,0.0417234303870538
Qwen/Qwen2-72B,hendrycksTest-public_relations,5-shot,acc_norm,0.7454545454545455,0.0417234303870538
Qwen/Qwen2-72B,hendrycksTest-security_studies,5-shot,accuracy,0.8408163265306122,0.0234209720691663
Qwen/Qwen2-72B,hendrycksTest-security_studies,5-shot,acc_norm,0.8408163265306122,0.0234209720691663
Qwen/Qwen2-72B,hendrycksTest-sociology,5-shot,accuracy,0.945273631840796,0.0160828157962632
Qwen/Qwen2-72B,hendrycksTest-sociology,5-shot,acc_norm,0.945273631840796,0.0160828157962632
Qwen/Qwen2-72B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.94,0.0238683256575942
Qwen/Qwen2-72B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.94,0.0238683256575942
Qwen/Qwen2-72B,hendrycksTest-virology,5-shot,accuracy,0.572289156626506,0.0385159768371853
Qwen/Qwen2-72B,hendrycksTest-virology,5-shot,acc_norm,0.572289156626506,0.0385159768371853
Qwen/Qwen2-72B,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.0251729843501557
Qwen/Qwen2-72B,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.0251729843501557
Qwen/Qwen2-72B,truthfulqa:mc,0-shot,mc1,0.3733170134638923,0.0169323705575706
Qwen/Qwen2-72B,truthfulqa:mc,0-shot,mc2,0.5473731276983239,0.0145197495819032
Qwen/Qwen2-72B,winogrande,5-shot,accuracy,0.8445146014206788,0.0101843082147757
Qwen/Qwen2-72B,gsm8k,5-shot,accuracy,0.8529188779378317,0.0097560636603598
EleutherAI/pythia-12b-deduped,drop,3-shot,em,0.0008389261744966,0.0002964962989801
EleutherAI/pythia-2.8b-deduped,drop,3-shot,em,0.0012583892617449,0.0003630560893119
EleutherAI/gpt-neox-20b,drop,3-shot,em,0.001363255033557,0.000377860919646
01-ai/Yi-34B,drop,3-shot,em,0.6081166107382551,0.0049993266298801
01-ai/Yi-34B-200K,arc:challenge,25-shot,accuracy,0.6228668941979523,0.0141633668961926
01-ai/Yi-34B-200K,arc:challenge,25-shot,acc_norm,0.6578498293515358,0.0138641521591772
01-ai/Yi-34B-200K,hellaswag,10-shot,accuracy,0.6189006174068911,0.0048466437356665
01-ai/Yi-34B-200K,hellaswag,10-shot,acc_norm,0.8205536745668194,0.0038294138051139
01-ai/Yi-34B-200K,hendrycksTest-abstract_algebra,5-shot,accuracy,0.42,0.0496044963748858
01-ai/Yi-34B-200K,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.42,0.0496044963748858
01-ai/Yi-34B-200K,hendrycksTest-anatomy,5-shot,accuracy,0.7333333333333333,0.038201699145179
01-ai/Yi-34B-200K,hendrycksTest-anatomy,5-shot,acc_norm,0.7333333333333333,0.038201699145179
01-ai/Yi-34B-200K,hendrycksTest-astronomy,5-shot,accuracy,0.8947368421052632,0.0249745334509207
01-ai/Yi-34B-200K,hendrycksTest-astronomy,5-shot,acc_norm,0.8947368421052632,0.0249745334509207
01-ai/Yi-34B-200K,hendrycksTest-business_ethics,5-shot,accuracy,0.78,0.0416333199893226
01-ai/Yi-34B-200K,hendrycksTest-business_ethics,5-shot,acc_norm,0.78,0.0416333199893226
01-ai/Yi-34B-200K,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8113207547169812,0.0240799951300622
01-ai/Yi-34B-200K,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8113207547169812,0.0240799951300622
01-ai/Yi-34B-200K,hendrycksTest-college_biology,5-shot,accuracy,0.8819444444444444,0.0269833465033093
01-ai/Yi-34B-200K,hendrycksTest-college_biology,5-shot,acc_norm,0.8819444444444444,0.0269833465033093
01-ai/Yi-34B-200K,hendrycksTest-college_chemistry,5-shot,accuracy,0.47,0.0501613558046591
01-ai/Yi-34B-200K,hendrycksTest-college_chemistry,5-shot,acc_norm,0.47,0.0501613558046591
01-ai/Yi-34B-200K,hendrycksTest-college_computer_science,5-shot,accuracy,0.63,0.048523658709391
01-ai/Yi-34B-200K,hendrycksTest-college_computer_science,5-shot,acc_norm,0.63,0.048523658709391
01-ai/Yi-34B-200K,hendrycksTest-college_mathematics,5-shot,accuracy,0.4,0.049236596391733
01-ai/Yi-34B-200K,hendrycksTest-college_mathematics,5-shot,acc_norm,0.4,0.049236596391733
01-ai/Yi-34B-200K,hendrycksTest-college_medicine,5-shot,accuracy,0.7109826589595376,0.0345642574508699
01-ai/Yi-34B-200K,hendrycksTest-college_medicine,5-shot,acc_norm,0.7109826589595376,0.0345642574508699
01-ai/Yi-34B-200K,hendrycksTest-college_physics,5-shot,accuracy,0.5490196078431373,0.0495121825239626
01-ai/Yi-34B-200K,hendrycksTest-college_physics,5-shot,acc_norm,0.5490196078431373,0.0495121825239626
01-ai/Yi-34B-200K,hendrycksTest-computer_security,5-shot,accuracy,0.8,0.0402015126103684
01-ai/Yi-34B-200K,hendrycksTest-computer_security,5-shot,acc_norm,0.8,0.0402015126103684
01-ai/Yi-34B-200K,hendrycksTest-conceptual_physics,5-shot,accuracy,0.774468085106383,0.0273210784173875
01-ai/Yi-34B-200K,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.774468085106383,0.0273210784173875
01-ai/Yi-34B-200K,hendrycksTest-econometrics,5-shot,accuracy,0.5614035087719298,0.0466800073851045
01-ai/Yi-34B-200K,hendrycksTest-econometrics,5-shot,acc_norm,0.5614035087719298,0.0466800073851045
01-ai/Yi-34B-200K,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7724137931034483,0.0349395038013118
01-ai/Yi-34B-200K,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7724137931034483,0.0349395038013118
01-ai/Yi-34B-200K,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.6455026455026455,0.024636830602842
01-ai/Yi-34B-200K,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.6455026455026455,0.024636830602842
01-ai/Yi-34B-200K,hendrycksTest-formal_logic,5-shot,accuracy,0.5158730158730159,0.044698818540726
01-ai/Yi-34B-200K,hendrycksTest-formal_logic,5-shot,acc_norm,0.5158730158730159,0.044698818540726
01-ai/Yi-34B-200K,hendrycksTest-global_facts,5-shot,accuracy,0.57,0.0497569851956242
01-ai/Yi-34B-200K,hendrycksTest-global_facts,5-shot,acc_norm,0.57,0.0497569851956242
01-ai/Yi-34B-200K,hendrycksTest-high_school_biology,5-shot,accuracy,0.896774193548387,0.0173083812810345
01-ai/Yi-34B-200K,hendrycksTest-high_school_biology,5-shot,acc_norm,0.896774193548387,0.0173083812810345
01-ai/Yi-34B-200K,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6502463054187192,0.0335540090496956
01-ai/Yi-34B-200K,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6502463054187192,0.0335540090496956
01-ai/Yi-34B-200K,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.82,0.0386122919665369
01-ai/Yi-34B-200K,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.82,0.0386122919665369
01-ai/Yi-34B-200K,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8484848484848485,0.0279980737987816
01-ai/Yi-34B-200K,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8484848484848485,0.0279980737987816
01-ai/Yi-34B-200K,hendrycksTest-high_school_geography,5-shot,accuracy,0.9292929292929292,0.0182631054201995
01-ai/Yi-34B-200K,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9292929292929292,0.0182631054201995
01-ai/Yi-34B-200K,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9740932642487048,0.0114645233569531
01-ai/Yi-34B-200K,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9740932642487048,0.0114645233569531
01-ai/Yi-34B-200K,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.782051282051282,0.0209324457744631
01-ai/Yi-34B-200K,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.782051282051282,0.0209324457744631
01-ai/Yi-34B-200K,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3814814814814815,0.0296167189274975
01-ai/Yi-34B-200K,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3814814814814815,0.0296167189274975
01-ai/Yi-34B-200K,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8403361344537815,0.0237933539975288
01-ai/Yi-34B-200K,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8403361344537815,0.0237933539975288
01-ai/Yi-34B-200K,hendrycksTest-high_school_physics,5-shot,accuracy,0.4635761589403973,0.0407163606594421
01-ai/Yi-34B-200K,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4635761589403973,0.0407163606594421
01-ai/Yi-34B-200K,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9211009174311928,0.0115581981137695
01-ai/Yi-34B-200K,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9211009174311928,0.0115581981137695
01-ai/Yi-34B-200K,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6712962962962963,0.0320361408467005
01-ai/Yi-34B-200K,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6712962962962963,0.0320361408467005
01-ai/Yi-34B-200K,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9068627450980392,0.0203978539694269
01-ai/Yi-34B-200K,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9068627450980392,0.0203978539694269
01-ai/Yi-34B-200K,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9071729957805909,0.0188897505509567
01-ai/Yi-34B-200K,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9071729957805909,0.0188897505509567
01-ai/Yi-34B-200K,hendrycksTest-human_aging,5-shot,accuracy,0.7937219730941704,0.0271571504795638
01-ai/Yi-34B-200K,hendrycksTest-human_aging,5-shot,acc_norm,0.7937219730941704,0.0271571504795638
01-ai/Yi-34B-200K,hendrycksTest-human_sexuality,5-shot,accuracy,0.8625954198473282,0.0301948239968044
01-ai/Yi-34B-200K,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8625954198473282,0.0301948239968044
01-ai/Yi-34B-200K,hendrycksTest-international_law,5-shot,accuracy,0.9173553719008264,0.0251353823566042
01-ai/Yi-34B-200K,hendrycksTest-international_law,5-shot,acc_norm,0.9173553719008264,0.0251353823566042
01-ai/Yi-34B-200K,hendrycksTest-jurisprudence,5-shot,accuracy,0.8888888888888888,0.0303815967566516
01-ai/Yi-34B-200K,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8888888888888888,0.0303815967566516
01-ai/Yi-34B-200K,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8834355828220859,0.025212327210507
01-ai/Yi-34B-200K,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8834355828220859,0.025212327210507
01-ai/Yi-34B-200K,hendrycksTest-machine_learning,5-shot,accuracy,0.5803571428571429,0.046840993210771
01-ai/Yi-34B-200K,hendrycksTest-machine_learning,5-shot,acc_norm,0.5803571428571429,0.046840993210771
01-ai/Yi-34B-200K,hendrycksTest-management,5-shot,accuracy,0.8932038834951457,0.0305810889283313
01-ai/Yi-34B-200K,hendrycksTest-management,5-shot,acc_norm,0.8932038834951457,0.0305810889283313
01-ai/Yi-34B-200K,hendrycksTest-marketing,5-shot,accuracy,0.9401709401709402,0.0155375142632538
01-ai/Yi-34B-200K,hendrycksTest-marketing,5-shot,acc_norm,0.9401709401709402,0.0155375142632538
01-ai/Yi-34B-200K,hendrycksTest-medical_genetics,5-shot,accuracy,0.88,0.032659863237109
01-ai/Yi-34B-200K,hendrycksTest-medical_genetics,5-shot,acc_norm,0.88,0.032659863237109
01-ai/Yi-34B-200K,hendrycksTest-miscellaneous,5-shot,accuracy,0.9029374201787996,0.0105864747120182
01-ai/Yi-34B-200K,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9029374201787996,0.0105864747120182
01-ai/Yi-34B-200K,hendrycksTest-moral_disputes,5-shot,accuracy,0.8034682080924855,0.0213939614043638
01-ai/Yi-34B-200K,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8034682080924855,0.0213939614043638
01-ai/Yi-34B-200K,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6759776536312849,0.0156525424964211
01-ai/Yi-34B-200K,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6759776536312849,0.0156525424964211
01-ai/Yi-34B-200K,hendrycksTest-nutrition,5-shot,accuracy,0.8464052287581699,0.0206455979104187
01-ai/Yi-34B-200K,hendrycksTest-nutrition,5-shot,acc_norm,0.8464052287581699,0.0206455979104187
01-ai/Yi-34B-200K,hendrycksTest-philosophy,5-shot,accuracy,0.819935691318328,0.0218234228577449
01-ai/Yi-34B-200K,hendrycksTest-philosophy,5-shot,acc_norm,0.819935691318328,0.0218234228577449
01-ai/Yi-34B-200K,hendrycksTest-prehistory,5-shot,accuracy,0.8364197530864198,0.0205814661382571
01-ai/Yi-34B-200K,hendrycksTest-prehistory,5-shot,acc_norm,0.8364197530864198,0.0205814661382571
01-ai/Yi-34B-200K,hendrycksTest-professional_accounting,5-shot,accuracy,0.6134751773049646,0.0290491903425434
01-ai/Yi-34B-200K,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6134751773049646,0.0290491903425434
01-ai/Yi-34B-200K,hendrycksTest-professional_law,5-shot,accuracy,0.5912646675358539,0.0125557013467033
01-ai/Yi-34B-200K,hendrycksTest-professional_law,5-shot,acc_norm,0.5912646675358539,0.0125557013467033
01-ai/Yi-34B-200K,hendrycksTest-professional_medicine,5-shot,accuracy,0.8088235294117647,0.0238868819224403
01-ai/Yi-34B-200K,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8088235294117647,0.0238868819224403
01-ai/Yi-34B-200K,hendrycksTest-professional_psychology,5-shot,accuracy,0.8202614379084967,0.0155337450833827
01-ai/Yi-34B-200K,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8202614379084967,0.0155337450833827
01-ai/Yi-34B-200K,hendrycksTest-public_relations,5-shot,accuracy,0.7272727272727273,0.0426579211094058
01-ai/Yi-34B-200K,hendrycksTest-public_relations,5-shot,acc_norm,0.7272727272727273,0.0426579211094058
01-ai/Yi-34B-200K,hendrycksTest-security_studies,5-shot,accuracy,0.8204081632653061,0.0245732935895856
01-ai/Yi-34B-200K,hendrycksTest-security_studies,5-shot,acc_norm,0.8204081632653061,0.0245732935895856
01-ai/Yi-34B-200K,hendrycksTest-sociology,5-shot,accuracy,0.8805970149253731,0.0229287932772197
01-ai/Yi-34B-200K,hendrycksTest-sociology,5-shot,acc_norm,0.8805970149253731,0.0229287932772197
01-ai/Yi-34B-200K,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
01-ai/Yi-34B-200K,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.0272659924344291
01-ai/Yi-34B-200K,hendrycksTest-virology,5-shot,accuracy,0.572289156626506,0.0385159768371853
01-ai/Yi-34B-200K,hendrycksTest-virology,5-shot,acc_norm,0.572289156626506,0.0385159768371853
01-ai/Yi-34B-200K,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.0251729843501557
01-ai/Yi-34B-200K,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.0251729843501557
01-ai/Yi-34B-200K,truthfulqa:mc,0-shot,mc1,0.2974296205630355,0.016002651487361
01-ai/Yi-34B-200K,truthfulqa:mc,0-shot,mc2,0.4260214587740499,0.0142185881661507
01-ai/Yi-34B-200K,winogrande,5-shot,accuracy,0.8287292817679558,0.0105884172949625
01-ai/Yi-34B-200K,gsm8k,5-shot,accuracy,0.3487490523123578,0.0131272270550358
Salesforce/codegen-6B-nl,arc:challenge,25-shot,accuracy,0.3899317406143344,0.0142529598488928
Salesforce/codegen-6B-nl,arc:challenge,25-shot,acc_norm,0.4232081911262799,0.014438036220848
Salesforce/codegen-6B-nl,hellaswag,10-shot,accuracy,0.5052778331009758,0.0049895034177672
Salesforce/codegen-6B-nl,hellaswag,10-shot,acc_norm,0.6859191396136228,0.0046320017323329
Salesforce/codegen-6B-nl,hendrycksTest-abstract_algebra,5-shot,accuracy,0.21,0.0409360180740332
Salesforce/codegen-6B-nl,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.21,0.0409360180740332
Salesforce/codegen-6B-nl,hendrycksTest-anatomy,5-shot,accuracy,0.2296296296296296,0.0363338441407346
Salesforce/codegen-6B-nl,hendrycksTest-anatomy,5-shot,acc_norm,0.2296296296296296,0.0363338441407346
Salesforce/codegen-6B-nl,hendrycksTest-astronomy,5-shot,accuracy,0.1842105263157894,0.0315469804508223
Salesforce/codegen-6B-nl,hendrycksTest-astronomy,5-shot,acc_norm,0.1842105263157894,0.0315469804508223
Salesforce/codegen-6B-nl,hendrycksTest-business_ethics,5-shot,accuracy,0.31,0.0464823198711731
Salesforce/codegen-6B-nl,hendrycksTest-business_ethics,5-shot,acc_norm,0.31,0.0464823198711731
Salesforce/codegen-6B-nl,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2754716981132075,0.027495663683724
Salesforce/codegen-6B-nl,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2754716981132075,0.027495663683724
Salesforce/codegen-6B-nl,hendrycksTest-college_biology,5-shot,accuracy,0.2986111111111111,0.0382705235795075
Salesforce/codegen-6B-nl,hendrycksTest-college_biology,5-shot,acc_norm,0.2986111111111111,0.0382705235795075
Salesforce/codegen-6B-nl,hendrycksTest-college_chemistry,5-shot,accuracy,0.15,0.0358870281282637
Salesforce/codegen-6B-nl,hendrycksTest-college_chemistry,5-shot,acc_norm,0.15,0.0358870281282637
Salesforce/codegen-6B-nl,hendrycksTest-college_computer_science,5-shot,accuracy,0.29,0.0456048021572068
Salesforce/codegen-6B-nl,hendrycksTest-college_computer_science,5-shot,acc_norm,0.29,0.0456048021572068
Salesforce/codegen-6B-nl,hendrycksTest-college_mathematics,5-shot,accuracy,0.23,0.042295258468165
Salesforce/codegen-6B-nl,hendrycksTest-college_mathematics,5-shot,acc_norm,0.23,0.042295258468165
Salesforce/codegen-6B-nl,hendrycksTest-college_medicine,5-shot,accuracy,0.2601156069364161,0.0334503691678899
Salesforce/codegen-6B-nl,hendrycksTest-college_medicine,5-shot,acc_norm,0.2601156069364161,0.0334503691678899
Salesforce/codegen-6B-nl,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
Salesforce/codegen-6B-nl,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
Salesforce/codegen-6B-nl,hendrycksTest-computer_security,5-shot,accuracy,0.37,0.0485236587093909
Salesforce/codegen-6B-nl,hendrycksTest-computer_security,5-shot,acc_norm,0.37,0.0485236587093909
Salesforce/codegen-6B-nl,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2595744680851063,0.0286591793742923
Salesforce/codegen-6B-nl,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2595744680851063,0.0286591793742923
Salesforce/codegen-6B-nl,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984366
Salesforce/codegen-6B-nl,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.0409698513984366
Salesforce/codegen-6B-nl,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.035659981741353
Salesforce/codegen-6B-nl,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.035659981741353
Salesforce/codegen-6B-nl,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2513227513227513,0.0223404823396438
Salesforce/codegen-6B-nl,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2513227513227513,0.0223404823396438
Salesforce/codegen-6B-nl,hendrycksTest-formal_logic,5-shot,accuracy,0.2857142857142857,0.0404061017820884
Salesforce/codegen-6B-nl,hendrycksTest-formal_logic,5-shot,acc_norm,0.2857142857142857,0.0404061017820884
Salesforce/codegen-6B-nl,hendrycksTest-global_facts,5-shot,accuracy,0.25,0.0435194139889244
Salesforce/codegen-6B-nl,hendrycksTest-global_facts,5-shot,acc_norm,0.25,0.0435194139889244
Salesforce/codegen-6B-nl,hendrycksTest-high_school_biology,5-shot,accuracy,0.2935483870967741,0.0259060870213192
Salesforce/codegen-6B-nl,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2935483870967741,0.0259060870213192
Salesforce/codegen-6B-nl,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.1625615763546798,0.0259603000646055
Salesforce/codegen-6B-nl,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.1625615763546798,0.0259603000646055
Salesforce/codegen-6B-nl,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.0429234695990928
Salesforce/codegen-6B-nl,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.0429234695990928
Salesforce/codegen-6B-nl,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2181818181818181,0.0322507810830628
Salesforce/codegen-6B-nl,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2181818181818181,0.0322507810830628
Salesforce/codegen-6B-nl,hendrycksTest-high_school_geography,5-shot,accuracy,0.1868686868686868,0.0277725333342189
Salesforce/codegen-6B-nl,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1868686868686868,0.0277725333342189
Salesforce/codegen-6B-nl,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2279792746113989,0.0302769099451782
Salesforce/codegen-6B-nl,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2279792746113989,0.0302769099451782
Salesforce/codegen-6B-nl,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2358974358974359,0.0215259654074087
Salesforce/codegen-6B-nl,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2358974358974359,0.0215259654074087
Salesforce/codegen-6B-nl,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2074074074074074,0.0247207131939521
Salesforce/codegen-6B-nl,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2074074074074074,0.0247207131939521
Salesforce/codegen-6B-nl,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2226890756302521,0.0270254334988823
Salesforce/codegen-6B-nl,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2226890756302521,0.0270254334988823
Salesforce/codegen-6B-nl,hendrycksTest-high_school_physics,5-shot,accuracy,0.2649006622516556,0.0360303854536038
Salesforce/codegen-6B-nl,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2649006622516556,0.0360303854536038
Salesforce/codegen-6B-nl,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2311926605504587,0.0180757502416331
Salesforce/codegen-6B-nl,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2311926605504587,0.0180757502416331
Salesforce/codegen-6B-nl,hendrycksTest-high_school_statistics,5-shot,accuracy,0.162037037037037,0.0251304536522684
Salesforce/codegen-6B-nl,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.162037037037037,0.0251304536522684
Salesforce/codegen-6B-nl,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2647058823529412,0.0309645179269234
Salesforce/codegen-6B-nl,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2647058823529412,0.0309645179269234
Salesforce/codegen-6B-nl,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2827004219409282,0.0293128141539559
Salesforce/codegen-6B-nl,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2827004219409282,0.0293128141539559
Salesforce/codegen-6B-nl,hendrycksTest-human_aging,5-shot,accuracy,0.3273542600896861,0.0314938467099413
Salesforce/codegen-6B-nl,hendrycksTest-human_aging,5-shot,acc_norm,0.3273542600896861,0.0314938467099413
Salesforce/codegen-6B-nl,hendrycksTest-human_sexuality,5-shot,accuracy,0.2519083969465648,0.0380738711630608
Salesforce/codegen-6B-nl,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2519083969465648,0.0380738711630608
Salesforce/codegen-6B-nl,hendrycksTest-international_law,5-shot,accuracy,0.396694214876033,0.04465869780531
Salesforce/codegen-6B-nl,hendrycksTest-international_law,5-shot,acc_norm,0.396694214876033,0.04465869780531
Salesforce/codegen-6B-nl,hendrycksTest-jurisprudence,5-shot,accuracy,0.287037037037037,0.0437331304091476
Salesforce/codegen-6B-nl,hendrycksTest-jurisprudence,5-shot,acc_norm,0.287037037037037,0.0437331304091476
Salesforce/codegen-6B-nl,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2085889570552147,0.0319219344893472
Salesforce/codegen-6B-nl,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2085889570552147,0.0319219344893472
Salesforce/codegen-6B-nl,hendrycksTest-machine_learning,5-shot,accuracy,0.3125,0.0439946505757152
Salesforce/codegen-6B-nl,hendrycksTest-machine_learning,5-shot,acc_norm,0.3125,0.0439946505757152
Salesforce/codegen-6B-nl,hendrycksTest-management,5-shot,accuracy,0.3106796116504854,0.0458212416016155
Salesforce/codegen-6B-nl,hendrycksTest-management,5-shot,acc_norm,0.3106796116504854,0.0458212416016155
Salesforce/codegen-6B-nl,hendrycksTest-marketing,5-shot,accuracy,0.3076923076923077,0.030236389942173
Salesforce/codegen-6B-nl,hendrycksTest-marketing,5-shot,acc_norm,0.3076923076923077,0.030236389942173
Salesforce/codegen-6B-nl,hendrycksTest-medical_genetics,5-shot,accuracy,0.34,0.0476095228569523
Salesforce/codegen-6B-nl,hendrycksTest-medical_genetics,5-shot,acc_norm,0.34,0.0476095228569523
Salesforce/codegen-6B-nl,hendrycksTest-miscellaneous,5-shot,accuracy,0.2694763729246487,0.015866243073215
Salesforce/codegen-6B-nl,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2694763729246487,0.015866243073215
Salesforce/codegen-6B-nl,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.0237030995252581
Salesforce/codegen-6B-nl,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.0237030995252581
Salesforce/codegen-6B-nl,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2804469273743016,0.0150240838833228
Salesforce/codegen-6B-nl,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2804469273743016,0.0150240838833228
Salesforce/codegen-6B-nl,hendrycksTest-nutrition,5-shot,accuracy,0.2320261437908496,0.024170840879341
Salesforce/codegen-6B-nl,hendrycksTest-nutrition,5-shot,acc_norm,0.2320261437908496,0.024170840879341
Salesforce/codegen-6B-nl,hendrycksTest-philosophy,5-shot,accuracy,0.2218649517684887,0.023598858292863
Salesforce/codegen-6B-nl,hendrycksTest-philosophy,5-shot,acc_norm,0.2218649517684887,0.023598858292863
Salesforce/codegen-6B-nl,hendrycksTest-prehistory,5-shot,accuracy,0.2438271604938271,0.0238918795419596
Salesforce/codegen-6B-nl,hendrycksTest-prehistory,5-shot,acc_norm,0.2438271604938271,0.0238918795419596
Salesforce/codegen-6B-nl,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.0267891723511402
Salesforce/codegen-6B-nl,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.0267891723511402
Salesforce/codegen-6B-nl,hendrycksTest-professional_law,5-shot,accuracy,0.2653194263363755,0.0112761988439588
Salesforce/codegen-6B-nl,hendrycksTest-professional_law,5-shot,acc_norm,0.2653194263363755,0.0112761988439588
Salesforce/codegen-6B-nl,hendrycksTest-professional_medicine,5-shot,accuracy,0.1911764705882352,0.0238868819224403
Salesforce/codegen-6B-nl,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1911764705882352,0.0238868819224403
Salesforce/codegen-6B-nl,hendrycksTest-professional_psychology,5-shot,accuracy,0.284313725490196,0.0182490244112076
Salesforce/codegen-6B-nl,hendrycksTest-professional_psychology,5-shot,acc_norm,0.284313725490196,0.0182490244112076
Salesforce/codegen-6B-nl,hendrycksTest-public_relations,5-shot,accuracy,0.3181818181818182,0.044612721759105
Salesforce/codegen-6B-nl,hendrycksTest-public_relations,5-shot,acc_norm,0.3181818181818182,0.044612721759105
Salesforce/codegen-6B-nl,hendrycksTest-security_studies,5-shot,accuracy,0.2489795918367346,0.0276829795229602
Salesforce/codegen-6B-nl,hendrycksTest-security_studies,5-shot,acc_norm,0.2489795918367346,0.0276829795229602
Salesforce/codegen-6B-nl,hendrycksTest-sociology,5-shot,accuracy,0.2786069651741293,0.031700561834973
Salesforce/codegen-6B-nl,hendrycksTest-sociology,5-shot,acc_norm,0.2786069651741293,0.031700561834973
Salesforce/codegen-6B-nl,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.32,0.046882617226215
Salesforce/codegen-6B-nl,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.32,0.046882617226215
Salesforce/codegen-6B-nl,hendrycksTest-virology,5-shot,accuracy,0.3012048192771084,0.0357160923005348
Salesforce/codegen-6B-nl,hendrycksTest-virology,5-shot,acc_norm,0.3012048192771084,0.0357160923005348
Salesforce/codegen-6B-nl,hendrycksTest-world_religions,5-shot,accuracy,0.2748538011695906,0.0342404292469158
Salesforce/codegen-6B-nl,hendrycksTest-world_religions,5-shot,acc_norm,0.2748538011695906,0.0342404292469158
Salesforce/codegen-6B-nl,truthfulqa:mc,0-shot,mc1,0.208078335373317,0.0142105034735766
Salesforce/codegen-6B-nl,truthfulqa:mc,0-shot,mc2,0.3447046127321627,0.0135414014253698
Salesforce/codegen-6B-nl,drop,3-shot,em,0.0008389261744966,0.0002964962989801
Salesforce/codegen-6B-nl,drop,3-shot,f1,0.0446371644295303,0.0011286825965254
Salesforce/codegen-6B-nl,gsm8k,5-shot,accuracy,0.0219863532979529,0.00403916275811
Salesforce/codegen-6B-nl,winogrande,5-shot,accuracy,0.664561957379637,0.0132695759048514
Dampish/StellarX-4B-V0,drop,3-shot,em,0.0269505033557047,0.0016584048452624
EleutherAI/pythia-12b,drop,3-shot,em,0.0006291946308724,0.0002568002749723
microsoft/phi-1_5,drop,3-shot,em,0.0025167785234899,0.0005131152834514
microsoft/phi-1_5,drop,3-shot,f1,0.050449874161074,0.0013066024783407
microsoft/phi-1_5,arc:challenge,25-shot,accuracy,0.5034129692832765,0.014611050403244
microsoft/phi-1_5,arc:challenge,25-shot,acc_norm,0.5290102389078498,0.0145867763552943
microsoft/phi-1_5,hendrycksTest-abstract_algebra,5-shot,accuracy,0.34,0.0476095228569523
microsoft/phi-1_5,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.34,0.0476095228569523
microsoft/phi-1_5,hendrycksTest-anatomy,5-shot,accuracy,0.4518518518518518,0.0429926890548086
microsoft/phi-1_5,hendrycksTest-anatomy,5-shot,acc_norm,0.4518518518518518,0.0429926890548086
microsoft/phi-1_5,hendrycksTest-astronomy,5-shot,accuracy,0.4013157894736842,0.0398890370333628
microsoft/phi-1_5,hendrycksTest-astronomy,5-shot,acc_norm,0.4013157894736842,0.0398890370333628
microsoft/phi-1_5,hendrycksTest-business_ethics,5-shot,accuracy,0.52,0.0502116731568677
microsoft/phi-1_5,hendrycksTest-business_ethics,5-shot,acc_norm,0.52,0.0502116731568677
microsoft/phi-1_5,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.5018867924528302,0.0307726536420756
microsoft/phi-1_5,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.5018867924528302,0.0307726536420756
microsoft/phi-1_5,hendrycksTest-college_biology,5-shot,accuracy,0.3680555555555556,0.0403299905396071
microsoft/phi-1_5,hendrycksTest-college_biology,5-shot,acc_norm,0.3680555555555556,0.0403299905396071
microsoft/phi-1_5,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.0440844002276807
microsoft/phi-1_5,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.0440844002276807
microsoft/phi-1_5,hendrycksTest-college_computer_science,5-shot,accuracy,0.46,0.0500908265962033
microsoft/phi-1_5,hendrycksTest-college_computer_science,5-shot,acc_norm,0.46,0.0500908265962033
microsoft/phi-1_5,hendrycksTest-college_mathematics,5-shot,accuracy,0.41,0.049431107042371
microsoft/phi-1_5,hendrycksTest-college_mathematics,5-shot,acc_norm,0.41,0.049431107042371
microsoft/phi-1_5,hendrycksTest-college_medicine,5-shot,accuracy,0.4046242774566474,0.0374246119388724
microsoft/phi-1_5,hendrycksTest-college_medicine,5-shot,acc_norm,0.4046242774566474,0.0374246119388724
microsoft/phi-1_5,hendrycksTest-college_physics,5-shot,accuracy,0.2745098039215686,0.0444052190617932
microsoft/phi-1_5,hendrycksTest-college_physics,5-shot,acc_norm,0.2745098039215686,0.0444052190617932
microsoft/phi-1_5,hendrycksTest-computer_security,5-shot,accuracy,0.54,0.0500908265962033
microsoft/phi-1_5,hendrycksTest-computer_security,5-shot,acc_norm,0.54,0.0500908265962033
microsoft/phi-1_5,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3659574468085106,0.0314895582974553
microsoft/phi-1_5,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3659574468085106,0.0314895582974553
microsoft/phi-1_5,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
microsoft/phi-1_5,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
microsoft/phi-1_5,hendrycksTest-electrical_engineering,5-shot,accuracy,0.4827586206896552,0.0416418872016937
microsoft/phi-1_5,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.4827586206896552,0.0416418872016937
microsoft/phi-1_5,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3015873015873015,0.0236369759961018
microsoft/phi-1_5,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3015873015873015,0.0236369759961018
microsoft/phi-1_5,hendrycksTest-formal_logic,5-shot,accuracy,0.2619047619047619,0.0393253768039287
microsoft/phi-1_5,hendrycksTest-formal_logic,5-shot,acc_norm,0.2619047619047619,0.0393253768039287
microsoft/phi-1_5,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.0460566186471838
microsoft/phi-1_5,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.0460566186471838
microsoft/phi-1_5,hendrycksTest-high_school_biology,5-shot,accuracy,0.4580645161290322,0.0283437872505406
microsoft/phi-1_5,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4580645161290322,0.0283437872505406
microsoft/phi-1_5,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3300492610837438,0.0330853042622825
microsoft/phi-1_5,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3300492610837438,0.0330853042622825
microsoft/phi-1_5,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.47,0.0501613558046591
microsoft/phi-1_5,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.47,0.0501613558046591
microsoft/phi-1_5,hendrycksTest-high_school_european_history,5-shot,accuracy,0.4848484848484848,0.0390255100737444
microsoft/phi-1_5,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.4848484848484848,0.0390255100737444
microsoft/phi-1_5,hendrycksTest-high_school_geography,5-shot,accuracy,0.5202020202020202,0.0355944356556391
microsoft/phi-1_5,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5202020202020202,0.0355944356556391
microsoft/phi-1_5,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.5492227979274611,0.0359091095223552
microsoft/phi-1_5,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.5492227979274611,0.0359091095223552
microsoft/phi-1_5,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4205128205128205,0.0250286102767108
microsoft/phi-1_5,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4205128205128205,0.0250286102767108
microsoft/phi-1_5,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2407407407407407,0.0260671592222757
microsoft/phi-1_5,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2407407407407407,0.0260671592222757
microsoft/phi-1_5,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.453781512605042,0.0323394346818208
microsoft/phi-1_5,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.453781512605042,0.0323394346818208
microsoft/phi-1_5,hendrycksTest-high_school_physics,5-shot,accuracy,0.2781456953642384,0.0365860326276374
microsoft/phi-1_5,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2781456953642384,0.0365860326276374
microsoft/phi-1_5,hendrycksTest-high_school_psychology,5-shot,accuracy,0.5577981651376147,0.0212936132075201
microsoft/phi-1_5,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.5577981651376147,0.0212936132075201
microsoft/phi-1_5,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2824074074074074,0.0307013721115109
microsoft/phi-1_5,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2824074074074074,0.0307013721115109
microsoft/phi-1_5,hendrycksTest-high_school_us_history,5-shot,accuracy,0.4607843137254901,0.0349850164936952
microsoft/phi-1_5,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.4607843137254901,0.0349850164936952
microsoft/phi-1_5,hendrycksTest-high_school_world_history,5-shot,accuracy,0.510548523206751,0.0325399837916628
microsoft/phi-1_5,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.510548523206751,0.0325399837916628
microsoft/phi-1_5,hendrycksTest-human_aging,5-shot,accuracy,0.4887892376681614,0.0335493665309847
microsoft/phi-1_5,hendrycksTest-human_aging,5-shot,acc_norm,0.4887892376681614,0.0335493665309847
microsoft/phi-1_5,hendrycksTest-human_sexuality,5-shot,accuracy,0.465648854961832,0.0437492856059973
microsoft/phi-1_5,hendrycksTest-human_sexuality,5-shot,acc_norm,0.465648854961832,0.0437492856059973
microsoft/phi-1_5,hendrycksTest-international_law,5-shot,accuracy,0.6363636363636364,0.0439132628672407
microsoft/phi-1_5,hendrycksTest-international_law,5-shot,acc_norm,0.6363636363636364,0.0439132628672407
microsoft/phi-1_5,hendrycksTest-jurisprudence,5-shot,accuracy,0.5277777777777778,0.0482621729413989
microsoft/phi-1_5,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5277777777777778,0.0482621729413989
microsoft/phi-1_5,hendrycksTest-logical_fallacies,5-shot,accuracy,0.50920245398773,0.0392770560078744
microsoft/phi-1_5,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.50920245398773,0.0392770560078744
microsoft/phi-1_5,hendrycksTest-machine_learning,5-shot,accuracy,0.3839285714285714,0.0461614307502854
microsoft/phi-1_5,hendrycksTest-machine_learning,5-shot,acc_norm,0.3839285714285714,0.0461614307502854
microsoft/phi-1_5,hendrycksTest-management,5-shot,accuracy,0.5825242718446602,0.0488284054821223
microsoft/phi-1_5,hendrycksTest-management,5-shot,acc_norm,0.5825242718446602,0.0488284054821223
microsoft/phi-1_5,hendrycksTest-marketing,5-shot,accuracy,0.7051282051282052,0.0298725777088911
microsoft/phi-1_5,hendrycksTest-marketing,5-shot,acc_norm,0.7051282051282052,0.0298725777088911
microsoft/phi-1_5,hendrycksTest-medical_genetics,5-shot,accuracy,0.51,0.0502418393795691
microsoft/phi-1_5,hendrycksTest-medical_genetics,5-shot,acc_norm,0.51,0.0502418393795691
microsoft/phi-1_5,hendrycksTest-miscellaneous,5-shot,accuracy,0.5044699872286079,0.0178792489705843
microsoft/phi-1_5,hendrycksTest-miscellaneous,5-shot,acc_norm,0.5044699872286079,0.0178792489705843
microsoft/phi-1_5,hendrycksTest-moral_disputes,5-shot,accuracy,0.5491329479768786,0.0267888119315627
microsoft/phi-1_5,hendrycksTest-moral_disputes,5-shot,acc_norm,0.5491329479768786,0.0267888119315627
microsoft/phi-1_5,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2480446927374301,0.0144441578082614
microsoft/phi-1_5,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2480446927374301,0.0144441578082614
microsoft/phi-1_5,hendrycksTest-nutrition,5-shot,accuracy,0.5032679738562091,0.0286293051940035
microsoft/phi-1_5,hendrycksTest-nutrition,5-shot,acc_norm,0.5032679738562091,0.0286293051940035
microsoft/phi-1_5,hendrycksTest-philosophy,5-shot,accuracy,0.4823151125401929,0.0283803228490771
microsoft/phi-1_5,hendrycksTest-philosophy,5-shot,acc_norm,0.4823151125401929,0.0283803228490771
microsoft/phi-1_5,hendrycksTest-prehistory,5-shot,accuracy,0.4290123456790123,0.0275389256134708
microsoft/phi-1_5,hendrycksTest-prehistory,5-shot,acc_norm,0.4290123456790123,0.0275389256134708
microsoft/phi-1_5,hendrycksTest-professional_accounting,5-shot,accuracy,0.3120567375886525,0.0276401205451699
microsoft/phi-1_5,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3120567375886525,0.0276401205451699
microsoft/phi-1_5,hendrycksTest-professional_law,5-shot,accuracy,0.3455019556714472,0.0121453030040872
microsoft/phi-1_5,hendrycksTest-professional_law,5-shot,acc_norm,0.3455019556714472,0.0121453030040872
microsoft/phi-1_5,hendrycksTest-professional_medicine,5-shot,accuracy,0.3382352941176471,0.0287393285139835
microsoft/phi-1_5,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3382352941176471,0.0287393285139835
microsoft/phi-1_5,hendrycksTest-professional_psychology,5-shot,accuracy,0.4019607843137255,0.0198351764843753
microsoft/phi-1_5,hendrycksTest-professional_psychology,5-shot,acc_norm,0.4019607843137255,0.0198351764843753
microsoft/phi-1_5,hendrycksTest-public_relations,5-shot,accuracy,0.509090909090909,0.0478833976870286
microsoft/phi-1_5,hendrycksTest-public_relations,5-shot,acc_norm,0.509090909090909,0.0478833976870286
microsoft/phi-1_5,hendrycksTest-security_studies,5-shot,accuracy,0.4938775510204081,0.032006820201639
microsoft/phi-1_5,hendrycksTest-security_studies,5-shot,acc_norm,0.4938775510204081,0.032006820201639
microsoft/phi-1_5,hendrycksTest-sociology,5-shot,accuracy,0.6467661691542289,0.0337979061179677
microsoft/phi-1_5,hendrycksTest-sociology,5-shot,acc_norm,0.6467661691542289,0.0337979061179677
microsoft/phi-1_5,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.64,0.0482418151324421
microsoft/phi-1_5,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.64,0.0482418151324421
microsoft/phi-1_5,hendrycksTest-virology,5-shot,accuracy,0.4096385542168674,0.0382840111507902
microsoft/phi-1_5,hendrycksTest-virology,5-shot,acc_norm,0.4096385542168674,0.0382840111507902
microsoft/phi-1_5,hendrycksTest-world_religions,5-shot,accuracy,0.4678362573099415,0.0382688241766036
microsoft/phi-1_5,hendrycksTest-world_religions,5-shot,acc_norm,0.4678362573099415,0.0382688241766036
microsoft/phi-1_5,truthfulqa:mc,0-shot,mc1,0.2680538555691554,0.0155062047228345
microsoft/phi-1_5,truthfulqa:mc,0-shot,mc2,0.4088993856119402,0.0148424345798732
Qwen/Qwen1.5-32B,arc:challenge,25-shot,accuracy,0.6040955631399317,0.0142912283935365
Qwen/Qwen1.5-32B,arc:challenge,25-shot,acc_norm,0.6339590443686007,0.0140772231084701
Qwen/Qwen1.5-32B,hellaswag,10-shot,accuracy,0.6534554869547898,0.0047489657172142
Qwen/Qwen1.5-32B,hellaswag,10-shot,acc_norm,0.8500298745269866,0.0035631244274585
Qwen/Qwen1.5-32B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.43,0.0497569851956242
Qwen/Qwen1.5-32B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.43,0.0497569851956242
Qwen/Qwen1.5-32B,hendrycksTest-anatomy,5-shot,accuracy,0.6296296296296297,0.0417165416135454
Qwen/Qwen1.5-32B,hendrycksTest-anatomy,5-shot,acc_norm,0.6296296296296297,0.0417165416135454
Qwen/Qwen1.5-32B,hendrycksTest-astronomy,5-shot,accuracy,0.8486842105263158,0.0291626315968439
Qwen/Qwen1.5-32B,hendrycksTest-astronomy,5-shot,acc_norm,0.8486842105263158,0.0291626315968439
Qwen/Qwen1.5-32B,hendrycksTest-business_ethics,5-shot,accuracy,0.78,0.0416333199893226
Qwen/Qwen1.5-32B,hendrycksTest-business_ethics,5-shot,acc_norm,0.78,0.0416333199893226
Qwen/Qwen1.5-32B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7886792452830189,0.0251257664848278
Qwen/Qwen1.5-32B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7886792452830189,0.0251257664848278
Qwen/Qwen1.5-32B,hendrycksTest-college_biology,5-shot,accuracy,0.8680555555555556,0.0283009683820444
Qwen/Qwen1.5-32B,hendrycksTest-college_biology,5-shot,acc_norm,0.8680555555555556,0.0283009683820444
Qwen/Qwen1.5-32B,hendrycksTest-college_chemistry,5-shot,accuracy,0.53,0.0501613558046591
Qwen/Qwen1.5-32B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.53,0.0501613558046591
Qwen/Qwen1.5-32B,hendrycksTest-college_computer_science,5-shot,accuracy,0.6,0.049236596391733
Qwen/Qwen1.5-32B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.6,0.049236596391733
Qwen/Qwen1.5-32B,hendrycksTest-college_mathematics,5-shot,accuracy,0.5,0.0502518907629606
Qwen/Qwen1.5-32B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.5,0.0502518907629606
Qwen/Qwen1.5-32B,hendrycksTest-college_medicine,5-shot,accuracy,0.7514450867052023,0.0329530469681831
Qwen/Qwen1.5-32B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7514450867052023,0.0329530469681831
Qwen/Qwen1.5-32B,hendrycksTest-college_physics,5-shot,accuracy,0.5098039215686274,0.0497422946042281
Qwen/Qwen1.5-32B,hendrycksTest-college_physics,5-shot,acc_norm,0.5098039215686274,0.0497422946042281
Qwen/Qwen1.5-32B,hendrycksTest-computer_security,5-shot,accuracy,0.76,0.0429234695990928
Qwen/Qwen1.5-32B,hendrycksTest-computer_security,5-shot,acc_norm,0.76,0.0429234695990928
Qwen/Qwen1.5-32B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7659574468085106,0.0276784525782123
Qwen/Qwen1.5-32B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7659574468085106,0.0276784525782123
Qwen/Qwen1.5-32B,hendrycksTest-econometrics,5-shot,accuracy,0.5614035087719298,0.0466800073851045
Qwen/Qwen1.5-32B,hendrycksTest-econometrics,5-shot,acc_norm,0.5614035087719298,0.0466800073851045
Qwen/Qwen1.5-32B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7379310344827587,0.0366466633722525
Qwen/Qwen1.5-32B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7379310344827587,0.0366466633722525
Qwen/Qwen1.5-32B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.6957671957671958,0.023695415009463
Qwen/Qwen1.5-32B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.6957671957671958,0.023695415009463
Qwen/Qwen1.5-32B,hendrycksTest-formal_logic,5-shot,accuracy,0.5396825396825397,0.0445802912547097
Qwen/Qwen1.5-32B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5396825396825397,0.0445802912547097
Qwen/Qwen1.5-32B,hendrycksTest-global_facts,5-shot,accuracy,0.47,0.0501613558046591
Qwen/Qwen1.5-32B,hendrycksTest-global_facts,5-shot,acc_norm,0.47,0.0501613558046591
Qwen/Qwen1.5-32B,hendrycksTest-high_school_biology,5-shot,accuracy,0.8838709677419355,0.0182257579494323
Qwen/Qwen1.5-32B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8838709677419355,0.0182257579494323
Qwen/Qwen1.5-32B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6305418719211823,0.0339597038199857
Qwen/Qwen1.5-32B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6305418719211823,0.0339597038199857
Qwen/Qwen1.5-32B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.86,0.0348735088019776
Qwen/Qwen1.5-32B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.86,0.0348735088019776
Qwen/Qwen1.5-32B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8545454545454545,0.0275301963550665
Qwen/Qwen1.5-32B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8545454545454545,0.0275301963550665
Qwen/Qwen1.5-32B,hendrycksTest-high_school_geography,5-shot,accuracy,0.9191919191919192,0.0194176818897245
Qwen/Qwen1.5-32B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9191919191919192,0.0194176818897245
Qwen/Qwen1.5-32B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9637305699481864,0.0134926597512951
Qwen/Qwen1.5-32B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9637305699481864,0.0134926597512951
Qwen/Qwen1.5-32B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7948717948717948,0.0204732331735519
Qwen/Qwen1.5-32B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.7948717948717948,0.0204732331735519
Qwen/Qwen1.5-32B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4814814814814814,0.0304646217188953
Qwen/Qwen1.5-32B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4814814814814814,0.0304646217188953
Qwen/Qwen1.5-32B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8403361344537815,0.0237933539975288
Qwen/Qwen1.5-32B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8403361344537815,0.0237933539975288
Qwen/Qwen1.5-32B,hendrycksTest-high_school_physics,5-shot,accuracy,0.4966887417218543,0.0408239337944965
Qwen/Qwen1.5-32B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4966887417218543,0.0408239337944965
Qwen/Qwen1.5-32B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9119266055045872,0.0121507437194816
Qwen/Qwen1.5-32B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9119266055045872,0.0121507437194816
Qwen/Qwen1.5-32B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.7037037037037037,0.031141447823536
Qwen/Qwen1.5-32B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.7037037037037037,0.031141447823536
Qwen/Qwen1.5-32B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8872549019607843,0.0221985710394567
Qwen/Qwen1.5-32B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8872549019607843,0.0221985710394567
Qwen/Qwen1.5-32B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8734177215189873,0.0216441957279551
Qwen/Qwen1.5-32B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8734177215189873,0.0216441957279551
Qwen/Qwen1.5-32B,hendrycksTest-human_aging,5-shot,accuracy,0.7757847533632287,0.0279915342585195
Qwen/Qwen1.5-32B,hendrycksTest-human_aging,5-shot,acc_norm,0.7757847533632287,0.0279915342585195
Qwen/Qwen1.5-32B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8473282442748091,0.0315452167200547
Qwen/Qwen1.5-32B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8473282442748091,0.0315452167200547
Qwen/Qwen1.5-32B,hendrycksTest-international_law,5-shot,accuracy,0.8925619834710744,0.0282688121925406
Qwen/Qwen1.5-32B,hendrycksTest-international_law,5-shot,acc_norm,0.8925619834710744,0.0282688121925406
Qwen/Qwen1.5-32B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8333333333333334,0.0360281417639264
Qwen/Qwen1.5-32B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8333333333333334,0.0360281417639264
Qwen/Qwen1.5-32B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8159509202453987,0.0304467776879717
Qwen/Qwen1.5-32B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8159509202453987,0.0304467776879717
Qwen/Qwen1.5-32B,hendrycksTest-machine_learning,5-shot,accuracy,0.5892857142857143,0.0466951066387519
Qwen/Qwen1.5-32B,hendrycksTest-machine_learning,5-shot,acc_norm,0.5892857142857143,0.0466951066387519
Qwen/Qwen1.5-32B,hendrycksTest-management,5-shot,accuracy,0.8737864077669902,0.0328818027880862
Qwen/Qwen1.5-32B,hendrycksTest-management,5-shot,acc_norm,0.8737864077669902,0.0328818027880862
Qwen/Qwen1.5-32B,hendrycksTest-marketing,5-shot,accuracy,0.9487179487179488,0.0144501811768727
Qwen/Qwen1.5-32B,hendrycksTest-marketing,5-shot,acc_norm,0.9487179487179488,0.0144501811768727
Qwen/Qwen1.5-32B,hendrycksTest-medical_genetics,5-shot,accuracy,0.84,0.0368452949177471
Qwen/Qwen1.5-32B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.84,0.0368452949177471
Qwen/Qwen1.5-32B,hendrycksTest-miscellaneous,5-shot,accuracy,0.8876117496807152,0.0112945413512165
Qwen/Qwen1.5-32B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8876117496807152,0.0112945413512165
Qwen/Qwen1.5-32B,hendrycksTest-moral_disputes,5-shot,accuracy,0.7890173410404624,0.0219663099470431
Qwen/Qwen1.5-32B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7890173410404624,0.0219663099470431
Qwen/Qwen1.5-32B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4748603351955307,0.0167013508426826
Qwen/Qwen1.5-32B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4748603351955307,0.0167013508426826
Qwen/Qwen1.5-32B,hendrycksTest-nutrition,5-shot,accuracy,0.8006535947712419,0.022875816993464
Qwen/Qwen1.5-32B,hendrycksTest-nutrition,5-shot,acc_norm,0.8006535947712419,0.022875816993464
Qwen/Qwen1.5-32B,hendrycksTest-philosophy,5-shot,accuracy,0.8392282958199357,0.0208623880823918
Qwen/Qwen1.5-32B,hendrycksTest-philosophy,5-shot,acc_norm,0.8392282958199357,0.0208623880823918
Qwen/Qwen1.5-32B,hendrycksTest-prehistory,5-shot,accuracy,0.8209876543209876,0.021330868762127
Qwen/Qwen1.5-32B,hendrycksTest-prehistory,5-shot,acc_norm,0.8209876543209876,0.021330868762127
Qwen/Qwen1.5-32B,hendrycksTest-professional_accounting,5-shot,accuracy,0.5886524822695035,0.0293549111599409
Qwen/Qwen1.5-32B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5886524822695035,0.0293549111599409
Qwen/Qwen1.5-32B,hendrycksTest-professional_law,5-shot,accuracy,0.5808344198174706,0.0126022445057882
Qwen/Qwen1.5-32B,hendrycksTest-professional_law,5-shot,acc_norm,0.5808344198174706,0.0126022445057882
Qwen/Qwen1.5-32B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8125,0.0237097882538117
Qwen/Qwen1.5-32B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8125,0.0237097882538117
Qwen/Qwen1.5-32B,hendrycksTest-professional_psychology,5-shot,accuracy,0.7565359477124183,0.0173624737621466
Qwen/Qwen1.5-32B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7565359477124183,0.0173624737621466
Qwen/Qwen1.5-32B,hendrycksTest-public_relations,5-shot,accuracy,0.6636363636363637,0.045253935963025
Qwen/Qwen1.5-32B,hendrycksTest-public_relations,5-shot,acc_norm,0.6636363636363637,0.045253935963025
Qwen/Qwen1.5-32B,hendrycksTest-security_studies,5-shot,accuracy,0.8204081632653061,0.0245732935895856
Qwen/Qwen1.5-32B,hendrycksTest-security_studies,5-shot,acc_norm,0.8204081632653061,0.0245732935895856
Qwen/Qwen1.5-32B,hendrycksTest-sociology,5-shot,accuracy,0.8855721393034826,0.0225093453251017
Qwen/Qwen1.5-32B,hendrycksTest-sociology,5-shot,acc_norm,0.8855721393034826,0.0225093453251017
Qwen/Qwen1.5-32B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
Qwen/Qwen1.5-32B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.0272659924344291
Qwen/Qwen1.5-32B,hendrycksTest-virology,5-shot,accuracy,0.5783132530120482,0.0384445318177091
Qwen/Qwen1.5-32B,hendrycksTest-virology,5-shot,acc_norm,0.5783132530120482,0.0384445318177091
Qwen/Qwen1.5-32B,hendrycksTest-world_religions,5-shot,accuracy,0.8538011695906432,0.0270972901180707
Qwen/Qwen1.5-32B,hendrycksTest-world_religions,5-shot,acc_norm,0.8538011695906432,0.0270972901180707
Qwen/Qwen1.5-32B,truthfulqa:mc,0-shot,mc1,0.3941248470012239,0.0171065881407003
Qwen/Qwen1.5-32B,truthfulqa:mc,0-shot,mc2,0.5742698608966924,0.0147587213349976
Qwen/Qwen1.5-32B,winogrande,5-shot,accuracy,0.8129439621152328,0.0109597164352429
Qwen/Qwen1.5-32B,gsm8k,5-shot,accuracy,0.6087945413191812,0.0134425024027943
meta-llama/Llama-2-70b-hf,drop,3-shot,em,0.0017827181208053,0.0004320097346038
Salesforce/codegen-350M-nl,lambada_openai,0-shot,perplexity,13.283951817016561,0.40612302407628836
Salesforce/codegen-350M-nl,lambada_openai,0-shot,accuracy,0.46807684843780323,0.006951765275756157
Salesforce/codegen-350M-nl,lambada_standard,0-shot,perplexity,27.777997393078923,1.002460558851895
Salesforce/codegen-350M-nl,lambada_standard,0-shot,accuracy,0.37803221424412964,0.006755545502651413
Salesforce/codegen-2B-nl,lambada_openai,0-shot,perplexity,5.074815189833182,0.1172964896542569
Salesforce/codegen-2B-nl,lambada_openai,0-shot,accuracy,0.6429264506112944,0.006675311856122322
Salesforce/codegen-2B-nl,lambada_standard,0-shot,perplexity,6.58941950475912,0.1640572956564464
Salesforce/codegen-2B-nl,lambada_standard,0-shot,accuracy,0.5878129245099942,0.006857705382640523
HuggingFaceTB/SmolLM-135M,lambada_openai,0-shot,perplexity,32.36039474312436,1.2800886427144471
HuggingFaceTB/SmolLM-135M,lambada_openai,0-shot,accuracy,0.377644090820881,0.0067541830765266665
HuggingFaceTB/SmolLM-135M,lambada_standard,0-shot,perplexity,77.3602123960688,3.3166966423029693
HuggingFaceTB/SmolLM-135M,lambada_standard,0-shot,accuracy,0.2835241606830972,0.006279251594000095
EleutherAI/pythia-1.4b-deduped,lambada_openai,0-shot,perplexity,6.1073764385419045,0.1531874800090112
EleutherAI/pythia-1.4b-deduped,lambada_openai,0-shot,accuracy,0.6186687366582573,0.006766940596952714
EleutherAI/pythia-1.4b-deduped,lambada_standard,0-shot,perplexity,11.274987625419257,0.3316572519313514
EleutherAI/pythia-1.4b-deduped,lambada_standard,0-shot,accuracy,0.49039394527459734,0.006964691949428184
EleutherAI/pythia-14m,lambada_openai,0-shot,perplexity,1796.0340788994565,82.52081492926854
EleutherAI/pythia-14m,lambada_openai,0-shot,accuracy,0.07199689501261401,0.003601170394017311
EleutherAI/pythia-14m,lambada_standard,0-shot,perplexity,6402.691437132342,336.55690530962346
EleutherAI/pythia-14m,lambada_standard,0-shot,accuracy,0.06578692024063652,0.003453860737178921
EleutherAI/pythia-1.4b,lambada_openai,0-shot,perplexity,6.083416652290185,0.15483960228423438
EleutherAI/pythia-1.4b,lambada_openai,0-shot,accuracy,0.61517562584902,0.006778645440514122
EleutherAI/pythia-1.4b,lambada_standard,0-shot,perplexity,10.869623431520665,0.3226306534977618
EleutherAI/pythia-1.4b,lambada_standard,0-shot,accuracy,0.4971861051814477,0.006965867343039681
EleutherAI/pythia-1b,lambada_openai,0-shot,perplexity,7.917109429930868,0.21095428121840443
EleutherAI/pythia-1b,lambada_openai,0-shot,accuracy,0.5618086551523385,0.0069125483688100104
EleutherAI/pythia-1b,lambada_standard,0-shot,perplexity,17.442797783198696,0.5742862853937696
EleutherAI/pythia-1b,lambada_standard,0-shot,accuracy,0.4207257908014749,0.006877866423280054
EleutherAI/gpt-neo-125m,lambada_openai,0-shot,perplexity,30.265470747581876,1.1220174580305828
EleutherAI/gpt-neo-125m,lambada_openai,0-shot,accuracy,0.3735687948767708,0.006739599048608376
EleutherAI/gpt-neo-125m,lambada_standard,0-shot,perplexity,111.82439684071775,5.0440222437365545
EleutherAI/gpt-neo-125m,lambada_standard,0-shot,accuracy,0.26062487871143025,0.006115788029333529
Salesforce/codegen-350M-nl,arc_challenge,25-shot,accuracy,0.23378839590443687,0.012368225378507156
Salesforce/codegen-350M-nl,arc_challenge,25-shot,acc_norm,0.27986348122866894,0.013119040897725922
EleutherAI/pythia-1.4b,arc_challenge,25-shot,accuracy,0.29948805460750855,0.013385021637313572
EleutherAI/pythia-1.4b,arc_challenge,25-shot,acc_norm,0.3293515358361775,0.013734057652635476
EleutherAI/gpt-neo-125m,arc_challenge,25-shot,accuracy,0.18771331058020477,0.011411001314155142
EleutherAI/gpt-neo-125m,arc_challenge,25-shot,acc_norm,0.22866894197952217,0.012272853582540816
EleutherAI/pythia-14m,arc_challenge,25-shot,accuracy,0.1766211604095563,0.011144042769316497
EleutherAI/pythia-14m,arc_challenge,25-shot,acc_norm,0.21245733788395904,0.011953482906582954
EleutherAI/pythia-1b,arc_challenge,25-shot,accuracy,0.2636518771331058,0.012875929151297039
EleutherAI/pythia-1b,arc_challenge,25-shot,acc_norm,0.29692832764505117,0.013352025976725223
HuggingFaceTB/SmolLM-135M,arc_challenge,25-shot,accuracy,0.29436860068259385,0.01331852846053942
HuggingFaceTB/SmolLM-135M,arc_challenge,25-shot,acc_norm,0.3216723549488055,0.013650488084494164
Salesforce/codegen-350M-nl,arithmetic_1dc,0-shot,accuracy,0.0115,0.0023846841214675827
Salesforce/codegen-350M-nl,arithmetic_2da,0-shot,accuracy,0.007,0.001864735536023767
Salesforce/codegen-350M-nl,arithmetic_2dm,0-shot,accuracy,0.0215,0.003244092641792819
Salesforce/codegen-350M-nl,arithmetic_2ds,0-shot,accuracy,0.0125,0.002484947178762671
Salesforce/codegen-350M-nl,arithmetic_3da,0-shot,accuracy,0.001,0.0007069298939339296
Salesforce/codegen-350M-nl,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
Salesforce/codegen-350M-nl,arithmetic_4da,0-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_4ds,0-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_5da,0-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_5ds,0-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_1dc,0-shot,accuracy,0.071,0.005744214306500121
Salesforce/codegen-2B-nl,arithmetic_2da,0-shot,accuracy,0.124,0.007371510671822565
Salesforce/codegen-2B-nl,arithmetic_2dm,0-shot,accuracy,0.0715,0.005762853480708966
Salesforce/codegen-2B-nl,arithmetic_2ds,0-shot,accuracy,0.2705,0.00993550361662936
Salesforce/codegen-2B-nl,arithmetic_3da,0-shot,accuracy,0.0195,0.0030926780189124165
Salesforce/codegen-2B-nl,arithmetic_3ds,0-shot,accuracy,0.016,0.0028064101569415345
Salesforce/codegen-2B-nl,arithmetic_4da,0-shot,accuracy,0.001,0.0007069298939339407
Salesforce/codegen-2B-nl,arithmetic_4ds,0-shot,accuracy,0.001,0.0007069298939339502
Salesforce/codegen-2B-nl,arithmetic_5da,0-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_1dc,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_2da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_2dm,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_2ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_3da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_3ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_4da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_1dc,0-shot,accuracy,0.0175,0.002932776088929071
EleutherAI/pythia-1.4b-deduped,arithmetic_2da,0-shot,accuracy,0.0145,0.0026736583971427533
EleutherAI/pythia-1.4b-deduped,arithmetic_2dm,0-shot,accuracy,0.0235,0.003388158025742493
EleutherAI/pythia-1.4b-deduped,arithmetic_2ds,0-shot,accuracy,0.012,0.0024353573624298405
EleutherAI/pythia-1.4b-deduped,arithmetic_3da,0-shot,accuracy,0.0015,0.0008655920660521572
EleutherAI/pythia-1.4b-deduped,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
EleutherAI/pythia-1.4b-deduped,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
EleutherAI/pythia-1.4b-deduped,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_1dc,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_2da,0-shot,accuracy,0.0065,0.0017973564602277753
EleutherAI/pythia-160m,arithmetic_2dm,0-shot,accuracy,0.023,0.0033527780362380454
EleutherAI/pythia-160m,arithmetic_2ds,0-shot,accuracy,0.012,0.0024353573624298326
EleutherAI/pythia-160m,arithmetic_3da,0-shot,accuracy,0.001,0.0007069298939339296
EleutherAI/pythia-160m,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
EleutherAI/pythia-160m,arithmetic_4da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_1dc,0-shot,accuracy,0.0075,0.0019296986470519833
EleutherAI/pythia-1b-deduped,arithmetic_2da,0-shot,accuracy,0.007,0.001864735536023767
EleutherAI/pythia-1b-deduped,arithmetic_2dm,0-shot,accuracy,0.025,0.003491933103368245
EleutherAI/pythia-1b-deduped,arithmetic_2ds,0-shot,accuracy,0.0125,0.002484947178762671
EleutherAI/pythia-1b-deduped,arithmetic_3da,0-shot,accuracy,0.001,0.0007069298939339296
EleutherAI/pythia-1b-deduped,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
EleutherAI/pythia-1b-deduped,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
EleutherAI/pythia-1b-deduped,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_1dc,0-shot,accuracy,0.0335,0.004024546370306088
EleutherAI/pythia-1b,arithmetic_2da,0-shot,accuracy,0.007,0.001864735536023767
EleutherAI/pythia-1b,arithmetic_2dm,0-shot,accuracy,0.023,0.003352778036238045
EleutherAI/pythia-1b,arithmetic_2ds,0-shot,accuracy,0.0125,0.002484947178762671
EleutherAI/pythia-1b,arithmetic_3da,0-shot,accuracy,0.001,0.0007069298939339296
EleutherAI/pythia-1b,arithmetic_3ds,0-shot,accuracy,0.001,0.0007069298939339551
EleutherAI/pythia-1b,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
EleutherAI/pythia-1b,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_1dc,0-shot,accuracy,0.0025,0.0011169148353275366
EleutherAI/gpt-neo-125m,arithmetic_2da,0-shot,accuracy,0.001,0.0007069298939339576
EleutherAI/gpt-neo-125m,arithmetic_2dm,0-shot,accuracy,0.0285,0.003721666347242926
EleutherAI/gpt-neo-125m,arithmetic_2ds,0-shot,accuracy,0.001,0.0007069298939339569
EleutherAI/gpt-neo-125m,arithmetic_3da,0-shot,accuracy,0.0005,0.0005000000000000138
EleutherAI/gpt-neo-125m,arithmetic_3ds,0-shot,accuracy,0.001,0.0007069298939339585
EleutherAI/gpt-neo-125m,arithmetic_4da,0-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_5ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_1dc,0-shot,accuracy,0.03,0.003815400193861728
EleutherAI/pythia-1.4b,arithmetic_2da,0-shot,accuracy,0.017,0.0028913110935905838
EleutherAI/pythia-1.4b,arithmetic_2dm,0-shot,accuracy,0.0225,0.0033169829948455224
EleutherAI/pythia-1.4b,arithmetic_2ds,0-shot,accuracy,0.013,0.0025335171905233266
EleutherAI/pythia-1.4b,arithmetic_3da,0-shot,accuracy,0.0015,0.0008655920660521572
EleutherAI/pythia-1.4b,arithmetic_3ds,0-shot,accuracy,0.0015,0.0008655920660521436
EleutherAI/pythia-1.4b,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
EleutherAI/pythia-1.4b,arithmetic_4ds,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_5da,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_5ds,0-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_1dc,0-shot,accuracy,0.6105,0.010906619649373084
google/gemma-2-2b,arithmetic_2da,0-shot,accuracy,0.99,0.002225415969682745
google/gemma-2-2b,arithmetic_2dm,0-shot,accuracy,0.59,0.011000477501118884
google/gemma-2-2b,arithmetic_2ds,0-shot,accuracy,0.9695,0.003846072169833617
google/gemma-2-2b,arithmetic_3da,0-shot,accuracy,0.9195,0.006085095660266774
google/gemma-2-2b,arithmetic_3ds,0-shot,accuracy,0.773,0.009369065572128745
google/gemma-2-2b,arithmetic_4da,0-shot,accuracy,0.728,0.009952764880392899
google/gemma-2-2b,arithmetic_4ds,0-shot,accuracy,0.5805,0.01103724537159067
google/gemma-2-2b,arithmetic_5da,0-shot,accuracy,0.4465,0.011118933867290124
google/gemma-2-2b,arithmetic_5ds,0-shot,accuracy,0.2515,0.009704172323296923
HuggingFaceTB/SmolLM-135M,arithmetic_1dc,0-shot,accuracy,0.039,0.004329997048176576
HuggingFaceTB/SmolLM-135M,arithmetic_2da,0-shot,accuracy,0.007,0.001864735536023767
HuggingFaceTB/SmolLM-135M,arithmetic_2dm,0-shot,accuracy,0.0245,0.0034577236625362444
HuggingFaceTB/SmolLM-135M,arithmetic_2ds,0-shot,accuracy,0.0245,0.003457723662536226
HuggingFaceTB/SmolLM-135M,arithmetic_3da,0-shot,accuracy,0.001,0.0007069298939339296
HuggingFaceTB/SmolLM-135M,arithmetic_3ds,0-shot,accuracy,0.003,0.0012232122154646802
HuggingFaceTB/SmolLM-135M,arithmetic_4da,0-shot,accuracy,0.0005,0.0005000000000000152
HuggingFaceTB/SmolLM-135M,arithmetic_4ds,0-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_5da,0-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_5ds,0-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,gsm8k,5-shot,accuracy,0.015163002274450341,0.003366022949726336
EleutherAI/gpt-neo-125m,gsm8k,5-shot,accuracy,0.0037907505686125853,0.001692700740150196
EleutherAI/pythia-14m,gsm8k,5-shot,accuracy,0.000758150113722517,0.0007581501137225211
EleutherAI/pythia-1b,gsm8k,5-shot,accuracy,0.01592115238817286,0.0034478192723889985
EleutherAI/pythia-1.4b,gsm8k,5-shot,accuracy,0.013646702047005308,0.003195747075480794
HuggingFaceTB/SmolLM-135M,gsm8k,5-shot,accuracy,0.012130401819560273,0.003015294242890947
Salesforce/codegen-350M-nl,hellaswag,10-shot,accuracy,0.3315076677952599,0.004697929774670314
Salesforce/codegen-350M-nl,hellaswag,10-shot,acc_norm,0.400318661621191,0.004889615413144175
EleutherAI/pythia-1.4b,hellaswag,10-shot,accuracy,0.4075881298546106,0.004903815885983277
EleutherAI/pythia-1.4b,hellaswag,10-shot,acc_norm,0.5353515236008763,0.0049772940247780045
EleutherAI/pythia-1b,hellaswag,10-shot,accuracy,0.377912766381199,0.004838747305783343
EleutherAI/pythia-1b,hellaswag,10-shot,acc_norm,0.4790878311093408,0.004985415250690908
EleutherAI/gpt-neo-125m,hellaswag,10-shot,accuracy,0.2863971320454093,0.004511533039406185
EleutherAI/gpt-neo-125m,hellaswag,10-shot,acc_norm,0.3057159928301135,0.004597684609707815
EleutherAI/pythia-14m,hellaswag,10-shot,accuracy,0.2629954192391954,0.004393601887506594
EleutherAI/pythia-14m,hellaswag,10-shot,acc_norm,0.2634933280223063,0.004396273173717432
HuggingFaceTB/SmolLM-135M,hellaswag,10-shot,accuracy,0.3496315475004979,0.004758790172436685
HuggingFaceTB/SmolLM-135M,hellaswag,10-shot,acc_norm,0.4296952798247361,0.004940208641372079
HuggingFaceTB/SmolLM-135M,mmlu_formal_logic,5-shot,accuracy,0.16666666666666666,0.03333333333333336
HuggingFaceTB/SmolLM-135M,mmlu_high_school_european_history,5-shot,accuracy,0.2787878787878788,0.035014387062967806
HuggingFaceTB/SmolLM-135M,mmlu_high_school_us_history,5-shot,accuracy,0.27941176470588236,0.031493281045079556
HuggingFaceTB/SmolLM-135M,mmlu_high_school_world_history,5-shot,accuracy,0.26582278481012656,0.028756799629658335
HuggingFaceTB/SmolLM-135M,mmlu_international_law,5-shot,accuracy,0.36363636363636365,0.043913262867240704
HuggingFaceTB/SmolLM-135M,mmlu_jurisprudence,5-shot,accuracy,0.3055555555555556,0.044531975073749834
HuggingFaceTB/SmolLM-135M,mmlu_logical_fallacies,5-shot,accuracy,0.2883435582822086,0.035590395316173425
HuggingFaceTB/SmolLM-135M,mmlu_moral_disputes,5-shot,accuracy,0.24566473988439305,0.023176298203992012
HuggingFaceTB/SmolLM-135M,mmlu_moral_scenarios,5-shot,accuracy,0.24134078212290502,0.014310999547961464
HuggingFaceTB/SmolLM-135M,mmlu_philosophy,5-shot,accuracy,0.2797427652733119,0.025494259350694905
HuggingFaceTB/SmolLM-135M,mmlu_prehistory,5-shot,accuracy,0.25925925925925924,0.02438366553103545
HuggingFaceTB/SmolLM-135M,mmlu_professional_law,5-shot,accuracy,0.2405475880052151,0.010916406735478947
HuggingFaceTB/SmolLM-135M,mmlu_world_religions,5-shot,accuracy,0.21637426900584794,0.031581495393387324
HuggingFaceTB/SmolLM-135M,mmlu_business_ethics,5-shot,accuracy,0.23,0.04229525846816505
HuggingFaceTB/SmolLM-135M,mmlu_clinical_knowledge,5-shot,accuracy,0.2792452830188679,0.027611163402399715
HuggingFaceTB/SmolLM-135M,mmlu_college_medicine,5-shot,accuracy,0.2254335260115607,0.03186209851641144
HuggingFaceTB/SmolLM-135M,mmlu_global_facts,5-shot,accuracy,0.31,0.04648231987117316
HuggingFaceTB/SmolLM-135M,mmlu_human_aging,5-shot,accuracy,0.3452914798206278,0.03191100192835794
HuggingFaceTB/SmolLM-135M,mmlu_management,5-shot,accuracy,0.23300970873786409,0.04185832598928315
HuggingFaceTB/SmolLM-135M,mmlu_marketing,5-shot,accuracy,0.2222222222222222,0.027236013946196687
HuggingFaceTB/SmolLM-135M,mmlu_medical_genetics,5-shot,accuracy,0.26,0.04408440022768079
HuggingFaceTB/SmolLM-135M,mmlu_miscellaneous,5-shot,accuracy,0.29246487867177523,0.016267000684598635
HuggingFaceTB/SmolLM-135M,mmlu_nutrition,5-shot,accuracy,0.27124183006535946,0.02545775669666787
HuggingFaceTB/SmolLM-135M,mmlu_professional_accounting,5-shot,accuracy,0.2624113475177305,0.026244920349843014
HuggingFaceTB/SmolLM-135M,mmlu_professional_medicine,5-shot,accuracy,0.375,0.029408372932278746
HuggingFaceTB/SmolLM-135M,mmlu_virology,5-shot,accuracy,0.2710843373493976,0.03460579907553026
HuggingFaceTB/SmolLM-135M,mmlu_econometrics,5-shot,accuracy,0.2631578947368421,0.041424397194893624
HuggingFaceTB/SmolLM-135M,mmlu_high_school_geography,5-shot,accuracy,0.2474747474747475,0.030746300742124495
HuggingFaceTB/SmolLM-135M,mmlu_high_school_government_and_politics,5-shot,accuracy,0.23834196891191708,0.030748905363909892
HuggingFaceTB/SmolLM-135M,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2358974358974359,0.021525965407408726
HuggingFaceTB/SmolLM-135M,mmlu_high_school_microeconomics,5-shot,accuracy,0.22268907563025211,0.02702543349888238
HuggingFaceTB/SmolLM-135M,mmlu_high_school_psychology,5-shot,accuracy,0.1871559633027523,0.01672268452620017
HuggingFaceTB/SmolLM-135M,mmlu_human_sexuality,5-shot,accuracy,0.25190839694656486,0.03807387116306086
HuggingFaceTB/SmolLM-135M,mmlu_professional_psychology,5-shot,accuracy,0.26143790849673204,0.017776947157528044
HuggingFaceTB/SmolLM-135M,mmlu_public_relations,5-shot,accuracy,0.32727272727272727,0.04494290866252091
HuggingFaceTB/SmolLM-135M,mmlu_security_studies,5-shot,accuracy,0.2938775510204082,0.02916273841024977
HuggingFaceTB/SmolLM-135M,mmlu_sociology,5-shot,accuracy,0.23383084577114427,0.029929415408348366
HuggingFaceTB/SmolLM-135M,mmlu_us_foreign_policy,5-shot,accuracy,0.24,0.04292346959909283
HuggingFaceTB/SmolLM-135M,mmlu_abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
HuggingFaceTB/SmolLM-135M,mmlu_anatomy,5-shot,accuracy,0.28888888888888886,0.0391545063041425
HuggingFaceTB/SmolLM-135M,mmlu_astronomy,5-shot,accuracy,0.2565789473684211,0.0355418036802569
HuggingFaceTB/SmolLM-135M,mmlu_college_biology,5-shot,accuracy,0.2361111111111111,0.03551446610810826
HuggingFaceTB/SmolLM-135M,mmlu_college_chemistry,5-shot,accuracy,0.23,0.042295258468165065
HuggingFaceTB/SmolLM-135M,mmlu_college_computer_science,5-shot,accuracy,0.22,0.04163331998932269
HuggingFaceTB/SmolLM-135M,mmlu_college_mathematics,5-shot,accuracy,0.22,0.04163331998932269
HuggingFaceTB/SmolLM-135M,mmlu_college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617746
HuggingFaceTB/SmolLM-135M,mmlu_computer_security,5-shot,accuracy,0.27,0.04461960433384741
HuggingFaceTB/SmolLM-135M,mmlu_conceptual_physics,5-shot,accuracy,0.32340425531914896,0.030579442773610348
HuggingFaceTB/SmolLM-135M,mmlu_electrical_engineering,5-shot,accuracy,0.2482758620689655,0.036001056927277716
HuggingFaceTB/SmolLM-135M,mmlu_elementary_mathematics,5-shot,accuracy,0.23544973544973544,0.02185150982203171
HuggingFaceTB/SmolLM-135M,mmlu_high_school_biology,5-shot,accuracy,0.25806451612903225,0.024892469172462836
HuggingFaceTB/SmolLM-135M,mmlu_high_school_chemistry,5-shot,accuracy,0.2660098522167488,0.031089826002937533
HuggingFaceTB/SmolLM-135M,mmlu_high_school_computer_science,5-shot,accuracy,0.32,0.046882617226215034
HuggingFaceTB/SmolLM-135M,mmlu_high_school_mathematics,5-shot,accuracy,0.23703703703703705,0.025928876132766107
HuggingFaceTB/SmolLM-135M,mmlu_high_school_physics,5-shot,accuracy,0.2119205298013245,0.033367670865679766
HuggingFaceTB/SmolLM-135M,mmlu_high_school_statistics,5-shot,accuracy,0.2777777777777778,0.030546745264953195
HuggingFaceTB/SmolLM-135M,mmlu_machine_learning,5-shot,accuracy,0.32142857142857145,0.04432804055291519
EleutherAI/pythia-160m,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.03932537680392872
EleutherAI/pythia-160m,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/pythia-160m,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
EleutherAI/pythia-160m,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pythia-160m,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
EleutherAI/pythia-160m,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
EleutherAI/pythia-160m,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
EleutherAI/pythia-160m,mmlu_moral_disputes,0-shot,accuracy,0.24566473988439305,0.02317629820399201
EleutherAI/pythia-160m,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
EleutherAI/pythia-160m,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/pythia-160m,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
EleutherAI/pythia-160m,mmlu_professional_law,0-shot,accuracy,0.24641460234680573,0.011005971399927244
EleutherAI/pythia-160m,mmlu_world_religions,0-shot,accuracy,0.3333333333333333,0.03615507630310937
EleutherAI/pythia-160m,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-160m,mmlu_clinical_knowledge,0-shot,accuracy,0.21132075471698114,0.025125766484827856
EleutherAI/pythia-160m,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749888
EleutherAI/pythia-160m,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
EleutherAI/pythia-160m,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.03126580522513713
EleutherAI/pythia-160m,mmlu_management,0-shot,accuracy,0.18446601941747573,0.03840423627288276
EleutherAI/pythia-160m,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.02987257770889117
EleutherAI/pythia-160m,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-160m,mmlu_miscellaneous,0-shot,accuracy,0.23754789272030652,0.015218733046150193
EleutherAI/pythia-160m,mmlu_nutrition,0-shot,accuracy,0.21895424836601307,0.02367908986180772
EleutherAI/pythia-160m,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
EleutherAI/pythia-160m,mmlu_professional_medicine,0-shot,accuracy,0.19117647058823528,0.02388688192244036
EleutherAI/pythia-160m,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
EleutherAI/pythia-160m,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
EleutherAI/pythia-160m,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
EleutherAI/pythia-160m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
EleutherAI/pythia-160m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20512820512820512,0.020473233173551965
EleutherAI/pythia-160m,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
EleutherAI/pythia-160m,mmlu_high_school_psychology,0-shot,accuracy,0.1944954128440367,0.01697028909045803
EleutherAI/pythia-160m,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/pythia-160m,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612378977
EleutherAI/pythia-160m,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-160m,mmlu_security_studies,0-shot,accuracy,0.19183673469387755,0.025206963154225395
EleutherAI/pythia-160m,mmlu_sociology,0-shot,accuracy,0.24875621890547264,0.03056767593891672
EleutherAI/pythia-160m,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-160m,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-160m,mmlu_anatomy,0-shot,accuracy,0.1925925925925926,0.03406542058502652
EleutherAI/pythia-160m,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-160m,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.03685651095897532
EleutherAI/pythia-160m,mmlu_college_chemistry,0-shot,accuracy,0.18,0.03861229196653694
EleutherAI/pythia-160m,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-160m,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-160m,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
EleutherAI/pythia-160m,mmlu_computer_security,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-160m,mmlu_conceptual_physics,0-shot,accuracy,0.25957446808510637,0.02865917937429232
EleutherAI/pythia-160m,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
EleutherAI/pythia-160m,mmlu_elementary_mathematics,0-shot,accuracy,0.21164021164021163,0.021037331505262886
EleutherAI/pythia-160m,mmlu_high_school_biology,0-shot,accuracy,0.18387096774193548,0.02203721734026784
EleutherAI/pythia-160m,mmlu_high_school_chemistry,0-shot,accuracy,0.1625615763546798,0.02596030006460558
EleutherAI/pythia-160m,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.04408440022768079
EleutherAI/pythia-160m,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
EleutherAI/pythia-160m,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.032979866484738336
EleutherAI/pythia-160m,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
EleutherAI/pythia-160m,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.0432704093257873
EleutherAI/pythia-1b,mmlu_formal_logic,0-shot,accuracy,0.25396825396825395,0.03893259610604673
EleutherAI/pythia-1b,mmlu_high_school_european_history,0-shot,accuracy,0.21212121212121213,0.03192271569548301
EleutherAI/pythia-1b,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.030587591351604236
EleutherAI/pythia-1b,mmlu_high_school_world_history,0-shot,accuracy,0.24050632911392406,0.02782078198114968
EleutherAI/pythia-1b,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.039418975265163025
EleutherAI/pythia-1b,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.04414343666854933
EleutherAI/pythia-1b,mmlu_logical_fallacies,0-shot,accuracy,0.24539877300613497,0.03380939813943354
EleutherAI/pythia-1b,mmlu_moral_disputes,0-shot,accuracy,0.24566473988439305,0.02317629820399201
EleutherAI/pythia-1b,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
EleutherAI/pythia-1b,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/pythia-1b,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
EleutherAI/pythia-1b,mmlu_professional_law,0-shot,accuracy,0.22946544980443284,0.010739489382279505
EleutherAI/pythia-1b,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
EleutherAI/pythia-1b,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-1b,mmlu_clinical_knowledge,0-shot,accuracy,0.21132075471698114,0.025125766484827856
EleutherAI/pythia-1b,mmlu_college_medicine,0-shot,accuracy,0.19653179190751446,0.03029957466478814
EleutherAI/pythia-1b,mmlu_global_facts,0-shot,accuracy,0.19,0.03942772444036623
EleutherAI/pythia-1b,mmlu_human_aging,0-shot,accuracy,0.30493273542600896,0.030898610882477515
EleutherAI/pythia-1b,mmlu_management,0-shot,accuracy,0.1941747572815534,0.03916667762822584
EleutherAI/pythia-1b,mmlu_marketing,0-shot,accuracy,0.2777777777777778,0.02934311479809446
EleutherAI/pythia-1b,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-1b,mmlu_miscellaneous,0-shot,accuracy,0.23627075351213284,0.01519047371703751
EleutherAI/pythia-1b,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.023805186524888135
EleutherAI/pythia-1b,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902006
EleutherAI/pythia-1b,mmlu_professional_medicine,0-shot,accuracy,0.24632352941176472,0.02617343857052
EleutherAI/pythia-1b,mmlu_virology,0-shot,accuracy,0.26506024096385544,0.03436024037944967
EleutherAI/pythia-1b,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.040969851398436695
EleutherAI/pythia-1b,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
EleutherAI/pythia-1b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19170984455958548,0.028408953626245268
EleutherAI/pythia-1b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2076923076923077,0.020567539567246797
EleutherAI/pythia-1b,mmlu_high_school_microeconomics,0-shot,accuracy,0.21428571428571427,0.02665353159671549
EleutherAI/pythia-1b,mmlu_high_school_psychology,0-shot,accuracy,0.20917431192660552,0.017437937173343236
EleutherAI/pythia-1b,mmlu_human_sexuality,0-shot,accuracy,0.25190839694656486,0.038073871163060866
EleutherAI/pythia-1b,mmlu_professional_psychology,0-shot,accuracy,0.24673202614379086,0.0174408203674025
EleutherAI/pythia-1b,mmlu_public_relations,0-shot,accuracy,0.2,0.038313051408846034
EleutherAI/pythia-1b,mmlu_security_studies,0-shot,accuracy,0.19183673469387755,0.025206963154225395
EleutherAI/pythia-1b,mmlu_sociology,0-shot,accuracy,0.23383084577114427,0.0299294154083484
EleutherAI/pythia-1b,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-1b,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.04229525846816506
EleutherAI/pythia-1b,mmlu_anatomy,0-shot,accuracy,0.1925925925925926,0.03406542058502653
EleutherAI/pythia-1b,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123394
EleutherAI/pythia-1b,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.03685651095897532
EleutherAI/pythia-1b,mmlu_college_chemistry,0-shot,accuracy,0.22,0.041633319989322695
EleutherAI/pythia-1b,mmlu_college_computer_science,0-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-1b,mmlu_college_mathematics,0-shot,accuracy,0.22,0.0416333199893227
EleutherAI/pythia-1b,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
EleutherAI/pythia-1b,mmlu_computer_security,0-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-1b,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
EleutherAI/pythia-1b,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
EleutherAI/pythia-1b,mmlu_elementary_mathematics,0-shot,accuracy,0.21957671957671956,0.021320018599770355
EleutherAI/pythia-1b,mmlu_high_school_biology,0-shot,accuracy,0.1935483870967742,0.022475258525536057
EleutherAI/pythia-1b,mmlu_high_school_chemistry,0-shot,accuracy,0.15763546798029557,0.025639014131172404
EleutherAI/pythia-1b,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.044084400227680794
EleutherAI/pythia-1b,mmlu_high_school_mathematics,0-shot,accuracy,0.2,0.02438843043398766
EleutherAI/pythia-1b,mmlu_high_school_physics,0-shot,accuracy,0.18543046357615894,0.03173284384294285
EleutherAI/pythia-1b,mmlu_high_school_statistics,0-shot,accuracy,0.22685185185185186,0.028561650102422273
EleutherAI/pythia-1b,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.04493949068613539
EleutherAI/pythia-14m,mmlu_formal_logic,0-shot,accuracy,0.2777777777777778,0.04006168083848876
EleutherAI/pythia-14m,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/pythia-14m,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
EleutherAI/pythia-14m,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pythia-14m,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
EleutherAI/pythia-14m,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
EleutherAI/pythia-14m,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
EleutherAI/pythia-14m,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/pythia-14m,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
EleutherAI/pythia-14m,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/pythia-14m,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
EleutherAI/pythia-14m,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
EleutherAI/pythia-14m,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
EleutherAI/pythia-14m,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-14m,mmlu_clinical_knowledge,0-shot,accuracy,0.2188679245283019,0.025447863825108604
EleutherAI/pythia-14m,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
EleutherAI/pythia-14m,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
EleutherAI/pythia-14m,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
EleutherAI/pythia-14m,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
EleutherAI/pythia-14m,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
EleutherAI/pythia-14m,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-14m,mmlu_miscellaneous,0-shot,accuracy,0.23627075351213284,0.01519047371703751
EleutherAI/pythia-14m,mmlu_nutrition,0-shot,accuracy,0.22549019607843138,0.023929155517351284
EleutherAI/pythia-14m,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
EleutherAI/pythia-14m,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
EleutherAI/pythia-14m,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
EleutherAI/pythia-14m,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
EleutherAI/pythia-14m,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
EleutherAI/pythia-14m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
EleutherAI/pythia-14m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
EleutherAI/pythia-14m,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
EleutherAI/pythia-14m,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936073
EleutherAI/pythia-14m,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/pythia-14m,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
EleutherAI/pythia-14m,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-14m,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
EleutherAI/pythia-14m,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/pythia-14m,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-14m,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-14m,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
EleutherAI/pythia-14m,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-14m,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.03685651095897532
EleutherAI/pythia-14m,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-14m,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
EleutherAI/pythia-14m,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-14m,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
EleutherAI/pythia-14m,mmlu_computer_security,0-shot,accuracy,0.26,0.04408440022768078
EleutherAI/pythia-14m,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
EleutherAI/pythia-14m,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
EleutherAI/pythia-14m,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
EleutherAI/pythia-14m,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.021732540689329265
EleutherAI/pythia-14m,mmlu_high_school_chemistry,0-shot,accuracy,0.15763546798029557,0.0256390141311724
EleutherAI/pythia-14m,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.042295258468165065
EleutherAI/pythia-14m,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
EleutherAI/pythia-14m,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
EleutherAI/pythia-14m,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
EleutherAI/pythia-14m,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
EleutherAI/pythia-1.4b-deduped,mmlu_formal_logic,0-shot,accuracy,0.1984126984126984,0.03567016675276862
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.19393939393939394,0.030874145136562108
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.030587591351604243
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.22784810126582278,0.02730348459906942
EleutherAI/pythia-1.4b-deduped,mmlu_international_law,0-shot,accuracy,0.2892561983471074,0.041391127276354626
EleutherAI/pythia-1.4b-deduped,mmlu_jurisprudence,0-shot,accuracy,0.2777777777777778,0.043300437496507437
EleutherAI/pythia-1.4b-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.2147239263803681,0.03226219377286774
EleutherAI/pythia-1.4b-deduped,mmlu_moral_disputes,0-shot,accuracy,0.2658959537572254,0.023786203255508287
EleutherAI/pythia-1.4b-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.24916201117318434,0.014465893829859936
EleutherAI/pythia-1.4b-deduped,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/pythia-1.4b-deduped,mmlu_prehistory,0-shot,accuracy,0.20679012345679013,0.022535006705942818
EleutherAI/pythia-1.4b-deduped,mmlu_professional_law,0-shot,accuracy,0.2438070404172099,0.010966507972178475
EleutherAI/pythia-1.4b-deduped,mmlu_world_religions,0-shot,accuracy,0.29239766081871343,0.03488647713457923
EleutherAI/pythia-1.4b-deduped,mmlu_business_ethics,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-1.4b-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.23018867924528302,0.02590789712240817
EleutherAI/pythia-1.4b-deduped,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.03126511206173042
EleutherAI/pythia-1.4b-deduped,mmlu_global_facts,0-shot,accuracy,0.2,0.04020151261036845
EleutherAI/pythia-1.4b-deduped,mmlu_human_aging,0-shot,accuracy,0.32286995515695066,0.03138147637575498
EleutherAI/pythia-1.4b-deduped,mmlu_management,0-shot,accuracy,0.18446601941747573,0.03840423627288276
EleutherAI/pythia-1.4b-deduped,mmlu_marketing,0-shot,accuracy,0.3034188034188034,0.03011821010694265
EleutherAI/pythia-1.4b-deduped,mmlu_medical_genetics,0-shot,accuracy,0.23,0.04229525846816506
EleutherAI/pythia-1.4b-deduped,mmlu_miscellaneous,0-shot,accuracy,0.24904214559386972,0.015464676163395951
EleutherAI/pythia-1.4b-deduped,mmlu_nutrition,0-shot,accuracy,0.21241830065359477,0.023420375478296118
EleutherAI/pythia-1.4b-deduped,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432403
EleutherAI/pythia-1.4b-deduped,mmlu_professional_medicine,0-shot,accuracy,0.19117647058823528,0.02388688192244034
EleutherAI/pythia-1.4b-deduped,mmlu_virology,0-shot,accuracy,0.25301204819277107,0.03384429155233136
EleutherAI/pythia-1.4b-deduped,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.041424397194893596
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_geography,0-shot,accuracy,0.20202020202020202,0.028606204289229872
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.22279792746113988,0.03003114797764154
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.0203776609703714
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.23949579831932774,0.027722065493361245
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.22935779816513763,0.018025349724618684
EleutherAI/pythia-1.4b-deduped,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596918
EleutherAI/pythia-1.4b-deduped,mmlu_professional_psychology,0-shot,accuracy,0.24836601307189543,0.017479487001364764
EleutherAI/pythia-1.4b-deduped,mmlu_public_relations,0-shot,accuracy,0.19090909090909092,0.03764425585984925
EleutherAI/pythia-1.4b-deduped,mmlu_security_studies,0-shot,accuracy,0.1673469387755102,0.023897144768914524
EleutherAI/pythia-1.4b-deduped,mmlu_sociology,0-shot,accuracy,0.23383084577114427,0.0299294154083484
EleutherAI/pythia-1.4b-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-1.4b-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-1.4b-deduped,mmlu_anatomy,0-shot,accuracy,0.2222222222222222,0.0359144408419697
EleutherAI/pythia-1.4b-deduped,mmlu_astronomy,0-shot,accuracy,0.20394736842105263,0.03279000406310051
EleutherAI/pythia-1.4b-deduped,mmlu_college_biology,0-shot,accuracy,0.2916666666666667,0.03800968060554857
EleutherAI/pythia-1.4b-deduped,mmlu_college_chemistry,0-shot,accuracy,0.24,0.04292346959909282
EleutherAI/pythia-1.4b-deduped,mmlu_college_computer_science,0-shot,accuracy,0.26,0.04408440022768079
EleutherAI/pythia-1.4b-deduped,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816506
EleutherAI/pythia-1.4b-deduped,mmlu_college_physics,0-shot,accuracy,0.23529411764705882,0.04220773659171453
EleutherAI/pythia-1.4b-deduped,mmlu_computer_security,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-1.4b-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2723404255319149,0.029101290698386687
EleutherAI/pythia-1.4b-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
EleutherAI/pythia-1.4b-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.021765961672154534
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_biology,0-shot,accuracy,0.22903225806451613,0.023904914311782655
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.22167487684729065,0.029225575892489607
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.2074074074074074,0.02472071319395216
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.03257847384436776
EleutherAI/pythia-1.4b-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.17592592592592593,0.025967420958258533
EleutherAI/pythia-1.4b-deduped,mmlu_machine_learning,0-shot,accuracy,0.3482142857142857,0.04521829902833586
EleutherAI/gpt-neo-125m,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
EleutherAI/gpt-neo-125m,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
EleutherAI/gpt-neo-125m,mmlu_high_school_us_history,0-shot,accuracy,0.24019607843137256,0.02998373305591361
EleutherAI/gpt-neo-125m,mmlu_high_school_world_history,0-shot,accuracy,0.23628691983122363,0.027652153144159274
EleutherAI/gpt-neo-125m,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.039418975265163025
EleutherAI/gpt-neo-125m,mmlu_jurisprudence,0-shot,accuracy,0.2777777777777778,0.043300437496507437
EleutherAI/gpt-neo-125m,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
EleutherAI/gpt-neo-125m,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/gpt-neo-125m,mmlu_moral_scenarios,0-shot,accuracy,0.2446927374301676,0.01437816988409842
EleutherAI/gpt-neo-125m,mmlu_philosophy,0-shot,accuracy,0.19292604501607716,0.022411516780911363
EleutherAI/gpt-neo-125m,mmlu_prehistory,0-shot,accuracy,0.22530864197530864,0.02324620264781975
EleutherAI/gpt-neo-125m,mmlu_professional_law,0-shot,accuracy,0.2438070404172099,0.010966507972178475
EleutherAI/gpt-neo-125m,mmlu_world_religions,0-shot,accuracy,0.32748538011695905,0.03599335771456027
EleutherAI/gpt-neo-125m,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neo-125m,mmlu_clinical_knowledge,0-shot,accuracy,0.2037735849056604,0.024790784501775416
EleutherAI/gpt-neo-125m,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.03186209851641144
EleutherAI/gpt-neo-125m,mmlu_global_facts,0-shot,accuracy,0.19,0.039427724440366234
EleutherAI/gpt-neo-125m,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.03126580522513713
EleutherAI/gpt-neo-125m,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
EleutherAI/gpt-neo-125m,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.02987257770889117
EleutherAI/gpt-neo-125m,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neo-125m,mmlu_miscellaneous,0-shot,accuracy,0.24393358876117496,0.015357212665829484
EleutherAI/gpt-neo-125m,mmlu_nutrition,0-shot,accuracy,0.21895424836601307,0.02367908986180772
EleutherAI/gpt-neo-125m,mmlu_professional_accounting,0-shot,accuracy,0.22340425531914893,0.024847921358063962
EleutherAI/gpt-neo-125m,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
EleutherAI/gpt-neo-125m,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.03529486801511115
EleutherAI/gpt-neo-125m,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.040493392977481384
EleutherAI/gpt-neo-125m,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
EleutherAI/gpt-neo-125m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
EleutherAI/gpt-neo-125m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.21025641025641026,0.02066059748502691
EleutherAI/gpt-neo-125m,mmlu_high_school_microeconomics,0-shot,accuracy,0.21428571428571427,0.02665353159671549
EleutherAI/gpt-neo-125m,mmlu_high_school_psychology,0-shot,accuracy,0.1908256880733945,0.01684767640009109
EleutherAI/gpt-neo-125m,mmlu_human_sexuality,0-shot,accuracy,0.24427480916030533,0.03768335959728744
EleutherAI/gpt-neo-125m,mmlu_professional_psychology,0-shot,accuracy,0.25326797385620914,0.017593486895366835
EleutherAI/gpt-neo-125m,mmlu_public_relations,0-shot,accuracy,0.23636363636363636,0.04069306319721376
EleutherAI/gpt-neo-125m,mmlu_security_studies,0-shot,accuracy,0.19591836734693877,0.025409301953225678
EleutherAI/gpt-neo-125m,mmlu_sociology,0-shot,accuracy,0.24875621890547264,0.03056767593891672
EleutherAI/gpt-neo-125m,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/gpt-neo-125m,mmlu_abstract_algebra,0-shot,accuracy,0.2,0.04020151261036846
EleutherAI/gpt-neo-125m,mmlu_anatomy,0-shot,accuracy,0.2,0.034554737023254366
EleutherAI/gpt-neo-125m,mmlu_astronomy,0-shot,accuracy,0.18421052631578946,0.0315469804508223
EleutherAI/gpt-neo-125m,mmlu_college_biology,0-shot,accuracy,0.25,0.03621034121889507
EleutherAI/gpt-neo-125m,mmlu_college_chemistry,0-shot,accuracy,0.19,0.039427724440366234
EleutherAI/gpt-neo-125m,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/gpt-neo-125m,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816506
EleutherAI/gpt-neo-125m,mmlu_college_physics,0-shot,accuracy,0.20588235294117646,0.04023382273617748
EleutherAI/gpt-neo-125m,mmlu_computer_security,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/gpt-neo-125m,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
EleutherAI/gpt-neo-125m,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
EleutherAI/gpt-neo-125m,mmlu_elementary_mathematics,0-shot,accuracy,0.22486772486772486,0.02150209607822914
EleutherAI/gpt-neo-125m,mmlu_high_school_biology,0-shot,accuracy,0.18064516129032257,0.021886178567172548
EleutherAI/gpt-neo-125m,mmlu_high_school_chemistry,0-shot,accuracy,0.1625615763546798,0.025960300064605573
EleutherAI/gpt-neo-125m,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/gpt-neo-125m,mmlu_high_school_mathematics,0-shot,accuracy,0.2222222222222222,0.025348097468097845
EleutherAI/gpt-neo-125m,mmlu_high_school_physics,0-shot,accuracy,0.18543046357615894,0.031732843842942844
EleutherAI/gpt-neo-125m,mmlu_high_school_statistics,0-shot,accuracy,0.14814814814814814,0.024227629273728353
EleutherAI/gpt-neo-125m,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.04287858751340456
EleutherAI/pythia-1.4b,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.03970158273235172
EleutherAI/pythia-1.4b,mmlu_high_school_european_history,0-shot,accuracy,0.22424242424242424,0.03256866661681102
EleutherAI/pythia-1.4b,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.030587591351604236
EleutherAI/pythia-1.4b,mmlu_high_school_world_history,0-shot,accuracy,0.2616033755274262,0.028609516716994934
EleutherAI/pythia-1.4b,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.040261875275912046
EleutherAI/pythia-1.4b,mmlu_jurisprudence,0-shot,accuracy,0.28703703703703703,0.043733130409147614
EleutherAI/pythia-1.4b,mmlu_logical_fallacies,0-shot,accuracy,0.22699386503067484,0.03291099578615768
EleutherAI/pythia-1.4b,mmlu_moral_disputes,0-shot,accuracy,0.2543352601156069,0.02344582627654555
EleutherAI/pythia-1.4b,mmlu_moral_scenarios,0-shot,accuracy,0.2346368715083799,0.014173044098303673
EleutherAI/pythia-1.4b,mmlu_philosophy,0-shot,accuracy,0.20257234726688103,0.02282731749105967
EleutherAI/pythia-1.4b,mmlu_prehistory,0-shot,accuracy,0.23765432098765432,0.023683591837008553
EleutherAI/pythia-1.4b,mmlu_professional_law,0-shot,accuracy,0.23859191655801826,0.010885929742002212
EleutherAI/pythia-1.4b,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.035650796707083134
EleutherAI/pythia-1.4b,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-1.4b,mmlu_clinical_knowledge,0-shot,accuracy,0.2981132075471698,0.028152837942493854
EleutherAI/pythia-1.4b,mmlu_college_medicine,0-shot,accuracy,0.1791907514450867,0.02924251305906328
EleutherAI/pythia-1.4b,mmlu_global_facts,0-shot,accuracy,0.36,0.048241815132442176
EleutherAI/pythia-1.4b,mmlu_human_aging,0-shot,accuracy,0.35874439461883406,0.03219079200419996
EleutherAI/pythia-1.4b,mmlu_management,0-shot,accuracy,0.1262135922330097,0.03288180278808629
EleutherAI/pythia-1.4b,mmlu_marketing,0-shot,accuracy,0.32051282051282054,0.030572811310299607
EleutherAI/pythia-1.4b,mmlu_medical_genetics,0-shot,accuracy,0.35,0.0479372485441102
EleutherAI/pythia-1.4b,mmlu_miscellaneous,0-shot,accuracy,0.22988505747126436,0.01504630184669182
EleutherAI/pythia-1.4b,mmlu_nutrition,0-shot,accuracy,0.24509803921568626,0.02463004897982478
EleutherAI/pythia-1.4b,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902
EleutherAI/pythia-1.4b,mmlu_professional_medicine,0-shot,accuracy,0.19852941176470587,0.024231013370541097
EleutherAI/pythia-1.4b,mmlu_virology,0-shot,accuracy,0.26506024096385544,0.03436024037944967
EleutherAI/pythia-1.4b,mmlu_econometrics,0-shot,accuracy,0.21052631578947367,0.038351539543994194
EleutherAI/pythia-1.4b,mmlu_high_school_geography,0-shot,accuracy,0.1919191919191919,0.028057791672989017
EleutherAI/pythia-1.4b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20207253886010362,0.02897908979429673
EleutherAI/pythia-1.4b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2230769230769231,0.021107730127244
EleutherAI/pythia-1.4b,mmlu_high_school_microeconomics,0-shot,accuracy,0.24789915966386555,0.028047967224176892
EleutherAI/pythia-1.4b,mmlu_high_school_psychology,0-shot,accuracy,0.23853211009174313,0.01827257581023186
EleutherAI/pythia-1.4b,mmlu_human_sexuality,0-shot,accuracy,0.21374045801526717,0.0359546161177469
EleutherAI/pythia-1.4b,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
EleutherAI/pythia-1.4b,mmlu_public_relations,0-shot,accuracy,0.33636363636363636,0.04525393596302506
EleutherAI/pythia-1.4b,mmlu_security_studies,0-shot,accuracy,0.2,0.025607375986579153
EleutherAI/pythia-1.4b,mmlu_sociology,0-shot,accuracy,0.22388059701492538,0.02947525023601719
EleutherAI/pythia-1.4b,mmlu_us_foreign_policy,0-shot,accuracy,0.32,0.046882617226215034
EleutherAI/pythia-1.4b,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.044084400227680794
EleutherAI/pythia-1.4b,mmlu_anatomy,0-shot,accuracy,0.2,0.034554737023254366
EleutherAI/pythia-1.4b,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-1.4b,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
EleutherAI/pythia-1.4b,mmlu_college_chemistry,0-shot,accuracy,0.17,0.03775251680686371
EleutherAI/pythia-1.4b,mmlu_college_computer_science,0-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-1.4b,mmlu_college_mathematics,0-shot,accuracy,0.28,0.04512608598542128
EleutherAI/pythia-1.4b,mmlu_college_physics,0-shot,accuracy,0.2549019607843137,0.04336432707993178
EleutherAI/pythia-1.4b,mmlu_computer_security,0-shot,accuracy,0.26,0.04408440022768078
EleutherAI/pythia-1.4b,mmlu_conceptual_physics,0-shot,accuracy,0.28085106382978725,0.029379170464124825
EleutherAI/pythia-1.4b,mmlu_electrical_engineering,0-shot,accuracy,0.2896551724137931,0.03780019230438014
EleutherAI/pythia-1.4b,mmlu_elementary_mathematics,0-shot,accuracy,0.24867724867724866,0.02226181769240016
EleutherAI/pythia-1.4b,mmlu_high_school_biology,0-shot,accuracy,0.21935483870967742,0.02354079935872329
EleutherAI/pythia-1.4b,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.02530890453938065
EleutherAI/pythia-1.4b,mmlu_high_school_computer_science,0-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-1.4b,mmlu_high_school_mathematics,0-shot,accuracy,0.24444444444444444,0.02620276653465215
EleutherAI/pythia-1.4b,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.03297986648473835
EleutherAI/pythia-1.4b,mmlu_high_school_statistics,0-shot,accuracy,0.18518518518518517,0.026491914727355185
EleutherAI/pythia-1.4b,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.043270409325787296
EleutherAI/pythia-1b-deduped,mmlu_formal_logic,0-shot,accuracy,0.25396825396825395,0.03893259610604672
EleutherAI/pythia-1b-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.21212121212121213,0.03192271569548299
EleutherAI/pythia-1b-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.24509803921568626,0.030190282453501933
EleutherAI/pythia-1b-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.24050632911392406,0.027820781981149675
EleutherAI/pythia-1b-deduped,mmlu_international_law,0-shot,accuracy,0.2231404958677686,0.03800754475228733
EleutherAI/pythia-1b-deduped,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
EleutherAI/pythia-1b-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.2883435582822086,0.035590395316173425
EleutherAI/pythia-1b-deduped,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/pythia-1b-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.27262569832402234,0.014893391735249603
EleutherAI/pythia-1b-deduped,mmlu_philosophy,0-shot,accuracy,0.21221864951768488,0.023222756797435126
EleutherAI/pythia-1b-deduped,mmlu_prehistory,0-shot,accuracy,0.2006172839506173,0.022282313949774892
EleutherAI/pythia-1b-deduped,mmlu_professional_law,0-shot,accuracy,0.2542372881355932,0.011121129007840682
EleutherAI/pythia-1b-deduped,mmlu_world_religions,0-shot,accuracy,0.27485380116959063,0.03424042924691583
EleutherAI/pythia-1b-deduped,mmlu_business_ethics,0-shot,accuracy,0.19,0.03942772444036624
EleutherAI/pythia-1b-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.27169811320754716,0.027377706624670713
EleutherAI/pythia-1b-deduped,mmlu_college_medicine,0-shot,accuracy,0.24855491329479767,0.03295304696818318
EleutherAI/pythia-1b-deduped,mmlu_global_facts,0-shot,accuracy,0.37,0.04852365870939099
EleutherAI/pythia-1b-deduped,mmlu_human_aging,0-shot,accuracy,0.32286995515695066,0.031381476375754995
EleutherAI/pythia-1b-deduped,mmlu_management,0-shot,accuracy,0.2524271844660194,0.04301250399690877
EleutherAI/pythia-1b-deduped,mmlu_marketing,0-shot,accuracy,0.28205128205128205,0.02948036054954119
EleutherAI/pythia-1b-deduped,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-1b-deduped,mmlu_miscellaneous,0-shot,accuracy,0.23499361430395913,0.015162024152278443
EleutherAI/pythia-1b-deduped,mmlu_nutrition,0-shot,accuracy,0.21568627450980393,0.02355083135199509
EleutherAI/pythia-1b-deduped,mmlu_professional_accounting,0-shot,accuracy,0.25886524822695034,0.02612957252718085
EleutherAI/pythia-1b-deduped,mmlu_professional_medicine,0-shot,accuracy,0.1875,0.023709788253811766
EleutherAI/pythia-1b-deduped,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
EleutherAI/pythia-1b-deduped,mmlu_econometrics,0-shot,accuracy,0.2982456140350877,0.04303684033537315
EleutherAI/pythia-1b-deduped,mmlu_high_school_geography,0-shot,accuracy,0.25252525252525254,0.03095405547036592
EleutherAI/pythia-1b-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.22279792746113988,0.030031147977641538
EleutherAI/pythia-1b-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.02037766097037139
EleutherAI/pythia-1b-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.18907563025210083,0.02543511943810536
EleutherAI/pythia-1b-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.24587155963302754,0.018461940968708433
EleutherAI/pythia-1b-deduped,mmlu_human_sexuality,0-shot,accuracy,0.26717557251908397,0.03880848301082396
EleutherAI/pythia-1b-deduped,mmlu_professional_psychology,0-shot,accuracy,0.24673202614379086,0.0174408203674025
EleutherAI/pythia-1b-deduped,mmlu_public_relations,0-shot,accuracy,0.23636363636363636,0.04069306319721376
EleutherAI/pythia-1b-deduped,mmlu_security_studies,0-shot,accuracy,0.17142857142857143,0.024127463462650156
EleutherAI/pythia-1b-deduped,mmlu_sociology,0-shot,accuracy,0.23383084577114427,0.029929415408348398
EleutherAI/pythia-1b-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-1b-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-1b-deduped,mmlu_anatomy,0-shot,accuracy,0.2,0.03455473702325436
EleutherAI/pythia-1b-deduped,mmlu_astronomy,0-shot,accuracy,0.13157894736842105,0.027508689533549912
EleutherAI/pythia-1b-deduped,mmlu_college_biology,0-shot,accuracy,0.24305555555555555,0.03586879280080342
EleutherAI/pythia-1b-deduped,mmlu_college_chemistry,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-1b-deduped,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-1b-deduped,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816507
EleutherAI/pythia-1b-deduped,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
EleutherAI/pythia-1b-deduped,mmlu_computer_security,0-shot,accuracy,0.29,0.04560480215720683
EleutherAI/pythia-1b-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2680851063829787,0.028957342788342347
EleutherAI/pythia-1b-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.18620689655172415,0.032439461590046174
EleutherAI/pythia-1b-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.25132275132275134,0.022340482339643898
EleutherAI/pythia-1b-deduped,mmlu_high_school_biology,0-shot,accuracy,0.25483870967741934,0.024790118459332208
EleutherAI/pythia-1b-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.18719211822660098,0.027444924966882618
EleutherAI/pythia-1b-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-1b-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.02684205787383371
EleutherAI/pythia-1b-deduped,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.035737053147634576
EleutherAI/pythia-1b-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.21296296296296297,0.027920963147993666
EleutherAI/pythia-1b-deduped,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
Salesforce/codegen-2B-nl,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
Salesforce/codegen-2B-nl,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
Salesforce/codegen-2B-nl,mmlu_high_school_us_history,0-shot,accuracy,0.22058823529411764,0.02910225438967409
Salesforce/codegen-2B-nl,mmlu_high_school_world_history,0-shot,accuracy,0.2320675105485232,0.027479744550808517
Salesforce/codegen-2B-nl,mmlu_international_law,0-shot,accuracy,0.30578512396694213,0.04205953933884122
Salesforce/codegen-2B-nl,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
Salesforce/codegen-2B-nl,mmlu_logical_fallacies,0-shot,accuracy,0.2392638036809816,0.033519538795212675
Salesforce/codegen-2B-nl,mmlu_moral_disputes,0-shot,accuracy,0.23410404624277456,0.022797110278071134
Salesforce/codegen-2B-nl,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
Salesforce/codegen-2B-nl,mmlu_philosophy,0-shot,accuracy,0.18971061093247588,0.02226819625878323
Salesforce/codegen-2B-nl,mmlu_prehistory,0-shot,accuracy,0.2345679012345679,0.023576881744005723
Salesforce/codegen-2B-nl,mmlu_professional_law,0-shot,accuracy,0.23076923076923078,0.010760840584471682
Salesforce/codegen-2B-nl,mmlu_world_religions,0-shot,accuracy,0.2631578947368421,0.033773102522091945
Salesforce/codegen-2B-nl,mmlu_business_ethics,0-shot,accuracy,0.21,0.04093601807403326
Salesforce/codegen-2B-nl,mmlu_clinical_knowledge,0-shot,accuracy,0.23773584905660378,0.02619980880756193
Salesforce/codegen-2B-nl,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
Salesforce/codegen-2B-nl,mmlu_global_facts,0-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-2B-nl,mmlu_human_aging,0-shot,accuracy,0.36771300448430494,0.03236198350928275
Salesforce/codegen-2B-nl,mmlu_management,0-shot,accuracy,0.2815533980582524,0.04453254836326469
Salesforce/codegen-2B-nl,mmlu_marketing,0-shot,accuracy,0.2863247863247863,0.02961432369045666
Salesforce/codegen-2B-nl,mmlu_medical_genetics,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-2B-nl,mmlu_miscellaneous,0-shot,accuracy,0.2567049808429119,0.015620480263064536
Salesforce/codegen-2B-nl,mmlu_nutrition,0-shot,accuracy,0.23202614379084968,0.024170840879341026
Salesforce/codegen-2B-nl,mmlu_professional_accounting,0-shot,accuracy,0.2695035460992908,0.02646903681859063
Salesforce/codegen-2B-nl,mmlu_professional_medicine,0-shot,accuracy,0.21691176470588236,0.025035845227711257
Salesforce/codegen-2B-nl,mmlu_virology,0-shot,accuracy,0.29518072289156627,0.035509201856896294
Salesforce/codegen-2B-nl,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.04142439719489361
Salesforce/codegen-2B-nl,mmlu_high_school_geography,0-shot,accuracy,0.20202020202020202,0.02860620428922987
Salesforce/codegen-2B-nl,mmlu_high_school_government_and_politics,0-shot,accuracy,0.23834196891191708,0.030748905363909892
Salesforce/codegen-2B-nl,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2846153846153846,0.0228783227997063
Salesforce/codegen-2B-nl,mmlu_high_school_microeconomics,0-shot,accuracy,0.2857142857142857,0.02934457250063435
Salesforce/codegen-2B-nl,mmlu_high_school_psychology,0-shot,accuracy,0.22752293577981653,0.0179744635787765
Salesforce/codegen-2B-nl,mmlu_human_sexuality,0-shot,accuracy,0.2900763358778626,0.03980066246467766
Salesforce/codegen-2B-nl,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594726
Salesforce/codegen-2B-nl,mmlu_public_relations,0-shot,accuracy,0.17272727272727273,0.036206918339292196
Salesforce/codegen-2B-nl,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904028
Salesforce/codegen-2B-nl,mmlu_sociology,0-shot,accuracy,0.1791044776119403,0.027113286753111837
Salesforce/codegen-2B-nl,mmlu_us_foreign_policy,0-shot,accuracy,0.17,0.03775251680686371
Salesforce/codegen-2B-nl,mmlu_abstract_algebra,0-shot,accuracy,0.27,0.04461960433384741
Salesforce/codegen-2B-nl,mmlu_anatomy,0-shot,accuracy,0.21481481481481482,0.03547854198560823
Salesforce/codegen-2B-nl,mmlu_astronomy,0-shot,accuracy,0.25,0.03523807393012047
Salesforce/codegen-2B-nl,mmlu_college_biology,0-shot,accuracy,0.25,0.03621034121889507
Salesforce/codegen-2B-nl,mmlu_college_chemistry,0-shot,accuracy,0.21,0.040936018074033256
Salesforce/codegen-2B-nl,mmlu_college_computer_science,0-shot,accuracy,0.27,0.04461960433384739
Salesforce/codegen-2B-nl,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816505
Salesforce/codegen-2B-nl,mmlu_college_physics,0-shot,accuracy,0.18627450980392157,0.0387395871414935
Salesforce/codegen-2B-nl,mmlu_computer_security,0-shot,accuracy,0.3,0.046056618647183814
Salesforce/codegen-2B-nl,mmlu_conceptual_physics,0-shot,accuracy,0.33191489361702126,0.03078373675774567
Salesforce/codegen-2B-nl,mmlu_electrical_engineering,0-shot,accuracy,0.296551724137931,0.03806142687309994
Salesforce/codegen-2B-nl,mmlu_elementary_mathematics,0-shot,accuracy,0.24603174603174602,0.02218203720294836
Salesforce/codegen-2B-nl,mmlu_high_school_biology,0-shot,accuracy,0.23225806451612904,0.024022256130308235
Salesforce/codegen-2B-nl,mmlu_high_school_chemistry,0-shot,accuracy,0.23645320197044334,0.029896114291733545
Salesforce/codegen-2B-nl,mmlu_high_school_computer_science,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-2B-nl,mmlu_high_school_mathematics,0-shot,accuracy,0.21851851851851853,0.02519575225182379
Salesforce/codegen-2B-nl,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.03710185726119994
Salesforce/codegen-2B-nl,mmlu_high_school_statistics,0-shot,accuracy,0.21296296296296297,0.027920963147993666
Salesforce/codegen-2B-nl,mmlu_machine_learning,0-shot,accuracy,0.21428571428571427,0.03894641120044792
Salesforce/codegen-350M-nl,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.03970158273235172
Salesforce/codegen-350M-nl,mmlu_high_school_european_history,0-shot,accuracy,0.23030303030303031,0.03287666758603489
Salesforce/codegen-350M-nl,mmlu_high_school_us_history,0-shot,accuracy,0.24019607843137256,0.02998373305591361
Salesforce/codegen-350M-nl,mmlu_high_school_world_history,0-shot,accuracy,0.2320675105485232,0.027479744550808517
Salesforce/codegen-350M-nl,mmlu_international_law,0-shot,accuracy,0.1322314049586777,0.030922788320445815
Salesforce/codegen-350M-nl,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
Salesforce/codegen-350M-nl,mmlu_logical_fallacies,0-shot,accuracy,0.2147239263803681,0.03226219377286774
Salesforce/codegen-350M-nl,mmlu_moral_disputes,0-shot,accuracy,0.25722543352601157,0.02353292543104429
Salesforce/codegen-350M-nl,mmlu_moral_scenarios,0-shot,accuracy,0.25921787709497207,0.01465578083749772
Salesforce/codegen-350M-nl,mmlu_philosophy,0-shot,accuracy,0.19614147909967847,0.022552447780478033
Salesforce/codegen-350M-nl,mmlu_prehistory,0-shot,accuracy,0.22839506172839505,0.023358211840626267
Salesforce/codegen-350M-nl,mmlu_professional_law,0-shot,accuracy,0.2438070404172099,0.010966507972178477
Salesforce/codegen-350M-nl,mmlu_world_religions,0-shot,accuracy,0.23976608187134502,0.03274485211946956
Salesforce/codegen-350M-nl,mmlu_business_ethics,0-shot,accuracy,0.26,0.04408440022768078
Salesforce/codegen-350M-nl,mmlu_clinical_knowledge,0-shot,accuracy,0.23773584905660378,0.02619980880756192
Salesforce/codegen-350M-nl,mmlu_college_medicine,0-shot,accuracy,0.1907514450867052,0.029957851329869337
Salesforce/codegen-350M-nl,mmlu_global_facts,0-shot,accuracy,0.17,0.0377525168068637
Salesforce/codegen-350M-nl,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.03126580522513713
Salesforce/codegen-350M-nl,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
Salesforce/codegen-350M-nl,mmlu_marketing,0-shot,accuracy,0.2564102564102564,0.028605953702004236
Salesforce/codegen-350M-nl,mmlu_medical_genetics,0-shot,accuracy,0.29,0.04560480215720684
Salesforce/codegen-350M-nl,mmlu_miscellaneous,0-shot,accuracy,0.24521072796934865,0.015384352284543932
Salesforce/codegen-350M-nl,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.023805186524888156
Salesforce/codegen-350M-nl,mmlu_professional_accounting,0-shot,accuracy,0.2695035460992908,0.026469036818590627
Salesforce/codegen-350M-nl,mmlu_professional_medicine,0-shot,accuracy,0.1948529411764706,0.02406059942348742
Salesforce/codegen-350M-nl,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.035294868015111155
Salesforce/codegen-350M-nl,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.040969851398436716
Salesforce/codegen-350M-nl,mmlu_high_school_geography,0-shot,accuracy,0.26262626262626265,0.031353050095330855
Salesforce/codegen-350M-nl,mmlu_high_school_government_and_politics,0-shot,accuracy,0.27461139896373055,0.032210245080411544
Salesforce/codegen-350M-nl,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2512820512820513,0.021992016662370568
Salesforce/codegen-350M-nl,mmlu_high_school_microeconomics,0-shot,accuracy,0.23949579831932774,0.027722065493361252
Salesforce/codegen-350M-nl,mmlu_high_school_psychology,0-shot,accuracy,0.20733944954128442,0.017381415563608674
Salesforce/codegen-350M-nl,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596919
Salesforce/codegen-350M-nl,mmlu_professional_psychology,0-shot,accuracy,0.2647058823529412,0.017848089574913226
Salesforce/codegen-350M-nl,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
Salesforce/codegen-350M-nl,mmlu_security_studies,0-shot,accuracy,0.20408163265306123,0.025801283475090517
Salesforce/codegen-350M-nl,mmlu_sociology,0-shot,accuracy,0.22388059701492538,0.02947525023601718
Salesforce/codegen-350M-nl,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.04292346959909282
Salesforce/codegen-350M-nl,mmlu_abstract_algebra,0-shot,accuracy,0.19,0.039427724440366234
Salesforce/codegen-350M-nl,mmlu_anatomy,0-shot,accuracy,0.17037037037037037,0.03247781185995593
Salesforce/codegen-350M-nl,mmlu_astronomy,0-shot,accuracy,0.27631578947368424,0.03639057569952925
Salesforce/codegen-350M-nl,mmlu_college_biology,0-shot,accuracy,0.3194444444444444,0.038990736873573344
Salesforce/codegen-350M-nl,mmlu_college_chemistry,0-shot,accuracy,0.28,0.04512608598542128
Salesforce/codegen-350M-nl,mmlu_college_computer_science,0-shot,accuracy,0.29,0.045604802157206845
Salesforce/codegen-350M-nl,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
Salesforce/codegen-350M-nl,mmlu_college_physics,0-shot,accuracy,0.23529411764705882,0.04220773659171451
Salesforce/codegen-350M-nl,mmlu_computer_security,0-shot,accuracy,0.29,0.04560480215720685
Salesforce/codegen-350M-nl,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.029513196625539355
Salesforce/codegen-350M-nl,mmlu_electrical_engineering,0-shot,accuracy,0.20689655172413793,0.03375672449560553
Salesforce/codegen-350M-nl,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.022494510767503154
Salesforce/codegen-350M-nl,mmlu_high_school_biology,0-shot,accuracy,0.22258064516129034,0.023664216671642518
Salesforce/codegen-350M-nl,mmlu_high_school_chemistry,0-shot,accuracy,0.20689655172413793,0.02850137816789395
Salesforce/codegen-350M-nl,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.04229525846816506
Salesforce/codegen-350M-nl,mmlu_high_school_mathematics,0-shot,accuracy,0.2222222222222222,0.025348097468097856
Salesforce/codegen-350M-nl,mmlu_high_school_physics,0-shot,accuracy,0.32450331125827814,0.038227469376587525
Salesforce/codegen-350M-nl,mmlu_high_school_statistics,0-shot,accuracy,0.39351851851851855,0.03331747876370312
Salesforce/codegen-350M-nl,mmlu_machine_learning,0-shot,accuracy,0.25892857142857145,0.041577515398656284
HuggingFaceTB/SmolLM-135M,mmlu_formal_logic,0-shot,accuracy,0.20634920634920634,0.0361960452412425
HuggingFaceTB/SmolLM-135M,mmlu_high_school_european_history,0-shot,accuracy,0.2787878787878788,0.035014387062967806
HuggingFaceTB/SmolLM-135M,mmlu_high_school_us_history,0-shot,accuracy,0.27941176470588236,0.031493281045079556
HuggingFaceTB/SmolLM-135M,mmlu_high_school_world_history,0-shot,accuracy,0.26582278481012656,0.028756799629658342
HuggingFaceTB/SmolLM-135M,mmlu_international_law,0-shot,accuracy,0.1652892561983471,0.03390780612972776
HuggingFaceTB/SmolLM-135M,mmlu_jurisprudence,0-shot,accuracy,0.24074074074074073,0.04133119440243839
HuggingFaceTB/SmolLM-135M,mmlu_logical_fallacies,0-shot,accuracy,0.3006134969325153,0.03602511318806771
HuggingFaceTB/SmolLM-135M,mmlu_moral_disputes,0-shot,accuracy,0.23410404624277456,0.022797110278071134
HuggingFaceTB/SmolLM-135M,mmlu_moral_scenarios,0-shot,accuracy,0.26145251396648045,0.014696599650364562
HuggingFaceTB/SmolLM-135M,mmlu_philosophy,0-shot,accuracy,0.26366559485530544,0.02502553850053234
HuggingFaceTB/SmolLM-135M,mmlu_prehistory,0-shot,accuracy,0.26851851851851855,0.02465968518596728
HuggingFaceTB/SmolLM-135M,mmlu_professional_law,0-shot,accuracy,0.24771838331160365,0.011025499291443738
HuggingFaceTB/SmolLM-135M,mmlu_world_religions,0-shot,accuracy,0.19883040935672514,0.03061111655743253
HuggingFaceTB/SmolLM-135M,mmlu_business_ethics,0-shot,accuracy,0.17,0.0377525168068637
HuggingFaceTB/SmolLM-135M,mmlu_clinical_knowledge,0-shot,accuracy,0.20754716981132076,0.024959918028911274
HuggingFaceTB/SmolLM-135M,mmlu_college_medicine,0-shot,accuracy,0.26011560693641617,0.03345036916788991
HuggingFaceTB/SmolLM-135M,mmlu_global_facts,0-shot,accuracy,0.25,0.04351941398892446
HuggingFaceTB/SmolLM-135M,mmlu_human_aging,0-shot,accuracy,0.28699551569506726,0.030360379710291964
HuggingFaceTB/SmolLM-135M,mmlu_management,0-shot,accuracy,0.27184466019417475,0.044052680241409216
HuggingFaceTB/SmolLM-135M,mmlu_marketing,0-shot,accuracy,0.2564102564102564,0.028605953702004257
HuggingFaceTB/SmolLM-135M,mmlu_medical_genetics,0-shot,accuracy,0.24,0.04292346959909283
HuggingFaceTB/SmolLM-135M,mmlu_miscellaneous,0-shot,accuracy,0.2656449553001277,0.015794302487888726
HuggingFaceTB/SmolLM-135M,mmlu_nutrition,0-shot,accuracy,0.2549019607843137,0.024954184324879905
HuggingFaceTB/SmolLM-135M,mmlu_professional_accounting,0-shot,accuracy,0.2375886524822695,0.025389512552729906
HuggingFaceTB/SmolLM-135M,mmlu_professional_medicine,0-shot,accuracy,0.26838235294117646,0.026917481224377218
HuggingFaceTB/SmolLM-135M,mmlu_virology,0-shot,accuracy,0.21686746987951808,0.03208284450356365
HuggingFaceTB/SmolLM-135M,mmlu_econometrics,0-shot,accuracy,0.21052631578947367,0.0383515395439942
HuggingFaceTB/SmolLM-135M,mmlu_high_school_geography,0-shot,accuracy,0.20202020202020202,0.02860620428922989
HuggingFaceTB/SmolLM-135M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.25906735751295334,0.0316187791793541
HuggingFaceTB/SmolLM-135M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2743589743589744,0.022622765767493228
HuggingFaceTB/SmolLM-135M,mmlu_high_school_microeconomics,0-shot,accuracy,0.27310924369747897,0.028942004040998167
HuggingFaceTB/SmolLM-135M,mmlu_high_school_psychology,0-shot,accuracy,0.22568807339449543,0.01792308766780305
HuggingFaceTB/SmolLM-135M,mmlu_human_sexuality,0-shot,accuracy,0.183206106870229,0.03392770926494733
HuggingFaceTB/SmolLM-135M,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
HuggingFaceTB/SmolLM-135M,mmlu_public_relations,0-shot,accuracy,0.3090909090909091,0.044262946482000985
HuggingFaceTB/SmolLM-135M,mmlu_security_studies,0-shot,accuracy,0.3142857142857143,0.02971932942241748
HuggingFaceTB/SmolLM-135M,mmlu_sociology,0-shot,accuracy,0.24875621890547264,0.030567675938916707
HuggingFaceTB/SmolLM-135M,mmlu_us_foreign_policy,0-shot,accuracy,0.15,0.0358870281282637
HuggingFaceTB/SmolLM-135M,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909283
HuggingFaceTB/SmolLM-135M,mmlu_anatomy,0-shot,accuracy,0.2074074074074074,0.03502553170678316
HuggingFaceTB/SmolLM-135M,mmlu_astronomy,0-shot,accuracy,0.23684210526315788,0.03459777606810536
HuggingFaceTB/SmolLM-135M,mmlu_college_biology,0-shot,accuracy,0.19444444444444445,0.03309615177059006
HuggingFaceTB/SmolLM-135M,mmlu_college_chemistry,0-shot,accuracy,0.18,0.03861229196653694
HuggingFaceTB/SmolLM-135M,mmlu_college_computer_science,0-shot,accuracy,0.24,0.042923469599092816
HuggingFaceTB/SmolLM-135M,mmlu_college_mathematics,0-shot,accuracy,0.24,0.04292346959909281
HuggingFaceTB/SmolLM-135M,mmlu_college_physics,0-shot,accuracy,0.28431372549019607,0.04488482852329017
HuggingFaceTB/SmolLM-135M,mmlu_computer_security,0-shot,accuracy,0.26,0.04408440022768081
HuggingFaceTB/SmolLM-135M,mmlu_conceptual_physics,0-shot,accuracy,0.251063829787234,0.028346963777162473
HuggingFaceTB/SmolLM-135M,mmlu_electrical_engineering,0-shot,accuracy,0.27586206896551724,0.037245636197746325
HuggingFaceTB/SmolLM-135M,mmlu_elementary_mathematics,0-shot,accuracy,0.29894179894179895,0.02357760479165581
HuggingFaceTB/SmolLM-135M,mmlu_high_school_biology,0-shot,accuracy,0.27419354838709675,0.025378139970885196
HuggingFaceTB/SmolLM-135M,mmlu_high_school_chemistry,0-shot,accuracy,0.20689655172413793,0.02850137816789395
HuggingFaceTB/SmolLM-135M,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.04760952285695235
HuggingFaceTB/SmolLM-135M,mmlu_high_school_mathematics,0-shot,accuracy,0.29259259259259257,0.027738969632176088
HuggingFaceTB/SmolLM-135M,mmlu_high_school_physics,0-shot,accuracy,0.25165562913907286,0.03543304234389985
HuggingFaceTB/SmolLM-135M,mmlu_high_school_statistics,0-shot,accuracy,0.375,0.033016908987210894
HuggingFaceTB/SmolLM-135M,mmlu_machine_learning,0-shot,accuracy,0.24107142857142858,0.04059867246952685
EleutherAI/gpt-neo-125m,mmlu_formal_logic,5-shot,accuracy,0.24603174603174602,0.03852273364924316
EleutherAI/gpt-neo-125m,mmlu_high_school_european_history,5-shot,accuracy,0.24848484848484848,0.03374402644139404
EleutherAI/gpt-neo-125m,mmlu_high_school_us_history,5-shot,accuracy,0.29901960784313725,0.03213325717373615
EleutherAI/gpt-neo-125m,mmlu_high_school_world_history,5-shot,accuracy,0.22362869198312235,0.027123298205229972
EleutherAI/gpt-neo-125m,mmlu_international_law,5-shot,accuracy,0.2231404958677686,0.03800754475228733
EleutherAI/gpt-neo-125m,mmlu_jurisprudence,5-shot,accuracy,0.23148148148148148,0.04077494709252627
EleutherAI/gpt-neo-125m,mmlu_logical_fallacies,5-shot,accuracy,0.24539877300613497,0.03380939813943354
EleutherAI/gpt-neo-125m,mmlu_moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/gpt-neo-125m,mmlu_moral_scenarios,5-shot,accuracy,0.2245810055865922,0.01395680366654464
EleutherAI/gpt-neo-125m,mmlu_philosophy,5-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/gpt-neo-125m,mmlu_prehistory,5-shot,accuracy,0.21604938271604937,0.022899162918445796
EleutherAI/gpt-neo-125m,mmlu_professional_law,5-shot,accuracy,0.2470664928292047,0.011015752255279327
EleutherAI/gpt-neo-125m,mmlu_world_religions,5-shot,accuracy,0.2046783625730994,0.030944459778533204
EleutherAI/gpt-neo-125m,mmlu_business_ethics,5-shot,accuracy,0.23,0.042295258468165044
EleutherAI/gpt-neo-125m,mmlu_clinical_knowledge,5-shot,accuracy,0.25660377358490566,0.02688064788905199
EleutherAI/gpt-neo-125m,mmlu_college_medicine,5-shot,accuracy,0.1907514450867052,0.029957851329869334
EleutherAI/gpt-neo-125m,mmlu_global_facts,5-shot,accuracy,0.18,0.038612291966536955
EleutherAI/gpt-neo-125m,mmlu_human_aging,5-shot,accuracy,0.27802690582959644,0.030069584874494043
EleutherAI/gpt-neo-125m,mmlu_management,5-shot,accuracy,0.2524271844660194,0.04301250399690878
EleutherAI/gpt-neo-125m,mmlu_marketing,5-shot,accuracy,0.2692307692307692,0.029058588303748845
EleutherAI/gpt-neo-125m,mmlu_medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neo-125m,mmlu_miscellaneous,5-shot,accuracy,0.24010217113665389,0.015274685213734191
EleutherAI/gpt-neo-125m,mmlu_nutrition,5-shot,accuracy,0.27124183006535946,0.02545775669666787
EleutherAI/gpt-neo-125m,mmlu_professional_accounting,5-shot,accuracy,0.24113475177304963,0.025518731049537766
EleutherAI/gpt-neo-125m,mmlu_professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121593
EleutherAI/gpt-neo-125m,mmlu_virology,5-shot,accuracy,0.19879518072289157,0.03106939026078942
EleutherAI/gpt-neo-125m,mmlu_econometrics,5-shot,accuracy,0.2631578947368421,0.041424397194893624
EleutherAI/gpt-neo-125m,mmlu_high_school_geography,5-shot,accuracy,0.36363636363636365,0.03427308652999935
EleutherAI/gpt-neo-125m,mmlu_high_school_government_and_politics,5-shot,accuracy,0.35751295336787564,0.034588160421810066
EleutherAI/gpt-neo-125m,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2512820512820513,0.021992016662370533
EleutherAI/gpt-neo-125m,mmlu_high_school_microeconomics,5-shot,accuracy,0.23109243697478993,0.027381406927868952
EleutherAI/gpt-neo-125m,mmlu_high_school_psychology,5-shot,accuracy,0.27889908256880735,0.019227468876463514
EleutherAI/gpt-neo-125m,mmlu_human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/gpt-neo-125m,mmlu_professional_psychology,5-shot,accuracy,0.27124183006535946,0.017986615304030295
EleutherAI/gpt-neo-125m,mmlu_public_relations,5-shot,accuracy,0.2545454545454545,0.041723430387053825
EleutherAI/gpt-neo-125m,mmlu_security_studies,5-shot,accuracy,0.4,0.031362502409358936
EleutherAI/gpt-neo-125m,mmlu_sociology,5-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/gpt-neo-125m,mmlu_us_foreign_policy,5-shot,accuracy,0.27,0.044619604333847394
EleutherAI/gpt-neo-125m,mmlu_abstract_algebra,5-shot,accuracy,0.21,0.040936018074033256
EleutherAI/gpt-neo-125m,mmlu_anatomy,5-shot,accuracy,0.2740740740740741,0.03853254836552003
EleutherAI/gpt-neo-125m,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/gpt-neo-125m,mmlu_college_biology,5-shot,accuracy,0.2708333333333333,0.03716177437566018
EleutherAI/gpt-neo-125m,mmlu_college_chemistry,5-shot,accuracy,0.24,0.042923469599092816
EleutherAI/gpt-neo-125m,mmlu_college_computer_science,5-shot,accuracy,0.33,0.047258156262526045
EleutherAI/gpt-neo-125m,mmlu_college_mathematics,5-shot,accuracy,0.27,0.044619604333847394
EleutherAI/gpt-neo-125m,mmlu_college_physics,5-shot,accuracy,0.22549019607843138,0.04158307533083286
EleutherAI/gpt-neo-125m,mmlu_computer_security,5-shot,accuracy,0.19,0.03942772444036624
EleutherAI/gpt-neo-125m,mmlu_conceptual_physics,5-shot,accuracy,0.2851063829787234,0.029513196625539355
EleutherAI/gpt-neo-125m,mmlu_electrical_engineering,5-shot,accuracy,0.23448275862068965,0.035306258743465914
EleutherAI/gpt-neo-125m,mmlu_elementary_mathematics,5-shot,accuracy,0.24074074074074073,0.022019080012217897
EleutherAI/gpt-neo-125m,mmlu_high_school_biology,5-shot,accuracy,0.25161290322580643,0.02468597928623996
EleutherAI/gpt-neo-125m,mmlu_high_school_chemistry,5-shot,accuracy,0.27586206896551724,0.03144712581678242
EleutherAI/gpt-neo-125m,mmlu_high_school_computer_science,5-shot,accuracy,0.21,0.040936018074033256
EleutherAI/gpt-neo-125m,mmlu_high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.026466117538959912
EleutherAI/gpt-neo-125m,mmlu_high_school_physics,5-shot,accuracy,0.31788079470198677,0.038020397601079024
EleutherAI/gpt-neo-125m,mmlu_high_school_statistics,5-shot,accuracy,0.4583333333333333,0.03398110890294636
EleutherAI/gpt-neo-125m,mmlu_machine_learning,5-shot,accuracy,0.26785714285714285,0.04203277291467763
EleutherAI/pythia-1.4b,mmlu_formal_logic,5-shot,accuracy,0.12698412698412698,0.029780417522688438
EleutherAI/pythia-1.4b,mmlu_high_school_european_history,5-shot,accuracy,0.2545454545454545,0.0340150671524904
EleutherAI/pythia-1.4b,mmlu_high_school_us_history,5-shot,accuracy,0.27450980392156865,0.031321798030832904
EleutherAI/pythia-1.4b,mmlu_high_school_world_history,5-shot,accuracy,0.25738396624472576,0.028458820991460288
EleutherAI/pythia-1.4b,mmlu_international_law,5-shot,accuracy,0.34710743801652894,0.04345724570292534
EleutherAI/pythia-1.4b,mmlu_jurisprudence,5-shot,accuracy,0.24074074074074073,0.041331194402438376
EleutherAI/pythia-1.4b,mmlu_logical_fallacies,5-shot,accuracy,0.294478527607362,0.03581165790474082
EleutherAI/pythia-1.4b,mmlu_moral_disputes,5-shot,accuracy,0.2774566473988439,0.024105712607754307
EleutherAI/pythia-1.4b,mmlu_moral_scenarios,5-shot,accuracy,0.24804469273743016,0.014444157808261436
EleutherAI/pythia-1.4b,mmlu_philosophy,5-shot,accuracy,0.29260450160771706,0.025839898334877976
EleutherAI/pythia-1.4b,mmlu_prehistory,5-shot,accuracy,0.25617283950617287,0.024288533637726095
EleutherAI/pythia-1.4b,mmlu_professional_law,5-shot,accuracy,0.25945241199478486,0.011195262076350316
EleutherAI/pythia-1.4b,mmlu_world_religions,5-shot,accuracy,0.29239766081871343,0.034886477134579215
EleutherAI/pythia-1.4b,mmlu_business_ethics,5-shot,accuracy,0.22,0.041633319989322695
EleutherAI/pythia-1.4b,mmlu_clinical_knowledge,5-shot,accuracy,0.26037735849056604,0.027008766090708094
EleutherAI/pythia-1.4b,mmlu_college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
EleutherAI/pythia-1.4b,mmlu_global_facts,5-shot,accuracy,0.35,0.047937248544110196
EleutherAI/pythia-1.4b,mmlu_human_aging,5-shot,accuracy,0.32286995515695066,0.031381476375754995
EleutherAI/pythia-1.4b,mmlu_management,5-shot,accuracy,0.24271844660194175,0.042450224863844935
EleutherAI/pythia-1.4b,mmlu_marketing,5-shot,accuracy,0.2606837606837607,0.028760348956523414
EleutherAI/pythia-1.4b,mmlu_medical_genetics,5-shot,accuracy,0.24,0.042923469599092816
EleutherAI/pythia-1.4b,mmlu_miscellaneous,5-shot,accuracy,0.280970625798212,0.016073127851221253
EleutherAI/pythia-1.4b,mmlu_nutrition,5-shot,accuracy,0.2549019607843137,0.024954184324879912
EleutherAI/pythia-1.4b,mmlu_professional_accounting,5-shot,accuracy,0.26595744680851063,0.026358065698880592
EleutherAI/pythia-1.4b,mmlu_professional_medicine,5-shot,accuracy,0.18382352941176472,0.02352924218519311
EleutherAI/pythia-1.4b,mmlu_virology,5-shot,accuracy,0.27710843373493976,0.034843315926805875
EleutherAI/pythia-1.4b,mmlu_econometrics,5-shot,accuracy,0.21929824561403508,0.03892431106518753
EleutherAI/pythia-1.4b,mmlu_high_school_geography,5-shot,accuracy,0.23737373737373738,0.030313710538198906
EleutherAI/pythia-1.4b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.19170984455958548,0.02840895362624526
EleutherAI/pythia-1.4b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2153846153846154,0.020843034557462874
EleutherAI/pythia-1.4b,mmlu_high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.02684151432295895
EleutherAI/pythia-1.4b,mmlu_high_school_psychology,5-shot,accuracy,0.25321100917431194,0.018644073041375053
EleutherAI/pythia-1.4b,mmlu_human_sexuality,5-shot,accuracy,0.24427480916030533,0.03768335959728743
EleutherAI/pythia-1.4b,mmlu_professional_psychology,5-shot,accuracy,0.2761437908496732,0.018087276935663137
EleutherAI/pythia-1.4b,mmlu_public_relations,5-shot,accuracy,0.3090909090909091,0.044262946482000985
EleutherAI/pythia-1.4b,mmlu_security_studies,5-shot,accuracy,0.20408163265306123,0.0258012834750905
EleutherAI/pythia-1.4b,mmlu_sociology,5-shot,accuracy,0.263681592039801,0.031157150869355575
EleutherAI/pythia-1.4b,mmlu_us_foreign_policy,5-shot,accuracy,0.23,0.042295258468165065
EleutherAI/pythia-1.4b,mmlu_abstract_algebra,5-shot,accuracy,0.26,0.04408440022768079
EleutherAI/pythia-1.4b,mmlu_anatomy,5-shot,accuracy,0.31851851851851853,0.0402477840197711
EleutherAI/pythia-1.4b,mmlu_astronomy,5-shot,accuracy,0.25,0.03523807393012047
EleutherAI/pythia-1.4b,mmlu_college_biology,5-shot,accuracy,0.2986111111111111,0.03827052357950756
EleutherAI/pythia-1.4b,mmlu_college_chemistry,5-shot,accuracy,0.23,0.04229525846816506
EleutherAI/pythia-1.4b,mmlu_college_computer_science,5-shot,accuracy,0.28,0.04512608598542127
EleutherAI/pythia-1.4b,mmlu_college_mathematics,5-shot,accuracy,0.24,0.04292346959909281
EleutherAI/pythia-1.4b,mmlu_college_physics,5-shot,accuracy,0.2549019607843137,0.04336432707993179
EleutherAI/pythia-1.4b,mmlu_computer_security,5-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-1.4b,mmlu_conceptual_physics,5-shot,accuracy,0.31063829787234043,0.030251237579213167
EleutherAI/pythia-1.4b,mmlu_electrical_engineering,5-shot,accuracy,0.2620689655172414,0.036646663372252565
EleutherAI/pythia-1.4b,mmlu_elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918435
EleutherAI/pythia-1.4b,mmlu_high_school_biology,5-shot,accuracy,0.25806451612903225,0.024892469172462857
EleutherAI/pythia-1.4b,mmlu_high_school_chemistry,5-shot,accuracy,0.24630541871921183,0.03031509928561773
EleutherAI/pythia-1.4b,mmlu_high_school_computer_science,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/pythia-1.4b,mmlu_high_school_mathematics,5-shot,accuracy,0.24814814814814815,0.026335739404055803
EleutherAI/pythia-1.4b,mmlu_high_school_physics,5-shot,accuracy,0.25165562913907286,0.035433042343899844
EleutherAI/pythia-1.4b,mmlu_high_school_statistics,5-shot,accuracy,0.2361111111111111,0.028963702570791033
EleutherAI/pythia-1.4b,mmlu_machine_learning,5-shot,accuracy,0.20535714285714285,0.03834241021419073
EleutherAI/pythia-14m,mmlu_formal_logic,5-shot,accuracy,0.16666666666666666,0.03333333333333337
EleutherAI/pythia-14m,mmlu_high_school_european_history,5-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/pythia-14m,mmlu_high_school_us_history,5-shot,accuracy,0.25,0.03039153369274154
EleutherAI/pythia-14m,mmlu_high_school_world_history,5-shot,accuracy,0.26582278481012656,0.028756799629658335
EleutherAI/pythia-14m,mmlu_international_law,5-shot,accuracy,0.2396694214876033,0.03896878985070417
EleutherAI/pythia-14m,mmlu_jurisprudence,5-shot,accuracy,0.25,0.04186091791394607
EleutherAI/pythia-14m,mmlu_logical_fallacies,5-shot,accuracy,0.2392638036809816,0.033519538795212696
EleutherAI/pythia-14m,mmlu_moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/pythia-14m,mmlu_moral_scenarios,5-shot,accuracy,0.23016759776536314,0.014078339253425817
EleutherAI/pythia-14m,mmlu_philosophy,5-shot,accuracy,0.2508038585209003,0.024619771956697168
EleutherAI/pythia-14m,mmlu_prehistory,5-shot,accuracy,0.23148148148148148,0.023468429832451152
EleutherAI/pythia-14m,mmlu_professional_law,5-shot,accuracy,0.24641460234680573,0.011005971399927249
EleutherAI/pythia-14m,mmlu_world_religions,5-shot,accuracy,0.21052631578947367,0.031267817146631786
EleutherAI/pythia-14m,mmlu_business_ethics,5-shot,accuracy,0.23,0.04229525846816505
EleutherAI/pythia-14m,mmlu_clinical_knowledge,5-shot,accuracy,0.2,0.02461829819586651
EleutherAI/pythia-14m,mmlu_college_medicine,5-shot,accuracy,0.2023121387283237,0.030631145539198816
EleutherAI/pythia-14m,mmlu_global_facts,5-shot,accuracy,0.18,0.038612291966536955
EleutherAI/pythia-14m,mmlu_human_aging,5-shot,accuracy,0.3094170403587444,0.031024411740572223
EleutherAI/pythia-14m,mmlu_management,5-shot,accuracy,0.1553398058252427,0.03586594738573974
EleutherAI/pythia-14m,mmlu_marketing,5-shot,accuracy,0.19658119658119658,0.02603538609895129
EleutherAI/pythia-14m,mmlu_medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-14m,mmlu_miscellaneous,5-shot,accuracy,0.2541507024265645,0.015569254692045766
EleutherAI/pythia-14m,mmlu_nutrition,5-shot,accuracy,0.21895424836601307,0.02367908986180772
EleutherAI/pythia-14m,mmlu_professional_accounting,5-shot,accuracy,0.23049645390070922,0.025123739226872405
EleutherAI/pythia-14m,mmlu_professional_medicine,5-shot,accuracy,0.43014705882352944,0.030074971917302875
EleutherAI/pythia-14m,mmlu_virology,5-shot,accuracy,0.24096385542168675,0.0332939411907353
EleutherAI/pythia-14m,mmlu_econometrics,5-shot,accuracy,0.21929824561403508,0.03892431106518753
EleutherAI/pythia-14m,mmlu_high_school_geography,5-shot,accuracy,0.2474747474747475,0.030746300742124498
EleutherAI/pythia-14m,mmlu_high_school_government_and_politics,5-shot,accuracy,0.22797927461139897,0.03027690994517826
EleutherAI/pythia-14m,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2128205128205128,0.020752423722128006
EleutherAI/pythia-14m,mmlu_high_school_microeconomics,5-shot,accuracy,0.23529411764705882,0.027553614467863804
EleutherAI/pythia-14m,mmlu_high_school_psychology,5-shot,accuracy,0.2,0.01714985851425093
EleutherAI/pythia-14m,mmlu_human_sexuality,5-shot,accuracy,0.2366412213740458,0.03727673575596919
EleutherAI/pythia-14m,mmlu_professional_psychology,5-shot,accuracy,0.23202614379084968,0.01707737337785698
EleutherAI/pythia-14m,mmlu_public_relations,5-shot,accuracy,0.2727272727272727,0.04265792110940589
EleutherAI/pythia-14m,mmlu_security_studies,5-shot,accuracy,0.2612244897959184,0.02812342933514279
EleutherAI/pythia-14m,mmlu_sociology,5-shot,accuracy,0.22885572139303484,0.029705284056772422
EleutherAI/pythia-14m,mmlu_us_foreign_policy,5-shot,accuracy,0.23,0.042295258468165065
EleutherAI/pythia-14m,mmlu_abstract_algebra,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-14m,mmlu_anatomy,5-shot,accuracy,0.3111111111111111,0.03999262876617722
EleutherAI/pythia-14m,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-14m,mmlu_college_biology,5-shot,accuracy,0.2222222222222222,0.03476590104304134
EleutherAI/pythia-14m,mmlu_college_chemistry,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-14m,mmlu_college_computer_science,5-shot,accuracy,0.28,0.04512608598542127
EleutherAI/pythia-14m,mmlu_college_mathematics,5-shot,accuracy,0.24,0.04292346959909282
EleutherAI/pythia-14m,mmlu_college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237655
EleutherAI/pythia-14m,mmlu_computer_security,5-shot,accuracy,0.28,0.04512608598542127
EleutherAI/pythia-14m,mmlu_conceptual_physics,5-shot,accuracy,0.26382978723404255,0.02880998985410298
EleutherAI/pythia-14m,mmlu_electrical_engineering,5-shot,accuracy,0.2413793103448276,0.03565998174135302
EleutherAI/pythia-14m,mmlu_elementary_mathematics,5-shot,accuracy,0.23544973544973544,0.021851509822031715
EleutherAI/pythia-14m,mmlu_high_school_biology,5-shot,accuracy,0.2709677419354839,0.02528441611490016
EleutherAI/pythia-14m,mmlu_high_school_chemistry,5-shot,accuracy,0.29064039408866993,0.03194740072265541
EleutherAI/pythia-14m,mmlu_high_school_computer_science,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-14m,mmlu_high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.02646611753895991
EleutherAI/pythia-14m,mmlu_high_school_physics,5-shot,accuracy,0.2119205298013245,0.03336767086567977
EleutherAI/pythia-14m,mmlu_high_school_statistics,5-shot,accuracy,0.47685185185185186,0.03406315360711507
EleutherAI/pythia-14m,mmlu_machine_learning,5-shot,accuracy,0.23214285714285715,0.04007341809755806
EleutherAI/pythia-1b,mmlu_formal_logic,5-shot,accuracy,0.23809523809523808,0.038095238095238106
EleutherAI/pythia-1b,mmlu_high_school_european_history,5-shot,accuracy,0.20606060606060606,0.03158415324047709
EleutherAI/pythia-1b,mmlu_high_school_us_history,5-shot,accuracy,0.27450980392156865,0.031321798030832904
EleutherAI/pythia-1b,mmlu_high_school_world_history,5-shot,accuracy,0.29957805907172996,0.029818024749753095
EleutherAI/pythia-1b,mmlu_international_law,5-shot,accuracy,0.3140495867768595,0.042369647530410184
EleutherAI/pythia-1b,mmlu_jurisprudence,5-shot,accuracy,0.21296296296296297,0.039578354719809784
EleutherAI/pythia-1b,mmlu_logical_fallacies,5-shot,accuracy,0.294478527607362,0.03581165790474082
EleutherAI/pythia-1b,mmlu_moral_disputes,5-shot,accuracy,0.21965317919075145,0.022289638852617904
EleutherAI/pythia-1b,mmlu_moral_scenarios,5-shot,accuracy,0.2659217877094972,0.014776765066438888
EleutherAI/pythia-1b,mmlu_philosophy,5-shot,accuracy,0.29260450160771706,0.025839898334877983
EleutherAI/pythia-1b,mmlu_prehistory,5-shot,accuracy,0.22530864197530864,0.023246202647819746
EleutherAI/pythia-1b,mmlu_professional_law,5-shot,accuracy,0.2438070404172099,0.010966507972178475
EleutherAI/pythia-1b,mmlu_world_religions,5-shot,accuracy,0.27485380116959063,0.03424042924691582
EleutherAI/pythia-1b,mmlu_business_ethics,5-shot,accuracy,0.23,0.04229525846816505
EleutherAI/pythia-1b,mmlu_clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.02825420034443866
EleutherAI/pythia-1b,mmlu_college_medicine,5-shot,accuracy,0.20809248554913296,0.030952890217749895
EleutherAI/pythia-1b,mmlu_global_facts,5-shot,accuracy,0.18,0.03861229196653694
EleutherAI/pythia-1b,mmlu_human_aging,5-shot,accuracy,0.1210762331838565,0.021894174113185758
EleutherAI/pythia-1b,mmlu_management,5-shot,accuracy,0.36893203883495146,0.047776151811567386
EleutherAI/pythia-1b,mmlu_marketing,5-shot,accuracy,0.2948717948717949,0.029872577708891176
EleutherAI/pythia-1b,mmlu_medical_genetics,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-1b,mmlu_miscellaneous,5-shot,accuracy,0.20051085568326948,0.014317653708594204
EleutherAI/pythia-1b,mmlu_nutrition,5-shot,accuracy,0.2581699346405229,0.025058503316958157
EleutherAI/pythia-1b,mmlu_professional_accounting,5-shot,accuracy,0.23049645390070922,0.025123739226872405
EleutherAI/pythia-1b,mmlu_professional_medicine,5-shot,accuracy,0.3713235294117647,0.029349803139765866
EleutherAI/pythia-1b,mmlu_virology,5-shot,accuracy,0.2469879518072289,0.03357351982064537
EleutherAI/pythia-1b,mmlu_econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
EleutherAI/pythia-1b,mmlu_high_school_geography,5-shot,accuracy,0.3333333333333333,0.03358618145732523
EleutherAI/pythia-1b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.33678756476683935,0.034107802518361846
EleutherAI/pythia-1b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.3282051282051282,0.023807633198657266
EleutherAI/pythia-1b,mmlu_high_school_microeconomics,5-shot,accuracy,0.21008403361344538,0.026461398717471874
EleutherAI/pythia-1b,mmlu_high_school_psychology,5-shot,accuracy,0.3486238532110092,0.020431254090714328
EleutherAI/pythia-1b,mmlu_human_sexuality,5-shot,accuracy,0.2824427480916031,0.03948406125768361
EleutherAI/pythia-1b,mmlu_professional_psychology,5-shot,accuracy,0.25163398692810457,0.017555818091322273
EleutherAI/pythia-1b,mmlu_public_relations,5-shot,accuracy,0.20909090909090908,0.03895091015724137
EleutherAI/pythia-1b,mmlu_security_studies,5-shot,accuracy,0.4,0.031362502409358936
EleutherAI/pythia-1b,mmlu_sociology,5-shot,accuracy,0.25870646766169153,0.030965903123573026
EleutherAI/pythia-1b,mmlu_us_foreign_policy,5-shot,accuracy,0.24,0.04292346959909281
EleutherAI/pythia-1b,mmlu_abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-1b,mmlu_anatomy,5-shot,accuracy,0.3037037037037037,0.039725528847851375
EleutherAI/pythia-1b,mmlu_astronomy,5-shot,accuracy,0.23026315789473684,0.03426059424403165
EleutherAI/pythia-1b,mmlu_college_biology,5-shot,accuracy,0.24305555555555555,0.03586879280080341
EleutherAI/pythia-1b,mmlu_college_chemistry,5-shot,accuracy,0.26,0.04408440022768078
EleutherAI/pythia-1b,mmlu_college_computer_science,5-shot,accuracy,0.27,0.04461960433384741
EleutherAI/pythia-1b,mmlu_college_mathematics,5-shot,accuracy,0.28,0.045126085985421296
EleutherAI/pythia-1b,mmlu_college_physics,5-shot,accuracy,0.29411764705882354,0.04533838195929775
EleutherAI/pythia-1b,mmlu_computer_security,5-shot,accuracy,0.18,0.03861229196653694
EleutherAI/pythia-1b,mmlu_conceptual_physics,5-shot,accuracy,0.2127659574468085,0.026754391348039766
EleutherAI/pythia-1b,mmlu_electrical_engineering,5-shot,accuracy,0.25517241379310346,0.03632984052707842
EleutherAI/pythia-1b,mmlu_elementary_mathematics,5-shot,accuracy,0.24074074074074073,0.022019080012217883
EleutherAI/pythia-1b,mmlu_high_school_biology,5-shot,accuracy,0.19032258064516128,0.022331707611823078
EleutherAI/pythia-1b,mmlu_high_school_chemistry,5-shot,accuracy,0.27586206896551724,0.031447125816782405
EleutherAI/pythia-1b,mmlu_high_school_computer_science,5-shot,accuracy,0.32,0.046882617226215034
EleutherAI/pythia-1b,mmlu_high_school_mathematics,5-shot,accuracy,0.2740740740740741,0.027195934804085626
EleutherAI/pythia-1b,mmlu_high_school_physics,5-shot,accuracy,0.2582781456953642,0.035737053147634576
EleutherAI/pythia-1b,mmlu_high_school_statistics,5-shot,accuracy,0.36574074074074076,0.03284738857647207
EleutherAI/pythia-1b,mmlu_machine_learning,5-shot,accuracy,0.19642857142857142,0.03770970049347019
Salesforce/codegen-350M-nl,mmlu_formal_logic,5-shot,accuracy,0.2222222222222222,0.03718489006818114
Salesforce/codegen-350M-nl,mmlu_high_school_european_history,5-shot,accuracy,0.2787878787878788,0.03501438706296781
Salesforce/codegen-350M-nl,mmlu_high_school_us_history,5-shot,accuracy,0.24509803921568626,0.030190282453501943
Salesforce/codegen-350M-nl,mmlu_high_school_world_history,5-shot,accuracy,0.25316455696202533,0.028304657943035286
Salesforce/codegen-350M-nl,mmlu_international_law,5-shot,accuracy,0.256198347107438,0.03984979653302871
Salesforce/codegen-350M-nl,mmlu_jurisprudence,5-shot,accuracy,0.21296296296296297,0.039578354719809784
Salesforce/codegen-350M-nl,mmlu_logical_fallacies,5-shot,accuracy,0.25153374233128833,0.034089978868575295
Salesforce/codegen-350M-nl,mmlu_moral_disputes,5-shot,accuracy,0.2254335260115607,0.02249723019096755
Salesforce/codegen-350M-nl,mmlu_moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217887
Salesforce/codegen-350M-nl,mmlu_philosophy,5-shot,accuracy,0.19292604501607716,0.022411516780911363
Salesforce/codegen-350M-nl,mmlu_prehistory,5-shot,accuracy,0.25,0.02409347123262133
Salesforce/codegen-350M-nl,mmlu_professional_law,5-shot,accuracy,0.24967405475880053,0.01105453837783233
Salesforce/codegen-350M-nl,mmlu_world_religions,5-shot,accuracy,0.2573099415204678,0.03352799844161865
Salesforce/codegen-350M-nl,mmlu_business_ethics,5-shot,accuracy,0.2,0.04020151261036843
Salesforce/codegen-350M-nl,mmlu_clinical_knowledge,5-shot,accuracy,0.2830188679245283,0.027724236492700904
Salesforce/codegen-350M-nl,mmlu_college_medicine,5-shot,accuracy,0.20809248554913296,0.030952890217749884
Salesforce/codegen-350M-nl,mmlu_global_facts,5-shot,accuracy,0.16,0.0368452949177471
Salesforce/codegen-350M-nl,mmlu_human_aging,5-shot,accuracy,0.23766816143497757,0.028568079464714274
Salesforce/codegen-350M-nl,mmlu_management,5-shot,accuracy,0.23300970873786409,0.04185832598928315
Salesforce/codegen-350M-nl,mmlu_marketing,5-shot,accuracy,0.2094017094017094,0.026655699653922737
Salesforce/codegen-350M-nl,mmlu_medical_genetics,5-shot,accuracy,0.28,0.04512608598542128
Salesforce/codegen-350M-nl,mmlu_miscellaneous,5-shot,accuracy,0.2771392081736909,0.016005636294122425
Salesforce/codegen-350M-nl,mmlu_nutrition,5-shot,accuracy,0.26143790849673204,0.025160998214292456
Salesforce/codegen-350M-nl,mmlu_professional_accounting,5-shot,accuracy,0.23049645390070922,0.025123739226872402
Salesforce/codegen-350M-nl,mmlu_professional_medicine,5-shot,accuracy,0.35294117647058826,0.0290294228156814
Salesforce/codegen-350M-nl,mmlu_virology,5-shot,accuracy,0.1927710843373494,0.03070982405056527
Salesforce/codegen-350M-nl,mmlu_econometrics,5-shot,accuracy,0.21052631578947367,0.0383515395439942
Salesforce/codegen-350M-nl,mmlu_high_school_geography,5-shot,accuracy,0.3333333333333333,0.03358618145732524
Salesforce/codegen-350M-nl,mmlu_high_school_government_and_politics,5-shot,accuracy,0.3626943005181347,0.03469713791704373
Salesforce/codegen-350M-nl,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2948717948717949,0.02311936275823228
Salesforce/codegen-350M-nl,mmlu_high_school_microeconomics,5-shot,accuracy,0.28991596638655465,0.029472485833136094
Salesforce/codegen-350M-nl,mmlu_high_school_psychology,5-shot,accuracy,0.29174311926605506,0.019489300968876532
Salesforce/codegen-350M-nl,mmlu_human_sexuality,5-shot,accuracy,0.22137404580152673,0.03641297081313729
Salesforce/codegen-350M-nl,mmlu_professional_psychology,5-shot,accuracy,0.2581699346405229,0.01770453165325007
Salesforce/codegen-350M-nl,mmlu_public_relations,5-shot,accuracy,0.2,0.03831305140884604
Salesforce/codegen-350M-nl,mmlu_security_studies,5-shot,accuracy,0.27346938775510204,0.028535560337128448
Salesforce/codegen-350M-nl,mmlu_sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409217
Salesforce/codegen-350M-nl,mmlu_us_foreign_policy,5-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-350M-nl,mmlu_abstract_algebra,5-shot,accuracy,0.18,0.038612291966536955
Salesforce/codegen-350M-nl,mmlu_anatomy,5-shot,accuracy,0.2518518518518518,0.03749850709174021
Salesforce/codegen-350M-nl,mmlu_astronomy,5-shot,accuracy,0.21710526315789475,0.03355045304882924
Salesforce/codegen-350M-nl,mmlu_college_biology,5-shot,accuracy,0.2916666666666667,0.03800968060554858
Salesforce/codegen-350M-nl,mmlu_college_chemistry,5-shot,accuracy,0.36,0.04824181513244218
Salesforce/codegen-350M-nl,mmlu_college_computer_science,5-shot,accuracy,0.32,0.04688261722621504
Salesforce/codegen-350M-nl,mmlu_college_mathematics,5-shot,accuracy,0.28,0.04512608598542127
Salesforce/codegen-350M-nl,mmlu_college_physics,5-shot,accuracy,0.2647058823529412,0.04389869956808778
Salesforce/codegen-350M-nl,mmlu_computer_security,5-shot,accuracy,0.2,0.04020151261036846
Salesforce/codegen-350M-nl,mmlu_conceptual_physics,5-shot,accuracy,0.2851063829787234,0.029513196625539355
Salesforce/codegen-350M-nl,mmlu_electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
Salesforce/codegen-350M-nl,mmlu_elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918403
Salesforce/codegen-350M-nl,mmlu_high_school_biology,5-shot,accuracy,0.2838709677419355,0.02564938106302926
Salesforce/codegen-350M-nl,mmlu_high_school_chemistry,5-shot,accuracy,0.2660098522167488,0.031089826002937523
Salesforce/codegen-350M-nl,mmlu_high_school_computer_science,5-shot,accuracy,0.18,0.03861229196653694
Salesforce/codegen-350M-nl,mmlu_high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.026719240783712152
Salesforce/codegen-350M-nl,mmlu_high_school_physics,5-shot,accuracy,0.23841059602649006,0.03479185572599661
Salesforce/codegen-350M-nl,mmlu_high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
Salesforce/codegen-350M-nl,mmlu_machine_learning,5-shot,accuracy,0.2767857142857143,0.042466243366976256
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,bleu_max,22.976446868635964,0.7322176924366015
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,bleu_acc,0.30599755201958384,0.016132229728155017
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,bleu_diff,-5.669552217028437,0.6752356618788148
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rouge1_max,47.76329294347572,0.8469182304680604
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rouge1_acc,0.27906976744186046,0.01570210709062793
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rouge1_diff,-8.916832117533176,0.7274305998629956
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rouge2_max,30.95793752645541,0.9470708736995246
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rouge2_acc,0.23623011015911874,0.014869755015871107
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rouge2_diff,-9.479709959757859,0.859623261031919
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rougeL_max,44.80001099182031,0.8553535173955262
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rougeL_acc,0.26560587515299877,0.015461027627253588
HuggingFaceTB/SmolLM-135M,truthfulqa_gen,0-shot,rougeL_diff,-8.643391122447083,0.7201080342491195
HuggingFaceTB/SmolLM-135M,truthfulqa_mc1,0-shot,accuracy,0.2521419828641371,0.015201522246299962
HuggingFaceTB/SmolLM-135M,truthfulqa_mc2,0-shot,accuracy,0.39257749912676493,0.01455245185272414
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,bleu_max,13.82766815700744,0.5154784183208384
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,bleu_acc,0.39167686658506734,0.017087795881769643
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,bleu_diff,-0.3071316802783103,0.48061175573860104
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rouge1_max,34.23389894145755,0.7612633125881414
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rouge1_acc,0.35862913096695226,0.016789289499502022
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rouge1_diff,-1.3810176678838904,0.7542591368961628
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rouge2_max,16.07909085959659,0.7862657983849908
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rouge2_acc,0.1909424724602203,0.013759285842685716
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rouge2_diff,-2.05928101776518,0.7203850974001921
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rougeL_max,31.690463611915124,0.7444436261611048
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rougeL_acc,0.3659730722154223,0.016862941684088383
EleutherAI/gpt-neo-125m,truthfulqa_gen,0-shot,rougeL_diff,-0.9361203963395062,0.7449460193846943
EleutherAI/gpt-neo-125m,truthfulqa_mc1,0-shot,accuracy,0.2582619339045288,0.015321821688476187
EleutherAI/gpt-neo-125m,truthfulqa_mc2,0-shot,accuracy,0.4557937365618801,0.015399305165519283
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,bleu_max,0.25189047657908453,0.04037026183161743
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,bleu_acc,0.22766217870257038,0.014679255032111066
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,bleu_diff,0.060071009059402276,0.026622709245018045
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rouge1_max,1.6710311652747238,0.10132139326544636
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rouge1_acc,0.3402692778457772,0.016586304901762564
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rouge1_diff,0.06397807601782618,0.04726377412147976
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rouge2_max,0.20145515631344832,0.07336194298219119
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rouge2_acc,0.03916768665850673,0.006791139969381785
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rouge2_diff,0.10863179700937718,0.023528012031076795
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rougeL_max,1.324601279649833,0.0977374874735816
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rougeL_acc,0.3072215422276622,0.01615020132132305
EleutherAI/pythia-14m,truthfulqa_gen,0-shot,rougeL_diff,0.06262566632400823,0.040955336270325944
EleutherAI/pythia-14m,truthfulqa_mc1,0-shot,accuracy,0.28886168910648713,0.01586634640138431
EleutherAI/pythia-14m,truthfulqa_mc2,0-shot,accuracy,0.5045372302075719,0.016071873715769958
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,bleu_max,17.38995428908282,0.5764198587628316
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,bleu_acc,0.3843329253365973,0.01702870730124523
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,bleu_diff,-4.175783199317602,0.5944771377329947
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rouge1_max,38.551539480776576,0.8251759257776778
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rouge1_acc,0.29008567931456547,0.015886236874209515
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rouge1_diff,-8.011749165750295,0.8122661612506262
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rouge2_max,20.028598109103235,0.8731560575743161
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rouge2_acc,0.15667074663402691,0.01272469729710282
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rouge2_diff,-8.988714954961019,0.8283402743171125
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rougeL_max,36.09734674406897,0.8123228435459117
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rougeL_acc,0.29498164014687883,0.015964400965589647
EleutherAI/pythia-1b-deduped,truthfulqa_gen,0-shot,rougeL_diff,-7.643852853052882,0.8107919114799352
EleutherAI/pythia-1b-deduped,truthfulqa_mc1,0-shot,accuracy,0.22643818849449204,0.014651337324602597
EleutherAI/pythia-1b-deduped,truthfulqa_mc2,0-shot,accuracy,0.3893826763431802,0.014315058922408686
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,bleu_max,11.559657012882608,0.4489429024279205
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,bleu_acc,0.4541003671970624,0.017429593091323522
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,bleu_diff,1.4000908646313832,0.3487959248177008
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rouge1_max,30.402067578135277,0.7266605758317433
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rouge1_acc,0.45165238678090575,0.017421480300277643
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rouge1_diff,4.02877441294543,0.7054076480080201
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rouge2_max,15.71551935984095,0.7157497029535042
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rouge2_acc,0.2962056303549572,0.015983595101811385
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rouge2_diff,3.475169643017325,0.6479636860787624
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rougeL_max,28.910308170307523,0.7145007910108319
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rougeL_acc,0.4589963280293758,0.017444544447661213
EleutherAI/pythia-70m,truthfulqa_gen,0-shot,rougeL_diff,4.335874804650357,0.6996097924624064
EleutherAI/pythia-70m,truthfulqa_mc1,0-shot,accuracy,0.2386780905752754,0.014922629695456416
EleutherAI/pythia-70m,truthfulqa_mc2,0-shot,accuracy,0.4704188330376441,0.015496119891378045
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,bleu_max,13.863489217499817,0.5003825869177526
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,bleu_acc,0.38555691554467564,0.017038839010591646
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,bleu_diff,-1.4631965094522126,0.4407555523552308
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rouge1_max,35.37054285515339,0.7329640387170557
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rouge1_acc,0.3488372093023256,0.01668441985998692
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rouge1_diff,-3.3741552095854512,0.6938574784135956
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rouge2_max,16.598782174134563,0.7469905866744578
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rouge2_acc,0.19828641370869032,0.01395760878338558
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rouge2_diff,-4.036971192147265,0.6746445926356409
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rougeL_max,32.73859389434468,0.7169123677156831
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rougeL_acc,0.3623011015911873,0.016826646897262258
EleutherAI/pythia-160m,truthfulqa_gen,0-shot,rougeL_diff,-2.7944114776141054,0.6851547483346115
EleutherAI/pythia-160m,truthfulqa_mc1,0-shot,accuracy,0.23990208078335373,0.014948812679062135
EleutherAI/pythia-160m,truthfulqa_mc2,0-shot,accuracy,0.4454914412486683,0.014968421539123982
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,bleu_max,20.598836860517356,0.6851467445643332
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,bleu_acc,0.30599755201958384,0.016132229728155017
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,bleu_diff,-6.086968824238429,0.7088694323459431
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rouge1_max,43.57229739260268,0.8692552756396812
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rouge1_acc,0.2631578947368421,0.015415241740236986
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rouge1_diff,-9.887773751253073,0.8289572470743722
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rouge2_max,25.763175973275516,0.956209074124331
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rouge2_acc,0.18482252141982863,0.013588091176049527
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rouge2_diff,-11.590380512744343,0.9479605854084453
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rougeL_max,40.90498907091988,0.8711216012613251
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rougeL_acc,0.24479804161566707,0.01505186948671501
EleutherAI/pythia-1.4b-deduped,truthfulqa_gen,0-shot,rougeL_diff,-9.968768105353298,0.8221821762692728
EleutherAI/pythia-1.4b-deduped,truthfulqa_mc1,0-shot,accuracy,0.22643818849449204,0.014651337324602595
EleutherAI/pythia-1.4b-deduped,truthfulqa_mc2,0-shot,accuracy,0.38627648254755087,0.013996768184817428
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,bleu_max,20.14377865976535,0.6676659406833327
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,bleu_acc,0.3684210526315789,0.016886551261046042
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,bleu_diff,-2.591624945022532,0.6274732091433534
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rouge1_max,43.146965367694236,0.8817643715382342
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rouge1_acc,0.30599755201958384,0.016132229728155034
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rouge1_diff,-5.212749620633643,0.8527293047979319
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rouge2_max,26.228706092192986,0.9581938298704613
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rouge2_acc,0.2252141982864137,0.014623240768023524
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rouge2_diff,-5.476081703097713,0.8905805137155465
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rougeL_max,40.51279341562454,0.8836785784151249
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rougeL_acc,0.3023255813953488,0.016077509266133036
EleutherAI/pythia-410m-deduped,truthfulqa_gen,0-shot,rougeL_diff,-4.500312127698717,0.8349787361020812
EleutherAI/pythia-410m-deduped,truthfulqa_mc1,0-shot,accuracy,0.23745410036719705,0.014896277441041845
EleutherAI/pythia-410m-deduped,truthfulqa_mc2,0-shot,accuracy,0.40903900073456295,0.014551742456342961
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,bleu_max,18.0747370935203,0.5926715375640783
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,bleu_acc,0.3427172582619339,0.016614949385347008
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,bleu_diff,-5.0014076723854615,0.6189429578709624
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rouge1_max,39.570763830712075,0.8614629936913234
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rouge1_acc,0.2692778457772338,0.01552856663708727
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rouge1_diff,-8.616743212239076,0.821911308286023
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rouge2_max,22.209365274166228,0.8880814176786992
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rouge2_acc,0.18115055079559364,0.01348269718781789
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rouge2_diff,-8.512649455472609,0.8438831243220375
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rougeL_max,36.92410946128289,0.8500947711273277
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rougeL_acc,0.26805385556915545,0.015506204722834548
EleutherAI/pythia-1b,truthfulqa_gen,0-shot,rougeL_diff,-8.5388927663562,0.8163979108946533
EleutherAI/pythia-1b,truthfulqa_mc1,0-shot,accuracy,0.23745410036719705,0.01489627744104185
EleutherAI/pythia-1b,truthfulqa_mc2,0-shot,accuracy,0.40471485106070104,0.014452792232467825
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,bleu_max,16.17996790310543,0.538437135170166
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,bleu_acc,0.39657282741738065,0.017124930942023518
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,bleu_diff,-3.1392135812443236,0.541463011580264
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rouge1_max,36.922410388721936,0.8165708623911705
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rouge1_acc,0.29498164014687883,0.01596440096558965
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rouge1_diff,-7.010457020693764,0.7691081708034396
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rouge2_max,18.972129850863173,0.8300565504676285
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rouge2_acc,0.17870257037943696,0.013411289952324447
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rouge2_diff,-7.6948138324861555,0.7614880065176521
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rougeL_max,34.15199293616335,0.7937093978709431
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rougeL_acc,0.2937576499388005,0.015945068581236628
EleutherAI/pythia-410m,truthfulqa_gen,0-shot,rougeL_diff,-6.918295350306394,0.7619294550157766
EleutherAI/pythia-410m,truthfulqa_mc1,0-shot,accuracy,0.23623011015911874,0.014869755015871093
EleutherAI/pythia-410m,truthfulqa_mc2,0-shot,accuracy,0.4123295875548702,0.01456736352889254
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,bleu_max,14.153287244100811,0.4968201438217264
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,bleu_acc,0.42105263157894735,0.017283936248136487
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,bleu_diff,-1.0598686369147488,0.4851227345947649
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rouge1_max,34.76380921741576,0.7396791317055083
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rouge1_acc,0.3635250917992656,0.016838862883965845
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rouge1_diff,-2.0556758948407894,0.7750045428405516
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rouge2_max,15.636895199393091,0.7752069556029646
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rouge2_acc,0.1909424724602203,0.013759285842685712
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rouge2_diff,-3.4900443083817807,0.7223809769877706
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rougeL_max,32.3937303970283,0.7267662482486981
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rougeL_acc,0.3769889840881273,0.016965517578930358
EleutherAI/pythia-160m-deduped,truthfulqa_gen,0-shot,rougeL_diff,-1.6077783248866415,0.7633972282035191
EleutherAI/pythia-160m-deduped,truthfulqa_mc1,0-shot,accuracy,0.24479804161566707,0.015051869486714999
EleutherAI/pythia-160m-deduped,truthfulqa_mc2,0-shot,accuracy,0.4415444351512997,0.015186568263342463
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,bleu_max,21.862414630647404,0.7011100350795473
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,bleu_acc,0.2998776009791922,0.01604035296671365
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,bleu_diff,-5.754993284829954,0.7264902560692984
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rouge1_max,45.98431374045107,0.8670259613224951
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rouge1_acc,0.26193390452876375,0.015392118805015034
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rouge1_diff,-8.485229970104195,0.8809008875013503
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rouge2_max,28.706302237514784,0.966892516029998
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rouge2_acc,0.20563035495716034,0.01414848221946095
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rouge2_diff,-10.223621796361323,0.9901321148095541
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rougeL_max,43.272527641849415,0.8663453978451358
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rougeL_acc,0.2484700122399021,0.015127427096520667
EleutherAI/pythia-1.4b,truthfulqa_gen,0-shot,rougeL_diff,-8.591027086730971,0.8722786829336309
EleutherAI/pythia-1.4b,truthfulqa_mc1,0-shot,accuracy,0.22643818849449204,0.014651337324602587
EleutherAI/pythia-1.4b,truthfulqa_mc2,0-shot,accuracy,0.38853365767172504,0.014259980857105628
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,bleu_max,22.51180868410586,0.7394846457109565
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,bleu_acc,0.3084455324357405,0.01616803938315687
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,bleu_diff,-5.824356060732945,0.7763279013743432
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rouge1_max,46.029510636886165,0.9066849126304414
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rouge1_acc,0.26193390452876375,0.015392118805015044
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rouge1_diff,-8.44358149088742,0.908074882927902
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rouge2_max,29.282589920842735,0.9979255385402216
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rouge2_acc,0.193390452876377,0.013826240752599066
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rouge2_diff,-10.259296916363766,1.021487866908179
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rougeL_max,43.1617978919707,0.9037909996175137
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rougeL_acc,0.25458996328029376,0.01525011707915652
Salesforce/codegen-2B-nl,truthfulqa_gen,0-shot,rougeL_diff,-8.67741920690234,0.9102763655264928
Salesforce/codegen-2B-nl,truthfulqa_mc1,0-shot,accuracy,0.23255813953488372,0.014789157531080517
Salesforce/codegen-2B-nl,truthfulqa_mc2,0-shot,accuracy,0.3815496354696802,0.0137873534282119
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,bleu_max,16.207907692763964,0.5795104940469697
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,bleu_acc,0.3463892288861689,0.016656997109125132
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,bleu_diff,-3.4259378621262586,0.562542186711278
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rouge1_max,37.50687330184676,0.8345080012732387
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rouge1_acc,0.26438188494492043,0.01543821111952251
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rouge1_diff,-6.334771922993749,0.7414672039374677
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rouge2_max,20.23171010594276,0.8611951448500218
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rouge2_acc,0.19706242350061198,0.013925080734473759
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rouge2_diff,-6.682956931555464,0.8107247822414363
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rougeL_max,34.63275735027349,0.8155711614165474
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rougeL_acc,0.27050183598531213,0.015550778332842895
Salesforce/codegen-350M-nl,truthfulqa_gen,0-shot,rougeL_diff,-6.010234321323337,0.7354612771469404
Salesforce/codegen-350M-nl,truthfulqa_mc1,0-shot,accuracy,0.24724602203182375,0.015102404797359652
Salesforce/codegen-350M-nl,truthfulqa_mc2,0-shot,accuracy,0.4126613376056213,0.014765264873751623
Salesforce/codegen-350M-nl,winogrande,5-shot,accuracy,0.5453827940015785,0.013994481027065993
Salesforce/codegen-2B-nl,winogrande,5-shot,accuracy,0.6124704025256511,0.01369235463601677
EleutherAI/pythia-1b,winogrande,5-shot,accuracy,0.5256511444356748,0.014033980956108558
EleutherAI/pythia-1.4b,winogrande,5-shot,accuracy,0.5761641673243884,0.013888492389944499
EleutherAI/pythia-14m,winogrande,5-shot,accuracy,0.5067087608524072,0.014051220692330349
EleutherAI/gpt-neo-125m,winogrande,5-shot,accuracy,0.5169692186266772,0.01404439040161297
HuggingFaceTB/SmolLM-135M,winogrande,5-shot,accuracy,0.5209155485398579,0.014040185494212945
LLM360/CrystalCoder,arc_challenge,25-shot,accuracy,0.2380546075085324,0.012445770028026206
LLM360/CrystalCoder,arc_challenge,25-shot,acc_norm,0.2883959044368601,0.013238394422428157
Salesforce/codegen-2B-nl,arc_challenge,25-shot,accuracy,0.34897610921501704,0.0139289334613825
Salesforce/codegen-2B-nl,arc_challenge,25-shot,acc_norm,0.378839590443686,0.014175915490000319
EleutherAI/pythia-2.8b,arc_challenge,25-shot,accuracy,0.318259385665529,0.013611993916971451
EleutherAI/pythia-2.8b,arc_challenge,25-shot,acc_norm,0.3643344709897611,0.014063260279882413
Qwen/Qwen2.5-3B,arithmetic_1dc,5-shot,accuracy,0.0355,0.004138651860160548
Qwen/Qwen2.5-3B,arithmetic_2da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-3B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-3B,arithmetic_2ds,5-shot,accuracy,0.0065,0.0017973564602277766
Qwen/Qwen2.5-3B,arithmetic_3da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-3B,arithmetic_3ds,5-shot,accuracy,0.0165,0.0028491988289663555
Qwen/Qwen2.5-3B,arithmetic_4da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-3B,arithmetic_4ds,5-shot,accuracy,0.0065,0.0017973564602277766
Qwen/Qwen2.5-3B,arithmetic_5da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-3B,arithmetic_5ds,5-shot,accuracy,0.0095,0.0021696148539100306
Qwen/Qwen-7B,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_2da,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_3da,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_4da,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_5da,5-shot,accuracy,0.0,0.0
Qwen/Qwen-7B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_2da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_3da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_4da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_5da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_2da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_3da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_4da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_5da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-6B-nl,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_2da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_3da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_4da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_5da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-350M-nl,arithmetic_5ds,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_1dc,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_2da,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_2dm,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_2ds,5-shot,accuracy,0.0005,0.0005000000000000042
LLM360/Amber,arithmetic_3da,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005000000000000035
LLM360/Amber,arithmetic_4da,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_4ds,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_5da,5-shot,accuracy,0.0,0.0
LLM360/Amber,arithmetic_5ds,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_1dc,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_2da,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_2dm,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_2ds,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_3da,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_3ds,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_4da,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_4ds,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_5da,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neo-125m,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-410m-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-6.9b-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-2.8b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-70m,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-160m,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-14m,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-1.4b-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_2da,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_3da,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_4da,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_5da,5-shot,accuracy,0.0,0.0
google/gemma-2-2b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_2da,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_3da,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_4da,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_5da,5-shot,accuracy,0.0,0.0
google/gemma-7b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_1dc,5-shot,accuracy,0.004,0.001411735279097714
microsoft/phi-2,arithmetic_2da,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_2dm,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_2ds,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_3da,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_3ds,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_4da,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_4ds,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_5da,5-shot,accuracy,0.0,0.0
microsoft/phi-2,arithmetic_5ds,5-shot,accuracy,0.0,0.0
microsoft/phi-1_5,arithmetic_1dc,5-shot,accuracy,0.004,0.0014117352790976926
microsoft/phi-1_5,arithmetic_2da,5-shot,accuracy,0.001,0.0007069298939339473
microsoft/phi-1_5,arithmetic_2dm,5-shot,accuracy,0.0,0.0
microsoft/phi-1_5,arithmetic_2ds,5-shot,accuracy,0.014,0.0026278228110668093
microsoft/phi-1_5,arithmetic_3da,5-shot,accuracy,0.0005,0.0005000000000000137
microsoft/phi-1_5,arithmetic_3ds,5-shot,accuracy,0.0,0.0
microsoft/phi-1_5,arithmetic_4da,5-shot,accuracy,0.0,0.0
microsoft/phi-1_5,arithmetic_4ds,5-shot,accuracy,0.0,0.0
microsoft/phi-1_5,arithmetic_5da,5-shot,accuracy,0.0,0.0
microsoft/phi-1_5,arithmetic_5ds,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_1dc,5-shot,accuracy,0.0125,0.002484947178762672
HuggingFaceTB/SmolLM-360M,arithmetic_2da,5-shot,accuracy,0.002,0.0009992493430694917
HuggingFaceTB/SmolLM-360M,arithmetic_2dm,5-shot,accuracy,0.014,0.0026278228110668163
HuggingFaceTB/SmolLM-360M,arithmetic_2ds,5-shot,accuracy,0.04,0.0043828763161194995
HuggingFaceTB/SmolLM-360M,arithmetic_3da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521591
HuggingFaceTB/SmolLM-360M,arithmetic_4da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_4ds,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_5da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-360M,arithmetic_5ds,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-1.7B,arithmetic_1dc,5-shot,accuracy,0.0005,0.0005000000000000148
HuggingFaceTB/SmolLM-1.7B,arithmetic_2da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-1.7B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-1.7B,arithmetic_2ds,5-shot,accuracy,0.0115,0.0023846841214675827
HuggingFaceTB/SmolLM-1.7B,arithmetic_3da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-1.7B,arithmetic_3ds,5-shot,accuracy,0.0025,0.001116914835327551
HuggingFaceTB/SmolLM-1.7B,arithmetic_4da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-1.7B,arithmetic_4ds,5-shot,accuracy,0.001,0.0007069298939339403
HuggingFaceTB/SmolLM-1.7B,arithmetic_5da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-1.7B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339513
HuggingFaceTB/SmolLM-135M,arithmetic_2da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_2dm,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_2ds,5-shot,accuracy,0.0005,0.0005000000000000052
HuggingFaceTB/SmolLM-135M,arithmetic_3da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_3ds,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_4da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_4ds,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_5da,5-shot,accuracy,0.0,0.0
HuggingFaceTB/SmolLM-135M,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-2B-nl,gsm8k,5-shot,accuracy,0.019711902956785442,0.003828982978735705
EleutherAI/pythia-2.8b,gsm8k,5-shot,accuracy,0.01819560272934041,0.003681611894073874
LLM360/CrystalCoder,hellaswag,10-shot,accuracy,0.25781716789484166,0.004365388351563077
LLM360/CrystalCoder,hellaswag,10-shot,acc_norm,0.2665803624775941,0.004412674170976467
Salesforce/codegen-2B-nl,hellaswag,10-shot,accuracy,0.46614220274845647,0.004978328190775528
Salesforce/codegen-2B-nl,hellaswag,10-shot,acc_norm,0.6288587930691097,0.004821228034624847
EleutherAI/pythia-2.8b,hellaswag,10-shot,accuracy,0.45249950209121687,0.004967213515483205
EleutherAI/pythia-2.8b,hellaswag,10-shot,acc_norm,0.6089424417446724,0.004869899297734548
Qwen/Qwen-7B,lambada_openai,0-shot,perplexity,3.9320442107272187,0.08593864703848028
Qwen/Qwen-7B,lambada_openai,0-shot,accuracy,0.6960993595963516,0.006407867125328475
Qwen/Qwen-7B,lambada_standard,0-shot,perplexity,5.101527531720394,0.11859344566507872
Qwen/Qwen-7B,lambada_standard,0-shot,accuracy,0.6396274015136814,0.00668885041433858
EleutherAI/pythia-2.8b,lambada_openai,0-shot,perplexity,5.038880069234476,0.11938809275869157
EleutherAI/pythia-2.8b,lambada_openai,0-shot,accuracy,0.6483601785367747,0.0066522607629188445
EleutherAI/pythia-2.8b,lambada_standard,0-shot,perplexity,8.231087909955143,0.22379640064438627
EleutherAI/pythia-2.8b,lambada_standard,0-shot,accuracy,0.5433727925480303,0.006939719384611019
EleutherAI/pythia-6.9b-deduped,mmlu_formal_logic,0-shot,accuracy,0.23809523809523808,0.038095238095238126
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.23636363636363636,0.03317505930009179
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.25980392156862747,0.030778554678693247
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.29535864978902954,0.02969633871342288
EleutherAI/pythia-6.9b-deduped,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.03941897526516303
EleutherAI/pythia-6.9b-deduped,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094633
EleutherAI/pythia-6.9b-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.2392638036809816,0.033519538795212696
EleutherAI/pythia-6.9b-deduped,mmlu_moral_disputes,0-shot,accuracy,0.24277456647398843,0.0230836585869842
EleutherAI/pythia-6.9b-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.2737430167597765,0.014912413096372432
EleutherAI/pythia-6.9b-deduped,mmlu_philosophy,0-shot,accuracy,0.27009646302250806,0.02521804037341062
EleutherAI/pythia-6.9b-deduped,mmlu_prehistory,0-shot,accuracy,0.25617283950617287,0.0242885336377261
EleutherAI/pythia-6.9b-deduped,mmlu_professional_law,0-shot,accuracy,0.2561929595827901,0.011149173153110582
EleutherAI/pythia-6.9b-deduped,mmlu_world_religions,0-shot,accuracy,0.23391812865497075,0.03246721765117826
EleutherAI/pythia-6.9b-deduped,mmlu_business_ethics,0-shot,accuracy,0.26,0.04408440022768078
EleutherAI/pythia-6.9b-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.2528301886792453,0.026749899771241235
EleutherAI/pythia-6.9b-deduped,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.03186209851641144
EleutherAI/pythia-6.9b-deduped,mmlu_global_facts,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-6.9b-deduped,mmlu_human_aging,0-shot,accuracy,0.3452914798206278,0.03191100192835794
EleutherAI/pythia-6.9b-deduped,mmlu_management,0-shot,accuracy,0.3106796116504854,0.0458212416016155
EleutherAI/pythia-6.9b-deduped,mmlu_marketing,0-shot,accuracy,0.23931623931623933,0.027951826808924333
EleutherAI/pythia-6.9b-deduped,mmlu_medical_genetics,0-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-6.9b-deduped,mmlu_miscellaneous,0-shot,accuracy,0.24393358876117496,0.015357212665829479
EleutherAI/pythia-6.9b-deduped,mmlu_nutrition,0-shot,accuracy,0.25163398692810457,0.0248480182638752
EleutherAI/pythia-6.9b-deduped,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
EleutherAI/pythia-6.9b-deduped,mmlu_professional_medicine,0-shot,accuracy,0.3382352941176471,0.02873932851398358
EleutherAI/pythia-6.9b-deduped,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.036108050180310235
EleutherAI/pythia-6.9b-deduped,mmlu_econometrics,0-shot,accuracy,0.21052631578947367,0.038351539543994194
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_geography,0-shot,accuracy,0.23232323232323232,0.030088629490217483
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.24870466321243523,0.031195840877700304
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.32051282051282054,0.02366129639396428
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.3319327731092437,0.030588697013783667
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.25688073394495414,0.01873249292834246
EleutherAI/pythia-6.9b-deduped,mmlu_human_sexuality,0-shot,accuracy,0.22900763358778625,0.036853466317118506
EleutherAI/pythia-6.9b-deduped,mmlu_professional_psychology,0-shot,accuracy,0.238562091503268,0.017242385828779606
EleutherAI/pythia-6.9b-deduped,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.04265792110940589
EleutherAI/pythia-6.9b-deduped,mmlu_security_studies,0-shot,accuracy,0.21224489795918366,0.026176967197866764
EleutherAI/pythia-6.9b-deduped,mmlu_sociology,0-shot,accuracy,0.21393034825870647,0.028996909693328937
EleutherAI/pythia-6.9b-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-6.9b-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-6.9b-deduped,mmlu_anatomy,0-shot,accuracy,0.2962962962962963,0.03944624162501117
EleutherAI/pythia-6.9b-deduped,mmlu_astronomy,0-shot,accuracy,0.2894736842105263,0.03690677986137282
EleutherAI/pythia-6.9b-deduped,mmlu_college_biology,0-shot,accuracy,0.2013888888888889,0.033536474697138406
EleutherAI/pythia-6.9b-deduped,mmlu_college_chemistry,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-6.9b-deduped,mmlu_college_computer_science,0-shot,accuracy,0.28,0.04512608598542127
EleutherAI/pythia-6.9b-deduped,mmlu_college_mathematics,0-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-6.9b-deduped,mmlu_college_physics,0-shot,accuracy,0.3137254901960784,0.04617034827006718
EleutherAI/pythia-6.9b-deduped,mmlu_computer_security,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-6.9b-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2978723404255319,0.029896145682095462
EleutherAI/pythia-6.9b-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.22758620689655173,0.03493950380131184
EleutherAI/pythia-6.9b-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.291005291005291,0.023393826500484875
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_biology,0-shot,accuracy,0.3161290322580645,0.026450874489042757
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.27586206896551724,0.03144712581678241
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.3037037037037037,0.028037929969114982
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_physics,0-shot,accuracy,0.2185430463576159,0.03374235550425694
EleutherAI/pythia-6.9b-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.27314814814814814,0.03038805130167812
EleutherAI/pythia-6.9b-deduped,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
EleutherAI/pythia-2.8b,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.03970158273235173
EleutherAI/pythia-2.8b,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
EleutherAI/pythia-2.8b,mmlu_high_school_us_history,0-shot,accuracy,0.23529411764705882,0.029771775228145652
EleutherAI/pythia-2.8b,mmlu_high_school_world_history,0-shot,accuracy,0.2616033755274262,0.028609516716994934
EleutherAI/pythia-2.8b,mmlu_international_law,0-shot,accuracy,0.2066115702479339,0.036959801280988254
EleutherAI/pythia-2.8b,mmlu_jurisprudence,0-shot,accuracy,0.24074074074074073,0.041331194402438376
EleutherAI/pythia-2.8b,mmlu_logical_fallacies,0-shot,accuracy,0.20245398773006135,0.03157065078911901
EleutherAI/pythia-2.8b,mmlu_moral_disputes,0-shot,accuracy,0.24277456647398843,0.023083658586984204
EleutherAI/pythia-2.8b,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.01435591196476786
EleutherAI/pythia-2.8b,mmlu_philosophy,0-shot,accuracy,0.2057877813504823,0.022961339906764234
EleutherAI/pythia-2.8b,mmlu_prehistory,0-shot,accuracy,0.2654320987654321,0.024569223600460845
EleutherAI/pythia-2.8b,mmlu_professional_law,0-shot,accuracy,0.2333767926988266,0.010803108481179081
EleutherAI/pythia-2.8b,mmlu_world_religions,0-shot,accuracy,0.23976608187134502,0.03274485211946956
EleutherAI/pythia-2.8b,mmlu_business_ethics,0-shot,accuracy,0.22,0.041633319989322695
EleutherAI/pythia-2.8b,mmlu_clinical_knowledge,0-shot,accuracy,0.24528301886792453,0.026480357179895702
EleutherAI/pythia-2.8b,mmlu_college_medicine,0-shot,accuracy,0.2774566473988439,0.03414014007044036
EleutherAI/pythia-2.8b,mmlu_global_facts,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-2.8b,mmlu_human_aging,0-shot,accuracy,0.2825112107623318,0.030216831011508773
EleutherAI/pythia-2.8b,mmlu_management,0-shot,accuracy,0.27184466019417475,0.044052680241409216
EleutherAI/pythia-2.8b,mmlu_marketing,0-shot,accuracy,0.28205128205128205,0.02948036054954119
EleutherAI/pythia-2.8b,mmlu_medical_genetics,0-shot,accuracy,0.26,0.04408440022768078
EleutherAI/pythia-2.8b,mmlu_miscellaneous,0-shot,accuracy,0.27458492975734355,0.015959829933084053
EleutherAI/pythia-2.8b,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.023805186524888132
EleutherAI/pythia-2.8b,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.026684564340460976
EleutherAI/pythia-2.8b,mmlu_professional_medicine,0-shot,accuracy,0.40441176470588236,0.029812630701569746
EleutherAI/pythia-2.8b,mmlu_virology,0-shot,accuracy,0.29518072289156627,0.035509201856896294
EleutherAI/pythia-2.8b,mmlu_econometrics,0-shot,accuracy,0.21052631578947367,0.0383515395439942
EleutherAI/pythia-2.8b,mmlu_high_school_geography,0-shot,accuracy,0.19696969696969696,0.028335609732463355
EleutherAI/pythia-2.8b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.24352331606217617,0.03097543638684544
EleutherAI/pythia-2.8b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.21794871794871795,0.020932445774463185
EleutherAI/pythia-2.8b,mmlu_high_school_microeconomics,0-shot,accuracy,0.23109243697478993,0.027381406927868973
EleutherAI/pythia-2.8b,mmlu_high_school_psychology,0-shot,accuracy,0.26422018348623855,0.018904164171510203
EleutherAI/pythia-2.8b,mmlu_human_sexuality,0-shot,accuracy,0.22137404580152673,0.03641297081313729
EleutherAI/pythia-2.8b,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612379
EleutherAI/pythia-2.8b,mmlu_public_relations,0-shot,accuracy,0.37272727272727274,0.04631381319425463
EleutherAI/pythia-2.8b,mmlu_security_studies,0-shot,accuracy,0.19183673469387755,0.0252069631542254
EleutherAI/pythia-2.8b,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409224
EleutherAI/pythia-2.8b,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.04292346959909282
EleutherAI/pythia-2.8b,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-2.8b,mmlu_anatomy,0-shot,accuracy,0.26666666666666666,0.038201699145179055
EleutherAI/pythia-2.8b,mmlu_astronomy,0-shot,accuracy,0.23026315789473684,0.034260594244031654
EleutherAI/pythia-2.8b,mmlu_college_biology,0-shot,accuracy,0.2986111111111111,0.03827052357950756
EleutherAI/pythia-2.8b,mmlu_college_chemistry,0-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-2.8b,mmlu_college_computer_science,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-2.8b,mmlu_college_mathematics,0-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-2.8b,mmlu_college_physics,0-shot,accuracy,0.3431372549019608,0.04724007352383888
EleutherAI/pythia-2.8b,mmlu_computer_security,0-shot,accuracy,0.28,0.04512608598542127
EleutherAI/pythia-2.8b,mmlu_conceptual_physics,0-shot,accuracy,0.3021276595744681,0.030017554471880554
EleutherAI/pythia-2.8b,mmlu_electrical_engineering,0-shot,accuracy,0.20689655172413793,0.03375672449560553
EleutherAI/pythia-2.8b,mmlu_elementary_mathematics,0-shot,accuracy,0.25925925925925924,0.02256989707491841
EleutherAI/pythia-2.8b,mmlu_high_school_biology,0-shot,accuracy,0.267741935483871,0.025189006660212385
EleutherAI/pythia-2.8b,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.031089826002937523
EleutherAI/pythia-2.8b,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.042295258468165065
EleutherAI/pythia-2.8b,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.026466117538959916
EleutherAI/pythia-2.8b,mmlu_high_school_physics,0-shot,accuracy,0.26490066225165565,0.03603038545360384
EleutherAI/pythia-2.8b,mmlu_high_school_statistics,0-shot,accuracy,0.21296296296296297,0.027920963147993666
EleutherAI/pythia-2.8b,mmlu_machine_learning,0-shot,accuracy,0.2767857142857143,0.042466243366976256
EleutherAI/pythia-2.8b-deduped,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.03718489006818114
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.20606060606060606,0.031584153240477086
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.22058823529411764,0.029102254389674082
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pythia-2.8b-deduped,mmlu_international_law,0-shot,accuracy,0.2231404958677686,0.03800754475228732
EleutherAI/pythia-2.8b-deduped,mmlu_jurisprudence,0-shot,accuracy,0.28703703703703703,0.043733130409147614
EleutherAI/pythia-2.8b-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.20245398773006135,0.031570650789119026
EleutherAI/pythia-2.8b-deduped,mmlu_moral_disputes,0-shot,accuracy,0.2658959537572254,0.023786203255508283
EleutherAI/pythia-2.8b-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
EleutherAI/pythia-2.8b-deduped,mmlu_philosophy,0-shot,accuracy,0.19935691318327975,0.022691033780549656
EleutherAI/pythia-2.8b-deduped,mmlu_prehistory,0-shot,accuracy,0.25925925925925924,0.02438366553103545
EleutherAI/pythia-2.8b-deduped,mmlu_professional_law,0-shot,accuracy,0.2333767926988266,0.010803108481179078
EleutherAI/pythia-2.8b-deduped,mmlu_world_religions,0-shot,accuracy,0.23976608187134502,0.03274485211946956
EleutherAI/pythia-2.8b-deduped,mmlu_business_ethics,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-2.8b-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.026880647889052
EleutherAI/pythia-2.8b-deduped,mmlu_college_medicine,0-shot,accuracy,0.21965317919075145,0.031568093627031744
EleutherAI/pythia-2.8b-deduped,mmlu_global_facts,0-shot,accuracy,0.34,0.04760952285695235
EleutherAI/pythia-2.8b-deduped,mmlu_human_aging,0-shot,accuracy,0.2825112107623318,0.030216831011508762
EleutherAI/pythia-2.8b-deduped,mmlu_management,0-shot,accuracy,0.2815533980582524,0.04453254836326469
EleutherAI/pythia-2.8b-deduped,mmlu_marketing,0-shot,accuracy,0.2692307692307692,0.02905858830374884
EleutherAI/pythia-2.8b-deduped,mmlu_medical_genetics,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-2.8b-deduped,mmlu_miscellaneous,0-shot,accuracy,0.2771392081736909,0.016005636294122425
EleutherAI/pythia-2.8b-deduped,mmlu_nutrition,0-shot,accuracy,0.22549019607843138,0.023929155517351298
EleutherAI/pythia-2.8b-deduped,mmlu_professional_accounting,0-shot,accuracy,0.24822695035460993,0.025770015644290416
EleutherAI/pythia-2.8b-deduped,mmlu_professional_medicine,0-shot,accuracy,0.3897058823529412,0.02962466358115969
EleutherAI/pythia-2.8b-deduped,mmlu_virology,0-shot,accuracy,0.24096385542168675,0.0332939411907353
EleutherAI/pythia-2.8b-deduped,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.0404933929774814
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_geography,0-shot,accuracy,0.21212121212121213,0.029126522834586808
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.23834196891191708,0.030748905363909902
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20512820512820512,0.02047323317355198
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.23109243697478993,0.02738140692786898
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.26788990825688075,0.018987462257978652
EleutherAI/pythia-2.8b-deduped,mmlu_human_sexuality,0-shot,accuracy,0.22900763358778625,0.036853466317118506
EleutherAI/pythia-2.8b-deduped,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594722
EleutherAI/pythia-2.8b-deduped,mmlu_public_relations,0-shot,accuracy,0.32727272727272727,0.04494290866252089
EleutherAI/pythia-2.8b-deduped,mmlu_security_studies,0-shot,accuracy,0.19591836734693877,0.025409301953225678
EleutherAI/pythia-2.8b-deduped,mmlu_sociology,0-shot,accuracy,0.21890547263681592,0.029239174636647
EleutherAI/pythia-2.8b-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-2.8b-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-2.8b-deduped,mmlu_anatomy,0-shot,accuracy,0.26666666666666666,0.038201699145179055
EleutherAI/pythia-2.8b-deduped,mmlu_astronomy,0-shot,accuracy,0.27631578947368424,0.03639057569952925
EleutherAI/pythia-2.8b-deduped,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.03476590104304134
EleutherAI/pythia-2.8b-deduped,mmlu_college_chemistry,0-shot,accuracy,0.11,0.03144660377352203
EleutherAI/pythia-2.8b-deduped,mmlu_college_computer_science,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-2.8b-deduped,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-2.8b-deduped,mmlu_college_physics,0-shot,accuracy,0.19607843137254902,0.03950581861179962
EleutherAI/pythia-2.8b-deduped,mmlu_computer_security,0-shot,accuracy,0.26,0.044084400227680794
EleutherAI/pythia-2.8b-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2978723404255319,0.02989614568209546
EleutherAI/pythia-2.8b-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.20689655172413793,0.03375672449560554
EleutherAI/pythia-2.8b-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.2698412698412698,0.02286083830923207
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_biology,0-shot,accuracy,0.24193548387096775,0.024362599693031096
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.030516530732694436
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.21851851851851853,0.025195752251823796
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
EleutherAI/pythia-2.8b-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.2037037037037037,0.027467401804057996
EleutherAI/pythia-2.8b-deduped,mmlu_machine_learning,0-shot,accuracy,0.23214285714285715,0.04007341809755805
Salesforce/codegen-6B-nl,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.03932537680392871
Salesforce/codegen-6B-nl,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
Salesforce/codegen-6B-nl,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
Salesforce/codegen-6B-nl,mmlu_high_school_world_history,0-shot,accuracy,0.3037974683544304,0.029936696387138615
Salesforce/codegen-6B-nl,mmlu_international_law,0-shot,accuracy,0.3305785123966942,0.04294340845212094
Salesforce/codegen-6B-nl,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
Salesforce/codegen-6B-nl,mmlu_logical_fallacies,0-shot,accuracy,0.25766871165644173,0.03436150827846917
Salesforce/codegen-6B-nl,mmlu_moral_disputes,0-shot,accuracy,0.26011560693641617,0.023618678310069363
Salesforce/codegen-6B-nl,mmlu_moral_scenarios,0-shot,accuracy,0.22569832402234638,0.01398139505845505
Salesforce/codegen-6B-nl,mmlu_philosophy,0-shot,accuracy,0.21864951768488747,0.0234755814178611
Salesforce/codegen-6B-nl,mmlu_prehistory,0-shot,accuracy,0.23148148148148148,0.023468429832451166
Salesforce/codegen-6B-nl,mmlu_professional_law,0-shot,accuracy,0.2607561929595828,0.011213471559602338
Salesforce/codegen-6B-nl,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.035650796707083106
Salesforce/codegen-6B-nl,mmlu_business_ethics,0-shot,accuracy,0.34,0.04760952285695236
Salesforce/codegen-6B-nl,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891373
Salesforce/codegen-6B-nl,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
Salesforce/codegen-6B-nl,mmlu_global_facts,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-6B-nl,mmlu_human_aging,0-shot,accuracy,0.36771300448430494,0.03236198350928276
Salesforce/codegen-6B-nl,mmlu_management,0-shot,accuracy,0.22330097087378642,0.04123553189891431
Salesforce/codegen-6B-nl,mmlu_marketing,0-shot,accuracy,0.32051282051282054,0.030572811310299614
Salesforce/codegen-6B-nl,mmlu_medical_genetics,0-shot,accuracy,0.26,0.04408440022768079
Salesforce/codegen-6B-nl,mmlu_miscellaneous,0-shot,accuracy,0.3128991060025543,0.016580935940304062
Salesforce/codegen-6B-nl,mmlu_nutrition,0-shot,accuracy,0.2875816993464052,0.02591780611714716
Salesforce/codegen-6B-nl,mmlu_professional_accounting,0-shot,accuracy,0.2872340425531915,0.026992199173064356
Salesforce/codegen-6B-nl,mmlu_professional_medicine,0-shot,accuracy,0.1875,0.023709788253811766
Salesforce/codegen-6B-nl,mmlu_virology,0-shot,accuracy,0.30120481927710846,0.035716092300534796
Salesforce/codegen-6B-nl,mmlu_econometrics,0-shot,accuracy,0.21052631578947367,0.03835153954399421
Salesforce/codegen-6B-nl,mmlu_high_school_geography,0-shot,accuracy,0.18686868686868688,0.027772533334218977
Salesforce/codegen-6B-nl,mmlu_high_school_government_and_politics,0-shot,accuracy,0.22797927461139897,0.030276909945178253
Salesforce/codegen-6B-nl,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2358974358974359,0.021525965407408726
Salesforce/codegen-6B-nl,mmlu_high_school_microeconomics,0-shot,accuracy,0.23949579831932774,0.02772206549336126
Salesforce/codegen-6B-nl,mmlu_high_school_psychology,0-shot,accuracy,0.21284403669724772,0.017549376389313694
Salesforce/codegen-6B-nl,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
Salesforce/codegen-6B-nl,mmlu_professional_psychology,0-shot,accuracy,0.27124183006535946,0.017986615304030305
Salesforce/codegen-6B-nl,mmlu_public_relations,0-shot,accuracy,0.22727272727272727,0.04013964554072773
Salesforce/codegen-6B-nl,mmlu_security_studies,0-shot,accuracy,0.19183673469387755,0.025206963154225395
Salesforce/codegen-6B-nl,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296018
Salesforce/codegen-6B-nl,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.044084400227680794
Salesforce/codegen-6B-nl,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909283
Salesforce/codegen-6B-nl,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.03749850709174021
Salesforce/codegen-6B-nl,mmlu_astronomy,0-shot,accuracy,0.26973684210526316,0.03611780560284898
Salesforce/codegen-6B-nl,mmlu_college_biology,0-shot,accuracy,0.2916666666666667,0.03800968060554858
Salesforce/codegen-6B-nl,mmlu_college_chemistry,0-shot,accuracy,0.15,0.03588702812826369
Salesforce/codegen-6B-nl,mmlu_college_computer_science,0-shot,accuracy,0.28,0.04512608598542127
Salesforce/codegen-6B-nl,mmlu_college_mathematics,0-shot,accuracy,0.22,0.04163331998932269
Salesforce/codegen-6B-nl,mmlu_college_physics,0-shot,accuracy,0.1568627450980392,0.03618664819936246
Salesforce/codegen-6B-nl,mmlu_computer_security,0-shot,accuracy,0.33,0.04725815626252605
Salesforce/codegen-6B-nl,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.028809989854102973
Salesforce/codegen-6B-nl,mmlu_electrical_engineering,0-shot,accuracy,0.2620689655172414,0.036646663372252565
Salesforce/codegen-6B-nl,mmlu_elementary_mathematics,0-shot,accuracy,0.2275132275132275,0.021591269407823778
Salesforce/codegen-6B-nl,mmlu_high_school_biology,0-shot,accuracy,0.2129032258064516,0.023287665127268545
Salesforce/codegen-6B-nl,mmlu_high_school_chemistry,0-shot,accuracy,0.18226600985221675,0.027163340859645148
Salesforce/codegen-6B-nl,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-6B-nl,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.02684205787383371
Salesforce/codegen-6B-nl,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.03297986648473834
Salesforce/codegen-6B-nl,mmlu_high_school_statistics,0-shot,accuracy,0.17592592592592593,0.025967420958258533
Salesforce/codegen-6B-nl,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.043270409325787296
Dampish/StellarX-4B-V0,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
Dampish/StellarX-4B-V0,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
Dampish/StellarX-4B-V0,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
Dampish/StellarX-4B-V0,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
Dampish/StellarX-4B-V0,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
Dampish/StellarX-4B-V0,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
Dampish/StellarX-4B-V0,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
Dampish/StellarX-4B-V0,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
Dampish/StellarX-4B-V0,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
Dampish/StellarX-4B-V0,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
Dampish/StellarX-4B-V0,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
Dampish/StellarX-4B-V0,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
Dampish/StellarX-4B-V0,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
Dampish/StellarX-4B-V0,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
Dampish/StellarX-4B-V0,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891377
Dampish/StellarX-4B-V0,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
Dampish/StellarX-4B-V0,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
Dampish/StellarX-4B-V0,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
Dampish/StellarX-4B-V0,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
Dampish/StellarX-4B-V0,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
Dampish/StellarX-4B-V0,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
Dampish/StellarX-4B-V0,mmlu_miscellaneous,0-shot,accuracy,0.23754789272030652,0.015218733046150195
Dampish/StellarX-4B-V0,mmlu_nutrition,0-shot,accuracy,0.22549019607843138,0.023929155517351284
Dampish/StellarX-4B-V0,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
Dampish/StellarX-4B-V0,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
Dampish/StellarX-4B-V0,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
Dampish/StellarX-4B-V0,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
Dampish/StellarX-4B-V0,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
Dampish/StellarX-4B-V0,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
Dampish/StellarX-4B-V0,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
Dampish/StellarX-4B-V0,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
Dampish/StellarX-4B-V0,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936073
Dampish/StellarX-4B-V0,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
Dampish/StellarX-4B-V0,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
Dampish/StellarX-4B-V0,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
Dampish/StellarX-4B-V0,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
Dampish/StellarX-4B-V0,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
Dampish/StellarX-4B-V0,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
Dampish/StellarX-4B-V0,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
Dampish/StellarX-4B-V0,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
Dampish/StellarX-4B-V0,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
Dampish/StellarX-4B-V0,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
Dampish/StellarX-4B-V0,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
Dampish/StellarX-4B-V0,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
Dampish/StellarX-4B-V0,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
Dampish/StellarX-4B-V0,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
Dampish/StellarX-4B-V0,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
Dampish/StellarX-4B-V0,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
Dampish/StellarX-4B-V0,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
Dampish/StellarX-4B-V0,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
Dampish/StellarX-4B-V0,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.021732540689329265
Dampish/StellarX-4B-V0,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.025308904539380624
Dampish/StellarX-4B-V0,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
Dampish/StellarX-4B-V0,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
Dampish/StellarX-4B-V0,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
Dampish/StellarX-4B-V0,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
Dampish/StellarX-4B-V0,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
LLM360/CrystalCoder,mmlu_formal_logic,0-shot,accuracy,0.29365079365079366,0.04073524322147127
LLM360/CrystalCoder,mmlu_high_school_european_history,0-shot,accuracy,0.2,0.031234752377721175
LLM360/CrystalCoder,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.03058759135160424
LLM360/CrystalCoder,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293423
LLM360/CrystalCoder,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.03941897526516303
LLM360/CrystalCoder,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
LLM360/CrystalCoder,mmlu_logical_fallacies,0-shot,accuracy,0.2147239263803681,0.03226219377286774
LLM360/CrystalCoder,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
LLM360/CrystalCoder,mmlu_moral_scenarios,0-shot,accuracy,0.23687150837988827,0.014219570788103982
LLM360/CrystalCoder,mmlu_philosophy,0-shot,accuracy,0.19292604501607716,0.022411516780911363
LLM360/CrystalCoder,mmlu_prehistory,0-shot,accuracy,0.21296296296296297,0.022779719088733396
LLM360/CrystalCoder,mmlu_professional_law,0-shot,accuracy,0.2470664928292047,0.01101575225527933
LLM360/CrystalCoder,mmlu_world_religions,0-shot,accuracy,0.32748538011695905,0.03599335771456027
LLM360/CrystalCoder,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
LLM360/CrystalCoder,mmlu_clinical_knowledge,0-shot,accuracy,0.20754716981132076,0.02495991802891127
LLM360/CrystalCoder,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
LLM360/CrystalCoder,mmlu_global_facts,0-shot,accuracy,0.17,0.0377525168068637
LLM360/CrystalCoder,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.031024411740572213
LLM360/CrystalCoder,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
LLM360/CrystalCoder,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
LLM360/CrystalCoder,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
LLM360/CrystalCoder,mmlu_miscellaneous,0-shot,accuracy,0.23627075351213284,0.01519047371703751
LLM360/CrystalCoder,mmlu_nutrition,0-shot,accuracy,0.21241830065359477,0.02342037547829613
LLM360/CrystalCoder,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539005,0.0258921511567094
LLM360/CrystalCoder,mmlu_professional_medicine,0-shot,accuracy,0.20588235294117646,0.024562204314142314
LLM360/CrystalCoder,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
LLM360/CrystalCoder,mmlu_econometrics,0-shot,accuracy,0.21052631578947367,0.0383515395439942
LLM360/CrystalCoder,mmlu_high_school_geography,0-shot,accuracy,0.1919191919191919,0.028057791672989017
LLM360/CrystalCoder,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20207253886010362,0.028979089794296725
LLM360/CrystalCoder,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2,0.020280805062535726
LLM360/CrystalCoder,mmlu_high_school_microeconomics,0-shot,accuracy,0.20588235294117646,0.026265024608275886
LLM360/CrystalCoder,mmlu_high_school_psychology,0-shot,accuracy,0.2018348623853211,0.01720857935778758
LLM360/CrystalCoder,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.03915345408847834
LLM360/CrystalCoder,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612378967
LLM360/CrystalCoder,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
LLM360/CrystalCoder,mmlu_security_studies,0-shot,accuracy,0.21224489795918366,0.026176967197866767
LLM360/CrystalCoder,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
LLM360/CrystalCoder,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
LLM360/CrystalCoder,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
LLM360/CrystalCoder,mmlu_anatomy,0-shot,accuracy,0.17777777777777778,0.03302789859901718
LLM360/CrystalCoder,mmlu_astronomy,0-shot,accuracy,0.17105263157894737,0.03064360707167709
LLM360/CrystalCoder,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
LLM360/CrystalCoder,mmlu_college_chemistry,0-shot,accuracy,0.2,0.04020151261036846
LLM360/CrystalCoder,mmlu_college_computer_science,0-shot,accuracy,0.27,0.044619604333847394
LLM360/CrystalCoder,mmlu_college_mathematics,0-shot,accuracy,0.23,0.042295258468165065
LLM360/CrystalCoder,mmlu_college_physics,0-shot,accuracy,0.20588235294117646,0.04023382273617749
LLM360/CrystalCoder,mmlu_computer_security,0-shot,accuracy,0.27,0.044619604333847394
LLM360/CrystalCoder,mmlu_conceptual_physics,0-shot,accuracy,0.2723404255319149,0.029101290698386694
LLM360/CrystalCoder,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
LLM360/CrystalCoder,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.02094048156533484
LLM360/CrystalCoder,mmlu_high_school_biology,0-shot,accuracy,0.2032258064516129,0.022891687984554963
LLM360/CrystalCoder,mmlu_high_school_chemistry,0-shot,accuracy,0.1724137931034483,0.026577672183036572
LLM360/CrystalCoder,mmlu_high_school_computer_science,0-shot,accuracy,0.27,0.044619604333847394
LLM360/CrystalCoder,mmlu_high_school_mathematics,0-shot,accuracy,0.22962962962962963,0.02564410863926763
LLM360/CrystalCoder,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
LLM360/CrystalCoder,mmlu_high_school_statistics,0-shot,accuracy,0.19907407407407407,0.027232298462690218
LLM360/CrystalCoder,mmlu_machine_learning,0-shot,accuracy,0.30357142857142855,0.04364226155841044
tiiuae/falcon-7b,mmlu_formal_logic,0-shot,accuracy,0.1984126984126984,0.03567016675276865
tiiuae/falcon-7b,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.033464098810559534
tiiuae/falcon-7b,mmlu_high_school_us_history,0-shot,accuracy,0.23529411764705882,0.02977177522814563
tiiuae/falcon-7b,mmlu_high_school_world_history,0-shot,accuracy,0.2616033755274262,0.028609516716994934
tiiuae/falcon-7b,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.03941897526516302
tiiuae/falcon-7b,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.04414343666854933
tiiuae/falcon-7b,mmlu_logical_fallacies,0-shot,accuracy,0.24539877300613497,0.03380939813943354
tiiuae/falcon-7b,mmlu_moral_disputes,0-shot,accuracy,0.24566473988439305,0.023176298203992005
tiiuae/falcon-7b,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
tiiuae/falcon-7b,mmlu_philosophy,0-shot,accuracy,0.2733118971061093,0.02531176597542612
tiiuae/falcon-7b,mmlu_prehistory,0-shot,accuracy,0.2654320987654321,0.024569223600460845
tiiuae/falcon-7b,mmlu_professional_law,0-shot,accuracy,0.2392438070404172,0.010896123652676669
tiiuae/falcon-7b,mmlu_world_religions,0-shot,accuracy,0.21052631578947367,0.031267817146631786
tiiuae/falcon-7b,mmlu_business_ethics,0-shot,accuracy,0.26,0.0440844002276808
tiiuae/falcon-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.2679245283018868,0.027257260322494845
tiiuae/falcon-7b,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749867
tiiuae/falcon-7b,mmlu_global_facts,0-shot,accuracy,0.31,0.04648231987117316
tiiuae/falcon-7b,mmlu_human_aging,0-shot,accuracy,0.37668161434977576,0.032521134899291884
tiiuae/falcon-7b,mmlu_management,0-shot,accuracy,0.2524271844660194,0.04301250399690878
tiiuae/falcon-7b,mmlu_marketing,0-shot,accuracy,0.2564102564102564,0.028605953702004264
tiiuae/falcon-7b,mmlu_medical_genetics,0-shot,accuracy,0.26,0.04408440022768079
tiiuae/falcon-7b,mmlu_miscellaneous,0-shot,accuracy,0.28735632183908044,0.0161824107306827
tiiuae/falcon-7b,mmlu_nutrition,0-shot,accuracy,0.22875816993464052,0.024051029739912255
tiiuae/falcon-7b,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902
tiiuae/falcon-7b,mmlu_professional_medicine,0-shot,accuracy,0.20220588235294118,0.024398192986654924
tiiuae/falcon-7b,mmlu_virology,0-shot,accuracy,0.3192771084337349,0.03629335329947859
tiiuae/falcon-7b,mmlu_econometrics,0-shot,accuracy,0.2807017543859649,0.04227054451232199
tiiuae/falcon-7b,mmlu_high_school_geography,0-shot,accuracy,0.21717171717171718,0.029376616484945637
tiiuae/falcon-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20725388601036268,0.029252823291803627
tiiuae/falcon-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2205128205128205,0.021020672680827912
tiiuae/falcon-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.23109243697478993,0.027381406927868973
tiiuae/falcon-7b,mmlu_high_school_psychology,0-shot,accuracy,0.23669724770642203,0.01822407811729907
tiiuae/falcon-7b,mmlu_human_sexuality,0-shot,accuracy,0.22900763358778625,0.036853466317118506
tiiuae/falcon-7b,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612379
tiiuae/falcon-7b,mmlu_public_relations,0-shot,accuracy,0.34545454545454546,0.04554619617541054
tiiuae/falcon-7b,mmlu_security_studies,0-shot,accuracy,0.17142857142857143,0.02412746346265016
tiiuae/falcon-7b,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409217
tiiuae/falcon-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.21,0.04093601807403326
tiiuae/falcon-7b,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768079
tiiuae/falcon-7b,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.03749850709174021
tiiuae/falcon-7b,mmlu_astronomy,0-shot,accuracy,0.18421052631578946,0.0315469804508223
tiiuae/falcon-7b,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.03476590104304134
tiiuae/falcon-7b,mmlu_college_chemistry,0-shot,accuracy,0.21,0.040936018074033256
tiiuae/falcon-7b,mmlu_college_computer_science,0-shot,accuracy,0.15,0.0358870281282637
tiiuae/falcon-7b,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816506
tiiuae/falcon-7b,mmlu_college_physics,0-shot,accuracy,0.19607843137254902,0.03950581861179964
tiiuae/falcon-7b,mmlu_computer_security,0-shot,accuracy,0.24,0.04292346959909282
tiiuae/falcon-7b,mmlu_conceptual_physics,0-shot,accuracy,0.32340425531914896,0.03057944277361035
tiiuae/falcon-7b,mmlu_electrical_engineering,0-shot,accuracy,0.2206896551724138,0.03455930201924811
tiiuae/falcon-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.022494510767503154
tiiuae/falcon-7b,mmlu_high_school_biology,0-shot,accuracy,0.25483870967741934,0.02479011845933221
tiiuae/falcon-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.031270907132976984
tiiuae/falcon-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.042295258468165044
tiiuae/falcon-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.02684205787383371
tiiuae/falcon-7b,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.03257847384436776
tiiuae/falcon-7b,mmlu_high_school_statistics,0-shot,accuracy,0.16203703703703703,0.02513045365226846
tiiuae/falcon-7b,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.04287858751340456
EleutherAI/pythia-2.8b,mmlu_formal_logic,5-shot,accuracy,0.20634920634920634,0.0361960452412425
EleutherAI/pythia-2.8b,mmlu_high_school_european_history,5-shot,accuracy,0.28484848484848485,0.03524390844511784
EleutherAI/pythia-2.8b,mmlu_high_school_us_history,5-shot,accuracy,0.19607843137254902,0.02786594228663931
EleutherAI/pythia-2.8b,mmlu_high_school_world_history,5-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pythia-2.8b,mmlu_international_law,5-shot,accuracy,0.30578512396694213,0.04205953933884124
EleutherAI/pythia-2.8b,mmlu_jurisprudence,5-shot,accuracy,0.2962962962962963,0.044143436668549335
EleutherAI/pythia-2.8b,mmlu_logical_fallacies,5-shot,accuracy,0.2331288343558282,0.033220157957767414
EleutherAI/pythia-2.8b,mmlu_moral_disputes,5-shot,accuracy,0.2658959537572254,0.023786203255508297
EleutherAI/pythia-2.8b,mmlu_moral_scenarios,5-shot,accuracy,0.2346368715083799,0.014173044098303647
EleutherAI/pythia-2.8b,mmlu_philosophy,5-shot,accuracy,0.2765273311897106,0.025403832978179615
EleutherAI/pythia-2.8b,mmlu_prehistory,5-shot,accuracy,0.25308641975308643,0.024191808600712995
EleutherAI/pythia-2.8b,mmlu_professional_law,5-shot,accuracy,0.27183833116036504,0.01136313527865141
EleutherAI/pythia-2.8b,mmlu_world_religions,5-shot,accuracy,0.2982456140350877,0.03508771929824563
EleutherAI/pythia-2.8b,mmlu_business_ethics,5-shot,accuracy,0.22,0.0416333199893227
EleutherAI/pythia-2.8b,mmlu_clinical_knowledge,5-shot,accuracy,0.26037735849056604,0.02700876609070809
EleutherAI/pythia-2.8b,mmlu_college_medicine,5-shot,accuracy,0.23121387283236994,0.032147373020294696
EleutherAI/pythia-2.8b,mmlu_global_facts,5-shot,accuracy,0.35,0.047937248544110196
EleutherAI/pythia-2.8b,mmlu_human_aging,5-shot,accuracy,0.3542600896860987,0.032100621541349864
EleutherAI/pythia-2.8b,mmlu_management,5-shot,accuracy,0.23300970873786409,0.04185832598928315
EleutherAI/pythia-2.8b,mmlu_marketing,5-shot,accuracy,0.2692307692307692,0.02905858830374884
EleutherAI/pythia-2.8b,mmlu_medical_genetics,5-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-2.8b,mmlu_miscellaneous,5-shot,accuracy,0.2835249042145594,0.016117318166832286
EleutherAI/pythia-2.8b,mmlu_nutrition,5-shot,accuracy,0.2875816993464052,0.02591780611714716
EleutherAI/pythia-2.8b,mmlu_professional_accounting,5-shot,accuracy,0.2695035460992908,0.026469036818590634
EleutherAI/pythia-2.8b,mmlu_professional_medicine,5-shot,accuracy,0.4375,0.030134614954403924
EleutherAI/pythia-2.8b,mmlu_virology,5-shot,accuracy,0.2710843373493976,0.03460579907553026
EleutherAI/pythia-2.8b,mmlu_econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
EleutherAI/pythia-2.8b,mmlu_high_school_geography,5-shot,accuracy,0.20707070707070707,0.02886977846026705
EleutherAI/pythia-2.8b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.2538860103626943,0.03141024780565319
EleutherAI/pythia-2.8b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2948717948717949,0.023119362758232283
EleutherAI/pythia-2.8b,mmlu_high_school_microeconomics,5-shot,accuracy,0.24789915966386555,0.028047967224176892
EleutherAI/pythia-2.8b,mmlu_high_school_psychology,5-shot,accuracy,0.23119266055045873,0.018075750241633156
EleutherAI/pythia-2.8b,mmlu_human_sexuality,5-shot,accuracy,0.2366412213740458,0.03727673575596917
EleutherAI/pythia-2.8b,mmlu_professional_psychology,5-shot,accuracy,0.2761437908496732,0.018087276935663137
EleutherAI/pythia-2.8b,mmlu_public_relations,5-shot,accuracy,0.36363636363636365,0.04607582090719976
EleutherAI/pythia-2.8b,mmlu_security_studies,5-shot,accuracy,0.1836734693877551,0.024789071332007636
EleutherAI/pythia-2.8b,mmlu_sociology,5-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/pythia-2.8b,mmlu_us_foreign_policy,5-shot,accuracy,0.21,0.04093601807403326
EleutherAI/pythia-2.8b,mmlu_abstract_algebra,5-shot,accuracy,0.26,0.04408440022768079
EleutherAI/pythia-2.8b,mmlu_anatomy,5-shot,accuracy,0.3111111111111111,0.039992628766177235
EleutherAI/pythia-2.8b,mmlu_astronomy,5-shot,accuracy,0.19736842105263158,0.03238981601699397
EleutherAI/pythia-2.8b,mmlu_college_biology,5-shot,accuracy,0.2708333333333333,0.03716177437566017
EleutherAI/pythia-2.8b,mmlu_college_chemistry,5-shot,accuracy,0.27,0.04461960433384741
EleutherAI/pythia-2.8b,mmlu_college_computer_science,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-2.8b,mmlu_college_mathematics,5-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-2.8b,mmlu_college_physics,5-shot,accuracy,0.16666666666666666,0.03708284662416542
EleutherAI/pythia-2.8b,mmlu_computer_security,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-2.8b,mmlu_conceptual_physics,5-shot,accuracy,0.2851063829787234,0.029513196625539345
EleutherAI/pythia-2.8b,mmlu_electrical_engineering,5-shot,accuracy,0.2827586206896552,0.03752833958003337
EleutherAI/pythia-2.8b,mmlu_elementary_mathematics,5-shot,accuracy,0.24867724867724866,0.022261817692400175
EleutherAI/pythia-2.8b,mmlu_high_school_biology,5-shot,accuracy,0.25806451612903225,0.02489246917246284
EleutherAI/pythia-2.8b,mmlu_high_school_chemistry,5-shot,accuracy,0.2561576354679803,0.030712730070982592
EleutherAI/pythia-2.8b,mmlu_high_school_computer_science,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-2.8b,mmlu_high_school_mathematics,5-shot,accuracy,0.26666666666666666,0.02696242432507384
EleutherAI/pythia-2.8b,mmlu_high_school_physics,5-shot,accuracy,0.3443708609271523,0.038796870240733264
EleutherAI/pythia-2.8b,mmlu_high_school_statistics,5-shot,accuracy,0.4444444444444444,0.03388857118502326
EleutherAI/pythia-2.8b,mmlu_machine_learning,5-shot,accuracy,0.2857142857142857,0.04287858751340455
allenai/OLMo-7B-hf,mmlu_formal_logic,5-shot,accuracy,0.2777777777777778,0.04006168083848876
allenai/OLMo-7B-hf,mmlu_high_school_european_history,5-shot,accuracy,0.21818181818181817,0.03225078108306289
allenai/OLMo-7B-hf,mmlu_high_school_us_history,5-shot,accuracy,0.25,0.03039153369274154
allenai/OLMo-7B-hf,mmlu_high_school_world_history,5-shot,accuracy,0.270042194092827,0.028900721906293426
allenai/OLMo-7B-hf,mmlu_international_law,5-shot,accuracy,0.24793388429752067,0.039418975265163025
allenai/OLMo-7B-hf,mmlu_jurisprudence,5-shot,accuracy,0.25925925925925924,0.042365112580946315
allenai/OLMo-7B-hf,mmlu_logical_fallacies,5-shot,accuracy,0.25766871165644173,0.03436150827846917
allenai/OLMo-7B-hf,mmlu_moral_disputes,5-shot,accuracy,0.2398843930635838,0.02298959254312357
allenai/OLMo-7B-hf,mmlu_moral_scenarios,5-shot,accuracy,0.23687150837988827,0.014219570788103982
allenai/OLMo-7B-hf,mmlu_philosophy,5-shot,accuracy,0.24758842443729903,0.024513879973621967
allenai/OLMo-7B-hf,mmlu_prehistory,5-shot,accuracy,0.21296296296296297,0.022779719088733396
allenai/OLMo-7B-hf,mmlu_professional_law,5-shot,accuracy,0.2457627118644068,0.01099615663514269
allenai/OLMo-7B-hf,mmlu_world_religions,5-shot,accuracy,0.27485380116959063,0.03424042924691584
allenai/OLMo-7B-hf,mmlu_business_ethics,5-shot,accuracy,0.3,0.046056618647183814
allenai/OLMo-7B-hf,mmlu_clinical_knowledge,5-shot,accuracy,0.24528301886792453,0.026480357179895685
allenai/OLMo-7B-hf,mmlu_college_medicine,5-shot,accuracy,0.23699421965317918,0.03242414757483099
allenai/OLMo-7B-hf,mmlu_global_facts,5-shot,accuracy,0.2,0.040201512610368445
allenai/OLMo-7B-hf,mmlu_human_aging,5-shot,accuracy,0.2825112107623318,0.030216831011508773
allenai/OLMo-7B-hf,mmlu_management,5-shot,accuracy,0.24271844660194175,0.04245022486384493
allenai/OLMo-7B-hf,mmlu_marketing,5-shot,accuracy,0.24358974358974358,0.0281209665039144
allenai/OLMo-7B-hf,mmlu_medical_genetics,5-shot,accuracy,0.26,0.0440844002276808
allenai/OLMo-7B-hf,mmlu_miscellaneous,5-shot,accuracy,0.24648786717752236,0.015411308769686929
allenai/OLMo-7B-hf,mmlu_nutrition,5-shot,accuracy,0.22549019607843138,0.023929155517351284
allenai/OLMo-7B-hf,mmlu_professional_accounting,5-shot,accuracy,0.22695035460992907,0.024987106365642976
allenai/OLMo-7B-hf,mmlu_professional_medicine,5-shot,accuracy,0.20220588235294118,0.024398192986654924
allenai/OLMo-7B-hf,mmlu_virology,5-shot,accuracy,0.2469879518072289,0.03357351982064537
allenai/OLMo-7B-hf,mmlu_econometrics,5-shot,accuracy,0.23684210526315788,0.039994238792813386
allenai/OLMo-7B-hf,mmlu_high_school_geography,5-shot,accuracy,0.23232323232323232,0.030088629490217487
allenai/OLMo-7B-hf,mmlu_high_school_government_and_politics,5-shot,accuracy,0.20207253886010362,0.02897908979429673
allenai/OLMo-7B-hf,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2717948717948718,0.022556551010132347
allenai/OLMo-7B-hf,mmlu_high_school_microeconomics,5-shot,accuracy,0.23529411764705882,0.027553614467863807
allenai/OLMo-7B-hf,mmlu_high_school_psychology,5-shot,accuracy,0.20917431192660552,0.017437937173343233
allenai/OLMo-7B-hf,mmlu_human_sexuality,5-shot,accuracy,0.17557251908396945,0.03336820338476074
allenai/OLMo-7B-hf,mmlu_professional_psychology,5-shot,accuracy,0.25,0.01751781884501444
allenai/OLMo-7B-hf,mmlu_public_relations,5-shot,accuracy,0.3,0.04389311454644287
allenai/OLMo-7B-hf,mmlu_security_studies,5-shot,accuracy,0.18775510204081633,0.02500025603954622
allenai/OLMo-7B-hf,mmlu_sociology,5-shot,accuracy,0.263681592039801,0.031157150869355575
allenai/OLMo-7B-hf,mmlu_us_foreign_policy,5-shot,accuracy,0.25,0.04351941398892446
allenai/OLMo-7B-hf,mmlu_abstract_algebra,5-shot,accuracy,0.29,0.04560480215720683
allenai/OLMo-7B-hf,mmlu_anatomy,5-shot,accuracy,0.2518518518518518,0.03749850709174021
allenai/OLMo-7B-hf,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
allenai/OLMo-7B-hf,mmlu_college_biology,5-shot,accuracy,0.20833333333333334,0.03396116205845335
allenai/OLMo-7B-hf,mmlu_college_chemistry,5-shot,accuracy,0.16,0.03684529491774708
allenai/OLMo-7B-hf,mmlu_college_computer_science,5-shot,accuracy,0.26,0.044084400227680794
allenai/OLMo-7B-hf,mmlu_college_mathematics,5-shot,accuracy,0.21,0.040936018074033256
allenai/OLMo-7B-hf,mmlu_college_physics,5-shot,accuracy,0.1568627450980392,0.036186648199362466
allenai/OLMo-7B-hf,mmlu_computer_security,5-shot,accuracy,0.25,0.04351941398892446
allenai/OLMo-7B-hf,mmlu_conceptual_physics,5-shot,accuracy,0.2425531914893617,0.028020226271200217
allenai/OLMo-7B-hf,mmlu_electrical_engineering,5-shot,accuracy,0.2896551724137931,0.03780019230438015
allenai/OLMo-7B-hf,mmlu_elementary_mathematics,5-shot,accuracy,0.23544973544973544,0.02185150982203172
allenai/OLMo-7B-hf,mmlu_high_school_biology,5-shot,accuracy,0.2032258064516129,0.022891687984554963
allenai/OLMo-7B-hf,mmlu_high_school_chemistry,5-shot,accuracy,0.15763546798029557,0.025639014131172404
allenai/OLMo-7B-hf,mmlu_high_school_computer_science,5-shot,accuracy,0.24,0.04292346959909284
allenai/OLMo-7B-hf,mmlu_high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.026842057873833706
allenai/OLMo-7B-hf,mmlu_high_school_physics,5-shot,accuracy,0.17218543046357615,0.030826136961962403
allenai/OLMo-7B-hf,mmlu_high_school_statistics,5-shot,accuracy,0.14814814814814814,0.024227629273728353
allenai/OLMo-7B-hf,mmlu_machine_learning,5-shot,accuracy,0.3125,0.043994650575715215
Qwen/Qwen-7B,mmlu_formal_logic,0-shot,accuracy,0.373015873015873,0.04325506042017087
Qwen/Qwen-7B,mmlu_high_school_european_history,0-shot,accuracy,0.6787878787878788,0.036462049632538136
Qwen/Qwen-7B,mmlu_high_school_us_history,0-shot,accuracy,0.7450980392156863,0.03058759135160425
Qwen/Qwen-7B,mmlu_high_school_world_history,0-shot,accuracy,0.729957805907173,0.028900721906293426
Qwen/Qwen-7B,mmlu_international_law,0-shot,accuracy,0.6942148760330579,0.042059539338841226
Qwen/Qwen-7B,mmlu_jurisprudence,0-shot,accuracy,0.6481481481481481,0.046166311118017146
Qwen/Qwen-7B,mmlu_logical_fallacies,0-shot,accuracy,0.6871165644171779,0.03642914578292404
Qwen/Qwen-7B,mmlu_moral_disputes,0-shot,accuracy,0.6213872832369942,0.026113749361310338
Qwen/Qwen-7B,mmlu_moral_scenarios,0-shot,accuracy,0.25921787709497207,0.014655780837497731
Qwen/Qwen-7B,mmlu_philosophy,0-shot,accuracy,0.6463022508038585,0.02715520810320089
Qwen/Qwen-7B,mmlu_prehistory,0-shot,accuracy,0.6481481481481481,0.026571483480719964
Qwen/Qwen-7B,mmlu_professional_law,0-shot,accuracy,0.43546284224250326,0.01266341210124834
Qwen/Qwen-7B,mmlu_world_religions,0-shot,accuracy,0.7368421052631579,0.03377310252209205
Qwen/Qwen-7B,mmlu_business_ethics,0-shot,accuracy,0.65,0.047937248544110196
Qwen/Qwen-7B,mmlu_clinical_knowledge,0-shot,accuracy,0.6264150943396226,0.029773082713319875
Qwen/Qwen-7B,mmlu_college_medicine,0-shot,accuracy,0.5722543352601156,0.03772446857518027
Qwen/Qwen-7B,mmlu_global_facts,0-shot,accuracy,0.28,0.04512608598542128
Qwen/Qwen-7B,mmlu_human_aging,0-shot,accuracy,0.6143497757847534,0.03266842214289201
Qwen/Qwen-7B,mmlu_management,0-shot,accuracy,0.6990291262135923,0.04541609446503947
Qwen/Qwen-7B,mmlu_marketing,0-shot,accuracy,0.8162393162393162,0.02537213967172293
Qwen/Qwen-7B,mmlu_medical_genetics,0-shot,accuracy,0.61,0.04902071300001974
Qwen/Qwen-7B,mmlu_miscellaneous,0-shot,accuracy,0.7547892720306514,0.015384352284543946
Qwen/Qwen-7B,mmlu_nutrition,0-shot,accuracy,0.6503267973856209,0.027305308076274695
Qwen/Qwen-7B,mmlu_professional_accounting,0-shot,accuracy,0.4148936170212766,0.0293922365846125
Qwen/Qwen-7B,mmlu_professional_medicine,0-shot,accuracy,0.5625,0.030134614954403924
Qwen/Qwen-7B,mmlu_virology,0-shot,accuracy,0.42771084337349397,0.038515976837185335
Qwen/Qwen-7B,mmlu_econometrics,0-shot,accuracy,0.3508771929824561,0.044895393502707
Qwen/Qwen-7B,mmlu_high_school_geography,0-shot,accuracy,0.7222222222222222,0.03191178226713546
Qwen/Qwen-7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8290155440414507,0.027171213683164542
Qwen/Qwen-7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.5615384615384615,0.02515826601686859
Qwen/Qwen-7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.592436974789916,0.031918633744784645
Qwen/Qwen-7B,mmlu_high_school_psychology,0-shot,accuracy,0.7724770642201835,0.017974463578776502
Qwen/Qwen-7B,mmlu_human_sexuality,0-shot,accuracy,0.7251908396946565,0.039153454088478354
Qwen/Qwen-7B,mmlu_professional_psychology,0-shot,accuracy,0.5898692810457516,0.019898412717635906
Qwen/Qwen-7B,mmlu_public_relations,0-shot,accuracy,0.6272727272727273,0.04631381319425465
Qwen/Qwen-7B,mmlu_security_studies,0-shot,accuracy,0.6612244897959184,0.030299506562154185
Qwen/Qwen-7B,mmlu_sociology,0-shot,accuracy,0.7810945273631841,0.029239174636647
Qwen/Qwen-7B,mmlu_us_foreign_policy,0-shot,accuracy,0.74,0.0440844002276808
Qwen/Qwen-7B,mmlu_abstract_algebra,0-shot,accuracy,0.32,0.046882617226215034
Qwen/Qwen-7B,mmlu_anatomy,0-shot,accuracy,0.4888888888888889,0.04318275491977976
Qwen/Qwen-7B,mmlu_astronomy,0-shot,accuracy,0.5526315789473685,0.0404633688397825
Qwen/Qwen-7B,mmlu_college_biology,0-shot,accuracy,0.5902777777777778,0.04112490974670787
Qwen/Qwen-7B,mmlu_college_chemistry,0-shot,accuracy,0.44,0.04988876515698589
Qwen/Qwen-7B,mmlu_college_computer_science,0-shot,accuracy,0.43,0.049756985195624284
Qwen/Qwen-7B,mmlu_college_mathematics,0-shot,accuracy,0.35,0.047937248544110196
Qwen/Qwen-7B,mmlu_college_physics,0-shot,accuracy,0.39215686274509803,0.04858083574266345
Qwen/Qwen-7B,mmlu_computer_security,0-shot,accuracy,0.66,0.04760952285695238
Qwen/Qwen-7B,mmlu_conceptual_physics,0-shot,accuracy,0.4978723404255319,0.03268572658667492
Qwen/Qwen-7B,mmlu_electrical_engineering,0-shot,accuracy,0.5241379310344828,0.0416180850350153
Qwen/Qwen-7B,mmlu_elementary_mathematics,0-shot,accuracy,0.4126984126984127,0.025355741263055277
Qwen/Qwen-7B,mmlu_high_school_biology,0-shot,accuracy,0.7,0.026069362295335137
Qwen/Qwen-7B,mmlu_high_school_chemistry,0-shot,accuracy,0.47783251231527096,0.03514528562175008
Qwen/Qwen-7B,mmlu_high_school_computer_science,0-shot,accuracy,0.58,0.049604496374885836
Qwen/Qwen-7B,mmlu_high_school_mathematics,0-shot,accuracy,0.3296296296296296,0.028661201116524575
Qwen/Qwen-7B,mmlu_high_school_physics,0-shot,accuracy,0.3841059602649007,0.03971301814719198
Qwen/Qwen-7B,mmlu_high_school_statistics,0-shot,accuracy,0.4444444444444444,0.03388857118502325
Qwen/Qwen-7B,mmlu_machine_learning,0-shot,accuracy,0.42857142857142855,0.04697113923010212
Salesforce/codegen-2B-nl,mmlu_formal_logic,5-shot,accuracy,0.25396825396825395,0.03893259610604672
Salesforce/codegen-2B-nl,mmlu_high_school_european_history,5-shot,accuracy,0.30303030303030304,0.03588624800091708
Salesforce/codegen-2B-nl,mmlu_high_school_us_history,5-shot,accuracy,0.2647058823529412,0.030964517926923413
Salesforce/codegen-2B-nl,mmlu_high_school_world_history,5-shot,accuracy,0.24050632911392406,0.02782078198114968
Salesforce/codegen-2B-nl,mmlu_international_law,5-shot,accuracy,0.24793388429752067,0.03941897526516302
Salesforce/codegen-2B-nl,mmlu_jurisprudence,5-shot,accuracy,0.2037037037037037,0.038935425188248475
Salesforce/codegen-2B-nl,mmlu_logical_fallacies,5-shot,accuracy,0.3067484662576687,0.03623089915724146
Salesforce/codegen-2B-nl,mmlu_moral_disputes,5-shot,accuracy,0.24277456647398843,0.023083658586984204
Salesforce/codegen-2B-nl,mmlu_moral_scenarios,5-shot,accuracy,0.24581005586592178,0.01440029642922562
Salesforce/codegen-2B-nl,mmlu_philosophy,5-shot,accuracy,0.2733118971061093,0.02531176597542612
Salesforce/codegen-2B-nl,mmlu_prehistory,5-shot,accuracy,0.20987654320987653,0.02265834408598136
Salesforce/codegen-2B-nl,mmlu_professional_law,5-shot,accuracy,0.258148631029987,0.01117692371931339
Salesforce/codegen-2B-nl,mmlu_world_religions,5-shot,accuracy,0.2982456140350877,0.03508771929824562
Salesforce/codegen-2B-nl,mmlu_business_ethics,5-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-2B-nl,mmlu_clinical_knowledge,5-shot,accuracy,0.2528301886792453,0.026749899771241238
Salesforce/codegen-2B-nl,mmlu_college_medicine,5-shot,accuracy,0.34104046242774566,0.03614665424180826
Salesforce/codegen-2B-nl,mmlu_global_facts,5-shot,accuracy,0.28,0.04512608598542128
Salesforce/codegen-2B-nl,mmlu_human_aging,5-shot,accuracy,0.3004484304932735,0.030769352008229136
Salesforce/codegen-2B-nl,mmlu_management,5-shot,accuracy,0.27184466019417475,0.044052680241409216
Salesforce/codegen-2B-nl,mmlu_marketing,5-shot,accuracy,0.2905982905982906,0.029745048572674057
Salesforce/codegen-2B-nl,mmlu_medical_genetics,5-shot,accuracy,0.26,0.04408440022768078
Salesforce/codegen-2B-nl,mmlu_miscellaneous,5-shot,accuracy,0.2835249042145594,0.016117318166832286
Salesforce/codegen-2B-nl,mmlu_nutrition,5-shot,accuracy,0.23529411764705882,0.02428861946604612
Salesforce/codegen-2B-nl,mmlu_professional_accounting,5-shot,accuracy,0.25886524822695034,0.026129572527180848
Salesforce/codegen-2B-nl,mmlu_professional_medicine,5-shot,accuracy,0.2867647058823529,0.027472274473233818
Salesforce/codegen-2B-nl,mmlu_virology,5-shot,accuracy,0.3132530120481928,0.036108050180310235
Salesforce/codegen-2B-nl,mmlu_econometrics,5-shot,accuracy,0.23684210526315788,0.039994238792813344
Salesforce/codegen-2B-nl,mmlu_high_school_geography,5-shot,accuracy,0.2727272727272727,0.03173071239071724
Salesforce/codegen-2B-nl,mmlu_high_school_government_and_politics,5-shot,accuracy,0.2538860103626943,0.03141024780565318
Salesforce/codegen-2B-nl,mmlu_high_school_macroeconomics,5-shot,accuracy,0.32564102564102565,0.02375966576741229
Salesforce/codegen-2B-nl,mmlu_high_school_microeconomics,5-shot,accuracy,0.24789915966386555,0.028047967224176896
Salesforce/codegen-2B-nl,mmlu_high_school_psychology,5-shot,accuracy,0.3339449541284404,0.020220554196736403
Salesforce/codegen-2B-nl,mmlu_human_sexuality,5-shot,accuracy,0.22900763358778625,0.036853466317118506
Salesforce/codegen-2B-nl,mmlu_professional_psychology,5-shot,accuracy,0.28431372549019607,0.01824902441120767
Salesforce/codegen-2B-nl,mmlu_public_relations,5-shot,accuracy,0.2636363636363636,0.04220224692971987
Salesforce/codegen-2B-nl,mmlu_security_studies,5-shot,accuracy,0.363265306122449,0.030789051139030802
Salesforce/codegen-2B-nl,mmlu_sociology,5-shot,accuracy,0.208955223880597,0.028748298931728658
Salesforce/codegen-2B-nl,mmlu_us_foreign_policy,5-shot,accuracy,0.17,0.03775251680686371
Salesforce/codegen-2B-nl,mmlu_abstract_algebra,5-shot,accuracy,0.26,0.04408440022768079
Salesforce/codegen-2B-nl,mmlu_anatomy,5-shot,accuracy,0.2518518518518518,0.0374985070917402
Salesforce/codegen-2B-nl,mmlu_astronomy,5-shot,accuracy,0.24342105263157895,0.034923496688842384
Salesforce/codegen-2B-nl,mmlu_college_biology,5-shot,accuracy,0.2777777777777778,0.03745554791462457
Salesforce/codegen-2B-nl,mmlu_college_chemistry,5-shot,accuracy,0.27,0.044619604333847394
Salesforce/codegen-2B-nl,mmlu_college_computer_science,5-shot,accuracy,0.34,0.04760952285695236
Salesforce/codegen-2B-nl,mmlu_college_mathematics,5-shot,accuracy,0.34,0.04760952285695235
Salesforce/codegen-2B-nl,mmlu_college_physics,5-shot,accuracy,0.2549019607843137,0.04336432707993177
Salesforce/codegen-2B-nl,mmlu_computer_security,5-shot,accuracy,0.28,0.045126085985421255
Salesforce/codegen-2B-nl,mmlu_conceptual_physics,5-shot,accuracy,0.33191489361702126,0.030783736757745667
Salesforce/codegen-2B-nl,mmlu_electrical_engineering,5-shot,accuracy,0.2689655172413793,0.036951833116502325
Salesforce/codegen-2B-nl,mmlu_elementary_mathematics,5-shot,accuracy,0.23809523809523808,0.021935878081184763
Salesforce/codegen-2B-nl,mmlu_high_school_biology,5-shot,accuracy,0.26129032258064516,0.024993053397764836
Salesforce/codegen-2B-nl,mmlu_high_school_chemistry,5-shot,accuracy,0.27586206896551724,0.03144712581678242
Salesforce/codegen-2B-nl,mmlu_high_school_computer_science,5-shot,accuracy,0.27,0.044619604333847394
Salesforce/codegen-2B-nl,mmlu_high_school_mathematics,5-shot,accuracy,0.24444444444444444,0.02620276653465215
Salesforce/codegen-2B-nl,mmlu_high_school_physics,5-shot,accuracy,0.2980132450331126,0.037345356767871984
Salesforce/codegen-2B-nl,mmlu_high_school_statistics,5-shot,accuracy,0.4027777777777778,0.033448873829978666
Salesforce/codegen-2B-nl,mmlu_machine_learning,5-shot,accuracy,0.19642857142857142,0.03770970049347019
huggyllama/llama-7b,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
huggyllama/llama-7b,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
huggyllama/llama-7b,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
huggyllama/llama-7b,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
huggyllama/llama-7b,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
huggyllama/llama-7b,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
huggyllama/llama-7b,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
huggyllama/llama-7b,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
huggyllama/llama-7b,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
huggyllama/llama-7b,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
huggyllama/llama-7b,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
huggyllama/llama-7b,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
huggyllama/llama-7b,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
huggyllama/llama-7b,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
huggyllama/llama-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891377
huggyllama/llama-7b,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
huggyllama/llama-7b,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
huggyllama/llama-7b,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
huggyllama/llama-7b,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
huggyllama/llama-7b,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
huggyllama/llama-7b,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
huggyllama/llama-7b,mmlu_miscellaneous,0-shot,accuracy,0.23754789272030652,0.015218733046150195
huggyllama/llama-7b,mmlu_nutrition,0-shot,accuracy,0.22549019607843138,0.023929155517351284
huggyllama/llama-7b,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
huggyllama/llama-7b,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
huggyllama/llama-7b,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
huggyllama/llama-7b,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
huggyllama/llama-7b,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
huggyllama/llama-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
huggyllama/llama-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
huggyllama/llama-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
huggyllama/llama-7b,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936073
huggyllama/llama-7b,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
huggyllama/llama-7b,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
huggyllama/llama-7b,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
huggyllama/llama-7b,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
huggyllama/llama-7b,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
huggyllama/llama-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
huggyllama/llama-7b,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
huggyllama/llama-7b,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
huggyllama/llama-7b,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
huggyllama/llama-7b,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
huggyllama/llama-7b,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
huggyllama/llama-7b,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
huggyllama/llama-7b,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
huggyllama/llama-7b,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
huggyllama/llama-7b,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
huggyllama/llama-7b,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
huggyllama/llama-7b,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
huggyllama/llama-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
huggyllama/llama-7b,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.021732540689329265
huggyllama/llama-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.025308904539380624
huggyllama/llama-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
huggyllama/llama-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
huggyllama/llama-7b,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
huggyllama/llama-7b,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
huggyllama/llama-7b,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
LLM360/CrystalCoder,mmlu_formal_logic,5-shot,accuracy,0.2777777777777778,0.04006168083848876
LLM360/CrystalCoder,mmlu_high_school_european_history,5-shot,accuracy,0.21818181818181817,0.03225078108306289
LLM360/CrystalCoder,mmlu_high_school_us_history,5-shot,accuracy,0.25,0.03039153369274154
LLM360/CrystalCoder,mmlu_high_school_world_history,5-shot,accuracy,0.26582278481012656,0.028756799629658335
LLM360/CrystalCoder,mmlu_international_law,5-shot,accuracy,0.23140495867768596,0.038498560987940904
LLM360/CrystalCoder,mmlu_jurisprudence,5-shot,accuracy,0.26851851851851855,0.04284467968052192
LLM360/CrystalCoder,mmlu_logical_fallacies,5-shot,accuracy,0.2147239263803681,0.03226219377286774
LLM360/CrystalCoder,mmlu_moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
LLM360/CrystalCoder,mmlu_moral_scenarios,5-shot,accuracy,0.23687150837988827,0.014219570788103982
LLM360/CrystalCoder,mmlu_philosophy,5-shot,accuracy,0.18971061093247588,0.022268196258783225
LLM360/CrystalCoder,mmlu_prehistory,5-shot,accuracy,0.2191358024691358,0.023016705640262206
LLM360/CrystalCoder,mmlu_professional_law,5-shot,accuracy,0.24641460234680573,0.011005971399927246
LLM360/CrystalCoder,mmlu_world_religions,5-shot,accuracy,0.2573099415204678,0.03352799844161865
LLM360/CrystalCoder,mmlu_business_ethics,5-shot,accuracy,0.3,0.046056618647183814
LLM360/CrystalCoder,mmlu_clinical_knowledge,5-shot,accuracy,0.23018867924528302,0.02590789712240817
LLM360/CrystalCoder,mmlu_college_medicine,5-shot,accuracy,0.20809248554913296,0.030952890217749884
LLM360/CrystalCoder,mmlu_global_facts,5-shot,accuracy,0.18,0.038612291966536955
LLM360/CrystalCoder,mmlu_human_aging,5-shot,accuracy,0.31390134529147984,0.03114679648297246
LLM360/CrystalCoder,mmlu_management,5-shot,accuracy,0.20388349514563106,0.039891398595317706
LLM360/CrystalCoder,mmlu_marketing,5-shot,accuracy,0.3034188034188034,0.030118210106942652
LLM360/CrystalCoder,mmlu_medical_genetics,5-shot,accuracy,0.28,0.045126085985421276
LLM360/CrystalCoder,mmlu_miscellaneous,5-shot,accuracy,0.23627075351213284,0.01519047371703751
LLM360/CrystalCoder,mmlu_nutrition,5-shot,accuracy,0.23529411764705882,0.024288619466046112
LLM360/CrystalCoder,mmlu_professional_accounting,5-shot,accuracy,0.23404255319148937,0.025257861359432407
LLM360/CrystalCoder,mmlu_professional_medicine,5-shot,accuracy,0.1875,0.023709788253811766
LLM360/CrystalCoder,mmlu_virology,5-shot,accuracy,0.28313253012048195,0.03507295431370518
LLM360/CrystalCoder,mmlu_econometrics,5-shot,accuracy,0.24561403508771928,0.04049339297748139
LLM360/CrystalCoder,mmlu_high_school_geography,5-shot,accuracy,0.1919191919191919,0.028057791672989017
LLM360/CrystalCoder,mmlu_high_school_government_and_politics,5-shot,accuracy,0.19689119170984457,0.02869787397186069
LLM360/CrystalCoder,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2153846153846154,0.020843034557462878
LLM360/CrystalCoder,mmlu_high_school_microeconomics,5-shot,accuracy,0.19747899159663865,0.02585916412205146
LLM360/CrystalCoder,mmlu_high_school_psychology,5-shot,accuracy,0.1944954128440367,0.01697028909045803
LLM360/CrystalCoder,mmlu_human_sexuality,5-shot,accuracy,0.25190839694656486,0.038073871163060866
LLM360/CrystalCoder,mmlu_professional_psychology,5-shot,accuracy,0.24836601307189543,0.017479487001364764
LLM360/CrystalCoder,mmlu_public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
LLM360/CrystalCoder,mmlu_security_studies,5-shot,accuracy,0.18775510204081633,0.02500025603954622
LLM360/CrystalCoder,mmlu_sociology,5-shot,accuracy,0.24378109452736318,0.030360490154014652
LLM360/CrystalCoder,mmlu_us_foreign_policy,5-shot,accuracy,0.3,0.046056618647183814
LLM360/CrystalCoder,mmlu_abstract_algebra,5-shot,accuracy,0.22,0.04163331998932269
LLM360/CrystalCoder,mmlu_anatomy,5-shot,accuracy,0.18518518518518517,0.03355677216313142
LLM360/CrystalCoder,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123394
LLM360/CrystalCoder,mmlu_college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
LLM360/CrystalCoder,mmlu_college_chemistry,5-shot,accuracy,0.21,0.040936018074033256
LLM360/CrystalCoder,mmlu_college_computer_science,5-shot,accuracy,0.26,0.044084400227680794
LLM360/CrystalCoder,mmlu_college_mathematics,5-shot,accuracy,0.2,0.040201512610368445
LLM360/CrystalCoder,mmlu_college_physics,5-shot,accuracy,0.21568627450980393,0.040925639582376556
LLM360/CrystalCoder,mmlu_computer_security,5-shot,accuracy,0.28,0.045126085985421276
LLM360/CrystalCoder,mmlu_conceptual_physics,5-shot,accuracy,0.26382978723404255,0.02880998985410298
LLM360/CrystalCoder,mmlu_electrical_engineering,5-shot,accuracy,0.2413793103448276,0.03565998174135302
LLM360/CrystalCoder,mmlu_elementary_mathematics,5-shot,accuracy,0.20634920634920634,0.020842290930114662
LLM360/CrystalCoder,mmlu_high_school_biology,5-shot,accuracy,0.1774193548387097,0.021732540689329265
LLM360/CrystalCoder,mmlu_high_school_chemistry,5-shot,accuracy,0.15270935960591134,0.025308904539380624
LLM360/CrystalCoder,mmlu_high_school_computer_science,5-shot,accuracy,0.25,0.04351941398892446
LLM360/CrystalCoder,mmlu_high_school_mathematics,5-shot,accuracy,0.2111111111111111,0.02488211685765508
LLM360/CrystalCoder,mmlu_high_school_physics,5-shot,accuracy,0.19205298013245034,0.03216298420593613
LLM360/CrystalCoder,mmlu_high_school_statistics,5-shot,accuracy,0.1527777777777778,0.02453632602613422
LLM360/CrystalCoder,mmlu_machine_learning,5-shot,accuracy,0.25,0.04109974682633932
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,bleu_max,22.653197042118084,0.6977462482504007
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,bleu_acc,0.2582619339045288,0.01532182168847617
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,bleu_diff,-8.600598721449371,0.7181881802706701
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rouge1_max,48.41828847315787,0.817476651655764
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rouge1_acc,0.24112607099143207,0.014974827279752351
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rouge1_diff,-10.796631296230201,0.7563516649964335
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rouge2_max,31.641528730202765,0.9065266163216646
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rouge2_acc,0.19951040391676866,0.013989929967559671
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rouge2_diff,-12.661226956966708,0.8862457887176071
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rougeL_max,45.35514964023815,0.8299008395111813
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rougeL_acc,0.23133414932680538,0.01476194517486264
EleutherAI/pythia-2.8b,truthfulqa_gen,0-shot,rougeL_diff,-11.124945721715576,0.7626207316962395
EleutherAI/pythia-2.8b,truthfulqa_mc1,0-shot,accuracy,0.21542227662178703,0.014391902652427678
EleutherAI/pythia-2.8b,truthfulqa_mc2,0-shot,accuracy,0.3588969882393517,0.013573083694031585
meta-llama/Llama-2-70b-hf,arc_challenge,25-shot,accuracy,0.6254266211604096,0.0141441934718934
meta-llama/Llama-2-70b-hf,arc_challenge,25-shot,acc_norm,0.6732081911262798,0.0137066659755873
meta-llama/Llama-2-70b-hf,hellaswag,10-shot,accuracy,0.6761601274646485,0.004669834130977
meta-llama/Llama-2-70b-hf,hellaswag,10-shot,acc_norm,0.8735311690898228,0.0033169770861701
meta-llama/Llama-2-70b-hf,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.046882617226215
meta-llama/Llama-2-70b-hf,hendrycksTest-anatomy,5-shot,accuracy,0.6296296296296297,0.0417165416135454
meta-llama/Llama-2-70b-hf,hendrycksTest-astronomy,5-shot,accuracy,0.8092105263157895,0.031975658210325
meta-llama/Llama-2-70b-hf,hendrycksTest-business_ethics,5-shot,accuracy,0.72,0.0451260859854212
meta-llama/Llama-2-70b-hf,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.720754716981132,0.0276111634023997
meta-llama/Llama-2-70b-hf,hendrycksTest-college_biology,5-shot,accuracy,0.8472222222222222,0.0300857432485656
meta-llama/Llama-2-70b-hf,hendrycksTest-college_chemistry,5-shot,accuracy,0.51,0.0502418393795691
meta-llama/Llama-2-70b-hf,hendrycksTest-college_computer_science,5-shot,accuracy,0.6,0.049236596391733
meta-llama/Llama-2-70b-hf,hendrycksTest-college_mathematics,5-shot,accuracy,0.37,0.048523658709391
meta-llama/Llama-2-70b-hf,hendrycksTest-college_medicine,5-shot,accuracy,0.6473988439306358,0.0364303716895854
meta-llama/Llama-2-70b-hf,hendrycksTest-college_physics,5-shot,accuracy,0.3725490196078431,0.0481084014808263
meta-llama/Llama-2-70b-hf,hendrycksTest-computer_security,5-shot,accuracy,0.77,0.042295258468165
meta-llama/Llama-2-70b-hf,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6638297872340425,0.0308816185206769
meta-llama/Llama-2-70b-hf,hendrycksTest-econometrics,5-shot,accuracy,0.4385964912280701,0.0466800073851045
meta-llama/Llama-2-70b-hf,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6551724137931034,0.039609335494512
meta-llama/Llama-2-70b-hf,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4338624338624338,0.0255250343824748
meta-llama/Llama-2-70b-hf,hendrycksTest-formal_logic,5-shot,accuracy,0.4761904761904761,0.0446706262840327
meta-llama/Llama-2-70b-hf,hendrycksTest-global_facts,5-shot,accuracy,0.47,0.0501613558046591
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_biology,5-shot,accuracy,0.8129032258064516,0.0221857100922522
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5172413793103449,0.0351589555116569
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.78,0.0416333199893226
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8303030303030303,0.0293111886749831
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_geography,5-shot,accuracy,0.8686868686868687,0.0240631564168225
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9430051813471504,0.0167310852936075
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7461538461538462,0.0220660543787262
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.337037037037037,0.0288208846662532
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.773109243697479,0.0272053715382794
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_physics,5-shot,accuracy,0.4304635761589404,0.0404280996139563
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8752293577981651,0.0141682983591563
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6342592592592593,0.032847388576472
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9117647058823528,0.0199073997913169
meta-llama/Llama-2-70b-hf,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8776371308016878,0.0213317418297467
meta-llama/Llama-2-70b-hf,hendrycksTest-human_aging,5-shot,accuracy,0.8026905829596412,0.0267098533449679
meta-llama/Llama-2-70b-hf,hendrycksTest-human_sexuality,5-shot,accuracy,0.8702290076335878,0.029473649496907
meta-llama/Llama-2-70b-hf,hendrycksTest-international_law,5-shot,accuracy,0.8760330578512396,0.0300830987160352
meta-llama/Llama-2-70b-hf,hendrycksTest-jurisprudence,5-shot,accuracy,0.8333333333333334,0.0360281417639264
meta-llama/Llama-2-70b-hf,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8098159509202454,0.0308334911462812
meta-llama/Llama-2-70b-hf,hendrycksTest-machine_learning,5-shot,accuracy,0.5267857142857143,0.0473897511927415
meta-llama/Llama-2-70b-hf,hendrycksTest-management,5-shot,accuracy,0.8349514563106796,0.0367566883223318
meta-llama/Llama-2-70b-hf,hendrycksTest-marketing,5-shot,accuracy,0.9102564102564102,0.0187243017419416
meta-llama/Llama-2-70b-hf,hendrycksTest-medical_genetics,5-shot,accuracy,0.74,0.0440844002276807
meta-llama/Llama-2-70b-hf,hendrycksTest-miscellaneous,5-shot,accuracy,0.8620689655172413,0.0123310093077956
meta-llama/Llama-2-70b-hf,hendrycksTest-moral_disputes,5-shot,accuracy,0.7716763005780347,0.0225987038043216
meta-llama/Llama-2-70b-hf,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4435754189944134,0.0166156804010037
meta-llama/Llama-2-70b-hf,hendrycksTest-nutrition,5-shot,accuracy,0.7712418300653595,0.0240510297399122
meta-llama/Llama-2-70b-hf,hendrycksTest-philosophy,5-shot,accuracy,0.7845659163987139,0.0233502254754714
meta-llama/Llama-2-70b-hf,hendrycksTest-prehistory,5-shot,accuracy,0.8395061728395061,0.020423955354778
meta-llama/Llama-2-70b-hf,hendrycksTest-professional_accounting,5-shot,accuracy,0.5602836879432624,0.0296099120755941
meta-llama/Llama-2-70b-hf,hendrycksTest-professional_law,5-shot,accuracy,0.529335071707953,0.0127482383973655
meta-llama/Llama-2-70b-hf,hendrycksTest-professional_medicine,5-shot,accuracy,0.7463235294117647,0.0264313298707895
meta-llama/Llama-2-70b-hf,hendrycksTest-professional_psychology,5-shot,accuracy,0.7565359477124183,0.0173624737621466
meta-llama/Llama-2-70b-hf,hendrycksTest-public_relations,5-shot,accuracy,0.6909090909090909,0.0442629464820009
meta-llama/Llama-2-70b-hf,hendrycksTest-security_studies,5-shot,accuracy,0.7836734693877551,0.026358916334904
meta-llama/Llama-2-70b-hf,hendrycksTest-sociology,5-shot,accuracy,0.9054726368159204,0.020687186951534
meta-llama/Llama-2-70b-hf,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
meta-llama/Llama-2-70b-hf,hendrycksTest-virology,5-shot,accuracy,0.536144578313253,0.0388231085089059
meta-llama/Llama-2-70b-hf,hendrycksTest-world_religions,5-shot,accuracy,0.8596491228070176,0.0266405825391332
meta-llama/Llama-2-70b-hf,truthfulqa:mc,0-shot,mc1,0.3108935128518972,0.0162033166735596
meta-llama/Llama-2-70b-hf,truthfulqa:mc,0-shot,mc2,0.4492197269463335,0.013903689329952
meta-llama/Llama-2-13b-hf,arc:challenge,25-shot,accuracy,0.5366894197952219,0.0145720005277569
meta-llama/Llama-2-13b-hf,arc:challenge,25-shot,acc_norm,0.5810580204778157,0.014418106953639
meta-llama/Llama-2-13b-hf,hellaswag,10-shot,accuracy,0.6059549890460068,0.0048764594346198
meta-llama/Llama-2-13b-hf,hellaswag,10-shot,acc_norm,0.809699263095001,0.0039173612541019
meta-llama/Llama-2-13b-hf,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.047258156262526
meta-llama/Llama-2-13b-hf,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.047258156262526
meta-llama/Llama-2-13b-hf,hendrycksTest-anatomy,5-shot,accuracy,0.5111111111111111,0.0431827549197797
meta-llama/Llama-2-13b-hf,hendrycksTest-anatomy,5-shot,acc_norm,0.5111111111111111,0.0431827549197797
meta-llama/Llama-2-13b-hf,hendrycksTest-astronomy,5-shot,accuracy,0.5131578947368421,0.0406753313630917
meta-llama/Llama-2-13b-hf,hendrycksTest-astronomy,5-shot,acc_norm,0.5131578947368421,0.0406753313630917
meta-llama/Llama-2-13b-hf,hendrycksTest-business_ethics,5-shot,accuracy,0.54,0.0500908265962033
meta-llama/Llama-2-13b-hf,hendrycksTest-business_ethics,5-shot,acc_norm,0.54,0.0500908265962033
meta-llama/Llama-2-13b-hf,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.6075471698113207,0.0300525805795578
meta-llama/Llama-2-13b-hf,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.6075471698113207,0.0300525805795578
meta-llama/Llama-2-13b-hf,hendrycksTest-college_biology,5-shot,accuracy,0.5555555555555556,0.0415531995559314
meta-llama/Llama-2-13b-hf,hendrycksTest-college_biology,5-shot,acc_norm,0.5555555555555556,0.0415531995559314
meta-llama/Llama-2-13b-hf,hendrycksTest-college_chemistry,5-shot,accuracy,0.43,0.0497569851956242
meta-llama/Llama-2-13b-hf,hendrycksTest-college_chemistry,5-shot,acc_norm,0.43,0.0497569851956242
meta-llama/Llama-2-13b-hf,hendrycksTest-college_computer_science,5-shot,accuracy,0.44,0.0498887651569858
meta-llama/Llama-2-13b-hf,hendrycksTest-college_computer_science,5-shot,acc_norm,0.44,0.0498887651569858
meta-llama/Llama-2-13b-hf,hendrycksTest-college_mathematics,5-shot,accuracy,0.34,0.0476095228569523
meta-llama/Llama-2-13b-hf,hendrycksTest-college_mathematics,5-shot,acc_norm,0.34,0.0476095228569523
meta-llama/Llama-2-13b-hf,hendrycksTest-college_medicine,5-shot,accuracy,0.5144508670520231,0.0381087163045476
meta-llama/Llama-2-13b-hf,hendrycksTest-college_medicine,5-shot,acc_norm,0.5144508670520231,0.0381087163045476
meta-llama/Llama-2-13b-hf,hendrycksTest-college_physics,5-shot,accuracy,0.2156862745098039,0.0409256395823765
meta-llama/Llama-2-13b-hf,hendrycksTest-college_physics,5-shot,acc_norm,0.2156862745098039,0.0409256395823765
meta-llama/Llama-2-13b-hf,hendrycksTest-computer_security,5-shot,accuracy,0.68,0.046882617226215
meta-llama/Llama-2-13b-hf,hendrycksTest-computer_security,5-shot,acc_norm,0.68,0.046882617226215
meta-llama/Llama-2-13b-hf,hendrycksTest-conceptual_physics,5-shot,accuracy,0.4553191489361702,0.0325552535934035
meta-llama/Llama-2-13b-hf,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.4553191489361702,0.0325552535934035
meta-llama/Llama-2-13b-hf,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.0426633944315939
meta-llama/Llama-2-13b-hf,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.0426633944315939
meta-llama/Llama-2-13b-hf,hendrycksTest-electrical_engineering,5-shot,accuracy,0.496551724137931,0.0416656757710157
meta-llama/Llama-2-13b-hf,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.496551724137931,0.0416656757710157
meta-llama/Llama-2-13b-hf,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3439153439153439,0.0244644266255964
meta-llama/Llama-2-13b-hf,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3439153439153439,0.0244644266255964
meta-llama/Llama-2-13b-hf,hendrycksTest-formal_logic,5-shot,accuracy,0.3333333333333333,0.0421637021355783
meta-llama/Llama-2-13b-hf,hendrycksTest-formal_logic,5-shot,acc_norm,0.3333333333333333,0.0421637021355783
meta-llama/Llama-2-13b-hf,hendrycksTest-global_facts,5-shot,accuracy,0.32,0.046882617226215
meta-llama/Llama-2-13b-hf,hendrycksTest-global_facts,5-shot,acc_norm,0.32,0.046882617226215
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_biology,5-shot,accuracy,0.6580645161290323,0.0269852895765527
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_biology,5-shot,acc_norm,0.6580645161290323,0.0269852895765527
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.458128078817734,0.0350563014078574
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.458128078817734,0.0350563014078574
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.55,0.05
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.55,0.05
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_european_history,5-shot,accuracy,0.6484848484848484,0.0372820699868265
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.6484848484848484,0.0372820699868265
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_geography,5-shot,accuracy,0.6868686868686869,0.0330420508781365
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_geography,5-shot,acc_norm,0.6868686868686869,0.0330420508781365
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.7668393782383419,0.030516111371476
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.7668393782383419,0.030516111371476
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4769230769230769,0.0253239908617361
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4769230769230769,0.0253239908617361
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2814814814814815,0.0274200193509452
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2814814814814815,0.0274200193509452
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.5546218487394958,0.0322841062671638
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.5546218487394958,0.0322841062671638
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_physics,5-shot,accuracy,0.390728476821192,0.039837983066598
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_physics,5-shot,acc_norm,0.390728476821192,0.039837983066598
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_psychology,5-shot,accuracy,0.7339449541284404,0.0189460223222255
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.7339449541284404,0.0189460223222255
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4675925925925926,0.0340280158135896
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4675925925925926,0.0340280158135896
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_us_history,5-shot,accuracy,0.6911764705882353,0.0324266171982721
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.6911764705882353,0.0324266171982721
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_world_history,5-shot,accuracy,0.7172995780590717,0.0293128141539559
meta-llama/Llama-2-13b-hf,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.7172995780590717,0.0293128141539559
meta-llama/Llama-2-13b-hf,hendrycksTest-human_aging,5-shot,accuracy,0.6636771300448431,0.031708824268455
meta-llama/Llama-2-13b-hf,hendrycksTest-human_aging,5-shot,acc_norm,0.6636771300448431,0.031708824268455
meta-llama/Llama-2-13b-hf,hendrycksTest-human_sexuality,5-shot,accuracy,0.6030534351145038,0.0429113567100922
meta-llama/Llama-2-13b-hf,hendrycksTest-human_sexuality,5-shot,acc_norm,0.6030534351145038,0.0429113567100922
meta-llama/Llama-2-13b-hf,hendrycksTest-international_law,5-shot,accuracy,0.7024793388429752,0.0417334914808349
meta-llama/Llama-2-13b-hf,hendrycksTest-international_law,5-shot,acc_norm,0.7024793388429752,0.0417334914808349
meta-llama/Llama-2-13b-hf,hendrycksTest-jurisprudence,5-shot,accuracy,0.7037037037037037,0.0441434366685493
meta-llama/Llama-2-13b-hf,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7037037037037037,0.0441434366685493
meta-llama/Llama-2-13b-hf,hendrycksTest-logical_fallacies,5-shot,accuracy,0.6871165644171779,0.036429145782924
meta-llama/Llama-2-13b-hf,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.6871165644171779,0.036429145782924
meta-llama/Llama-2-13b-hf,hendrycksTest-machine_learning,5-shot,accuracy,0.2678571428571428,0.0420327729146776
meta-llama/Llama-2-13b-hf,hendrycksTest-machine_learning,5-shot,acc_norm,0.2678571428571428,0.0420327729146776
meta-llama/Llama-2-13b-hf,hendrycksTest-management,5-shot,accuracy,0.7572815533980582,0.0424502248638449
meta-llama/Llama-2-13b-hf,hendrycksTest-management,5-shot,acc_norm,0.7572815533980582,0.0424502248638449
meta-llama/Llama-2-13b-hf,hendrycksTest-marketing,5-shot,accuracy,0.7991452991452992,0.0262467729468904
meta-llama/Llama-2-13b-hf,hendrycksTest-marketing,5-shot,acc_norm,0.7991452991452992,0.0262467729468904
meta-llama/Llama-2-13b-hf,hendrycksTest-medical_genetics,5-shot,accuracy,0.55,0.0499999999999999
meta-llama/Llama-2-13b-hf,hendrycksTest-medical_genetics,5-shot,acc_norm,0.55,0.0499999999999999
meta-llama/Llama-2-13b-hf,hendrycksTest-miscellaneous,5-shot,accuracy,0.7266922094508301,0.0159366810626285
meta-llama/Llama-2-13b-hf,hendrycksTest-miscellaneous,5-shot,acc_norm,0.7266922094508301,0.0159366810626285
meta-llama/Llama-2-13b-hf,hendrycksTest-moral_disputes,5-shot,accuracy,0.6358381502890174,0.0259066326310161
meta-llama/Llama-2-13b-hf,hendrycksTest-moral_disputes,5-shot,acc_norm,0.6358381502890174,0.0259066326310161
meta-llama/Llama-2-13b-hf,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2759776536312849,0.0149501030024753
meta-llama/Llama-2-13b-hf,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2759776536312849,0.0149501030024753
meta-llama/Llama-2-13b-hf,hendrycksTest-nutrition,5-shot,accuracy,0.5947712418300654,0.028110928492809
meta-llama/Llama-2-13b-hf,hendrycksTest-nutrition,5-shot,acc_norm,0.5947712418300654,0.028110928492809
meta-llama/Llama-2-13b-hf,hendrycksTest-philosophy,5-shot,accuracy,0.6495176848874598,0.0270986526213017
meta-llama/Llama-2-13b-hf,hendrycksTest-philosophy,5-shot,acc_norm,0.6495176848874598,0.0270986526213017
meta-llama/Llama-2-13b-hf,hendrycksTest-prehistory,5-shot,accuracy,0.6080246913580247,0.0271636860382711
meta-llama/Llama-2-13b-hf,hendrycksTest-prehistory,5-shot,acc_norm,0.6080246913580247,0.0271636860382711
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_accounting,5-shot,accuracy,0.3971631205673759,0.0291898056735871
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3971631205673759,0.0291898056735871
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_law,5-shot,accuracy,0.4159061277705345,0.0125883238503136
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_law,5-shot,acc_norm,0.4159061277705345,0.0125883238503136
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_medicine,5-shot,accuracy,0.5147058823529411,0.0303596970790461
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_medicine,5-shot,acc_norm,0.5147058823529411,0.0303596970790461
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_psychology,5-shot,accuracy,0.5424836601307189,0.0201546857125908
meta-llama/Llama-2-13b-hf,hendrycksTest-professional_psychology,5-shot,acc_norm,0.5424836601307189,0.0201546857125908
meta-llama/Llama-2-13b-hf,hendrycksTest-public_relations,5-shot,accuracy,0.5727272727272728,0.0473819870354548
meta-llama/Llama-2-13b-hf,hendrycksTest-public_relations,5-shot,acc_norm,0.5727272727272728,0.0473819870354548
meta-llama/Llama-2-13b-hf,hendrycksTest-security_studies,5-shot,accuracy,0.6122448979591837,0.0311922307267956
meta-llama/Llama-2-13b-hf,hendrycksTest-security_studies,5-shot,acc_norm,0.6122448979591837,0.0311922307267956
meta-llama/Llama-2-13b-hf,hendrycksTest-sociology,5-shot,accuracy,0.7114427860696517,0.0320384104021332
meta-llama/Llama-2-13b-hf,hendrycksTest-sociology,5-shot,acc_norm,0.7114427860696517,0.0320384104021332
meta-llama/Llama-2-13b-hf,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.83,0.0377525168068637
meta-llama/Llama-2-13b-hf,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.83,0.0377525168068637
meta-llama/Llama-2-13b-hf,hendrycksTest-virology,5-shot,accuracy,0.4156626506024096,0.0383672217659805
meta-llama/Llama-2-13b-hf,hendrycksTest-virology,5-shot,acc_norm,0.4156626506024096,0.0383672217659805
meta-llama/Llama-2-13b-hf,hendrycksTest-world_religions,5-shot,accuracy,0.7368421052631579,0.033773102522092
meta-llama/Llama-2-13b-hf,hendrycksTest-world_religions,5-shot,acc_norm,0.7368421052631579,0.033773102522092
meta-llama/Llama-2-13b-hf,truthfulqa:mc,0-shot,mc1,0.2386780905752754,0.0149226296954564
meta-llama/Llama-2-13b-hf,truthfulqa:mc,0-shot,mc2,0.3417240296370838,0.0133220535600087
meta-llama/Llama-2-13b-hf,winogrande,5-shot,accuracy,0.7663772691397001,0.0118921944771835
meta-llama/Llama-2-13b-hf,drop,3-shot,em,0.0014681208053691,0.0003921042190298
meta-llama/Llama-2-13b-hf,drop,3-shot,f1,0.0607822986577181,0.0013583957676382
meta-llama/Llama-2-13b-hf,gsm8k,5-shot,accuracy,0.2282031842304776,0.0115599148773173
EleutherAI/pythia-2.8b,winogrande,5-shot,accuracy,0.5114443567482242,0.014048804199859325
LLM360/CrystalCoder,winogrande,5-shot,accuracy,0.5059194948697711,0.014051500838485807
Dampish/StellarX-4B-V0,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_2da,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_3da,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_4da,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_5da,5-shot,accuracy,0.0,0.0
Dampish/StellarX-4B-V0,arithmetic_5ds,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,lambada_openai,0-shot,perplexity,454611124.49460846,56070448.37103204
LLM360/CrystalCoder,lambada_openai,0-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,lambada_standard,0-shot,perplexity,803465838.2685665,92887793.56685513
LLM360/CrystalCoder,lambada_standard,0-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_1dc,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_2da,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_2ds,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_3da,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_3ds,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_4da,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_4ds,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_5da,5-shot,accuracy,0.0,0.0
01-ai/Yi-9B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
LLM360/CrystalCoder,gsm8k,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
EleutherAI/pythia-12b,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/pythia-12b,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
EleutherAI/pythia-12b,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pythia-12b,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
EleutherAI/pythia-12b,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
EleutherAI/pythia-12b,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
EleutherAI/pythia-12b,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/pythia-12b,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
EleutherAI/pythia-12b,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/pythia-12b,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
EleutherAI/pythia-12b,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
EleutherAI/pythia-12b,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
EleutherAI/pythia-12b,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-12b,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891377
EleutherAI/pythia-12b,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
EleutherAI/pythia-12b,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
EleutherAI/pythia-12b,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
EleutherAI/pythia-12b,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
EleutherAI/pythia-12b,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
EleutherAI/pythia-12b,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-12b,mmlu_miscellaneous,0-shot,accuracy,0.2413793103448276,0.015302380123542087
EleutherAI/pythia-12b,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.023805186524888142
EleutherAI/pythia-12b,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
EleutherAI/pythia-12b,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
EleutherAI/pythia-12b,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
EleutherAI/pythia-12b,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
EleutherAI/pythia-12b,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
EleutherAI/pythia-12b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
EleutherAI/pythia-12b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
EleutherAI/pythia-12b,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
EleutherAI/pythia-12b,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936073
EleutherAI/pythia-12b,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/pythia-12b,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
EleutherAI/pythia-12b,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-12b,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
EleutherAI/pythia-12b,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/pythia-12b,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-12b,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-12b,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
EleutherAI/pythia-12b,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-12b,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
EleutherAI/pythia-12b,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-12b,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
EleutherAI/pythia-12b,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-12b,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
EleutherAI/pythia-12b,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-12b,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
EleutherAI/pythia-12b,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
EleutherAI/pythia-12b,mmlu_elementary_mathematics,0-shot,accuracy,0.21164021164021163,0.021037331505262886
EleutherAI/pythia-12b,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.021732540689329265
EleutherAI/pythia-12b,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.025308904539380624
EleutherAI/pythia-12b,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-12b,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
EleutherAI/pythia-12b,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
EleutherAI/pythia-12b,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
EleutherAI/pythia-12b,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
EleutherAI/pythia-12b-deduped,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
EleutherAI/pythia-12b-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/pythia-12b-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
EleutherAI/pythia-12b-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pythia-12b-deduped,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
EleutherAI/pythia-12b-deduped,mmlu_jurisprudence,0-shot,accuracy,0.26851851851851855,0.04284467968052192
EleutherAI/pythia-12b-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.20245398773006135,0.031570650789119026
EleutherAI/pythia-12b-deduped,mmlu_moral_disputes,0-shot,accuracy,0.2514450867052023,0.023357365785874037
EleutherAI/pythia-12b-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
EleutherAI/pythia-12b-deduped,mmlu_philosophy,0-shot,accuracy,0.19614147909967847,0.022552447780478026
EleutherAI/pythia-12b-deduped,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
EleutherAI/pythia-12b-deduped,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
EleutherAI/pythia-12b-deduped,mmlu_world_religions,0-shot,accuracy,0.29239766081871343,0.03488647713457922
EleutherAI/pythia-12b-deduped,mmlu_business_ethics,0-shot,accuracy,0.33,0.047258156262526045
EleutherAI/pythia-12b-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891377
EleutherAI/pythia-12b-deduped,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749895
EleutherAI/pythia-12b-deduped,mmlu_global_facts,0-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-12b-deduped,mmlu_human_aging,0-shot,accuracy,0.2825112107623318,0.030216831011508773
EleutherAI/pythia-12b-deduped,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
EleutherAI/pythia-12b-deduped,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.029872577708891172
EleutherAI/pythia-12b-deduped,mmlu_medical_genetics,0-shot,accuracy,0.29,0.04560480215720683
EleutherAI/pythia-12b-deduped,mmlu_miscellaneous,0-shot,accuracy,0.23371647509578544,0.01513338327898884
EleutherAI/pythia-12b-deduped,mmlu_nutrition,0-shot,accuracy,0.21568627450980393,0.02355083135199509
EleutherAI/pythia-12b-deduped,mmlu_professional_accounting,0-shot,accuracy,0.23049645390070922,0.025123739226872405
EleutherAI/pythia-12b-deduped,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
EleutherAI/pythia-12b-deduped,mmlu_virology,0-shot,accuracy,0.29518072289156627,0.035509201856896294
EleutherAI/pythia-12b-deduped,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748139
EleutherAI/pythia-12b-deduped,mmlu_high_school_geography,0-shot,accuracy,0.20202020202020202,0.02860620428922987
EleutherAI/pythia-12b-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.18652849740932642,0.028112091210117474
EleutherAI/pythia-12b-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2076923076923077,0.020567539567246787
EleutherAI/pythia-12b-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
EleutherAI/pythia-12b-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936077
EleutherAI/pythia-12b-deduped,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.03915345408847835
EleutherAI/pythia-12b-deduped,mmlu_professional_psychology,0-shot,accuracy,0.25163398692810457,0.017555818091322253
EleutherAI/pythia-12b-deduped,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-12b-deduped,mmlu_security_studies,0-shot,accuracy,0.19183673469387755,0.025206963154225395
EleutherAI/pythia-12b-deduped,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/pythia-12b-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-12b-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.041633319989322695
EleutherAI/pythia-12b-deduped,mmlu_anatomy,0-shot,accuracy,0.17777777777777778,0.03302789859901718
EleutherAI/pythia-12b-deduped,mmlu_astronomy,0-shot,accuracy,0.17105263157894737,0.030643607071677088
EleutherAI/pythia-12b-deduped,mmlu_college_biology,0-shot,accuracy,0.24305555555555555,0.03586879280080342
EleutherAI/pythia-12b-deduped,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-12b-deduped,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
EleutherAI/pythia-12b-deduped,mmlu_college_mathematics,0-shot,accuracy,0.19,0.03942772444036623
EleutherAI/pythia-12b-deduped,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.04158307533083286
EleutherAI/pythia-12b-deduped,mmlu_computer_security,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-12b-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.2680851063829787,0.02895734278834235
EleutherAI/pythia-12b-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
EleutherAI/pythia-12b-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.20105820105820105,0.020641810782370144
EleutherAI/pythia-12b-deduped,mmlu_high_school_biology,0-shot,accuracy,0.18064516129032257,0.021886178567172544
EleutherAI/pythia-12b-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.025308904539380624
EleutherAI/pythia-12b-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-12b-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.21481481481481482,0.025040443877000683
EleutherAI/pythia-12b-deduped,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
EleutherAI/pythia-12b-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
EleutherAI/pythia-12b-deduped,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.043270409325787296
meta-llama/Meta-Llama-3-8B,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
meta-llama/Meta-Llama-3-8B,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
meta-llama/Meta-Llama-3-8B,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
meta-llama/Meta-Llama-3-8B,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
meta-llama/Meta-Llama-3-8B,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
meta-llama/Meta-Llama-3-8B,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
meta-llama/Meta-Llama-3-8B,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
meta-llama/Meta-Llama-3-8B,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
meta-llama/Meta-Llama-3-8B,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
meta-llama/Meta-Llama-3-8B,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
meta-llama/Meta-Llama-3-8B,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
meta-llama/Meta-Llama-3-8B,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
meta-llama/Meta-Llama-3-8B,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
meta-llama/Meta-Llama-3-8B,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
meta-llama/Meta-Llama-3-8B,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891377
meta-llama/Meta-Llama-3-8B,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
meta-llama/Meta-Llama-3-8B,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
meta-llama/Meta-Llama-3-8B,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
meta-llama/Meta-Llama-3-8B,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
meta-llama/Meta-Llama-3-8B,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
meta-llama/Meta-Llama-3-8B,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
meta-llama/Meta-Llama-3-8B,mmlu_miscellaneous,0-shot,accuracy,0.23754789272030652,0.015218733046150195
meta-llama/Meta-Llama-3-8B,mmlu_nutrition,0-shot,accuracy,0.22549019607843138,0.023929155517351284
meta-llama/Meta-Llama-3-8B,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
meta-llama/Meta-Llama-3-8B,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
meta-llama/Meta-Llama-3-8B,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
meta-llama/Meta-Llama-3-8B,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
meta-llama/Meta-Llama-3-8B,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
meta-llama/Meta-Llama-3-8B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
meta-llama/Meta-Llama-3-8B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
meta-llama/Meta-Llama-3-8B,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
meta-llama/Meta-Llama-3-8B,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936073
meta-llama/Meta-Llama-3-8B,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
meta-llama/Meta-Llama-3-8B,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
meta-llama/Meta-Llama-3-8B,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
meta-llama/Meta-Llama-3-8B,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
meta-llama/Meta-Llama-3-8B,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
meta-llama/Meta-Llama-3-8B,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
meta-llama/Meta-Llama-3-8B,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
meta-llama/Meta-Llama-3-8B,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
meta-llama/Meta-Llama-3-8B,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
meta-llama/Meta-Llama-3-8B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
meta-llama/Meta-Llama-3-8B,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
meta-llama/Meta-Llama-3-8B,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
meta-llama/Meta-Llama-3-8B,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
meta-llama/Meta-Llama-3-8B,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
meta-llama/Meta-Llama-3-8B,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
meta-llama/Meta-Llama-3-8B,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
meta-llama/Meta-Llama-3-8B,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
meta-llama/Meta-Llama-3-8B,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
meta-llama/Meta-Llama-3-8B,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.021732540689329265
meta-llama/Meta-Llama-3-8B,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.025308904539380624
meta-llama/Meta-Llama-3-8B,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
meta-llama/Meta-Llama-3-8B,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
meta-llama/Meta-Llama-3-8B,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
meta-llama/Meta-Llama-3-8B,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
meta-llama/Meta-Llama-3-8B,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
LLM360/CrystalCoder,truthfulqa_gen,0-shot,bleu_max,0.023011683883776005,0.0061146623992637305
LLM360/CrystalCoder,truthfulqa_gen,0-shot,bleu_acc,0.03671970624235006,0.006583864096483336
LLM360/CrystalCoder,truthfulqa_gen,0-shot,bleu_diff,0.005241440951055133,0.006422829247420502
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rouge1_max,0.08768339819823978,0.01899078918168978
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rouge1_acc,0.044063647490820076,0.0071847164226778075
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rouge1_diff,0.005049801524112862,0.023839128097233472
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rouge2_max,0.0,0.0
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rouge2_acc,0.0,0.0
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rouge2_diff,0.0,0.0
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rougeL_max,0.08768339819823978,0.01899078918168978
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rougeL_acc,0.044063647490820076,0.0071847164226778075
LLM360/CrystalCoder,truthfulqa_gen,0-shot,rougeL_diff,0.005049801524112862,0.023839128097233472
LLM360/CrystalCoder,truthfulqa_mc1,0-shot,accuracy,0.2558139534883721,0.01527417621928335
LLM360/CrystalCoder,truthfulqa_mc2,0-shot,accuracy,0.4879337394333515,0.016116098796196365
Qwen/Qwen2.5-14B,arc_challenge,25-shot,accuracy,0.6450511945392492,0.013983036904094082
Qwen/Qwen2.5-14B,arc_challenge,25-shot,acc_norm,0.6706484641638225,0.013734057652635474
Qwen/Qwen1.5-14B,arithmetic_1dc,5-shot,accuracy,0.002,0.0009992493430695131
Qwen/Qwen1.5-14B,arithmetic_2da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_3da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_4da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_5da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-14B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_2da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_3da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_4da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_5da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-13b-hf,arithmetic_1dc,5-shot,accuracy,0.003,0.0012232122154647062
meta-llama/Llama-2-13b-hf,arithmetic_2da,5-shot,accuracy,0.002,0.0009992493430695105
meta-llama/Llama-2-13b-hf,arithmetic_2dm,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-13b-hf,arithmetic_2ds,5-shot,accuracy,0.066,0.005553144938623082
meta-llama/Llama-2-13b-hf,arithmetic_3da,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-13b-hf,arithmetic_3ds,5-shot,accuracy,0.037,0.004221896754552659
meta-llama/Llama-2-13b-hf,arithmetic_4da,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-13b-hf,arithmetic_4ds,5-shot,accuracy,0.0115,0.0023846841214675827
meta-llama/Llama-2-13b-hf,arithmetic_5da,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-13b-hf,arithmetic_5ds,5-shot,accuracy,0.0075,0.0019296986470519835
Salesforce/codegen-16B-nl,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_2da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_3da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005000000000000157
Salesforce/codegen-16B-nl,arithmetic_4da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_5da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-nl,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_2da,5-shot,accuracy,0.001,0.0007069298939339479
Salesforce/codegen-16B-mono,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_3da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_4da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_5da,5-shot,accuracy,0.0,0.0
Salesforce/codegen-16B-mono,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/gpt-neox-20b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_1dc,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_2da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_2dm,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_2ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_3da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_3ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_4da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_4ds,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_5da,5-shot,accuracy,0.0,0.0
EleutherAI/pythia-12b-deduped,arithmetic_5ds,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_2da,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_3da,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_4da,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_5da,5-shot,accuracy,0.0,0.0
google/gemma-2-9b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-14B,gsm8k,5-shot,accuracy,0.8718726307808946,0.009206398549980031
Qwen/Qwen2.5-14B,hellaswag,10-shot,accuracy,0.6457876916948815,0.00477296469794137
Qwen/Qwen2.5-14B,hellaswag,10-shot,acc_norm,0.8449512049392551,0.0036121146706990003
Salesforce/codegen-6B-nl,lambada_openai,0-shot,perplexity,4.2886137680327945,0.09250888632877358
Salesforce/codegen-6B-nl,lambada_openai,0-shot,accuracy,0.6714535222200659,0.006543625841488836
Salesforce/codegen-6B-nl,lambada_standard,0-shot,perplexity,5.588772655633852,0.13078812661352987
Salesforce/codegen-6B-nl,lambada_standard,0-shot,accuracy,0.609353774500291,0.006797334493142837
Salesforce/codegen-16B-nl,lambada_openai,0-shot,perplexity,3.806994366787162,0.07881182228518817
Salesforce/codegen-16B-nl,lambada_openai,0-shot,accuracy,0.6933824956336115,0.006423873526429438
Salesforce/codegen-16B-nl,lambada_standard,0-shot,perplexity,5.228156339038241,0.11865665364819865
Salesforce/codegen-16B-nl,lambada_standard,0-shot,accuracy,0.6130409470211527,0.006785616642963458
meta-llama/Llama-2-13b-hf,lambada_openai,0-shot,perplexity,3.044072745841782,0.05617085329846811
meta-llama/Llama-2-13b-hf,lambada_openai,0-shot,accuracy,0.7675140694740927,0.005885096429388086
meta-llama/Llama-2-13b-hf,lambada_standard,0-shot,perplexity,3.694449299255739,0.06968939420903135
meta-llama/Llama-2-13b-hf,lambada_standard,0-shot,accuracy,0.7030855812148263,0.006365489598037049
Qwen/Qwen2.5-14B,lambada_openai,0-shot,perplexity,3.1970587276234044,0.06161265631663553
Qwen/Qwen2.5-14B,lambada_openai,0-shot,accuracy,0.7413157384048127,0.006100967149142441
Qwen/Qwen2.5-14B,lambada_standard,0-shot,perplexity,3.5237625309108545,0.06831658453743557
Qwen/Qwen2.5-14B,lambada_standard,0-shot,accuracy,0.7075490005821852,0.006337484186544318
EleutherAI/gpt-neox-20b,lambada_openai,0-shot,perplexity,3.638787476179755,0.07466771975960518
EleutherAI/gpt-neox-20b,lambada_openai,0-shot,accuracy,0.7197748884145159,0.006256968140793456
EleutherAI/gpt-neox-20b,lambada_standard,0-shot,perplexity,5.296799244128978,0.12356199522825428
EleutherAI/gpt-neox-20b,lambada_standard,0-shot,accuracy,0.6252668348534834,0.006743817908692017
EleutherAI/gpt-neox-20b,mmlu_formal_logic,0-shot,accuracy,0.25396825396825395,0.03893259610604674
EleutherAI/gpt-neox-20b,mmlu_high_school_european_history,0-shot,accuracy,0.2606060606060606,0.034277431758165236
EleutherAI/gpt-neox-20b,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.030964517926923403
EleutherAI/gpt-neox-20b,mmlu_high_school_world_history,0-shot,accuracy,0.25738396624472576,0.02845882099146029
EleutherAI/gpt-neox-20b,mmlu_international_law,0-shot,accuracy,0.2809917355371901,0.041032038305145124
EleutherAI/gpt-neox-20b,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
EleutherAI/gpt-neox-20b,mmlu_logical_fallacies,0-shot,accuracy,0.20245398773006135,0.03157065078911902
EleutherAI/gpt-neox-20b,mmlu_moral_disputes,0-shot,accuracy,0.24277456647398843,0.023083658586984204
EleutherAI/gpt-neox-20b,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217892
EleutherAI/gpt-neox-20b,mmlu_philosophy,0-shot,accuracy,0.22186495176848875,0.02359885829286305
EleutherAI/gpt-neox-20b,mmlu_prehistory,0-shot,accuracy,0.24382716049382716,0.023891879541959624
EleutherAI/gpt-neox-20b,mmlu_professional_law,0-shot,accuracy,0.23142112125162972,0.01077146171157647
EleutherAI/gpt-neox-20b,mmlu_world_religions,0-shot,accuracy,0.29239766081871343,0.03488647713457921
EleutherAI/gpt-neox-20b,mmlu_business_ethics,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/gpt-neox-20b,mmlu_clinical_knowledge,0-shot,accuracy,0.2528301886792453,0.02674989977124123
EleutherAI/gpt-neox-20b,mmlu_college_medicine,0-shot,accuracy,0.19653179190751446,0.030299574664788137
EleutherAI/gpt-neox-20b,mmlu_global_facts,0-shot,accuracy,0.27,0.044619604333847415
EleutherAI/gpt-neox-20b,mmlu_human_aging,0-shot,accuracy,0.36771300448430494,0.032361983509282745
EleutherAI/gpt-neox-20b,mmlu_management,0-shot,accuracy,0.21359223300970873,0.04058042015646033
EleutherAI/gpt-neox-20b,mmlu_marketing,0-shot,accuracy,0.2606837606837607,0.028760348956523414
EleutherAI/gpt-neox-20b,mmlu_medical_genetics,0-shot,accuracy,0.23,0.04229525846816505
EleutherAI/gpt-neox-20b,mmlu_miscellaneous,0-shot,accuracy,0.26436781609195403,0.015769984840690518
EleutherAI/gpt-neox-20b,mmlu_nutrition,0-shot,accuracy,0.2581699346405229,0.025058503316958157
EleutherAI/gpt-neox-20b,mmlu_professional_accounting,0-shot,accuracy,0.24468085106382978,0.025645553622266736
EleutherAI/gpt-neox-20b,mmlu_professional_medicine,0-shot,accuracy,0.19852941176470587,0.0242310133705411
EleutherAI/gpt-neox-20b,mmlu_virology,0-shot,accuracy,0.3493975903614458,0.037117251907407486
EleutherAI/gpt-neox-20b,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748142
EleutherAI/gpt-neox-20b,mmlu_high_school_geography,0-shot,accuracy,0.18181818181818182,0.027479603010538783
EleutherAI/gpt-neox-20b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20207253886010362,0.02897908979429673
EleutherAI/gpt-neox-20b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24358974358974358,0.02176373368417392
EleutherAI/gpt-neox-20b,mmlu_high_school_microeconomics,0-shot,accuracy,0.23109243697478993,0.027381406927868966
EleutherAI/gpt-neox-20b,mmlu_high_school_psychology,0-shot,accuracy,0.1963302752293578,0.017030719339154354
EleutherAI/gpt-neox-20b,mmlu_human_sexuality,0-shot,accuracy,0.29770992366412213,0.04010358942462203
EleutherAI/gpt-neox-20b,mmlu_professional_psychology,0-shot,accuracy,0.2434640522875817,0.017362473762146623
EleutherAI/gpt-neox-20b,mmlu_public_relations,0-shot,accuracy,0.39090909090909093,0.04673752333670238
EleutherAI/gpt-neox-20b,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.025000256039546195
EleutherAI/gpt-neox-20b,mmlu_sociology,0-shot,accuracy,0.27860696517412936,0.031700561834973086
EleutherAI/gpt-neox-20b,mmlu_us_foreign_policy,0-shot,accuracy,0.33,0.047258156262526045
EleutherAI/gpt-neox-20b,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/gpt-neox-20b,mmlu_anatomy,0-shot,accuracy,0.24444444444444444,0.03712537833614866
EleutherAI/gpt-neox-20b,mmlu_astronomy,0-shot,accuracy,0.20394736842105263,0.032790004063100495
EleutherAI/gpt-neox-20b,mmlu_college_biology,0-shot,accuracy,0.20833333333333334,0.03396116205845335
EleutherAI/gpt-neox-20b,mmlu_college_chemistry,0-shot,accuracy,0.17,0.0377525168068637
EleutherAI/gpt-neox-20b,mmlu_college_computer_science,0-shot,accuracy,0.2,0.04020151261036846
EleutherAI/gpt-neox-20b,mmlu_college_mathematics,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/gpt-neox-20b,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237653
EleutherAI/gpt-neox-20b,mmlu_computer_security,0-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-neox-20b,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.03047297336338005
EleutherAI/gpt-neox-20b,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
EleutherAI/gpt-neox-20b,mmlu_elementary_mathematics,0-shot,accuracy,0.25925925925925924,0.022569897074918407
EleutherAI/gpt-neox-20b,mmlu_high_school_biology,0-shot,accuracy,0.22903225806451613,0.02390491431178265
EleutherAI/gpt-neox-20b,mmlu_high_school_chemistry,0-shot,accuracy,0.2315270935960591,0.029678333141444437
EleutherAI/gpt-neox-20b,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/gpt-neox-20b,mmlu_high_school_mathematics,0-shot,accuracy,0.2777777777777778,0.027309140588230193
EleutherAI/gpt-neox-20b,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649006,0.034791855725996586
EleutherAI/gpt-neox-20b,mmlu_high_school_statistics,0-shot,accuracy,0.16666666666666666,0.02541642838876747
EleutherAI/gpt-neox-20b,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.04327040932578728
Salesforce/codegen-16B-nl,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.03970158273235172
Salesforce/codegen-16B-nl,mmlu_high_school_european_history,0-shot,accuracy,0.3212121212121212,0.03646204963253814
Salesforce/codegen-16B-nl,mmlu_high_school_us_history,0-shot,accuracy,0.28431372549019607,0.031660096793998116
Salesforce/codegen-16B-nl,mmlu_high_school_world_history,0-shot,accuracy,0.31645569620253167,0.03027497488021897
Salesforce/codegen-16B-nl,mmlu_international_law,0-shot,accuracy,0.2892561983471074,0.04139112727635463
Salesforce/codegen-16B-nl,mmlu_jurisprudence,0-shot,accuracy,0.32407407407407407,0.04524596007030048
Salesforce/codegen-16B-nl,mmlu_logical_fallacies,0-shot,accuracy,0.294478527607362,0.03581165790474082
Salesforce/codegen-16B-nl,mmlu_moral_disputes,0-shot,accuracy,0.3236994219653179,0.025190181327608405
Salesforce/codegen-16B-nl,mmlu_moral_scenarios,0-shot,accuracy,0.2536312849162011,0.014551553659369922
Salesforce/codegen-16B-nl,mmlu_philosophy,0-shot,accuracy,0.24115755627009647,0.02429659403476343
Salesforce/codegen-16B-nl,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.024748624490537382
Salesforce/codegen-16B-nl,mmlu_professional_law,0-shot,accuracy,0.28552803129074317,0.011535751586665657
Salesforce/codegen-16B-nl,mmlu_world_religions,0-shot,accuracy,0.3508771929824561,0.03660298834049164
Salesforce/codegen-16B-nl,mmlu_business_ethics,0-shot,accuracy,0.34,0.04760952285695236
Salesforce/codegen-16B-nl,mmlu_clinical_knowledge,0-shot,accuracy,0.2792452830188679,0.027611163402399715
Salesforce/codegen-16B-nl,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
Salesforce/codegen-16B-nl,mmlu_global_facts,0-shot,accuracy,0.3,0.046056618647183814
Salesforce/codegen-16B-nl,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.03126580522513714
Salesforce/codegen-16B-nl,mmlu_management,0-shot,accuracy,0.2621359223300971,0.043546310772605956
Salesforce/codegen-16B-nl,mmlu_marketing,0-shot,accuracy,0.3034188034188034,0.03011821010694266
Salesforce/codegen-16B-nl,mmlu_medical_genetics,0-shot,accuracy,0.33,0.047258156262526045
Salesforce/codegen-16B-nl,mmlu_miscellaneous,0-shot,accuracy,0.3001277139208174,0.016389249691317432
Salesforce/codegen-16B-nl,mmlu_nutrition,0-shot,accuracy,0.2777777777777778,0.02564686309713791
Salesforce/codegen-16B-nl,mmlu_professional_accounting,0-shot,accuracy,0.24468085106382978,0.02564555362226673
Salesforce/codegen-16B-nl,mmlu_professional_medicine,0-shot,accuracy,0.20220588235294118,0.024398192986654924
Salesforce/codegen-16B-nl,mmlu_virology,0-shot,accuracy,0.3433734939759036,0.03696584317010601
Salesforce/codegen-16B-nl,mmlu_econometrics,0-shot,accuracy,0.20175438596491227,0.037752050135836386
Salesforce/codegen-16B-nl,mmlu_high_school_geography,0-shot,accuracy,0.26262626262626265,0.03135305009533084
Salesforce/codegen-16B-nl,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2849740932642487,0.03257714077709661
Salesforce/codegen-16B-nl,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24358974358974358,0.021763733684173923
Salesforce/codegen-16B-nl,mmlu_high_school_microeconomics,0-shot,accuracy,0.2815126050420168,0.02921354941437216
Salesforce/codegen-16B-nl,mmlu_high_school_psychology,0-shot,accuracy,0.22935779816513763,0.018025349724618684
Salesforce/codegen-16B-nl,mmlu_human_sexuality,0-shot,accuracy,0.33587786259541985,0.041423137719966634
Salesforce/codegen-16B-nl,mmlu_professional_psychology,0-shot,accuracy,0.29248366013071897,0.018403415710109793
Salesforce/codegen-16B-nl,mmlu_public_relations,0-shot,accuracy,0.2818181818181818,0.043091187099464585
Salesforce/codegen-16B-nl,mmlu_security_studies,0-shot,accuracy,0.24897959183673468,0.027682979522960244
Salesforce/codegen-16B-nl,mmlu_sociology,0-shot,accuracy,0.31840796019900497,0.032941184790540944
Salesforce/codegen-16B-nl,mmlu_us_foreign_policy,0-shot,accuracy,0.39,0.04902071300001974
Salesforce/codegen-16B-nl,mmlu_abstract_algebra,0-shot,accuracy,0.27,0.04461960433384741
Salesforce/codegen-16B-nl,mmlu_anatomy,0-shot,accuracy,0.31851851851851853,0.0402477840197711
Salesforce/codegen-16B-nl,mmlu_astronomy,0-shot,accuracy,0.3026315789473684,0.03738520676119669
Salesforce/codegen-16B-nl,mmlu_college_biology,0-shot,accuracy,0.2916666666666667,0.03800968060554858
Salesforce/codegen-16B-nl,mmlu_college_chemistry,0-shot,accuracy,0.28,0.04512608598542127
Salesforce/codegen-16B-nl,mmlu_college_computer_science,0-shot,accuracy,0.32,0.046882617226215034
Salesforce/codegen-16B-nl,mmlu_college_mathematics,0-shot,accuracy,0.42,0.049604496374885836
Salesforce/codegen-16B-nl,mmlu_college_physics,0-shot,accuracy,0.2647058823529412,0.043898699568087785
Salesforce/codegen-16B-nl,mmlu_computer_security,0-shot,accuracy,0.3,0.046056618647183814
Salesforce/codegen-16B-nl,mmlu_conceptual_physics,0-shot,accuracy,0.33617021276595743,0.030881618520676942
Salesforce/codegen-16B-nl,mmlu_electrical_engineering,0-shot,accuracy,0.3103448275862069,0.03855289616378948
Salesforce/codegen-16B-nl,mmlu_elementary_mathematics,0-shot,accuracy,0.25396825396825395,0.022418042891113946
Salesforce/codegen-16B-nl,mmlu_high_school_biology,0-shot,accuracy,0.27741935483870966,0.025470196835900055
Salesforce/codegen-16B-nl,mmlu_high_school_chemistry,0-shot,accuracy,0.2561576354679803,0.0307127300709826
Salesforce/codegen-16B-nl,mmlu_high_school_computer_science,0-shot,accuracy,0.24,0.04292346959909283
Salesforce/codegen-16B-nl,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.026466117538959916
Salesforce/codegen-16B-nl,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.03710185726119993
Salesforce/codegen-16B-nl,mmlu_high_school_statistics,0-shot,accuracy,0.20833333333333334,0.027696910713093936
Salesforce/codegen-16B-nl,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
meta-llama/Llama-2-13b-hf,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.0404061017820884
meta-llama/Llama-2-13b-hf,mmlu_high_school_european_history,0-shot,accuracy,0.6181818181818182,0.03793713171165635
meta-llama/Llama-2-13b-hf,mmlu_high_school_us_history,0-shot,accuracy,0.6715686274509803,0.03296245110172229
meta-llama/Llama-2-13b-hf,mmlu_high_school_world_history,0-shot,accuracy,0.7088607594936709,0.029571601065753374
meta-llama/Llama-2-13b-hf,mmlu_international_law,0-shot,accuracy,0.71900826446281,0.04103203830514512
meta-llama/Llama-2-13b-hf,mmlu_jurisprudence,0-shot,accuracy,0.6388888888888888,0.04643454608906275
meta-llama/Llama-2-13b-hf,mmlu_logical_fallacies,0-shot,accuracy,0.6319018404907976,0.03789213935838396
meta-llama/Llama-2-13b-hf,mmlu_moral_disputes,0-shot,accuracy,0.5289017341040463,0.026874085883518348
meta-llama/Llama-2-13b-hf,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.01442229220480885
meta-llama/Llama-2-13b-hf,mmlu_philosophy,0-shot,accuracy,0.6430868167202572,0.027210420375934005
meta-llama/Llama-2-13b-hf,mmlu_prehistory,0-shot,accuracy,0.6080246913580247,0.027163686038271146
meta-llama/Llama-2-13b-hf,mmlu_professional_law,0-shot,accuracy,0.4041720990873533,0.012533504046491362
meta-llama/Llama-2-13b-hf,mmlu_world_religions,0-shot,accuracy,0.7602339181286549,0.03274485211946956
meta-llama/Llama-2-13b-hf,mmlu_business_ethics,0-shot,accuracy,0.52,0.050211673156867795
meta-llama/Llama-2-13b-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.5886792452830188,0.030285009259009787
meta-llama/Llama-2-13b-hf,mmlu_college_medicine,0-shot,accuracy,0.49710982658959535,0.038124005659748335
meta-llama/Llama-2-13b-hf,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621504
meta-llama/Llama-2-13b-hf,mmlu_human_aging,0-shot,accuracy,0.5650224215246636,0.033272833702713445
meta-llama/Llama-2-13b-hf,mmlu_management,0-shot,accuracy,0.7378640776699029,0.04354631077260595
meta-llama/Llama-2-13b-hf,mmlu_marketing,0-shot,accuracy,0.7564102564102564,0.028120966503914397
meta-llama/Llama-2-13b-hf,mmlu_medical_genetics,0-shot,accuracy,0.55,0.05
meta-llama/Llama-2-13b-hf,mmlu_miscellaneous,0-shot,accuracy,0.7241379310344828,0.01598281477469563
meta-llama/Llama-2-13b-hf,mmlu_nutrition,0-shot,accuracy,0.6241830065359477,0.027732834353363947
meta-llama/Llama-2-13b-hf,mmlu_professional_accounting,0-shot,accuracy,0.40070921985815605,0.029233465745573093
meta-llama/Llama-2-13b-hf,mmlu_professional_medicine,0-shot,accuracy,0.5294117647058824,0.030320243265004137
meta-llama/Llama-2-13b-hf,mmlu_virology,0-shot,accuracy,0.43373493975903615,0.03858158940685516
meta-llama/Llama-2-13b-hf,mmlu_econometrics,0-shot,accuracy,0.22807017543859648,0.03947152782669415
meta-llama/Llama-2-13b-hf,mmlu_high_school_geography,0-shot,accuracy,0.6818181818181818,0.03318477333845331
meta-llama/Llama-2-13b-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.7616580310880829,0.030748905363909895
meta-llama/Llama-2-13b-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4948717948717949,0.02534967290683865
meta-llama/Llama-2-13b-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.5378151260504201,0.032385469487589795
meta-llama/Llama-2-13b-hf,mmlu_high_school_psychology,0-shot,accuracy,0.708256880733945,0.019489300968876515
meta-llama/Llama-2-13b-hf,mmlu_human_sexuality,0-shot,accuracy,0.6641221374045801,0.041423137719966634
meta-llama/Llama-2-13b-hf,mmlu_professional_psychology,0-shot,accuracy,0.5261437908496732,0.020200164564804588
meta-llama/Llama-2-13b-hf,mmlu_public_relations,0-shot,accuracy,0.6090909090909091,0.04673752333670239
meta-llama/Llama-2-13b-hf,mmlu_security_studies,0-shot,accuracy,0.6448979591836734,0.030635655150387634
meta-llama/Llama-2-13b-hf,mmlu_sociology,0-shot,accuracy,0.7512437810945274,0.03056767593891672
meta-llama/Llama-2-13b-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.81,0.03942772444036624
meta-llama/Llama-2-13b-hf,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276808
meta-llama/Llama-2-13b-hf,mmlu_anatomy,0-shot,accuracy,0.48148148148148145,0.04316378599511324
meta-llama/Llama-2-13b-hf,mmlu_astronomy,0-shot,accuracy,0.5657894736842105,0.0403356566784832
meta-llama/Llama-2-13b-hf,mmlu_college_biology,0-shot,accuracy,0.5208333333333334,0.04177578950739994
meta-llama/Llama-2-13b-hf,mmlu_college_chemistry,0-shot,accuracy,0.44,0.049888765156985884
meta-llama/Llama-2-13b-hf,mmlu_college_computer_science,0-shot,accuracy,0.36,0.048241815132442176
meta-llama/Llama-2-13b-hf,mmlu_college_mathematics,0-shot,accuracy,0.32,0.04688261722621505
meta-llama/Llama-2-13b-hf,mmlu_college_physics,0-shot,accuracy,0.24509803921568626,0.042801058373643945
meta-llama/Llama-2-13b-hf,mmlu_computer_security,0-shot,accuracy,0.65,0.047937248544110196
meta-llama/Llama-2-13b-hf,mmlu_conceptual_physics,0-shot,accuracy,0.40425531914893614,0.03208115750788684
meta-llama/Llama-2-13b-hf,mmlu_electrical_engineering,0-shot,accuracy,0.5172413793103449,0.04164188720169375
meta-llama/Llama-2-13b-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.3148148148148148,0.023919984164047732
meta-llama/Llama-2-13b-hf,mmlu_high_school_biology,0-shot,accuracy,0.6516129032258065,0.027104826328100944
meta-llama/Llama-2-13b-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.4630541871921182,0.035083705204426656
meta-llama/Llama-2-13b-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.53,0.050161355804659205
meta-llama/Llama-2-13b-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.02646611753895991
meta-llama/Llama-2-13b-hf,mmlu_high_school_physics,0-shot,accuracy,0.31788079470198677,0.038020397601079024
meta-llama/Llama-2-13b-hf,mmlu_high_school_statistics,0-shot,accuracy,0.4305555555555556,0.03376922151252336
meta-llama/Llama-2-13b-hf,mmlu_machine_learning,0-shot,accuracy,0.26785714285714285,0.04203277291467762
google/gemma-2-2b,mmlu_formal_logic,5-shot,accuracy,0.3253968253968254,0.041905964388711366
google/gemma-2-2b,mmlu_high_school_european_history,5-shot,accuracy,0.6545454545454545,0.03713158067481912
google/gemma-2-2b,mmlu_high_school_us_history,5-shot,accuracy,0.6715686274509803,0.03296245110172229
google/gemma-2-2b,mmlu_high_school_world_history,5-shot,accuracy,0.6666666666666666,0.030685820596610812
google/gemma-2-2b,mmlu_international_law,5-shot,accuracy,0.6859504132231405,0.04236964753041018
google/gemma-2-2b,mmlu_jurisprudence,5-shot,accuracy,0.6018518518518519,0.04732332615978813
google/gemma-2-2b,mmlu_logical_fallacies,5-shot,accuracy,0.6134969325153374,0.03825825548848607
google/gemma-2-2b,mmlu_moral_disputes,5-shot,accuracy,0.5982658959537572,0.026394104177643627
google/gemma-2-2b,mmlu_moral_scenarios,5-shot,accuracy,0.2770949720670391,0.014968772435812145
google/gemma-2-2b,mmlu_philosophy,5-shot,accuracy,0.5884244372990354,0.027950481494401255
google/gemma-2-2b,mmlu_prehistory,5-shot,accuracy,0.5648148148148148,0.027586006221607715
google/gemma-2-2b,mmlu_professional_law,5-shot,accuracy,0.40352020860495436,0.012530241301193169
google/gemma-2-2b,mmlu_world_religions,5-shot,accuracy,0.695906432748538,0.0352821125824523
google/gemma-2-2b,mmlu_business_ethics,5-shot,accuracy,0.53,0.05016135580465919
google/gemma-2-2b,mmlu_clinical_knowledge,5-shot,accuracy,0.569811320754717,0.030471445867183238
google/gemma-2-2b,mmlu_college_medicine,5-shot,accuracy,0.6127167630057804,0.037143259063020656
google/gemma-2-2b,mmlu_global_facts,5-shot,accuracy,0.29,0.04560480215720684
google/gemma-2-2b,mmlu_human_aging,5-shot,accuracy,0.5829596412556054,0.03309266936071721
google/gemma-2-2b,mmlu_management,5-shot,accuracy,0.6893203883495146,0.04582124160161549
google/gemma-2-2b,mmlu_marketing,5-shot,accuracy,0.7991452991452992,0.026246772946890463
google/gemma-2-2b,mmlu_medical_genetics,5-shot,accuracy,0.69,0.04648231987117316
google/gemma-2-2b,mmlu_miscellaneous,5-shot,accuracy,0.669220945083014,0.016824818462563756
google/gemma-2-2b,mmlu_nutrition,5-shot,accuracy,0.5980392156862745,0.028074158947600666
google/gemma-2-2b,mmlu_professional_accounting,5-shot,accuracy,0.40425531914893614,0.029275532159704725
google/gemma-2-2b,mmlu_professional_medicine,5-shot,accuracy,0.4227941176470588,0.030008562845003486
google/gemma-2-2b,mmlu_virology,5-shot,accuracy,0.463855421686747,0.03882310850890594
google/gemma-2-2b,mmlu_econometrics,5-shot,accuracy,0.32456140350877194,0.04404556157374767
google/gemma-2-2b,mmlu_high_school_geography,5-shot,accuracy,0.7525252525252525,0.03074630074212451
google/gemma-2-2b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.7668393782383419,0.030516111371476008
google/gemma-2-2b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.5307692307692308,0.025302958890850154
google/gemma-2-2b,mmlu_high_school_microeconomics,5-shot,accuracy,0.6134453781512605,0.0316314580755238
google/gemma-2-2b,mmlu_high_school_psychology,5-shot,accuracy,0.7412844036697248,0.01877605231961962
google/gemma-2-2b,mmlu_human_sexuality,5-shot,accuracy,0.5954198473282443,0.043046937953806645
google/gemma-2-2b,mmlu_professional_psychology,5-shot,accuracy,0.5359477124183006,0.020175488765484043
google/gemma-2-2b,mmlu_public_relations,5-shot,accuracy,0.6,0.0469237132203465
google/gemma-2-2b,mmlu_security_studies,5-shot,accuracy,0.5959183673469388,0.03141470802586589
google/gemma-2-2b,mmlu_sociology,5-shot,accuracy,0.7512437810945274,0.030567675938916718
google/gemma-2-2b,mmlu_us_foreign_policy,5-shot,accuracy,0.71,0.045604802157206845
google/gemma-2-2b,mmlu_abstract_algebra,5-shot,accuracy,0.26,0.04408440022768077
google/gemma-2-2b,mmlu_anatomy,5-shot,accuracy,0.5037037037037037,0.04319223625811331
google/gemma-2-2b,mmlu_astronomy,5-shot,accuracy,0.5657894736842105,0.04033565667848319
google/gemma-2-2b,mmlu_college_biology,5-shot,accuracy,0.6180555555555556,0.040629907841466674
google/gemma-2-2b,mmlu_college_chemistry,5-shot,accuracy,0.42,0.049604496374885836
google/gemma-2-2b,mmlu_college_computer_science,5-shot,accuracy,0.46,0.05009082659620333
google/gemma-2-2b,mmlu_college_mathematics,5-shot,accuracy,0.33,0.047258156262526045
google/gemma-2-2b,mmlu_college_physics,5-shot,accuracy,0.37254901960784315,0.048108401480826346
google/gemma-2-2b,mmlu_computer_security,5-shot,accuracy,0.65,0.0479372485441102
google/gemma-2-2b,mmlu_conceptual_physics,5-shot,accuracy,0.4765957446808511,0.03265019475033582
google/gemma-2-2b,mmlu_electrical_engineering,5-shot,accuracy,0.6137931034482759,0.04057324734419035
google/gemma-2-2b,mmlu_elementary_mathematics,5-shot,accuracy,0.3492063492063492,0.024552292209342665
google/gemma-2-2b,mmlu_high_school_biology,5-shot,accuracy,0.6709677419354839,0.026729499068349965
google/gemma-2-2b,mmlu_high_school_chemistry,5-shot,accuracy,0.4729064039408867,0.03512819077876106
google/gemma-2-2b,mmlu_high_school_computer_science,5-shot,accuracy,0.48,0.050211673156867795
google/gemma-2-2b,mmlu_high_school_mathematics,5-shot,accuracy,0.28888888888888886,0.027634907264178544
google/gemma-2-2b,mmlu_high_school_physics,5-shot,accuracy,0.4105960264900662,0.04016689594849927
google/gemma-2-2b,mmlu_high_school_statistics,5-shot,accuracy,0.46296296296296297,0.03400603625538272
google/gemma-2-2b,mmlu_machine_learning,5-shot,accuracy,0.3392857142857143,0.04493949068613538
Qwen/Qwen2.5-14B,mmlu_formal_logic,0-shot,accuracy,0.6111111111111112,0.04360314860077459
Qwen/Qwen2.5-14B,mmlu_high_school_european_history,0-shot,accuracy,0.8484848484848485,0.027998073798781668
Qwen/Qwen2.5-14B,mmlu_high_school_us_history,0-shot,accuracy,0.8970588235294118,0.02132833757080437
Qwen/Qwen2.5-14B,mmlu_high_school_world_history,0-shot,accuracy,0.9156118143459916,0.01809424711647331
Qwen/Qwen2.5-14B,mmlu_international_law,0-shot,accuracy,0.8760330578512396,0.030083098716035202
Qwen/Qwen2.5-14B,mmlu_jurisprudence,0-shot,accuracy,0.8333333333333334,0.036028141763926456
Qwen/Qwen2.5-14B,mmlu_logical_fallacies,0-shot,accuracy,0.8834355828220859,0.025212327210507087
Qwen/Qwen2.5-14B,mmlu_moral_disputes,0-shot,accuracy,0.8179190751445087,0.02077676110251298
Qwen/Qwen2.5-14B,mmlu_moral_scenarios,0-shot,accuracy,0.5094972067039106,0.01671948464334877
Qwen/Qwen2.5-14B,mmlu_philosophy,0-shot,accuracy,0.8102893890675241,0.022268196258783228
Qwen/Qwen2.5-14B,mmlu_prehistory,0-shot,accuracy,0.8765432098765432,0.018303868806891808
Qwen/Qwen2.5-14B,mmlu_professional_law,0-shot,accuracy,0.5867014341590613,0.012576779494860078
Qwen/Qwen2.5-14B,mmlu_world_religions,0-shot,accuracy,0.8830409356725146,0.024648068961366162
Qwen/Qwen2.5-14B,mmlu_business_ethics,0-shot,accuracy,0.81,0.03942772444036623
Qwen/Qwen2.5-14B,mmlu_clinical_knowledge,0-shot,accuracy,0.8528301886792453,0.02180412613479736
Qwen/Qwen2.5-14B,mmlu_college_medicine,0-shot,accuracy,0.7514450867052023,0.03295304696818318
Qwen/Qwen2.5-14B,mmlu_global_facts,0-shot,accuracy,0.56,0.049888765156985884
Qwen/Qwen2.5-14B,mmlu_human_aging,0-shot,accuracy,0.820627802690583,0.025749819569192794
Qwen/Qwen2.5-14B,mmlu_management,0-shot,accuracy,0.8932038834951457,0.030581088928331352
Qwen/Qwen2.5-14B,mmlu_marketing,0-shot,accuracy,0.9487179487179487,0.014450181176872726
Qwen/Qwen2.5-14B,mmlu_medical_genetics,0-shot,accuracy,0.9,0.030151134457776334
Qwen/Qwen2.5-14B,mmlu_miscellaneous,0-shot,accuracy,0.9029374201787995,0.010586474712018276
Qwen/Qwen2.5-14B,mmlu_nutrition,0-shot,accuracy,0.8366013071895425,0.021170623011213488
Qwen/Qwen2.5-14B,mmlu_professional_accounting,0-shot,accuracy,0.6382978723404256,0.028663820147199492
Qwen/Qwen2.5-14B,mmlu_professional_medicine,0-shot,accuracy,0.8492647058823529,0.021734235515652848
Qwen/Qwen2.5-14B,mmlu_virology,0-shot,accuracy,0.5662650602409639,0.03858158940685516
Qwen/Qwen2.5-14B,mmlu_econometrics,0-shot,accuracy,0.7105263157894737,0.04266339443159395
Qwen/Qwen2.5-14B,mmlu_high_school_geography,0-shot,accuracy,0.898989898989899,0.021469735576055332
Qwen/Qwen2.5-14B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9585492227979274,0.014385432857476442
Qwen/Qwen2.5-14B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.8384615384615385,0.018659703705332972
Qwen/Qwen2.5-14B,mmlu_high_school_microeconomics,0-shot,accuracy,0.9285714285714286,0.016728980212631635
Qwen/Qwen2.5-14B,mmlu_high_school_psychology,0-shot,accuracy,0.8990825688073395,0.012914673545364403
Qwen/Qwen2.5-14B,mmlu_human_sexuality,0-shot,accuracy,0.8854961832061069,0.027927473753597453
Qwen/Qwen2.5-14B,mmlu_professional_psychology,0-shot,accuracy,0.8316993464052288,0.015135803338693347
Qwen/Qwen2.5-14B,mmlu_public_relations,0-shot,accuracy,0.7090909090909091,0.04350271442923243
Qwen/Qwen2.5-14B,mmlu_security_studies,0-shot,accuracy,0.8163265306122449,0.02478907133200764
Qwen/Qwen2.5-14B,mmlu_sociology,0-shot,accuracy,0.8805970149253731,0.02292879327721974
Qwen/Qwen2.5-14B,mmlu_us_foreign_policy,0-shot,accuracy,0.93,0.025643239997624294
Qwen/Qwen2.5-14B,mmlu_abstract_algebra,0-shot,accuracy,0.63,0.048523658709391
Qwen/Qwen2.5-14B,mmlu_anatomy,0-shot,accuracy,0.7555555555555555,0.03712537833614866
Qwen/Qwen2.5-14B,mmlu_astronomy,0-shot,accuracy,0.9013157894736842,0.024270227737522725
Qwen/Qwen2.5-14B,mmlu_college_biology,0-shot,accuracy,0.8819444444444444,0.02698334650330937
Qwen/Qwen2.5-14B,mmlu_college_chemistry,0-shot,accuracy,0.55,0.04999999999999999
Qwen/Qwen2.5-14B,mmlu_college_computer_science,0-shot,accuracy,0.68,0.04688261722621504
Qwen/Qwen2.5-14B,mmlu_college_mathematics,0-shot,accuracy,0.6,0.049236596391733084
Qwen/Qwen2.5-14B,mmlu_college_physics,0-shot,accuracy,0.6078431372549019,0.04858083574266345
Qwen/Qwen2.5-14B,mmlu_computer_security,0-shot,accuracy,0.8,0.040201512610368445
Qwen/Qwen2.5-14B,mmlu_conceptual_physics,0-shot,accuracy,0.8382978723404255,0.024068505289695317
Qwen/Qwen2.5-14B,mmlu_electrical_engineering,0-shot,accuracy,0.7517241379310344,0.03600105692727773
Qwen/Qwen2.5-14B,mmlu_elementary_mathematics,0-shot,accuracy,0.8650793650793651,0.01759529244322067
Qwen/Qwen2.5-14B,mmlu_high_school_biology,0-shot,accuracy,0.9032258064516129,0.016818943416345197
Qwen/Qwen2.5-14B,mmlu_high_school_chemistry,0-shot,accuracy,0.7044334975369458,0.032104944337514575
Qwen/Qwen2.5-14B,mmlu_high_school_computer_science,0-shot,accuracy,0.89,0.03144660377352203
Qwen/Qwen2.5-14B,mmlu_high_school_mathematics,0-shot,accuracy,0.6259259259259259,0.02950286112895529
Qwen/Qwen2.5-14B,mmlu_high_school_physics,0-shot,accuracy,0.6887417218543046,0.03780445850526733
Qwen/Qwen2.5-14B,mmlu_high_school_statistics,0-shot,accuracy,0.7546296296296297,0.029346665094372944
Qwen/Qwen2.5-14B,mmlu_machine_learning,0-shot,accuracy,0.6607142857142857,0.04493949068613539
Qwen/Qwen2.5-14B,mmlu_formal_logic,5-shot,accuracy,0.6111111111111112,0.04360314860077459
Qwen/Qwen2.5-14B,mmlu_high_school_european_history,5-shot,accuracy,0.8606060606060606,0.027045948825865383
Qwen/Qwen2.5-14B,mmlu_high_school_us_history,5-shot,accuracy,0.9166666666666666,0.01939845213581391
Qwen/Qwen2.5-14B,mmlu_high_school_world_history,5-shot,accuracy,0.919831223628692,0.017676679991891642
Qwen/Qwen2.5-14B,mmlu_international_law,5-shot,accuracy,0.8677685950413223,0.030922788320445784
Qwen/Qwen2.5-14B,mmlu_jurisprudence,5-shot,accuracy,0.8518518518518519,0.03434300243631002
Qwen/Qwen2.5-14B,mmlu_logical_fallacies,5-shot,accuracy,0.8834355828220859,0.02521232721050708
Qwen/Qwen2.5-14B,mmlu_moral_disputes,5-shot,accuracy,0.8236994219653179,0.020516425672490714
Qwen/Qwen2.5-14B,mmlu_moral_scenarios,5-shot,accuracy,0.693854748603352,0.015414494487903208
Qwen/Qwen2.5-14B,mmlu_philosophy,5-shot,accuracy,0.8327974276527331,0.021193872528034965
Qwen/Qwen2.5-14B,mmlu_prehistory,5-shot,accuracy,0.8796296296296297,0.018105414094329676
Qwen/Qwen2.5-14B,mmlu_professional_law,5-shot,accuracy,0.6075619295958279,0.012471243669229096
Qwen/Qwen2.5-14B,mmlu_world_religions,5-shot,accuracy,0.9064327485380117,0.02233599323116327
Qwen/Qwen2.5-14B,mmlu_business_ethics,5-shot,accuracy,0.78,0.04163331998932262
Qwen/Qwen2.5-14B,mmlu_clinical_knowledge,5-shot,accuracy,0.8566037735849057,0.02157033497662494
Qwen/Qwen2.5-14B,mmlu_college_medicine,5-shot,accuracy,0.7861271676300579,0.031265112061730424
Qwen/Qwen2.5-14B,mmlu_global_facts,5-shot,accuracy,0.59,0.04943110704237102
Qwen/Qwen2.5-14B,mmlu_human_aging,5-shot,accuracy,0.7982062780269058,0.02693611191280227
Qwen/Qwen2.5-14B,mmlu_management,5-shot,accuracy,0.9029126213592233,0.029315962918813453
Qwen/Qwen2.5-14B,mmlu_marketing,5-shot,accuracy,0.9529914529914529,0.013866120058594842
Qwen/Qwen2.5-14B,mmlu_medical_genetics,5-shot,accuracy,0.91,0.028762349126466115
Qwen/Qwen2.5-14B,mmlu_miscellaneous,5-shot,accuracy,0.9029374201787995,0.01058647471201827
Qwen/Qwen2.5-14B,mmlu_nutrition,5-shot,accuracy,0.8366013071895425,0.021170623011213495
Qwen/Qwen2.5-14B,mmlu_professional_accounting,5-shot,accuracy,0.6666666666666666,0.028121636040639882
Qwen/Qwen2.5-14B,mmlu_professional_medicine,5-shot,accuracy,0.8602941176470589,0.02105940891901251
Qwen/Qwen2.5-14B,mmlu_virology,5-shot,accuracy,0.5783132530120482,0.038444531817709175
Qwen/Qwen2.5-14B,mmlu_econometrics,5-shot,accuracy,0.6842105263157895,0.04372748290278007
Qwen/Qwen2.5-14B,mmlu_high_school_geography,5-shot,accuracy,0.9141414141414141,0.019960225563172885
Qwen/Qwen2.5-14B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.9637305699481865,0.013492659751295138
Qwen/Qwen2.5-14B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.8461538461538461,0.018293347632158605
Qwen/Qwen2.5-14B,mmlu_high_school_microeconomics,5-shot,accuracy,0.9327731092436975,0.016266171559293906
Qwen/Qwen2.5-14B,mmlu_high_school_psychology,5-shot,accuracy,0.9192660550458716,0.011680172292862076
Qwen/Qwen2.5-14B,mmlu_human_sexuality,5-shot,accuracy,0.8931297709923665,0.027096548624883733
Qwen/Qwen2.5-14B,mmlu_professional_psychology,5-shot,accuracy,0.8202614379084967,0.01553374508338279
Qwen/Qwen2.5-14B,mmlu_public_relations,5-shot,accuracy,0.7090909090909091,0.04350271442923243
Qwen/Qwen2.5-14B,mmlu_security_studies,5-shot,accuracy,0.8571428571428571,0.022401787435256386
Qwen/Qwen2.5-14B,mmlu_sociology,5-shot,accuracy,0.9054726368159204,0.02068718695153408
Qwen/Qwen2.5-14B,mmlu_us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
Qwen/Qwen2.5-14B,mmlu_abstract_algebra,5-shot,accuracy,0.63,0.04852365870939099
Qwen/Qwen2.5-14B,mmlu_anatomy,5-shot,accuracy,0.7407407407407407,0.03785714465066652
Qwen/Qwen2.5-14B,mmlu_astronomy,5-shot,accuracy,0.9078947368421053,0.02353268597044349
Qwen/Qwen2.5-14B,mmlu_college_biology,5-shot,accuracy,0.9375,0.02024219611347799
Qwen/Qwen2.5-14B,mmlu_college_chemistry,5-shot,accuracy,0.54,0.05009082659620333
Qwen/Qwen2.5-14B,mmlu_college_computer_science,5-shot,accuracy,0.73,0.044619604333847394
Qwen/Qwen2.5-14B,mmlu_college_mathematics,5-shot,accuracy,0.57,0.04975698519562428
Qwen/Qwen2.5-14B,mmlu_college_physics,5-shot,accuracy,0.6274509803921569,0.048108401480826346
Qwen/Qwen2.5-14B,mmlu_computer_security,5-shot,accuracy,0.78,0.04163331998932263
Qwen/Qwen2.5-14B,mmlu_conceptual_physics,5-shot,accuracy,0.8595744680851064,0.022712077616627854
Qwen/Qwen2.5-14B,mmlu_electrical_engineering,5-shot,accuracy,0.7724137931034483,0.03493950380131184
Qwen/Qwen2.5-14B,mmlu_elementary_mathematics,5-shot,accuracy,0.8677248677248677,0.017448554290680433
Qwen/Qwen2.5-14B,mmlu_high_school_biology,5-shot,accuracy,0.9129032258064517,0.01604110074169668
Qwen/Qwen2.5-14B,mmlu_high_school_chemistry,5-shot,accuracy,0.7044334975369458,0.032104944337514575
Qwen/Qwen2.5-14B,mmlu_high_school_computer_science,5-shot,accuracy,0.9,0.030151134457776348
Qwen/Qwen2.5-14B,mmlu_high_school_mathematics,5-shot,accuracy,0.6333333333333333,0.029381620726465073
Qwen/Qwen2.5-14B,mmlu_high_school_physics,5-shot,accuracy,0.7019867549668874,0.037345356767871984
Qwen/Qwen2.5-14B,mmlu_high_school_statistics,5-shot,accuracy,0.8055555555555556,0.02699145450203672
Qwen/Qwen2.5-14B,mmlu_machine_learning,5-shot,accuracy,0.6964285714285714,0.04364226155841044
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,bleu_max,15.636998496774373,0.770969797685962
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,bleu_acc,0.26805385556915545,0.015506204722834562
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,bleu_diff,1.7129626489173806,0.6487939099951827
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rouge1_max,30.278710566807646,1.1416894539831812
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rouge1_acc,0.28886168910648713,0.01586634640138431
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rouge1_diff,2.7973257741936304,0.8511906293959041
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rouge2_max,22.74914704376283,1.0382162577447338
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rouge2_acc,0.2484700122399021,0.015127427096520683
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rouge2_diff,1.9984998759212294,0.9734137281943971
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rougeL_max,28.601576455841556,1.1068359673749302
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rougeL_acc,0.2766217870257038,0.015659605755326926
Qwen/Qwen2.5-14B,truthfulqa_gen,0-shot,rougeL_diff,2.381416202573925,0.8622782809935915
Qwen/Qwen2.5-14B,truthfulqa_mc1,0-shot,accuracy,0.40269277845777235,0.017168830935187215
Qwen/Qwen2.5-14B,truthfulqa_mc2,0-shot,accuracy,0.5843576792327699,0.014839658798012889
Qwen/Qwen2.5-14B,winogrande,5-shot,accuracy,0.7995264404104183,0.011251958281205064
Qwen/Qwen1.5-32B,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_2da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_3da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_4da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_5da,5-shot,accuracy,0.0,0.0
Qwen/Qwen1.5-32B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_2da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_3da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_4da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_5da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2.5-32B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
meta-llama/Meta-Llama-3-70B,arithmetic_1dc,5-shot,accuracy,0.0005,0.0005
meta-llama/Meta-Llama-3-70B,arithmetic_2da,5-shot,accuracy,0.0185,0.0030138707185866
meta-llama/Meta-Llama-3-70B,arithmetic_2dm,5-shot,accuracy,0.0075,0.0019296986470519
meta-llama/Meta-Llama-3-70B,arithmetic_2ds,5-shot,accuracy,0.3585,0.01072596840379
meta-llama/Meta-Llama-3-70B,arithmetic_3da,5-shot,accuracy,0.2205,0.0092726946694699
meta-llama/Meta-Llama-3-70B,arithmetic_3ds,5-shot,accuracy,0.4865,0.0111790590248168
meta-llama/Meta-Llama-3-70B,arithmetic_4da,5-shot,accuracy,0.003,0.0012232122154646
meta-llama/Meta-Llama-3-70B,arithmetic_4ds,5-shot,accuracy,0.026,0.0035592603398856
meta-llama/Meta-Llama-3-70B,arithmetic_5da,5-shot,accuracy,0.0105,0.002279796863071
meta-llama/Meta-Llama-3-70B,arithmetic_5ds,5-shot,accuracy,0.026,0.0035592603398856
meta-llama/Llama-2-70b-hf,arithmetic_1dc,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-70b-hf,arithmetic_2da,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-70b-hf,arithmetic_2dm,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-70b-hf,arithmetic_2ds,5-shot,accuracy,0.0005,0.0005000000000000152
meta-llama/Llama-2-70b-hf,arithmetic_3da,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-70b-hf,arithmetic_3ds,5-shot,accuracy,0.002,0.0009992493430694908
meta-llama/Llama-2-70b-hf,arithmetic_4da,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-70b-hf,arithmetic_4ds,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-70b-hf,arithmetic_5da,5-shot,accuracy,0.0,0.0
meta-llama/Llama-2-70b-hf,arithmetic_5ds,5-shot,accuracy,0.0005,0.0005000000000000071
mosaicml/mpt-30b,arithmetic_1dc,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_2da,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_2dm,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_2ds,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_3da,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_3ds,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_4da,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_4ds,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_5da,5-shot,accuracy,0.0,0.0
mosaicml/mpt-30b,arithmetic_5ds,5-shot,accuracy,0.0,0.0
AbacusResearch/Jallabi-34B,arithmetic_1dc,5-shot,accuracy,0.0165,0.0028491988289663403
AbacusResearch/Jallabi-34B,arithmetic_2da,5-shot,accuracy,0.001,0.0007069298939339529
AbacusResearch/Jallabi-34B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
AbacusResearch/Jallabi-34B,arithmetic_2ds,5-shot,accuracy,0.0105,0.002279796863070993
AbacusResearch/Jallabi-34B,arithmetic_3da,5-shot,accuracy,0.0,0.0
AbacusResearch/Jallabi-34B,arithmetic_3ds,5-shot,accuracy,0.003,0.001223212215464704
AbacusResearch/Jallabi-34B,arithmetic_4da,5-shot,accuracy,0.0,0.0
AbacusResearch/Jallabi-34B,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005000000000000071
AbacusResearch/Jallabi-34B,arithmetic_5da,5-shot,accuracy,0.0005,0.0005000000000000019
AbacusResearch/Jallabi-34B,arithmetic_5ds,5-shot,accuracy,0.0075,0.0019296986470519835
01-ai/Yi-34B,arithmetic_1dc,5-shot,accuracy,0.025,0.0034919331033682497
01-ai/Yi-34B,arithmetic_2da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B,arithmetic_2ds,5-shot,accuracy,0.0145,0.0026736583971427824
01-ai/Yi-34B,arithmetic_3da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B,arithmetic_3ds,5-shot,accuracy,0.0125,0.002484947178762671
01-ai/Yi-34B,arithmetic_4da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B,arithmetic_4ds,5-shot,accuracy,0.0095,0.0021696148539100423
01-ai/Yi-34B,arithmetic_5da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B,arithmetic_5ds,5-shot,accuracy,0.0205,0.003169368619886977
01-ai/Yi-34B-200K,arithmetic_1dc,5-shot,accuracy,0.0005,0.0005000000000000088
01-ai/Yi-34B-200K,arithmetic_2da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B-200K,arithmetic_2dm,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B-200K,arithmetic_2ds,5-shot,accuracy,0.001,0.0007069298939339415
01-ai/Yi-34B-200K,arithmetic_3da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B-200K,arithmetic_3ds,5-shot,accuracy,0.002,0.0009992493430694845
01-ai/Yi-34B-200K,arithmetic_4da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B-200K,arithmetic_4ds,5-shot,accuracy,0.001,0.0007069298939339522
01-ai/Yi-34B-200K,arithmetic_5da,5-shot,accuracy,0.0,0.0
01-ai/Yi-34B-200K,arithmetic_5ds,5-shot,accuracy,0.003,0.0012232122154647066
AbacusResearch/Jallabi-34B,lambada_openai,0-shot,perplexity,nan,nan
AbacusResearch/Jallabi-34B,lambada_openai,0-shot,accuracy,0.7129827285076654,0.0063023946289093935
AbacusResearch/Jallabi-34B,lambada_standard,0-shot,perplexity,nan,nan
AbacusResearch/Jallabi-34B,lambada_standard,0-shot,accuracy,0.7127886667960411,0.006303666845525233
meta-llama/Llama-2-70b-hf,lambada_openai,0-shot,perplexity,2.664624213997193,0.04555130991038834
meta-llama/Llama-2-70b-hf,lambada_openai,0-shot,accuracy,0.7983698816223559,0.00558974309090606
meta-llama/Llama-2-70b-hf,lambada_standard,0-shot,perplexity,2.969581955455061,0.0498688225320638
meta-llama/Llama-2-70b-hf,lambada_standard,0-shot,accuracy,0.7492722685814089,0.006038555858387702
Qwen/Qwen1.5-32B,lambada_openai,0-shot,perplexity,3.0627750535457143,0.059121684057378276
Qwen/Qwen1.5-32B,lambada_openai,0-shot,accuracy,0.7483019600232874,0.00604631029126968
Qwen/Qwen1.5-32B,lambada_standard,0-shot,perplexity,4.803247372250258,0.09942156285563382
Qwen/Qwen1.5-32B,lambada_standard,0-shot,accuracy,0.6010091209004463,0.006822351451474121
Qwen/Qwen2.5-32B,lambada_openai,0-shot,perplexity,3.016859375967635,0.05831569827706775
Qwen/Qwen2.5-32B,lambada_openai,0-shot,accuracy,0.7609159712788667,0.005942315486471666
Qwen/Qwen2.5-32B,lambada_standard,0-shot,perplexity,3.848256205913645,0.07663743982358202
Qwen/Qwen2.5-32B,lambada_standard,0-shot,accuracy,0.6951290510382302,0.006413613926848417
01-ai/Yi-34B-200K,lambada_openai,0-shot,perplexity,3.0188037033224346,0.05535929279917449
01-ai/Yi-34B-200K,lambada_openai,0-shot,accuracy,0.7306423442654765,0.0061805788826355705
01-ai/Yi-34B-200K,lambada_standard,0-shot,perplexity,3.293739860886253,0.061929708682067056
01-ai/Yi-34B-200K,lambada_standard,0-shot,accuracy,0.7376285658839511,0.006128994208430881
01-ai/Yi-34B,lambada_openai,0-shot,perplexity,3.02925704842076,0.054300141696497724
01-ai/Yi-34B,lambada_openai,0-shot,accuracy,0.7323888996700951,0.006167867157277622
01-ai/Yi-34B,lambada_standard,0-shot,perplexity,3.5203780347503506,0.06473572690599792
01-ai/Yi-34B,lambada_standard,0-shot,accuracy,0.6906656316708714,0.006439617662597977
mosaicml/mpt-30b,lambada_openai,0-shot,perplexity,3.05805590474418,0.0600107644203849
mosaicml/mpt-30b,lambada_openai,0-shot,accuracy,0.7343295167863381,0.006153599372822536
mosaicml/mpt-30b,lambada_standard,0-shot,perplexity,3.5769840685445167,0.0678698876833466
mosaicml/mpt-30b,lambada_standard,0-shot,accuracy,0.7054143217543178,0.006350969451144859
AbacusResearch/Jallabi-34B,mmlu_formal_logic,0-shot,accuracy,0.5634920634920635,0.04435932892851466
AbacusResearch/Jallabi-34B,mmlu_high_school_european_history,0-shot,accuracy,0.8606060606060606,0.027045948825865394
AbacusResearch/Jallabi-34B,mmlu_high_school_us_history,0-shot,accuracy,0.9313725490196079,0.017744453647073322
AbacusResearch/Jallabi-34B,mmlu_high_school_world_history,0-shot,accuracy,0.9282700421940928,0.016796989611119587
AbacusResearch/Jallabi-34B,mmlu_international_law,0-shot,accuracy,0.8429752066115702,0.0332124484254713
AbacusResearch/Jallabi-34B,mmlu_jurisprudence,0-shot,accuracy,0.8981481481481481,0.029239272675632717
AbacusResearch/Jallabi-34B,mmlu_logical_fallacies,0-shot,accuracy,0.852760736196319,0.027839915278339657
AbacusResearch/Jallabi-34B,mmlu_moral_disputes,0-shot,accuracy,0.7947976878612717,0.021742519835276294
AbacusResearch/Jallabi-34B,mmlu_moral_scenarios,0-shot,accuracy,0.5318435754189944,0.016688553415612213
AbacusResearch/Jallabi-34B,mmlu_philosophy,0-shot,accuracy,0.7781350482315113,0.023598858292863047
AbacusResearch/Jallabi-34B,mmlu_prehistory,0-shot,accuracy,0.845679012345679,0.020100830999851004
AbacusResearch/Jallabi-34B,mmlu_professional_law,0-shot,accuracy,0.5840938722294654,0.012588323850313606
AbacusResearch/Jallabi-34B,mmlu_world_religions,0-shot,accuracy,0.8830409356725146,0.024648068961366152
AbacusResearch/Jallabi-34B,mmlu_business_ethics,0-shot,accuracy,0.82,0.03861229196653693
AbacusResearch/Jallabi-34B,mmlu_clinical_knowledge,0-shot,accuracy,0.8188679245283019,0.0237029635267578
AbacusResearch/Jallabi-34B,mmlu_college_medicine,0-shot,accuracy,0.7167630057803468,0.034355680560478746
AbacusResearch/Jallabi-34B,mmlu_global_facts,0-shot,accuracy,0.54,0.05009082659620332
AbacusResearch/Jallabi-34B,mmlu_human_aging,0-shot,accuracy,0.7623318385650224,0.028568079464714277
AbacusResearch/Jallabi-34B,mmlu_management,0-shot,accuracy,0.8640776699029126,0.0339329572976101
AbacusResearch/Jallabi-34B,mmlu_marketing,0-shot,accuracy,0.9145299145299145,0.018315891685625852
AbacusResearch/Jallabi-34B,mmlu_medical_genetics,0-shot,accuracy,0.88,0.03265986323710905
AbacusResearch/Jallabi-34B,mmlu_miscellaneous,0-shot,accuracy,0.9144316730523627,0.010002965568647288
AbacusResearch/Jallabi-34B,mmlu_nutrition,0-shot,accuracy,0.826797385620915,0.021668400256514297
AbacusResearch/Jallabi-34B,mmlu_professional_accounting,0-shot,accuracy,0.6099290780141844,0.02909767559946393
AbacusResearch/Jallabi-34B,mmlu_professional_medicine,0-shot,accuracy,0.7977941176470589,0.02439819298665492
AbacusResearch/Jallabi-34B,mmlu_virology,0-shot,accuracy,0.5662650602409639,0.03858158940685517
AbacusResearch/Jallabi-34B,mmlu_econometrics,0-shot,accuracy,0.543859649122807,0.04685473041907789
AbacusResearch/Jallabi-34B,mmlu_high_school_geography,0-shot,accuracy,0.8939393939393939,0.021938047738853092
AbacusResearch/Jallabi-34B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9637305699481865,0.013492659751295122
AbacusResearch/Jallabi-34B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7846153846153846,0.020843034557462878
AbacusResearch/Jallabi-34B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8739495798319328,0.02155962312121391
AbacusResearch/Jallabi-34B,mmlu_high_school_psychology,0-shot,accuracy,0.9174311926605505,0.011800361363016588
AbacusResearch/Jallabi-34B,mmlu_human_sexuality,0-shot,accuracy,0.8854961832061069,0.027927473753597446
AbacusResearch/Jallabi-34B,mmlu_professional_psychology,0-shot,accuracy,0.7990196078431373,0.016211938889655574
AbacusResearch/Jallabi-34B,mmlu_public_relations,0-shot,accuracy,0.7272727272727273,0.04265792110940588
AbacusResearch/Jallabi-34B,mmlu_security_studies,0-shot,accuracy,0.8122448979591836,0.02500025603954621
AbacusResearch/Jallabi-34B,mmlu_sociology,0-shot,accuracy,0.8557213930348259,0.02484575321230605
AbacusResearch/Jallabi-34B,mmlu_us_foreign_policy,0-shot,accuracy,0.9,0.030151134457776334
AbacusResearch/Jallabi-34B,mmlu_abstract_algebra,0-shot,accuracy,0.5,0.050251890762960605
AbacusResearch/Jallabi-34B,mmlu_anatomy,0-shot,accuracy,0.7185185185185186,0.03885004245800254
AbacusResearch/Jallabi-34B,mmlu_astronomy,0-shot,accuracy,0.8881578947368421,0.025648341251693605
AbacusResearch/Jallabi-34B,mmlu_college_biology,0-shot,accuracy,0.8958333333333334,0.02554523921025691
AbacusResearch/Jallabi-34B,mmlu_college_chemistry,0-shot,accuracy,0.55,0.049999999999999996
AbacusResearch/Jallabi-34B,mmlu_college_computer_science,0-shot,accuracy,0.66,0.04760952285695237
AbacusResearch/Jallabi-34B,mmlu_college_mathematics,0-shot,accuracy,0.4,0.049236596391733084
AbacusResearch/Jallabi-34B,mmlu_college_physics,0-shot,accuracy,0.45098039215686275,0.04951218252396262
AbacusResearch/Jallabi-34B,mmlu_computer_security,0-shot,accuracy,0.82,0.03861229196653694
AbacusResearch/Jallabi-34B,mmlu_conceptual_physics,0-shot,accuracy,0.7787234042553192,0.027136349602424052
AbacusResearch/Jallabi-34B,mmlu_electrical_engineering,0-shot,accuracy,0.7241379310344828,0.03724563619774632
AbacusResearch/Jallabi-34B,mmlu_elementary_mathematics,0-shot,accuracy,0.6931216931216931,0.02375292871211215
AbacusResearch/Jallabi-34B,mmlu_high_school_biology,0-shot,accuracy,0.9,0.01706640371965727
AbacusResearch/Jallabi-34B,mmlu_high_school_chemistry,0-shot,accuracy,0.5812807881773399,0.03471192860518468
AbacusResearch/Jallabi-34B,mmlu_high_school_computer_science,0-shot,accuracy,0.77,0.042295258468165044
AbacusResearch/Jallabi-34B,mmlu_high_school_mathematics,0-shot,accuracy,0.4074074074074074,0.029958249250082118
AbacusResearch/Jallabi-34B,mmlu_high_school_physics,0-shot,accuracy,0.48344370860927155,0.04080244185628972
AbacusResearch/Jallabi-34B,mmlu_high_school_statistics,0-shot,accuracy,0.6435185185185185,0.032664783315272714
AbacusResearch/Jallabi-34B,mmlu_machine_learning,0-shot,accuracy,0.5803571428571429,0.04684099321077106
meta-llama/Llama-2-70b-hf,mmlu_formal_logic,0-shot,accuracy,0.4365079365079365,0.04435932892851466
meta-llama/Llama-2-70b-hf,mmlu_high_school_european_history,0-shot,accuracy,0.8,0.031234752377721175
meta-llama/Llama-2-70b-hf,mmlu_high_school_us_history,0-shot,accuracy,0.8676470588235294,0.023784297520918853
meta-llama/Llama-2-70b-hf,mmlu_high_school_world_history,0-shot,accuracy,0.8227848101265823,0.024856364184503228
meta-llama/Llama-2-70b-hf,mmlu_international_law,0-shot,accuracy,0.8347107438016529,0.03390780612972776
meta-llama/Llama-2-70b-hf,mmlu_jurisprudence,0-shot,accuracy,0.7407407407407407,0.042365112580946315
meta-llama/Llama-2-70b-hf,mmlu_logical_fallacies,0-shot,accuracy,0.7852760736196319,0.03226219377286774
meta-llama/Llama-2-70b-hf,mmlu_moral_disputes,0-shot,accuracy,0.7225433526011561,0.024105712607754307
meta-llama/Llama-2-70b-hf,mmlu_moral_scenarios,0-shot,accuracy,0.26145251396648045,0.014696599650364553
meta-llama/Llama-2-70b-hf,mmlu_philosophy,0-shot,accuracy,0.7717041800643086,0.023839303311398212
meta-llama/Llama-2-70b-hf,mmlu_prehistory,0-shot,accuracy,0.7839506172839507,0.022899162918445806
meta-llama/Llama-2-70b-hf,mmlu_professional_law,0-shot,accuracy,0.5443285528031291,0.012719949543032209
meta-llama/Llama-2-70b-hf,mmlu_world_religions,0-shot,accuracy,0.8245614035087719,0.029170885500727665
meta-llama/Llama-2-70b-hf,mmlu_business_ethics,0-shot,accuracy,0.68,0.04688261722621504
meta-llama/Llama-2-70b-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.6981132075471698,0.02825420034443865
meta-llama/Llama-2-70b-hf,mmlu_college_medicine,0-shot,accuracy,0.6705202312138728,0.03583901754736411
meta-llama/Llama-2-70b-hf,mmlu_global_facts,0-shot,accuracy,0.45,0.049999999999999996
meta-llama/Llama-2-70b-hf,mmlu_human_aging,0-shot,accuracy,0.7085201793721974,0.030500283176545854
meta-llama/Llama-2-70b-hf,mmlu_management,0-shot,accuracy,0.8446601941747572,0.03586594738573974
meta-llama/Llama-2-70b-hf,mmlu_marketing,0-shot,accuracy,0.8632478632478633,0.022509033937077778
meta-llama/Llama-2-70b-hf,mmlu_medical_genetics,0-shot,accuracy,0.71,0.04560480215720684
meta-llama/Llama-2-70b-hf,mmlu_miscellaneous,0-shot,accuracy,0.8212005108556832,0.013702643715368983
meta-llama/Llama-2-70b-hf,mmlu_nutrition,0-shot,accuracy,0.7156862745098039,0.02582916327275748
meta-llama/Llama-2-70b-hf,mmlu_professional_accounting,0-shot,accuracy,0.549645390070922,0.02968010556502904
meta-llama/Llama-2-70b-hf,mmlu_professional_medicine,0-shot,accuracy,0.7426470588235294,0.02655651947004151
meta-llama/Llama-2-70b-hf,mmlu_virology,0-shot,accuracy,0.4939759036144578,0.03892212195333045
meta-llama/Llama-2-70b-hf,mmlu_econometrics,0-shot,accuracy,0.3684210526315789,0.04537815354939391
meta-llama/Llama-2-70b-hf,mmlu_high_school_geography,0-shot,accuracy,0.803030303030303,0.028335609732463362
meta-llama/Llama-2-70b-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9067357512953368,0.0209868545932897
meta-llama/Llama-2-70b-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7153846153846154,0.0228783227997063
meta-llama/Llama-2-70b-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.7563025210084033,0.027886828078380544
meta-llama/Llama-2-70b-hf,mmlu_high_school_psychology,0-shot,accuracy,0.8458715596330275,0.015480826865374284
meta-llama/Llama-2-70b-hf,mmlu_human_sexuality,0-shot,accuracy,0.8091603053435115,0.03446513350752597
meta-llama/Llama-2-70b-hf,mmlu_professional_psychology,0-shot,accuracy,0.7369281045751634,0.01781267654232066
meta-llama/Llama-2-70b-hf,mmlu_public_relations,0-shot,accuracy,0.7363636363636363,0.04220224692971987
meta-llama/Llama-2-70b-hf,mmlu_security_studies,0-shot,accuracy,0.7346938775510204,0.02826388994378461
meta-llama/Llama-2-70b-hf,mmlu_sociology,0-shot,accuracy,0.8159203980099502,0.02740385941078687
meta-llama/Llama-2-70b-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.88,0.032659863237109066
meta-llama/Llama-2-70b-hf,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.04824181513244218
meta-llama/Llama-2-70b-hf,mmlu_anatomy,0-shot,accuracy,0.562962962962963,0.042849586397534015
meta-llama/Llama-2-70b-hf,mmlu_astronomy,0-shot,accuracy,0.756578947368421,0.034923496688842384
meta-llama/Llama-2-70b-hf,mmlu_college_biology,0-shot,accuracy,0.7638888888888888,0.03551446610810826
meta-llama/Llama-2-70b-hf,mmlu_college_chemistry,0-shot,accuracy,0.49,0.05024183937956911
meta-llama/Llama-2-70b-hf,mmlu_college_computer_science,0-shot,accuracy,0.49,0.05024183937956911
meta-llama/Llama-2-70b-hf,mmlu_college_mathematics,0-shot,accuracy,0.42,0.049604496374885836
meta-llama/Llama-2-70b-hf,mmlu_college_physics,0-shot,accuracy,0.3431372549019608,0.04724007352383888
meta-llama/Llama-2-70b-hf,mmlu_computer_security,0-shot,accuracy,0.74,0.0440844002276808
meta-llama/Llama-2-70b-hf,mmlu_conceptual_physics,0-shot,accuracy,0.6468085106382979,0.031245325202761923
meta-llama/Llama-2-70b-hf,mmlu_electrical_engineering,0-shot,accuracy,0.5724137931034483,0.04122737111370332
meta-llama/Llama-2-70b-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.3968253968253968,0.025197101074246494
meta-llama/Llama-2-70b-hf,mmlu_high_school_biology,0-shot,accuracy,0.7935483870967742,0.023025899617188723
meta-llama/Llama-2-70b-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.4827586206896552,0.035158955511657
meta-llama/Llama-2-70b-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.66,0.04760952285695238
meta-llama/Llama-2-70b-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.3592592592592593,0.02925290592725198
meta-llama/Llama-2-70b-hf,mmlu_high_school_physics,0-shot,accuracy,0.4304635761589404,0.04042809961395634
meta-llama/Llama-2-70b-hf,mmlu_high_school_statistics,0-shot,accuracy,0.5694444444444444,0.03376922151252336
meta-llama/Llama-2-70b-hf,mmlu_machine_learning,0-shot,accuracy,0.49107142857142855,0.04745033255489123
mosaicml/mpt-30b,mmlu_formal_logic,0-shot,accuracy,0.31746031746031744,0.04163453031302859
mosaicml/mpt-30b,mmlu_high_school_european_history,0-shot,accuracy,0.5696969696969697,0.03866225962879077
mosaicml/mpt-30b,mmlu_high_school_us_history,0-shot,accuracy,0.6274509803921569,0.033933885849584025
mosaicml/mpt-30b,mmlu_high_school_world_history,0-shot,accuracy,0.6582278481012658,0.030874537537553617
mosaicml/mpt-30b,mmlu_international_law,0-shot,accuracy,0.38016528925619836,0.04431324501968432
mosaicml/mpt-30b,mmlu_jurisprudence,0-shot,accuracy,0.49074074074074076,0.04832853553437055
mosaicml/mpt-30b,mmlu_logical_fallacies,0-shot,accuracy,0.5214723926380368,0.0392474687675113
mosaicml/mpt-30b,mmlu_moral_disputes,0-shot,accuracy,0.4653179190751445,0.0268542579282589
mosaicml/mpt-30b,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
mosaicml/mpt-30b,mmlu_philosophy,0-shot,accuracy,0.5659163987138264,0.0281502322445356
mosaicml/mpt-30b,mmlu_prehistory,0-shot,accuracy,0.49074074074074076,0.027815973433878014
mosaicml/mpt-30b,mmlu_professional_law,0-shot,accuracy,0.38722294654498046,0.012441155326854917
mosaicml/mpt-30b,mmlu_world_religions,0-shot,accuracy,0.672514619883041,0.035993357714560276
mosaicml/mpt-30b,mmlu_business_ethics,0-shot,accuracy,0.54,0.05009082659620332
mosaicml/mpt-30b,mmlu_clinical_knowledge,0-shot,accuracy,0.44528301886792454,0.030588052974270655
mosaicml/mpt-30b,mmlu_college_medicine,0-shot,accuracy,0.3930635838150289,0.03724249595817728
mosaicml/mpt-30b,mmlu_global_facts,0-shot,accuracy,0.31,0.04648231987117316
mosaicml/mpt-30b,mmlu_human_aging,0-shot,accuracy,0.5560538116591929,0.03334625674242728
mosaicml/mpt-30b,mmlu_management,0-shot,accuracy,0.5533980582524272,0.049224241534589326
mosaicml/mpt-30b,mmlu_marketing,0-shot,accuracy,0.7393162393162394,0.02876034895652341
mosaicml/mpt-30b,mmlu_medical_genetics,0-shot,accuracy,0.48,0.05021167315686779
mosaicml/mpt-30b,mmlu_miscellaneous,0-shot,accuracy,0.6360153256704981,0.017205684809032232
mosaicml/mpt-30b,mmlu_nutrition,0-shot,accuracy,0.4869281045751634,0.028620130800700246
mosaicml/mpt-30b,mmlu_professional_accounting,0-shot,accuracy,0.36524822695035464,0.028723863853281274
mosaicml/mpt-30b,mmlu_professional_medicine,0-shot,accuracy,0.3639705882352941,0.029227192460032025
mosaicml/mpt-30b,mmlu_virology,0-shot,accuracy,0.4939759036144578,0.03892212195333047
mosaicml/mpt-30b,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.0414243971948936
mosaicml/mpt-30b,mmlu_high_school_geography,0-shot,accuracy,0.5202020202020202,0.03559443565563918
mosaicml/mpt-30b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.5803108808290155,0.035615873276858834
mosaicml/mpt-30b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4230769230769231,0.02504919787604235
mosaicml/mpt-30b,mmlu_high_school_microeconomics,0-shot,accuracy,0.38235294117647056,0.03156663099215416
mosaicml/mpt-30b,mmlu_high_school_psychology,0-shot,accuracy,0.6073394495412844,0.020937505161201096
mosaicml/mpt-30b,mmlu_human_sexuality,0-shot,accuracy,0.5343511450381679,0.04374928560599738
mosaicml/mpt-30b,mmlu_professional_psychology,0-shot,accuracy,0.4395424836601307,0.020079420408087925
mosaicml/mpt-30b,mmlu_public_relations,0-shot,accuracy,0.5181818181818182,0.04785964010794916
mosaicml/mpt-30b,mmlu_security_studies,0-shot,accuracy,0.5469387755102041,0.03186785930004128
mosaicml/mpt-30b,mmlu_sociology,0-shot,accuracy,0.572139303482587,0.03498541988407795
mosaicml/mpt-30b,mmlu_us_foreign_policy,0-shot,accuracy,0.65,0.0479372485441102
mosaicml/mpt-30b,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.044084400227680794
mosaicml/mpt-30b,mmlu_anatomy,0-shot,accuracy,0.42962962962962964,0.04276349494376599
mosaicml/mpt-30b,mmlu_astronomy,0-shot,accuracy,0.40789473684210525,0.03999309712777471
mosaicml/mpt-30b,mmlu_college_biology,0-shot,accuracy,0.5,0.04181210050035455
mosaicml/mpt-30b,mmlu_college_chemistry,0-shot,accuracy,0.32,0.046882617226215034
mosaicml/mpt-30b,mmlu_college_computer_science,0-shot,accuracy,0.37,0.048523658709391
mosaicml/mpt-30b,mmlu_college_mathematics,0-shot,accuracy,0.31,0.04648231987117316
mosaicml/mpt-30b,mmlu_college_physics,0-shot,accuracy,0.3137254901960784,0.04617034827006717
mosaicml/mpt-30b,mmlu_computer_security,0-shot,accuracy,0.56,0.04988876515698589
mosaicml/mpt-30b,mmlu_conceptual_physics,0-shot,accuracy,0.39574468085106385,0.031967586978353627
mosaicml/mpt-30b,mmlu_electrical_engineering,0-shot,accuracy,0.503448275862069,0.041665675771015785
mosaicml/mpt-30b,mmlu_elementary_mathematics,0-shot,accuracy,0.2698412698412698,0.022860838309232072
mosaicml/mpt-30b,mmlu_high_school_biology,0-shot,accuracy,0.5193548387096775,0.028422687404312107
mosaicml/mpt-30b,mmlu_high_school_chemistry,0-shot,accuracy,0.3054187192118227,0.03240661565868408
mosaicml/mpt-30b,mmlu_high_school_computer_science,0-shot,accuracy,0.44,0.049888765156985884
mosaicml/mpt-30b,mmlu_high_school_mathematics,0-shot,accuracy,0.22962962962962963,0.02564410863926762
mosaicml/mpt-30b,mmlu_high_school_physics,0-shot,accuracy,0.271523178807947,0.03631329803969654
mosaicml/mpt-30b,mmlu_high_school_statistics,0-shot,accuracy,0.27314814814814814,0.030388051301678116
mosaicml/mpt-30b,mmlu_machine_learning,0-shot,accuracy,0.42857142857142855,0.04697113923010212
Qwen/Qwen1.5-32B,mmlu_formal_logic,0-shot,accuracy,0.5079365079365079,0.044715725362943486
Qwen/Qwen1.5-32B,mmlu_high_school_european_history,0-shot,accuracy,0.8484848484848485,0.027998073798781678
Qwen/Qwen1.5-32B,mmlu_high_school_us_history,0-shot,accuracy,0.9019607843137255,0.020871118455552087
Qwen/Qwen1.5-32B,mmlu_high_school_world_history,0-shot,accuracy,0.8818565400843882,0.021011052659878463
Qwen/Qwen1.5-32B,mmlu_international_law,0-shot,accuracy,0.8842975206611571,0.029199802455622814
Qwen/Qwen1.5-32B,mmlu_jurisprudence,0-shot,accuracy,0.8611111111111112,0.033432700628696244
Qwen/Qwen1.5-32B,mmlu_logical_fallacies,0-shot,accuracy,0.8343558282208589,0.029208296231259104
Qwen/Qwen1.5-32B,mmlu_moral_disputes,0-shot,accuracy,0.8005780346820809,0.021511900654252535
Qwen/Qwen1.5-32B,mmlu_moral_scenarios,0-shot,accuracy,0.3128491620111732,0.015506892594647262
Qwen/Qwen1.5-32B,mmlu_philosophy,0-shot,accuracy,0.7845659163987139,0.023350225475471442
Qwen/Qwen1.5-32B,mmlu_prehistory,0-shot,accuracy,0.8179012345679012,0.02147349183480836
Qwen/Qwen1.5-32B,mmlu_professional_law,0-shot,accuracy,0.5743155149934811,0.012628393551811942
Qwen/Qwen1.5-32B,mmlu_world_religions,0-shot,accuracy,0.8713450292397661,0.025679342723276908
Qwen/Qwen1.5-32B,mmlu_business_ethics,0-shot,accuracy,0.75,0.04351941398892446
Qwen/Qwen1.5-32B,mmlu_clinical_knowledge,0-shot,accuracy,0.7811320754716982,0.025447863825108614
Qwen/Qwen1.5-32B,mmlu_college_medicine,0-shot,accuracy,0.6994219653179191,0.0349610148119118
Qwen/Qwen1.5-32B,mmlu_global_facts,0-shot,accuracy,0.55,0.05
Qwen/Qwen1.5-32B,mmlu_human_aging,0-shot,accuracy,0.7443946188340808,0.029275891003969927
Qwen/Qwen1.5-32B,mmlu_management,0-shot,accuracy,0.8640776699029126,0.03393295729761011
Qwen/Qwen1.5-32B,mmlu_marketing,0-shot,accuracy,0.9273504273504274,0.017004368568132353
Qwen/Qwen1.5-32B,mmlu_medical_genetics,0-shot,accuracy,0.82,0.038612291966536955
Qwen/Qwen1.5-32B,mmlu_miscellaneous,0-shot,accuracy,0.876117496807152,0.011781017100950735
Qwen/Qwen1.5-32B,mmlu_nutrition,0-shot,accuracy,0.7941176470588235,0.0231527224394023
Qwen/Qwen1.5-32B,mmlu_professional_accounting,0-shot,accuracy,0.5531914893617021,0.029658235097666907
Qwen/Qwen1.5-32B,mmlu_professional_medicine,0-shot,accuracy,0.8198529411764706,0.02334516361654485
Qwen/Qwen1.5-32B,mmlu_virology,0-shot,accuracy,0.5662650602409639,0.03858158940685517
Qwen/Qwen1.5-32B,mmlu_econometrics,0-shot,accuracy,0.5263157894736842,0.046970851366478626
Qwen/Qwen1.5-32B,mmlu_high_school_geography,0-shot,accuracy,0.8686868686868687,0.02406315641682252
Qwen/Qwen1.5-32B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.927461139896373,0.018718998520678185
Qwen/Qwen1.5-32B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7923076923076923,0.020567539567246815
Qwen/Qwen1.5-32B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8277310924369747,0.02452866497130544
Qwen/Qwen1.5-32B,mmlu_high_school_psychology,0-shot,accuracy,0.9009174311926605,0.012809780081878918
Qwen/Qwen1.5-32B,mmlu_human_sexuality,0-shot,accuracy,0.8091603053435115,0.03446513350752599
Qwen/Qwen1.5-32B,mmlu_professional_psychology,0-shot,accuracy,0.7581699346405228,0.017322789207784326
Qwen/Qwen1.5-32B,mmlu_public_relations,0-shot,accuracy,0.6727272727272727,0.0449429086625209
Qwen/Qwen1.5-32B,mmlu_security_studies,0-shot,accuracy,0.7836734693877551,0.026358916334904045
Qwen/Qwen1.5-32B,mmlu_sociology,0-shot,accuracy,0.8756218905472637,0.023335401790166327
Qwen/Qwen1.5-32B,mmlu_us_foreign_policy,0-shot,accuracy,0.96,0.01969463855669323
Qwen/Qwen1.5-32B,mmlu_abstract_algebra,0-shot,accuracy,0.43,0.049756985195624284
Qwen/Qwen1.5-32B,mmlu_anatomy,0-shot,accuracy,0.6888888888888889,0.03999262876617722
Qwen/Qwen1.5-32B,mmlu_astronomy,0-shot,accuracy,0.7828947368421053,0.03355045304882924
Qwen/Qwen1.5-32B,mmlu_college_biology,0-shot,accuracy,0.8680555555555556,0.028300968382044423
Qwen/Qwen1.5-32B,mmlu_college_chemistry,0-shot,accuracy,0.49,0.05024183937956912
Qwen/Qwen1.5-32B,mmlu_college_computer_science,0-shot,accuracy,0.59,0.049431107042371025
Qwen/Qwen1.5-32B,mmlu_college_mathematics,0-shot,accuracy,0.47,0.050161355804659205
Qwen/Qwen1.5-32B,mmlu_college_physics,0-shot,accuracy,0.5784313725490197,0.04913595201274504
Qwen/Qwen1.5-32B,mmlu_computer_security,0-shot,accuracy,0.75,0.04351941398892446
Qwen/Qwen1.5-32B,mmlu_conceptual_physics,0-shot,accuracy,0.7106382978723405,0.02964400657700962
Qwen/Qwen1.5-32B,mmlu_electrical_engineering,0-shot,accuracy,0.7103448275862069,0.03780019230438013
Qwen/Qwen1.5-32B,mmlu_elementary_mathematics,0-shot,accuracy,0.6375661375661376,0.02475747390275206
Qwen/Qwen1.5-32B,mmlu_high_school_biology,0-shot,accuracy,0.896774193548387,0.01730838128103449
Qwen/Qwen1.5-32B,mmlu_high_school_chemistry,0-shot,accuracy,0.645320197044335,0.03366124489051449
Qwen/Qwen1.5-32B,mmlu_high_school_computer_science,0-shot,accuracy,0.84,0.03684529491774709
Qwen/Qwen1.5-32B,mmlu_high_school_mathematics,0-shot,accuracy,0.45185185185185184,0.030343862998512626
Qwen/Qwen1.5-32B,mmlu_high_school_physics,0-shot,accuracy,0.5364238410596026,0.04071636065944215
Qwen/Qwen1.5-32B,mmlu_high_school_statistics,0-shot,accuracy,0.7129629629629629,0.030851992993257013
Qwen/Qwen1.5-32B,mmlu_machine_learning,0-shot,accuracy,0.5446428571428571,0.04726835553719097
01-ai/Yi-34B-200K,mmlu_formal_logic,0-shot,accuracy,0.5158730158730159,0.044698818540726076
01-ai/Yi-34B-200K,mmlu_high_school_european_history,0-shot,accuracy,0.8484848484848485,0.027998073798781678
01-ai/Yi-34B-200K,mmlu_high_school_us_history,0-shot,accuracy,0.8970588235294118,0.021328337570804358
01-ai/Yi-34B-200K,mmlu_high_school_world_history,0-shot,accuracy,0.9071729957805907,0.018889750550956718
01-ai/Yi-34B-200K,mmlu_international_law,0-shot,accuracy,0.8347107438016529,0.033907806129727755
01-ai/Yi-34B-200K,mmlu_jurisprudence,0-shot,accuracy,0.9074074074074074,0.02802188803860939
01-ai/Yi-34B-200K,mmlu_logical_fallacies,0-shot,accuracy,0.8957055214723927,0.02401351731943907
01-ai/Yi-34B-200K,mmlu_moral_disputes,0-shot,accuracy,0.7861271676300579,0.02207570925175717
01-ai/Yi-34B-200K,mmlu_moral_scenarios,0-shot,accuracy,0.4558659217877095,0.01665722942458631
01-ai/Yi-34B-200K,mmlu_philosophy,0-shot,accuracy,0.797427652733119,0.02282731749105967
01-ai/Yi-34B-200K,mmlu_prehistory,0-shot,accuracy,0.8333333333333334,0.02073635840806001
01-ai/Yi-34B-200K,mmlu_professional_law,0-shot,accuracy,0.5645371577574967,0.01266341210124834
01-ai/Yi-34B-200K,mmlu_world_religions,0-shot,accuracy,0.8830409356725146,0.02464806896136616
01-ai/Yi-34B-200K,mmlu_business_ethics,0-shot,accuracy,0.78,0.04163331998932261
01-ai/Yi-34B-200K,mmlu_clinical_knowledge,0-shot,accuracy,0.8150943396226416,0.02389335183446432
01-ai/Yi-34B-200K,mmlu_college_medicine,0-shot,accuracy,0.7398843930635838,0.033450369167889925
01-ai/Yi-34B-200K,mmlu_global_facts,0-shot,accuracy,0.56,0.049888765156985884
01-ai/Yi-34B-200K,mmlu_human_aging,0-shot,accuracy,0.7937219730941704,0.027157150479563824
01-ai/Yi-34B-200K,mmlu_management,0-shot,accuracy,0.8543689320388349,0.0349260647662379
01-ai/Yi-34B-200K,mmlu_marketing,0-shot,accuracy,0.9102564102564102,0.01872430174194167
01-ai/Yi-34B-200K,mmlu_medical_genetics,0-shot,accuracy,0.83,0.03775251680686371
01-ai/Yi-34B-200K,mmlu_miscellaneous,0-shot,accuracy,0.8952745849297573,0.010949664098633398
01-ai/Yi-34B-200K,mmlu_nutrition,0-shot,accuracy,0.826797385620915,0.02166840025651429
01-ai/Yi-34B-200K,mmlu_professional_accounting,0-shot,accuracy,0.6099290780141844,0.02909767559946393
01-ai/Yi-34B-200K,mmlu_professional_medicine,0-shot,accuracy,0.7941176470588235,0.02456220431414231
01-ai/Yi-34B-200K,mmlu_virology,0-shot,accuracy,0.5662650602409639,0.03858158940685516
01-ai/Yi-34B-200K,mmlu_econometrics,0-shot,accuracy,0.6052631578947368,0.04598188057816542
01-ai/Yi-34B-200K,mmlu_high_school_geography,0-shot,accuracy,0.8737373737373737,0.02366435940288022
01-ai/Yi-34B-200K,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9637305699481865,0.013492659751295134
01-ai/Yi-34B-200K,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7615384615384615,0.02160629449464773
01-ai/Yi-34B-200K,mmlu_high_school_microeconomics,0-shot,accuracy,0.8319327731092437,0.02428910211569227
01-ai/Yi-34B-200K,mmlu_high_school_psychology,0-shot,accuracy,0.8899082568807339,0.013419939018681205
01-ai/Yi-34B-200K,mmlu_human_sexuality,0-shot,accuracy,0.8320610687022901,0.03278548537343139
01-ai/Yi-34B-200K,mmlu_professional_psychology,0-shot,accuracy,0.7973856209150327,0.01626105528374613
01-ai/Yi-34B-200K,mmlu_public_relations,0-shot,accuracy,0.6818181818181818,0.044612721759105085
01-ai/Yi-34B-200K,mmlu_security_studies,0-shot,accuracy,0.7183673469387755,0.028795185574291293
01-ai/Yi-34B-200K,mmlu_sociology,0-shot,accuracy,0.8805970149253731,0.02292879327721974
01-ai/Yi-34B-200K,mmlu_us_foreign_policy,0-shot,accuracy,0.92,0.0272659924344291
01-ai/Yi-34B-200K,mmlu_abstract_algebra,0-shot,accuracy,0.44,0.04988876515698589
01-ai/Yi-34B-200K,mmlu_anatomy,0-shot,accuracy,0.7185185185185186,0.03885004245800254
01-ai/Yi-34B-200K,mmlu_astronomy,0-shot,accuracy,0.8486842105263158,0.029162631596843996
01-ai/Yi-34B-200K,mmlu_college_biology,0-shot,accuracy,0.8680555555555556,0.028300968382044423
01-ai/Yi-34B-200K,mmlu_college_chemistry,0-shot,accuracy,0.51,0.05024183937956912
01-ai/Yi-34B-200K,mmlu_college_computer_science,0-shot,accuracy,0.6,0.04923659639173309
01-ai/Yi-34B-200K,mmlu_college_mathematics,0-shot,accuracy,0.42,0.049604496374885836
01-ai/Yi-34B-200K,mmlu_college_physics,0-shot,accuracy,0.47058823529411764,0.049665709039785295
01-ai/Yi-34B-200K,mmlu_computer_security,0-shot,accuracy,0.83,0.03775251680686371
01-ai/Yi-34B-200K,mmlu_conceptual_physics,0-shot,accuracy,0.7191489361702128,0.029379170464124818
01-ai/Yi-34B-200K,mmlu_electrical_engineering,0-shot,accuracy,0.7241379310344828,0.037245636197746325
01-ai/Yi-34B-200K,mmlu_elementary_mathematics,0-shot,accuracy,0.5555555555555556,0.025591857761382182
01-ai/Yi-34B-200K,mmlu_high_school_biology,0-shot,accuracy,0.8774193548387097,0.018656720991789395
01-ai/Yi-34B-200K,mmlu_high_school_chemistry,0-shot,accuracy,0.6206896551724138,0.034139638059062345
01-ai/Yi-34B-200K,mmlu_high_school_computer_science,0-shot,accuracy,0.79,0.040936018074033256
01-ai/Yi-34B-200K,mmlu_high_school_mathematics,0-shot,accuracy,0.3962962962962963,0.029822619458533997
01-ai/Yi-34B-200K,mmlu_high_school_physics,0-shot,accuracy,0.46357615894039733,0.04071636065944217
01-ai/Yi-34B-200K,mmlu_high_school_statistics,0-shot,accuracy,0.625,0.033016908987210894
01-ai/Yi-34B-200K,mmlu_machine_learning,0-shot,accuracy,0.5267857142857143,0.047389751192741546
Qwen/Qwen2.5-32B,mmlu_formal_logic,5-shot,accuracy,0.7301587301587301,0.039701582732351734
Qwen/Qwen2.5-32B,mmlu_high_school_european_history,5-shot,accuracy,0.8787878787878788,0.02548549837334323
Qwen/Qwen2.5-32B,mmlu_high_school_us_history,5-shot,accuracy,0.9362745098039216,0.01714392165552496
Qwen/Qwen2.5-32B,mmlu_high_school_world_history,5-shot,accuracy,0.9240506329113924,0.01724463325106569
Qwen/Qwen2.5-32B,mmlu_international_law,5-shot,accuracy,0.9338842975206612,0.022683403691723315
Qwen/Qwen2.5-32B,mmlu_jurisprudence,5-shot,accuracy,0.8425925925925926,0.03520703990517965
Qwen/Qwen2.5-32B,mmlu_logical_fallacies,5-shot,accuracy,0.8895705521472392,0.024624937788941318
Qwen/Qwen2.5-32B,mmlu_moral_disputes,5-shot,accuracy,0.8439306358381503,0.019539014685374036
Qwen/Qwen2.5-32B,mmlu_moral_scenarios,5-shot,accuracy,0.7553072625698324,0.014378169884098405
Qwen/Qwen2.5-32B,mmlu_philosophy,5-shot,accuracy,0.8778135048231511,0.018600811252967916
Qwen/Qwen2.5-32B,mmlu_prehistory,5-shot,accuracy,0.9166666666666666,0.015378494985372717
Qwen/Qwen2.5-32B,mmlu_professional_law,5-shot,accuracy,0.6492829204693612,0.012187773370741523
Qwen/Qwen2.5-32B,mmlu_world_religions,5-shot,accuracy,0.9122807017543859,0.021696383943889223
Qwen/Qwen2.5-32B,mmlu_business_ethics,5-shot,accuracy,0.8,0.04020151261036845
Qwen/Qwen2.5-32B,mmlu_clinical_knowledge,5-shot,accuracy,0.8754716981132076,0.020321376630696195
Qwen/Qwen2.5-32B,mmlu_college_medicine,5-shot,accuracy,0.8265895953757225,0.02886810787497064
Qwen/Qwen2.5-32B,mmlu_global_facts,5-shot,accuracy,0.71,0.04560480215720684
Qwen/Qwen2.5-32B,mmlu_human_aging,5-shot,accuracy,0.8116591928251121,0.02624113299640726
Qwen/Qwen2.5-32B,mmlu_management,5-shot,accuracy,0.883495145631068,0.03176683948640407
Qwen/Qwen2.5-32B,mmlu_marketing,5-shot,accuracy,0.9102564102564102,0.01872430174194167
Qwen/Qwen2.5-32B,mmlu_medical_genetics,5-shot,accuracy,0.91,0.028762349126466125
Qwen/Qwen2.5-32B,mmlu_miscellaneous,5-shot,accuracy,0.9169859514687101,0.009866287394639578
Qwen/Qwen2.5-32B,mmlu_nutrition,5-shot,accuracy,0.869281045751634,0.019301873624215267
Qwen/Qwen2.5-32B,mmlu_professional_accounting,5-shot,accuracy,0.6914893617021277,0.02755336616510137
Qwen/Qwen2.5-32B,mmlu_professional_medicine,5-shot,accuracy,0.9044117647058824,0.017860790568515642
Qwen/Qwen2.5-32B,mmlu_virology,5-shot,accuracy,0.572289156626506,0.038515976837185335
Qwen/Qwen2.5-32B,mmlu_econometrics,5-shot,accuracy,0.7807017543859649,0.03892431106518754
Qwen/Qwen2.5-32B,mmlu_high_school_geography,5-shot,accuracy,0.9444444444444444,0.016319950700767364
Qwen/Qwen2.5-32B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.9740932642487047,0.01146452335695318
Qwen/Qwen2.5-32B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.8794871794871795,0.016506560244881556
Qwen/Qwen2.5-32B,mmlu_high_school_microeconomics,5-shot,accuracy,0.9411764705882353,0.015283995352038384
Qwen/Qwen2.5-32B,mmlu_high_school_psychology,5-shot,accuracy,0.9339449541284404,0.010649131487858943
Qwen/Qwen2.5-32B,mmlu_human_sexuality,5-shot,accuracy,0.9236641221374046,0.02328893953617375
Qwen/Qwen2.5-32B,mmlu_professional_psychology,5-shot,accuracy,0.8725490196078431,0.013491054682536615
Qwen/Qwen2.5-32B,mmlu_public_relations,5-shot,accuracy,0.7636363636363637,0.040693063197213775
Qwen/Qwen2.5-32B,mmlu_security_studies,5-shot,accuracy,0.8571428571428571,0.022401787435256393
Qwen/Qwen2.5-32B,mmlu_sociology,5-shot,accuracy,0.9104477611940298,0.02019067053502794
Qwen/Qwen2.5-32B,mmlu_us_foreign_policy,5-shot,accuracy,0.97,0.01714466079977655
Qwen/Qwen2.5-32B,mmlu_abstract_algebra,5-shot,accuracy,0.72,0.04512608598542127
Qwen/Qwen2.5-32B,mmlu_anatomy,5-shot,accuracy,0.7851851851851852,0.035478541985608236
Qwen/Qwen2.5-32B,mmlu_astronomy,5-shot,accuracy,0.9342105263157895,0.02017493344016284
Qwen/Qwen2.5-32B,mmlu_college_biology,5-shot,accuracy,0.9444444444444444,0.01915507853243363
Qwen/Qwen2.5-32B,mmlu_college_chemistry,5-shot,accuracy,0.63,0.04852365870939099
Qwen/Qwen2.5-32B,mmlu_college_computer_science,5-shot,accuracy,0.79,0.04093601807403326
Qwen/Qwen2.5-32B,mmlu_college_mathematics,5-shot,accuracy,0.75,0.04351941398892446
Qwen/Qwen2.5-32B,mmlu_college_physics,5-shot,accuracy,0.7254901960784313,0.044405219061793254
Qwen/Qwen2.5-32B,mmlu_computer_security,5-shot,accuracy,0.85,0.0358870281282637
Qwen/Qwen2.5-32B,mmlu_conceptual_physics,5-shot,accuracy,0.9106382978723404,0.01864836423253194
Qwen/Qwen2.5-32B,mmlu_electrical_engineering,5-shot,accuracy,0.8206896551724138,0.031967664333731875
Qwen/Qwen2.5-32B,mmlu_elementary_mathematics,5-shot,accuracy,0.9047619047619048,0.015118260644547908
Qwen/Qwen2.5-32B,mmlu_high_school_biology,5-shot,accuracy,0.9483870967741935,0.012586144774300242
Qwen/Qwen2.5-32B,mmlu_high_school_chemistry,5-shot,accuracy,0.7733990147783252,0.02945486383529297
Qwen/Qwen2.5-32B,mmlu_high_school_computer_science,5-shot,accuracy,0.94,0.02386832565759418
Qwen/Qwen2.5-32B,mmlu_high_school_mathematics,5-shot,accuracy,0.6592592592592592,0.028897748741131137
Qwen/Qwen2.5-32B,mmlu_high_school_physics,5-shot,accuracy,0.7615894039735099,0.034791855725996586
Qwen/Qwen2.5-32B,mmlu_high_school_statistics,5-shot,accuracy,0.8425925925925926,0.024837173518242387
Qwen/Qwen2.5-32B,mmlu_machine_learning,5-shot,accuracy,0.7857142857142857,0.03894641120044792
HuggingFaceTB/SmolLM-360M,humaneval,0-shot,accuracy,0.012195121951219513,
EleutherAI/pythia-410m-deduped,humaneval,0-shot,accuracy,0.024390243902439025,
EleutherAI/pythia-70m,humaneval,0-shot,accuracy,0.0,
EleutherAI/pythia-410m,humaneval,0-shot,accuracy,0.018292682926829267,
facebook/xglm-2.9B,humaneval,0-shot,accuracy,0.0,
microsoft/phi-2,humaneval,0-shot,accuracy,0.4634146341463415,
meta-llama/Meta-Llama-3-8B,humaneval,0-shot,accuracy,0.34146341463414637,
openlm-research/open_llama_3b_v2,humaneval,0-shot,accuracy,0.0,
openlm-research/open_llama_7b,humaneval,0-shot,accuracy,0.0,
HuggingFaceTB/SmolLM-1.7B,humaneval,0-shot,accuracy,0.0,
HuggingFaceTB/SmolLM-135M,humaneval,0-shot,accuracy,0.0,
facebook/opt-350m,humaneval,0-shot,accuracy,0.0,
facebook/opt-2.7b,humaneval,0-shot,accuracy,0.0,
facebook/opt-125m,humaneval,0-shot,accuracy,0.0,
facebook/opt-6.7b,humaneval,0-shot,accuracy,0.006097560975609756,
facebook/xglm-564M,humaneval,0-shot,accuracy,0.0,
bigscience/bloom-7b1,humaneval,0-shot,accuracy,0.006097560975609756,
bigscience/bloom-1b7,humaneval,0-shot,accuracy,0.0,
Salesforce/codegen-6B-nl,humaneval,0-shot,accuracy,0.10975609756097561,
Salesforce/codegen-6B-multi,humaneval,0-shot,accuracy,0.1951219512195122,
Salesforce/codegen-350M-nl,humaneval,0-shot,accuracy,0.024390243902439025,
Salesforce/codegen-2B-multi,humaneval,0-shot,accuracy,0.1402439024390244,
Salesforce/codegen-350M-mono,humaneval,0-shot,accuracy,0.13414634146341464,
Salesforce/codegen-2B-mono,humaneval,0-shot,accuracy,0.25,
Salesforce/codegen-2B-nl,humaneval,0-shot,accuracy,0.07926829268292683,
Salesforce/codegen-350M-multi,humaneval,0-shot,accuracy,0.06707317073170732,
Salesforce/codegen-6B-mono,humaneval,0-shot,accuracy,0.29878048780487804,
microsoft/phi-1_5,humaneval,0-shot,accuracy,0.34146341463414637,
EleutherAI/gpt-neo-125m,humaneval,0-shot,accuracy,0.006097560975609756,
EleutherAI/gpt-neo-1.3B,humaneval,0-shot,accuracy,0.036585365853658534,
EleutherAI/pythia-160m,humaneval,0-shot,accuracy,0.006097560975609756,
EleutherAI/pythia-14m,humaneval,0-shot,accuracy,0.0,
EleutherAI/pythia-70m-deduped,humaneval,0-shot,accuracy,0.0,
EleutherAI/pythia-6.9b-deduped,humaneval,0-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,humaneval,0-shot,accuracy,0.07317073170731707,
EleutherAI/pythia-1b-deduped,humaneval,0-shot,accuracy,0.042682926829268296,
EleutherAI/gpt-j-6b,humaneval,0-shot,accuracy,0.12195121951219512,
EleutherAI/pythia-160m-deduped,humaneval,0-shot,accuracy,0.0,
EleutherAI/pythia-2.8b,humaneval,0-shot,accuracy,0.06097560975609756,
EleutherAI/pythia-2.8b-deduped,humaneval,0-shot,accuracy,0.07317073170731707,
EleutherAI/pythia-1.4b,humaneval,0-shot,accuracy,0.04878048780487805,
EleutherAI/pythia-1.4b-deduped,humaneval,0-shot,accuracy,0.042682926829268296,
EleutherAI/gpt-neo-2.7B,humaneval,0-shot,accuracy,0.07926829268292683,
EleutherAI/pythia-1b,humaneval,0-shot,accuracy,0.036585365853658534,
mosaicml/mpt-7b,humaneval,0-shot,accuracy,0.16463414634146342,
tiiuae/falcon-7b,humaneval,0-shot,accuracy,0.0,
NinedayWang/PolyCoder-2.7B,humaneval,0-shot,accuracy,0.06097560975609756,
Dampish/StellarX-4B-V0,humaneval,0-shot,accuracy,0.012195121951219513,
LLM360/CrystalCoder,humaneval,0-shot,accuracy,0.006097560975609756,
LLM360/Amber,humaneval,0-shot,accuracy,0.006097560975609756,
huggyllama/llama-7b,humaneval,0-shot,accuracy,0.0,
google/gemma-7b,humaneval,0-shot,accuracy,0.3048780487804878,
google/gemma-2-2b,humaneval,0-shot,accuracy,0.20121951219512196,
google/gemma-2-9b,humaneval,0-shot,accuracy,0.40853658536585363,
rinna/bilingual-gpt-neox-4b,humaneval,0-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,humaneval,0-shot,accuracy,0.08536585365853659,
cerebras/Cerebras-GPT-6.7B,humaneval,0-shot,accuracy,0.08536585365853659,
cerebras/Cerebras-GPT-1.3B,humaneval,0-shot,accuracy,0.024390243902439025,
01-ai/Yi-9B,humaneval,0-shot,accuracy,0.0,
Qwen/Qwen2-72B,arithmetic_1dc,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_2da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_2dm,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_2ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_3da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_3ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_4da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_4ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_5da,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,arithmetic_5ds,5-shot,accuracy,0.0,0.0
Qwen/Qwen2-72B,lambada_openai,0-shot,perplexity,2.680363233238911,0.04686156820468055
Qwen/Qwen2-72B,lambada_openai,0-shot,accuracy,0.7913836600038813,0.005660825573438171
Qwen/Qwen2-72B,lambada_standard,0-shot,perplexity,3.8312695671573356,0.06722698192497788
Qwen/Qwen2-72B,lambada_standard,0-shot,accuracy,0.6726178924898117,0.006537681377487905
Qwen/Qwen2-72B,mmlu_formal_logic,0-shot,accuracy,0.6984126984126984,0.04104947269903394
Qwen/Qwen2-72B,mmlu_high_school_european_history,0-shot,accuracy,0.8909090909090909,0.02434383813514564
Qwen/Qwen2-72B,mmlu_high_school_us_history,0-shot,accuracy,0.9411764705882353,0.01651440956102582
Qwen/Qwen2-72B,mmlu_high_school_world_history,0-shot,accuracy,0.9367088607594937,0.015849580400549967
Qwen/Qwen2-72B,mmlu_international_law,0-shot,accuracy,0.9008264462809917,0.027285246312758957
Qwen/Qwen2-72B,mmlu_jurisprudence,0-shot,accuracy,0.8796296296296297,0.031457038543062525
Qwen/Qwen2-72B,mmlu_logical_fallacies,0-shot,accuracy,0.901840490797546,0.0233761802310596
Qwen/Qwen2-72B,mmlu_moral_disputes,0-shot,accuracy,0.8728323699421965,0.017936766865149868
Qwen/Qwen2-72B,mmlu_moral_scenarios,0-shot,accuracy,0.7016759776536313,0.015301840045129272
Qwen/Qwen2-72B,mmlu_philosophy,0-shot,accuracy,0.8681672025723473,0.019214654265652387
Qwen/Qwen2-72B,mmlu_prehistory,0-shot,accuracy,0.8888888888888888,0.0174864327858807
Qwen/Qwen2-72B,mmlu_professional_law,0-shot,accuracy,0.6851368970013038,0.011862561755715938
Qwen/Qwen2-72B,mmlu_world_religions,0-shot,accuracy,0.8888888888888888,0.024103384202072864
Qwen/Qwen2-72B,mmlu_business_ethics,0-shot,accuracy,0.83,0.0377525168068637
Qwen/Qwen2-72B,mmlu_clinical_knowledge,0-shot,accuracy,0.879245283018868,0.020054189400972373
Qwen/Qwen2-72B,mmlu_college_medicine,0-shot,accuracy,0.8092485549132948,0.029957851329869323
Qwen/Qwen2-72B,mmlu_global_facts,0-shot,accuracy,0.62,0.048783173121456316
Qwen/Qwen2-72B,mmlu_human_aging,0-shot,accuracy,0.8430493273542601,0.024413587174907412
Qwen/Qwen2-72B,mmlu_management,0-shot,accuracy,0.9029126213592233,0.02931596291881347
Qwen/Qwen2-72B,mmlu_marketing,0-shot,accuracy,0.9572649572649573,0.013250436685245014
Qwen/Qwen2-72B,mmlu_medical_genetics,0-shot,accuracy,0.9,0.030151134457776348
Qwen/Qwen2-72B,mmlu_miscellaneous,0-shot,accuracy,0.9501915708812261,0.0077795348866793465
Qwen/Qwen2-72B,mmlu_nutrition,0-shot,accuracy,0.8888888888888888,0.017995029559531434
Qwen/Qwen2-72B,mmlu_professional_accounting,0-shot,accuracy,0.6950354609929078,0.027464708442022135
Qwen/Qwen2-72B,mmlu_professional_medicine,0-shot,accuracy,0.9007352941176471,0.018163995046407477
Qwen/Qwen2-72B,mmlu_virology,0-shot,accuracy,0.5843373493975904,0.038367221765980515
Qwen/Qwen2-72B,mmlu_econometrics,0-shot,accuracy,0.7192982456140351,0.042270544512322
Qwen/Qwen2-72B,mmlu_high_school_geography,0-shot,accuracy,0.9191919191919192,0.019417681889724536
Qwen/Qwen2-72B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9844559585492227,0.00892749271508433
Qwen/Qwen2-72B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.8846153846153846,0.01619855954727394
Qwen/Qwen2-72B,mmlu_high_school_microeconomics,0-shot,accuracy,0.9369747899159664,0.01578508522367093
Qwen/Qwen2-72B,mmlu_high_school_psychology,0-shot,accuracy,0.9302752293577982,0.01091942641184859
Qwen/Qwen2-72B,mmlu_human_sexuality,0-shot,accuracy,0.916030534351145,0.024324504024906605
Qwen/Qwen2-72B,mmlu_professional_psychology,0-shot,accuracy,0.8839869281045751,0.012955547759523033
Qwen/Qwen2-72B,mmlu_public_relations,0-shot,accuracy,0.7545454545454545,0.04122066502878284
Qwen/Qwen2-72B,mmlu_security_studies,0-shot,accuracy,0.8163265306122449,0.02478907133200765
Qwen/Qwen2-72B,mmlu_sociology,0-shot,accuracy,0.9353233830845771,0.017391600291491064
Qwen/Qwen2-72B,mmlu_us_foreign_policy,0-shot,accuracy,0.95,0.021904291355759036
Qwen/Qwen2-72B,mmlu_abstract_algebra,0-shot,accuracy,0.57,0.04975698519562428
Qwen/Qwen2-72B,mmlu_anatomy,0-shot,accuracy,0.8222222222222222,0.03302789859901719
Qwen/Qwen2-72B,mmlu_astronomy,0-shot,accuracy,0.9210526315789473,0.021944342818247958
Qwen/Qwen2-72B,mmlu_college_biology,0-shot,accuracy,0.9236111111111112,0.02221220393834591
Qwen/Qwen2-72B,mmlu_college_chemistry,0-shot,accuracy,0.55,0.05
Qwen/Qwen2-72B,mmlu_college_computer_science,0-shot,accuracy,0.77,0.04229525846816507
Qwen/Qwen2-72B,mmlu_college_mathematics,0-shot,accuracy,0.58,0.04960449637488583
Qwen/Qwen2-72B,mmlu_college_physics,0-shot,accuracy,0.6862745098039216,0.04617034827006717
Qwen/Qwen2-72B,mmlu_computer_security,0-shot,accuracy,0.83,0.0377525168068637
Qwen/Qwen2-72B,mmlu_conceptual_physics,0-shot,accuracy,0.8553191489361702,0.02299649754076049
Qwen/Qwen2-72B,mmlu_electrical_engineering,0-shot,accuracy,0.8206896551724138,0.03196766433373187
Qwen/Qwen2-72B,mmlu_elementary_mathematics,0-shot,accuracy,0.8862433862433863,0.0163528764804948
Qwen/Qwen2-72B,mmlu_high_school_biology,0-shot,accuracy,0.932258064516129,0.014296101903893356
Qwen/Qwen2-72B,mmlu_high_school_chemistry,0-shot,accuracy,0.7931034482758621,0.02850137816789395
Qwen/Qwen2-72B,mmlu_high_school_computer_science,0-shot,accuracy,0.91,0.028762349126466146
Qwen/Qwen2-72B,mmlu_high_school_mathematics,0-shot,accuracy,0.6962962962962963,0.028037929969114986
Qwen/Qwen2-72B,mmlu_high_school_physics,0-shot,accuracy,0.7019867549668874,0.03734535676787198
Qwen/Qwen2-72B,mmlu_high_school_statistics,0-shot,accuracy,0.7731481481481481,0.028561650102422263
Qwen/Qwen2-72B,mmlu_machine_learning,0-shot,accuracy,0.75,0.04109974682633932
meta-llama/Llama-2-13b-hf,humaneval,0-shot,accuracy,0.006097560975609756,
meta-llama/Llama-2-70b-hf,humaneval,0-shot,accuracy,0.31097560975609756,
facebook/xglm-7.5B,humaneval,0-shot,accuracy,0.0,
Salesforce/codegen-16B-nl,humaneval,0-shot,accuracy,0.14634146341463414,
Salesforce/codegen-16B-mono,humaneval,0-shot,accuracy,0.34146341463414637,
EleutherAI/pythia-12b-deduped,humaneval,0-shot,accuracy,0.0,
EleutherAI/pythia-12b,humaneval,0-shot,accuracy,0.0,
EleutherAI/gpt-neox-20b,humaneval,0-shot,accuracy,0.14634146341463414,
Qwen/Qwen2-72B,humaneval,0-shot,accuracy,0.6097560975609756,
Qwen/Qwen2.5-72B,humaneval,0-shot,accuracy,0.5548780487804879,
Qwen/Qwen2.5-3B,humaneval,0-shot,accuracy,0.3902439024390244,
Qwen/Qwen1.5-32B,humaneval,0-shot,accuracy,0.3902439024390244,
Qwen/Qwen1.5-14B,humaneval,0-shot,accuracy,0.39634146341463417,
Qwen/Qwen2.5-32B,humaneval,0-shot,accuracy,0.5060975609756098,
Qwen/Qwen2.5-14B,humaneval,0-shot,accuracy,0.5548780487804879,
cerebras/Cerebras-GPT-13B,humaneval,0-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,humaneval,0-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,humaneval,0-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,humaneval,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b,humaneval,0-shot,accuracy,0.0,
01-ai/Yi-34B,humaneval,0-shot,accuracy,0.2682926829268293,
01-ai/Yi-34B-200K,humaneval,0-shot,accuracy,0.0,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,arc_challenge,0-shot,accuracy,0.17064846416382254,0.010993654168413728
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,arc_challenge,0-shot,acc_norm,0.22440273037542663,0.012191404938603831
llama2_220M_nl_only_shuf-hf,arc_challenge,0-shot,accuracy,0.18344709897610922,0.011310170179554536
llama2_220M_nl_only_shuf-hf,arc_challenge,0-shot,acc_norm,0.21928327645051193,0.012091245787615716
llama2_220M_nl_0_code_100,arc_challenge,25-shot,accuracy,0.17320819112627986,0.011058694183280338
llama2_220M_nl_0_code_100,arc_challenge,25-shot,acc_norm,0.2150170648464164,0.012005717634133611
llama2_220M_nl_20_code_80,arc_challenge,25-shot,accuracy,0.1697952218430034,0.010971775157784192
llama2_220M_nl_20_code_80,arc_challenge,25-shot,acc_norm,0.22013651877133106,0.012108124883460983
llama2_220M_nl_40_code_60,arc_challenge,25-shot,accuracy,0.18771331058020477,0.011411001314155136
llama2_220M_nl_40_code_60,arc_challenge,25-shot,acc_norm,0.2295221843003413,0.012288926760890793
llama2_220M_nl_0_code_100,hellaswag,10-shot,accuracy,0.27126070503883687,0.004437016600956913
llama2_220M_nl_0_code_100,hellaswag,10-shot,acc_norm,0.2824138617805218,0.0044925357480975975
llama2_220M_nl_20_code_80,hellaswag,10-shot,accuracy,0.29506074487153955,0.004551379838156113
llama2_220M_nl_20_code_80,hellaswag,10-shot,acc_norm,0.3236407090221072,0.004669085411342224
llama2_220M_nl_40_code_60,hellaswag,10-shot,accuracy,0.3035251941844254,0.004588403419449654
llama2_220M_nl_40_code_60,hellaswag,10-shot,acc_norm,0.3377813184624577,0.004719870074967255
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,hellaswag,0-shot,accuracy,0.29665405297749453,0.004558491550673709
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,hellaswag,0-shot,acc_norm,0.3189603664608644,0.004651211311633821
llama2_220M_nl_only_shuf-hf,lambada_openai,0-shot,perplexity,55.07006208523438,2.083488647380621
llama2_220M_nl_only_shuf-hf,lambada_openai,0-shot,accuracy,0.2862410246458374,0.0062972915844510665
llama2_220M_nl_only_shuf-hf,lambada_standard,0-shot,perplexity,123.53927913323088,4.964971762238395
llama2_220M_nl_only_shuf-hf,lambada_standard,0-shot,accuracy,0.22724626431205122,0.005838227385425755
cerebras/Cerebras-GPT-13B,lambada_openai,0-shot,perplexity,456019.7984919974,62689.450778364764
cerebras/Cerebras-GPT-13B,lambada_openai,0-shot,accuracy,0.013778381525325054,0.0016240464072475181
cerebras/Cerebras-GPT-13B,lambada_standard,0-shot,perplexity,1844726.5125700482,257320.04390469077
cerebras/Cerebras-GPT-13B,lambada_standard,0-shot,accuracy,0.014748690083446536,0.001679431552210901
NinedayWang/PolyCoder-2.7B,lambada_openai,0-shot,perplexity,546.08861034253,24.948617105055877
NinedayWang/PolyCoder-2.7B,lambada_openai,0-shot,accuracy,0.11895982922569377,0.004510348441325372
NinedayWang/PolyCoder-2.7B,lambada_standard,0-shot,perplexity,1206.9252752016537,55.549736901369876
NinedayWang/PolyCoder-2.7B,lambada_standard,0-shot,accuracy,0.10906268193285465,0.004342839052621139
llama2_220M_nl_only_shuf-hf,mmlu_formal_logic,5-shot,accuracy,0.19047619047619047,0.035122074123020514
llama2_220M_nl_only_shuf-hf,mmlu_high_school_european_history,5-shot,accuracy,0.2727272727272727,0.03477691162163659
llama2_220M_nl_only_shuf-hf,mmlu_high_school_us_history,5-shot,accuracy,0.23529411764705882,0.029771775228145628
llama2_220M_nl_only_shuf-hf,mmlu_high_school_world_history,5-shot,accuracy,0.25738396624472576,0.028458820991460305
llama2_220M_nl_only_shuf-hf,mmlu_international_law,5-shot,accuracy,0.2644628099173554,0.040261875275912046
llama2_220M_nl_only_shuf-hf,mmlu_jurisprudence,5-shot,accuracy,0.24074074074074073,0.04133119440243838
llama2_220M_nl_only_shuf-hf,mmlu_logical_fallacies,5-shot,accuracy,0.2822085889570552,0.03536117886664743
llama2_220M_nl_only_shuf-hf,mmlu_moral_disputes,5-shot,accuracy,0.2514450867052023,0.023357365785874037
llama2_220M_nl_only_shuf-hf,mmlu_moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217887
llama2_220M_nl_only_shuf-hf,mmlu_philosophy,5-shot,accuracy,0.19614147909967847,0.02255244778047803
llama2_220M_nl_only_shuf-hf,mmlu_prehistory,5-shot,accuracy,0.21604938271604937,0.02289916291844581
llama2_220M_nl_only_shuf-hf,mmlu_professional_law,5-shot,accuracy,0.24445893089960888,0.010976425013113912
llama2_220M_nl_only_shuf-hf,mmlu_world_religions,5-shot,accuracy,0.23391812865497075,0.03246721765117825
llama2_220M_nl_only_shuf-hf,mmlu_business_ethics,5-shot,accuracy,0.27,0.04461960433384741
llama2_220M_nl_only_shuf-hf,mmlu_clinical_knowledge,5-shot,accuracy,0.2188679245283019,0.02544786382510861
llama2_220M_nl_only_shuf-hf,mmlu_college_medicine,5-shot,accuracy,0.2023121387283237,0.030631145539198823
llama2_220M_nl_only_shuf-hf,mmlu_global_facts,5-shot,accuracy,0.17,0.0377525168068637
llama2_220M_nl_only_shuf-hf,mmlu_human_aging,5-shot,accuracy,0.32286995515695066,0.03138147637575499
llama2_220M_nl_only_shuf-hf,mmlu_management,5-shot,accuracy,0.17475728155339806,0.03760178006026621
llama2_220M_nl_only_shuf-hf,mmlu_marketing,5-shot,accuracy,0.19230769230769232,0.02581923325648372
llama2_220M_nl_only_shuf-hf,mmlu_medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_only_shuf-hf,mmlu_miscellaneous,5-shot,accuracy,0.24648786717752236,0.015411308769686927
llama2_220M_nl_only_shuf-hf,mmlu_nutrition,5-shot,accuracy,0.20588235294117646,0.023152722439402307
llama2_220M_nl_only_shuf-hf,mmlu_professional_accounting,5-shot,accuracy,0.2198581560283688,0.024706141070705477
llama2_220M_nl_only_shuf-hf,mmlu_professional_medicine,5-shot,accuracy,0.4411764705882353,0.030161911930767102
llama2_220M_nl_only_shuf-hf,mmlu_virology,5-shot,accuracy,0.2710843373493976,0.034605799075530276
llama2_220M_nl_only_shuf-hf,mmlu_econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
llama2_220M_nl_only_shuf-hf,mmlu_high_school_geography,5-shot,accuracy,0.2828282828282828,0.03208779558786751
llama2_220M_nl_only_shuf-hf,mmlu_high_school_government_and_politics,5-shot,accuracy,0.18652849740932642,0.02811209121011746
llama2_220M_nl_only_shuf-hf,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2358974358974359,0.021525965407408726
llama2_220M_nl_only_shuf-hf,mmlu_high_school_microeconomics,5-shot,accuracy,0.22268907563025211,0.02702543349888238
llama2_220M_nl_only_shuf-hf,mmlu_high_school_psychology,5-shot,accuracy,0.1834862385321101,0.016595259710399306
llama2_220M_nl_only_shuf-hf,mmlu_human_sexuality,5-shot,accuracy,0.2824427480916031,0.03948406125768362
llama2_220M_nl_only_shuf-hf,mmlu_professional_psychology,5-shot,accuracy,0.25,0.01751781884501444
llama2_220M_nl_only_shuf-hf,mmlu_public_relations,5-shot,accuracy,0.22727272727272727,0.040139645540727756
llama2_220M_nl_only_shuf-hf,mmlu_security_studies,5-shot,accuracy,0.24081632653061225,0.027372942201788167
llama2_220M_nl_only_shuf-hf,mmlu_sociology,5-shot,accuracy,0.22885572139303484,0.02970528405677245
llama2_220M_nl_only_shuf-hf,mmlu_us_foreign_policy,5-shot,accuracy,0.34,0.04760952285695235
llama2_220M_nl_only_shuf-hf,mmlu_abstract_algebra,5-shot,accuracy,0.26,0.0440844002276808
llama2_220M_nl_only_shuf-hf,mmlu_anatomy,5-shot,accuracy,0.31851851851851853,0.04024778401977111
llama2_220M_nl_only_shuf-hf,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_only_shuf-hf,mmlu_college_biology,5-shot,accuracy,0.2222222222222222,0.03476590104304134
llama2_220M_nl_only_shuf-hf,mmlu_college_chemistry,5-shot,accuracy,0.2,0.040201512610368445
llama2_220M_nl_only_shuf-hf,mmlu_college_computer_science,5-shot,accuracy,0.15,0.035887028128263714
llama2_220M_nl_only_shuf-hf,mmlu_college_mathematics,5-shot,accuracy,0.23,0.04229525846816506
llama2_220M_nl_only_shuf-hf,mmlu_college_physics,5-shot,accuracy,0.27450980392156865,0.044405219061793275
llama2_220M_nl_only_shuf-hf,mmlu_computer_security,5-shot,accuracy,0.35,0.047937248544110196
llama2_220M_nl_only_shuf-hf,mmlu_conceptual_physics,5-shot,accuracy,0.2680851063829787,0.02895734278834235
llama2_220M_nl_only_shuf-hf,mmlu_electrical_engineering,5-shot,accuracy,0.20689655172413793,0.03375672449560553
llama2_220M_nl_only_shuf-hf,mmlu_elementary_mathematics,5-shot,accuracy,0.24867724867724866,0.022261817692400168
llama2_220M_nl_only_shuf-hf,mmlu_high_school_biology,5-shot,accuracy,0.2838709677419355,0.02564938106302926
llama2_220M_nl_only_shuf-hf,mmlu_high_school_chemistry,5-shot,accuracy,0.270935960591133,0.031270907132976984
llama2_220M_nl_only_shuf-hf,mmlu_high_school_computer_science,5-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_only_shuf-hf,mmlu_high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.02684205787383371
llama2_220M_nl_only_shuf-hf,mmlu_high_school_physics,5-shot,accuracy,0.2185430463576159,0.03374235550425694
llama2_220M_nl_only_shuf-hf,mmlu_high_school_statistics,5-shot,accuracy,0.3148148148148148,0.03167468706828979
llama2_220M_nl_only_shuf-hf,mmlu_machine_learning,5-shot,accuracy,0.29464285714285715,0.043270409325787317
NinedayWang/PolyCoder-2.7B,mmlu_formal_logic,5-shot,accuracy,0.18253968253968253,0.03455071019102147
NinedayWang/PolyCoder-2.7B,mmlu_high_school_european_history,5-shot,accuracy,0.2606060606060606,0.03427743175816524
NinedayWang/PolyCoder-2.7B,mmlu_high_school_us_history,5-shot,accuracy,0.2647058823529412,0.03096451792692341
NinedayWang/PolyCoder-2.7B,mmlu_high_school_world_history,5-shot,accuracy,0.2742616033755274,0.02904133351059802
NinedayWang/PolyCoder-2.7B,mmlu_international_law,5-shot,accuracy,0.3140495867768595,0.04236964753041019
NinedayWang/PolyCoder-2.7B,mmlu_jurisprudence,5-shot,accuracy,0.21296296296296297,0.03957835471980978
NinedayWang/PolyCoder-2.7B,mmlu_logical_fallacies,5-shot,accuracy,0.27607361963190186,0.0351238528370505
NinedayWang/PolyCoder-2.7B,mmlu_moral_disputes,5-shot,accuracy,0.23699421965317918,0.02289408248992599
NinedayWang/PolyCoder-2.7B,mmlu_moral_scenarios,5-shot,accuracy,0.24804469273743016,0.014444157808261436
NinedayWang/PolyCoder-2.7B,mmlu_philosophy,5-shot,accuracy,0.26366559485530544,0.02502553850053234
NinedayWang/PolyCoder-2.7B,mmlu_prehistory,5-shot,accuracy,0.2191358024691358,0.023016705640262196
NinedayWang/PolyCoder-2.7B,mmlu_professional_law,5-shot,accuracy,0.24771838331160365,0.011025499291443737
NinedayWang/PolyCoder-2.7B,mmlu_world_religions,5-shot,accuracy,0.3157894736842105,0.03565079670708312
NinedayWang/PolyCoder-2.7B,mmlu_business_ethics,5-shot,accuracy,0.22,0.041633319989322695
NinedayWang/PolyCoder-2.7B,mmlu_clinical_knowledge,5-shot,accuracy,0.2641509433962264,0.027134291628741702
NinedayWang/PolyCoder-2.7B,mmlu_college_medicine,5-shot,accuracy,0.23121387283236994,0.032147373020294696
NinedayWang/PolyCoder-2.7B,mmlu_global_facts,5-shot,accuracy,0.18,0.038612291966536955
NinedayWang/PolyCoder-2.7B,mmlu_human_aging,5-shot,accuracy,0.31390134529147984,0.031146796482972465
NinedayWang/PolyCoder-2.7B,mmlu_management,5-shot,accuracy,0.13592233009708737,0.033932957297610145
NinedayWang/PolyCoder-2.7B,mmlu_marketing,5-shot,accuracy,0.1794871794871795,0.02514093595033545
NinedayWang/PolyCoder-2.7B,mmlu_medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
NinedayWang/PolyCoder-2.7B,mmlu_miscellaneous,5-shot,accuracy,0.25287356321839083,0.015543377313719681
NinedayWang/PolyCoder-2.7B,mmlu_nutrition,5-shot,accuracy,0.23529411764705882,0.024288619466046105
NinedayWang/PolyCoder-2.7B,mmlu_professional_accounting,5-shot,accuracy,0.26595744680851063,0.026358065698880592
NinedayWang/PolyCoder-2.7B,mmlu_professional_medicine,5-shot,accuracy,0.4411764705882353,0.0301619119307671
NinedayWang/PolyCoder-2.7B,mmlu_virology,5-shot,accuracy,0.1746987951807229,0.029560326211256822
NinedayWang/PolyCoder-2.7B,mmlu_econometrics,5-shot,accuracy,0.23684210526315788,0.03999423879281335
NinedayWang/PolyCoder-2.7B,mmlu_high_school_geography,5-shot,accuracy,0.2727272727272727,0.03173071239071724
NinedayWang/PolyCoder-2.7B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.31088082901554404,0.03340361906276585
NinedayWang/PolyCoder-2.7B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.34615384615384615,0.024121125416941183
NinedayWang/PolyCoder-2.7B,mmlu_high_school_microeconomics,5-shot,accuracy,0.23529411764705882,0.027553614467863804
NinedayWang/PolyCoder-2.7B,mmlu_high_school_psychology,5-shot,accuracy,0.20733944954128442,0.017381415563608674
NinedayWang/PolyCoder-2.7B,mmlu_human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
NinedayWang/PolyCoder-2.7B,mmlu_professional_psychology,5-shot,accuracy,0.24509803921568626,0.017401816711427657
NinedayWang/PolyCoder-2.7B,mmlu_public_relations,5-shot,accuracy,0.22727272727272727,0.04013964554072773
NinedayWang/PolyCoder-2.7B,mmlu_security_studies,5-shot,accuracy,0.1836734693877551,0.024789071332007643
NinedayWang/PolyCoder-2.7B,mmlu_sociology,5-shot,accuracy,0.22388059701492538,0.029475250236017193
NinedayWang/PolyCoder-2.7B,mmlu_us_foreign_policy,5-shot,accuracy,0.24,0.042923469599092816
NinedayWang/PolyCoder-2.7B,mmlu_abstract_algebra,5-shot,accuracy,0.24,0.04292346959909283
NinedayWang/PolyCoder-2.7B,mmlu_anatomy,5-shot,accuracy,0.3333333333333333,0.04072314811876837
NinedayWang/PolyCoder-2.7B,mmlu_astronomy,5-shot,accuracy,0.19078947368421054,0.031975658210325
NinedayWang/PolyCoder-2.7B,mmlu_college_biology,5-shot,accuracy,0.2569444444444444,0.03653946969442099
NinedayWang/PolyCoder-2.7B,mmlu_college_chemistry,5-shot,accuracy,0.19,0.039427724440366234
NinedayWang/PolyCoder-2.7B,mmlu_college_computer_science,5-shot,accuracy,0.34,0.04760952285695236
NinedayWang/PolyCoder-2.7B,mmlu_college_mathematics,5-shot,accuracy,0.27,0.044619604333847394
NinedayWang/PolyCoder-2.7B,mmlu_college_physics,5-shot,accuracy,0.21568627450980393,0.040925639582376556
NinedayWang/PolyCoder-2.7B,mmlu_computer_security,5-shot,accuracy,0.28,0.04512608598542127
NinedayWang/PolyCoder-2.7B,mmlu_conceptual_physics,5-shot,accuracy,0.2170212765957447,0.02694748312149622
NinedayWang/PolyCoder-2.7B,mmlu_electrical_engineering,5-shot,accuracy,0.22758620689655173,0.03493950380131184
NinedayWang/PolyCoder-2.7B,mmlu_elementary_mathematics,5-shot,accuracy,0.2222222222222222,0.021411684393694196
NinedayWang/PolyCoder-2.7B,mmlu_high_school_biology,5-shot,accuracy,0.3032258064516129,0.02614868593067175
NinedayWang/PolyCoder-2.7B,mmlu_high_school_chemistry,5-shot,accuracy,0.270935960591133,0.031270907132976984
NinedayWang/PolyCoder-2.7B,mmlu_high_school_computer_science,5-shot,accuracy,0.3,0.04605661864718381
NinedayWang/PolyCoder-2.7B,mmlu_high_school_mathematics,5-shot,accuracy,0.25555555555555554,0.02659393910184407
NinedayWang/PolyCoder-2.7B,mmlu_high_school_physics,5-shot,accuracy,0.2781456953642384,0.03658603262763743
NinedayWang/PolyCoder-2.7B,mmlu_high_school_statistics,5-shot,accuracy,0.38425925925925924,0.03317354514310742
NinedayWang/PolyCoder-2.7B,mmlu_machine_learning,5-shot,accuracy,0.25892857142857145,0.04157751539865629
cerebras/Cerebras-GPT-13B,mmlu_formal_logic,0-shot,accuracy,0.23015873015873015,0.037649508797906045
cerebras/Cerebras-GPT-13B,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
cerebras/Cerebras-GPT-13B,mmlu_high_school_us_history,0-shot,accuracy,0.23039215686274508,0.029554292605695066
cerebras/Cerebras-GPT-13B,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
cerebras/Cerebras-GPT-13B,mmlu_international_law,0-shot,accuracy,0.2727272727272727,0.04065578140908705
cerebras/Cerebras-GPT-13B,mmlu_jurisprudence,0-shot,accuracy,0.3055555555555556,0.044531975073749834
cerebras/Cerebras-GPT-13B,mmlu_logical_fallacies,0-shot,accuracy,0.25766871165644173,0.03436150827846917
cerebras/Cerebras-GPT-13B,mmlu_moral_disputes,0-shot,accuracy,0.23699421965317918,0.022894082489925992
cerebras/Cerebras-GPT-13B,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
cerebras/Cerebras-GPT-13B,mmlu_philosophy,0-shot,accuracy,0.24758842443729903,0.024513879973621967
cerebras/Cerebras-GPT-13B,mmlu_prehistory,0-shot,accuracy,0.2808641975308642,0.025006469755799208
cerebras/Cerebras-GPT-13B,mmlu_professional_law,0-shot,accuracy,0.23989569752281617,0.010906282617981633
cerebras/Cerebras-GPT-13B,mmlu_world_religions,0-shot,accuracy,0.19298245614035087,0.030267457554898465
cerebras/Cerebras-GPT-13B,mmlu_business_ethics,0-shot,accuracy,0.27,0.04461960433384741
cerebras/Cerebras-GPT-13B,mmlu_clinical_knowledge,0-shot,accuracy,0.2792452830188679,0.027611163402399715
cerebras/Cerebras-GPT-13B,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.031265112061730445
cerebras/Cerebras-GPT-13B,mmlu_global_facts,0-shot,accuracy,0.28,0.04512608598542128
cerebras/Cerebras-GPT-13B,mmlu_human_aging,0-shot,accuracy,0.35874439461883406,0.032190792004199956
cerebras/Cerebras-GPT-13B,mmlu_management,0-shot,accuracy,0.2621359223300971,0.043546310772605956
cerebras/Cerebras-GPT-13B,mmlu_marketing,0-shot,accuracy,0.2564102564102564,0.02860595370200426
cerebras/Cerebras-GPT-13B,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
cerebras/Cerebras-GPT-13B,mmlu_miscellaneous,0-shot,accuracy,0.28735632183908044,0.0161824107306827
cerebras/Cerebras-GPT-13B,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.025261691219729477
cerebras/Cerebras-GPT-13B,mmlu_professional_accounting,0-shot,accuracy,0.26595744680851063,0.026358065698880592
cerebras/Cerebras-GPT-13B,mmlu_professional_medicine,0-shot,accuracy,0.2610294117647059,0.02667925227010313
cerebras/Cerebras-GPT-13B,mmlu_virology,0-shot,accuracy,0.3253012048192771,0.03647168523683228
cerebras/Cerebras-GPT-13B,mmlu_econometrics,0-shot,accuracy,0.2894736842105263,0.04266339443159394
cerebras/Cerebras-GPT-13B,mmlu_high_school_geography,0-shot,accuracy,0.20707070707070707,0.02886977846026705
cerebras/Cerebras-GPT-13B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.21243523316062177,0.02951928261681725
cerebras/Cerebras-GPT-13B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2230769230769231,0.021107730127244
cerebras/Cerebras-GPT-13B,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.027553614467863814
cerebras/Cerebras-GPT-13B,mmlu_high_school_psychology,0-shot,accuracy,0.23486238532110093,0.018175110510343585
cerebras/Cerebras-GPT-13B,mmlu_human_sexuality,0-shot,accuracy,0.25190839694656486,0.03807387116306085
cerebras/Cerebras-GPT-13B,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612379
cerebras/Cerebras-GPT-13B,mmlu_public_relations,0-shot,accuracy,0.35454545454545455,0.045820048415054174
cerebras/Cerebras-GPT-13B,mmlu_security_studies,0-shot,accuracy,0.17142857142857143,0.02412746346265015
cerebras/Cerebras-GPT-13B,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014645
cerebras/Cerebras-GPT-13B,mmlu_us_foreign_policy,0-shot,accuracy,0.21,0.04093601807403326
cerebras/Cerebras-GPT-13B,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.048241815132442176
cerebras/Cerebras-GPT-13B,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.03749850709174021
cerebras/Cerebras-GPT-13B,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123387
cerebras/Cerebras-GPT-13B,mmlu_college_biology,0-shot,accuracy,0.2152777777777778,0.03437079344106133
cerebras/Cerebras-GPT-13B,mmlu_college_chemistry,0-shot,accuracy,0.2,0.04020151261036844
cerebras/Cerebras-GPT-13B,mmlu_college_computer_science,0-shot,accuracy,0.15,0.0358870281282637
cerebras/Cerebras-GPT-13B,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816507
cerebras/Cerebras-GPT-13B,mmlu_college_physics,0-shot,accuracy,0.17647058823529413,0.037932811853078084
cerebras/Cerebras-GPT-13B,mmlu_computer_security,0-shot,accuracy,0.25,0.04351941398892446
cerebras/Cerebras-GPT-13B,mmlu_conceptual_physics,0-shot,accuracy,0.32340425531914896,0.03057944277361035
cerebras/Cerebras-GPT-13B,mmlu_electrical_engineering,0-shot,accuracy,0.22758620689655173,0.03493950380131184
cerebras/Cerebras-GPT-13B,mmlu_elementary_mathematics,0-shot,accuracy,0.24074074074074073,0.022019080012217897
cerebras/Cerebras-GPT-13B,mmlu_high_school_biology,0-shot,accuracy,0.25161290322580643,0.024685979286239963
cerebras/Cerebras-GPT-13B,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.030516530732694436
cerebras/Cerebras-GPT-13B,mmlu_high_school_computer_science,0-shot,accuracy,0.2,0.040201512610368445
cerebras/Cerebras-GPT-13B,mmlu_high_school_mathematics,0-shot,accuracy,0.2740740740740741,0.027195934804085626
cerebras/Cerebras-GPT-13B,mmlu_high_school_physics,0-shot,accuracy,0.2119205298013245,0.03336767086567977
cerebras/Cerebras-GPT-13B,mmlu_high_school_statistics,0-shot,accuracy,0.16666666666666666,0.02541642838876747
cerebras/Cerebras-GPT-13B,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.04287858751340456
llama2_220M_nl_0_code_100,mmlu_formal_logic,5-shot,accuracy,0.16666666666666666,0.03333333333333335
llama2_220M_nl_0_code_100,mmlu_high_school_european_history,5-shot,accuracy,0.28484848484848485,0.03524390844511783
llama2_220M_nl_0_code_100,mmlu_high_school_us_history,5-shot,accuracy,0.30392156862745096,0.03228210387037895
llama2_220M_nl_0_code_100,mmlu_high_school_world_history,5-shot,accuracy,0.2320675105485232,0.027479744550808524
llama2_220M_nl_0_code_100,mmlu_international_law,5-shot,accuracy,0.35537190082644626,0.04369236326573981
llama2_220M_nl_0_code_100,mmlu_jurisprudence,5-shot,accuracy,0.2037037037037037,0.03893542518824847
llama2_220M_nl_0_code_100,mmlu_logical_fallacies,5-shot,accuracy,0.25766871165644173,0.03436150827846917
llama2_220M_nl_0_code_100,mmlu_moral_disputes,5-shot,accuracy,0.20520231213872833,0.021742519835276294
llama2_220M_nl_0_code_100,mmlu_moral_scenarios,5-shot,accuracy,0.24692737430167597,0.014422292204808838
llama2_220M_nl_0_code_100,mmlu_philosophy,5-shot,accuracy,0.2733118971061093,0.02531176597542612
llama2_220M_nl_0_code_100,mmlu_prehistory,5-shot,accuracy,0.2345679012345679,0.023576881744005716
llama2_220M_nl_0_code_100,mmlu_professional_law,5-shot,accuracy,0.2392438070404172,0.010896123652676669
llama2_220M_nl_0_code_100,mmlu_world_religions,5-shot,accuracy,0.2631578947368421,0.03377310252209197
llama2_220M_nl_0_code_100,mmlu_business_ethics,5-shot,accuracy,0.15,0.03588702812826373
llama2_220M_nl_0_code_100,mmlu_clinical_knowledge,5-shot,accuracy,0.19622641509433963,0.024442388131100837
llama2_220M_nl_0_code_100,mmlu_college_medicine,5-shot,accuracy,0.2658959537572254,0.033687629322594316
llama2_220M_nl_0_code_100,mmlu_global_facts,5-shot,accuracy,0.15,0.03588702812826371
llama2_220M_nl_0_code_100,mmlu_human_aging,5-shot,accuracy,0.14349775784753363,0.02352937126961819
llama2_220M_nl_0_code_100,mmlu_management,5-shot,accuracy,0.2621359223300971,0.04354631077260595
llama2_220M_nl_0_code_100,mmlu_marketing,5-shot,accuracy,0.18803418803418803,0.02559819368665226
llama2_220M_nl_0_code_100,mmlu_medical_genetics,5-shot,accuracy,0.29,0.04560480215720684
llama2_220M_nl_0_code_100,mmlu_miscellaneous,5-shot,accuracy,0.24521072796934865,0.015384352284543946
llama2_220M_nl_0_code_100,mmlu_nutrition,5-shot,accuracy,0.2549019607843137,0.024954184324879905
llama2_220M_nl_0_code_100,mmlu_professional_accounting,5-shot,accuracy,0.2801418439716312,0.026789172351140242
llama2_220M_nl_0_code_100,mmlu_professional_medicine,5-shot,accuracy,0.44485294117647056,0.03018753206032938
llama2_220M_nl_0_code_100,mmlu_virology,5-shot,accuracy,0.21084337349397592,0.031755547866299194
llama2_220M_nl_0_code_100,mmlu_econometrics,5-shot,accuracy,0.21052631578947367,0.038351539543994194
llama2_220M_nl_0_code_100,mmlu_high_school_geography,5-shot,accuracy,0.3333333333333333,0.03358618145732523
llama2_220M_nl_0_code_100,mmlu_high_school_government_and_politics,5-shot,accuracy,0.3626943005181347,0.03469713791704372
llama2_220M_nl_0_code_100,mmlu_high_school_macroeconomics,5-shot,accuracy,0.358974358974359,0.024321738484602357
llama2_220M_nl_0_code_100,mmlu_high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.026841514322958945
llama2_220M_nl_0_code_100,mmlu_high_school_psychology,5-shot,accuracy,0.3192660550458716,0.019987829069750006
llama2_220M_nl_0_code_100,mmlu_human_sexuality,5-shot,accuracy,0.2900763358778626,0.03980066246467765
llama2_220M_nl_0_code_100,mmlu_professional_psychology,5-shot,accuracy,0.23202614379084968,0.01707737337785701
llama2_220M_nl_0_code_100,mmlu_public_relations,5-shot,accuracy,0.2727272727272727,0.04265792110940589
llama2_220M_nl_0_code_100,mmlu_security_studies,5-shot,accuracy,0.4,0.031362502409358936
llama2_220M_nl_0_code_100,mmlu_sociology,5-shot,accuracy,0.24875621890547264,0.030567675938916707
llama2_220M_nl_0_code_100,mmlu_us_foreign_policy,5-shot,accuracy,0.28,0.04512608598542129
llama2_220M_nl_0_code_100,mmlu_abstract_algebra,5-shot,accuracy,0.24,0.04292346959909284
llama2_220M_nl_0_code_100,mmlu_anatomy,5-shot,accuracy,0.34814814814814815,0.041153246103369526
llama2_220M_nl_0_code_100,mmlu_astronomy,5-shot,accuracy,0.15789473684210525,0.02967416752010144
llama2_220M_nl_0_code_100,mmlu_college_biology,5-shot,accuracy,0.24305555555555555,0.035868792800803406
llama2_220M_nl_0_code_100,mmlu_college_chemistry,5-shot,accuracy,0.23,0.04229525846816505
llama2_220M_nl_0_code_100,mmlu_college_computer_science,5-shot,accuracy,0.33,0.04725815626252604
llama2_220M_nl_0_code_100,mmlu_college_mathematics,5-shot,accuracy,0.24,0.042923469599092816
llama2_220M_nl_0_code_100,mmlu_college_physics,5-shot,accuracy,0.17647058823529413,0.03793281185307811
llama2_220M_nl_0_code_100,mmlu_computer_security,5-shot,accuracy,0.2,0.04020151261036845
llama2_220M_nl_0_code_100,mmlu_conceptual_physics,5-shot,accuracy,0.23829787234042554,0.027851252973889778
llama2_220M_nl_0_code_100,mmlu_electrical_engineering,5-shot,accuracy,0.22758620689655173,0.03493950380131183
llama2_220M_nl_0_code_100,mmlu_elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918417
llama2_220M_nl_0_code_100,mmlu_high_school_biology,5-shot,accuracy,0.2806451612903226,0.02556060472102289
llama2_220M_nl_0_code_100,mmlu_high_school_chemistry,5-shot,accuracy,0.27586206896551724,0.031447125816782426
llama2_220M_nl_0_code_100,mmlu_high_school_computer_science,5-shot,accuracy,0.28,0.045126085985421276
llama2_220M_nl_0_code_100,mmlu_high_school_mathematics,5-shot,accuracy,0.2814814814814815,0.02742001935094528
llama2_220M_nl_0_code_100,mmlu_high_school_physics,5-shot,accuracy,0.23178807947019867,0.034454062719870546
llama2_220M_nl_0_code_100,mmlu_high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
llama2_220M_nl_0_code_100,mmlu_machine_learning,5-shot,accuracy,0.25,0.04109974682633932
llama2_220M_nl_0_code_100,mmlu_formal_logic,0-shot,accuracy,0.2777777777777778,0.04006168083848876
llama2_220M_nl_0_code_100,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
llama2_220M_nl_0_code_100,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
llama2_220M_nl_0_code_100,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
llama2_220M_nl_0_code_100,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794088
llama2_220M_nl_0_code_100,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
llama2_220M_nl_0_code_100,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
llama2_220M_nl_0_code_100,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
llama2_220M_nl_0_code_100,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
llama2_220M_nl_0_code_100,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
llama2_220M_nl_0_code_100,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
llama2_220M_nl_0_code_100,mmlu_professional_law,0-shot,accuracy,0.24641460234680573,0.011005971399927244
llama2_220M_nl_0_code_100,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
llama2_220M_nl_0_code_100,mmlu_business_ethics,0-shot,accuracy,0.31,0.04648231987117316
llama2_220M_nl_0_code_100,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891377
llama2_220M_nl_0_code_100,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
llama2_220M_nl_0_code_100,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
llama2_220M_nl_0_code_100,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
llama2_220M_nl_0_code_100,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
llama2_220M_nl_0_code_100,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
llama2_220M_nl_0_code_100,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_0_code_100,mmlu_miscellaneous,0-shot,accuracy,0.23627075351213284,0.01519047371703751
llama2_220M_nl_0_code_100,mmlu_nutrition,0-shot,accuracy,0.22549019607843138,0.023929155517351284
llama2_220M_nl_0_code_100,mmlu_professional_accounting,0-shot,accuracy,0.23049645390070922,0.025123739226872402
llama2_220M_nl_0_code_100,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
llama2_220M_nl_0_code_100,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
llama2_220M_nl_0_code_100,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
llama2_220M_nl_0_code_100,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
llama2_220M_nl_0_code_100,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
llama2_220M_nl_0_code_100,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
llama2_220M_nl_0_code_100,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
llama2_220M_nl_0_code_100,mmlu_high_school_psychology,0-shot,accuracy,0.1908256880733945,0.01684767640009109
llama2_220M_nl_0_code_100,mmlu_human_sexuality,0-shot,accuracy,0.26717557251908397,0.038808483010823965
llama2_220M_nl_0_code_100,mmlu_professional_psychology,0-shot,accuracy,0.25163398692810457,0.017555818091322246
llama2_220M_nl_0_code_100,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
llama2_220M_nl_0_code_100,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
llama2_220M_nl_0_code_100,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
llama2_220M_nl_0_code_100,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
llama2_220M_nl_0_code_100,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
llama2_220M_nl_0_code_100,mmlu_anatomy,0-shot,accuracy,0.1925925925925926,0.03406542058502652
llama2_220M_nl_0_code_100,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_0_code_100,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
llama2_220M_nl_0_code_100,mmlu_college_chemistry,0-shot,accuracy,0.19,0.03942772444036623
llama2_220M_nl_0_code_100,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
llama2_220M_nl_0_code_100,mmlu_college_mathematics,0-shot,accuracy,0.22,0.0416333199893227
llama2_220M_nl_0_code_100,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
llama2_220M_nl_0_code_100,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
llama2_220M_nl_0_code_100,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
llama2_220M_nl_0_code_100,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
llama2_220M_nl_0_code_100,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
llama2_220M_nl_0_code_100,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.021732540689329265
llama2_220M_nl_0_code_100,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.025308904539380624
llama2_220M_nl_0_code_100,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
llama2_220M_nl_0_code_100,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
llama2_220M_nl_0_code_100,mmlu_high_school_physics,0-shot,accuracy,0.19205298013245034,0.03216298420593613
llama2_220M_nl_0_code_100,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
llama2_220M_nl_0_code_100,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
llama2_220M_nl_40_code_60,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
llama2_220M_nl_40_code_60,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
llama2_220M_nl_40_code_60,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
llama2_220M_nl_40_code_60,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
llama2_220M_nl_40_code_60,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794088
llama2_220M_nl_40_code_60,mmlu_jurisprudence,0-shot,accuracy,0.26851851851851855,0.04284467968052192
llama2_220M_nl_40_code_60,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
llama2_220M_nl_40_code_60,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
llama2_220M_nl_40_code_60,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
llama2_220M_nl_40_code_60,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
llama2_220M_nl_40_code_60,mmlu_prehistory,0-shot,accuracy,0.21296296296296297,0.022779719088733396
llama2_220M_nl_40_code_60,mmlu_professional_law,0-shot,accuracy,0.24641460234680573,0.011005971399927246
llama2_220M_nl_40_code_60,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.03565079670708313
llama2_220M_nl_40_code_60,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_40_code_60,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891373
llama2_220M_nl_40_code_60,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.03126511206173043
llama2_220M_nl_40_code_60,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
llama2_220M_nl_40_code_60,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.031146796482972465
llama2_220M_nl_40_code_60,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
llama2_220M_nl_40_code_60,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
llama2_220M_nl_40_code_60,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_40_code_60,mmlu_miscellaneous,0-shot,accuracy,0.23371647509578544,0.015133383278988846
llama2_220M_nl_40_code_60,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.023805186524888142
llama2_220M_nl_40_code_60,mmlu_professional_accounting,0-shot,accuracy,0.22695035460992907,0.024987106365642976
llama2_220M_nl_40_code_60,mmlu_professional_medicine,0-shot,accuracy,0.1801470588235294,0.023345163616544866
llama2_220M_nl_40_code_60,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
llama2_220M_nl_40_code_60,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
llama2_220M_nl_40_code_60,mmlu_high_school_geography,0-shot,accuracy,0.18181818181818182,0.027479603010538808
llama2_220M_nl_40_code_60,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
llama2_220M_nl_40_code_60,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
llama2_220M_nl_40_code_60,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
llama2_220M_nl_40_code_60,mmlu_high_school_psychology,0-shot,accuracy,0.1908256880733945,0.01684767640009109
llama2_220M_nl_40_code_60,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.03915345408847835
llama2_220M_nl_40_code_60,mmlu_professional_psychology,0-shot,accuracy,0.25326797385620914,0.017593486895366835
llama2_220M_nl_40_code_60,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
llama2_220M_nl_40_code_60,mmlu_security_studies,0-shot,accuracy,0.19183673469387755,0.025206963154225395
llama2_220M_nl_40_code_60,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
llama2_220M_nl_40_code_60,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
llama2_220M_nl_40_code_60,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_40_code_60,mmlu_anatomy,0-shot,accuracy,0.2,0.03455473702325437
llama2_220M_nl_40_code_60,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_40_code_60,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.03685651095897532
llama2_220M_nl_40_code_60,mmlu_college_chemistry,0-shot,accuracy,0.19,0.03942772444036623
llama2_220M_nl_40_code_60,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
llama2_220M_nl_40_code_60,mmlu_college_mathematics,0-shot,accuracy,0.22,0.0416333199893227
llama2_220M_nl_40_code_60,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
llama2_220M_nl_40_code_60,mmlu_computer_security,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_40_code_60,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
llama2_220M_nl_40_code_60,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
llama2_220M_nl_40_code_60,mmlu_elementary_mathematics,0-shot,accuracy,0.21164021164021163,0.021037331505262886
llama2_220M_nl_40_code_60,mmlu_high_school_biology,0-shot,accuracy,0.18064516129032257,0.021886178567172555
llama2_220M_nl_40_code_60,mmlu_high_school_chemistry,0-shot,accuracy,0.15763546798029557,0.025639014131172404
llama2_220M_nl_40_code_60,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
llama2_220M_nl_40_code_60,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
llama2_220M_nl_40_code_60,mmlu_high_school_physics,0-shot,accuracy,0.19205298013245034,0.032162984205936135
llama2_220M_nl_40_code_60,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
llama2_220M_nl_40_code_60,mmlu_machine_learning,0-shot,accuracy,0.32142857142857145,0.044328040552915185
llama2_220M_nl_40_code_60,mmlu_formal_logic,5-shot,accuracy,0.18253968253968253,0.03455071019102146
llama2_220M_nl_40_code_60,mmlu_high_school_european_history,5-shot,accuracy,0.23636363636363636,0.03317505930009181
llama2_220M_nl_40_code_60,mmlu_high_school_us_history,5-shot,accuracy,0.28431372549019607,0.03166009679399812
llama2_220M_nl_40_code_60,mmlu_high_school_world_history,5-shot,accuracy,0.27848101265822783,0.029178682304842562
llama2_220M_nl_40_code_60,mmlu_international_law,5-shot,accuracy,0.2396694214876033,0.03896878985070417
llama2_220M_nl_40_code_60,mmlu_jurisprudence,5-shot,accuracy,0.26851851851851855,0.04284467968052192
llama2_220M_nl_40_code_60,mmlu_logical_fallacies,5-shot,accuracy,0.26380368098159507,0.03462419931615623
llama2_220M_nl_40_code_60,mmlu_moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
llama2_220M_nl_40_code_60,mmlu_moral_scenarios,5-shot,accuracy,0.24022346368715083,0.014288343803925319
llama2_220M_nl_40_code_60,mmlu_philosophy,5-shot,accuracy,0.19292604501607716,0.022411516780911363
llama2_220M_nl_40_code_60,mmlu_prehistory,5-shot,accuracy,0.2623456790123457,0.024477222856135114
llama2_220M_nl_40_code_60,mmlu_professional_law,5-shot,accuracy,0.24511082138200782,0.010986307870045505
llama2_220M_nl_40_code_60,mmlu_world_religions,5-shot,accuracy,0.27485380116959063,0.03424042924691584
llama2_220M_nl_40_code_60,mmlu_business_ethics,5-shot,accuracy,0.34,0.04760952285695235
llama2_220M_nl_40_code_60,mmlu_clinical_knowledge,5-shot,accuracy,0.2641509433962264,0.027134291628741713
llama2_220M_nl_40_code_60,mmlu_college_medicine,5-shot,accuracy,0.2254335260115607,0.03186209851641145
llama2_220M_nl_40_code_60,mmlu_global_facts,5-shot,accuracy,0.18,0.038612291966536955
llama2_220M_nl_40_code_60,mmlu_human_aging,5-shot,accuracy,0.34080717488789236,0.0318114974705536
llama2_220M_nl_40_code_60,mmlu_management,5-shot,accuracy,0.17475728155339806,0.03760178006026621
llama2_220M_nl_40_code_60,mmlu_marketing,5-shot,accuracy,0.2264957264957265,0.027421007295392923
llama2_220M_nl_40_code_60,mmlu_medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
llama2_220M_nl_40_code_60,mmlu_miscellaneous,5-shot,accuracy,0.22860791826309068,0.015016884698539887
llama2_220M_nl_40_code_60,mmlu_nutrition,5-shot,accuracy,0.25163398692810457,0.0248480182638752
llama2_220M_nl_40_code_60,mmlu_professional_accounting,5-shot,accuracy,0.2198581560283688,0.02470614107070548
llama2_220M_nl_40_code_60,mmlu_professional_medicine,5-shot,accuracy,0.35661764705882354,0.02909720956841195
llama2_220M_nl_40_code_60,mmlu_virology,5-shot,accuracy,0.2469879518072289,0.03357351982064536
llama2_220M_nl_40_code_60,mmlu_econometrics,5-shot,accuracy,0.30701754385964913,0.04339138322579861
llama2_220M_nl_40_code_60,mmlu_high_school_geography,5-shot,accuracy,0.36363636363636365,0.03427308652999935
llama2_220M_nl_40_code_60,mmlu_high_school_government_and_politics,5-shot,accuracy,0.3626943005181347,0.03469713791704372
llama2_220M_nl_40_code_60,mmlu_high_school_macroeconomics,5-shot,accuracy,0.30256410256410254,0.02329088805377274
llama2_220M_nl_40_code_60,mmlu_high_school_microeconomics,5-shot,accuracy,0.20588235294117646,0.026265024608275886
llama2_220M_nl_40_code_60,mmlu_high_school_psychology,5-shot,accuracy,0.29174311926605506,0.019489300968876522
llama2_220M_nl_40_code_60,mmlu_human_sexuality,5-shot,accuracy,0.2748091603053435,0.03915345408847834
llama2_220M_nl_40_code_60,mmlu_professional_psychology,5-shot,accuracy,0.25163398692810457,0.01755581809132225
llama2_220M_nl_40_code_60,mmlu_public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
llama2_220M_nl_40_code_60,mmlu_security_studies,5-shot,accuracy,0.3510204081632653,0.030555316755573644
llama2_220M_nl_40_code_60,mmlu_sociology,5-shot,accuracy,0.21393034825870647,0.028996909693328937
llama2_220M_nl_40_code_60,mmlu_us_foreign_policy,5-shot,accuracy,0.31,0.04648231987117316
llama2_220M_nl_40_code_60,mmlu_abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
llama2_220M_nl_40_code_60,mmlu_anatomy,5-shot,accuracy,0.23703703703703705,0.03673731683969506
llama2_220M_nl_40_code_60,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_40_code_60,mmlu_college_biology,5-shot,accuracy,0.2361111111111111,0.03551446610810826
llama2_220M_nl_40_code_60,mmlu_college_chemistry,5-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_40_code_60,mmlu_college_computer_science,5-shot,accuracy,0.24,0.042923469599092816
llama2_220M_nl_40_code_60,mmlu_college_mathematics,5-shot,accuracy,0.24,0.04292346959909282
llama2_220M_nl_40_code_60,mmlu_college_physics,5-shot,accuracy,0.20588235294117646,0.040233822736177476
llama2_220M_nl_40_code_60,mmlu_computer_security,5-shot,accuracy,0.22,0.04163331998932269
llama2_220M_nl_40_code_60,mmlu_conceptual_physics,5-shot,accuracy,0.2936170212765957,0.029771642712491234
llama2_220M_nl_40_code_60,mmlu_electrical_engineering,5-shot,accuracy,0.2413793103448276,0.03565998174135302
llama2_220M_nl_40_code_60,mmlu_elementary_mathematics,5-shot,accuracy,0.2275132275132275,0.021591269407823785
llama2_220M_nl_40_code_60,mmlu_high_school_biology,5-shot,accuracy,0.1967741935483871,0.022616409420742032
llama2_220M_nl_40_code_60,mmlu_high_school_chemistry,5-shot,accuracy,0.2413793103448276,0.03010833071801162
llama2_220M_nl_40_code_60,mmlu_high_school_computer_science,5-shot,accuracy,0.2,0.04020151261036845
llama2_220M_nl_40_code_60,mmlu_high_school_mathematics,5-shot,accuracy,0.2814814814814815,0.02742001935094528
llama2_220M_nl_40_code_60,mmlu_high_school_physics,5-shot,accuracy,0.2251655629139073,0.03410435282008937
llama2_220M_nl_40_code_60,mmlu_high_school_statistics,5-shot,accuracy,0.4398148148148148,0.03385177976044811
llama2_220M_nl_40_code_60,mmlu_machine_learning,5-shot,accuracy,0.35714285714285715,0.04547960999764376
llama2_220M_nl_20_code_80,mmlu_formal_logic,5-shot,accuracy,0.21428571428571427,0.03670066451047182
llama2_220M_nl_20_code_80,mmlu_high_school_european_history,5-shot,accuracy,0.2606060606060606,0.034277431758165236
llama2_220M_nl_20_code_80,mmlu_high_school_us_history,5-shot,accuracy,0.19607843137254902,0.027865942286639325
llama2_220M_nl_20_code_80,mmlu_high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598035
llama2_220M_nl_20_code_80,mmlu_international_law,5-shot,accuracy,0.2644628099173554,0.04026187527591207
llama2_220M_nl_20_code_80,mmlu_jurisprudence,5-shot,accuracy,0.24074074074074073,0.041331194402438376
llama2_220M_nl_20_code_80,mmlu_logical_fallacies,5-shot,accuracy,0.26993865030674846,0.034878251684978906
llama2_220M_nl_20_code_80,mmlu_moral_disputes,5-shot,accuracy,0.20809248554913296,0.021855255263421802
llama2_220M_nl_20_code_80,mmlu_moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
llama2_220M_nl_20_code_80,mmlu_philosophy,5-shot,accuracy,0.2057877813504823,0.022961339906764237
llama2_220M_nl_20_code_80,mmlu_prehistory,5-shot,accuracy,0.2222222222222222,0.023132376234543343
llama2_220M_nl_20_code_80,mmlu_professional_law,5-shot,accuracy,0.24511082138200782,0.010986307870045503
llama2_220M_nl_20_code_80,mmlu_world_religions,5-shot,accuracy,0.25146198830409355,0.033275044238468436
llama2_220M_nl_20_code_80,mmlu_business_ethics,5-shot,accuracy,0.29,0.04560480215720683
llama2_220M_nl_20_code_80,mmlu_clinical_knowledge,5-shot,accuracy,0.21132075471698114,0.025125766484827845
llama2_220M_nl_20_code_80,mmlu_college_medicine,5-shot,accuracy,0.23699421965317918,0.03242414757483098
llama2_220M_nl_20_code_80,mmlu_global_facts,5-shot,accuracy,0.18,0.038612291966536955
llama2_220M_nl_20_code_80,mmlu_human_aging,5-shot,accuracy,0.27802690582959644,0.03006958487449405
llama2_220M_nl_20_code_80,mmlu_management,5-shot,accuracy,0.17475728155339806,0.03760178006026621
llama2_220M_nl_20_code_80,mmlu_marketing,5-shot,accuracy,0.19230769230769232,0.025819233256483706
llama2_220M_nl_20_code_80,mmlu_medical_genetics,5-shot,accuracy,0.32,0.046882617226215034
llama2_220M_nl_20_code_80,mmlu_miscellaneous,5-shot,accuracy,0.2388250319284802,0.015246803197398691
llama2_220M_nl_20_code_80,mmlu_nutrition,5-shot,accuracy,0.2222222222222222,0.023805186524888146
llama2_220M_nl_20_code_80,mmlu_professional_accounting,5-shot,accuracy,0.25886524822695034,0.026129572527180844
llama2_220M_nl_20_code_80,mmlu_professional_medicine,5-shot,accuracy,0.4522058823529412,0.030233758551596445
llama2_220M_nl_20_code_80,mmlu_virology,5-shot,accuracy,0.23493975903614459,0.03300533186128922
llama2_220M_nl_20_code_80,mmlu_econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
llama2_220M_nl_20_code_80,mmlu_high_school_geography,5-shot,accuracy,0.21212121212121213,0.029126522834586818
llama2_220M_nl_20_code_80,mmlu_high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.032577140777096614
llama2_220M_nl_20_code_80,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2358974358974359,0.02152596540740872
llama2_220M_nl_20_code_80,mmlu_high_school_microeconomics,5-shot,accuracy,0.23109243697478993,0.027381406927868952
llama2_220M_nl_20_code_80,mmlu_high_school_psychology,5-shot,accuracy,0.22752293577981653,0.017974463578776502
llama2_220M_nl_20_code_80,mmlu_human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
llama2_220M_nl_20_code_80,mmlu_professional_psychology,5-shot,accuracy,0.25,0.01751781884501444
llama2_220M_nl_20_code_80,mmlu_public_relations,5-shot,accuracy,0.20909090909090908,0.03895091015724135
llama2_220M_nl_20_code_80,mmlu_security_studies,5-shot,accuracy,0.22857142857142856,0.026882144922307744
llama2_220M_nl_20_code_80,mmlu_sociology,5-shot,accuracy,0.263681592039801,0.03115715086935558
llama2_220M_nl_20_code_80,mmlu_us_foreign_policy,5-shot,accuracy,0.26,0.04408440022768078
llama2_220M_nl_20_code_80,mmlu_abstract_algebra,5-shot,accuracy,0.22,0.041633319989322695
llama2_220M_nl_20_code_80,mmlu_anatomy,5-shot,accuracy,0.28888888888888886,0.0391545063041425
llama2_220M_nl_20_code_80,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_20_code_80,mmlu_college_biology,5-shot,accuracy,0.2361111111111111,0.03551446610810826
llama2_220M_nl_20_code_80,mmlu_college_chemistry,5-shot,accuracy,0.2,0.040201512610368445
llama2_220M_nl_20_code_80,mmlu_college_computer_science,5-shot,accuracy,0.23,0.04229525846816506
llama2_220M_nl_20_code_80,mmlu_college_mathematics,5-shot,accuracy,0.26,0.044084400227680794
llama2_220M_nl_20_code_80,mmlu_college_physics,5-shot,accuracy,0.22549019607843138,0.04158307533083286
llama2_220M_nl_20_code_80,mmlu_computer_security,5-shot,accuracy,0.27,0.0446196043338474
llama2_220M_nl_20_code_80,mmlu_conceptual_physics,5-shot,accuracy,0.2851063829787234,0.029513196625539355
llama2_220M_nl_20_code_80,mmlu_electrical_engineering,5-shot,accuracy,0.25517241379310346,0.03632984052707842
llama2_220M_nl_20_code_80,mmlu_elementary_mathematics,5-shot,accuracy,0.25132275132275134,0.022340482339643895
llama2_220M_nl_20_code_80,mmlu_high_school_biology,5-shot,accuracy,0.24516129032258063,0.02447224384089552
llama2_220M_nl_20_code_80,mmlu_high_school_chemistry,5-shot,accuracy,0.24630541871921183,0.03031509928561773
llama2_220M_nl_20_code_80,mmlu_high_school_computer_science,5-shot,accuracy,0.27,0.0446196043338474
llama2_220M_nl_20_code_80,mmlu_high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.026719240783712173
llama2_220M_nl_20_code_80,mmlu_high_school_physics,5-shot,accuracy,0.2251655629139073,0.03410435282008936
llama2_220M_nl_20_code_80,mmlu_high_school_statistics,5-shot,accuracy,0.38425925925925924,0.03317354514310742
llama2_220M_nl_20_code_80,mmlu_machine_learning,5-shot,accuracy,0.35714285714285715,0.04547960999764376
llama2_220M_nl_20_code_80,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
llama2_220M_nl_20_code_80,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
llama2_220M_nl_20_code_80,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.03058759135160424
llama2_220M_nl_20_code_80,mmlu_high_school_world_history,0-shot,accuracy,0.26582278481012656,0.028756799629658335
llama2_220M_nl_20_code_80,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794088
llama2_220M_nl_20_code_80,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
llama2_220M_nl_20_code_80,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
llama2_220M_nl_20_code_80,mmlu_moral_disputes,0-shot,accuracy,0.24566473988439305,0.02317629820399201
llama2_220M_nl_20_code_80,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
llama2_220M_nl_20_code_80,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
llama2_220M_nl_20_code_80,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.023016705640262203
llama2_220M_nl_20_code_80,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
llama2_220M_nl_20_code_80,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.03565079670708313
llama2_220M_nl_20_code_80,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_20_code_80,mmlu_clinical_knowledge,0-shot,accuracy,0.2188679245283019,0.025447863825108604
llama2_220M_nl_20_code_80,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.03126511206173043
llama2_220M_nl_20_code_80,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
llama2_220M_nl_20_code_80,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.031024411740572206
llama2_220M_nl_20_code_80,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
llama2_220M_nl_20_code_80,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
llama2_220M_nl_20_code_80,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
llama2_220M_nl_20_code_80,mmlu_miscellaneous,0-shot,accuracy,0.23116219667943805,0.015075523238101074
llama2_220M_nl_20_code_80,mmlu_nutrition,0-shot,accuracy,0.21241830065359477,0.02342037547829613
llama2_220M_nl_20_code_80,mmlu_professional_accounting,0-shot,accuracy,0.23049645390070922,0.025123739226872405
llama2_220M_nl_20_code_80,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
llama2_220M_nl_20_code_80,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
llama2_220M_nl_20_code_80,mmlu_econometrics,0-shot,accuracy,0.22807017543859648,0.03947152782669415
llama2_220M_nl_20_code_80,mmlu_high_school_geography,0-shot,accuracy,0.18181818181818182,0.027479603010538804
llama2_220M_nl_20_code_80,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
llama2_220M_nl_20_code_80,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
llama2_220M_nl_20_code_80,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
llama2_220M_nl_20_code_80,mmlu_high_school_psychology,0-shot,accuracy,0.1908256880733945,0.01684767640009109
llama2_220M_nl_20_code_80,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
llama2_220M_nl_20_code_80,mmlu_professional_psychology,0-shot,accuracy,0.25163398692810457,0.01755581809132225
llama2_220M_nl_20_code_80,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
llama2_220M_nl_20_code_80,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
llama2_220M_nl_20_code_80,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
llama2_220M_nl_20_code_80,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
llama2_220M_nl_20_code_80,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
llama2_220M_nl_20_code_80,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
llama2_220M_nl_20_code_80,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_20_code_80,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
llama2_220M_nl_20_code_80,mmlu_college_chemistry,0-shot,accuracy,0.19,0.03942772444036623
llama2_220M_nl_20_code_80,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
llama2_220M_nl_20_code_80,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_20_code_80,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
llama2_220M_nl_20_code_80,mmlu_computer_security,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_20_code_80,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
llama2_220M_nl_20_code_80,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
llama2_220M_nl_20_code_80,mmlu_elementary_mathematics,0-shot,accuracy,0.20634920634920634,0.02084229093011466
llama2_220M_nl_20_code_80,mmlu_high_school_biology,0-shot,accuracy,0.18387096774193548,0.02203721734026784
llama2_220M_nl_20_code_80,mmlu_high_school_chemistry,0-shot,accuracy,0.16748768472906403,0.02627308604753542
llama2_220M_nl_20_code_80,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
llama2_220M_nl_20_code_80,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
llama2_220M_nl_20_code_80,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
llama2_220M_nl_20_code_80,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
llama2_220M_nl_20_code_80,mmlu_machine_learning,0-shot,accuracy,0.32142857142857145,0.044328040552915185
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.03058759135160424
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_world_history,0-shot,accuracy,0.26582278481012656,0.028756799629658335
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794088
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_moral_disputes,0-shot,accuracy,0.24566473988439305,0.02317629820399201
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.023016705640262203
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.03565079670708313
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_clinical_knowledge,0-shot,accuracy,0.2188679245283019,0.025447863825108604
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.03126511206173043
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.031024411740572206
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_miscellaneous,0-shot,accuracy,0.23116219667943805,0.015075523238101074
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_nutrition,0-shot,accuracy,0.21241830065359477,0.02342037547829613
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_professional_accounting,0-shot,accuracy,0.23049645390070922,0.025123739226872405
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_econometrics,0-shot,accuracy,0.22807017543859648,0.03947152782669415
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_geography,0-shot,accuracy,0.18181818181818182,0.027479603010538804
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_psychology,0-shot,accuracy,0.1908256880733945,0.01684767640009109
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_professional_psychology,0-shot,accuracy,0.25163398692810457,0.01755581809132225
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_college_chemistry,0-shot,accuracy,0.19,0.03942772444036623
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_computer_security,0-shot,accuracy,0.3,0.046056618647183814
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_elementary_mathematics,0-shot,accuracy,0.20634920634920634,0.02084229093011466
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_biology,0-shot,accuracy,0.18387096774193548,0.02203721734026784
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_chemistry,0-shot,accuracy,0.16748768472906403,0.02627308604753542
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,mmlu_machine_learning,0-shot,accuracy,0.32142857142857145,0.044328040552915185
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,bleu_max,15.877975769698015,0.5771958617322904
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,bleu_acc,0.32313341493268055,0.016371836286454607
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,bleu_diff,-2.2296304192311505,0.4598514923480091
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge1_max,37.603304357108065,0.808857210400507
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge1_acc,0.30599755201958384,0.016132229728155034
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge1_diff,-4.76960065954478,0.6362016553926024
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge2_max,20.66021486384751,0.8393148447488252
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge2_acc,0.211750305997552,0.014302068353925609
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge2_diff,-4.741822300044189,0.6469842034576851
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rougeL_max,34.61787246177079,0.7960851454536261
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rougeL_acc,0.3084455324357405,0.016168039383156873
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rougeL_diff,-4.472748227965035,0.6209345755808581
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_mc1,0-shot,accuracy,0.23133414932680538,0.014761945174862661
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,truthfulqa_mc2,0-shot,accuracy,0.4244172501731057,0.015309424992981652
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,bleu_max,14.304044706921127,0.5579122226826114
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,bleu_acc,0.3292533659730722,0.01645126444006821
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,bleu_diff,0.43733094102075576,0.4644469788436442
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge1_max,33.59162254718034,0.8000229732541629
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge1_acc,0.3219094247246022,0.01635556761196043
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge1_diff,-2.4213671645690935,0.7119318106950181
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge2_max,17.868150237378845,0.822231531855864
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge2_acc,0.200734394124847,0.014022045717482145
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rouge2_diff,-0.7390323263152959,0.6781232524313218
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rougeL_max,30.97952552658296,0.7772749173461311
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rougeL_acc,0.3219094247246022,0.01635556761196043
llama2_220M_nl_0_code_100,truthfulqa_gen,0-shot,rougeL_diff,-1.8241322231169401,0.6918870034374448
llama2_220M_nl_0_code_100,truthfulqa_mc1,0-shot,accuracy,0.26560587515299877,0.015461027627253586
llama2_220M_nl_0_code_100,truthfulqa_mc2,0-shot,accuracy,0.47845193527633556,0.015860486879985935
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,bleu_max,15.877975769698015,0.5771958617322904
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,bleu_acc,0.32313341493268055,0.016371836286454607
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,bleu_diff,-2.2296304192311505,0.4598514923480091
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge1_max,37.603304357108065,0.808857210400507
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge1_acc,0.30599755201958384,0.016132229728155034
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge1_diff,-4.76960065954478,0.6362016553926024
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge2_max,20.66021486384751,0.8393148447488252
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge2_acc,0.211750305997552,0.014302068353925609
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rouge2_diff,-4.741822300044189,0.6469842034576851
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rougeL_max,34.61787246177079,0.7960851454536261
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rougeL_acc,0.3084455324357405,0.016168039383156873
llama2_220M_nl_20_code_80,truthfulqa_gen,0-shot,rougeL_diff,-4.472748227965035,0.6209345755808581
llama2_220M_nl_20_code_80,truthfulqa_mc1,0-shot,accuracy,0.23133414932680538,0.014761945174862661
llama2_220M_nl_20_code_80,truthfulqa_mc2,0-shot,accuracy,0.4244172501731057,0.015309424992981652
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,bleu_max,16.11502512737246,0.5869657185528809
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,bleu_acc,0.33414932680538556,0.016512530677150562
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,bleu_diff,-2.8298732346152873,0.5432367796999206
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rouge1_max,37.473755700855754,0.7962482671243429
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rouge1_acc,0.28151774785801714,0.01574402724825605
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rouge1_diff,-5.493021225155249,0.6991156357240162
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rouge2_max,20.025993009614204,0.8384686530545362
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rouge2_acc,0.18237454100367198,0.013518055636187203
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rouge2_diff,-5.867863422869925,0.7194832753893369
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rougeL_max,34.6838158485818,0.7845118753622836
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rougeL_acc,0.2876376988984088,0.01584631510139479
llama2_220M_nl_40_code_60,truthfulqa_gen,0-shot,rougeL_diff,-5.18696756062025,0.6888336868345359
llama2_220M_nl_40_code_60,truthfulqa_mc1,0-shot,accuracy,0.2423500611995104,0.015000674373570347
llama2_220M_nl_40_code_60,truthfulqa_mc2,0-shot,accuracy,0.42423540280260225,0.015225537193682557
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,winogrande,0-shot,accuracy,0.5185477505919495,0.014042813708888378
llama2_220M_nl_40_code_60,winogrande,5-shot,accuracy,0.5201262825572218,0.014041096664344324
llama2_220M_nl_20_code_80,winogrande,5-shot,accuracy,0.5201262825572218,0.014041096664344327
llama2_220M_nl_0_code_100,winogrande,5-shot,accuracy,0.49171270718232046,0.014050555322824194
llama2_220M_nl_only_shuf-hf,humaneval,0-shot,accuracy,0.0,
mosaicml/mpt-30b,humaneval,0-shot,accuracy,0.21951219512195122,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts_hf_final/llama2_220M_nl_20_code_80,humaneval,0-shot,accuracy,0.0,
facebook/xglm-4.5B,humaneval,0-shot,accuracy,0.0,
allenai/OLMo-7B-hf,humaneval,0-shot,accuracy,0.1402439024390244,
cerebras/Cerebras-GPT-2.7B,humaneval,0-shot,accuracy,0.042682926829268296,
LLM360/CrystalCoder,anli_r1,0-shot,brier_score,0.8812255586836111,
LLM360/CrystalCoder,anli_r2,0-shot,brier_score,0.8827915610880336,
LLM360/CrystalCoder,anli_r3,0-shot,brier_score,0.8723603626882253,
LLM360/CrystalCoder,logiqa2,0-shot,brier_score,1.5175826953881866,
LLM360/CrystalCoder,mathqa,0-shot,brier_score,1.0365719895785452,
LLM360/CrystalCoder,xnli_ar,0-shot,brier_score,0.7850920366475629,
LLM360/CrystalCoder,xnli_bg,0-shot,brier_score,0.8903630765159165,
LLM360/CrystalCoder,xnli_de,0-shot,brier_score,0.8835421292417125,
LLM360/CrystalCoder,xnli_el,0-shot,brier_score,0.8771971661349651,
LLM360/CrystalCoder,xnli_en,0-shot,brier_score,0.7771089846951509,
LLM360/CrystalCoder,xnli_es,0-shot,brier_score,1.328462274612668,
LLM360/CrystalCoder,xnli_fr,0-shot,brier_score,1.3222677487428272,
LLM360/CrystalCoder,xnli_hi,0-shot,brier_score,1.2942448832705324,
LLM360/CrystalCoder,xnli_ru,0-shot,brier_score,0.8922781717696626,
LLM360/CrystalCoder,xnli_sw,0-shot,brier_score,0.907621966930984,
LLM360/CrystalCoder,xnli_th,0-shot,brier_score,1.3166329915919412,
LLM360/CrystalCoder,xnli_tr,0-shot,brier_score,1.3150672733218385,
LLM360/CrystalCoder,xnli_ur,0-shot,brier_score,1.243104511149593,
LLM360/CrystalCoder,xnli_vi,0-shot,brier_score,1.068366835074418,
LLM360/CrystalCoder,xnli_zh,0-shot,brier_score,0.9281001714216832,
LLM360/Amber,anli_r1,0-shot,brier_score,nan,
LLM360/Amber,anli_r2,0-shot,brier_score,nan,
LLM360/Amber,anli_r3,0-shot,brier_score,nan,
LLM360/Amber,logiqa2,0-shot,brier_score,nan,
LLM360/Amber,mathqa,0-shot,brier_score,nan,
LLM360/Amber,xnli_ar,0-shot,brier_score,nan,
LLM360/Amber,xnli_bg,0-shot,brier_score,nan,
LLM360/Amber,xnli_de,0-shot,brier_score,nan,
LLM360/Amber,xnli_el,0-shot,brier_score,nan,
LLM360/Amber,xnli_en,0-shot,brier_score,nan,
LLM360/Amber,xnli_es,0-shot,brier_score,nan,
LLM360/Amber,xnli_fr,0-shot,brier_score,nan,
LLM360/Amber,xnli_hi,0-shot,brier_score,nan,
LLM360/Amber,xnli_ru,0-shot,brier_score,nan,
LLM360/Amber,xnli_sw,0-shot,brier_score,nan,
LLM360/Amber,xnli_th,0-shot,brier_score,nan,
LLM360/Amber,xnli_tr,0-shot,brier_score,nan,
LLM360/Amber,xnli_ur,0-shot,brier_score,nan,
LLM360/Amber,xnli_vi,0-shot,brier_score,nan,
LLM360/Amber,xnli_zh,0-shot,brier_score,nan,
EleutherAI/pythia-6.9b-deduped,anli_r1,0-shot,brier_score,0.8416143517113052,
EleutherAI/pythia-6.9b-deduped,anli_r2,0-shot,brier_score,0.8184571855085381,
EleutherAI/pythia-6.9b-deduped,anli_r3,0-shot,brier_score,0.8017969311343001,
EleutherAI/pythia-6.9b-deduped,logiqa2,0-shot,brier_score,1.0986781115807636,
EleutherAI/pythia-6.9b-deduped,mathqa,0-shot,brier_score,0.9517119900854306,
EleutherAI/pythia-6.9b-deduped,xnli_ar,0-shot,brier_score,1.1209373017098156,
EleutherAI/pythia-6.9b-deduped,xnli_bg,0-shot,brier_score,0.817099832260647,
EleutherAI/pythia-6.9b-deduped,xnli_de,0-shot,brier_score,0.8357250209133503,
EleutherAI/pythia-6.9b-deduped,xnli_el,0-shot,brier_score,0.929360524495495,
EleutherAI/pythia-6.9b-deduped,xnli_en,0-shot,brier_score,0.6408658367843229,
EleutherAI/pythia-6.9b-deduped,xnli_es,0-shot,brier_score,0.8411056515605171,
EleutherAI/pythia-6.9b-deduped,xnli_fr,0-shot,brier_score,0.7463606648679049,
EleutherAI/pythia-6.9b-deduped,xnli_hi,0-shot,brier_score,0.7298080069431987,
EleutherAI/pythia-6.9b-deduped,xnli_ru,0-shot,brier_score,0.7033517686535136,
EleutherAI/pythia-6.9b-deduped,xnli_sw,0-shot,brier_score,0.822937488319687,
EleutherAI/pythia-6.9b-deduped,xnli_th,0-shot,brier_score,0.7769398342705626,
EleutherAI/pythia-6.9b-deduped,xnli_tr,0-shot,brier_score,0.8640975744849078,
EleutherAI/pythia-6.9b-deduped,xnli_ur,0-shot,brier_score,0.9682074838208192,
EleutherAI/pythia-6.9b-deduped,xnli_vi,0-shot,brier_score,0.7627104180158375,
EleutherAI/pythia-6.9b-deduped,xnli_zh,0-shot,brier_score,0.8992995710521737,
EleutherAI/pythia-2.8b,anli_r1,0-shot,brier_score,0.8184821455136903,
EleutherAI/pythia-2.8b,anli_r2,0-shot,brier_score,0.8203348236829492,
EleutherAI/pythia-2.8b,anli_r3,0-shot,brier_score,0.7786130486085269,
EleutherAI/pythia-2.8b,logiqa2,0-shot,brier_score,1.1738511673088983,
EleutherAI/pythia-2.8b,mathqa,0-shot,brier_score,0.9548314965636602,
EleutherAI/pythia-2.8b,xnli_ar,0-shot,brier_score,1.149351395934471,
EleutherAI/pythia-2.8b,xnli_bg,0-shot,brier_score,0.9194366741630119,
EleutherAI/pythia-2.8b,xnli_de,0-shot,brier_score,0.8181965503394755,
EleutherAI/pythia-2.8b,xnli_el,0-shot,brier_score,0.8747238999288425,
EleutherAI/pythia-2.8b,xnli_en,0-shot,brier_score,0.6472083160313256,
EleutherAI/pythia-2.8b,xnli_es,0-shot,brier_score,0.8343332039248772,
EleutherAI/pythia-2.8b,xnli_fr,0-shot,brier_score,0.7886138338084571,
EleutherAI/pythia-2.8b,xnli_hi,0-shot,brier_score,0.7962466975832869,
EleutherAI/pythia-2.8b,xnli_ru,0-shot,brier_score,0.7925641991287486,
EleutherAI/pythia-2.8b,xnli_sw,0-shot,brier_score,0.9021779105424378,
EleutherAI/pythia-2.8b,xnli_th,0-shot,brier_score,0.8133303406854515,
EleutherAI/pythia-2.8b,xnli_tr,0-shot,brier_score,0.8242051073553582,
EleutherAI/pythia-2.8b,xnli_ur,0-shot,brier_score,1.0568756773024743,
EleutherAI/pythia-2.8b,xnli_vi,0-shot,brier_score,0.7849191354086876,
EleutherAI/pythia-2.8b,xnli_zh,0-shot,brier_score,1.106508528412566,
EleutherAI/pythia-160m-deduped,anli_r1,0-shot,brier_score,0.920705363338551,
EleutherAI/pythia-160m-deduped,anli_r2,0-shot,brier_score,0.9111046921733259,
EleutherAI/pythia-160m-deduped,anli_r3,0-shot,brier_score,0.8700027911319644,
EleutherAI/pythia-160m-deduped,logiqa2,0-shot,brier_score,1.1307008102764402,
EleutherAI/pythia-160m-deduped,mathqa,0-shot,brier_score,1.0042873221176545,
EleutherAI/pythia-160m-deduped,xnli_ar,0-shot,brier_score,1.0103226922466955,
EleutherAI/pythia-160m-deduped,xnli_bg,0-shot,brier_score,0.9464292758839111,
EleutherAI/pythia-160m-deduped,xnli_de,0-shot,brier_score,0.9413020291983456,
EleutherAI/pythia-160m-deduped,xnli_el,0-shot,brier_score,1.3330444278405043,
EleutherAI/pythia-160m-deduped,xnli_en,0-shot,brier_score,0.7360568337008669,
EleutherAI/pythia-160m-deduped,xnli_es,0-shot,brier_score,0.9718750183724897,
EleutherAI/pythia-160m-deduped,xnli_fr,0-shot,brier_score,0.9678052650279382,
EleutherAI/pythia-160m-deduped,xnli_hi,0-shot,brier_score,1.0350893447973,
EleutherAI/pythia-160m-deduped,xnli_ru,0-shot,brier_score,0.8280610496744621,
EleutherAI/pythia-160m-deduped,xnli_sw,0-shot,brier_score,1.0347646845019387,
EleutherAI/pythia-160m-deduped,xnli_th,0-shot,brier_score,0.9511135021738109,
EleutherAI/pythia-160m-deduped,xnli_tr,0-shot,brier_score,0.9423854795420442,
EleutherAI/pythia-160m-deduped,xnli_ur,0-shot,brier_score,1.3250493018797287,
EleutherAI/pythia-160m-deduped,xnli_vi,0-shot,brier_score,1.171776564505068,
EleutherAI/pythia-160m-deduped,xnli_zh,0-shot,brier_score,0.9711283027256441,
EleutherAI/pythia-1b-deduped,anli_r1,0-shot,brier_score,0.9911970843208271,
EleutherAI/pythia-1b-deduped,anli_r2,0-shot,brier_score,0.973895085552014,
EleutherAI/pythia-1b-deduped,anli_r3,0-shot,brier_score,0.8853229744501222,
EleutherAI/pythia-1b-deduped,logiqa2,0-shot,brier_score,1.2061844650412292,
EleutherAI/pythia-1b-deduped,mathqa,0-shot,brier_score,0.9856041802836759,
EleutherAI/pythia-1b-deduped,xnli_ar,0-shot,brier_score,1.2860791752346812,
EleutherAI/pythia-1b-deduped,xnli_bg,0-shot,brier_score,0.9257915918101831,
EleutherAI/pythia-1b-deduped,xnli_de,0-shot,brier_score,0.8585873809354816,
EleutherAI/pythia-1b-deduped,xnli_el,0-shot,brier_score,1.2364868161235738,
EleutherAI/pythia-1b-deduped,xnli_en,0-shot,brier_score,0.680053645450785,
EleutherAI/pythia-1b-deduped,xnli_es,0-shot,brier_score,0.893284647291574,
EleutherAI/pythia-1b-deduped,xnli_fr,0-shot,brier_score,0.8401467254159914,
EleutherAI/pythia-1b-deduped,xnli_hi,0-shot,brier_score,0.8212227273440394,
EleutherAI/pythia-1b-deduped,xnli_ru,0-shot,brier_score,0.7596252704385034,
EleutherAI/pythia-1b-deduped,xnli_sw,0-shot,brier_score,0.959918783932522,
EleutherAI/pythia-1b-deduped,xnli_th,0-shot,brier_score,0.7844355358057665,
EleutherAI/pythia-1b-deduped,xnli_tr,0-shot,brier_score,0.943928951713455,
EleutherAI/pythia-1b-deduped,xnli_ur,0-shot,brier_score,1.1429941046302474,
EleutherAI/pythia-1b-deduped,xnli_vi,0-shot,brier_score,0.7736434155664499,
EleutherAI/pythia-1b-deduped,xnli_zh,0-shot,brier_score,0.9955002442048652,
EleutherAI/pythia-12b,anli_r1,0-shot,brier_score,nan,
EleutherAI/pythia-12b,anli_r2,0-shot,brier_score,nan,
EleutherAI/pythia-12b,anli_r3,0-shot,brier_score,nan,
EleutherAI/pythia-12b,logiqa2,0-shot,brier_score,nan,
EleutherAI/pythia-12b,mathqa,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_ar,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_bg,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_de,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_el,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_en,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_es,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_fr,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_hi,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_ru,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_sw,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_th,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_tr,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_ur,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_vi,0-shot,brier_score,nan,
EleutherAI/pythia-12b,xnli_zh,0-shot,brier_score,nan,
EleutherAI/pythia-1b,anli_r1,0-shot,brier_score,0.8769161593101005,
EleutherAI/pythia-1b,anli_r2,0-shot,brier_score,0.8803955863868214,
EleutherAI/pythia-1b,anli_r3,0-shot,brier_score,0.8542586738896557,
EleutherAI/pythia-1b,logiqa2,0-shot,brier_score,1.209477212590458,
EleutherAI/pythia-1b,mathqa,0-shot,brier_score,0.9956111803975747,
EleutherAI/pythia-1b,xnli_ar,0-shot,brier_score,1.1556803342715476,
EleutherAI/pythia-1b,xnli_bg,0-shot,brier_score,0.901065625493324,
EleutherAI/pythia-1b,xnli_de,0-shot,brier_score,0.8628262449267354,
EleutherAI/pythia-1b,xnli_el,0-shot,brier_score,1.02157679384408,
EleutherAI/pythia-1b,xnli_en,0-shot,brier_score,0.6791075636189999,
EleutherAI/pythia-1b,xnli_es,0-shot,brier_score,0.8819352611271246,
EleutherAI/pythia-1b,xnli_fr,0-shot,brier_score,0.8533803987796642,
EleutherAI/pythia-1b,xnli_hi,0-shot,brier_score,0.8082987565768401,
EleutherAI/pythia-1b,xnli_ru,0-shot,brier_score,0.8381512547586453,
EleutherAI/pythia-1b,xnli_sw,0-shot,brier_score,0.9732581732353762,
EleutherAI/pythia-1b,xnli_th,0-shot,brier_score,0.8769932280776286,
EleutherAI/pythia-1b,xnli_tr,0-shot,brier_score,0.8418142064317129,
EleutherAI/pythia-1b,xnli_ur,0-shot,brier_score,1.0392151978104032,
EleutherAI/pythia-1b,xnli_vi,0-shot,brier_score,0.800826608425218,
EleutherAI/pythia-1b,xnli_zh,0-shot,brier_score,0.9091939287645795,
EleutherAI/gpt-neox-20b,anli_r1,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,anli_r2,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,anli_r3,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,logiqa2,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,mathqa,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_ar,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_bg,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_de,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_el,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_en,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_es,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_fr,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_hi,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_ru,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_sw,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_th,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_tr,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_ur,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_vi,0-shot,brier_score,nan,
EleutherAI/gpt-neox-20b,xnli_zh,0-shot,brier_score,nan,
EleutherAI/pythia-160m,anli_r1,0-shot,brier_score,1.0676285782654396,
EleutherAI/pythia-160m,anli_r2,0-shot,brier_score,1.0698867379205763,
EleutherAI/pythia-160m,anli_r3,0-shot,brier_score,1.0390060682878968,
EleutherAI/pythia-160m,logiqa2,0-shot,brier_score,1.2042332112152054,
EleutherAI/pythia-160m,mathqa,0-shot,brier_score,1.000862389099545,
EleutherAI/pythia-160m,xnli_ar,0-shot,brier_score,1.1329729344727542,
EleutherAI/pythia-160m,xnli_bg,0-shot,brier_score,1.0401740056264728,
EleutherAI/pythia-160m,xnli_de,0-shot,brier_score,0.9571180981812406,
EleutherAI/pythia-160m,xnli_el,0-shot,brier_score,1.3347698257797498,
EleutherAI/pythia-160m,xnli_en,0-shot,brier_score,0.7436876499139015,
EleutherAI/pythia-160m,xnli_es,0-shot,brier_score,1.0851193831064803,
EleutherAI/pythia-160m,xnli_fr,0-shot,brier_score,0.9378646875333526,
EleutherAI/pythia-160m,xnli_hi,0-shot,brier_score,1.069387556954085,
EleutherAI/pythia-160m,xnli_ru,0-shot,brier_score,0.9376969005532549,
EleutherAI/pythia-160m,xnli_sw,0-shot,brier_score,0.9587163575243525,
EleutherAI/pythia-160m,xnli_th,0-shot,brier_score,1.1192107569737562,
EleutherAI/pythia-160m,xnli_tr,0-shot,brier_score,0.8823375123930616,
EleutherAI/pythia-160m,xnli_ur,0-shot,brier_score,1.3119389446620005,
EleutherAI/pythia-160m,xnli_vi,0-shot,brier_score,1.036833790013862,
EleutherAI/pythia-160m,xnli_zh,0-shot,brier_score,1.2790470085532075,
EleutherAI/pythia-410m-deduped,anli_r1,0-shot,brier_score,0.784437519474957,
EleutherAI/pythia-410m-deduped,anli_r2,0-shot,brier_score,0.7760679721276644,
EleutherAI/pythia-410m-deduped,anli_r3,0-shot,brier_score,0.7753232699465682,
EleutherAI/pythia-410m-deduped,logiqa2,0-shot,brier_score,1.209478610574182,
EleutherAI/pythia-410m-deduped,mathqa,0-shot,brier_score,0.99865565548703,
EleutherAI/pythia-410m-deduped,xnli_ar,0-shot,brier_score,1.051762829063465,
EleutherAI/pythia-410m-deduped,xnli_bg,0-shot,brier_score,0.9010391769343441,
EleutherAI/pythia-410m-deduped,xnli_de,0-shot,brier_score,0.8631552218297733,
EleutherAI/pythia-410m-deduped,xnli_el,0-shot,brier_score,1.2117574937034687,
EleutherAI/pythia-410m-deduped,xnli_en,0-shot,brier_score,0.6768025405847381,
EleutherAI/pythia-410m-deduped,xnli_es,0-shot,brier_score,0.8677240615035718,
EleutherAI/pythia-410m-deduped,xnli_fr,0-shot,brier_score,0.8463129855964044,
EleutherAI/pythia-410m-deduped,xnli_hi,0-shot,brier_score,0.8462825031785315,
EleutherAI/pythia-410m-deduped,xnli_ru,0-shot,brier_score,0.8122646259842906,
EleutherAI/pythia-410m-deduped,xnli_sw,0-shot,brier_score,0.933325177665682,
EleutherAI/pythia-410m-deduped,xnli_th,0-shot,brier_score,0.8294298483610969,
EleutherAI/pythia-410m-deduped,xnli_tr,0-shot,brier_score,0.9257249777728359,
EleutherAI/pythia-410m-deduped,xnli_ur,0-shot,brier_score,1.0099745833201328,
EleutherAI/pythia-410m-deduped,xnli_vi,0-shot,brier_score,0.8364283546097718,
EleutherAI/pythia-410m-deduped,xnli_zh,0-shot,brier_score,0.9663613710261606,
EleutherAI/pythia-1.4b-deduped,anli_r1,0-shot,brier_score,0.7867722220427156,
EleutherAI/pythia-1.4b-deduped,anli_r2,0-shot,brier_score,0.7890777193302807,
EleutherAI/pythia-1.4b-deduped,anli_r3,0-shot,brier_score,0.768151038234545,
EleutherAI/pythia-1.4b-deduped,logiqa2,0-shot,brier_score,1.1697216869972387,
EleutherAI/pythia-1.4b-deduped,mathqa,0-shot,brier_score,0.9700555202562146,
EleutherAI/pythia-1.4b-deduped,xnli_ar,0-shot,brier_score,1.2270728334770908,
EleutherAI/pythia-1.4b-deduped,xnli_bg,0-shot,brier_score,0.8252014693948149,
EleutherAI/pythia-1.4b-deduped,xnli_de,0-shot,brier_score,0.838631954559967,
EleutherAI/pythia-1.4b-deduped,xnli_el,0-shot,brier_score,0.9065494347422645,
EleutherAI/pythia-1.4b-deduped,xnli_en,0-shot,brier_score,0.6501678739021461,
EleutherAI/pythia-1.4b-deduped,xnli_es,0-shot,brier_score,0.8920104499485286,
EleutherAI/pythia-1.4b-deduped,xnli_fr,0-shot,brier_score,0.8173749815246898,
EleutherAI/pythia-1.4b-deduped,xnli_hi,0-shot,brier_score,0.7761131993279851,
EleutherAI/pythia-1.4b-deduped,xnli_ru,0-shot,brier_score,0.7883666752469697,
EleutherAI/pythia-1.4b-deduped,xnli_sw,0-shot,brier_score,0.9643682117105973,
EleutherAI/pythia-1.4b-deduped,xnli_th,0-shot,brier_score,0.8139721081788506,
EleutherAI/pythia-1.4b-deduped,xnli_tr,0-shot,brier_score,0.8929770609346074,
EleutherAI/pythia-1.4b-deduped,xnli_ur,0-shot,brier_score,0.9437211179714035,
EleutherAI/pythia-1.4b-deduped,xnli_vi,0-shot,brier_score,0.7979218378774825,
EleutherAI/pythia-1.4b-deduped,xnli_zh,0-shot,brier_score,0.9759585506612879,
EleutherAI/pythia-70m,anli_r1,0-shot,brier_score,0.9040260292258899,
EleutherAI/pythia-70m,anli_r2,0-shot,brier_score,0.9010766053720226,
EleutherAI/pythia-70m,anli_r3,0-shot,brier_score,0.8447182628698706,
EleutherAI/pythia-70m,logiqa2,0-shot,brier_score,1.1995013832847539,
EleutherAI/pythia-70m,mathqa,0-shot,brier_score,1.0449585699011112,
EleutherAI/pythia-70m,xnli_ar,0-shot,brier_score,0.9903640035295053,
EleutherAI/pythia-70m,xnli_bg,0-shot,brier_score,1.0946747445018923,
EleutherAI/pythia-70m,xnli_de,0-shot,brier_score,0.9264004837408497,
EleutherAI/pythia-70m,xnli_el,0-shot,brier_score,1.0712342886327977,
EleutherAI/pythia-70m,xnli_en,0-shot,brier_score,0.8251854337147393,
EleutherAI/pythia-70m,xnli_es,0-shot,brier_score,1.186663384681775,
EleutherAI/pythia-70m,xnli_fr,0-shot,brier_score,1.1024633994037367,
EleutherAI/pythia-70m,xnli_hi,0-shot,brier_score,1.1607563550824922,
EleutherAI/pythia-70m,xnli_ru,0-shot,brier_score,0.9804936667932032,
EleutherAI/pythia-70m,xnli_sw,0-shot,brier_score,0.9950850675504571,
EleutherAI/pythia-70m,xnli_th,0-shot,brier_score,1.1049336783034702,
EleutherAI/pythia-70m,xnli_tr,0-shot,brier_score,1.0107587196802033,
EleutherAI/pythia-70m,xnli_ur,0-shot,brier_score,1.3085704559361209,
EleutherAI/pythia-70m,xnli_vi,0-shot,brier_score,0.9920125714210457,
EleutherAI/pythia-70m,xnli_zh,0-shot,brier_score,1.1515953172950861,
EleutherAI/pythia-410m,anli_r1,0-shot,brier_score,0.9233587447090633,
EleutherAI/pythia-410m,anli_r2,0-shot,brier_score,0.9151460678051118,
EleutherAI/pythia-410m,anli_r3,0-shot,brier_score,0.8982848220034725,
EleutherAI/pythia-410m,logiqa2,0-shot,brier_score,1.2006036847444257,
EleutherAI/pythia-410m,mathqa,0-shot,brier_score,1.003122659646284,
EleutherAI/pythia-410m,xnli_ar,0-shot,brier_score,0.9919485935393514,
EleutherAI/pythia-410m,xnli_bg,0-shot,brier_score,0.9206072871772593,
EleutherAI/pythia-410m,xnli_de,0-shot,brier_score,0.8759353613461505,
EleutherAI/pythia-410m,xnli_el,0-shot,brier_score,1.1530366926408497,
EleutherAI/pythia-410m,xnli_en,0-shot,brier_score,0.6850776102409131,
EleutherAI/pythia-410m,xnli_es,0-shot,brier_score,0.879996230864522,
EleutherAI/pythia-410m,xnli_fr,0-shot,brier_score,0.8510288570363232,
EleutherAI/pythia-410m,xnli_hi,0-shot,brier_score,0.7975820405032517,
EleutherAI/pythia-410m,xnli_ru,0-shot,brier_score,0.8309452187413989,
EleutherAI/pythia-410m,xnli_sw,0-shot,brier_score,0.9813139217874577,
EleutherAI/pythia-410m,xnli_th,0-shot,brier_score,0.7899616470901687,
EleutherAI/pythia-410m,xnli_tr,0-shot,brier_score,0.9799105022880229,
EleutherAI/pythia-410m,xnli_ur,0-shot,brier_score,1.255452729212506,
EleutherAI/pythia-410m,xnli_vi,0-shot,brier_score,0.7819250579178086,
EleutherAI/pythia-410m,xnli_zh,0-shot,brier_score,0.9668177611886649,
EleutherAI/pythia-70m-deduped,anli_r1,0-shot,brier_score,0.84618874446298,
EleutherAI/pythia-70m-deduped,anli_r2,0-shot,brier_score,0.8442903011879083,
EleutherAI/pythia-70m-deduped,anli_r3,0-shot,brier_score,0.8696664870849224,
EleutherAI/pythia-70m-deduped,logiqa2,0-shot,brier_score,1.223584814604129,
EleutherAI/pythia-70m-deduped,mathqa,0-shot,brier_score,1.0515379438631467,
EleutherAI/pythia-70m-deduped,xnli_ar,0-shot,brier_score,1.1454489857093324,
EleutherAI/pythia-70m-deduped,xnli_bg,0-shot,brier_score,1.1649911560435604,
EleutherAI/pythia-70m-deduped,xnli_de,0-shot,brier_score,0.9693149139473257,
EleutherAI/pythia-70m-deduped,xnli_el,0-shot,brier_score,1.3149773944594167,
EleutherAI/pythia-70m-deduped,xnli_en,0-shot,brier_score,0.834686177456504,
EleutherAI/pythia-70m-deduped,xnli_es,0-shot,brier_score,1.1270936122500395,
EleutherAI/pythia-70m-deduped,xnli_fr,0-shot,brier_score,0.9945433194357076,
EleutherAI/pythia-70m-deduped,xnli_hi,0-shot,brier_score,1.0934980622326698,
EleutherAI/pythia-70m-deduped,xnli_ru,0-shot,brier_score,0.9631995176963422,
EleutherAI/pythia-70m-deduped,xnli_sw,0-shot,brier_score,1.0273342003078163,
EleutherAI/pythia-70m-deduped,xnli_th,0-shot,brier_score,1.0305246775995445,
EleutherAI/pythia-70m-deduped,xnli_tr,0-shot,brier_score,1.1383454136781193,
EleutherAI/pythia-70m-deduped,xnli_ur,0-shot,brier_score,1.2568172238149347,
EleutherAI/pythia-70m-deduped,xnli_vi,0-shot,brier_score,1.0647635424789346,
EleutherAI/pythia-70m-deduped,xnli_zh,0-shot,brier_score,1.235776776549947,
EleutherAI/pythia-2.8b-deduped,anli_r1,0-shot,brier_score,0.8054265719261477,
EleutherAI/pythia-2.8b-deduped,anli_r2,0-shot,brier_score,0.8037749000576744,
EleutherAI/pythia-2.8b-deduped,anli_r3,0-shot,brier_score,0.7519467476823256,
EleutherAI/pythia-2.8b-deduped,logiqa2,0-shot,brier_score,1.1767426880813139,
EleutherAI/pythia-2.8b-deduped,mathqa,0-shot,brier_score,0.9572051828708428,
EleutherAI/pythia-2.8b-deduped,xnli_ar,0-shot,brier_score,1.1222663740682257,
EleutherAI/pythia-2.8b-deduped,xnli_bg,0-shot,brier_score,0.9330685720570514,
EleutherAI/pythia-2.8b-deduped,xnli_de,0-shot,brier_score,0.8133194748992477,
EleutherAI/pythia-2.8b-deduped,xnli_el,0-shot,brier_score,0.8478411242364374,
EleutherAI/pythia-2.8b-deduped,xnli_en,0-shot,brier_score,0.6508830704804908,
EleutherAI/pythia-2.8b-deduped,xnli_es,0-shot,brier_score,0.8300496422693401,
EleutherAI/pythia-2.8b-deduped,xnli_fr,0-shot,brier_score,0.7833625137736738,
EleutherAI/pythia-2.8b-deduped,xnli_hi,0-shot,brier_score,0.782797792467933,
EleutherAI/pythia-2.8b-deduped,xnli_ru,0-shot,brier_score,0.7756385072270832,
EleutherAI/pythia-2.8b-deduped,xnli_sw,0-shot,brier_score,0.890166968244077,
EleutherAI/pythia-2.8b-deduped,xnli_th,0-shot,brier_score,0.7923064689410347,
EleutherAI/pythia-2.8b-deduped,xnli_tr,0-shot,brier_score,0.8469738590258294,
EleutherAI/pythia-2.8b-deduped,xnli_ur,0-shot,brier_score,1.008320925531779,
EleutherAI/pythia-2.8b-deduped,xnli_vi,0-shot,brier_score,0.7897670766883305,
EleutherAI/pythia-2.8b-deduped,xnli_zh,0-shot,brier_score,1.134668426667195,
EleutherAI/pythia-1.4b,anli_r1,0-shot,brier_score,0.9124617361534068,
EleutherAI/pythia-1.4b,anli_r2,0-shot,brier_score,0.8694720876494368,
EleutherAI/pythia-1.4b,anli_r3,0-shot,brier_score,0.804560431145024,
EleutherAI/pythia-1.4b,logiqa2,0-shot,brier_score,1.171443291540983,
EleutherAI/pythia-1.4b,mathqa,0-shot,brier_score,0.9979727008408344,
EleutherAI/pythia-1.4b,xnli_ar,0-shot,brier_score,1.2194822216639924,
EleutherAI/pythia-1.4b,xnli_bg,0-shot,brier_score,0.9648766515167643,
EleutherAI/pythia-1.4b,xnli_de,0-shot,brier_score,0.8532046622500541,
EleutherAI/pythia-1.4b,xnli_el,0-shot,brier_score,0.8911747050378601,
EleutherAI/pythia-1.4b,xnli_en,0-shot,brier_score,0.6696892907770583,
EleutherAI/pythia-1.4b,xnli_es,0-shot,brier_score,0.8445473144554614,
EleutherAI/pythia-1.4b,xnli_fr,0-shot,brier_score,0.797044915876603,
EleutherAI/pythia-1.4b,xnli_hi,0-shot,brier_score,0.7705139572729219,
EleutherAI/pythia-1.4b,xnli_ru,0-shot,brier_score,0.8155662684064818,
EleutherAI/pythia-1.4b,xnli_sw,0-shot,brier_score,0.8960871116199123,
EleutherAI/pythia-1.4b,xnli_th,0-shot,brier_score,0.7979554650191145,
EleutherAI/pythia-1.4b,xnli_tr,0-shot,brier_score,0.8493830773822799,
EleutherAI/pythia-1.4b,xnli_ur,0-shot,brier_score,1.0453125532688705,
EleutherAI/pythia-1.4b,xnli_vi,0-shot,brier_score,0.7976252558289455,
EleutherAI/pythia-1.4b,xnli_zh,0-shot,brier_score,0.9672616986183904,
EleutherAI/pythia-12b-deduped,anli_r1,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,anli_r2,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,anli_r3,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,logiqa2,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,mathqa,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_ar,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_bg,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_de,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_el,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_en,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_es,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_fr,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_hi,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_ru,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_sw,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_th,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_tr,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_ur,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_vi,0-shot,brier_score,nan,
EleutherAI/pythia-12b-deduped,xnli_zh,0-shot,brier_score,nan,
EleutherAI/pythia-14m,anli_r1,0-shot,brier_score,0.86196130659241,
EleutherAI/pythia-14m,anli_r2,0-shot,brier_score,0.8833318519541471,
EleutherAI/pythia-14m,anli_r3,0-shot,brier_score,0.8399194323449526,
EleutherAI/pythia-14m,logiqa2,0-shot,brier_score,1.3406486986443875,
EleutherAI/pythia-14m,mathqa,0-shot,brier_score,1.035563812879629,
EleutherAI/pythia-14m,xnli_ar,0-shot,brier_score,1.0356335373851189,
EleutherAI/pythia-14m,xnli_bg,0-shot,brier_score,1.0260567522247561,
EleutherAI/pythia-14m,xnli_de,0-shot,brier_score,1.044635346593406,
EleutherAI/pythia-14m,xnli_el,0-shot,brier_score,1.0965010850497763,
EleutherAI/pythia-14m,xnli_en,0-shot,brier_score,0.8977252076152767,
EleutherAI/pythia-14m,xnli_es,0-shot,brier_score,1.3079798331726602,
EleutherAI/pythia-14m,xnli_fr,0-shot,brier_score,0.9766617950776101,
EleutherAI/pythia-14m,xnli_hi,0-shot,brier_score,1.2808599509026932,
EleutherAI/pythia-14m,xnli_ru,0-shot,brier_score,0.9803690245911869,
EleutherAI/pythia-14m,xnli_sw,0-shot,brier_score,1.0855379129714675,
EleutherAI/pythia-14m,xnli_th,0-shot,brier_score,1.1930099986175904,
EleutherAI/pythia-14m,xnli_tr,0-shot,brier_score,1.2031708296266874,
EleutherAI/pythia-14m,xnli_ur,0-shot,brier_score,1.2753841813794928,
EleutherAI/pythia-14m,xnli_vi,0-shot,brier_score,1.1910671117304006,
EleutherAI/pythia-14m,xnli_zh,0-shot,brier_score,1.0440660244799151,
EleutherAI/gpt-neo-125m,anli_r1,0-shot,brier_score,0.9622492360653291,
EleutherAI/gpt-neo-125m,anli_r2,0-shot,brier_score,0.9548361976549536,
EleutherAI/gpt-neo-125m,anli_r3,0-shot,brier_score,0.8891578780411301,
EleutherAI/gpt-neo-125m,logiqa2,0-shot,brier_score,1.1460934268525604,
EleutherAI/gpt-neo-125m,mathqa,0-shot,brier_score,1.0224773384148746,
EleutherAI/gpt-neo-125m,xnli_ar,0-shot,brier_score,0.8957306684785973,
EleutherAI/gpt-neo-125m,xnli_bg,0-shot,brier_score,1.2387009446415858,
EleutherAI/gpt-neo-125m,xnli_de,0-shot,brier_score,0.9295152791271126,
EleutherAI/gpt-neo-125m,xnli_el,0-shot,brier_score,1.2797794083033087,
EleutherAI/gpt-neo-125m,xnli_en,0-shot,brier_score,0.7190039756238711,
EleutherAI/gpt-neo-125m,xnli_es,0-shot,brier_score,1.0427899297048275,
EleutherAI/gpt-neo-125m,xnli_fr,0-shot,brier_score,0.9206911656059417,
EleutherAI/gpt-neo-125m,xnli_hi,0-shot,brier_score,0.7891559554495032,
EleutherAI/gpt-neo-125m,xnli_ru,0-shot,brier_score,0.9576393746871624,
EleutherAI/gpt-neo-125m,xnli_sw,0-shot,brier_score,1.2813890150716822,
EleutherAI/gpt-neo-125m,xnli_th,0-shot,brier_score,1.0078742843997812,
EleutherAI/gpt-neo-125m,xnli_tr,0-shot,brier_score,0.9095949652003831,
EleutherAI/gpt-neo-125m,xnli_ur,0-shot,brier_score,1.3284008195848354,
EleutherAI/gpt-neo-125m,xnli_vi,0-shot,brier_score,0.8499209329411836,
EleutherAI/gpt-neo-125m,xnli_zh,0-shot,brier_score,0.9929663168650544,
google/gemma-2-2b,anli_r1,0-shot,brier_score,0.6922118186639241,
google/gemma-2-2b,anli_r2,0-shot,brier_score,0.6791496593600604,
google/gemma-2-2b,anli_r3,0-shot,brier_score,0.6667727064663,
google/gemma-2-2b,logiqa2,0-shot,brier_score,0.9978272422405768,
google/gemma-2-2b,mathqa,0-shot,brier_score,0.8444095244243373,
google/gemma-2-2b,xnli_ar,0-shot,brier_score,1.275321188791143,
google/gemma-2-2b,xnli_bg,0-shot,brier_score,0.8198878809580369,
google/gemma-2-2b,xnli_de,0-shot,brier_score,0.7936104382723572,
google/gemma-2-2b,xnli_el,0-shot,brier_score,0.9549307998213671,
google/gemma-2-2b,xnli_en,0-shot,brier_score,0.693918209584472,
google/gemma-2-2b,xnli_es,0-shot,brier_score,0.8416840427369002,
google/gemma-2-2b,xnli_fr,0-shot,brier_score,0.7748182746333067,
google/gemma-2-2b,xnli_hi,0-shot,brier_score,0.7450503730980739,
google/gemma-2-2b,xnli_ru,0-shot,brier_score,0.7259627209186423,
google/gemma-2-2b,xnli_sw,0-shot,brier_score,0.809226623287106,
google/gemma-2-2b,xnli_th,0-shot,brier_score,0.7926070540369429,
google/gemma-2-2b,xnli_tr,0-shot,brier_score,0.8183973734394238,
google/gemma-2-2b,xnli_ur,0-shot,brier_score,1.178530087804445,
google/gemma-2-2b,xnli_vi,0-shot,brier_score,0.7329901680263249,
google/gemma-2-2b,xnli_zh,0-shot,brier_score,0.9712081808215097,
google/gemma-7b,anli_r1,0-shot,brier_score,0.7351424931222604,
google/gemma-7b,anli_r2,0-shot,brier_score,0.7344492988758864,
google/gemma-7b,anli_r3,0-shot,brier_score,0.7276779955889722,
google/gemma-7b,logiqa2,0-shot,brier_score,0.8861672457574569,
google/gemma-7b,mathqa,0-shot,brier_score,0.7777008426633938,
google/gemma-7b,xnli_ar,0-shot,brier_score,1.2573963336253395,
google/gemma-7b,xnli_bg,0-shot,brier_score,0.9052473707651211,
google/gemma-7b,xnli_de,0-shot,brier_score,0.8338987242853484,
google/gemma-7b,xnli_el,0-shot,brier_score,1.0150227629176587,
google/gemma-7b,xnli_en,0-shot,brier_score,0.704058724788044,
google/gemma-7b,xnli_es,0-shot,brier_score,0.9045148085316418,
google/gemma-7b,xnli_fr,0-shot,brier_score,0.7922325316525264,
google/gemma-7b,xnli_hi,0-shot,brier_score,0.8375805241096305,
google/gemma-7b,xnli_ru,0-shot,brier_score,0.7891322694242375,
google/gemma-7b,xnli_sw,0-shot,brier_score,0.9070287523501387,
google/gemma-7b,xnli_th,0-shot,brier_score,0.9305061120758106,
google/gemma-7b,xnli_tr,0-shot,brier_score,0.873174088903731,
google/gemma-7b,xnli_ur,0-shot,brier_score,1.121581506484636,
google/gemma-7b,xnli_vi,0-shot,brier_score,0.8706039114260455,
google/gemma-7b,xnli_zh,0-shot,brier_score,1.040048771473294,
Qwen/Qwen2.5-3B,anli_r1,0-shot,brier_score,0.7057190459872422,
Qwen/Qwen2.5-3B,anli_r2,0-shot,brier_score,0.7345955697732015,
Qwen/Qwen2.5-3B,anli_r3,0-shot,brier_score,0.7071468884690394,
Qwen/Qwen2.5-3B,logiqa2,0-shot,brier_score,0.792272703697499,
Qwen/Qwen2.5-3B,mathqa,0-shot,brier_score,0.8460303784950912,
Qwen/Qwen2.5-3B,xnli_ar,0-shot,brier_score,1.2468522998105116,
Qwen/Qwen2.5-3B,xnli_bg,0-shot,brier_score,0.9590427225499502,
Qwen/Qwen2.5-3B,xnli_de,0-shot,brier_score,0.9353236952939464,
Qwen/Qwen2.5-3B,xnli_el,0-shot,brier_score,1.137713162528639,
Qwen/Qwen2.5-3B,xnli_en,0-shot,brier_score,0.8190600384327849,
Qwen/Qwen2.5-3B,xnli_es,0-shot,brier_score,1.0540419509900376,
Qwen/Qwen2.5-3B,xnli_fr,0-shot,brier_score,0.8783843934674826,
Qwen/Qwen2.5-3B,xnli_hi,0-shot,brier_score,1.1111555489179925,
Qwen/Qwen2.5-3B,xnli_ru,0-shot,brier_score,0.8850924700338371,
Qwen/Qwen2.5-3B,xnli_sw,0-shot,brier_score,1.0337986105694354,
Qwen/Qwen2.5-3B,xnli_th,0-shot,brier_score,0.8977315765983429,
Qwen/Qwen2.5-3B,xnli_tr,0-shot,brier_score,0.985617514502125,
Qwen/Qwen2.5-3B,xnli_ur,0-shot,brier_score,1.2761517336215404,
Qwen/Qwen2.5-3B,xnli_vi,0-shot,brier_score,0.9289523435509678,
Qwen/Qwen2.5-3B,xnli_zh,0-shot,brier_score,1.0578870389654262,
Qwen/Qwen1.5-32B,anli_r1,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,anli_r2,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,anli_r3,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,logiqa2,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,mathqa,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_ar,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_bg,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_de,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_el,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_en,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_es,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_fr,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_hi,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_ru,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_sw,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_th,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_tr,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_ur,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_vi,0-shot,brier_score,nan,
Qwen/Qwen1.5-32B,xnli_zh,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,anli_r1,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,anli_r2,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,anli_r3,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,logiqa2,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,mathqa,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_ar,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_bg,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_de,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_el,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_en,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_es,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_fr,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_hi,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_ru,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_sw,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_th,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_tr,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_ur,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_vi,0-shot,brier_score,nan,
Qwen/Qwen1.5-14B,xnli_zh,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,anli_r1,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,anli_r2,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,anli_r3,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,logiqa2,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,mathqa,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_ar,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_bg,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_de,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_el,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_en,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_es,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_fr,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_hi,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_ru,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_sw,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_th,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_tr,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_ur,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_vi,0-shot,brier_score,nan,
Qwen/Qwen2.5-32B,xnli_zh,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,anli_r1,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,anli_r2,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,anli_r3,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,logiqa2,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,mathqa,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_ar,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_bg,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_de,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_el,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_en,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_es,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_fr,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_hi,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_ru,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_sw,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_th,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_tr,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_ur,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_vi,0-shot,brier_score,nan,
Qwen/Qwen2.5-14B,xnli_zh,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,anli_r1,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,anli_r2,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,anli_r3,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,logiqa2,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,mathqa,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_ar,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_bg,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_de,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_el,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_en,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_es,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_fr,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_hi,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_ru,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_sw,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_th,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_tr,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_ur,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_vi,0-shot,brier_score,nan,
meta-llama/Llama-2-13b-hf,xnli_zh,0-shot,brier_score,nan,
cerebras/Cerebras-GPT-13B,anli_r1,0-shot,brier_score,0.9829881363103726,
cerebras/Cerebras-GPT-13B,anli_r2,0-shot,brier_score,0.9494427338704345,
cerebras/Cerebras-GPT-13B,anli_r3,0-shot,brier_score,0.9325075332431227,
cerebras/Cerebras-GPT-13B,mathqa,0-shot,brier_score,1.0246758388933506,
cerebras/Cerebras-GPT-13B,xnli_ar,0-shot,brier_score,1.221744010533068,
cerebras/Cerebras-GPT-13B,xnli_bg,0-shot,brier_score,1.087635872087599,
cerebras/Cerebras-GPT-13B,xnli_de,0-shot,brier_score,1.2478321775739627,
cerebras/Cerebras-GPT-13B,xnli_el,0-shot,brier_score,1.0076504290503356,
cerebras/Cerebras-GPT-13B,xnli_en,0-shot,brier_score,0.8104565938851825,
cerebras/Cerebras-GPT-13B,xnli_es,0-shot,brier_score,1.3329923911474466,
cerebras/Cerebras-GPT-13B,xnli_fr,0-shot,brier_score,1.3163221899624866,
cerebras/Cerebras-GPT-13B,xnli_hi,0-shot,brier_score,1.3262858432286815,
cerebras/Cerebras-GPT-13B,xnli_ru,0-shot,brier_score,1.0857177707028212,
cerebras/Cerebras-GPT-13B,xnli_sw,0-shot,brier_score,1.184050193990361,
cerebras/Cerebras-GPT-13B,xnli_th,0-shot,brier_score,1.0808255365347776,
cerebras/Cerebras-GPT-13B,xnli_tr,0-shot,brier_score,1.2826028710892114,
cerebras/Cerebras-GPT-13B,xnli_ur,0-shot,brier_score,1.2787494425211892,
cerebras/Cerebras-GPT-13B,xnli_vi,0-shot,brier_score,1.1172506930300632,
cerebras/Cerebras-GPT-13B,xnli_zh,0-shot,brier_score,1.2053069376640628,
Salesforce/codegen-6B-nl,anli_r1,0-shot,brier_score,0.8244078920096608,
Salesforce/codegen-6B-nl,anli_r2,0-shot,brier_score,0.8353679775400464,
Salesforce/codegen-6B-nl,anli_r3,0-shot,brier_score,0.8069712718890119,
Salesforce/codegen-6B-nl,logiqa2,0-shot,brier_score,1.1868574679576918,
Salesforce/codegen-6B-nl,mathqa,0-shot,brier_score,0.9502372507926286,
Salesforce/codegen-6B-nl,xnli_ar,0-shot,brier_score,1.158668865269312,
Salesforce/codegen-6B-nl,xnli_bg,0-shot,brier_score,0.7434879581839554,
Salesforce/codegen-6B-nl,xnli_de,0-shot,brier_score,0.8254749730396426,
Salesforce/codegen-6B-nl,xnli_el,0-shot,brier_score,0.8958281928372329,
Salesforce/codegen-6B-nl,xnli_en,0-shot,brier_score,0.6452633479292383,
Salesforce/codegen-6B-nl,xnli_es,0-shot,brier_score,0.8067938602478157,
Salesforce/codegen-6B-nl,xnli_fr,0-shot,brier_score,0.7450259321870722,
Salesforce/codegen-6B-nl,xnli_hi,0-shot,brier_score,0.7459636948049773,
Salesforce/codegen-6B-nl,xnli_ru,0-shot,brier_score,0.7811098512755507,
Salesforce/codegen-6B-nl,xnli_sw,0-shot,brier_score,0.8448533149101904,
Salesforce/codegen-6B-nl,xnli_th,0-shot,brier_score,0.7636295114580167,
Salesforce/codegen-6B-nl,xnli_tr,0-shot,brier_score,0.8732320791786231,
Salesforce/codegen-6B-nl,xnli_ur,0-shot,brier_score,1.0240493250324039,
Salesforce/codegen-6B-nl,xnli_vi,0-shot,brier_score,0.7679678179269834,
Salesforce/codegen-6B-nl,xnli_zh,0-shot,brier_score,1.0769558598785194,
Salesforce/codegen-2B-nl,anli_r1,0-shot,brier_score,0.8466748163182493,
Salesforce/codegen-2B-nl,anli_r2,0-shot,brier_score,0.842656698136594,
Salesforce/codegen-2B-nl,anli_r3,0-shot,brier_score,0.8003132006944266,
Salesforce/codegen-2B-nl,logiqa2,0-shot,brier_score,1.1282012575996851,
Salesforce/codegen-2B-nl,mathqa,0-shot,brier_score,0.9386145865138823,
Salesforce/codegen-2B-nl,xnli_ar,0-shot,brier_score,1.1600449543007592,
Salesforce/codegen-2B-nl,xnli_bg,0-shot,brier_score,0.8634338204863551,
Salesforce/codegen-2B-nl,xnli_de,0-shot,brier_score,0.8311668391402853,
Salesforce/codegen-2B-nl,xnli_el,0-shot,brier_score,0.9617928955125609,
Salesforce/codegen-2B-nl,xnli_en,0-shot,brier_score,0.6438032801872805,
Salesforce/codegen-2B-nl,xnli_es,0-shot,brier_score,0.8078448057461889,
Salesforce/codegen-2B-nl,xnli_fr,0-shot,brier_score,0.7594704215990777,
Salesforce/codegen-2B-nl,xnli_hi,0-shot,brier_score,0.7808486981455898,
Salesforce/codegen-2B-nl,xnli_ru,0-shot,brier_score,0.7418056621509782,
Salesforce/codegen-2B-nl,xnli_sw,0-shot,brier_score,0.8721529647628053,
Salesforce/codegen-2B-nl,xnli_th,0-shot,brier_score,0.8232587721086458,
Salesforce/codegen-2B-nl,xnli_tr,0-shot,brier_score,0.7879820218439921,
Salesforce/codegen-2B-nl,xnli_ur,0-shot,brier_score,1.0663952650864756,
Salesforce/codegen-2B-nl,xnli_vi,0-shot,brier_score,0.8329243743439266,
Salesforce/codegen-2B-nl,xnli_zh,0-shot,brier_score,0.9191167708286697,
Salesforce/codegen-350M-nl,anli_r1,0-shot,brier_score,0.7832755947741512,
Salesforce/codegen-350M-nl,anli_r2,0-shot,brier_score,0.7721366117671389,
Salesforce/codegen-350M-nl,anli_r3,0-shot,brier_score,0.7598347852911768,
Salesforce/codegen-350M-nl,logiqa2,0-shot,brier_score,1.162699031890885,
Salesforce/codegen-350M-nl,mathqa,0-shot,brier_score,1.0192569876875879,
Salesforce/codegen-350M-nl,xnli_ar,0-shot,brier_score,1.1176099900541623,
Salesforce/codegen-350M-nl,xnli_bg,0-shot,brier_score,0.9437329432975828,
Salesforce/codegen-350M-nl,xnli_de,0-shot,brier_score,0.8867987787470919,
Salesforce/codegen-350M-nl,xnli_el,0-shot,brier_score,1.1100257436801613,
Salesforce/codegen-350M-nl,xnli_en,0-shot,brier_score,0.6538106416271013,
Salesforce/codegen-350M-nl,xnli_es,0-shot,brier_score,0.9467145794211785,
Salesforce/codegen-350M-nl,xnli_fr,0-shot,brier_score,0.8427476274096662,
Salesforce/codegen-350M-nl,xnli_hi,0-shot,brier_score,0.8593055397343814,
Salesforce/codegen-350M-nl,xnli_ru,0-shot,brier_score,0.8005696607277384,
Salesforce/codegen-350M-nl,xnli_sw,0-shot,brier_score,1.0263449270309604,
Salesforce/codegen-350M-nl,xnli_th,0-shot,brier_score,0.8702486844525543,
Salesforce/codegen-350M-nl,xnli_tr,0-shot,brier_score,1.0305454289112623,
Salesforce/codegen-350M-nl,xnli_ur,0-shot,brier_score,1.3214177688135904,
Salesforce/codegen-350M-nl,xnli_vi,0-shot,brier_score,1.0128766281584527,
Salesforce/codegen-350M-nl,xnli_zh,0-shot,brier_score,0.9549835516552038,
microsoft/phi-1_5,anli_r1,0-shot,brier_score,nan,
microsoft/phi-1_5,anli_r2,0-shot,brier_score,nan,
microsoft/phi-1_5,anli_r3,0-shot,brier_score,nan,
microsoft/phi-1_5,logiqa2,0-shot,brier_score,nan,
microsoft/phi-1_5,mathqa,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_ar,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_bg,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_de,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_el,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_en,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_es,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_fr,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_hi,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_ru,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_sw,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_th,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_tr,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_ur,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_vi,0-shot,brier_score,nan,
microsoft/phi-1_5,xnli_zh,0-shot,brier_score,nan,
HuggingFaceTB/SmolLM-360M,anli_r1,0-shot,brier_score,0.7744403881187941,
HuggingFaceTB/SmolLM-360M,anli_r2,0-shot,brier_score,0.762191950807321,
HuggingFaceTB/SmolLM-360M,anli_r3,0-shot,brier_score,0.7402894981274437,
HuggingFaceTB/SmolLM-360M,logiqa2,0-shot,brier_score,1.0837861925375507,
HuggingFaceTB/SmolLM-360M,mathqa,0-shot,brier_score,0.9570749958198153,
HuggingFaceTB/SmolLM-360M,xnli_ar,0-shot,brier_score,0.9189521068056412,
HuggingFaceTB/SmolLM-360M,xnli_bg,0-shot,brier_score,1.2113465946470032,
HuggingFaceTB/SmolLM-360M,xnli_de,0-shot,brier_score,0.8238229602303524,
HuggingFaceTB/SmolLM-360M,xnli_el,0-shot,brier_score,1.0261902340578992,
HuggingFaceTB/SmolLM-360M,xnli_en,0-shot,brier_score,0.6911061498386665,
HuggingFaceTB/SmolLM-360M,xnli_es,0-shot,brier_score,1.000360514408625,
HuggingFaceTB/SmolLM-360M,xnli_fr,0-shot,brier_score,0.9062288085886576,
HuggingFaceTB/SmolLM-360M,xnli_hi,0-shot,brier_score,1.0219160445502224,
HuggingFaceTB/SmolLM-360M,xnli_ru,0-shot,brier_score,0.8189287166423925,
HuggingFaceTB/SmolLM-360M,xnli_sw,0-shot,brier_score,1.0458361366916742,
HuggingFaceTB/SmolLM-360M,xnli_th,0-shot,brier_score,1.0875334562557126,
HuggingFaceTB/SmolLM-360M,xnli_tr,0-shot,brier_score,0.8654235812265006,
HuggingFaceTB/SmolLM-360M,xnli_ur,0-shot,brier_score,1.3264840949462546,
HuggingFaceTB/SmolLM-360M,xnli_vi,0-shot,brier_score,1.1288648438572513,
HuggingFaceTB/SmolLM-360M,xnli_zh,0-shot,brier_score,1.2314959286521574,
HuggingFaceTB/SmolLM-135M,anli_r1,0-shot,brier_score,0.9448508185716854,
HuggingFaceTB/SmolLM-135M,anli_r2,0-shot,brier_score,0.9191197374401392,
HuggingFaceTB/SmolLM-135M,anli_r3,0-shot,brier_score,0.8950339396000206,
HuggingFaceTB/SmolLM-135M,logiqa2,0-shot,brier_score,1.164592528072951,
HuggingFaceTB/SmolLM-135M,mathqa,0-shot,brier_score,0.9792664070602055,
HuggingFaceTB/SmolLM-135M,xnli_ar,0-shot,brier_score,0.9122007687357555,
HuggingFaceTB/SmolLM-135M,xnli_bg,0-shot,brier_score,1.0306533551794366,
HuggingFaceTB/SmolLM-135M,xnli_de,0-shot,brier_score,0.8263938959427168,
HuggingFaceTB/SmolLM-135M,xnli_el,0-shot,brier_score,0.9392356341521358,
HuggingFaceTB/SmolLM-135M,xnli_en,0-shot,brier_score,0.7203207983576004,
HuggingFaceTB/SmolLM-135M,xnli_es,0-shot,brier_score,1.0375029153150297,
HuggingFaceTB/SmolLM-135M,xnli_fr,0-shot,brier_score,0.9144907514712247,
HuggingFaceTB/SmolLM-135M,xnli_hi,0-shot,brier_score,0.9307079937549121,
HuggingFaceTB/SmolLM-135M,xnli_ru,0-shot,brier_score,0.9100107772355447,
HuggingFaceTB/SmolLM-135M,xnli_sw,0-shot,brier_score,1.04624364103704,
HuggingFaceTB/SmolLM-135M,xnli_th,0-shot,brier_score,1.0461643965896628,
HuggingFaceTB/SmolLM-135M,xnli_tr,0-shot,brier_score,1.0079105950038227,
HuggingFaceTB/SmolLM-135M,xnli_ur,0-shot,brier_score,1.3139018760598151,
HuggingFaceTB/SmolLM-135M,xnli_vi,0-shot,brier_score,1.3089220481358328,
HuggingFaceTB/SmolLM-135M,xnli_zh,0-shot,brier_score,1.3195059311041935,
HuggingFaceTB/SmolLM-1.7B,anli_r1,0-shot,brier_score,0.8380929652439938,
HuggingFaceTB/SmolLM-1.7B,anli_r2,0-shot,brier_score,0.8144458376257794,
HuggingFaceTB/SmolLM-1.7B,anli_r3,0-shot,brier_score,0.7983700636432682,
HuggingFaceTB/SmolLM-1.7B,logiqa2,0-shot,brier_score,1.050757415309663,
HuggingFaceTB/SmolLM-1.7B,mathqa,0-shot,brier_score,0.924862598354004,
HuggingFaceTB/SmolLM-1.7B,xnli_ar,0-shot,brier_score,0.9698963991863401,
HuggingFaceTB/SmolLM-1.7B,xnli_bg,0-shot,brier_score,0.9690789799401124,
HuggingFaceTB/SmolLM-1.7B,xnli_de,0-shot,brier_score,0.9088984527335026,
HuggingFaceTB/SmolLM-1.7B,xnli_el,0-shot,brier_score,1.0566293855736777,
HuggingFaceTB/SmolLM-1.7B,xnli_en,0-shot,brier_score,0.6718924938378293,
HuggingFaceTB/SmolLM-1.7B,xnli_es,0-shot,brier_score,0.9266742857580375,
HuggingFaceTB/SmolLM-1.7B,xnli_fr,0-shot,brier_score,0.8293844732465998,
HuggingFaceTB/SmolLM-1.7B,xnli_hi,0-shot,brier_score,0.9327786617840376,
HuggingFaceTB/SmolLM-1.7B,xnli_ru,0-shot,brier_score,0.8467059094628328,
HuggingFaceTB/SmolLM-1.7B,xnli_sw,0-shot,brier_score,0.8964542166898899,
HuggingFaceTB/SmolLM-1.7B,xnli_th,0-shot,brier_score,0.9811479529753393,
HuggingFaceTB/SmolLM-1.7B,xnli_tr,0-shot,brier_score,0.9389545036969975,
HuggingFaceTB/SmolLM-1.7B,xnli_ur,0-shot,brier_score,1.2619967622341257,
HuggingFaceTB/SmolLM-1.7B,xnli_vi,0-shot,brier_score,1.1044543985145796,
HuggingFaceTB/SmolLM-1.7B,xnli_zh,0-shot,brier_score,0.9877425176266813,
NinedayWang/PolyCoder-2.7B,anli_r1,0-shot,brier_score,1.035323393569203,
NinedayWang/PolyCoder-2.7B,anli_r2,0-shot,brier_score,1.0318087395566509,
NinedayWang/PolyCoder-2.7B,anli_r3,0-shot,brier_score,1.0179966956441433,
NinedayWang/PolyCoder-2.7B,logiqa2,0-shot,brier_score,1.0801294144806082,
NinedayWang/PolyCoder-2.7B,mathqa,0-shot,brier_score,1.0232642648711627,
NinedayWang/PolyCoder-2.7B,xnli_ar,0-shot,brier_score,0.869976186883612,
NinedayWang/PolyCoder-2.7B,xnli_bg,0-shot,brier_score,1.1321112693445037,
NinedayWang/PolyCoder-2.7B,xnli_de,0-shot,brier_score,0.9818235830053477,
NinedayWang/PolyCoder-2.7B,xnli_el,0-shot,brier_score,1.259852161478483,
NinedayWang/PolyCoder-2.7B,xnli_en,0-shot,brier_score,0.7996282156973187,
NinedayWang/PolyCoder-2.7B,xnli_es,0-shot,brier_score,1.1379677665383798,
NinedayWang/PolyCoder-2.7B,xnli_fr,0-shot,brier_score,1.0531836366021692,
NinedayWang/PolyCoder-2.7B,xnli_hi,0-shot,brier_score,1.1511390281806206,
NinedayWang/PolyCoder-2.7B,xnli_ru,0-shot,brier_score,0.8180355929168044,
NinedayWang/PolyCoder-2.7B,xnli_sw,0-shot,brier_score,1.0225597940734914,
NinedayWang/PolyCoder-2.7B,xnli_th,0-shot,brier_score,1.1059382230490835,
NinedayWang/PolyCoder-2.7B,xnli_tr,0-shot,brier_score,0.8745321529098302,
NinedayWang/PolyCoder-2.7B,xnli_ur,0-shot,brier_score,1.320901783124348,
NinedayWang/PolyCoder-2.7B,xnli_vi,0-shot,brier_score,1.256078426281492,
NinedayWang/PolyCoder-2.7B,xnli_zh,0-shot,brier_score,0.9373495874793953,
mosaicml/mpt-30b,anli_r1,0-shot,brier_score,0.7654155968523039,
mosaicml/mpt-30b,anli_r2,0-shot,brier_score,0.7403697865616283,
mosaicml/mpt-30b,anli_r3,0-shot,brier_score,0.7115859068155269,
mosaicml/mpt-30b,logiqa2,0-shot,brier_score,0.9299918879430111,
mosaicml/mpt-30b,mathqa,0-shot,brier_score,0.8901540790890418,
mosaicml/mpt-30b,xnli_ar,0-shot,brier_score,1.2354071845524182,
mosaicml/mpt-30b,xnli_bg,0-shot,brier_score,0.8897305677766573,
mosaicml/mpt-30b,xnli_de,0-shot,brier_score,0.8665262703051413,
mosaicml/mpt-30b,xnli_el,0-shot,brier_score,1.0637448704381534,
mosaicml/mpt-30b,xnli_en,0-shot,brier_score,0.6887054591099756,
mosaicml/mpt-30b,xnli_es,0-shot,brier_score,0.8751435744595971,
mosaicml/mpt-30b,xnli_fr,0-shot,brier_score,0.8150411801757018,
mosaicml/mpt-30b,xnli_hi,0-shot,brier_score,0.9174855260987378,
mosaicml/mpt-30b,xnli_ru,0-shot,brier_score,0.7777861895433706,
mosaicml/mpt-30b,xnli_sw,0-shot,brier_score,0.881915624913882,
mosaicml/mpt-30b,xnli_th,0-shot,brier_score,1.0082962037060197,
mosaicml/mpt-30b,xnli_tr,0-shot,brier_score,0.8468006808192986,
mosaicml/mpt-30b,xnli_ur,0-shot,brier_score,1.1616178833650177,
mosaicml/mpt-30b,xnli_vi,0-shot,brier_score,0.9359266886950646,
mosaicml/mpt-30b,xnli_zh,0-shot,brier_score,1.019870757922932,
01-ai/Yi-9B,anli_r1,0-shot,brier_score,0.5992915448608958,
01-ai/Yi-9B,anli_r2,0-shot,brier_score,0.6477493365483759,
01-ai/Yi-9B,anli_r3,0-shot,brier_score,0.6842018063682501,
01-ai/Yi-9B,logiqa2,0-shot,brier_score,0.9458032757856502,
01-ai/Yi-9B,mathqa,0-shot,brier_score,0.7729848357448853,
01-ai/Yi-9B,xnli_ar,0-shot,brier_score,1.074928120601521,
01-ai/Yi-9B,xnli_bg,0-shot,brier_score,1.0639564468626193,
01-ai/Yi-9B,xnli_de,0-shot,brier_score,0.7951015653515845,
01-ai/Yi-9B,xnli_el,0-shot,brier_score,0.7908353201686236,
01-ai/Yi-9B,xnli_en,0-shot,brier_score,0.6492377705386986,
01-ai/Yi-9B,xnli_es,0-shot,brier_score,0.8332407842602538,
01-ai/Yi-9B,xnli_fr,0-shot,brier_score,0.8082028778653751,
01-ai/Yi-9B,xnli_hi,0-shot,brier_score,0.905492651812963,
01-ai/Yi-9B,xnli_ru,0-shot,brier_score,0.93144527698822,
01-ai/Yi-9B,xnli_sw,0-shot,brier_score,0.8803834492225039,
01-ai/Yi-9B,xnli_th,0-shot,brier_score,0.8389866307114463,
01-ai/Yi-9B,xnli_tr,0-shot,brier_score,0.9511385651552083,
01-ai/Yi-9B,xnli_ur,0-shot,brier_score,1.2922603905273808,
01-ai/Yi-9B,xnli_vi,0-shot,brier_score,0.8983805138025034,
01-ai/Yi-9B,xnli_zh,0-shot,brier_score,0.9972237332119696,
01-ai/Yi-34B,anli_r1,0-shot,brier_score,0.5673694235181767,
01-ai/Yi-34B,anli_r2,0-shot,brier_score,0.6210721171762478,
01-ai/Yi-34B,anli_r3,0-shot,brier_score,0.6154796088990331,
01-ai/Yi-34B,logiqa2,0-shot,brier_score,0.7050496208136091,
01-ai/Yi-34B,mathqa,0-shot,brier_score,0.7127439694487179,
01-ai/Yi-34B,xnli_ar,0-shot,brier_score,1.0515934665002846,
01-ai/Yi-34B,xnli_bg,0-shot,brier_score,0.8658008886258591,
01-ai/Yi-34B,xnli_de,0-shot,brier_score,0.7745044644800935,
01-ai/Yi-34B,xnli_el,0-shot,brier_score,0.8331637882418294,
01-ai/Yi-34B,xnli_en,0-shot,brier_score,0.6741966595977996,
01-ai/Yi-34B,xnli_es,0-shot,brier_score,0.8140204412856384,
01-ai/Yi-34B,xnli_fr,0-shot,brier_score,0.7589599440081976,
01-ai/Yi-34B,xnli_hi,0-shot,brier_score,0.8378508531917391,
01-ai/Yi-34B,xnli_ru,0-shot,brier_score,0.8178377821056645,
01-ai/Yi-34B,xnli_sw,0-shot,brier_score,0.9040175415393078,
01-ai/Yi-34B,xnli_th,0-shot,brier_score,0.8925298548005901,
01-ai/Yi-34B,xnli_tr,0-shot,brier_score,0.8270109287786306,
01-ai/Yi-34B,xnli_ur,0-shot,brier_score,1.2044820768760847,
01-ai/Yi-34B,xnli_vi,0-shot,brier_score,0.7867217346125692,
01-ai/Yi-34B,xnli_zh,0-shot,brier_score,1.0523447395896952,
01-ai/Yi-34B-200K,anli_r1,0-shot,brier_score,0.594207407902069,
01-ai/Yi-34B-200K,anli_r2,0-shot,brier_score,0.6369372723314054,
01-ai/Yi-34B-200K,anli_r3,0-shot,brier_score,0.63329383486102,
01-ai/Yi-34B-200K,logiqa2,0-shot,brier_score,0.7120140051595713,
01-ai/Yi-34B-200K,mathqa,0-shot,brier_score,0.7104244045215328,
01-ai/Yi-34B-200K,xnli_ar,0-shot,brier_score,1.0926430393068953,
01-ai/Yi-34B-200K,xnli_bg,0-shot,brier_score,0.8789149654838301,
01-ai/Yi-34B-200K,xnli_de,0-shot,brier_score,0.7763617033033662,
01-ai/Yi-34B-200K,xnli_el,0-shot,brier_score,0.9005444899245502,
01-ai/Yi-34B-200K,xnli_en,0-shot,brier_score,0.6597267889715761,
01-ai/Yi-34B-200K,xnli_es,0-shot,brier_score,0.796134723952561,
01-ai/Yi-34B-200K,xnli_fr,0-shot,brier_score,0.786837837946139,
01-ai/Yi-34B-200K,xnli_hi,0-shot,brier_score,0.9181140151129877,
01-ai/Yi-34B-200K,xnli_ru,0-shot,brier_score,0.7355250105165524,
01-ai/Yi-34B-200K,xnli_sw,0-shot,brier_score,0.8497804843146121,
01-ai/Yi-34B-200K,xnli_th,0-shot,brier_score,0.8402005475891648,
01-ai/Yi-34B-200K,xnli_tr,0-shot,brier_score,0.7743678412719037,
01-ai/Yi-34B-200K,xnli_ur,0-shot,brier_score,1.1395651858640607,
01-ai/Yi-34B-200K,xnli_vi,0-shot,brier_score,0.774378584926305,
01-ai/Yi-34B-200K,xnli_zh,0-shot,brier_score,0.9429489252281595,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,anli_r1,0-shot,brier_score,0.9754516569844279,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,anli_r2,0-shot,brier_score,0.9620995466442864,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,anli_r3,0-shot,brier_score,0.9581436229493746,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,logiqa2,0-shot,brier_score,1.1242641197347254,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,mathqa,0-shot,brier_score,1.024405976800741,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_ar,0-shot,brier_score,0.9508899611466485,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_bg,0-shot,brier_score,1.084985871096817,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_de,0-shot,brier_score,0.9084453122000963,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_el,0-shot,brier_score,1.2579248539464813,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_en,0-shot,brier_score,0.7262764320277776,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_es,0-shot,brier_score,1.192938528318084,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_fr,0-shot,brier_score,0.9233664665637669,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_hi,0-shot,brier_score,0.9053445535840215,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_ru,0-shot,brier_score,0.8492643141372731,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_sw,0-shot,brier_score,1.0441150785543194,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_th,0-shot,brier_score,0.8513774544030811,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_tr,0-shot,brier_score,0.8371025072015056,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_ur,0-shot,brier_score,1.155595653199475,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_vi,0-shot,brier_score,1.0380080076942502,
/data/tir/projects/tir5/users/mengyan3/dolma_checkpts/llama2_460M_nl_code_shuf-hf,xnli_zh,0-shot,brier_score,1.067567640595577,
llama2_220M_nl_only_shuf-hf,anli_r1,0-shot,brier_score,0.9408528443872346,
llama2_220M_nl_only_shuf-hf,anli_r2,0-shot,brier_score,0.9469990238789766,
llama2_220M_nl_only_shuf-hf,anli_r3,0-shot,brier_score,0.9510673427291049,
llama2_220M_nl_only_shuf-hf,logiqa2,0-shot,brier_score,1.1075991315250615,
llama2_220M_nl_only_shuf-hf,mathqa,0-shot,brier_score,1.0360415112287131,
llama2_220M_nl_only_shuf-hf,xnli_ar,0-shot,brier_score,0.9294599701406733,
llama2_220M_nl_only_shuf-hf,xnli_bg,0-shot,brier_score,1.2423873282271456,
llama2_220M_nl_only_shuf-hf,xnli_de,0-shot,brier_score,0.9027859194132087,
llama2_220M_nl_only_shuf-hf,xnli_el,0-shot,brier_score,0.8875121506237424,
llama2_220M_nl_only_shuf-hf,xnli_en,0-shot,brier_score,0.7360195008123878,
llama2_220M_nl_only_shuf-hf,xnli_es,0-shot,brier_score,1.1272289578296562,
llama2_220M_nl_only_shuf-hf,xnli_fr,0-shot,brier_score,0.910925635163094,
llama2_220M_nl_only_shuf-hf,xnli_hi,0-shot,brier_score,1.3006584007555146,
llama2_220M_nl_only_shuf-hf,xnli_ru,0-shot,brier_score,1.1098562870678697,
llama2_220M_nl_only_shuf-hf,xnli_sw,0-shot,brier_score,1.0547240230672577,
llama2_220M_nl_only_shuf-hf,xnli_th,0-shot,brier_score,0.9999073398544283,
llama2_220M_nl_only_shuf-hf,xnli_tr,0-shot,brier_score,1.3054789569098169,
llama2_220M_nl_only_shuf-hf,xnli_ur,0-shot,brier_score,1.3238925860702087,
llama2_220M_nl_only_shuf-hf,xnli_vi,0-shot,brier_score,1.1280955560275951,
llama2_220M_nl_only_shuf-hf,xnli_zh,0-shot,brier_score,1.2511954643032939,
llama2_220M_nl_40_code_60,mathqa,0-shot,brier_score,1.0390906605332784,
llama2_220M_nl_20_code_80,mathqa,0-shot,brier_score,1.022392446218242,
llama2_220M_nl_0_code_100,mathqa,0-shot,brier_score,1.0060822192862073,
Dampish/StellarX-4B-V0,anli_r1,0-shot,brier_score,0.7853559884018767,
Dampish/StellarX-4B-V0,anli_r2,0-shot,brier_score,0.7501894269025156,
Dampish/StellarX-4B-V0,anli_r3,0-shot,brier_score,0.7187707400729225,
Dampish/StellarX-4B-V0,logiqa2,0-shot,brier_score,1.1020544749775099,
Dampish/StellarX-4B-V0,mathqa,0-shot,brier_score,0.970262982067276,
Dampish/StellarX-4B-V0,xnli_ar,0-shot,brier_score,0.9918876040618517,
Dampish/StellarX-4B-V0,xnli_bg,0-shot,brier_score,0.8693681402513717,
Dampish/StellarX-4B-V0,xnli_de,0-shot,brier_score,0.9654886860360751,
Dampish/StellarX-4B-V0,xnli_el,0-shot,brier_score,1.3176945830655935,
Dampish/StellarX-4B-V0,xnli_en,0-shot,brier_score,0.6110928563938816,
Dampish/StellarX-4B-V0,xnli_es,0-shot,brier_score,0.8854900464068887,
Dampish/StellarX-4B-V0,xnli_fr,0-shot,brier_score,0.8068540782224392,
Dampish/StellarX-4B-V0,xnli_hi,0-shot,brier_score,0.8559204094878804,
Dampish/StellarX-4B-V0,xnli_ru,0-shot,brier_score,0.8512461802917572,
Dampish/StellarX-4B-V0,xnli_sw,0-shot,brier_score,0.9563642237418547,
Dampish/StellarX-4B-V0,xnli_th,0-shot,brier_score,1.2333350310773676,
Dampish/StellarX-4B-V0,xnli_tr,0-shot,brier_score,0.9272282829266268,
Dampish/StellarX-4B-V0,xnli_ur,0-shot,brier_score,1.1344972218475662,
Dampish/StellarX-4B-V0,xnli_vi,0-shot,brier_score,1.301654611904121,
Dampish/StellarX-4B-V0,xnli_zh,0-shot,brier_score,1.2192790731005505,
facebook/opt-6.7b,anli_r1,0-shot,brier_score,0.7350012374045926,
facebook/opt-6.7b,anli_r2,0-shot,brier_score,0.7137322911370647,
facebook/opt-6.7b,anli_r3,0-shot,brier_score,0.7302723034432603,
facebook/opt-6.7b,logiqa2,0-shot,brier_score,1.1253559309998244,
facebook/opt-6.7b,mathqa,0-shot,brier_score,0.9749868838154989,
facebook/opt-6.7b,xnli_ar,0-shot,brier_score,0.9331789394410451,
facebook/opt-6.7b,xnli_bg,0-shot,brier_score,0.7658184846834938,
facebook/opt-6.7b,xnli_de,0-shot,brier_score,0.8656475159599006,
facebook/opt-6.7b,xnli_el,0-shot,brier_score,1.0486956300329149,
facebook/opt-6.7b,xnli_en,0-shot,brier_score,0.6113638969760262,
facebook/opt-6.7b,xnli_es,0-shot,brier_score,0.8343426727446113,
facebook/opt-6.7b,xnli_fr,0-shot,brier_score,0.7736146779331431,
facebook/opt-6.7b,xnli_hi,0-shot,brier_score,0.9043777922288855,
facebook/opt-6.7b,xnli_ru,0-shot,brier_score,0.8058511014121713,
facebook/opt-6.7b,xnli_sw,0-shot,brier_score,1.0318389881019738,
facebook/opt-6.7b,xnli_th,0-shot,brier_score,1.2696007823399533,
facebook/opt-6.7b,xnli_tr,0-shot,brier_score,0.9224041271854614,
facebook/opt-6.7b,xnli_ur,0-shot,brier_score,1.2906413710419729,
facebook/opt-6.7b,xnli_vi,0-shot,brier_score,0.971043820648589,
facebook/opt-6.7b,xnli_zh,0-shot,brier_score,1.2137398155422976,
Qwen/Qwen2.5-72B,mmlu_formal_logic,0-shot,accuracy,0.7142857142857143,0.040406101782088394
Qwen/Qwen2.5-72B,mmlu_high_school_european_history,0-shot,accuracy,0.8787878787878788,0.02548549837334322
Qwen/Qwen2.5-72B,mmlu_high_school_us_history,0-shot,accuracy,0.946078431372549,0.01585246528110692
Qwen/Qwen2.5-72B,mmlu_high_school_world_history,0-shot,accuracy,0.9282700421940928,0.01679698961111959
Qwen/Qwen2.5-72B,mmlu_international_law,0-shot,accuracy,0.9173553719008265,0.025135382356604227
Qwen/Qwen2.5-72B,mmlu_jurisprudence,0-shot,accuracy,0.8981481481481481,0.02923927267563274
Qwen/Qwen2.5-72B,mmlu_logical_fallacies,0-shot,accuracy,0.8895705521472392,0.024624937788941315
Qwen/Qwen2.5-72B,mmlu_moral_disputes,0-shot,accuracy,0.8554913294797688,0.01892976451346872
Qwen/Qwen2.5-72B,mmlu_moral_scenarios,0-shot,accuracy,0.6558659217877095,0.015889221313307094
Qwen/Qwen2.5-72B,mmlu_philosophy,0-shot,accuracy,0.8585209003215434,0.019794326658090552
Qwen/Qwen2.5-72B,mmlu_prehistory,0-shot,accuracy,0.8950617283950617,0.017052662081885314
Qwen/Qwen2.5-72B,mmlu_professional_law,0-shot,accuracy,0.6720990873533247,0.011989936640666521
Qwen/Qwen2.5-72B,mmlu_world_religions,0-shot,accuracy,0.9122807017543859,0.02169638394388923
Qwen/Qwen2.5-72B,mmlu_business_ethics,0-shot,accuracy,0.86,0.0348735088019777
Qwen/Qwen2.5-72B,mmlu_clinical_knowledge,0-shot,accuracy,0.8905660377358491,0.019213530010965426
Qwen/Qwen2.5-72B,mmlu_college_medicine,0-shot,accuracy,0.815028901734104,0.0296056239817712
Qwen/Qwen2.5-72B,mmlu_global_facts,0-shot,accuracy,0.66,0.04760952285695237
Qwen/Qwen2.5-72B,mmlu_human_aging,0-shot,accuracy,0.8385650224215246,0.024693957899128472
Qwen/Qwen2.5-72B,mmlu_management,0-shot,accuracy,0.9320388349514563,0.02491995914251448
Qwen/Qwen2.5-72B,mmlu_marketing,0-shot,accuracy,0.9487179487179487,0.014450181176872736
Qwen/Qwen2.5-72B,mmlu_medical_genetics,0-shot,accuracy,0.91,0.02876234912646613
Qwen/Qwen2.5-72B,mmlu_miscellaneous,0-shot,accuracy,0.9450830140485313,0.00814676050075231
Qwen/Qwen2.5-72B,mmlu_nutrition,0-shot,accuracy,0.9052287581699346,0.016771331271836463
Qwen/Qwen2.5-72B,mmlu_professional_accounting,0-shot,accuracy,0.7553191489361702,0.02564555362226673
Qwen/Qwen2.5-72B,mmlu_professional_medicine,0-shot,accuracy,0.9264705882352942,0.015854834709136192
Qwen/Qwen2.5-72B,mmlu_virology,0-shot,accuracy,0.572289156626506,0.038515976837185335
Qwen/Qwen2.5-72B,mmlu_econometrics,0-shot,accuracy,0.7719298245614035,0.03947152782669415
Qwen/Qwen2.5-72B,mmlu_high_school_geography,0-shot,accuracy,0.9393939393939394,0.016999994927421627
Qwen/Qwen2.5-72B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9792746113989638,0.010281417011909025
Qwen/Qwen2.5-72B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.9179487179487179,0.013914804045779782
Qwen/Qwen2.5-72B,mmlu_high_school_microeconomics,0-shot,accuracy,0.9495798319327731,0.014213260391884343
Qwen/Qwen2.5-72B,mmlu_high_school_psychology,0-shot,accuracy,0.9559633027522936,0.00879687721823403
Qwen/Qwen2.5-72B,mmlu_human_sexuality,0-shot,accuracy,0.8854961832061069,0.027927473753597453
Qwen/Qwen2.5-72B,mmlu_professional_psychology,0-shot,accuracy,0.8676470588235294,0.013709377734592328
Qwen/Qwen2.5-72B,mmlu_public_relations,0-shot,accuracy,0.8090909090909091,0.03764425585984926
Qwen/Qwen2.5-72B,mmlu_security_studies,0-shot,accuracy,0.8408163265306122,0.023420972069166365
Qwen/Qwen2.5-72B,mmlu_sociology,0-shot,accuracy,0.9253731343283582,0.018581939698490608
Qwen/Qwen2.5-72B,mmlu_us_foreign_policy,0-shot,accuracy,0.91,0.02876234912646613
Qwen/Qwen2.5-72B,mmlu_abstract_algebra,0-shot,accuracy,0.67,0.04725815626252607
Qwen/Qwen2.5-72B,mmlu_anatomy,0-shot,accuracy,0.8074074074074075,0.03406542058502653
Qwen/Qwen2.5-72B,mmlu_astronomy,0-shot,accuracy,0.9210526315789473,0.02194434281824795
Qwen/Qwen2.5-72B,mmlu_college_biology,0-shot,accuracy,0.9652777777777778,0.015309531175003695
Qwen/Qwen2.5-72B,mmlu_college_chemistry,0-shot,accuracy,0.67,0.04725815626252609
Qwen/Qwen2.5-72B,mmlu_college_computer_science,0-shot,accuracy,0.78,0.04163331998932262
Qwen/Qwen2.5-72B,mmlu_college_mathematics,0-shot,accuracy,0.67,0.04725815626252607
Qwen/Qwen2.5-72B,mmlu_college_physics,0-shot,accuracy,0.7450980392156863,0.043364327079931785
Qwen/Qwen2.5-72B,mmlu_computer_security,0-shot,accuracy,0.89,0.03144660377352201
Qwen/Qwen2.5-72B,mmlu_conceptual_physics,0-shot,accuracy,0.8936170212765957,0.020155977307049863
Qwen/Qwen2.5-72B,mmlu_electrical_engineering,0-shot,accuracy,0.8344827586206897,0.030970559966224068
Qwen/Qwen2.5-72B,mmlu_elementary_mathematics,0-shot,accuracy,0.8994708994708994,0.015487052903261146
Qwen/Qwen2.5-72B,mmlu_high_school_biology,0-shot,accuracy,0.9451612903225807,0.012951418509899204
Qwen/Qwen2.5-72B,mmlu_high_school_chemistry,0-shot,accuracy,0.7832512315270936,0.02899033125251624
Qwen/Qwen2.5-72B,mmlu_high_school_computer_science,0-shot,accuracy,0.95,0.021904291355759047
Qwen/Qwen2.5-72B,mmlu_high_school_mathematics,0-shot,accuracy,0.6814814814814815,0.02840653309060846
Qwen/Qwen2.5-72B,mmlu_high_school_physics,0-shot,accuracy,0.7152317880794702,0.03684881521389024
Qwen/Qwen2.5-72B,mmlu_high_school_statistics,0-shot,accuracy,0.8009259259259259,0.02723229846269024
Qwen/Qwen2.5-72B,mmlu_machine_learning,0-shot,accuracy,0.8035714285714286,0.037709700493470166
Qwen/Qwen2.5-72B,gsm8k,5-shot,accuracy,0.8877937831690674,0.008693743138242331
Qwen/Qwen1.5-110B,lambada_openai,0-shot,accuracy,0.7952648942363671,0.005621654323022829
Qwen/Qwen1.5-110B,lambada_standard,0-shot,accuracy,0.7275373568794877,0.0062028775008403465
Qwen/Qwen2.5-72B,lambada_openai,0-shot,accuracy,0.7849796235202795,0.0057237554957901
Qwen/Qwen2.5-72B,lambada_standard,0-shot,accuracy,0.7089074325635553,0.0063288149295274675
Qwen/Qwen-2.5-72B,truthfulqa_mc1,0-shot,accuracy,0.40269277845777235,0.017168830935187215
Qwen/Qwen-2.5-72B,truthfulqa_mc2,0-shot,accuracy,0.6046754209898251,0.014377239189451992
Qwen/Qwen-2.5-72B,winogrande,5-shot,accuracy,0.8397790055248618,0.010309209498187468
llama2_220M_nl_code_shuf-hf,arc_challenge,25-shot,accuracy,0.19880546075085323,0.011662850198175537
llama2_220M_nl_code_shuf-hf,hellaswag,10-shot,accuracy,0.2991435968930492,0.004569470678071229
llama2_220M_nl_code_shuf-hf,truthfulqa_mc1,0-shot,accuracy,0.2484700122399021,0.015127427096520677
llama2_220M_nl_code_shuf-hf,truthfulqa_mc2,0-shot,accuracy,0.4449090089333025,0.015142693665281863
llama2_220M_nl_code_shuf-hf,winogrande,5-shot,accuracy,0.5153906866614049,0.014045826789783656
llama2_220M_nl_code_shuf-hf,lambada_openai,0-shot,accuracy,0.3399961187657675,0.006599671169668113
llama2_220M_nl_code_shuf-hf,lambada_standard,0-shot,accuracy,0.2450999417814865,0.005992780988422182
llama2_220M_nl_code_shuf-hf,gsm8k,5-shot,accuracy,0.009097801364670205,0.002615326510775673
llama2_220M_nl_code_shuf-hf,humaneval,0-shot,accuracy,0.0,
llama2_220M_nl_code_shuf-hf,mmlu_abstract_algebra,5-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_code_shuf-hf,mmlu_anatomy,5-shot,accuracy,0.28888888888888886,0.0391545063041425
llama2_220M_nl_code_shuf-hf,mmlu_astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
llama2_220M_nl_code_shuf-hf,mmlu_business_ethics,5-shot,accuracy,0.29,0.045604802157206845
llama2_220M_nl_code_shuf-hf,mmlu_clinical_knowledge,5-shot,accuracy,0.22264150943396227,0.02560423347089909
llama2_220M_nl_code_shuf-hf,mmlu_college_biology,5-shot,accuracy,0.24305555555555555,0.035868792800803406
llama2_220M_nl_code_shuf-hf,mmlu_college_chemistry,5-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_code_shuf-hf,mmlu_college_computer_science,5-shot,accuracy,0.19,0.03942772444036624
llama2_220M_nl_code_shuf-hf,mmlu_college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_code_shuf-hf,mmlu_college_medicine,5-shot,accuracy,0.2658959537572254,0.03368762932259431
llama2_220M_nl_code_shuf-hf,mmlu_college_physics,5-shot,accuracy,0.20588235294117646,0.040233822736177455
llama2_220M_nl_code_shuf-hf,mmlu_computer_security,5-shot,accuracy,0.27,0.0446196043338474
llama2_220M_nl_code_shuf-hf,mmlu_conceptual_physics,5-shot,accuracy,0.2553191489361702,0.02850485647051421
llama2_220M_nl_code_shuf-hf,mmlu_econometrics,5-shot,accuracy,0.2719298245614035,0.04185774424022056
llama2_220M_nl_code_shuf-hf,mmlu_electrical_engineering,5-shot,accuracy,0.2896551724137931,0.03780019230438015
llama2_220M_nl_code_shuf-hf,mmlu_elementary_mathematics,5-shot,accuracy,0.23809523809523808,0.02193587808118476
llama2_220M_nl_code_shuf-hf,mmlu_formal_logic,5-shot,accuracy,0.15079365079365079,0.03200686497287391
llama2_220M_nl_code_shuf-hf,mmlu_global_facts,5-shot,accuracy,0.19,0.039427724440366234
llama2_220M_nl_code_shuf-hf,mmlu_high_school_biology,5-shot,accuracy,0.2806451612903226,0.025560604721022884
llama2_220M_nl_code_shuf-hf,mmlu_high_school_chemistry,5-shot,accuracy,0.2660098522167488,0.03108982600293753
llama2_220M_nl_code_shuf-hf,mmlu_high_school_computer_science,5-shot,accuracy,0.28,0.04512608598542129
llama2_220M_nl_code_shuf-hf,mmlu_high_school_european_history,5-shot,accuracy,0.296969696969697,0.035679697722680474
llama2_220M_nl_code_shuf-hf,mmlu_high_school_geography,5-shot,accuracy,0.20202020202020202,0.028606204289229876
llama2_220M_nl_code_shuf-hf,mmlu_high_school_government_and_politics,5-shot,accuracy,0.22279792746113988,0.03003114797764154
llama2_220M_nl_code_shuf-hf,mmlu_high_school_macroeconomics,5-shot,accuracy,0.2794871794871795,0.022752388839776823
llama2_220M_nl_code_shuf-hf,mmlu_high_school_mathematics,5-shot,accuracy,0.29259259259259257,0.02773896963217609
llama2_220M_nl_code_shuf-hf,mmlu_high_school_microeconomics,5-shot,accuracy,0.24789915966386555,0.028047967224176892
llama2_220M_nl_code_shuf-hf,mmlu_high_school_physics,5-shot,accuracy,0.2582781456953642,0.035737053147634576
llama2_220M_nl_code_shuf-hf,mmlu_high_school_psychology,5-shot,accuracy,0.23486238532110093,0.01817511051034359
llama2_220M_nl_code_shuf-hf,mmlu_high_school_statistics,5-shot,accuracy,0.37037037037037035,0.03293377139415191
llama2_220M_nl_code_shuf-hf,mmlu_high_school_us_history,5-shot,accuracy,0.2647058823529412,0.0309645179269234
llama2_220M_nl_code_shuf-hf,mmlu_high_school_world_history,5-shot,accuracy,0.24050632911392406,0.027820781981149675
llama2_220M_nl_code_shuf-hf,mmlu_human_aging,5-shot,accuracy,0.22869955156950672,0.0281882400469292
llama2_220M_nl_code_shuf-hf,mmlu_human_sexuality,5-shot,accuracy,0.2748091603053435,0.03915345408847835
llama2_220M_nl_code_shuf-hf,mmlu_international_law,5-shot,accuracy,0.2231404958677686,0.03800754475228732
llama2_220M_nl_code_shuf-hf,mmlu_jurisprudence,5-shot,accuracy,0.2222222222222222,0.0401910747255735
llama2_220M_nl_code_shuf-hf,mmlu_logical_fallacies,5-shot,accuracy,0.2331288343558282,0.0332201579577674
llama2_220M_nl_code_shuf-hf,mmlu_machine_learning,5-shot,accuracy,0.22321428571428573,0.039523019677025116
llama2_220M_nl_code_shuf-hf,mmlu_management,5-shot,accuracy,0.1941747572815534,0.03916667762822584
llama2_220M_nl_code_shuf-hf,mmlu_marketing,5-shot,accuracy,0.21367521367521367,0.02685345037700915
llama2_220M_nl_code_shuf-hf,mmlu_medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_code_shuf-hf,mmlu_miscellaneous,5-shot,accuracy,0.2681992337164751,0.015842430835269445
llama2_220M_nl_code_shuf-hf,mmlu_moral_disputes,5-shot,accuracy,0.18208092485549132,0.020776761102513003
llama2_220M_nl_code_shuf-hf,mmlu_moral_scenarios,5-shot,accuracy,0.24581005586592178,0.014400296429225598
llama2_220M_nl_code_shuf-hf,mmlu_nutrition,5-shot,accuracy,0.22875816993464052,0.024051029739912258
llama2_220M_nl_code_shuf-hf,mmlu_philosophy,5-shot,accuracy,0.19935691318327975,0.022691033780549656
llama2_220M_nl_code_shuf-hf,mmlu_prehistory,5-shot,accuracy,0.24074074074074073,0.02378858355165854
llama2_220M_nl_code_shuf-hf,mmlu_professional_accounting,5-shot,accuracy,0.26595744680851063,0.026358065698880596
llama2_220M_nl_code_shuf-hf,mmlu_professional_law,5-shot,accuracy,0.2405475880052151,0.010916406735478947
llama2_220M_nl_code_shuf-hf,mmlu_professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121596
llama2_220M_nl_code_shuf-hf,mmlu_professional_psychology,5-shot,accuracy,0.2581699346405229,0.017704531653250075
llama2_220M_nl_code_shuf-hf,mmlu_public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
llama2_220M_nl_code_shuf-hf,mmlu_security_studies,5-shot,accuracy,0.27346938775510204,0.028535560337128445
llama2_220M_nl_code_shuf-hf,mmlu_sociology,5-shot,accuracy,0.24378109452736318,0.030360490154014652
llama2_220M_nl_code_shuf-hf,mmlu_us_foreign_policy,5-shot,accuracy,0.27,0.0446196043338474
llama2_220M_nl_code_shuf-hf,mmlu_virology,5-shot,accuracy,0.23493975903614459,0.03300533186128922
llama2_220M_nl_code_shuf-hf,mmlu_world_religions,5-shot,accuracy,0.2982456140350877,0.03508771929824565
llama2_220M_nl_code_shuf-hf,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
llama2_220M_nl_code_shuf-hf,mmlu_anatomy,0-shot,accuracy,0.21481481481481482,0.035478541985608236
llama2_220M_nl_code_shuf-hf,mmlu_astronomy,0-shot,accuracy,0.20394736842105263,0.0327900040631005
llama2_220M_nl_code_shuf-hf,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
llama2_220M_nl_code_shuf-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.22264150943396227,0.025604233470899098
llama2_220M_nl_code_shuf-hf,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.03685651095897532
llama2_220M_nl_code_shuf-hf,mmlu_college_chemistry,0-shot,accuracy,0.19,0.03942772444036623
llama2_220M_nl_code_shuf-hf,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
llama2_220M_nl_code_shuf-hf,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
llama2_220M_nl_code_shuf-hf,mmlu_college_medicine,0-shot,accuracy,0.21965317919075145,0.031568093627031744
llama2_220M_nl_code_shuf-hf,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
llama2_220M_nl_code_shuf-hf,mmlu_computer_security,0-shot,accuracy,0.29,0.045604802157206845
llama2_220M_nl_code_shuf-hf,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
llama2_220M_nl_code_shuf-hf,mmlu_econometrics,0-shot,accuracy,0.21929824561403508,0.0389243110651875
llama2_220M_nl_code_shuf-hf,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
llama2_220M_nl_code_shuf-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
llama2_220M_nl_code_shuf-hf,mmlu_formal_logic,0-shot,accuracy,0.23809523809523808,0.03809523809523809
llama2_220M_nl_code_shuf-hf,mmlu_global_facts,0-shot,accuracy,0.17,0.0377525168068637
llama2_220M_nl_code_shuf-hf,mmlu_high_school_biology,0-shot,accuracy,0.1870967741935484,0.022185710092252266
llama2_220M_nl_code_shuf-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.1724137931034483,0.02657767218303657
llama2_220M_nl_code_shuf-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.04408440022768079
llama2_220M_nl_code_shuf-hf,mmlu_high_school_european_history,0-shot,accuracy,0.21212121212121213,0.03192271569548301
llama2_220M_nl_code_shuf-hf,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
llama2_220M_nl_code_shuf-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.18652849740932642,0.028112091210117474
llama2_220M_nl_code_shuf-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.19487179487179487,0.020083167595181393
llama2_220M_nl_code_shuf-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.21851851851851853,0.02519575225182379
llama2_220M_nl_code_shuf-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.22268907563025211,0.027025433498882385
llama2_220M_nl_code_shuf-hf,mmlu_high_school_physics,0-shot,accuracy,0.18543046357615894,0.03173284384294284
llama2_220M_nl_code_shuf-hf,mmlu_high_school_psychology,0-shot,accuracy,0.1944954128440367,0.01697028909045803
llama2_220M_nl_code_shuf-hf,mmlu_high_school_statistics,0-shot,accuracy,0.1574074074074074,0.024837173518242384
llama2_220M_nl_code_shuf-hf,mmlu_high_school_us_history,0-shot,accuracy,0.24509803921568626,0.030190282453501967
llama2_220M_nl_code_shuf-hf,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.029041333510598042
llama2_220M_nl_code_shuf-hf,mmlu_human_aging,0-shot,accuracy,0.30493273542600896,0.030898610882477515
llama2_220M_nl_code_shuf-hf,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.0384487613978527
llama2_220M_nl_code_shuf-hf,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794088
llama2_220M_nl_code_shuf-hf,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
llama2_220M_nl_code_shuf-hf,mmlu_logical_fallacies,0-shot,accuracy,0.22699386503067484,0.03291099578615768
llama2_220M_nl_code_shuf-hf,mmlu_machine_learning,0-shot,accuracy,0.30357142857142855,0.04364226155841044
llama2_220M_nl_code_shuf-hf,mmlu_management,0-shot,accuracy,0.1941747572815534,0.03916667762822584
llama2_220M_nl_code_shuf-hf,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.02987257770889117
llama2_220M_nl_code_shuf-hf,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
llama2_220M_nl_code_shuf-hf,mmlu_miscellaneous,0-shot,accuracy,0.23627075351213284,0.015190473717037509
llama2_220M_nl_code_shuf-hf,mmlu_moral_disputes,0-shot,accuracy,0.25722543352601157,0.02353292543104428
llama2_220M_nl_code_shuf-hf,mmlu_moral_scenarios,0-shot,accuracy,0.2346368715083799,0.014173044098303673
llama2_220M_nl_code_shuf-hf,mmlu_nutrition,0-shot,accuracy,0.21241830065359477,0.023420375478296125
llama2_220M_nl_code_shuf-hf,mmlu_philosophy,0-shot,accuracy,0.18971061093247588,0.022268196258783225
llama2_220M_nl_code_shuf-hf,mmlu_prehistory,0-shot,accuracy,0.22530864197530864,0.02324620264781975
llama2_220M_nl_code_shuf-hf,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
llama2_220M_nl_code_shuf-hf,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
llama2_220M_nl_code_shuf-hf,mmlu_professional_medicine,0-shot,accuracy,0.20220588235294118,0.024398192986654924
llama2_220M_nl_code_shuf-hf,mmlu_professional_psychology,0-shot,accuracy,0.2581699346405229,0.017704531653250078
llama2_220M_nl_code_shuf-hf,mmlu_public_relations,0-shot,accuracy,0.20909090909090908,0.038950910157241364
llama2_220M_nl_code_shuf-hf,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
llama2_220M_nl_code_shuf-hf,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409227
llama2_220M_nl_code_shuf-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
llama2_220M_nl_code_shuf-hf,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.03529486801511115
llama2_220M_nl_code_shuf-hf,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.03565079670708313
cerebras/Cerebras-GPT-13B,logiqa2,0-shot,brier_score,1.4932929706639089,
Salesforce/codegen-16B-nl,anli_r1,0-shot,brier_score,0.7826872054092496,
Salesforce/codegen-16B-nl,anli_r2,0-shot,brier_score,0.7646217403386586,
Salesforce/codegen-16B-nl,anli_r3,0-shot,brier_score,0.7519199657938849,
Salesforce/codegen-16B-nl,logiqa2,0-shot,brier_score,1.0967609960716458,
Salesforce/codegen-16B-nl,mathqa,0-shot,brier_score,0.9280289974622424,
Salesforce/codegen-16B-nl,xnli_ar,0-shot,brier_score,1.2396199665338918,
Salesforce/codegen-16B-nl,xnli_bg,0-shot,brier_score,0.7786483294827469,
Salesforce/codegen-16B-nl,xnli_de,0-shot,brier_score,0.8290257645584538,
Salesforce/codegen-16B-nl,xnli_el,0-shot,brier_score,0.7824971503228734,
Salesforce/codegen-16B-nl,xnli_en,0-shot,brier_score,0.6526307683975848,
Salesforce/codegen-16B-nl,xnli_es,0-shot,brier_score,0.8092516468491676,
Salesforce/codegen-16B-nl,xnli_fr,0-shot,brier_score,0.7478150294211345,
Salesforce/codegen-16B-nl,xnli_hi,0-shot,brier_score,0.7735635391635689,
Salesforce/codegen-16B-nl,xnli_ru,0-shot,brier_score,0.7675387820610992,
Salesforce/codegen-16B-nl,xnli_sw,0-shot,brier_score,0.8334576655284431,
Salesforce/codegen-16B-nl,xnli_th,0-shot,brier_score,0.7852029774806583,
Salesforce/codegen-16B-nl,xnli_tr,0-shot,brier_score,0.8332399538729073,
Salesforce/codegen-16B-nl,xnli_ur,0-shot,brier_score,0.919582369322642,
Salesforce/codegen-16B-nl,xnli_vi,0-shot,brier_score,0.767876374228158,
Salesforce/codegen-16B-nl,xnli_zh,0-shot,brier_score,1.0214843021140374,
microsoft/phi-2,anli_r1,0-shot,brier_score,0.7827672289103969,
microsoft/phi-2,anli_r2,0-shot,brier_score,0.8210280981433832,
microsoft/phi-2,anli_r3,0-shot,brier_score,0.7456143537722137,
microsoft/phi-2,logiqa2,0-shot,brier_score,1.003463760796325,
microsoft/phi-2,mathqa,0-shot,brier_score,0.8998724664167802,
microsoft/phi-2,xnli_ar,0-shot,brier_score,0.8700806209283553,
microsoft/phi-2,xnli_bg,0-shot,brier_score,0.8763314074236563,
microsoft/phi-2,xnli_de,0-shot,brier_score,0.8411184016404186,
microsoft/phi-2,xnli_el,0-shot,brier_score,0.9399982372024984,
microsoft/phi-2,xnli_en,0-shot,brier_score,0.6553374710177062,
microsoft/phi-2,xnli_es,0-shot,brier_score,1.0289741693964864,
microsoft/phi-2,xnli_fr,0-shot,brier_score,0.8241393908902388,
microsoft/phi-2,xnli_hi,0-shot,brier_score,0.9662455468379467,
microsoft/phi-2,xnli_ru,0-shot,brier_score,0.7868965082979368,
microsoft/phi-2,xnli_sw,0-shot,brier_score,1.1043181085638436,
microsoft/phi-2,xnli_th,0-shot,brier_score,1.226654123317517,
microsoft/phi-2,xnli_tr,0-shot,brier_score,1.0997481620371237,
microsoft/phi-2,xnli_ur,0-shot,brier_score,1.324133290105516,
microsoft/phi-2,xnli_vi,0-shot,brier_score,1.3113716462614782,
microsoft/phi-2,xnli_zh,0-shot,brier_score,1.0868265479730823,
llama2_220M_nl_code_shuf-hf,anli_r1,0-shot,brier_score,1.0081686408585258,
llama2_220M_nl_code_shuf-hf,anli_r2,0-shot,brier_score,0.9852053802385999,
llama2_220M_nl_code_shuf-hf,anli_r3,0-shot,brier_score,0.9389923202367694,
llama2_220M_nl_code_shuf-hf,logiqa2,0-shot,brier_score,1.1167933513788422,
llama2_220M_nl_code_shuf-hf,mathqa,0-shot,brier_score,1.0518929714534901,
llama2_220M_nl_code_shuf-hf,xnli_ar,0-shot,brier_score,0.9777956368438607,
llama2_220M_nl_code_shuf-hf,xnli_bg,0-shot,brier_score,1.0497268464522957,
llama2_220M_nl_code_shuf-hf,xnli_de,0-shot,brier_score,0.9357302776012506,
llama2_220M_nl_code_shuf-hf,xnli_el,0-shot,brier_score,1.1958921383150567,
llama2_220M_nl_code_shuf-hf,xnli_en,0-shot,brier_score,0.7247810030822124,
llama2_220M_nl_code_shuf-hf,xnli_es,0-shot,brier_score,1.060417172912037,
llama2_220M_nl_code_shuf-hf,xnli_fr,0-shot,brier_score,0.8225662429554118,
llama2_220M_nl_code_shuf-hf,xnli_hi,0-shot,brier_score,1.1933696609935265,
llama2_220M_nl_code_shuf-hf,xnli_ru,0-shot,brier_score,0.8435936384517854,
llama2_220M_nl_code_shuf-hf,xnli_sw,0-shot,brier_score,0.9013246866577334,
llama2_220M_nl_code_shuf-hf,xnli_th,0-shot,brier_score,0.985639645945832,
llama2_220M_nl_code_shuf-hf,xnli_tr,0-shot,brier_score,1.0569274770608035,
llama2_220M_nl_code_shuf-hf,xnli_ur,0-shot,brier_score,1.3190438870036263,
llama2_220M_nl_code_shuf-hf,xnli_vi,0-shot,brier_score,0.9858949189836345,
llama2_220M_nl_code_shuf-hf,xnli_zh,0-shot,brier_score,1.207955999970105,
Qwen/Qwen2.5-72B,arc_challenge,25-shot,accuracy,0.6825938566552902,0.013602239088038169
Qwen/Qwen2.5-72B,arc_challenge,25-shot,acc_norm,0.7218430034129693,0.013094469919538804
llama2_220M_nl_code_shuf-hf,arc_challenge,0-shot,accuracy,0.189419795221843,0.01145070511591077
llama2_220M_nl_code_shuf-hf,arc_challenge,0-shot,acc_norm,0.22525597269624573,0.01220783999540733
llama2_220M_nl_code_shuf-hf,arc_challenge,25-shot,acc_norm,0.23720136518771331,0.01243039982926083
llama2_220M_nl_code_shuf-hf,hellaswag,10-shot,acc_norm,0.33170683130850426,0.00469864068827124
llama2_220M_nl_code_shuf-hf,lambada_openai,0-shot,perplexity,39.10861367206069,1.4675080314556632
llama2_220M_nl_code_shuf-hf,lambada_standard,0-shot,perplexity,96.04521673175341,4.066482828562728
meta-llama/Meta-Llama-3-70B,mmlu_formal_logic,0-shot,accuracy,0.5714285714285714,0.04426266681379909
meta-llama/Meta-Llama-3-70B,mmlu_high_school_european_history,0-shot,accuracy,0.8424242424242424,0.02845038880528437
meta-llama/Meta-Llama-3-70B,mmlu_high_school_us_history,0-shot,accuracy,0.9411764705882353,0.01651440956102582
meta-llama/Meta-Llama-3-70B,mmlu_high_school_world_history,0-shot,accuracy,0.9367088607594937,0.015849580400549967
meta-llama/Meta-Llama-3-70B,mmlu_international_law,0-shot,accuracy,0.8760330578512396,0.030083098716035206
meta-llama/Meta-Llama-3-70B,mmlu_jurisprudence,0-shot,accuracy,0.8796296296296297,0.03145703854306252
meta-llama/Meta-Llama-3-70B,mmlu_logical_fallacies,0-shot,accuracy,0.8343558282208589,0.029208296231259104
meta-llama/Meta-Llama-3-70B,mmlu_moral_disputes,0-shot,accuracy,0.8439306358381503,0.019539014685374036
meta-llama/Meta-Llama-3-70B,mmlu_moral_scenarios,0-shot,accuracy,0.3139664804469274,0.015521923933523644
meta-llama/Meta-Llama-3-70B,mmlu_philosophy,0-shot,accuracy,0.8327974276527331,0.021193872528034948
meta-llama/Meta-Llama-3-70B,mmlu_prehistory,0-shot,accuracy,0.8950617283950617,0.017052662081885307
meta-llama/Meta-Llama-3-70B,mmlu_professional_law,0-shot,accuracy,0.6095176010430248,0.01246013591394507
meta-llama/Meta-Llama-3-70B,mmlu_world_religions,0-shot,accuracy,0.8947368421052632,0.02353755765789256
meta-llama/Meta-Llama-3-70B,mmlu_business_ethics,0-shot,accuracy,0.8,0.04020151261036847
meta-llama/Meta-Llama-3-70B,mmlu_clinical_knowledge,0-shot,accuracy,0.8377358490566038,0.022691482872035353
meta-llama/Meta-Llama-3-70B,mmlu_college_medicine,0-shot,accuracy,0.7514450867052023,0.03295304696818317
meta-llama/Meta-Llama-3-70B,mmlu_global_facts,0-shot,accuracy,0.51,0.05024183937956912
meta-llama/Meta-Llama-3-70B,mmlu_human_aging,0-shot,accuracy,0.8116591928251121,0.026241132996407256
meta-llama/Meta-Llama-3-70B,mmlu_management,0-shot,accuracy,0.912621359223301,0.027960689125970658
meta-llama/Meta-Llama-3-70B,mmlu_marketing,0-shot,accuracy,0.9316239316239316,0.016534627684311364
meta-llama/Meta-Llama-3-70B,mmlu_medical_genetics,0-shot,accuracy,0.84,0.036845294917747094
meta-llama/Meta-Llama-3-70B,mmlu_miscellaneous,0-shot,accuracy,0.9118773946360154,0.010136978203312642
meta-llama/Meta-Llama-3-70B,mmlu_nutrition,0-shot,accuracy,0.8496732026143791,0.020464175124332618
meta-llama/Meta-Llama-3-70B,mmlu_professional_accounting,0-shot,accuracy,0.6276595744680851,0.028838921471251458
meta-llama/Meta-Llama-3-70B,mmlu_professional_medicine,0-shot,accuracy,0.8639705882352942,0.020824819397794334
meta-llama/Meta-Llama-3-70B,mmlu_virology,0-shot,accuracy,0.5421686746987951,0.038786267710023595
meta-llama/Meta-Llama-3-70B,mmlu_econometrics,0-shot,accuracy,0.6929824561403509,0.0433913832257986
meta-llama/Meta-Llama-3-70B,mmlu_high_school_geography,0-shot,accuracy,0.9191919191919192,0.019417681889724536
meta-llama/Meta-Llama-3-70B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9637305699481865,0.013492659751295134
meta-llama/Meta-Llama-3-70B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.8307692307692308,0.019011004523651048
meta-llama/Meta-Llama-3-70B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8697478991596639,0.021863258494852104
meta-llama/Meta-Llama-3-70B,mmlu_high_school_psychology,0-shot,accuracy,0.9192660550458716,0.011680172292862064
meta-llama/Meta-Llama-3-70B,mmlu_human_sexuality,0-shot,accuracy,0.8473282442748091,0.031545216720054704
meta-llama/Meta-Llama-3-70B,mmlu_professional_psychology,0-shot,accuracy,0.8464052287581699,0.014586690876223236
meta-llama/Meta-Llama-3-70B,mmlu_public_relations,0-shot,accuracy,0.7909090909090909,0.03895091015724138
meta-llama/Meta-Llama-3-70B,mmlu_security_studies,0-shot,accuracy,0.8204081632653061,0.024573293589585637
meta-llama/Meta-Llama-3-70B,mmlu_sociology,0-shot,accuracy,0.9203980099502488,0.019139685633503815
meta-llama/Meta-Llama-3-70B,mmlu_us_foreign_policy,0-shot,accuracy,0.91,0.02876234912646613
meta-llama/Meta-Llama-3-70B,mmlu_abstract_algebra,0-shot,accuracy,0.42,0.049604496374885836
meta-llama/Meta-Llama-3-70B,mmlu_anatomy,0-shot,accuracy,0.7925925925925926,0.03502553170678316
meta-llama/Meta-Llama-3-70B,mmlu_astronomy,0-shot,accuracy,0.881578947368421,0.026293995855474907
meta-llama/Meta-Llama-3-70B,mmlu_college_biology,0-shot,accuracy,0.9166666666666666,0.023112508176051233
meta-llama/Meta-Llama-3-70B,mmlu_college_chemistry,0-shot,accuracy,0.59,0.04943110704237101
meta-llama/Meta-Llama-3-70B,mmlu_college_computer_science,0-shot,accuracy,0.64,0.04824181513244218
meta-llama/Meta-Llama-3-70B,mmlu_college_mathematics,0-shot,accuracy,0.47,0.050161355804659205
meta-llama/Meta-Llama-3-70B,mmlu_college_physics,0-shot,accuracy,0.5294117647058824,0.04966570903978529
meta-llama/Meta-Llama-3-70B,mmlu_computer_security,0-shot,accuracy,0.86,0.03487350880197769
meta-llama/Meta-Llama-3-70B,mmlu_conceptual_physics,0-shot,accuracy,0.7404255319148936,0.028659179374292323
meta-llama/Meta-Llama-3-70B,mmlu_electrical_engineering,0-shot,accuracy,0.7241379310344828,0.037245636197746325
meta-llama/Meta-Llama-3-70B,mmlu_elementary_mathematics,0-shot,accuracy,0.6190476190476191,0.025010749116137595
meta-llama/Meta-Llama-3-70B,mmlu_high_school_biology,0-shot,accuracy,0.9,0.017066403719657296
meta-llama/Meta-Llama-3-70B,mmlu_high_school_chemistry,0-shot,accuracy,0.7192118226600985,0.0316185633535861
meta-llama/Meta-Llama-3-70B,mmlu_high_school_computer_science,0-shot,accuracy,0.87,0.03379976689896309
meta-llama/Meta-Llama-3-70B,mmlu_high_school_mathematics,0-shot,accuracy,0.4074074074074074,0.029958249250082118
meta-llama/Meta-Llama-3-70B,mmlu_high_school_physics,0-shot,accuracy,0.5496688741721855,0.04062290018683775
meta-llama/Meta-Llama-3-70B,mmlu_high_school_statistics,0-shot,accuracy,0.6805555555555556,0.03179876342176852
meta-llama/Meta-Llama-3-70B,mmlu_machine_learning,0-shot,accuracy,0.6428571428571429,0.04547960999764376
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,bleu_max,18.309967726993154,0.8671035764849415
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,bleu_acc,0.34394124847001223,0.016629087514276768
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,bleu_diff,8.48116332088891,0.7874359255168288
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rouge1_max,32.42478629105221,1.2581288824283765
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rouge1_acc,0.3574051407588739,0.016776599676729405
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rouge1_diff,12.88351190043425,1.110793729893203
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rouge2_max,26.070504624804226,1.1855214778754946
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rouge2_acc,0.3219094247246022,0.016355567611960393
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rouge2_diff,12.95826245456893,1.1956063149055145
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rougeL_max,31.156967423980273,1.2428443950288541
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rougeL_acc,0.3488372093023256,0.016684419859986883
Qwen/Qwen2.5-72B,truthfulqa_gen,0-shot,rougeL_diff,12.748090873551925,1.1258172175049193
Qwen/Qwen2.5-72B,truthfulqa_mc1,0-shot,accuracy,0.40024479804161567,0.01715160555574914
Qwen/Qwen2.5-72B,truthfulqa_mc2,0-shot,accuracy,0.6035273460107534,0.014385331624621188
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,bleu_max,15.293326250001723,0.5132343971640962
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,bleu_acc,0.3880048959608323,0.017058761501347983
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,bleu_diff,-1.1247854552851988,0.4839586031704394
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge1_max,35.883393205804175,0.7759164759269835
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge1_acc,0.3329253365973072,0.01649740238201205
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge1_diff,-2.978599244361348,0.7637855921800152
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge2_max,18.22410194896594,0.8084438166951624
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge2_acc,0.18604651162790697,0.013622771770442053
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rouge2_diff,-3.4500804806351977,0.7201480121874275
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rougeL_max,33.19168845229128,0.7536889121182447
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rougeL_acc,0.35495716034271724,0.016750862381375905
llama2_220M_nl_code_shuf-hf,truthfulqa_gen,0-shot,rougeL_diff,-2.575998882093555,0.752052796341635
Qwen/Qwen2.5-72B,winogrande,5-shot,accuracy,0.8421468034727704,0.010247165248719764
google/gemma-2-9b,anli_r1,0-shot,brier_score,0.6926212728961615,
google/gemma-2-9b,anli_r2,0-shot,brier_score,0.6813470235640697,
google/gemma-2-9b,anli_r3,0-shot,brier_score,0.6664149240451884,
google/gemma-2-9b,logiqa2,0-shot,brier_score,0.8560195810391652,
google/gemma-2-9b,mathqa,0-shot,brier_score,0.693253342007907,
google/gemma-2-9b,xnli_ar,0-shot,brier_score,1.2119782827245293,
google/gemma-2-9b,xnli_bg,0-shot,brier_score,0.7382471726525737,
google/gemma-2-9b,xnli_de,0-shot,brier_score,0.771650835821252,
google/gemma-2-9b,xnli_el,0-shot,brier_score,0.8132657360468067,
google/gemma-2-9b,xnli_en,0-shot,brier_score,0.7166038434075869,
google/gemma-2-9b,xnli_es,0-shot,brier_score,0.7547720635923001,
google/gemma-2-9b,xnli_fr,0-shot,brier_score,0.7098819136976718,
google/gemma-2-9b,xnli_hi,0-shot,brier_score,0.7253723386837408,
google/gemma-2-9b,xnli_ru,0-shot,brier_score,0.731751585946106,
google/gemma-2-9b,xnli_sw,0-shot,brier_score,0.7891145338754766,
google/gemma-2-9b,xnli_th,0-shot,brier_score,0.7036264732179458,
google/gemma-2-9b,xnli_tr,0-shot,brier_score,0.7001955965693614,
google/gemma-2-9b,xnli_ur,0-shot,brier_score,0.8194444086194235,
google/gemma-2-9b,xnli_vi,0-shot,brier_score,0.7540418183617248,
google/gemma-2-9b,xnli_zh,0-shot,brier_score,0.9880045906864705,
llama2_460M_nl_code_shuf-hf,anli_r1,0-shot,brier_score,0.9754516569844279,
llama2_460M_nl_code_shuf-hf,anli_r2,0-shot,brier_score,0.9620995466442864,
llama2_460M_nl_code_shuf-hf,anli_r3,0-shot,brier_score,0.9581436229493746,
llama2_460M_nl_code_shuf-hf,logiqa2,0-shot,brier_score,1.1242641197347254,
llama2_460M_nl_code_shuf-hf,mathqa,0-shot,brier_score,1.024405976800741,
llama2_460M_nl_code_shuf-hf,xnli_ar,0-shot,brier_score,0.9508899611466485,
llama2_460M_nl_code_shuf-hf,xnli_bg,0-shot,brier_score,1.084985871096817,
llama2_460M_nl_code_shuf-hf,xnli_de,0-shot,brier_score,0.9084453122000963,
llama2_460M_nl_code_shuf-hf,xnli_el,0-shot,brier_score,1.2579248539464813,
llama2_460M_nl_code_shuf-hf,xnli_en,0-shot,brier_score,0.7262764320277776,
llama2_460M_nl_code_shuf-hf,xnli_es,0-shot,brier_score,1.192938528318084,
llama2_460M_nl_code_shuf-hf,xnli_fr,0-shot,brier_score,0.9233664665637669,
llama2_460M_nl_code_shuf-hf,xnli_hi,0-shot,brier_score,0.9053445535840215,
llama2_460M_nl_code_shuf-hf,xnli_ru,0-shot,brier_score,0.8492643141372731,
llama2_460M_nl_code_shuf-hf,xnli_sw,0-shot,brier_score,1.0441150785543194,
llama2_460M_nl_code_shuf-hf,xnli_th,0-shot,brier_score,0.8513774544030811,
llama2_460M_nl_code_shuf-hf,xnli_tr,0-shot,brier_score,0.8371025072015056,
llama2_460M_nl_code_shuf-hf,xnli_ur,0-shot,brier_score,1.155595653199475,
llama2_460M_nl_code_shuf-hf,xnli_vi,0-shot,brier_score,1.0380080076942502,
llama2_460M_nl_code_shuf-hf,xnli_zh,0-shot,brier_score,1.067567640595577,
meta-llama/Meta-Llama-3.1-70B,arc_challenge,25-shot,accuracy,0.6493174061433447,0.013944635930726096
meta-llama/Meta-Llama-3.1-70B,arc_challenge,25-shot,acc_norm,0.6962457337883959,0.013438909184778759
meta-llama/Meta-Llama-3.1-70B,gsm8k,5-shot,accuracy,0.7952994692949203,0.011113916396062962
meta-llama/Meta-Llama-3.1-70B,hellaswag,10-shot,accuracy,0.6954789882493527,0.004592637369905814
meta-llama/Meta-Llama-3.1-70B,hellaswag,10-shot,acc_norm,0.8775144393547102,0.0032717574453291916
Qwen/Qwen2.5-72B,hellaswag,10-shot,accuracy,0.686018721370245,0.004631603539751937
Qwen/Qwen2.5-72B,hellaswag,10-shot,acc_norm,0.8757219677355108,0.0032922425436374522
meta-llama/Meta-Llama-3.1-70B,lambada_openai,0-shot,perplexity,2.6557077918606526,0.045594183874323214
meta-llama/Meta-Llama-3.1-70B,lambada_openai,0-shot,accuracy,0.8006986221618475,0.0055654689478833895
meta-llama/Meta-Llama-3.1-70B,lambada_standard,0-shot,perplexity,3.088903466451903,0.05933247330937044
meta-llama/Meta-Llama-3.1-70B,lambada_standard,0-shot,accuracy,0.7490782068697845,0.006040109961800765
meta-llama/Meta-Llama-3-70B,lambada_openai,0-shot,perplexity,2.580769395093475,0.042337653524767004
meta-llama/Meta-Llama-3-70B,lambada_openai,0-shot,accuracy,0.8061323500873278,0.005507670121645631
meta-llama/Meta-Llama-3-70B,lambada_standard,0-shot,perplexity,2.9490917395174088,0.05134523591631795
meta-llama/Meta-Llama-3-70B,lambada_standard,0-shot,accuracy,0.7496603920046575,0.006035442817612808
Qwen/Qwen2.5-72B,mmlu_formal_logic,5-shot,accuracy,0.7063492063492064,0.040735243221471255
Qwen/Qwen2.5-72B,mmlu_high_school_european_history,5-shot,accuracy,0.8727272727272727,0.0260246576516562
Qwen/Qwen2.5-72B,mmlu_high_school_us_history,5-shot,accuracy,0.946078431372549,0.01585246528110692
Qwen/Qwen2.5-72B,mmlu_high_school_world_history,5-shot,accuracy,0.9367088607594937,0.01584958040054998
Qwen/Qwen2.5-72B,mmlu_international_law,5-shot,accuracy,0.9256198347107438,0.02395268883667677
Qwen/Qwen2.5-72B,mmlu_jurisprudence,5-shot,accuracy,0.9074074074074074,0.02802188803860944
Qwen/Qwen2.5-72B,mmlu_logical_fallacies,5-shot,accuracy,0.901840490797546,0.02337618023105962
Qwen/Qwen2.5-72B,mmlu_moral_disputes,5-shot,accuracy,0.8728323699421965,0.01793676686514987
Qwen/Qwen2.5-72B,mmlu_moral_scenarios,5-shot,accuracy,0.8145251396648044,0.012999480996301174
Qwen/Qwen2.5-72B,mmlu_philosophy,5-shot,accuracy,0.8778135048231511,0.018600811252967916
Qwen/Qwen2.5-72B,mmlu_prehistory,5-shot,accuracy,0.9259259259259259,0.014572027321567347
Qwen/Qwen2.5-72B,mmlu_professional_law,5-shot,accuracy,0.7027379400260756,0.011673346173086036
Qwen/Qwen2.5-72B,mmlu_world_religions,5-shot,accuracy,0.8888888888888888,0.02410338420207285
Qwen/Qwen2.5-72B,mmlu_business_ethics,5-shot,accuracy,0.86,0.0348735088019777
Qwen/Qwen2.5-72B,mmlu_clinical_knowledge,5-shot,accuracy,0.8981132075471698,0.01861754975827668
Qwen/Qwen2.5-72B,mmlu_college_medicine,5-shot,accuracy,0.8439306358381503,0.027672473701627075
Qwen/Qwen2.5-72B,mmlu_global_facts,5-shot,accuracy,0.67,0.04725815626252607
Qwen/Qwen2.5-72B,mmlu_human_aging,5-shot,accuracy,0.8385650224215246,0.02469395789912846
Qwen/Qwen2.5-72B,mmlu_management,5-shot,accuracy,0.912621359223301,0.027960689125970654
Qwen/Qwen2.5-72B,mmlu_marketing,5-shot,accuracy,0.9529914529914529,0.013866120058594845
Qwen/Qwen2.5-72B,mmlu_medical_genetics,5-shot,accuracy,0.92,0.027265992434429093
Qwen/Qwen2.5-72B,mmlu_miscellaneous,5-shot,accuracy,0.9450830140485313,0.00814676050075231
Qwen/Qwen2.5-72B,mmlu_nutrition,5-shot,accuracy,0.9215686274509803,0.015394260411062113
Qwen/Qwen2.5-72B,mmlu_professional_accounting,5-shot,accuracy,0.7836879432624113,0.024561720560562793
Qwen/Qwen2.5-72B,mmlu_professional_medicine,5-shot,accuracy,0.9154411764705882,0.016900908171490585
Qwen/Qwen2.5-72B,mmlu_virology,5-shot,accuracy,0.5903614457831325,0.03828401115079023
Qwen/Qwen2.5-72B,mmlu_econometrics,5-shot,accuracy,0.7807017543859649,0.03892431106518754
Qwen/Qwen2.5-72B,mmlu_high_school_geography,5-shot,accuracy,0.9494949494949495,0.015602012491972257
Qwen/Qwen2.5-72B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.9896373056994818,0.007308424386792209
Qwen/Qwen2.5-72B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.9205128205128205,0.013714774254965293
Qwen/Qwen2.5-72B,mmlu_high_school_microeconomics,5-shot,accuracy,0.9537815126050421,0.013638234108978643
Qwen/Qwen2.5-72B,mmlu_high_school_psychology,5-shot,accuracy,0.9559633027522936,0.008796877218234028
Qwen/Qwen2.5-72B,mmlu_human_sexuality,5-shot,accuracy,0.9236641221374046,0.02328893953617374
Qwen/Qwen2.5-72B,mmlu_professional_psychology,5-shot,accuracy,0.8986928104575164,0.012206893687651028
Qwen/Qwen2.5-72B,mmlu_public_relations,5-shot,accuracy,0.8,0.03831305140884601
Qwen/Qwen2.5-72B,mmlu_security_studies,5-shot,accuracy,0.889795918367347,0.02004698804327474
Qwen/Qwen2.5-72B,mmlu_sociology,5-shot,accuracy,0.9502487562189055,0.015374663821256171
Qwen/Qwen2.5-72B,mmlu_us_foreign_policy,5-shot,accuracy,0.95,0.021904291355759036
Qwen/Qwen2.5-72B,mmlu_abstract_algebra,5-shot,accuracy,0.73,0.044619604333847394
Qwen/Qwen2.5-72B,mmlu_anatomy,5-shot,accuracy,0.8148148148148148,0.033556772163131396
Qwen/Qwen2.5-72B,mmlu_astronomy,5-shot,accuracy,0.9473684210526315,0.018171642241529545
Qwen/Qwen2.5-72B,mmlu_college_biology,5-shot,accuracy,0.9444444444444444,0.01915507853243362
Qwen/Qwen2.5-72B,mmlu_college_chemistry,5-shot,accuracy,0.7,0.046056618647183814
Qwen/Qwen2.5-72B,mmlu_college_computer_science,5-shot,accuracy,0.81,0.03942772444036623
Qwen/Qwen2.5-72B,mmlu_college_mathematics,5-shot,accuracy,0.7,0.04605661864718381
Qwen/Qwen2.5-72B,mmlu_college_physics,5-shot,accuracy,0.7647058823529411,0.04220773659171452
Qwen/Qwen2.5-72B,mmlu_computer_security,5-shot,accuracy,0.88,0.03265986323710906
Qwen/Qwen2.5-72B,mmlu_conceptual_physics,5-shot,accuracy,0.9446808510638298,0.014944189720358486
Qwen/Qwen2.5-72B,mmlu_electrical_engineering,5-shot,accuracy,0.8482758620689655,0.029896107594574634
Qwen/Qwen2.5-72B,mmlu_elementary_mathematics,5-shot,accuracy,0.917989417989418,0.014131332312091788
Qwen/Qwen2.5-72B,mmlu_high_school_biology,5-shot,accuracy,0.9451612903225807,0.012951418509899204
Qwen/Qwen2.5-72B,mmlu_high_school_chemistry,5-shot,accuracy,0.8078817733990148,0.02771931570961478
Qwen/Qwen2.5-72B,mmlu_high_school_computer_science,5-shot,accuracy,0.95,0.021904291355759054
Qwen/Qwen2.5-72B,mmlu_high_school_mathematics,5-shot,accuracy,0.7185185185185186,0.027420019350945277
Qwen/Qwen2.5-72B,mmlu_high_school_physics,5-shot,accuracy,0.7615894039735099,0.0347918557259966
Qwen/Qwen2.5-72B,mmlu_high_school_statistics,5-shot,accuracy,0.8287037037037037,0.025695341643824685
Qwen/Qwen2.5-72B,mmlu_machine_learning,5-shot,accuracy,0.8214285714285714,0.03635209121577806
Qwen/Qwen1.5-110B,mmlu_formal_logic,0-shot,accuracy,0.6507936507936508,0.042639068927951315
Qwen/Qwen1.5-110B,mmlu_high_school_european_history,0-shot,accuracy,0.8606060606060606,0.027045948825865383
Qwen/Qwen1.5-110B,mmlu_high_school_us_history,0-shot,accuracy,0.9264705882352942,0.01831885585008968
Qwen/Qwen1.5-110B,mmlu_high_school_world_history,0-shot,accuracy,0.9324894514767933,0.016332466673244412
Qwen/Qwen1.5-110B,mmlu_international_law,0-shot,accuracy,0.9008264462809917,0.02728524631275896
Qwen/Qwen1.5-110B,mmlu_jurisprudence,0-shot,accuracy,0.8518518518518519,0.03434300243631002
Qwen/Qwen1.5-110B,mmlu_logical_fallacies,0-shot,accuracy,0.9202453987730062,0.021284928419899075
Qwen/Qwen1.5-110B,mmlu_moral_disputes,0-shot,accuracy,0.8265895953757225,0.02038322955113501
Qwen/Qwen1.5-110B,mmlu_moral_scenarios,0-shot,accuracy,0.4994413407821229,0.016722491114073347
Qwen/Qwen1.5-110B,mmlu_philosophy,0-shot,accuracy,0.8295819935691319,0.021355343028264057
Qwen/Qwen1.5-110B,mmlu_prehistory,0-shot,accuracy,0.8518518518518519,0.019766459563597252
Qwen/Qwen1.5-110B,mmlu_professional_law,0-shot,accuracy,0.652542372881356,0.0121614177297498
Qwen/Qwen1.5-110B,mmlu_world_religions,0-shot,accuracy,0.9005847953216374,0.022949025579355027
Qwen/Qwen1.5-110B,mmlu_business_ethics,0-shot,accuracy,0.82,0.03861229196653693
Qwen/Qwen1.5-110B,mmlu_clinical_knowledge,0-shot,accuracy,0.8150943396226416,0.02389335183446431
Qwen/Qwen1.5-110B,mmlu_college_medicine,0-shot,accuracy,0.7456647398843931,0.0332055644308557
Qwen/Qwen1.5-110B,mmlu_global_facts,0-shot,accuracy,0.62,0.048783173121456316
Qwen/Qwen1.5-110B,mmlu_human_aging,0-shot,accuracy,0.820627802690583,0.025749819569192804
Qwen/Qwen1.5-110B,mmlu_management,0-shot,accuracy,0.8932038834951457,0.030581088928331356
Qwen/Qwen1.5-110B,mmlu_marketing,0-shot,accuracy,0.9401709401709402,0.015537514263253878
Qwen/Qwen1.5-110B,mmlu_medical_genetics,0-shot,accuracy,0.88,0.03265986323710906
Qwen/Qwen1.5-110B,mmlu_miscellaneous,0-shot,accuracy,0.9386973180076629,0.008578258902395434
Qwen/Qwen1.5-110B,mmlu_nutrition,0-shot,accuracy,0.8856209150326797,0.018224151682733433
Qwen/Qwen1.5-110B,mmlu_professional_accounting,0-shot,accuracy,0.6560283687943262,0.028338017428611324
Qwen/Qwen1.5-110B,mmlu_professional_medicine,0-shot,accuracy,0.9080882352941176,0.01754950743251041
Qwen/Qwen1.5-110B,mmlu_virology,0-shot,accuracy,0.5602409638554217,0.03864139923699121
Qwen/Qwen1.5-110B,mmlu_econometrics,0-shot,accuracy,0.6403508771929824,0.04514496132873633
Qwen/Qwen1.5-110B,mmlu_high_school_geography,0-shot,accuracy,0.9191919191919192,0.019417681889724536
Qwen/Qwen1.5-110B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9896373056994818,0.007308424386792208
Qwen/Qwen1.5-110B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.8153846153846154,0.01967163241310029
Qwen/Qwen1.5-110B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8991596638655462,0.019559663430480812
Qwen/Qwen1.5-110B,mmlu_high_school_psychology,0-shot,accuracy,0.9357798165137615,0.010510494713201425
Qwen/Qwen1.5-110B,mmlu_human_sexuality,0-shot,accuracy,0.8702290076335878,0.029473649496907065
Qwen/Qwen1.5-110B,mmlu_professional_psychology,0-shot,accuracy,0.8496732026143791,0.014458510616681903
Qwen/Qwen1.5-110B,mmlu_public_relations,0-shot,accuracy,0.7545454545454545,0.04122066502878284
Qwen/Qwen1.5-110B,mmlu_security_studies,0-shot,accuracy,0.8244897959183674,0.02435280072297001
Qwen/Qwen1.5-110B,mmlu_sociology,0-shot,accuracy,0.9203980099502488,0.019139685633503815
Qwen/Qwen1.5-110B,mmlu_us_foreign_policy,0-shot,accuracy,0.92,0.027265992434429086
Qwen/Qwen1.5-110B,mmlu_abstract_algebra,0-shot,accuracy,0.56,0.049888765156985884
Qwen/Qwen1.5-110B,mmlu_anatomy,0-shot,accuracy,0.7407407407407407,0.03785714465066653
Qwen/Qwen1.5-110B,mmlu_astronomy,0-shot,accuracy,0.9078947368421053,0.02353268597044349
Qwen/Qwen1.5-110B,mmlu_college_biology,0-shot,accuracy,0.9236111111111112,0.022212203938345915
Qwen/Qwen1.5-110B,mmlu_college_chemistry,0-shot,accuracy,0.58,0.049604496374885836
Qwen/Qwen1.5-110B,mmlu_college_computer_science,0-shot,accuracy,0.68,0.04688261722621504
Qwen/Qwen1.5-110B,mmlu_college_mathematics,0-shot,accuracy,0.54,0.05009082659620332
Qwen/Qwen1.5-110B,mmlu_college_physics,0-shot,accuracy,0.5686274509803921,0.04928099597287534
Qwen/Qwen1.5-110B,mmlu_computer_security,0-shot,accuracy,0.82,0.038612291966536934
Qwen/Qwen1.5-110B,mmlu_conceptual_physics,0-shot,accuracy,0.7829787234042553,0.02694748312149622
Qwen/Qwen1.5-110B,mmlu_electrical_engineering,0-shot,accuracy,0.7517241379310344,0.03600105692727771
Qwen/Qwen1.5-110B,mmlu_elementary_mathematics,0-shot,accuracy,0.791005291005291,0.02094048156533486
Qwen/Qwen1.5-110B,mmlu_high_school_biology,0-shot,accuracy,0.9032258064516129,0.016818943416345197
Qwen/Qwen1.5-110B,mmlu_high_school_chemistry,0-shot,accuracy,0.7339901477832512,0.03108982600293753
Qwen/Qwen1.5-110B,mmlu_high_school_computer_science,0-shot,accuracy,0.86,0.03487350880197769
Qwen/Qwen1.5-110B,mmlu_high_school_mathematics,0-shot,accuracy,0.5962962962962963,0.029914812342227624
Qwen/Qwen1.5-110B,mmlu_high_school_physics,0-shot,accuracy,0.5761589403973509,0.04034846678603396
Qwen/Qwen1.5-110B,mmlu_high_school_statistics,0-shot,accuracy,0.7175925925925926,0.030701372111510923
Qwen/Qwen1.5-110B,mmlu_machine_learning,0-shot,accuracy,0.5803571428571429,0.04684099321077106
meta-llama/Meta-Llama-3.1-70B,mmlu_formal_logic,0-shot,accuracy,0.5793650793650794,0.04415438226743745
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_european_history,0-shot,accuracy,0.8545454545454545,0.027530196355066584
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_us_history,0-shot,accuracy,0.946078431372549,0.01585246528110692
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_world_history,0-shot,accuracy,0.9367088607594937,0.01584958040054997
meta-llama/Meta-Llama-3.1-70B,mmlu_international_law,0-shot,accuracy,0.8842975206611571,0.029199802455622804
meta-llama/Meta-Llama-3.1-70B,mmlu_jurisprudence,0-shot,accuracy,0.8518518518518519,0.03434300243631003
meta-llama/Meta-Llama-3.1-70B,mmlu_logical_fallacies,0-shot,accuracy,0.8588957055214724,0.027351605518389752
meta-llama/Meta-Llama-3.1-70B,mmlu_moral_disputes,0-shot,accuracy,0.8439306358381503,0.019539014685374036
meta-llama/Meta-Llama-3.1-70B,mmlu_moral_scenarios,0-shot,accuracy,0.3128491620111732,0.015506892594647279
meta-llama/Meta-Llama-3.1-70B,mmlu_philosophy,0-shot,accuracy,0.8327974276527331,0.021193872528034948
meta-llama/Meta-Llama-3.1-70B,mmlu_prehistory,0-shot,accuracy,0.8888888888888888,0.0174864327858807
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_law,0-shot,accuracy,0.6199478487614081,0.012397328205137814
meta-llama/Meta-Llama-3.1-70B,mmlu_world_religions,0-shot,accuracy,0.8830409356725146,0.024648068961366183
meta-llama/Meta-Llama-3.1-70B,mmlu_business_ethics,0-shot,accuracy,0.78,0.04163331998932263
meta-llama/Meta-Llama-3.1-70B,mmlu_clinical_knowledge,0-shot,accuracy,0.8377358490566038,0.022691482872035363
meta-llama/Meta-Llama-3.1-70B,mmlu_college_medicine,0-shot,accuracy,0.7630057803468208,0.03242414757483098
meta-llama/Meta-Llama-3.1-70B,mmlu_global_facts,0-shot,accuracy,0.53,0.05016135580465919
meta-llama/Meta-Llama-3.1-70B,mmlu_human_aging,0-shot,accuracy,0.8071748878923767,0.02647824096048936
meta-llama/Meta-Llama-3.1-70B,mmlu_management,0-shot,accuracy,0.883495145631068,0.031766839486404075
meta-llama/Meta-Llama-3.1-70B,mmlu_marketing,0-shot,accuracy,0.9316239316239316,0.016534627684311368
meta-llama/Meta-Llama-3.1-70B,mmlu_medical_genetics,0-shot,accuracy,0.87,0.033799766898963086
meta-llama/Meta-Llama-3.1-70B,mmlu_miscellaneous,0-shot,accuracy,0.9054916985951469,0.010461015338193068
meta-llama/Meta-Llama-3.1-70B,mmlu_nutrition,0-shot,accuracy,0.8496732026143791,0.020464175124332618
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_accounting,0-shot,accuracy,0.6028368794326241,0.029189805673587095
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_medicine,0-shot,accuracy,0.8823529411764706,0.019571632869719708
meta-llama/Meta-Llama-3.1-70B,mmlu_virology,0-shot,accuracy,0.5481927710843374,0.03874371556587953
meta-llama/Meta-Llama-3.1-70B,mmlu_econometrics,0-shot,accuracy,0.6491228070175439,0.044895393502706986
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_geography,0-shot,accuracy,0.9040404040404041,0.020984808610047947
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9637305699481865,0.013492659751295134
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.8256410256410256,0.01923724980340523
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_microeconomics,0-shot,accuracy,0.865546218487395,0.022159373072744442
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_psychology,0-shot,accuracy,0.9247706422018349,0.011308662537571734
meta-llama/Meta-Llama-3.1-70B,mmlu_human_sexuality,0-shot,accuracy,0.8396946564885496,0.0321782942074463
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_psychology,0-shot,accuracy,0.8447712418300654,0.01464991304482305
meta-llama/Meta-Llama-3.1-70B,mmlu_public_relations,0-shot,accuracy,0.7636363636363637,0.04069306319721377
meta-llama/Meta-Llama-3.1-70B,mmlu_security_studies,0-shot,accuracy,0.8367346938775511,0.023661699177098622
meta-llama/Meta-Llama-3.1-70B,mmlu_sociology,0-shot,accuracy,0.9154228855721394,0.019675343217199173
meta-llama/Meta-Llama-3.1-70B,mmlu_us_foreign_policy,0-shot,accuracy,0.91,0.028762349126466125
meta-llama/Meta-Llama-3.1-70B,mmlu_abstract_algebra,0-shot,accuracy,0.45,0.05000000000000001
meta-llama/Meta-Llama-3.1-70B,mmlu_anatomy,0-shot,accuracy,0.7555555555555555,0.037125378336148665
meta-llama/Meta-Llama-3.1-70B,mmlu_astronomy,0-shot,accuracy,0.9013157894736842,0.024270227737522732
meta-llama/Meta-Llama-3.1-70B,mmlu_college_biology,0-shot,accuracy,0.9166666666666666,0.023112508176051233
meta-llama/Meta-Llama-3.1-70B,mmlu_college_chemistry,0-shot,accuracy,0.57,0.04975698519562428
meta-llama/Meta-Llama-3.1-70B,mmlu_college_computer_science,0-shot,accuracy,0.67,0.04725815626252609
meta-llama/Meta-Llama-3.1-70B,mmlu_college_mathematics,0-shot,accuracy,0.46,0.05009082659620333
meta-llama/Meta-Llama-3.1-70B,mmlu_college_physics,0-shot,accuracy,0.5686274509803921,0.04928099597287533
meta-llama/Meta-Llama-3.1-70B,mmlu_computer_security,0-shot,accuracy,0.86,0.03487350880197769
meta-llama/Meta-Llama-3.1-70B,mmlu_conceptual_physics,0-shot,accuracy,0.774468085106383,0.027321078417387533
meta-llama/Meta-Llama-3.1-70B,mmlu_electrical_engineering,0-shot,accuracy,0.7310344827586207,0.036951833116502325
meta-llama/Meta-Llama-3.1-70B,mmlu_elementary_mathematics,0-shot,accuracy,0.6243386243386243,0.024942368931159777
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_biology,0-shot,accuracy,0.8903225806451613,0.01777677870048516
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_chemistry,0-shot,accuracy,0.7339901477832512,0.03108982600293752
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_computer_science,0-shot,accuracy,0.84,0.03684529491774709
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_mathematics,0-shot,accuracy,0.4074074074074074,0.029958249250082118
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_physics,0-shot,accuracy,0.5298013245033113,0.04075224992216979
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_statistics,0-shot,accuracy,0.7129629629629629,0.03085199299325701
meta-llama/Meta-Llama-3.1-70B,mmlu_machine_learning,0-shot,accuracy,0.625,0.04595091388086298
meta-llama/Meta-Llama-3.1-70B,mmlu_formal_logic,5-shot,accuracy,0.6428571428571429,0.04285714285714281
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_european_history,5-shot,accuracy,0.8606060606060606,0.027045948825865366
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_us_history,5-shot,accuracy,0.9313725490196079,0.017744453647073315
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_world_history,5-shot,accuracy,0.9409282700421941,0.015346597463888684
meta-llama/Meta-Llama-3.1-70B,mmlu_international_law,5-shot,accuracy,0.8842975206611571,0.0291998024556228
meta-llama/Meta-Llama-3.1-70B,mmlu_jurisprudence,5-shot,accuracy,0.8796296296296297,0.031457038543062525
meta-llama/Meta-Llama-3.1-70B,mmlu_logical_fallacies,5-shot,accuracy,0.8773006134969326,0.025777328426978927
meta-llama/Meta-Llama-3.1-70B,mmlu_moral_disputes,5-shot,accuracy,0.8583815028901735,0.018771138684059014
meta-llama/Meta-Llama-3.1-70B,mmlu_moral_scenarios,5-shot,accuracy,0.6692737430167598,0.01573502625896612
meta-llama/Meta-Llama-3.1-70B,mmlu_philosophy,5-shot,accuracy,0.8520900321543409,0.020163253806284094
meta-llama/Meta-Llama-3.1-70B,mmlu_prehistory,5-shot,accuracy,0.904320987654321,0.016366973744175266
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_law,5-shot,accuracy,0.6192959582790091,0.012401430654645882
meta-llama/Meta-Llama-3.1-70B,mmlu_world_religions,5-shot,accuracy,0.9064327485380117,0.02233599323116327
meta-llama/Meta-Llama-3.1-70B,mmlu_business_ethics,5-shot,accuracy,0.81,0.039427724440366255
meta-llama/Meta-Llama-3.1-70B,mmlu_clinical_knowledge,5-shot,accuracy,0.8415094339622642,0.02247652871016771
meta-llama/Meta-Llama-3.1-70B,mmlu_college_medicine,5-shot,accuracy,0.7630057803468208,0.03242414757483098
meta-llama/Meta-Llama-3.1-70B,mmlu_global_facts,5-shot,accuracy,0.47,0.050161355804659205
meta-llama/Meta-Llama-3.1-70B,mmlu_human_aging,5-shot,accuracy,0.8251121076233184,0.02549528462644497
meta-llama/Meta-Llama-3.1-70B,mmlu_management,5-shot,accuracy,0.9029126213592233,0.02931596291881347
meta-llama/Meta-Llama-3.1-70B,mmlu_marketing,5-shot,accuracy,0.9273504273504274,0.01700436856813236
meta-llama/Meta-Llama-3.1-70B,mmlu_medical_genetics,5-shot,accuracy,0.88,0.03265986323710906
meta-llama/Meta-Llama-3.1-70B,mmlu_miscellaneous,5-shot,accuracy,0.9054916985951469,0.01046101533819307
meta-llama/Meta-Llama-3.1-70B,mmlu_nutrition,5-shot,accuracy,0.8823529411764706,0.018448530829034675
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_accounting,5-shot,accuracy,0.6347517730496454,0.028723863853281278
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_medicine,5-shot,accuracy,0.8897058823529411,0.019028947191474507
meta-llama/Meta-Llama-3.1-70B,mmlu_virology,5-shot,accuracy,0.572289156626506,0.038515976837185335
meta-llama/Meta-Llama-3.1-70B,mmlu_econometrics,5-shot,accuracy,0.6842105263157895,0.04372748290278007
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_geography,5-shot,accuracy,0.9393939393939394,0.016999994927421602
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_government_and_politics,5-shot,accuracy,0.9792746113989638,0.01028141701190904
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_macroeconomics,5-shot,accuracy,0.8333333333333334,0.018895524482604953
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_microeconomics,5-shot,accuracy,0.8865546218487395,0.020600225750204818
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_psychology,5-shot,accuracy,0.9412844036697248,0.010079470534014015
meta-llama/Meta-Llama-3.1-70B,mmlu_human_sexuality,5-shot,accuracy,0.8854961832061069,0.027927473753597453
meta-llama/Meta-Llama-3.1-70B,mmlu_professional_psychology,5-shot,accuracy,0.8464052287581699,0.014586690876223224
meta-llama/Meta-Llama-3.1-70B,mmlu_public_relations,5-shot,accuracy,0.7545454545454545,0.04122066502878285
meta-llama/Meta-Llama-3.1-70B,mmlu_security_studies,5-shot,accuracy,0.8408163265306122,0.023420972069166362
meta-llama/Meta-Llama-3.1-70B,mmlu_sociology,5-shot,accuracy,0.9402985074626866,0.01675368979152507
meta-llama/Meta-Llama-3.1-70B,mmlu_us_foreign_policy,5-shot,accuracy,0.94,0.023868325657594173
meta-llama/Meta-Llama-3.1-70B,mmlu_abstract_algebra,5-shot,accuracy,0.47,0.050161355804659205
meta-llama/Meta-Llama-3.1-70B,mmlu_anatomy,5-shot,accuracy,0.7703703703703704,0.036333844140734636
meta-llama/Meta-Llama-3.1-70B,mmlu_astronomy,5-shot,accuracy,0.9276315789473685,0.021085011261884105
meta-llama/Meta-Llama-3.1-70B,mmlu_college_biology,5-shot,accuracy,0.9305555555555556,0.02125797482283204
meta-llama/Meta-Llama-3.1-70B,mmlu_college_chemistry,5-shot,accuracy,0.57,0.04975698519562428
meta-llama/Meta-Llama-3.1-70B,mmlu_college_computer_science,5-shot,accuracy,0.73,0.0446196043338474
meta-llama/Meta-Llama-3.1-70B,mmlu_college_mathematics,5-shot,accuracy,0.56,0.04988876515698589
meta-llama/Meta-Llama-3.1-70B,mmlu_college_physics,5-shot,accuracy,0.5,0.04975185951049946
meta-llama/Meta-Llama-3.1-70B,mmlu_computer_security,5-shot,accuracy,0.87,0.033799766898963086
meta-llama/Meta-Llama-3.1-70B,mmlu_conceptual_physics,5-shot,accuracy,0.8127659574468085,0.025501588341883586
meta-llama/Meta-Llama-3.1-70B,mmlu_electrical_engineering,5-shot,accuracy,0.7655172413793103,0.035306258743465914
meta-llama/Meta-Llama-3.1-70B,mmlu_elementary_mathematics,5-shot,accuracy,0.5767195767195767,0.02544636563440677
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_biology,5-shot,accuracy,0.9096774193548387,0.016306570644488316
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_chemistry,5-shot,accuracy,0.729064039408867,0.031270907132976984
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_computer_science,5-shot,accuracy,0.87,0.033799766898963086
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_mathematics,5-shot,accuracy,0.4703703703703704,0.030431963547936577
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_physics,5-shot,accuracy,0.543046357615894,0.04067325174247443
meta-llama/Meta-Llama-3.1-70B,mmlu_high_school_statistics,5-shot,accuracy,0.7175925925925926,0.030701372111510934
meta-llama/Meta-Llama-3.1-70B,mmlu_machine_learning,5-shot,accuracy,0.6607142857142857,0.04493949068613539
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,bleu_max,21.984890774035545,0.7770739126704136
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,bleu_acc,0.3708690330477356,0.016909693580248835
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,bleu_diff,-1.777639208678571,0.7503516775675271
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rouge1_max,44.89275656733229,0.9449349576719165
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rouge1_acc,0.37209302325581395,0.016921090118814035
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rouge1_diff,-3.3399767496918664,0.9016684628926397
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rouge2_max,31.148807722180507,1.0025496938118115
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rouge2_acc,0.32558139534883723,0.01640398946990783
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rouge2_diff,-4.274714991502365,1.0613341025571699
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rougeL_max,42.1326972047669,0.9418403334319636
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rougeL_acc,0.3708690330477356,0.016909693580248835
meta-llama/Meta-Llama-3.1-70B,truthfulqa_gen,0-shot,rougeL_diff,-3.6709278805778522,0.9173664530107648
meta-llama/Meta-Llama-3.1-70B,truthfulqa_mc1,0-shot,accuracy,0.3292533659730722,0.01645126444006822
meta-llama/Meta-Llama-3.1-70B,truthfulqa_mc2,0-shot,accuracy,0.4984024747756476,0.014101107603435808
meta-llama/Meta-Llama-3.1-70B,winogrande,5-shot,accuracy,0.8539857932123125,0.009924440374585241
Qwen/Qwen1.5-110B,humaneval,0-shot,accuracy,0.5487804878048781,
AbacusResearch/Jallabi-34B,anli_r1,0-shot,brier_score,0.5939763633980498,
AbacusResearch/Jallabi-34B,anli_r2,0-shot,brier_score,0.6347790012373384,
AbacusResearch/Jallabi-34B,anli_r3,0-shot,brier_score,0.6138727310326602,
AbacusResearch/Jallabi-34B,logiqa2,0-shot,brier_score,0.8754278626618299,
AbacusResearch/Jallabi-34B,mathqa,0-shot,brier_score,0.7472976364377096,
AbacusResearch/Jallabi-34B,xnli_ar,0-shot,brier_score,1.0864594338535682,
AbacusResearch/Jallabi-34B,xnli_bg,0-shot,brier_score,0.8761748647351875,
AbacusResearch/Jallabi-34B,xnli_de,0-shot,brier_score,0.7947084856942528,
AbacusResearch/Jallabi-34B,xnli_el,0-shot,brier_score,0.8248232083509227,
AbacusResearch/Jallabi-34B,xnli_en,0-shot,brier_score,0.6406212964548199,
AbacusResearch/Jallabi-34B,xnli_es,0-shot,brier_score,0.8840559310644012,
AbacusResearch/Jallabi-34B,xnli_fr,0-shot,brier_score,0.7798253847230157,
AbacusResearch/Jallabi-34B,xnli_hi,0-shot,brier_score,0.8417850304725969,
AbacusResearch/Jallabi-34B,xnli_ru,0-shot,brier_score,0.7693571108492075,
AbacusResearch/Jallabi-34B,xnli_sw,0-shot,brier_score,0.7842726603920626,
AbacusResearch/Jallabi-34B,xnli_th,0-shot,brier_score,0.8716524864270154,
AbacusResearch/Jallabi-34B,xnli_tr,0-shot,brier_score,0.8567911205029841,
AbacusResearch/Jallabi-34B,xnli_ur,0-shot,brier_score,1.177826559883124,
AbacusResearch/Jallabi-34B,xnli_vi,0-shot,brier_score,0.8427822067583179,
AbacusResearch/Jallabi-34B,xnli_zh,0-shot,brier_score,1.0424648600985114,
AbacusResearch/Jallabi-34B,humaneval,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-70B,humaneval,0-shot,accuracy,0.4146341463414634,
meta-llama/Meta-Llama-3.1-70B,humaneval,0-shot,accuracy,0.5121951219512195,
meta-llama/Meta-Llama-3.1-70B,anli_r1,0-shot,brier_score,0.6071925346327082,
meta-llama/Meta-Llama-3.1-70B,anli_r2,0-shot,brier_score,0.6219101456688398,
meta-llama/Meta-Llama-3.1-70B,anli_r3,0-shot,brier_score,0.6173417527137168,
meta-llama/Meta-Llama-3.1-70B,logiqa2,0-shot,brier_score,0.7097161018431145,
meta-llama/Meta-Llama-3.1-70B,mathqa,0-shot,brier_score,0.6314149631092432,
meta-llama/Meta-Llama-3.1-70B,xnli_ar,0-shot,brier_score,1.2071098371679319,
meta-llama/Meta-Llama-3.1-70B,xnli_bg,0-shot,brier_score,0.8520065775162584,
meta-llama/Meta-Llama-3.1-70B,xnli_de,0-shot,brier_score,0.7743623152242308,
meta-llama/Meta-Llama-3.1-70B,xnli_el,0-shot,brier_score,0.7523209172898698,
meta-llama/Meta-Llama-3.1-70B,xnli_en,0-shot,brier_score,0.6792682750257742,
meta-llama/Meta-Llama-3.1-70B,xnli_es,0-shot,brier_score,0.8323398038874044,
meta-llama/Meta-Llama-3.1-70B,xnli_fr,0-shot,brier_score,0.7364607552154235,
meta-llama/Meta-Llama-3.1-70B,xnli_hi,0-shot,brier_score,0.735023626634317,
meta-llama/Meta-Llama-3.1-70B,xnli_ru,0-shot,brier_score,0.7634519587344754,
meta-llama/Meta-Llama-3.1-70B,xnli_sw,0-shot,brier_score,0.7740529099506941,
meta-llama/Meta-Llama-3.1-70B,xnli_th,0-shot,brier_score,0.7419994930347813,
meta-llama/Meta-Llama-3.1-70B,xnli_tr,0-shot,brier_score,0.7759135605889255,
meta-llama/Meta-Llama-3.1-70B,xnli_ur,0-shot,brier_score,1.0360701437534388,
meta-llama/Meta-Llama-3.1-70B,xnli_vi,0-shot,brier_score,0.7877570139182698,
meta-llama/Meta-Llama-3.1-70B,xnli_zh,0-shot,brier_score,1.0071682939292674,
Qwen/Qwen1.5-110B,anli_r1,0-shot,brier_score,0.7032319877170848,
Qwen/Qwen1.5-110B,anli_r2,0-shot,brier_score,0.7227464051591158,
Qwen/Qwen1.5-110B,anli_r3,0-shot,brier_score,0.7057967672798352,
Qwen/Qwen1.5-110B,logiqa2,0-shot,brier_score,0.6588035358256442,
Qwen/Qwen1.5-110B,mathqa,0-shot,brier_score,0.7028395140869409,
Qwen/Qwen1.5-110B,xnli_ar,0-shot,brier_score,1.1290451927873673,
Qwen/Qwen1.5-110B,xnli_bg,0-shot,brier_score,0.8396113361676188,
Qwen/Qwen1.5-110B,xnli_de,0-shot,brier_score,0.7824896200918989,
Qwen/Qwen1.5-110B,xnli_el,0-shot,brier_score,0.8954486194580706,
Qwen/Qwen1.5-110B,xnli_en,0-shot,brier_score,0.7036803802765601,
Qwen/Qwen1.5-110B,xnli_es,0-shot,brier_score,0.8098659965562826,
Qwen/Qwen1.5-110B,xnli_fr,0-shot,brier_score,0.7623290598320213,
Qwen/Qwen1.5-110B,xnli_hi,0-shot,brier_score,0.8096685122746652,
Qwen/Qwen1.5-110B,xnli_ru,0-shot,brier_score,0.7521460516236584,
Qwen/Qwen1.5-110B,xnli_sw,0-shot,brier_score,0.8631075541388825,
Qwen/Qwen1.5-110B,xnli_th,0-shot,brier_score,0.8257113037582946,
Qwen/Qwen1.5-110B,xnli_tr,0-shot,brier_score,0.8729170656394402,
Qwen/Qwen1.5-110B,xnli_ur,0-shot,brier_score,0.9692476239407187,
Qwen/Qwen1.5-110B,xnli_vi,0-shot,brier_score,0.8260133372288128,
Qwen/Qwen1.5-110B,xnli_zh,0-shot,brier_score,1.0940834545794687,
Qwen/Qwen2-72B,anli_r1,0-shot,brier_score,0.5093057256862707,
Qwen/Qwen2-72B,anli_r2,0-shot,brier_score,0.5734405935181002,
Qwen/Qwen2-72B,anli_r3,0-shot,brier_score,0.575751462708844,
Qwen/Qwen2-72B,logiqa2,0-shot,brier_score,0.5903687785383076,
Qwen/Qwen2-72B,mathqa,0-shot,brier_score,0.5932972217667383,
Qwen/Qwen2-72B,xnli_ar,0-shot,brier_score,1.268307566326948,
Qwen/Qwen2-72B,xnli_bg,0-shot,brier_score,0.7925672941494593,
Qwen/Qwen2-72B,xnli_de,0-shot,brier_score,0.8165958624506711,
Qwen/Qwen2-72B,xnli_el,0-shot,brier_score,0.9146391303390352,
Qwen/Qwen2-72B,xnli_en,0-shot,brier_score,0.6711729609496091,
Qwen/Qwen2-72B,xnli_es,0-shot,brier_score,0.7918015415646688,
Qwen/Qwen2-72B,xnli_fr,0-shot,brier_score,0.7770745662824733,
Qwen/Qwen2-72B,xnli_hi,0-shot,brier_score,0.7429476955702332,
Qwen/Qwen2-72B,xnli_ru,0-shot,brier_score,0.7661362674199756,
Qwen/Qwen2-72B,xnli_sw,0-shot,brier_score,0.8168077769241109,
Qwen/Qwen2-72B,xnli_th,0-shot,brier_score,0.7829142037393128,
Qwen/Qwen2-72B,xnli_tr,0-shot,brier_score,0.8721175562261136,
Qwen/Qwen2-72B,xnli_ur,0-shot,brier_score,1.0767755716673717,
Qwen/Qwen2-72B,xnli_vi,0-shot,brier_score,0.8210976977350116,
Qwen/Qwen2-72B,xnli_zh,0-shot,brier_score,0.971363056059206,
Qwen/Qwen2.5-72B,anli_r1,0-shot,brier_score,0.5530480931930974,
Qwen/Qwen2.5-72B,anli_r2,0-shot,brier_score,0.6219219259251367,
Qwen/Qwen2.5-72B,anli_r3,0-shot,brier_score,0.5907234645340913,
Qwen/Qwen2.5-72B,logiqa2,0-shot,brier_score,0.5163734100169589,
Qwen/Qwen2.5-72B,mathqa,0-shot,brier_score,0.5970668088920026,
Qwen/Qwen2.5-72B,xnli_ar,0-shot,brier_score,1.0607185931952678,
Qwen/Qwen2.5-72B,xnli_bg,0-shot,brier_score,0.7287898698184277,
Qwen/Qwen2.5-72B,xnli_de,0-shot,brier_score,0.8344040468597513,
Qwen/Qwen2.5-72B,xnli_el,0-shot,brier_score,0.8712391919515994,
Qwen/Qwen2.5-72B,xnli_en,0-shot,brier_score,0.6865434757595162,
Qwen/Qwen2.5-72B,xnli_es,0-shot,brier_score,0.834539698753449,
Qwen/Qwen2.5-72B,xnli_fr,0-shot,brier_score,0.7425688889313649,
Qwen/Qwen2.5-72B,xnli_hi,0-shot,brier_score,0.7664856184717933,
Qwen/Qwen2.5-72B,xnli_ru,0-shot,brier_score,0.7092142489378831,
Qwen/Qwen2.5-72B,xnli_sw,0-shot,brier_score,0.834025131653735,
Qwen/Qwen2.5-72B,xnli_th,0-shot,brier_score,0.7590628986576033,
Qwen/Qwen2.5-72B,xnli_tr,0-shot,brier_score,0.8312265845842917,
Qwen/Qwen2.5-72B,xnli_ur,0-shot,brier_score,0.8610535606357043,
Qwen/Qwen2.5-72B,xnli_vi,0-shot,brier_score,0.7392686538537654,
Qwen/Qwen2.5-72B,xnli_zh,0-shot,brier_score,0.979474275567642,
meta-llama/Llama-2-70b-hf,anli_r1,0-shot,brier_score,0.6756612994403143,
meta-llama/Llama-2-70b-hf,anli_r2,0-shot,brier_score,0.6572226818747267,
meta-llama/Llama-2-70b-hf,anli_r3,0-shot,brier_score,0.6493467645731651,
meta-llama/Llama-2-70b-hf,logiqa2,0-shot,brier_score,0.7826571607692985,
meta-llama/Llama-2-70b-hf,mathqa,0-shot,brier_score,0.7897677333347816,
meta-llama/Llama-2-70b-hf,xnli_ar,0-shot,brier_score,1.2130641619086282,
meta-llama/Llama-2-70b-hf,xnli_bg,0-shot,brier_score,0.855309824750747,
meta-llama/Llama-2-70b-hf,xnli_de,0-shot,brier_score,0.7928847001729086,
meta-llama/Llama-2-70b-hf,xnli_el,0-shot,brier_score,0.7948031260006005,
meta-llama/Llama-2-70b-hf,xnli_en,0-shot,brier_score,0.6400608139509957,
meta-llama/Llama-2-70b-hf,xnli_es,0-shot,brier_score,0.848705601589917,
meta-llama/Llama-2-70b-hf,xnli_fr,0-shot,brier_score,0.7453924269432578,
meta-llama/Llama-2-70b-hf,xnli_hi,0-shot,brier_score,0.8364536236859881,
meta-llama/Llama-2-70b-hf,xnli_ru,0-shot,brier_score,0.756608101784352,
meta-llama/Llama-2-70b-hf,xnli_sw,0-shot,brier_score,0.7931705000845496,
meta-llama/Llama-2-70b-hf,xnli_th,0-shot,brier_score,0.7912810830368358,
meta-llama/Llama-2-70b-hf,xnli_tr,0-shot,brier_score,0.8434901680880397,
meta-llama/Llama-2-70b-hf,xnli_ur,0-shot,brier_score,0.9976906248267555,
meta-llama/Llama-2-70b-hf,xnli_vi,0-shot,brier_score,0.7685972942319997,
meta-llama/Llama-2-70b-hf,xnli_zh,0-shot,brier_score,0.9681328415784646,
meta-llama/Meta-Llama-3-70B,anli_r1,0-shot,brier_score,0.6187985947650089,
meta-llama/Meta-Llama-3-70B,anli_r2,0-shot,brier_score,0.6208829688369709,
meta-llama/Meta-Llama-3-70B,anli_r3,0-shot,brier_score,0.6250376704356775,
meta-llama/Meta-Llama-3-70B,logiqa2,0-shot,brier_score,0.6343541690784891,
meta-llama/Meta-Llama-3-70B,mathqa,0-shot,brier_score,0.6456540866154749,
meta-llama/Meta-Llama-3-70B,xnli_ar,0-shot,brier_score,1.2386147362243198,
meta-llama/Meta-Llama-3-70B,xnli_bg,0-shot,brier_score,0.8552126408341112,
meta-llama/Meta-Llama-3-70B,xnli_de,0-shot,brier_score,0.776637083275541,
meta-llama/Meta-Llama-3-70B,xnli_el,0-shot,brier_score,0.8183814377420586,
meta-llama/Meta-Llama-3-70B,xnli_en,0-shot,brier_score,0.6762824814407985,
meta-llama/Meta-Llama-3-70B,xnli_es,0-shot,brier_score,0.834943415994751,
meta-llama/Meta-Llama-3-70B,xnli_fr,0-shot,brier_score,0.7373538093134094,
meta-llama/Meta-Llama-3-70B,xnli_hi,0-shot,brier_score,0.7699847512972416,
meta-llama/Meta-Llama-3-70B,xnli_ru,0-shot,brier_score,0.7487629681125629,
meta-llama/Meta-Llama-3-70B,xnli_sw,0-shot,brier_score,0.7773162894223107,
meta-llama/Meta-Llama-3-70B,xnli_th,0-shot,brier_score,0.7571858265802022,
meta-llama/Meta-Llama-3-70B,xnli_tr,0-shot,brier_score,0.7681267633944308,
meta-llama/Meta-Llama-3-70B,xnli_ur,0-shot,brier_score,1.0699126792788578,
meta-llama/Meta-Llama-3-70B,xnli_vi,0-shot,brier_score,0.7734301496377668,
meta-llama/Meta-Llama-3-70B,xnli_zh,0-shot,brier_score,0.9120775638117163,
google/gemma-2-9b,mmlu_abstract_algebra,5-shot,accuracy,0.42,0.049604496374885836
google/gemma-2-9b,mmlu_anatomy,5-shot,accuracy,0.7037037037037037,0.03944624162501116
google/gemma-2-9b,mmlu_astronomy,5-shot,accuracy,0.7631578947368421,0.03459777606810537
google/gemma-2-9b,mmlu_business_ethics,5-shot,accuracy,0.67,0.04725815626252607
google/gemma-2-9b,mmlu_clinical_knowledge,5-shot,accuracy,0.769811320754717,0.025907897122408173
google/gemma-2-9b,mmlu_college_biology,5-shot,accuracy,0.8680555555555556,0.028300968382044423
google/gemma-2-9b,mmlu_college_chemistry,5-shot,accuracy,0.57,0.04975698519562428
google/gemma-2-9b,mmlu_college_computer_science,5-shot,accuracy,0.52,0.050211673156867795
google/gemma-2-9b,mmlu_college_mathematics,5-shot,accuracy,0.45,0.049999999999999996
google/gemma-2-9b,mmlu_college_medicine,5-shot,accuracy,0.6820809248554913,0.03550683989165582
google/gemma-2-9b,mmlu_college_physics,5-shot,accuracy,0.5294117647058824,0.049665709039785295
google/gemma-2-9b,mmlu_computer_security,5-shot,accuracy,0.76,0.042923469599092816
google/gemma-2-9b,mmlu_conceptual_physics,5-shot,accuracy,0.6851063829787234,0.030363582197238153
google/gemma-2-9b,mmlu_econometrics,5-shot,accuracy,0.5614035087719298,0.04668000738510455
google/gemma-2-9b,mmlu_electrical_engineering,5-shot,accuracy,0.6896551724137931,0.03855289616378947
google/gemma-2-9b,mmlu_elementary_mathematics,5-shot,accuracy,0.5846560846560847,0.025379524910778398
google/gemma-2-9b,mmlu_formal_logic,5-shot,accuracy,0.4603174603174603,0.04458029125470973
google/gemma-2-9b,mmlu_global_facts,5-shot,accuracy,0.5,0.050251890762960605
google/gemma-2-9b,mmlu_high_school_biology,5-shot,accuracy,0.8870967741935484,0.018003603325863614
google/gemma-2-9b,mmlu_high_school_chemistry,5-shot,accuracy,0.6551724137931034,0.033442837442804574
google/gemma-2-9b,mmlu_high_school_computer_science,5-shot,accuracy,0.77,0.04229525846816506
google/gemma-2-9b,mmlu_high_school_european_history,5-shot,accuracy,0.7757575757575758,0.03256866661681102
google/gemma-2-9b,mmlu_high_school_geography,5-shot,accuracy,0.8787878787878788,0.023253157951942067
google/gemma-2-9b,mmlu_high_school_government_and_politics,5-shot,accuracy,0.9326424870466321,0.018088393839078922
google/gemma-2-9b,mmlu_high_school_macroeconomics,5-shot,accuracy,0.764102564102564,0.02152596540740872
google/gemma-2-9b,mmlu_high_school_mathematics,5-shot,accuracy,0.45555555555555555,0.03036486250482443
google/gemma-2-9b,mmlu_high_school_microeconomics,5-shot,accuracy,0.8151260504201681,0.025215992877954205
google/gemma-2-9b,mmlu_high_school_physics,5-shot,accuracy,0.5562913907284768,0.04056527902281731
google/gemma-2-9b,mmlu_high_school_psychology,5-shot,accuracy,0.8990825688073395,0.012914673545364441
google/gemma-2-9b,mmlu_high_school_statistics,5-shot,accuracy,0.6620370370370371,0.03225941352631296
google/gemma-2-9b,mmlu_high_school_us_history,5-shot,accuracy,0.8725490196078431,0.02340553048084631
google/gemma-2-9b,mmlu_high_school_world_history,5-shot,accuracy,0.8523206751054853,0.02309432958259569
google/gemma-2-9b,mmlu_human_aging,5-shot,accuracy,0.7668161434977578,0.028380391147094702
google/gemma-2-9b,mmlu_human_sexuality,5-shot,accuracy,0.7938931297709924,0.03547771004159462
google/gemma-2-9b,mmlu_international_law,5-shot,accuracy,0.8429752066115702,0.03321244842547128
google/gemma-2-9b,mmlu_jurisprudence,5-shot,accuracy,0.8148148148148148,0.03755265865037182
google/gemma-2-9b,mmlu_logical_fallacies,5-shot,accuracy,0.8282208588957055,0.02963471727237103
google/gemma-2-9b,mmlu_machine_learning,5-shot,accuracy,0.45535714285714285,0.04726835553719099
google/gemma-2-9b,mmlu_management,5-shot,accuracy,0.7961165048543689,0.0398913985953177
google/gemma-2-9b,mmlu_marketing,5-shot,accuracy,0.8974358974358975,0.019875655027867457
google/gemma-2-9b,mmlu_medical_genetics,5-shot,accuracy,0.82,0.03861229196653694
google/gemma-2-9b,mmlu_miscellaneous,5-shot,accuracy,0.8492975734355045,0.012793420883120821
google/gemma-2-9b,mmlu_moral_disputes,5-shot,accuracy,0.7341040462427746,0.02378620325550829
google/gemma-2-9b,mmlu_moral_scenarios,5-shot,accuracy,0.2905027932960894,0.015183844307206151
google/gemma-2-9b,mmlu_nutrition,5-shot,accuracy,0.7549019607843137,0.024630048979824775
google/gemma-2-9b,mmlu_philosophy,5-shot,accuracy,0.7331189710610932,0.025122637608816646
google/gemma-2-9b,mmlu_prehistory,5-shot,accuracy,0.7777777777777778,0.023132376234543332
google/gemma-2-9b,mmlu_professional_accounting,5-shot,accuracy,0.5283687943262412,0.029779450957303062
google/gemma-2-9b,mmlu_professional_law,5-shot,accuracy,0.5352020860495437,0.012738547371303956
google/gemma-2-9b,mmlu_professional_medicine,5-shot,accuracy,0.7683823529411765,0.025626533803777565
google/gemma-2-9b,mmlu_professional_psychology,5-shot,accuracy,0.7434640522875817,0.01766784161237897
google/gemma-2-9b,mmlu_public_relations,5-shot,accuracy,0.7636363636363637,0.04069306319721376
google/gemma-2-9b,mmlu_security_studies,5-shot,accuracy,0.7591836734693878,0.027372942201788163
google/gemma-2-9b,mmlu_sociology,5-shot,accuracy,0.8606965174129353,0.024484487162913973
google/gemma-2-9b,mmlu_us_foreign_policy,5-shot,accuracy,0.91,0.028762349126466115
google/gemma-2-9b,mmlu_virology,5-shot,accuracy,0.5180722891566265,0.038899512528272166
google/gemma-2-9b,mmlu_world_religions,5-shot,accuracy,0.8596491228070176,0.0266405825391332
