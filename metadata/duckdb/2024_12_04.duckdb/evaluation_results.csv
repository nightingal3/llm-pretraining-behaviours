id,benchmark,setting,metric,metric_value,metric_stderr
Deci/DeciCoder-1b,drop,3-shot,accuracy,0.0006291946308724832,0.0002568002749723942
Deci/DeciCoder-1b,drop,3-shot,f1,0.02978817114093966,0.0009513874747103622
Deci/DeciCoder-1b,gsm8k,5-shot,accuracy,0.017437452615617893,0.003605486867998233
Deci/DeciCoder-1b,winogrande,5-shot,accuracy,0.5082872928176796,0.014050555322824189
Deci/DeciCoder-1b,arc:challenge,25-shot,accuracy,0.16040955631399317,0.010724336059110964
Deci/DeciCoder-1b,arc:challenge,25-shot,acc_norm,0.21160409556313994,0.011935916358632875
Deci/DeciCoder-1b,hellaswag,10-shot,accuracy,0.2826130252937662,0.004493495872000129
Deci/DeciCoder-1b,hellaswag,10-shot,acc_norm,0.31089424417446726,0.004619136497359843
Deci/DeciCoder-1b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.29,0.04560480215720683
Deci/DeciCoder-1b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.29,0.04560480215720683
Deci/DeciCoder-1b,hendrycksTest-anatomy,5-shot,accuracy,0.25925925925925924,0.03785714465066654
Deci/DeciCoder-1b,hendrycksTest-anatomy,5-shot,acc_norm,0.25925925925925924,0.03785714465066654
Deci/DeciCoder-1b,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.035834961763610645
Deci/DeciCoder-1b,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.035834961763610645
Deci/DeciCoder-1b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.047258156262526045
Deci/DeciCoder-1b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.047258156262526045
Deci/DeciCoder-1b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2,0.024618298195866518
Deci/DeciCoder-1b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2,0.024618298195866518
Deci/DeciCoder-1b,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
Deci/DeciCoder-1b,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.03685651095897532
Deci/DeciCoder-1b,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.04408440022768078
Deci/DeciCoder-1b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.04408440022768078
Deci/DeciCoder-1b,hendrycksTest-college_computer_science,5-shot,accuracy,0.19,0.039427724440366255
Deci/DeciCoder-1b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.19,0.039427724440366255
Deci/DeciCoder-1b,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.04163331998932268
Deci/DeciCoder-1b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.04163331998932268
Deci/DeciCoder-1b,hendrycksTest-college_medicine,5-shot,accuracy,0.2254335260115607,0.03186209851641144
Deci/DeciCoder-1b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2254335260115607,0.03186209851641144
Deci/DeciCoder-1b,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.043364327079931785
Deci/DeciCoder-1b,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.043364327079931785
Deci/DeciCoder-1b,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
Deci/DeciCoder-1b,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
Deci/DeciCoder-1b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.28085106382978725,0.02937917046412482
Deci/DeciCoder-1b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.28085106382978725,0.02937917046412482
Deci/DeciCoder-1b,hendrycksTest-econometrics,5-shot,accuracy,0.20175438596491227,0.037752050135836386
Deci/DeciCoder-1b,hendrycksTest-econometrics,5-shot,acc_norm,0.20175438596491227,0.037752050135836386
Deci/DeciCoder-1b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.22758620689655173,0.03493950380131184
Deci/DeciCoder-1b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.22758620689655173,0.03493950380131184
Deci/DeciCoder-1b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2328042328042328,0.02176596167215453
Deci/DeciCoder-1b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2328042328042328,0.02176596167215453
Deci/DeciCoder-1b,hendrycksTest-formal_logic,5-shot,accuracy,0.2698412698412698,0.03970158273235173
Deci/DeciCoder-1b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2698412698412698,0.03970158273235173
Deci/DeciCoder-1b,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.046056618647183814
Deci/DeciCoder-1b,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.046056618647183814
Deci/DeciCoder-1b,hendrycksTest-high_school_biology,5-shot,accuracy,0.17419354838709677,0.021576248184514583
Deci/DeciCoder-1b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.17419354838709677,0.021576248184514583
Deci/DeciCoder-1b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.18719211822660098,0.027444924966882618
Deci/DeciCoder-1b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.18719211822660098,0.027444924966882618
Deci/DeciCoder-1b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.3,0.046056618647183814
Deci/DeciCoder-1b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.3,0.046056618647183814
Deci/DeciCoder-1b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23030303030303031,0.03287666758603488
Deci/DeciCoder-1b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23030303030303031,0.03287666758603488
Deci/DeciCoder-1b,hendrycksTest-high_school_geography,5-shot,accuracy,0.20707070707070707,0.02886977846026705
Deci/DeciCoder-1b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.20707070707070707,0.02886977846026705
Deci/DeciCoder-1b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.20725388601036268,0.029252823291803617
Deci/DeciCoder-1b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.20725388601036268,0.029252823291803617
Deci/DeciCoder-1b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.19487179487179487,0.02008316759518139
Deci/DeciCoder-1b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.19487179487179487,0.02008316759518139
Deci/DeciCoder-1b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24444444444444444,0.026202766534652148
Deci/DeciCoder-1b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24444444444444444,0.026202766534652148
Deci/DeciCoder-1b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.19327731092436976,0.02564947026588919
Deci/DeciCoder-1b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.19327731092436976,0.02564947026588919
Deci/DeciCoder-1b,hendrycksTest-high_school_physics,5-shot,accuracy,0.26490066225165565,0.03603038545360384
Deci/DeciCoder-1b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.26490066225165565,0.03603038545360384
Deci/DeciCoder-1b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1908256880733945,0.016847676400091112
Deci/DeciCoder-1b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1908256880733945,0.016847676400091112
Deci/DeciCoder-1b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1712962962962963,0.025695341643824688
Deci/DeciCoder-1b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1712962962962963,0.025695341643824688
Deci/DeciCoder-1b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25980392156862747,0.030778554678693268
Deci/DeciCoder-1b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25980392156862747,0.030778554678693268
Deci/DeciCoder-1b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.028900721906293426
Deci/DeciCoder-1b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.028900721906293426
Deci/DeciCoder-1b,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.031024411740572206
Deci/DeciCoder-1b,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.031024411740572206
Deci/DeciCoder-1b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2824427480916031,0.03948406125768361
Deci/DeciCoder-1b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2824427480916031,0.03948406125768361
Deci/DeciCoder-1b,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.03984979653302871
Deci/DeciCoder-1b,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.03984979653302871
Deci/DeciCoder-1b,hendrycksTest-jurisprudence,5-shot,accuracy,0.23148148148148148,0.04077494709252626
Deci/DeciCoder-1b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.23148148148148148,0.04077494709252626
Deci/DeciCoder-1b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26380368098159507,0.03462419931615623
Deci/DeciCoder-1b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26380368098159507,0.03462419931615623
Deci/DeciCoder-1b,hendrycksTest-machine_learning,5-shot,accuracy,0.25892857142857145,0.041577515398656284
Deci/DeciCoder-1b,hendrycksTest-machine_learning,5-shot,acc_norm,0.25892857142857145,0.041577515398656284
Deci/DeciCoder-1b,hendrycksTest-management,5-shot,accuracy,0.18446601941747573,0.03840423627288276
Deci/DeciCoder-1b,hendrycksTest-management,5-shot,acc_norm,0.18446601941747573,0.03840423627288276
Deci/DeciCoder-1b,hendrycksTest-marketing,5-shot,accuracy,0.28205128205128205,0.02948036054954119
Deci/DeciCoder-1b,hendrycksTest-marketing,5-shot,acc_norm,0.28205128205128205,0.02948036054954119
Deci/DeciCoder-1b,hendrycksTest-medical_genetics,5-shot,accuracy,0.2,0.04020151261036845
Deci/DeciCoder-1b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.2,0.04020151261036845
Deci/DeciCoder-1b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2796934865900383,0.01605079214803655
Deci/DeciCoder-1b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2796934865900383,0.01605079214803655
Deci/DeciCoder-1b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2398843930635838,0.022989592543123567
Deci/DeciCoder-1b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2398843930635838,0.022989592543123567
Deci/DeciCoder-1b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24692737430167597,0.014422292204808835
Deci/DeciCoder-1b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24692737430167597,0.014422292204808835
Deci/DeciCoder-1b,hendrycksTest-nutrition,5-shot,accuracy,0.21895424836601307,0.02367908986180772
Deci/DeciCoder-1b,hendrycksTest-nutrition,5-shot,acc_norm,0.21895424836601307,0.02367908986180772
Deci/DeciCoder-1b,hendrycksTest-philosophy,5-shot,accuracy,0.2829581993569132,0.025583062489984827
Deci/DeciCoder-1b,hendrycksTest-philosophy,5-shot,acc_norm,0.2829581993569132,0.025583062489984827
Deci/DeciCoder-1b,hendrycksTest-prehistory,5-shot,accuracy,0.24382716049382716,0.023891879541959614
Deci/DeciCoder-1b,hendrycksTest-prehistory,5-shot,acc_norm,0.24382716049382716,0.023891879541959614
Deci/DeciCoder-1b,hendrycksTest-professional_accounting,5-shot,accuracy,0.25177304964539005,0.025892151156709405
Deci/DeciCoder-1b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25177304964539005,0.025892151156709405
Deci/DeciCoder-1b,hendrycksTest-professional_law,5-shot,accuracy,0.24185136897001303,0.010936550813827066
Deci/DeciCoder-1b,hendrycksTest-professional_law,5-shot,acc_norm,0.24185136897001303,0.010936550813827066
Deci/DeciCoder-1b,hendrycksTest-professional_medicine,5-shot,accuracy,0.17279411764705882,0.022966067585581788
Deci/DeciCoder-1b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.17279411764705882,0.022966067585581788
Deci/DeciCoder-1b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2826797385620915,0.018217269552053435
Deci/DeciCoder-1b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2826797385620915,0.018217269552053435
Deci/DeciCoder-1b,hendrycksTest-public_relations,5-shot,accuracy,0.24545454545454545,0.04122066502878284
Deci/DeciCoder-1b,hendrycksTest-public_relations,5-shot,acc_norm,0.24545454545454545,0.04122066502878284
Deci/DeciCoder-1b,hendrycksTest-security_studies,5-shot,accuracy,0.19591836734693877,0.025409301953225678
Deci/DeciCoder-1b,hendrycksTest-security_studies,5-shot,acc_norm,0.19591836734693877,0.025409301953225678
Deci/DeciCoder-1b,hendrycksTest-sociology,5-shot,accuracy,0.25870646766169153,0.030965903123573012
Deci/DeciCoder-1b,hendrycksTest-sociology,5-shot,acc_norm,0.25870646766169153,0.030965903123573012
Deci/DeciCoder-1b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.0440844002276808
Deci/DeciCoder-1b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.0440844002276808
Deci/DeciCoder-1b,hendrycksTest-virology,5-shot,accuracy,0.25301204819277107,0.03384429155233135
Deci/DeciCoder-1b,hendrycksTest-virology,5-shot,acc_norm,0.25301204819277107,0.03384429155233135
Deci/DeciCoder-1b,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.03508771929824563
Deci/DeciCoder-1b,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.03508771929824563
Deci/DeciCoder-1b,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.015321821688476194
Deci/DeciCoder-1b,truthfulqa:mc,0-shot,mc2,0.4705381335286149,0.015491012979962984
rinna/bilingual-gpt-neox-4b,minerva_math_precalc,5-shot,accuracy,0.007326007326007326,0.0036529080893830334
rinna/bilingual-gpt-neox-4b,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458095,0.0034284443646836484
rinna/bilingual-gpt-neox-4b,minerva_math_num_theory,5-shot,accuracy,0.018518518518518517,0.005806972807912266
rinna/bilingual-gpt-neox-4b,minerva_math_intermediate_algebra,5-shot,accuracy,0.01107419712070875,0.0034844537978317453
rinna/bilingual-gpt-neox-4b,minerva_math_geometry,5-shot,accuracy,0.018789144050104383,0.006210416427997402
rinna/bilingual-gpt-neox-4b,minerva_math_counting_and_prob,5-shot,accuracy,0.006329113924050633,0.003646382041065029
rinna/bilingual-gpt-neox-4b,minerva_math_algebra,5-shot,accuracy,0.009267059814658803,0.002782319118488801
rinna/bilingual-gpt-neox-4b,fld_default,0-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,fld_star,0-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521547
rinna/bilingual-gpt-neox-4b,arithmetic_3ds,5-shot,accuracy,0.0075,0.0019296986470519833
rinna/bilingual-gpt-neox-4b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
rinna/bilingual-gpt-neox-4b,arithmetic_2ds,5-shot,accuracy,0.0415,0.0044608098381578925
rinna/bilingual-gpt-neox-4b,arithmetic_5ds,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_5da,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_1dc,5-shot,accuracy,0.066,0.005553144938623081
rinna/bilingual-gpt-neox-4b,arithmetic_4ds,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,arithmetic_2dm,5-shot,accuracy,0.033,0.003995432609977368
rinna/bilingual-gpt-neox-4b,arithmetic_2da,5-shot,accuracy,0.035,0.004110468096699808
rinna/bilingual-gpt-neox-4b,gsm8k_cot,5-shot,accuracy,0.021986353297952996,0.0040391627581100676
rinna/bilingual-gpt-neox-4b,gsm8k,5-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,anli_r2,0-shot,brier_score,0.7465044173321834,
rinna/bilingual-gpt-neox-4b,anli_r3,0-shot,brier_score,0.7382616042295201,
rinna/bilingual-gpt-neox-4b,anli_r1,0-shot,brier_score,0.7422539652276323,
rinna/bilingual-gpt-neox-4b,xnli_eu,0-shot,brier_score,0.9935567744254022,
rinna/bilingual-gpt-neox-4b,xnli_vi,0-shot,brier_score,0.8210592681442176,
rinna/bilingual-gpt-neox-4b,xnli_ru,0-shot,brier_score,0.8190550378326039,
rinna/bilingual-gpt-neox-4b,xnli_zh,0-shot,brier_score,0.9435664261389121,
rinna/bilingual-gpt-neox-4b,xnli_tr,0-shot,brier_score,0.8247652276169989,
rinna/bilingual-gpt-neox-4b,xnli_fr,0-shot,brier_score,0.7935452141545868,
rinna/bilingual-gpt-neox-4b,xnli_en,0-shot,brier_score,0.673510778129045,
rinna/bilingual-gpt-neox-4b,xnli_ur,0-shot,brier_score,0.9384835031798199,
rinna/bilingual-gpt-neox-4b,xnli_ar,0-shot,brier_score,1.080089095089373,
rinna/bilingual-gpt-neox-4b,xnli_de,0-shot,brier_score,0.8497396490738153,
rinna/bilingual-gpt-neox-4b,xnli_hi,0-shot,brier_score,0.7784793501566398,
rinna/bilingual-gpt-neox-4b,xnli_es,0-shot,brier_score,0.8344890347288778,
rinna/bilingual-gpt-neox-4b,xnli_bg,0-shot,brier_score,0.9131127340061094,
rinna/bilingual-gpt-neox-4b,xnli_sw,0-shot,brier_score,0.8375909307024677,
rinna/bilingual-gpt-neox-4b,xnli_el,0-shot,brier_score,0.920078994998459,
rinna/bilingual-gpt-neox-4b,xnli_th,0-shot,brier_score,0.7626190761994815,
rinna/bilingual-gpt-neox-4b,logiqa2,0-shot,brier_score,1.0595557248900274,
rinna/bilingual-gpt-neox-4b,mathqa,5-shot,brier_score,0.9543318277043356,
rinna/bilingual-gpt-neox-4b,lambada_standard,0-shot,perplexity,9.40445963354471,0.25202597356824585
rinna/bilingual-gpt-neox-4b,lambada_standard,0-shot,accuracy,0.518338831748496,0.006961290586136407
rinna/bilingual-gpt-neox-4b,lambada_openai,0-shot,perplexity,5.365362213035259,0.12842366560448085
rinna/bilingual-gpt-neox-4b,lambada_openai,0-shot,accuracy,0.6394333398020571,0.006689636112543833
rinna/bilingual-gpt-neox-4b,mmlu_world_religions,0-shot,accuracy,0.25146198830409355,0.033275044238468436
rinna/bilingual-gpt-neox-4b,mmlu_formal_logic,0-shot,accuracy,0.24603174603174602,0.038522733649243156
rinna/bilingual-gpt-neox-4b,mmlu_prehistory,0-shot,accuracy,0.2777777777777778,0.024922001168886338
rinna/bilingual-gpt-neox-4b,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.01442229220480886
rinna/bilingual-gpt-neox-4b,mmlu_high_school_world_history,0-shot,accuracy,0.23628691983122363,0.027652153144159274
rinna/bilingual-gpt-neox-4b,mmlu_moral_disputes,0-shot,accuracy,0.1994219653179191,0.021511900654252562
rinna/bilingual-gpt-neox-4b,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.01106415102716544
rinna/bilingual-gpt-neox-4b,mmlu_logical_fallacies,0-shot,accuracy,0.294478527607362,0.03581165790474082
rinna/bilingual-gpt-neox-4b,mmlu_high_school_us_history,0-shot,accuracy,0.23039215686274508,0.029554292605695066
rinna/bilingual-gpt-neox-4b,mmlu_philosophy,0-shot,accuracy,0.3054662379421222,0.026160584450140485
rinna/bilingual-gpt-neox-4b,mmlu_jurisprudence,0-shot,accuracy,0.2777777777777778,0.04330043749650741
rinna/bilingual-gpt-neox-4b,mmlu_international_law,0-shot,accuracy,0.3140495867768595,0.04236964753041018
rinna/bilingual-gpt-neox-4b,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
rinna/bilingual-gpt-neox-4b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.22797927461139897,0.03027690994517825
rinna/bilingual-gpt-neox-4b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2689075630252101,0.028801392193631276
rinna/bilingual-gpt-neox-4b,mmlu_high_school_geography,0-shot,accuracy,0.23232323232323232,0.030088629490217483
rinna/bilingual-gpt-neox-4b,mmlu_high_school_psychology,0-shot,accuracy,0.22018348623853212,0.01776597865232757
rinna/bilingual-gpt-neox-4b,mmlu_public_relations,0-shot,accuracy,0.19090909090909092,0.03764425585984926
rinna/bilingual-gpt-neox-4b,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.04408440022768077
rinna/bilingual-gpt-neox-4b,mmlu_sociology,0-shot,accuracy,0.25870646766169153,0.030965903123573026
rinna/bilingual-gpt-neox-4b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24615384615384617,0.021840866990423077
rinna/bilingual-gpt-neox-4b,mmlu_security_studies,0-shot,accuracy,0.17959183673469387,0.024573293589585637
rinna/bilingual-gpt-neox-4b,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
rinna/bilingual-gpt-neox-4b,mmlu_human_sexuality,0-shot,accuracy,0.21374045801526717,0.0359546161177469
rinna/bilingual-gpt-neox-4b,mmlu_econometrics,0-shot,accuracy,0.22807017543859648,0.03947152782669415
rinna/bilingual-gpt-neox-4b,mmlu_miscellaneous,0-shot,accuracy,0.27330779054916987,0.015936681062628556
rinna/bilingual-gpt-neox-4b,mmlu_marketing,0-shot,accuracy,0.23504273504273504,0.027778835904935444
rinna/bilingual-gpt-neox-4b,mmlu_management,0-shot,accuracy,0.20388349514563106,0.03989139859531769
rinna/bilingual-gpt-neox-4b,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.025261691219729487
rinna/bilingual-gpt-neox-4b,mmlu_medical_genetics,0-shot,accuracy,0.24,0.04292346959909282
rinna/bilingual-gpt-neox-4b,mmlu_human_aging,0-shot,accuracy,0.30493273542600896,0.030898610882477515
rinna/bilingual-gpt-neox-4b,mmlu_professional_medicine,0-shot,accuracy,0.1801470588235294,0.02334516361654486
rinna/bilingual-gpt-neox-4b,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
rinna/bilingual-gpt-neox-4b,mmlu_business_ethics,0-shot,accuracy,0.2,0.04020151261036843
rinna/bilingual-gpt-neox-4b,mmlu_clinical_knowledge,0-shot,accuracy,0.2490566037735849,0.02661648298050171
rinna/bilingual-gpt-neox-4b,mmlu_global_facts,0-shot,accuracy,0.3,0.04605661864718381
rinna/bilingual-gpt-neox-4b,mmlu_virology,0-shot,accuracy,0.30120481927710846,0.0357160923005348
rinna/bilingual-gpt-neox-4b,mmlu_professional_accounting,0-shot,accuracy,0.2730496453900709,0.02657786094330785
rinna/bilingual-gpt-neox-4b,mmlu_college_physics,0-shot,accuracy,0.2647058823529412,0.043898699568087785
rinna/bilingual-gpt-neox-4b,mmlu_high_school_physics,0-shot,accuracy,0.25165562913907286,0.035433042343899844
rinna/bilingual-gpt-neox-4b,mmlu_high_school_biology,0-shot,accuracy,0.2870967741935484,0.025736542745594525
rinna/bilingual-gpt-neox-4b,mmlu_college_biology,0-shot,accuracy,0.22916666666666666,0.03514697467862388
rinna/bilingual-gpt-neox-4b,mmlu_anatomy,0-shot,accuracy,0.28888888888888886,0.0391545063041425
rinna/bilingual-gpt-neox-4b,mmlu_college_chemistry,0-shot,accuracy,0.18,0.03861229196653694
rinna/bilingual-gpt-neox-4b,mmlu_computer_security,0-shot,accuracy,0.19,0.03942772444036622
rinna/bilingual-gpt-neox-4b,mmlu_college_computer_science,0-shot,accuracy,0.33,0.04725815626252604
rinna/bilingual-gpt-neox-4b,mmlu_astronomy,0-shot,accuracy,0.26973684210526316,0.03611780560284898
rinna/bilingual-gpt-neox-4b,mmlu_college_mathematics,0-shot,accuracy,0.28,0.04512608598542127
rinna/bilingual-gpt-neox-4b,mmlu_conceptual_physics,0-shot,accuracy,0.2425531914893617,0.028020226271200217
rinna/bilingual-gpt-neox-4b,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.04512608598542128
rinna/bilingual-gpt-neox-4b,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.046056618647183814
rinna/bilingual-gpt-neox-4b,mmlu_machine_learning,0-shot,accuracy,0.24107142857142858,0.04059867246952687
rinna/bilingual-gpt-neox-4b,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.031089826002937523
rinna/bilingual-gpt-neox-4b,mmlu_high_school_statistics,0-shot,accuracy,0.30092592592592593,0.03128039084329882
rinna/bilingual-gpt-neox-4b,mmlu_elementary_mathematics,0-shot,accuracy,0.24603174603174602,0.022182037202948365
rinna/bilingual-gpt-neox-4b,mmlu_electrical_engineering,0-shot,accuracy,0.2689655172413793,0.036951833116502325
rinna/bilingual-gpt-neox-4b,mmlu_high_school_mathematics,0-shot,accuracy,0.25925925925925924,0.02671924078371216
rinna/bilingual-gpt-neox-4b,arc_challenge,25-shot,accuracy,0.2901023890784983,0.013261573677520776
rinna/bilingual-gpt-neox-4b,arc_challenge,25-shot,acc_norm,0.3216723549488055,0.013650488084494162
rinna/bilingual-gpt-neox-4b,hellaswag,10-shot,accuracy,0.3646683927504481,0.004803533333364231
rinna/bilingual-gpt-neox-4b,hellaswag,10-shot,acc_norm,0.4372634933280223,0.0049503473337018334
rinna/bilingual-gpt-neox-4b,truthfulqa_mc2,0-shot,accuracy,0.3587180858240576,0.013702244823217082
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,bleu_max,20.298502784294392,0.6658217344882357
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,bleu_acc,0.29498164014687883,0.01596440096558967
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,bleu_diff,-6.505009210061705,0.6268960954843193
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge1_max,43.773621090740846,0.8400411954477274
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge1_acc,0.24112607099143207,0.014974827279752329
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge1_diff,-10.723279586053893,0.7370334795276966
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge2_max,26.5835203998597,0.8994004691527028
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge2_acc,0.1799265605875153,0.013447109235537583
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rouge2_diff,-11.606543714790986,0.8349217988871113
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rougeL_max,40.61834366861577,0.833488462597628
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rougeL_acc,0.23623011015911874,0.014869755015871124
rinna/bilingual-gpt-neox-4b,truthfulqa_gen,0-shot,rougeL_diff,-10.553870158245887,0.7246002434925973
rinna/bilingual-gpt-neox-4b,truthfulqa_mc1,0-shot,accuracy,0.2141982864137087,0.014362148155690478
rinna/bilingual-gpt-neox-4b,winogrande,5-shot,accuracy,0.5185477505919495,0.014042813708888378
kevin009/babyllama-v0.6,arc:challenge,25-shot,accuracy,0.35238907849829354,0.013960142600598677
kevin009/babyllama-v0.6,arc:challenge,25-shot,acc_norm,0.3609215017064846,0.014034761386175458
kevin009/babyllama-v0.6,hellaswag,10-shot,accuracy,0.46335391356303524,0.004976361454341339
kevin009/babyllama-v0.6,hellaswag,10-shot,acc_norm,0.6159131647082254,0.0048538457503921415
kevin009/babyllama-v0.6,hendrycksTest-abstract_algebra,5-shot,accuracy,0.24,0.04292346959909284
kevin009/babyllama-v0.6,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.24,0.04292346959909284
kevin009/babyllama-v0.6,hendrycksTest-anatomy,5-shot,accuracy,0.17777777777777778,0.03302789859901717
kevin009/babyllama-v0.6,hendrycksTest-anatomy,5-shot,acc_norm,0.17777777777777778,0.03302789859901717
kevin009/babyllama-v0.6,hendrycksTest-astronomy,5-shot,accuracy,0.17105263157894737,0.030643607071677077
kevin009/babyllama-v0.6,hendrycksTest-astronomy,5-shot,acc_norm,0.17105263157894737,0.030643607071677077
kevin009/babyllama-v0.6,hendrycksTest-business_ethics,5-shot,accuracy,0.24,0.042923469599092816
kevin009/babyllama-v0.6,hendrycksTest-business_ethics,5-shot,acc_norm,0.24,0.042923469599092816
kevin009/babyllama-v0.6,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2830188679245283,0.027724236492700904
kevin009/babyllama-v0.6,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2830188679245283,0.027724236492700904
kevin009/babyllama-v0.6,hendrycksTest-college_biology,5-shot,accuracy,0.24305555555555555,0.035868792800803406
kevin009/babyllama-v0.6,hendrycksTest-college_biology,5-shot,acc_norm,0.24305555555555555,0.035868792800803406
kevin009/babyllama-v0.6,hendrycksTest-college_chemistry,5-shot,accuracy,0.25,0.04351941398892446
kevin009/babyllama-v0.6,hendrycksTest-college_chemistry,5-shot,acc_norm,0.25,0.04351941398892446
kevin009/babyllama-v0.6,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.0446196043338474
kevin009/babyllama-v0.6,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.0446196043338474
kevin009/babyllama-v0.6,hendrycksTest-college_mathematics,5-shot,accuracy,0.35,0.0479372485441102
kevin009/babyllama-v0.6,hendrycksTest-college_mathematics,5-shot,acc_norm,0.35,0.0479372485441102
kevin009/babyllama-v0.6,hendrycksTest-college_medicine,5-shot,accuracy,0.19653179190751446,0.030299574664788147
kevin009/babyllama-v0.6,hendrycksTest-college_medicine,5-shot,acc_norm,0.19653179190751446,0.030299574664788147
kevin009/babyllama-v0.6,hendrycksTest-college_physics,5-shot,accuracy,0.19607843137254902,0.03950581861179961
kevin009/babyllama-v0.6,hendrycksTest-college_physics,5-shot,acc_norm,0.19607843137254902,0.03950581861179961
kevin009/babyllama-v0.6,hendrycksTest-computer_security,5-shot,accuracy,0.31,0.04648231987117316
kevin009/babyllama-v0.6,hendrycksTest-computer_security,5-shot,acc_norm,0.31,0.04648231987117316
kevin009/babyllama-v0.6,hendrycksTest-conceptual_physics,5-shot,accuracy,0.28085106382978725,0.02937917046412482
kevin009/babyllama-v0.6,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.28085106382978725,0.02937917046412482
kevin009/babyllama-v0.6,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.041424397194893624
kevin009/babyllama-v0.6,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.041424397194893624
kevin009/babyllama-v0.6,hendrycksTest-electrical_engineering,5-shot,accuracy,0.23448275862068965,0.035306258743465914
kevin009/babyllama-v0.6,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.23448275862068965,0.035306258743465914
kevin009/babyllama-v0.6,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918417
kevin009/babyllama-v0.6,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.022569897074918417
kevin009/babyllama-v0.6,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.03567016675276864
kevin009/babyllama-v0.6,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.03567016675276864
kevin009/babyllama-v0.6,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
kevin009/babyllama-v0.6,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
kevin009/babyllama-v0.6,hendrycksTest-high_school_biology,5-shot,accuracy,0.2645161290322581,0.025091892378859275
kevin009/babyllama-v0.6,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2645161290322581,0.025091892378859275
kevin009/babyllama-v0.6,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2413793103448276,0.03010833071801162
kevin009/babyllama-v0.6,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2413793103448276,0.03010833071801162
kevin009/babyllama-v0.6,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.22,0.041633319989322695
kevin009/babyllama-v0.6,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.22,0.041633319989322695
kevin009/babyllama-v0.6,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2909090909090909,0.03546563019624336
kevin009/babyllama-v0.6,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2909090909090909,0.03546563019624336
kevin009/babyllama-v0.6,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.029620227874790486
kevin009/babyllama-v0.6,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.029620227874790486
kevin009/babyllama-v0.6,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.22279792746113988,0.03003114797764154
kevin009/babyllama-v0.6,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.22279792746113988,0.03003114797764154
kevin009/babyllama-v0.6,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.24615384615384617,0.021840866990423088
kevin009/babyllama-v0.6,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.24615384615384617,0.021840866990423088
kevin009/babyllama-v0.6,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26666666666666666,0.02696242432507384
kevin009/babyllama-v0.6,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26666666666666666,0.02696242432507384
kevin009/babyllama-v0.6,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23529411764705882,0.027553614467863818
kevin009/babyllama-v0.6,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23529411764705882,0.027553614467863818
kevin009/babyllama-v0.6,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.03297986648473836
kevin009/babyllama-v0.6,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.03297986648473836
kevin009/babyllama-v0.6,hendrycksTest-high_school_psychology,5-shot,accuracy,0.24036697247706423,0.01832060732096407
kevin009/babyllama-v0.6,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.24036697247706423,0.01832060732096407
kevin009/babyllama-v0.6,hendrycksTest-high_school_statistics,5-shot,accuracy,0.38425925925925924,0.03317354514310742
kevin009/babyllama-v0.6,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.38425925925925924,0.03317354514310742
kevin009/babyllama-v0.6,hendrycksTest-high_school_us_history,5-shot,accuracy,0.24019607843137256,0.02998373305591362
kevin009/babyllama-v0.6,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.24019607843137256,0.02998373305591362
kevin009/babyllama-v0.6,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25316455696202533,0.0283046579430353
kevin009/babyllama-v0.6,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25316455696202533,0.0283046579430353
kevin009/babyllama-v0.6,hendrycksTest-human_aging,5-shot,accuracy,0.3632286995515695,0.03227790442850499
kevin009/babyllama-v0.6,hendrycksTest-human_aging,5-shot,acc_norm,0.3632286995515695,0.03227790442850499
kevin009/babyllama-v0.6,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.037276735755969195
kevin009/babyllama-v0.6,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.037276735755969195
kevin009/babyllama-v0.6,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.03984979653302871
kevin009/babyllama-v0.6,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.03984979653302871
kevin009/babyllama-v0.6,hendrycksTest-jurisprudence,5-shot,accuracy,0.25925925925925924,0.04236511258094633
kevin009/babyllama-v0.6,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25925925925925924,0.04236511258094633
kevin009/babyllama-v0.6,hendrycksTest-logical_fallacies,5-shot,accuracy,0.22085889570552147,0.032591773927421776
kevin009/babyllama-v0.6,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.22085889570552147,0.032591773927421776
kevin009/babyllama-v0.6,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.04327040932578728
kevin009/babyllama-v0.6,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.04327040932578728
kevin009/babyllama-v0.6,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.04354631077260597
kevin009/babyllama-v0.6,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.04354631077260597
kevin009/babyllama-v0.6,hendrycksTest-marketing,5-shot,accuracy,0.28205128205128205,0.029480360549541194
kevin009/babyllama-v0.6,hendrycksTest-marketing,5-shot,acc_norm,0.28205128205128205,0.029480360549541194
kevin009/babyllama-v0.6,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.04461960433384741
kevin009/babyllama-v0.6,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.04461960433384741
kevin009/babyllama-v0.6,hendrycksTest-miscellaneous,5-shot,accuracy,0.280970625798212,0.01607312785122125
kevin009/babyllama-v0.6,hendrycksTest-miscellaneous,5-shot,acc_norm,0.280970625798212,0.01607312785122125
kevin009/babyllama-v0.6,hendrycksTest-moral_disputes,5-shot,accuracy,0.2254335260115607,0.022497230190967547
kevin009/babyllama-v0.6,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2254335260115607,0.022497230190967547
kevin009/babyllama-v0.6,hendrycksTest-moral_scenarios,5-shot,accuracy,0.22681564245810057,0.014005843570897897
kevin009/babyllama-v0.6,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.22681564245810057,0.014005843570897897
kevin009/babyllama-v0.6,hendrycksTest-nutrition,5-shot,accuracy,0.24183006535947713,0.024518195641879334
kevin009/babyllama-v0.6,hendrycksTest-nutrition,5-shot,acc_norm,0.24183006535947713,0.024518195641879334
kevin009/babyllama-v0.6,hendrycksTest-philosophy,5-shot,accuracy,0.26688102893890675,0.02512263760881665
kevin009/babyllama-v0.6,hendrycksTest-philosophy,5-shot,acc_norm,0.26688102893890675,0.02512263760881665
kevin009/babyllama-v0.6,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.024569223600460845
kevin009/babyllama-v0.6,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.024569223600460845
kevin009/babyllama-v0.6,hendrycksTest-professional_accounting,5-shot,accuracy,0.25886524822695034,0.026129572527180848
kevin009/babyllama-v0.6,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25886524822695034,0.026129572527180848
kevin009/babyllama-v0.6,hendrycksTest-professional_law,5-shot,accuracy,0.23859191655801826,0.0108859297420022
kevin009/babyllama-v0.6,hendrycksTest-professional_law,5-shot,acc_norm,0.23859191655801826,0.0108859297420022
kevin009/babyllama-v0.6,hendrycksTest-professional_medicine,5-shot,accuracy,0.20220588235294118,0.02439819298665492
kevin009/babyllama-v0.6,hendrycksTest-professional_medicine,5-shot,acc_norm,0.20220588235294118,0.02439819298665492
kevin009/babyllama-v0.6,hendrycksTest-professional_psychology,5-shot,accuracy,0.26633986928104575,0.017883188134667192
kevin009/babyllama-v0.6,hendrycksTest-professional_psychology,5-shot,acc_norm,0.26633986928104575,0.017883188134667192
kevin009/babyllama-v0.6,hendrycksTest-public_relations,5-shot,accuracy,0.3181818181818182,0.04461272175910507
kevin009/babyllama-v0.6,hendrycksTest-public_relations,5-shot,acc_norm,0.3181818181818182,0.04461272175910507
kevin009/babyllama-v0.6,hendrycksTest-security_studies,5-shot,accuracy,0.17142857142857143,0.02412746346265015
kevin009/babyllama-v0.6,hendrycksTest-security_studies,5-shot,acc_norm,0.17142857142857143,0.02412746346265015
kevin009/babyllama-v0.6,hendrycksTest-sociology,5-shot,accuracy,0.23383084577114427,0.029929415408348384
kevin009/babyllama-v0.6,hendrycksTest-sociology,5-shot,acc_norm,0.23383084577114427,0.029929415408348384
kevin009/babyllama-v0.6,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.040936018074033256
kevin009/babyllama-v0.6,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.040936018074033256
kevin009/babyllama-v0.6,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.03629335329947861
kevin009/babyllama-v0.6,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.03629335329947861
kevin009/babyllama-v0.6,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.034462962170884265
kevin009/babyllama-v0.6,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.034462962170884265
kevin009/babyllama-v0.6,truthfulqa:mc,0-shot,mc1,0.22031823745410037,0.014509045171487295
kevin009/babyllama-v0.6,truthfulqa:mc,0-shot,mc2,0.3584100057903431,0.013776314892170112
kevin009/babyllama-v0.6,winogrande,5-shot,accuracy,0.6101026045777427,0.013707547317008463
kevin009/babyllama-v0.6,gsm8k,5-shot,accuracy,0.022744503411675512,0.004106620637749683
kevin009/babyllama-v0.6,minerva_math_precalc,5-shot,accuracy,0.009157509157509158,0.0040803060650489754
kevin009/babyllama-v0.6,minerva_math_prealgebra,5-shot,accuracy,0.022962112514351322,0.005078108741895631
kevin009/babyllama-v0.6,minerva_math_num_theory,5-shot,accuracy,0.024074074074074074,0.006602202509815374
kevin009/babyllama-v0.6,minerva_math_intermediate_algebra,5-shot,accuracy,0.017718715393133997,0.0043926922934929
kevin009/babyllama-v0.6,minerva_math_geometry,5-shot,accuracy,0.025052192066805846,0.007148247838013832
kevin009/babyllama-v0.6,minerva_math_counting_and_prob,5-shot,accuracy,0.02109704641350211,0.006607696365626559
kevin009/babyllama-v0.6,minerva_math_algebra,5-shot,accuracy,0.020219039595619208,0.0040869790805184445
kevin009/babyllama-v0.6,fld_default,0-shot,accuracy,0.0,
kevin009/babyllama-v0.6,fld_star,0-shot,accuracy,0.0,
kevin009/babyllama-v0.6,arithmetic_3da,5-shot,accuracy,0.3475,0.010650285878540916
kevin009/babyllama-v0.6,arithmetic_3ds,5-shot,accuracy,0.244,0.00960615110590775
kevin009/babyllama-v0.6,arithmetic_4da,5-shot,accuracy,0.096,0.006588907864997607
kevin009/babyllama-v0.6,arithmetic_2ds,5-shot,accuracy,0.4245,0.011054907529701136
kevin009/babyllama-v0.6,arithmetic_5ds,5-shot,accuracy,0.024,0.003423135832751179
kevin009/babyllama-v0.6,arithmetic_5da,5-shot,accuracy,0.0195,0.003092678018912421
kevin009/babyllama-v0.6,arithmetic_1dc,5-shot,accuracy,0.0375,0.004249223805764546
kevin009/babyllama-v0.6,arithmetic_4ds,5-shot,accuracy,0.097,0.006619471935460824
kevin009/babyllama-v0.6,arithmetic_2dm,5-shot,accuracy,0.0985,0.0066649145187895826
kevin009/babyllama-v0.6,arithmetic_2da,5-shot,accuracy,0.5085,0.011181519941139164
kevin009/babyllama-v0.6,gsm8k_cot,5-shot,accuracy,0.026535253980288095,0.004427045987265166
kevin009/babyllama-v0.6,anli_r2,0-shot,brier_score,0.7480318885162998,
kevin009/babyllama-v0.6,anli_r3,0-shot,brier_score,0.7193364105546545,
kevin009/babyllama-v0.6,anli_r1,0-shot,brier_score,0.767149399796389,
kevin009/babyllama-v0.6,xnli_eu,0-shot,brier_score,1.0432410172151099,
kevin009/babyllama-v0.6,xnli_vi,0-shot,brier_score,0.9579775898410937,
kevin009/babyllama-v0.6,xnli_ru,0-shot,brier_score,0.8504576037078885,
kevin009/babyllama-v0.6,xnli_zh,0-shot,brier_score,1.1220197141708859,
kevin009/babyllama-v0.6,xnli_tr,0-shot,brier_score,0.9338172675485504,
kevin009/babyllama-v0.6,xnli_fr,0-shot,brier_score,0.8695514801542201,
kevin009/babyllama-v0.6,xnli_en,0-shot,brier_score,0.6284457155142247,
kevin009/babyllama-v0.6,xnli_ur,0-shot,brier_score,1.3316570161601293,
kevin009/babyllama-v0.6,xnli_ar,0-shot,brier_score,1.2875160416993416,
kevin009/babyllama-v0.6,xnli_de,0-shot,brier_score,0.8691284199259482,
kevin009/babyllama-v0.6,xnli_hi,0-shot,brier_score,0.9802004371134695,
kevin009/babyllama-v0.6,xnli_es,0-shot,brier_score,0.9155068659792368,
kevin009/babyllama-v0.6,xnli_bg,0-shot,brier_score,0.8526807535621683,
kevin009/babyllama-v0.6,xnli_sw,0-shot,brier_score,0.9587101927586477,
kevin009/babyllama-v0.6,xnli_el,0-shot,brier_score,0.8194813150819024,
kevin009/babyllama-v0.6,xnli_th,0-shot,brier_score,1.0261811798875147,
kevin009/babyllama-v0.6,logiqa2,0-shot,brier_score,1.1193557858325283,
kevin009/babyllama-v0.6,mathqa,5-shot,brier_score,0.9959932566490026,
kevin009/babyllama-v0.6,lambada_standard,0-shot,perplexity,10.607375165530296,0.3083784957379466
kevin009/babyllama-v0.6,lambada_standard,0-shot,accuracy,0.49951484572093924,0.006965974377961561
kevin009/babyllama-v0.6,lambada_openai,0-shot,perplexity,6.514190794191786,0.17637310033361717
kevin009/babyllama-v0.6,lambada_openai,0-shot,accuracy,0.5899476033378614,0.0068523331681548845
EleutherAI/gpt-neo-1.3B,arc:challenge,25-shot,accuracy,0.2713310580204778,0.012993807727545797
EleutherAI/gpt-neo-1.3B,arc:challenge,25-shot,acc_norm,0.3122866894197952,0.013542598541688065
EleutherAI/gpt-neo-1.3B,hellaswag,10-shot,accuracy,0.38787094204341765,0.004862690594815702
EleutherAI/gpt-neo-1.3B,hellaswag,10-shot,acc_norm,0.4863572993427604,0.0049879236366285554
EleutherAI/gpt-neo-1.3B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/gpt-neo-1.3B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.04512608598542128
EleutherAI/gpt-neo-1.3B,hendrycksTest-anatomy,5-shot,accuracy,0.14814814814814814,0.030688647610352667
EleutherAI/gpt-neo-1.3B,hendrycksTest-anatomy,5-shot,acc_norm,0.14814814814814814,0.030688647610352667
EleutherAI/gpt-neo-1.3B,hendrycksTest-astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/gpt-neo-1.3B,hendrycksTest-astronomy,5-shot,acc_norm,0.17763157894736842,0.031103182383123398
EleutherAI/gpt-neo-1.3B,hendrycksTest-business_ethics,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neo-1.3B,hendrycksTest-business_ethics,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/gpt-neo-1.3B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.27169811320754716,0.02737770662467071
EleutherAI/gpt-neo-1.3B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.27169811320754716,0.02737770662467071
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.03653946969442099
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.03653946969442099
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_chemistry,5-shot,accuracy,0.21,0.04093601807403326
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.21,0.04093601807403326
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.04512608598542126
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.04512608598542126
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_medicine,5-shot,accuracy,0.20809248554913296,0.030952890217749874
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_medicine,5-shot,acc_norm,0.20809248554913296,0.030952890217749874
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237654
EleutherAI/gpt-neo-1.3B,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237654
EleutherAI/gpt-neo-1.3B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/gpt-neo-1.3B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.04512608598542128
EleutherAI/gpt-neo-1.3B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2851063829787234,0.029513196625539355
EleutherAI/gpt-neo-1.3B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2851063829787234,0.029513196625539355
EleutherAI/gpt-neo-1.3B,hendrycksTest-econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
EleutherAI/gpt-neo-1.3B,hendrycksTest-econometrics,5-shot,acc_norm,0.22807017543859648,0.03947152782669415
EleutherAI/gpt-neo-1.3B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2827586206896552,0.03752833958003337
EleutherAI/gpt-neo-1.3B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2827586206896552,0.03752833958003337
EleutherAI/gpt-neo-1.3B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25396825396825395,0.02241804289111394
EleutherAI/gpt-neo-1.3B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25396825396825395,0.02241804289111394
EleutherAI/gpt-neo-1.3B,hendrycksTest-formal_logic,5-shot,accuracy,0.25396825396825395,0.03893259610604674
EleutherAI/gpt-neo-1.3B,hendrycksTest-formal_logic,5-shot,acc_norm,0.25396825396825395,0.03893259610604674
EleutherAI/gpt-neo-1.3B,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.03942772444036624
EleutherAI/gpt-neo-1.3B,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.03942772444036624
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2,0.02275520495954294
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2,0.02275520495954294
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.19704433497536947,0.02798672466673622
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.19704433497536947,0.02798672466673622
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.27,0.04461960433384739
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.27,0.04461960433384739
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.21818181818181817,0.03225078108306289
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_geography,5-shot,accuracy,0.18686868686868688,0.027772533334218974
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.18686868686868688,0.027772533334218974
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.20725388601036268,0.029252823291803613
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.20725388601036268,0.029252823291803613
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3128205128205128,0.023507579020645365
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3128205128205128,0.023507579020645365
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.02646611753895991
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.02646611753895991
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23109243697478993,0.027381406927868963
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23109243697478993,0.027381406927868963
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_physics,5-shot,accuracy,0.26490066225165565,0.036030385453603826
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.26490066225165565,0.036030385453603826
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1981651376146789,0.017090573804217885
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1981651376146789,0.017090573804217885
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4074074074074074,0.03350991604696043
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4074074074074074,0.03350991604696043
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.23039215686274508,0.029554292605695066
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.23039215686274508,0.029554292605695066
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.28270042194092826,0.029312814153955924
EleutherAI/gpt-neo-1.3B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.28270042194092826,0.029312814153955924
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.031024411740572203
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.031024411740572203
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/gpt-neo-1.3B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
EleutherAI/gpt-neo-1.3B,hendrycksTest-international_law,5-shot,accuracy,0.2892561983471074,0.04139112727635464
EleutherAI/gpt-neo-1.3B,hendrycksTest-international_law,5-shot,acc_norm,0.2892561983471074,0.04139112727635464
EleutherAI/gpt-neo-1.3B,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.04186091791394607
EleutherAI/gpt-neo-1.3B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.04186091791394607
EleutherAI/gpt-neo-1.3B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2392638036809816,0.033519538795212696
EleutherAI/gpt-neo-1.3B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2392638036809816,0.033519538795212696
EleutherAI/gpt-neo-1.3B,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.04327040932578729
EleutherAI/gpt-neo-1.3B,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.04327040932578729
EleutherAI/gpt-neo-1.3B,hendrycksTest-management,5-shot,accuracy,0.17475728155339806,0.037601780060266224
EleutherAI/gpt-neo-1.3B,hendrycksTest-management,5-shot,acc_norm,0.17475728155339806,0.037601780060266224
EleutherAI/gpt-neo-1.3B,hendrycksTest-marketing,5-shot,accuracy,0.2948717948717949,0.029872577708891145
EleutherAI/gpt-neo-1.3B,hendrycksTest-marketing,5-shot,acc_norm,0.2948717948717949,0.029872577708891145
EleutherAI/gpt-neo-1.3B,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/gpt-neo-1.3B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/gpt-neo-1.3B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2222222222222222,0.014866821664709597
EleutherAI/gpt-neo-1.3B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2222222222222222,0.014866821664709597
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24855491329479767,0.023267528432100174
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24022346368715083,0.014288343803925293
EleutherAI/gpt-neo-1.3B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24022346368715083,0.014288343803925293
EleutherAI/gpt-neo-1.3B,hendrycksTest-nutrition,5-shot,accuracy,0.22875816993464052,0.024051029739912255
EleutherAI/gpt-neo-1.3B,hendrycksTest-nutrition,5-shot,acc_norm,0.22875816993464052,0.024051029739912255
EleutherAI/gpt-neo-1.3B,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/gpt-neo-1.3B,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.02212243977248077
EleutherAI/gpt-neo-1.3B,hendrycksTest-prehistory,5-shot,accuracy,0.23148148148148148,0.023468429832451163
EleutherAI/gpt-neo-1.3B,hendrycksTest-prehistory,5-shot,acc_norm,0.23148148148148148,0.023468429832451163
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2375886524822695,0.025389512552729906
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2375886524822695,0.025389512552729906
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_law,5-shot,accuracy,0.24967405475880053,0.011054538377832313
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_law,5-shot,acc_norm,0.24967405475880053,0.011054538377832313
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_medicine,5-shot,accuracy,0.16544117647058823,0.022571771025494767
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.16544117647058823,0.022571771025494767
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_psychology,5-shot,accuracy,0.24836601307189543,0.017479487001364764
EleutherAI/gpt-neo-1.3B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.24836601307189543,0.017479487001364764
EleutherAI/gpt-neo-1.3B,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/gpt-neo-1.3B,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
EleutherAI/gpt-neo-1.3B,hendrycksTest-security_studies,5-shot,accuracy,0.2163265306122449,0.026358916334904038
EleutherAI/gpt-neo-1.3B,hendrycksTest-security_studies,5-shot,acc_norm,0.2163265306122449,0.026358916334904038
EleutherAI/gpt-neo-1.3B,hendrycksTest-sociology,5-shot,accuracy,0.24378109452736318,0.03036049015401465
EleutherAI/gpt-neo-1.3B,hendrycksTest-sociology,5-shot,acc_norm,0.24378109452736318,0.03036049015401465
EleutherAI/gpt-neo-1.3B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-neo-1.3B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/gpt-neo-1.3B,hendrycksTest-virology,5-shot,accuracy,0.27710843373493976,0.034843315926805875
EleutherAI/gpt-neo-1.3B,hendrycksTest-virology,5-shot,acc_norm,0.27710843373493976,0.034843315926805875
EleutherAI/gpt-neo-1.3B,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.03582529442573122
EleutherAI/gpt-neo-1.3B,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.03582529442573122
EleutherAI/gpt-neo-1.3B,truthfulqa:mc,0-shot,mc1,0.23133414932680538,0.01476194517486268
EleutherAI/gpt-neo-1.3B,truthfulqa:mc,0-shot,mc2,0.3962930604436832,0.014269584253275801
EleutherAI/gpt-neo-1.3B,drop,3-shot,accuracy,0.0018875838926174498,0.00044451099905591613
EleutherAI/gpt-neo-1.3B,drop,3-shot,f1,0.04604656040268471,0.0011927407325477777
EleutherAI/gpt-neo-1.3B,gsm8k,5-shot,accuracy,0.017437452615617893,0.003605486867998249
EleutherAI/gpt-neo-1.3B,winogrande,5-shot,accuracy,0.5564325177584846,0.0139626949076204
EleutherAI/pythia-6.9b,minerva_math_precalc,5-shot,accuracy,0.003663003663003663,0.0025877573681934475
EleutherAI/pythia-6.9b,minerva_math_prealgebra,5-shot,accuracy,0.029850746268656716,0.0057694876379482535
EleutherAI/pythia-6.9b,minerva_math_num_theory,5-shot,accuracy,0.005555555555555556,0.0032015451273209095
EleutherAI/pythia-6.9b,minerva_math_intermediate_algebra,5-shot,accuracy,0.018826135105204873,0.004525330498668475
EleutherAI/pythia-6.9b,minerva_math_geometry,5-shot,accuracy,0.018789144050104383,0.006210416427997405
EleutherAI/pythia-6.9b,minerva_math_counting_and_prob,5-shot,accuracy,0.014767932489451477,0.005546238589668472
EleutherAI/pythia-6.9b,minerva_math_algebra,5-shot,accuracy,0.016006739679865205,0.0036442247924417305
EleutherAI/pythia-6.9b,fld_default,0-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,fld_star,0-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,arithmetic_3da,5-shot,accuracy,0.007,0.0018647355360237436
EleutherAI/pythia-6.9b,arithmetic_3ds,5-shot,accuracy,0.0085,0.002053285901060985
EleutherAI/pythia-6.9b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
EleutherAI/pythia-6.9b,arithmetic_2ds,5-shot,accuracy,0.093,0.006495890878020431
EleutherAI/pythia-6.9b,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/pythia-6.9b,arithmetic_1dc,5-shot,accuracy,0.0565,0.0051640302675624965
EleutherAI/pythia-6.9b,arithmetic_4ds,5-shot,accuracy,0.0005,0.000500000000000002
EleutherAI/pythia-6.9b,arithmetic_2dm,5-shot,accuracy,0.054,0.005055173329243414
EleutherAI/pythia-6.9b,arithmetic_2da,5-shot,accuracy,0.053,0.005010793752192678
EleutherAI/pythia-6.9b,gsm8k_cot,5-shot,accuracy,0.029567854435178165,0.004665893134220779
EleutherAI/pythia-6.9b,gsm8k,5-shot,accuracy,0.02880970432145565,0.004607484283767438
EleutherAI/pythia-6.9b,anli_r2,0-shot,brier_score,0.8480871011410138,
EleutherAI/pythia-6.9b,anli_r3,0-shot,brier_score,0.8179776021045503,
EleutherAI/pythia-6.9b,anli_r1,0-shot,brier_score,0.8722694615179778,
EleutherAI/pythia-6.9b,xnli_eu,0-shot,brier_score,0.8999839016489077,
EleutherAI/pythia-6.9b,xnli_vi,0-shot,brier_score,0.8204605652395003,
EleutherAI/pythia-6.9b,xnli_ru,0-shot,brier_score,0.8106878224944725,
EleutherAI/pythia-6.9b,xnli_zh,0-shot,brier_score,1.09036580135667,
EleutherAI/pythia-6.9b,xnli_tr,0-shot,brier_score,0.8878553632357027,
EleutherAI/pythia-6.9b,xnli_fr,0-shot,brier_score,0.7961302677354256,
EleutherAI/pythia-6.9b,xnli_en,0-shot,brier_score,0.653954455047553,
EleutherAI/pythia-6.9b,xnli_ur,0-shot,brier_score,1.191915875742246,
EleutherAI/pythia-6.9b,xnli_ar,0-shot,brier_score,1.2272435798439718,
EleutherAI/pythia-6.9b,xnli_de,0-shot,brier_score,0.828333282541149,
EleutherAI/pythia-6.9b,xnli_hi,0-shot,brier_score,0.801853810502381,
EleutherAI/pythia-6.9b,xnli_es,0-shot,brier_score,0.8599668349851329,
EleutherAI/pythia-6.9b,xnli_bg,0-shot,brier_score,0.7686484176938864,
EleutherAI/pythia-6.9b,xnli_sw,0-shot,brier_score,0.8502894613384985,
EleutherAI/pythia-6.9b,xnli_el,0-shot,brier_score,0.8562714843114025,
EleutherAI/pythia-6.9b,xnli_th,0-shot,brier_score,0.7751103774504331,
EleutherAI/pythia-6.9b,logiqa2,0-shot,brier_score,1.165272169859693,
EleutherAI/pythia-6.9b,mathqa,5-shot,brier_score,0.95247880187882,
EleutherAI/pythia-6.9b,lambada_standard,0-shot,perplexity,6.937810377810337,0.1757119002721059
EleutherAI/pythia-6.9b,lambada_standard,0-shot,accuracy,0.5587036677663497,0.006917799856218216
EleutherAI/pythia-6.9b,lambada_openai,0-shot,perplexity,4.459300840447834,0.10000526645129007
EleutherAI/pythia-6.9b,lambada_openai,0-shot,accuracy,0.6710653987968174,0.006545597195850585
EleutherAI/pythia-6.9b,mmlu_world_religions,0-shot,accuracy,0.29239766081871343,0.034886477134579215
EleutherAI/pythia-6.9b,mmlu_formal_logic,0-shot,accuracy,0.15873015873015872,0.03268454013011744
EleutherAI/pythia-6.9b,mmlu_prehistory,0-shot,accuracy,0.23765432098765432,0.02368359183700856
EleutherAI/pythia-6.9b,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.014422292204808838
EleutherAI/pythia-6.9b,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.02904133351059804
EleutherAI/pythia-6.9b,mmlu_moral_disputes,0-shot,accuracy,0.2658959537572254,0.023786203255508283
EleutherAI/pythia-6.9b,mmlu_professional_law,0-shot,accuracy,0.23859191655801826,0.010885929742002212
EleutherAI/pythia-6.9b,mmlu_logical_fallacies,0-shot,accuracy,0.294478527607362,0.03581165790474082
EleutherAI/pythia-6.9b,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.03096451792692339
EleutherAI/pythia-6.9b,mmlu_philosophy,0-shot,accuracy,0.2733118971061093,0.025311765975426115
EleutherAI/pythia-6.9b,mmlu_jurisprudence,0-shot,accuracy,0.3055555555555556,0.044531975073749834
EleutherAI/pythia-6.9b,mmlu_international_law,0-shot,accuracy,0.38016528925619836,0.04431324501968432
EleutherAI/pythia-6.9b,mmlu_high_school_european_history,0-shot,accuracy,0.24848484848484848,0.03374402644139405
EleutherAI/pythia-6.9b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.24352331606217617,0.03097543638684543
EleutherAI/pythia-6.9b,mmlu_high_school_microeconomics,0-shot,accuracy,0.22268907563025211,0.02702543349888236
EleutherAI/pythia-6.9b,mmlu_high_school_geography,0-shot,accuracy,0.2222222222222222,0.02962022787479048
EleutherAI/pythia-6.9b,mmlu_high_school_psychology,0-shot,accuracy,0.23119266055045873,0.018075750241633163
EleutherAI/pythia-6.9b,mmlu_public_relations,0-shot,accuracy,0.35454545454545455,0.045820048415054174
EleutherAI/pythia-6.9b,mmlu_us_foreign_policy,0-shot,accuracy,0.23,0.04229525846816508
EleutherAI/pythia-6.9b,mmlu_sociology,0-shot,accuracy,0.25870646766169153,0.030965903123573026
EleutherAI/pythia-6.9b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.30512820512820515,0.023346335293325887
EleutherAI/pythia-6.9b,mmlu_security_studies,0-shot,accuracy,0.17551020408163265,0.024352800722970015
EleutherAI/pythia-6.9b,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594722
EleutherAI/pythia-6.9b,mmlu_human_sexuality,0-shot,accuracy,0.24427480916030533,0.037683359597287434
EleutherAI/pythia-6.9b,mmlu_econometrics,0-shot,accuracy,0.21929824561403508,0.03892431106518753
EleutherAI/pythia-6.9b,mmlu_miscellaneous,0-shot,accuracy,0.29246487867177523,0.016267000684598645
EleutherAI/pythia-6.9b,mmlu_marketing,0-shot,accuracy,0.2222222222222222,0.0272360139461967
EleutherAI/pythia-6.9b,mmlu_management,0-shot,accuracy,0.27184466019417475,0.044052680241409216
EleutherAI/pythia-6.9b,mmlu_nutrition,0-shot,accuracy,0.2581699346405229,0.025058503316958157
EleutherAI/pythia-6.9b,mmlu_medical_genetics,0-shot,accuracy,0.24,0.04292346959909281
EleutherAI/pythia-6.9b,mmlu_human_aging,0-shot,accuracy,0.28699551569506726,0.030360379710291954
EleutherAI/pythia-6.9b,mmlu_professional_medicine,0-shot,accuracy,0.3125,0.02815637344037142
EleutherAI/pythia-6.9b,mmlu_college_medicine,0-shot,accuracy,0.19653179190751446,0.030299574664788147
EleutherAI/pythia-6.9b,mmlu_business_ethics,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-6.9b,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.02688064788905197
EleutherAI/pythia-6.9b,mmlu_global_facts,0-shot,accuracy,0.33,0.04725815626252605
EleutherAI/pythia-6.9b,mmlu_virology,0-shot,accuracy,0.3192771084337349,0.036293353299478595
EleutherAI/pythia-6.9b,mmlu_professional_accounting,0-shot,accuracy,0.29432624113475175,0.027187127011503796
EleutherAI/pythia-6.9b,mmlu_college_physics,0-shot,accuracy,0.24509803921568626,0.04280105837364396
EleutherAI/pythia-6.9b,mmlu_high_school_physics,0-shot,accuracy,0.25165562913907286,0.035433042343899844
EleutherAI/pythia-6.9b,mmlu_high_school_biology,0-shot,accuracy,0.24838709677419354,0.024580028921480992
EleutherAI/pythia-6.9b,mmlu_college_biology,0-shot,accuracy,0.2777777777777778,0.03745554791462457
EleutherAI/pythia-6.9b,mmlu_anatomy,0-shot,accuracy,0.34814814814814815,0.041153246103369526
EleutherAI/pythia-6.9b,mmlu_college_chemistry,0-shot,accuracy,0.18,0.03861229196653696
EleutherAI/pythia-6.9b,mmlu_computer_security,0-shot,accuracy,0.28,0.04512608598542127
EleutherAI/pythia-6.9b,mmlu_college_computer_science,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-6.9b,mmlu_astronomy,0-shot,accuracy,0.29605263157894735,0.03715062154998905
EleutherAI/pythia-6.9b,mmlu_college_mathematics,0-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-6.9b,mmlu_conceptual_physics,0-shot,accuracy,0.32340425531914896,0.030579442773610348
EleutherAI/pythia-6.9b,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-6.9b,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.04725815626252605
EleutherAI/pythia-6.9b,mmlu_machine_learning,0-shot,accuracy,0.26785714285714285,0.04203277291467762
EleutherAI/pythia-6.9b,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.030516530732694436
EleutherAI/pythia-6.9b,mmlu_high_school_statistics,0-shot,accuracy,0.3425925925925926,0.03236585252602157
EleutherAI/pythia-6.9b,mmlu_elementary_mathematics,0-shot,accuracy,0.2830687830687831,0.023201392938194978
EleutherAI/pythia-6.9b,mmlu_electrical_engineering,0-shot,accuracy,0.27586206896551724,0.037245636197746325
EleutherAI/pythia-6.9b,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
EleutherAI/pythia-6.9b,arc_challenge,25-shot,accuracy,0.3575085324232082,0.014005494275916576
EleutherAI/pythia-6.9b,arc_challenge,25-shot,acc_norm,0.39590443686006827,0.014291228393536587
EleutherAI/pythia-6.9b,hellaswag,10-shot,accuracy,0.4816769567815176,0.004986429808146771
EleutherAI/pythia-6.9b,hellaswag,10-shot,acc_norm,0.6514638518223461,0.004755329243976686
EleutherAI/pythia-6.9b,truthfulqa_mc2,0-shot,accuracy,0.3515928441032545,0.013622121924448312
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,bleu_max,24.822049251809254,0.7441915100142679
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,bleu_acc,0.2876376988984088,0.015846315101394795
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,bleu_diff,-8.72973777939928,0.799560880574795
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge1_max,50.64779573586556,0.8314230941524794
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge1_acc,0.2692778457772338,0.015528566637087264
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge1_diff,-11.027700507612522,0.8410289087382377
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge2_max,33.75326270225714,0.9630024578384584
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge2_acc,0.22643818849449204,0.014651337324602585
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rouge2_diff,-13.190984697589347,1.019946221960148
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rougeL_max,47.78257310566027,0.8446585069903312
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rougeL_acc,0.2594859241126071,0.015345409485558
EleutherAI/pythia-6.9b,truthfulqa_gen,0-shot,rougeL_diff,-11.191544484143854,0.859662499239064
EleutherAI/pythia-6.9b,truthfulqa_mc1,0-shot,accuracy,0.2178702570379437,0.014450846714123899
EleutherAI/pythia-6.9b,winogrande,5-shot,accuracy,0.6400947119179163,0.013489609590266797
EleutherAI/pile-t5-large,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
EleutherAI/pile-t5-large,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
EleutherAI/pile-t5-large,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445813
EleutherAI/pile-t5-large,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
EleutherAI/pile-t5-large,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pile-t5-large,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/pile-t5-large,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
EleutherAI/pile-t5-large,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
EleutherAI/pile-t5-large,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
EleutherAI/pile-t5-large,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/pile-t5-large,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094634
EleutherAI/pile-t5-large,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
EleutherAI/pile-t5-large,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/pile-t5-large,mmlu_high_school_government_and_politics,0-shot,accuracy,0.19689119170984457,0.02869787397186069
EleutherAI/pile-t5-large,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
EleutherAI/pile-t5-large,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
EleutherAI/pile-t5-large,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936073
EleutherAI/pile-t5-large,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pile-t5-large,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pile-t5-large,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/pile-t5-large,mmlu_high_school_macroeconomics,0-shot,accuracy,0.20256410256410257,0.020377660970371397
EleutherAI/pile-t5-large,mmlu_security_studies,0-shot,accuracy,0.18775510204081633,0.02500025603954622
EleutherAI/pile-t5-large,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
EleutherAI/pile-t5-large,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/pile-t5-large,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813386
EleutherAI/pile-t5-large,mmlu_miscellaneous,0-shot,accuracy,0.23754789272030652,0.015218733046150195
EleutherAI/pile-t5-large,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
EleutherAI/pile-t5-large,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
EleutherAI/pile-t5-large,mmlu_nutrition,0-shot,accuracy,0.22549019607843138,0.023929155517351284
EleutherAI/pile-t5-large,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pile-t5-large,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
EleutherAI/pile-t5-large,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.02352924218519311
EleutherAI/pile-t5-large,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749884
EleutherAI/pile-t5-large,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pile-t5-large,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891377
EleutherAI/pile-t5-large,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
EleutherAI/pile-t5-large,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370518
EleutherAI/pile-t5-large,mmlu_professional_accounting,0-shot,accuracy,0.23404255319148937,0.025257861359432407
EleutherAI/pile-t5-large,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
EleutherAI/pile-t5-large,mmlu_high_school_physics,0-shot,accuracy,0.1986754966887417,0.032578473844367746
EleutherAI/pile-t5-large,mmlu_high_school_biology,0-shot,accuracy,0.1774193548387097,0.021732540689329265
EleutherAI/pile-t5-large,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
EleutherAI/pile-t5-large,mmlu_anatomy,0-shot,accuracy,0.18518518518518517,0.03355677216313142
EleutherAI/pile-t5-large,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pile-t5-large,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pile-t5-large,mmlu_college_computer_science,0-shot,accuracy,0.26,0.044084400227680794
EleutherAI/pile-t5-large,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pile-t5-large,mmlu_college_mathematics,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pile-t5-large,mmlu_conceptual_physics,0-shot,accuracy,0.26382978723404255,0.02880998985410298
EleutherAI/pile-t5-large,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pile-t5-large,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pile-t5-large,mmlu_machine_learning,0-shot,accuracy,0.3125,0.043994650575715215
EleutherAI/pile-t5-large,mmlu_high_school_chemistry,0-shot,accuracy,0.15270935960591134,0.025308904539380624
EleutherAI/pile-t5-large,mmlu_high_school_statistics,0-shot,accuracy,0.1527777777777778,0.02453632602613422
EleutherAI/pile-t5-large,mmlu_elementary_mathematics,0-shot,accuracy,0.20899470899470898,0.020940481565334835
EleutherAI/pile-t5-large,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
EleutherAI/pile-t5-large,mmlu_high_school_mathematics,0-shot,accuracy,0.2111111111111111,0.02488211685765508
EleutherAI/pile-t5-large,arc_challenge,25-shot,accuracy,0.2158703071672355,0.012022975360030672
EleutherAI/pile-t5-large,arc_challenge,25-shot,acc_norm,0.27559726962457337,0.01305716965576184
EleutherAI/pile-t5-large,hellaswag,10-shot,accuracy,0.28231428002389963,0.004492055279407092
EleutherAI/pile-t5-large,hellaswag,10-shot,acc_norm,0.30551682931686913,0.004596845936356621
EleutherAI/pile-t5-large,truthfulqa_mc2,0-shot,accuracy,0.48723332373660094,0.0163067633521161
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,bleu_max,1.0499198159270122,0.04876187889287449
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,bleu_acc,0.26193390452876375,0.015392118805015065
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,bleu_diff,0.13599492249561146,0.03779198159918627
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge1_max,6.835958235823023,0.22067055055215312
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge1_acc,0.3329253365973072,0.016497402382012045
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge1_diff,-0.20332352132988943,0.17365837360458108
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge2_max,1.9870130473864616,0.16437722496756121
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge2_acc,0.11995104039167687,0.011373924658319483
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rouge2_diff,0.22582388628383504,0.1327658805426097
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rougeL_max,6.194140588727995,0.20426988775326968
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rougeL_acc,0.3219094247246022,0.016355567611960428
EleutherAI/pile-t5-large,truthfulqa_gen,0-shot,rougeL_diff,-0.0526910210370656,0.16062420728649413
EleutherAI/pile-t5-large,truthfulqa_mc1,0-shot,accuracy,0.26438188494492043,0.015438211119522502
EleutherAI/pile-t5-large,winogrande,5-shot,accuracy,0.5224940805051302,0.014038257824059895
EleutherAI/pile-t5-large,gsm8k,5-shot,accuracy,0.01061410159211524,0.0028227133223877035
NousResearch/Nous-Hermes-2-Yi-34B,arc:challenge,25-shot,accuracy,0.64419795221843,0.013990571137918762
NousResearch/Nous-Hermes-2-Yi-34B,arc:challenge,25-shot,acc_norm,0.6689419795221843,0.01375206241981783
NousResearch/Nous-Hermes-2-Yi-34B,hellaswag,10-shot,accuracy,0.6577375024895439,0.004734972668299615
NousResearch/Nous-Hermes-2-Yi-34B,hellaswag,10-shot,acc_norm,0.8549093806014738,0.0035147239847366095
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.49,0.05024183937956913
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.49,0.05024183937956913
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-anatomy,5-shot,accuracy,0.7185185185185186,0.03885004245800253
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-anatomy,5-shot,acc_norm,0.7185185185185186,0.03885004245800253
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-astronomy,5-shot,accuracy,0.8947368421052632,0.024974533450920707
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-astronomy,5-shot,acc_norm,0.8947368421052632,0.024974533450920707
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-business_ethics,5-shot,accuracy,0.78,0.04163331998932262
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-business_ethics,5-shot,acc_norm,0.78,0.04163331998932262
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8,0.02461829819586651
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8,0.02461829819586651
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_biology,5-shot,accuracy,0.9027777777777778,0.02477451625044017
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_biology,5-shot,acc_norm,0.9027777777777778,0.02477451625044017
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_chemistry,5-shot,accuracy,0.51,0.05024183937956912
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.51,0.05024183937956912
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_computer_science,5-shot,accuracy,0.65,0.04793724854411018
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.65,0.04793724854411018
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_mathematics,5-shot,accuracy,0.49,0.05024183937956914
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.49,0.05024183937956914
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_medicine,5-shot,accuracy,0.6994219653179191,0.0349610148119118
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_medicine,5-shot,acc_norm,0.6994219653179191,0.0349610148119118
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_physics,5-shot,accuracy,0.5294117647058824,0.049665709039785295
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-college_physics,5-shot,acc_norm,0.5294117647058824,0.049665709039785295
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-computer_security,5-shot,accuracy,0.83,0.03775251680686371
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-computer_security,5-shot,acc_norm,0.83,0.03775251680686371
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7914893617021277,0.026556982117838728
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7914893617021277,0.026556982117838728
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-econometrics,5-shot,accuracy,0.5701754385964912,0.04657047260594963
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-econometrics,5-shot,acc_norm,0.5701754385964912,0.04657047260594963
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7724137931034483,0.03493950380131184
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7724137931034483,0.03493950380131184
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.6904761904761905,0.023809523809523864
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.6904761904761905,0.023809523809523864
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-formal_logic,5-shot,accuracy,0.5793650793650794,0.04415438226743745
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5793650793650794,0.04415438226743745
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-global_facts,5-shot,accuracy,0.51,0.05024183937956911
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-global_facts,5-shot,acc_norm,0.51,0.05024183937956911
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_biology,5-shot,accuracy,0.896774193548387,0.01730838128103453
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.896774193548387,0.01730838128103453
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6206896551724138,0.03413963805906235
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6206896551724138,0.03413963805906235
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.84,0.03684529491774709
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.84,0.03684529491774709
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8787878787878788,0.02548549837334323
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8787878787878788,0.02548549837334323
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_geography,5-shot,accuracy,0.898989898989899,0.021469735576055353
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.898989898989899,0.021469735576055353
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9792746113989638,0.010281417011909039
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9792746113989638,0.010281417011909039
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8282051282051283,0.01912490360342356
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8282051282051283,0.01912490360342356
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4148148148148148,0.03003984245406929
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4148148148148148,0.03003984245406929
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8529411764705882,0.023005459446673936
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8529411764705882,0.023005459446673936
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_physics,5-shot,accuracy,0.5033112582781457,0.04082393379449654
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5033112582781457,0.04082393379449654
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9211009174311927,0.011558198113769569
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9211009174311927,0.011558198113769569
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6620370370370371,0.03225941352631295
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6620370370370371,0.03225941352631295
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9117647058823529,0.019907399791316952
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9117647058823529,0.019907399791316952
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9071729957805907,0.01888975055095671
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9071729957805907,0.01888975055095671
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_aging,5-shot,accuracy,0.7937219730941704,0.027157150479563824
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_aging,5-shot,acc_norm,0.7937219730941704,0.027157150479563824
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8931297709923665,0.027096548624883733
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8931297709923665,0.027096548624883733
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-international_law,5-shot,accuracy,0.9090909090909091,0.02624319405407388
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-international_law,5-shot,acc_norm,0.9090909090909091,0.02624319405407388
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8981481481481481,0.02923927267563274
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8981481481481481,0.02923927267563274
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8711656441717791,0.026321383198783653
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8711656441717791,0.026321383198783653
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-machine_learning,5-shot,accuracy,0.6071428571428571,0.04635550135609976
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-machine_learning,5-shot,acc_norm,0.6071428571428571,0.04635550135609976
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-management,5-shot,accuracy,0.9223300970873787,0.026501440784762766
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-management,5-shot,acc_norm,0.9223300970873787,0.026501440784762766
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-marketing,5-shot,accuracy,0.9188034188034188,0.017893784904018536
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-marketing,5-shot,acc_norm,0.9188034188034188,0.017893784904018536
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-medical_genetics,5-shot,accuracy,0.86,0.03487350880197771
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.86,0.03487350880197771
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9106002554278416,0.010203017847688307
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9106002554278416,0.010203017847688307
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_disputes,5-shot,accuracy,0.8352601156069365,0.019971040982442265
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8352601156069365,0.019971040982442265
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.7106145251396648,0.015166544550490288
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.7106145251396648,0.015166544550490288
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-nutrition,5-shot,accuracy,0.8431372549019608,0.02082375883758091
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-nutrition,5-shot,acc_norm,0.8431372549019608,0.02082375883758091
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-philosophy,5-shot,accuracy,0.8135048231511254,0.022122439772480768
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-philosophy,5-shot,acc_norm,0.8135048231511254,0.022122439772480768
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-prehistory,5-shot,accuracy,0.8888888888888888,0.017486432785880704
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-prehistory,5-shot,acc_norm,0.8888888888888888,0.017486432785880704
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_accounting,5-shot,accuracy,0.648936170212766,0.02847350127296376
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.648936170212766,0.02847350127296376
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_law,5-shot,accuracy,0.6166883963494133,0.012417603662901185
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_law,5-shot,acc_norm,0.6166883963494133,0.012417603662901185
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8308823529411765,0.02277086801011301
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8308823529411765,0.02277086801011301
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8235294117647058,0.015422512066262549
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8235294117647058,0.015422512066262549
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-public_relations,5-shot,accuracy,0.7181818181818181,0.043091187099464585
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-public_relations,5-shot,acc_norm,0.7181818181818181,0.043091187099464585
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-security_studies,5-shot,accuracy,0.8408163265306122,0.023420972069166344
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-security_studies,5-shot,acc_norm,0.8408163265306122,0.023420972069166344
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-sociology,5-shot,accuracy,0.8756218905472637,0.023335401790166323
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-sociology,5-shot,acc_norm,0.8756218905472637,0.023335401790166323
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.0272659924344291
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-virology,5-shot,accuracy,0.572289156626506,0.038515976837185335
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-virology,5-shot,acc_norm,0.572289156626506,0.038515976837185335
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-world_religions,5-shot,accuracy,0.8713450292397661,0.025679342723276908
NousResearch/Nous-Hermes-2-Yi-34B,hendrycksTest-world_religions,5-shot,acc_norm,0.8713450292397661,0.025679342723276908
NousResearch/Nous-Hermes-2-Yi-34B,truthfulqa:mc,0-shot,mc1,0.4357405140758874,0.017358345398863127
NousResearch/Nous-Hermes-2-Yi-34B,truthfulqa:mc,0-shot,mc2,0.6037423421940498,0.014892857583579324
NousResearch/Nous-Hermes-2-Yi-34B,winogrande,5-shot,accuracy,0.829518547750592,0.01056902112282592
NousResearch/Nous-Hermes-2-Yi-34B,gsm8k,5-shot,accuracy,0.7005307050796058,0.012616300735519658
cerebras/Cerebras-GPT-2.7B,minerva_math_precalc,5-shot,accuracy,0.02197802197802198,0.0062801549282525335
cerebras/Cerebras-GPT-2.7B,minerva_math_prealgebra,5-shot,accuracy,0.02525832376578645,0.005319703220303023
cerebras/Cerebras-GPT-2.7B,minerva_math_num_theory,5-shot,accuracy,0.011111111111111112,0.004515003707694652
cerebras/Cerebras-GPT-2.7B,minerva_math_intermediate_algebra,5-shot,accuracy,0.014396456256921373,0.003966209590910285
cerebras/Cerebras-GPT-2.7B,minerva_math_geometry,5-shot,accuracy,0.016701461377870562,0.005861462425818025
cerebras/Cerebras-GPT-2.7B,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.006275362513989594
cerebras/Cerebras-GPT-2.7B,minerva_math_algebra,5-shot,accuracy,0.009267059814658803,0.002782319118488812
cerebras/Cerebras-GPT-2.7B,fld_default,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,fld_star,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
cerebras/Cerebras-GPT-2.7B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
cerebras/Cerebras-GPT-2.7B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
cerebras/Cerebras-GPT-2.7B,arithmetic_2ds,5-shot,accuracy,0.0125,0.00248494717876267
cerebras/Cerebras-GPT-2.7B,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_1dc,5-shot,accuracy,0.0135,0.0025811249685072746
cerebras/Cerebras-GPT-2.7B,arithmetic_4ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-2.7B,arithmetic_2dm,5-shot,accuracy,0.022,0.0032807593162018913
cerebras/Cerebras-GPT-2.7B,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237512
cerebras/Cerebras-GPT-2.7B,gsm8k_cot,5-shot,accuracy,0.027293404094010616,0.004488095380209752
cerebras/Cerebras-GPT-2.7B,gsm8k,5-shot,accuracy,0.004548900682335102,0.0018535550440036204
cerebras/Cerebras-GPT-2.7B,anli_r2,0-shot,brier_score,0.7441656976970576,
cerebras/Cerebras-GPT-2.7B,anli_r3,0-shot,brier_score,0.7347729274486875,
cerebras/Cerebras-GPT-2.7B,anli_r1,0-shot,brier_score,0.7559520884886023,
cerebras/Cerebras-GPT-2.7B,xnli_eu,0-shot,brier_score,1.1749589886887095,
cerebras/Cerebras-GPT-2.7B,xnli_vi,0-shot,brier_score,0.7928664423474424,
cerebras/Cerebras-GPT-2.7B,xnli_ru,0-shot,brier_score,0.7994578741034748,
cerebras/Cerebras-GPT-2.7B,xnli_zh,0-shot,brier_score,0.940473825271897,
cerebras/Cerebras-GPT-2.7B,xnli_tr,0-shot,brier_score,0.8423086576456662,
cerebras/Cerebras-GPT-2.7B,xnli_fr,0-shot,brier_score,0.8332420930715818,
cerebras/Cerebras-GPT-2.7B,xnli_en,0-shot,brier_score,0.6809222796174058,
cerebras/Cerebras-GPT-2.7B,xnli_ur,0-shot,brier_score,1.016116798585533,
cerebras/Cerebras-GPT-2.7B,xnli_ar,0-shot,brier_score,1.0259751211437746,
cerebras/Cerebras-GPT-2.7B,xnli_de,0-shot,brier_score,0.9495039241411007,
cerebras/Cerebras-GPT-2.7B,xnli_hi,0-shot,brier_score,0.7931095115018697,
cerebras/Cerebras-GPT-2.7B,xnli_es,0-shot,brier_score,0.8553823700128562,
cerebras/Cerebras-GPT-2.7B,xnli_bg,0-shot,brier_score,0.7603246399463625,
cerebras/Cerebras-GPT-2.7B,xnli_sw,0-shot,brier_score,0.8740183453743544,
cerebras/Cerebras-GPT-2.7B,xnli_el,0-shot,brier_score,1.1103931638852953,
cerebras/Cerebras-GPT-2.7B,xnli_th,0-shot,brier_score,0.8312683414736515,
cerebras/Cerebras-GPT-2.7B,logiqa2,0-shot,brier_score,1.1527212168892447,
cerebras/Cerebras-GPT-2.7B,mathqa,5-shot,brier_score,0.9819685448386691,
cerebras/Cerebras-GPT-2.7B,lambada_standard,0-shot,perplexity,12.37585271202106,0.3686403362745451
cerebras/Cerebras-GPT-2.7B,lambada_standard,0-shot,accuracy,0.46807684843780323,0.006951765275756158
cerebras/Cerebras-GPT-2.7B,lambada_openai,0-shot,perplexity,7.735706433404285,0.20963908845319387
cerebras/Cerebras-GPT-2.7B,lambada_openai,0-shot,accuracy,0.5651077042499515,0.006906667423619273
cerebras/Cerebras-GPT-2.7B,mmlu_world_religions,0-shot,accuracy,0.26900584795321636,0.0340105262010409
cerebras/Cerebras-GPT-2.7B,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.03718489006818114
cerebras/Cerebras-GPT-2.7B,mmlu_prehistory,0-shot,accuracy,0.23765432098765432,0.02368359183700856
cerebras/Cerebras-GPT-2.7B,mmlu_moral_scenarios,0-shot,accuracy,0.27150837988826815,0.014874252168095271
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_world_history,0-shot,accuracy,0.25738396624472576,0.028458820991460288
cerebras/Cerebras-GPT-2.7B,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763006,0.023868003262500104
cerebras/Cerebras-GPT-2.7B,mmlu_professional_law,0-shot,accuracy,0.2633637548891786,0.011249506403605284
cerebras/Cerebras-GPT-2.7B,mmlu_logical_fallacies,0-shot,accuracy,0.31901840490797545,0.03661997551073836
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_us_history,0-shot,accuracy,0.2696078431372549,0.031145570659486782
cerebras/Cerebras-GPT-2.7B,mmlu_philosophy,0-shot,accuracy,0.27009646302250806,0.025218040373410622
cerebras/Cerebras-GPT-2.7B,mmlu_jurisprudence,0-shot,accuracy,0.18518518518518517,0.037552658650371835
cerebras/Cerebras-GPT-2.7B,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_european_history,0-shot,accuracy,0.22424242424242424,0.03256866661681102
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.21761658031088082,0.02977866303775296
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.027553614467863804
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_geography,0-shot,accuracy,0.22727272727272727,0.02985751567338641
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_psychology,0-shot,accuracy,0.23302752293577983,0.018125669180861503
cerebras/Cerebras-GPT-2.7B,mmlu_public_relations,0-shot,accuracy,0.16363636363636364,0.035434330542986774
cerebras/Cerebras-GPT-2.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.04560480215720683
cerebras/Cerebras-GPT-2.7B,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.03076944496729601
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24102564102564103,0.021685546665333195
cerebras/Cerebras-GPT-2.7B,mmlu_security_studies,0-shot,accuracy,0.32653061224489793,0.030021056238440307
cerebras/Cerebras-GPT-2.7B,mmlu_professional_psychology,0-shot,accuracy,0.2581699346405229,0.017704531653250078
cerebras/Cerebras-GPT-2.7B,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.037276735755969174
cerebras/Cerebras-GPT-2.7B,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.040493392977481425
cerebras/Cerebras-GPT-2.7B,mmlu_miscellaneous,0-shot,accuracy,0.26181353767560667,0.01572083867844527
cerebras/Cerebras-GPT-2.7B,mmlu_marketing,0-shot,accuracy,0.2564102564102564,0.02860595370200425
cerebras/Cerebras-GPT-2.7B,mmlu_management,0-shot,accuracy,0.22330097087378642,0.04123553189891431
cerebras/Cerebras-GPT-2.7B,mmlu_nutrition,0-shot,accuracy,0.2679738562091503,0.025360603796242553
cerebras/Cerebras-GPT-2.7B,mmlu_medical_genetics,0-shot,accuracy,0.21,0.040936018074033256
cerebras/Cerebras-GPT-2.7B,mmlu_human_aging,0-shot,accuracy,0.242152466367713,0.028751392398694755
cerebras/Cerebras-GPT-2.7B,mmlu_professional_medicine,0-shot,accuracy,0.25,0.026303648393696036
cerebras/Cerebras-GPT-2.7B,mmlu_college_medicine,0-shot,accuracy,0.23121387283236994,0.032147373020294696
cerebras/Cerebras-GPT-2.7B,mmlu_business_ethics,0-shot,accuracy,0.22,0.04163331998932269
cerebras/Cerebras-GPT-2.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.025288394502891373
cerebras/Cerebras-GPT-2.7B,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
cerebras/Cerebras-GPT-2.7B,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370519
cerebras/Cerebras-GPT-2.7B,mmlu_professional_accounting,0-shot,accuracy,0.2695035460992908,0.026469036818590634
cerebras/Cerebras-GPT-2.7B,mmlu_college_physics,0-shot,accuracy,0.17647058823529413,0.037932811853078105
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_biology,0-shot,accuracy,0.24838709677419354,0.024580028921481003
cerebras/Cerebras-GPT-2.7B,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.03716177437566016
cerebras/Cerebras-GPT-2.7B,mmlu_anatomy,0-shot,accuracy,0.2814814814814815,0.03885004245800255
cerebras/Cerebras-GPT-2.7B,mmlu_college_chemistry,0-shot,accuracy,0.19,0.03942772444036623
cerebras/Cerebras-GPT-2.7B,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276808
cerebras/Cerebras-GPT-2.7B,mmlu_college_computer_science,0-shot,accuracy,0.37,0.048523658709391
cerebras/Cerebras-GPT-2.7B,mmlu_astronomy,0-shot,accuracy,0.19736842105263158,0.03238981601699397
cerebras/Cerebras-GPT-2.7B,mmlu_college_mathematics,0-shot,accuracy,0.25,0.04351941398892446
cerebras/Cerebras-GPT-2.7B,mmlu_conceptual_physics,0-shot,accuracy,0.25957446808510637,0.028659179374292316
cerebras/Cerebras-GPT-2.7B,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.042295258468165044
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.39,0.04902071300001975
cerebras/Cerebras-GPT-2.7B,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.04493949068613539
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.03108982600293752
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_statistics,0-shot,accuracy,0.2824074074074074,0.030701372111510927
cerebras/Cerebras-GPT-2.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.2037037037037037,0.020742740560122656
cerebras/Cerebras-GPT-2.7B,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.036001056927277716
cerebras/Cerebras-GPT-2.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.26666666666666666,0.02696242432507384
cerebras/Cerebras-GPT-2.7B,arc_challenge,25-shot,accuracy,0.26109215017064846,0.01283552390947384
cerebras/Cerebras-GPT-2.7B,arc_challenge,25-shot,acc_norm,0.2883959044368601,0.013238394422428175
cerebras/Cerebras-GPT-2.7B,hellaswag,10-shot,accuracy,0.38548097988448515,0.004857140410776741
cerebras/Cerebras-GPT-2.7B,hellaswag,10-shot,acc_norm,0.4929296952798247,0.004989282516055396
cerebras/Cerebras-GPT-2.7B,truthfulqa_mc2,0-shot,accuracy,0.413716384386627,0.014440498911294912
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,bleu_max,23.035243335291693,0.7624195895417043
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,bleu_acc,0.4467564259485924,0.017403977522557144
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,bleu_diff,2.6100154453645934,0.9107727769465377
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge1_max,45.78174774695968,0.9626102741710353
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge1_acc,0.38555691554467564,0.017038839010591667
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge1_diff,3.3839378780303964,1.307720244891136
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge2_max,29.129612331742987,1.1201208139741414
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge2_acc,0.2839657282741738,0.01578537085839671
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rouge2_diff,2.646752250018767,1.3612360250946927
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rougeL_max,43.715314843247654,0.971141942560561
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rougeL_acc,0.390452876376989,0.017078230743431448
cerebras/Cerebras-GPT-2.7B,truthfulqa_gen,0-shot,rougeL_diff,3.7178865764791853,1.3006816230973743
cerebras/Cerebras-GPT-2.7B,truthfulqa_mc1,0-shot,accuracy,0.2460220318237454,0.015077219200662587
cerebras/Cerebras-GPT-2.7B,winogrande,5-shot,accuracy,0.5414364640883977,0.014004146853791914
cerebras/Cerebras-GPT-6.7B,minerva_math_precalc,5-shot,accuracy,0.01098901098901099,0.004465618427331413
cerebras/Cerebras-GPT-6.7B,minerva_math_prealgebra,5-shot,accuracy,0.021814006888633754,0.0049524353688735236
cerebras/Cerebras-GPT-6.7B,minerva_math_num_theory,5-shot,accuracy,0.022222222222222223,0.006349206349206325
cerebras/Cerebras-GPT-6.7B,minerva_math_intermediate_algebra,5-shot,accuracy,0.018826135105204873,0.004525330498668476
cerebras/Cerebras-GPT-6.7B,minerva_math_geometry,5-shot,accuracy,0.016701461377870562,0.0058614624258180245
cerebras/Cerebras-GPT-6.7B,minerva_math_counting_and_prob,5-shot,accuracy,0.012658227848101266,0.005140313889578842
cerebras/Cerebras-GPT-6.7B,minerva_math_algebra,5-shot,accuracy,0.015164279696714406,0.003548546043132535
cerebras/Cerebras-GPT-6.7B,fld_default,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,fld_star,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521528
cerebras/Cerebras-GPT-6.7B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
cerebras/Cerebras-GPT-6.7B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
cerebras/Cerebras-GPT-6.7B,arithmetic_2ds,5-shot,accuracy,0.016,0.002806410156941534
cerebras/Cerebras-GPT-6.7B,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_1dc,5-shot,accuracy,0.033,0.003995432609977371
cerebras/Cerebras-GPT-6.7B,arithmetic_4ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-6.7B,arithmetic_2dm,5-shot,accuracy,0.0225,0.003316982994845522
cerebras/Cerebras-GPT-6.7B,arithmetic_2da,5-shot,accuracy,0.0195,0.003092678018912424
cerebras/Cerebras-GPT-6.7B,gsm8k_cot,5-shot,accuracy,0.032600454890068235,0.00489166902193957
cerebras/Cerebras-GPT-6.7B,gsm8k,5-shot,accuracy,0.00530705079605762,0.0020013057209480483
cerebras/Cerebras-GPT-6.7B,anli_r2,0-shot,brier_score,0.755675118118756,
cerebras/Cerebras-GPT-6.7B,anli_r3,0-shot,brier_score,0.6999705904132336,
cerebras/Cerebras-GPT-6.7B,anli_r1,0-shot,brier_score,0.7536494384689287,
cerebras/Cerebras-GPT-6.7B,xnli_eu,0-shot,brier_score,1.089933422701538,
cerebras/Cerebras-GPT-6.7B,xnli_vi,0-shot,brier_score,0.7966443228842566,
cerebras/Cerebras-GPT-6.7B,xnli_ru,0-shot,brier_score,0.7989806201813834,
cerebras/Cerebras-GPT-6.7B,xnli_zh,0-shot,brier_score,1.0334911487693128,
cerebras/Cerebras-GPT-6.7B,xnli_tr,0-shot,brier_score,0.839021250171503,
cerebras/Cerebras-GPT-6.7B,xnli_fr,0-shot,brier_score,0.8071513485603421,
cerebras/Cerebras-GPT-6.7B,xnli_en,0-shot,brier_score,0.6656814608095787,
cerebras/Cerebras-GPT-6.7B,xnli_ur,0-shot,brier_score,0.9428490629298696,
cerebras/Cerebras-GPT-6.7B,xnli_ar,0-shot,brier_score,1.0302855031988798,
cerebras/Cerebras-GPT-6.7B,xnli_de,0-shot,brier_score,0.8364057760607875,
cerebras/Cerebras-GPT-6.7B,xnli_hi,0-shot,brier_score,0.7590215516967309,
cerebras/Cerebras-GPT-6.7B,xnli_es,0-shot,brier_score,0.8316353906107824,
cerebras/Cerebras-GPT-6.7B,xnli_bg,0-shot,brier_score,0.8758042326498179,
cerebras/Cerebras-GPT-6.7B,xnli_sw,0-shot,brier_score,0.8831334480577933,
cerebras/Cerebras-GPT-6.7B,xnli_el,0-shot,brier_score,0.8987268713940024,
cerebras/Cerebras-GPT-6.7B,xnli_th,0-shot,brier_score,0.8062685621961146,
cerebras/Cerebras-GPT-6.7B,logiqa2,0-shot,brier_score,1.1489525363355495,
cerebras/Cerebras-GPT-6.7B,mathqa,5-shot,brier_score,0.9586676691784485,
cerebras/Cerebras-GPT-6.7B,lambada_standard,0-shot,perplexity,7.7343856575560315,0.2042279549336044
cerebras/Cerebras-GPT-6.7B,lambada_standard,0-shot,accuracy,0.548418397050262,0.006933239470474408
cerebras/Cerebras-GPT-6.7B,lambada_openai,0-shot,perplexity,5.350437725308013,0.13313966140293174
cerebras/Cerebras-GPT-6.7B,lambada_openai,0-shot,accuracy,0.6367164758393169,0.0067005116674768135
cerebras/Cerebras-GPT-6.7B,mmlu_world_religions,0-shot,accuracy,0.32748538011695905,0.03599335771456027
cerebras/Cerebras-GPT-6.7B,mmlu_formal_logic,0-shot,accuracy,0.23809523809523808,0.03809523809523811
cerebras/Cerebras-GPT-6.7B,mmlu_prehistory,0-shot,accuracy,0.24074074074074073,0.02378858355165853
cerebras/Cerebras-GPT-6.7B,mmlu_moral_scenarios,0-shot,accuracy,0.27262569832402234,0.014893391735249608
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_world_history,0-shot,accuracy,0.22362869198312235,0.027123298205229972
cerebras/Cerebras-GPT-6.7B,mmlu_moral_disputes,0-shot,accuracy,0.28034682080924855,0.02418242749657762
cerebras/Cerebras-GPT-6.7B,mmlu_professional_law,0-shot,accuracy,0.26988265971316816,0.01133738108425041
cerebras/Cerebras-GPT-6.7B,mmlu_logical_fallacies,0-shot,accuracy,0.27607361963190186,0.03512385283705051
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_us_history,0-shot,accuracy,0.23529411764705882,0.029771775228145638
cerebras/Cerebras-GPT-6.7B,mmlu_philosophy,0-shot,accuracy,0.2604501607717042,0.02492672322484554
cerebras/Cerebras-GPT-6.7B,mmlu_jurisprudence,0-shot,accuracy,0.23148148148148148,0.04077494709252626
cerebras/Cerebras-GPT-6.7B,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.04026187527591207
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_european_history,0-shot,accuracy,0.2727272727272727,0.03477691162163659
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.21761658031088082,0.02977866303775296
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.24789915966386555,0.028047967224176892
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_geography,0-shot,accuracy,0.18686868686868688,0.027772533334218977
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_psychology,0-shot,accuracy,0.29908256880733947,0.01963041728541517
cerebras/Cerebras-GPT-6.7B,mmlu_public_relations,0-shot,accuracy,0.24545454545454545,0.04122066502878285
cerebras/Cerebras-GPT-6.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.04461960433384739
cerebras/Cerebras-GPT-6.7B,mmlu_sociology,0-shot,accuracy,0.24875621890547264,0.030567675938916714
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2564102564102564,0.02213908110397153
cerebras/Cerebras-GPT-6.7B,mmlu_security_studies,0-shot,accuracy,0.19591836734693877,0.025409301953225678
cerebras/Cerebras-GPT-6.7B,mmlu_professional_psychology,0-shot,accuracy,0.24673202614379086,0.017440820367402493
cerebras/Cerebras-GPT-6.7B,mmlu_human_sexuality,0-shot,accuracy,0.25190839694656486,0.038073871163060866
cerebras/Cerebras-GPT-6.7B,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748142
cerebras/Cerebras-GPT-6.7B,mmlu_miscellaneous,0-shot,accuracy,0.26436781609195403,0.01576998484069052
cerebras/Cerebras-GPT-6.7B,mmlu_marketing,0-shot,accuracy,0.23931623931623933,0.027951826808924333
cerebras/Cerebras-GPT-6.7B,mmlu_management,0-shot,accuracy,0.27184466019417475,0.044052680241409216
cerebras/Cerebras-GPT-6.7B,mmlu_nutrition,0-shot,accuracy,0.23529411764705882,0.02428861946604612
cerebras/Cerebras-GPT-6.7B,mmlu_medical_genetics,0-shot,accuracy,0.26,0.04408440022768078
cerebras/Cerebras-GPT-6.7B,mmlu_human_aging,0-shot,accuracy,0.29596412556053814,0.030636591348699817
cerebras/Cerebras-GPT-6.7B,mmlu_professional_medicine,0-shot,accuracy,0.41911764705882354,0.029972807170464622
cerebras/Cerebras-GPT-6.7B,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
cerebras/Cerebras-GPT-6.7B,mmlu_business_ethics,0-shot,accuracy,0.14,0.03487350880197773
cerebras/Cerebras-GPT-6.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.23018867924528302,0.025907897122408173
cerebras/Cerebras-GPT-6.7B,mmlu_global_facts,0-shot,accuracy,0.18,0.03861229196653697
cerebras/Cerebras-GPT-6.7B,mmlu_virology,0-shot,accuracy,0.26506024096385544,0.03436024037944967
cerebras/Cerebras-GPT-6.7B,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902013
cerebras/Cerebras-GPT-6.7B,mmlu_college_physics,0-shot,accuracy,0.24509803921568626,0.042801058373643966
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_physics,0-shot,accuracy,0.271523178807947,0.03631329803969653
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_biology,0-shot,accuracy,0.21935483870967742,0.023540799358723295
cerebras/Cerebras-GPT-6.7B,mmlu_college_biology,0-shot,accuracy,0.20833333333333334,0.033961162058453336
cerebras/Cerebras-GPT-6.7B,mmlu_anatomy,0-shot,accuracy,0.22962962962962963,0.036333844140734636
cerebras/Cerebras-GPT-6.7B,mmlu_college_chemistry,0-shot,accuracy,0.31,0.04648231987117316
cerebras/Cerebras-GPT-6.7B,mmlu_computer_security,0-shot,accuracy,0.21,0.040936018074033256
cerebras/Cerebras-GPT-6.7B,mmlu_college_computer_science,0-shot,accuracy,0.4,0.049236596391733084
cerebras/Cerebras-GPT-6.7B,mmlu_astronomy,0-shot,accuracy,0.2565789473684211,0.0355418036802569
cerebras/Cerebras-GPT-6.7B,mmlu_college_mathematics,0-shot,accuracy,0.3,0.046056618647183814
cerebras/Cerebras-GPT-6.7B,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.030472973363380056
cerebras/Cerebras-GPT-6.7B,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.04512608598542128
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.37,0.048523658709391
cerebras/Cerebras-GPT-6.7B,mmlu_machine_learning,0-shot,accuracy,0.23214285714285715,0.04007341809755807
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.20689655172413793,0.02850137816789395
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_statistics,0-shot,accuracy,0.3425925925925926,0.032365852526021574
cerebras/Cerebras-GPT-6.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.23015873015873015,0.021679219663693145
cerebras/Cerebras-GPT-6.7B,mmlu_electrical_engineering,0-shot,accuracy,0.25517241379310346,0.03632984052707842
cerebras/Cerebras-GPT-6.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.026842057873833713
cerebras/Cerebras-GPT-6.7B,arc_challenge,25-shot,accuracy,0.3242320819112628,0.013678810399518813
cerebras/Cerebras-GPT-6.7B,arc_challenge,25-shot,acc_norm,0.35238907849829354,0.01396014260059869
cerebras/Cerebras-GPT-6.7B,hellaswag,10-shot,accuracy,0.4451304521011751,0.00495964526339023
cerebras/Cerebras-GPT-6.7B,hellaswag,10-shot,acc_norm,0.5936068512248556,0.00490155813233552
cerebras/Cerebras-GPT-6.7B,truthfulqa_mc2,0-shot,accuracy,0.3803445214836313,0.01392905684581036
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,bleu_max,22.34064093494146,0.7183987511971597
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,bleu_acc,0.2741738066095471,0.015616518497219367
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,bleu_diff,-7.446457636161483,0.7565638438274547
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge1_max,48.16517696338372,0.8420427437367002
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge1_acc,0.2717258261933905,0.01557284045287583
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge1_diff,-8.43929405627723,0.823148680564784
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge2_max,31.457102690393747,0.9472192452069558
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge2_acc,0.22888616891064872,0.014706994909055027
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rouge2_diff,-10.247194848412033,0.9595694112827735
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rougeL_max,44.8539232504777,0.8545145349902917
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rougeL_acc,0.25703794369645044,0.015298077509485083
cerebras/Cerebras-GPT-6.7B,truthfulqa_gen,0-shot,rougeL_diff,-8.674604971448174,0.8352252105721707
cerebras/Cerebras-GPT-6.7B,truthfulqa_mc1,0-shot,accuracy,0.24357405140758873,0.015026354824910782
cerebras/Cerebras-GPT-6.7B,winogrande,5-shot,accuracy,0.5872138910812944,0.013837060648682103
AbacusResearch/jaLLAbi2-7b,minerva_math_precalc,5-shot,accuracy,0.07875457875457875,0.0115379147687345
AbacusResearch/jaLLAbi2-7b,minerva_math_prealgebra,5-shot,accuracy,0.42479908151549944,0.016758762397009255
AbacusResearch/jaLLAbi2-7b,minerva_math_num_theory,5-shot,accuracy,0.15,0.015380154912113008
AbacusResearch/jaLLAbi2-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.07862679955703211,0.008961894321625518
AbacusResearch/jaLLAbi2-7b,minerva_math_geometry,5-shot,accuracy,0.19206680584551147,0.018017724185011356
AbacusResearch/jaLLAbi2-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.21729957805907174,0.01896254634032935
AbacusResearch/jaLLAbi2-7b,minerva_math_algebra,5-shot,accuracy,0.3201347935973041,0.01354676204212894
AbacusResearch/jaLLAbi2-7b,fld_default,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi2-7b,fld_star,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi2-7b,arithmetic_3da,5-shot,accuracy,0.988,0.002435357362429865
AbacusResearch/jaLLAbi2-7b,arithmetic_3ds,5-shot,accuracy,0.985,0.002718675338799951
AbacusResearch/jaLLAbi2-7b,arithmetic_4da,5-shot,accuracy,0.9445,0.005120838456077827
AbacusResearch/jaLLAbi2-7b,arithmetic_2ds,5-shot,accuracy,0.995,0.0015775754727385177
AbacusResearch/jaLLAbi2-7b,arithmetic_5ds,5-shot,accuracy,0.9005,0.0066949448200168534
AbacusResearch/jaLLAbi2-7b,arithmetic_5da,5-shot,accuracy,0.9245,0.005909089072508007
AbacusResearch/jaLLAbi2-7b,arithmetic_1dc,5-shot,accuracy,0.769,0.009426766782199622
AbacusResearch/jaLLAbi2-7b,arithmetic_4ds,5-shot,accuracy,0.9545,0.004661087627253434
AbacusResearch/jaLLAbi2-7b,arithmetic_2dm,5-shot,accuracy,0.674,0.01048412888509276
AbacusResearch/jaLLAbi2-7b,arithmetic_2da,5-shot,accuracy,1.0,
AbacusResearch/jaLLAbi2-7b,gsm8k_cot,5-shot,accuracy,0.759666413949962,0.011769580703836952
AbacusResearch/jaLLAbi2-7b,gsm8k,5-shot,accuracy,0.7354056103108415,0.012150554001563233
AbacusResearch/jaLLAbi2-7b,anli_r2,0-shot,brier_score,0.7735189327159094,
AbacusResearch/jaLLAbi2-7b,anli_r3,0-shot,brier_score,0.8359426771055016,
AbacusResearch/jaLLAbi2-7b,anli_r1,0-shot,brier_score,0.6783474989688411,
AbacusResearch/jaLLAbi2-7b,xnli_eu,0-shot,brier_score,1.0950678701767658,
AbacusResearch/jaLLAbi2-7b,xnli_vi,0-shot,brier_score,0.9853321581143759,
AbacusResearch/jaLLAbi2-7b,xnli_ru,0-shot,brier_score,0.9458423472607614,
AbacusResearch/jaLLAbi2-7b,xnli_zh,0-shot,brier_score,1.0653229084833014,
AbacusResearch/jaLLAbi2-7b,xnli_tr,0-shot,brier_score,1.0516964598791096,
AbacusResearch/jaLLAbi2-7b,xnli_fr,0-shot,brier_score,0.9213564341895442,
AbacusResearch/jaLLAbi2-7b,xnli_en,0-shot,brier_score,0.7308615429117169,
AbacusResearch/jaLLAbi2-7b,xnli_ur,0-shot,brier_score,1.1916179316711006,
AbacusResearch/jaLLAbi2-7b,xnli_ar,0-shot,brier_score,1.2229906458740496,
AbacusResearch/jaLLAbi2-7b,xnli_de,0-shot,brier_score,0.8943690517747402,
AbacusResearch/jaLLAbi2-7b,xnli_hi,0-shot,brier_score,0.9606897711791605,
AbacusResearch/jaLLAbi2-7b,xnli_es,0-shot,brier_score,0.970678317354639,
AbacusResearch/jaLLAbi2-7b,xnli_bg,0-shot,brier_score,0.9716211440198455,
AbacusResearch/jaLLAbi2-7b,xnli_sw,0-shot,brier_score,1.04639506388785,
AbacusResearch/jaLLAbi2-7b,xnli_el,0-shot,brier_score,0.9811536445855956,
AbacusResearch/jaLLAbi2-7b,xnli_th,0-shot,brier_score,1.060485339632959,
AbacusResearch/jaLLAbi2-7b,logiqa2,0-shot,brier_score,0.9724763634284794,
AbacusResearch/jaLLAbi2-7b,mathqa,5-shot,brier_score,0.9560746958174166,
AbacusResearch/jaLLAbi2-7b,lambada_standard,0-shot,perplexity,4.06627682379339,0.10368161889943868
AbacusResearch/jaLLAbi2-7b,lambada_standard,0-shot,accuracy,0.6598098195226082,0.006600583766063726
AbacusResearch/jaLLAbi2-7b,lambada_openai,0-shot,perplexity,3.3960452191861408,0.07798732129358217
AbacusResearch/jaLLAbi2-7b,lambada_openai,0-shot,accuracy,0.7135649136425384,0.006298569473987373
AbacusResearch/jaLLAbi2-7b,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.028380919596145866
AbacusResearch/jaLLAbi2-7b,mmlu_formal_logic,0-shot,accuracy,0.4603174603174603,0.04458029125470973
AbacusResearch/jaLLAbi2-7b,mmlu_prehistory,0-shot,accuracy,0.7283950617283951,0.02474862449053737
AbacusResearch/jaLLAbi2-7b,mmlu_moral_scenarios,0-shot,accuracy,0.4581005586592179,0.01666368329502053
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_world_history,0-shot,accuracy,0.8016877637130801,0.02595502084162111
AbacusResearch/jaLLAbi2-7b,mmlu_moral_disputes,0-shot,accuracy,0.7514450867052023,0.023267528432100174
AbacusResearch/jaLLAbi2-7b,mmlu_professional_law,0-shot,accuracy,0.4680573663624511,0.012744149704869647
AbacusResearch/jaLLAbi2-7b,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.033220157957767414
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_us_history,0-shot,accuracy,0.8529411764705882,0.024857478080250482
AbacusResearch/jaLLAbi2-7b,mmlu_philosophy,0-shot,accuracy,0.7266881028938906,0.02531176597542612
AbacusResearch/jaLLAbi2-7b,mmlu_jurisprudence,0-shot,accuracy,0.7962962962962963,0.03893542518824847
AbacusResearch/jaLLAbi2-7b,mmlu_international_law,0-shot,accuracy,0.7851239669421488,0.03749492448709696
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_european_history,0-shot,accuracy,0.7757575757575758,0.03256866661681102
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8860103626943006,0.022935144053919426
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.6722689075630253,0.03048991141767323
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_geography,0-shot,accuracy,0.8232323232323232,0.027178752639044915
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_psychology,0-shot,accuracy,0.8403669724770643,0.015703498348461752
AbacusResearch/jaLLAbi2-7b,mmlu_public_relations,0-shot,accuracy,0.6727272727272727,0.0449429086625209
AbacusResearch/jaLLAbi2-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.86,0.03487350880197769
AbacusResearch/jaLLAbi2-7b,mmlu_sociology,0-shot,accuracy,0.835820895522388,0.02619392354445414
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.658974358974359,0.024035489676335082
AbacusResearch/jaLLAbi2-7b,mmlu_security_studies,0-shot,accuracy,0.746938775510204,0.0278330238713997
AbacusResearch/jaLLAbi2-7b,mmlu_professional_psychology,0-shot,accuracy,0.6699346405228758,0.019023726160724553
AbacusResearch/jaLLAbi2-7b,mmlu_human_sexuality,0-shot,accuracy,0.7938931297709924,0.03547771004159463
AbacusResearch/jaLLAbi2-7b,mmlu_econometrics,0-shot,accuracy,0.49122807017543857,0.04702880432049615
AbacusResearch/jaLLAbi2-7b,mmlu_miscellaneous,0-shot,accuracy,0.8250319284802043,0.013586619219903348
AbacusResearch/jaLLAbi2-7b,mmlu_marketing,0-shot,accuracy,0.8803418803418803,0.021262719400406957
AbacusResearch/jaLLAbi2-7b,mmlu_management,0-shot,accuracy,0.7766990291262136,0.04123553189891431
AbacusResearch/jaLLAbi2-7b,mmlu_nutrition,0-shot,accuracy,0.7222222222222222,0.02564686309713791
AbacusResearch/jaLLAbi2-7b,mmlu_medical_genetics,0-shot,accuracy,0.72,0.045126085985421276
AbacusResearch/jaLLAbi2-7b,mmlu_human_aging,0-shot,accuracy,0.6995515695067265,0.03076935200822914
AbacusResearch/jaLLAbi2-7b,mmlu_professional_medicine,0-shot,accuracy,0.6801470588235294,0.028332959514031236
AbacusResearch/jaLLAbi2-7b,mmlu_college_medicine,0-shot,accuracy,0.6647398843930635,0.03599586301247077
AbacusResearch/jaLLAbi2-7b,mmlu_business_ethics,0-shot,accuracy,0.65,0.047937248544110196
AbacusResearch/jaLLAbi2-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.7132075471698113,0.02783491252754407
AbacusResearch/jaLLAbi2-7b,mmlu_global_facts,0-shot,accuracy,0.34,0.04760952285695236
AbacusResearch/jaLLAbi2-7b,mmlu_virology,0-shot,accuracy,0.5301204819277109,0.03885425420866766
AbacusResearch/jaLLAbi2-7b,mmlu_professional_accounting,0-shot,accuracy,0.5070921985815603,0.02982449855912901
AbacusResearch/jaLLAbi2-7b,mmlu_college_physics,0-shot,accuracy,0.39215686274509803,0.048580835742663434
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_physics,0-shot,accuracy,0.37748344370860926,0.0395802723112157
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_biology,0-shot,accuracy,0.7935483870967742,0.023025899617188723
AbacusResearch/jaLLAbi2-7b,mmlu_college_biology,0-shot,accuracy,0.7708333333333334,0.035146974678623884
AbacusResearch/jaLLAbi2-7b,mmlu_anatomy,0-shot,accuracy,0.6518518518518519,0.041153246103369526
AbacusResearch/jaLLAbi2-7b,mmlu_college_chemistry,0-shot,accuracy,0.47,0.05016135580465919
AbacusResearch/jaLLAbi2-7b,mmlu_computer_security,0-shot,accuracy,0.77,0.04229525846816505
AbacusResearch/jaLLAbi2-7b,mmlu_college_computer_science,0-shot,accuracy,0.52,0.050211673156867795
AbacusResearch/jaLLAbi2-7b,mmlu_astronomy,0-shot,accuracy,0.7039473684210527,0.03715062154998905
AbacusResearch/jaLLAbi2-7b,mmlu_college_mathematics,0-shot,accuracy,0.29,0.045604802157206845
AbacusResearch/jaLLAbi2-7b,mmlu_conceptual_physics,0-shot,accuracy,0.5787234042553191,0.03227834510146268
AbacusResearch/jaLLAbi2-7b,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.04824181513244218
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.7,0.046056618647183814
AbacusResearch/jaLLAbi2-7b,mmlu_machine_learning,0-shot,accuracy,0.4375,0.04708567521880525
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.5123152709359606,0.035169204442208966
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_statistics,0-shot,accuracy,0.49537037037037035,0.03409825519163572
AbacusResearch/jaLLAbi2-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.40476190476190477,0.0252798503974049
AbacusResearch/jaLLAbi2-7b,mmlu_electrical_engineering,0-shot,accuracy,0.5586206896551724,0.04137931034482757
AbacusResearch/jaLLAbi2-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.35185185185185186,0.02911661760608301
AbacusResearch/jaLLAbi2-7b,arc_challenge,25-shot,accuracy,0.6919795221843004,0.01349142951729204
AbacusResearch/jaLLAbi2-7b,arc_challenge,25-shot,acc_norm,0.7226962457337884,0.013082095839059374
AbacusResearch/jaLLAbi2-7b,hellaswag,10-shot,accuracy,0.7108145787691695,0.0045245758929529185
AbacusResearch/jaLLAbi2-7b,hellaswag,10-shot,acc_norm,0.8833897629954193,0.0032029933469910625
AbacusResearch/jaLLAbi2-7b,truthfulqa_mc2,0-shot,accuracy,0.7074488603697261,0.01469831169732992
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,bleu_max,16.994584405212713,0.5978346623164609
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,bleu_acc,0.5214198286413708,0.01748743214471164
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,bleu_diff,2.5161099258317203,0.4812096593307796
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge1_max,43.149301424681205,0.7451779721995008
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge1_acc,0.5495716034271726,0.01741726437196764
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge1_diff,4.598718031567308,0.7629498840038131
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge2_max,28.509782535112226,0.8264201912092511
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge2_acc,0.4724602203182375,0.017476930190712187
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rouge2_diff,4.175284034929332,0.8296229945452628
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rougeL_max,39.81725800184248,0.7558047743596359
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rougeL_acc,0.5483476132190942,0.017421480300277643
AbacusResearch/jaLLAbi2-7b,truthfulqa_gen,0-shot,rougeL_diff,4.509539289026502,0.764155345941694
AbacusResearch/jaLLAbi2-7b,truthfulqa_mc1,0-shot,accuracy,0.5520195838433293,0.01740851306342291
AbacusResearch/jaLLAbi2-7b,winogrande,5-shot,accuracy,0.835043409629045,0.01043091746823742
EleutherAI/pythia-70m,drop,3-shot,accuracy,0.004404362416107382,0.0006781451620479578
EleutherAI/pythia-70m,drop,3-shot,f1,0.03329383389261746,0.001229822313283723
EleutherAI/pythia-70m,gsm8k,5-shot,accuracy,0.003032600454890068,0.0015145735612245575
EleutherAI/pythia-70m,winogrande,5-shot,accuracy,0.5146014206787688,0.014046492383275839
EleutherAI/pythia-70m,arc:challenge,25-shot,accuracy,0.18088737201365188,0.011248574467407027
EleutherAI/pythia-70m,arc:challenge,25-shot,acc_norm,0.2158703071672355,0.012022975360030674
EleutherAI/pythia-70m,hellaswag,10-shot,accuracy,0.2658832901812388,0.0044089948686501
EleutherAI/pythia-70m,hellaswag,10-shot,acc_norm,0.27285401314479185,0.004445160997618377
EleutherAI/pythia-70m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.042295258468165065
EleutherAI/pythia-70m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.042295258468165065
EleutherAI/pythia-70m,hendrycksTest-anatomy,5-shot,accuracy,0.2740740740740741,0.03853254836552003
EleutherAI/pythia-70m,hendrycksTest-anatomy,5-shot,acc_norm,0.2740740740740741,0.03853254836552003
EleutherAI/pythia-70m,hendrycksTest-astronomy,5-shot,accuracy,0.23684210526315788,0.034597776068105386
EleutherAI/pythia-70m,hendrycksTest-astronomy,5-shot,acc_norm,0.23684210526315788,0.034597776068105386
EleutherAI/pythia-70m,hendrycksTest-business_ethics,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-70m,hendrycksTest-business_ethics,5-shot,acc_norm,0.25,0.04351941398892446
EleutherAI/pythia-70m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.24150943396226415,0.02634148037111836
EleutherAI/pythia-70m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.24150943396226415,0.02634148037111836
EleutherAI/pythia-70m,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.03476590104304134
EleutherAI/pythia-70m,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.03476590104304134
EleutherAI/pythia-70m,hendrycksTest-college_chemistry,5-shot,accuracy,0.4,0.04923659639173309
EleutherAI/pythia-70m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.4,0.04923659639173309
EleutherAI/pythia-70m,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.04408440022768079
EleutherAI/pythia-70m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.04408440022768079
EleutherAI/pythia-70m,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-70m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-70m,hendrycksTest-college_medicine,5-shot,accuracy,0.2947976878612717,0.03476599607516478
EleutherAI/pythia-70m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2947976878612717,0.03476599607516478
EleutherAI/pythia-70m,hendrycksTest-college_physics,5-shot,accuracy,0.28431372549019607,0.04488482852329017
EleutherAI/pythia-70m,hendrycksTest-college_physics,5-shot,acc_norm,0.28431372549019607,0.04488482852329017
EleutherAI/pythia-70m,hendrycksTest-computer_security,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-70m,hendrycksTest-computer_security,5-shot,acc_norm,0.25,0.04351941398892446
EleutherAI/pythia-70m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.028957342788342343
EleutherAI/pythia-70m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.028957342788342343
EleutherAI/pythia-70m,hendrycksTest-econometrics,5-shot,accuracy,0.20175438596491227,0.037752050135836386
EleutherAI/pythia-70m,hendrycksTest-econometrics,5-shot,acc_norm,0.20175438596491227,0.037752050135836386
EleutherAI/pythia-70m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2689655172413793,0.036951833116502325
EleutherAI/pythia-70m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2689655172413793,0.036951833116502325
EleutherAI/pythia-70m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.022494510767503154
EleutherAI/pythia-70m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.022494510767503154
EleutherAI/pythia-70m,hendrycksTest-formal_logic,5-shot,accuracy,0.16666666666666666,0.03333333333333336
EleutherAI/pythia-70m,hendrycksTest-formal_logic,5-shot,acc_norm,0.16666666666666666,0.03333333333333336
EleutherAI/pythia-70m,hendrycksTest-global_facts,5-shot,accuracy,0.14,0.034873508801977704
EleutherAI/pythia-70m,hendrycksTest-global_facts,5-shot,acc_norm,0.14,0.034873508801977704
EleutherAI/pythia-70m,hendrycksTest-high_school_biology,5-shot,accuracy,0.267741935483871,0.025189006660212378
EleutherAI/pythia-70m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.267741935483871,0.025189006660212378
EleutherAI/pythia-70m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.28078817733990147,0.0316185633535861
EleutherAI/pythia-70m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.28078817733990147,0.0316185633535861
EleutherAI/pythia-70m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-70m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/pythia-70m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.26666666666666666,0.03453131801885415
EleutherAI/pythia-70m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.26666666666666666,0.03453131801885415
EleutherAI/pythia-70m,hendrycksTest-high_school_geography,5-shot,accuracy,0.31313131313131315,0.033042050878136525
EleutherAI/pythia-70m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.31313131313131315,0.033042050878136525
EleutherAI/pythia-70m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3471502590673575,0.034356961683613546
EleutherAI/pythia-70m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3471502590673575,0.034356961683613546
EleutherAI/pythia-70m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.33076923076923076,0.02385479568097113
EleutherAI/pythia-70m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.33076923076923076,0.02385479568097113
EleutherAI/pythia-70m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2851851851851852,0.027528599210340492
EleutherAI/pythia-70m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2851851851851852,0.027528599210340492
EleutherAI/pythia-70m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.02880139219363128
EleutherAI/pythia-70m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.02880139219363128
EleutherAI/pythia-70m,hendrycksTest-high_school_physics,5-shot,accuracy,0.2913907284768212,0.03710185726119995
EleutherAI/pythia-70m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2913907284768212,0.03710185726119995
EleutherAI/pythia-70m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.25871559633027524,0.01877605231961962
EleutherAI/pythia-70m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.25871559633027524,0.01877605231961962
EleutherAI/pythia-70m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4675925925925926,0.03402801581358966
EleutherAI/pythia-70m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4675925925925926,0.03402801581358966
EleutherAI/pythia-70m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.23529411764705882,0.02977177522814562
EleutherAI/pythia-70m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.23529411764705882,0.02977177522814562
EleutherAI/pythia-70m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25738396624472576,0.028458820991460295
EleutherAI/pythia-70m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25738396624472576,0.028458820991460295
EleutherAI/pythia-70m,hendrycksTest-human_aging,5-shot,accuracy,0.273542600896861,0.029918586707798837
EleutherAI/pythia-70m,hendrycksTest-human_aging,5-shot,acc_norm,0.273542600896861,0.029918586707798837
EleutherAI/pythia-70m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2748091603053435,0.03915345408847836
EleutherAI/pythia-70m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2748091603053435,0.03915345408847836
EleutherAI/pythia-70m,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.03896878985070417
EleutherAI/pythia-70m,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.03896878985070417
EleutherAI/pythia-70m,hendrycksTest-jurisprudence,5-shot,accuracy,0.23148148148148148,0.04077494709252627
EleutherAI/pythia-70m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.23148148148148148,0.04077494709252627
EleutherAI/pythia-70m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26993865030674846,0.034878251684978906
EleutherAI/pythia-70m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26993865030674846,0.034878251684978906
EleutherAI/pythia-70m,hendrycksTest-machine_learning,5-shot,accuracy,0.21428571428571427,0.038946411200447915
EleutherAI/pythia-70m,hendrycksTest-machine_learning,5-shot,acc_norm,0.21428571428571427,0.038946411200447915
EleutherAI/pythia-70m,hendrycksTest-management,5-shot,accuracy,0.18446601941747573,0.03840423627288276
EleutherAI/pythia-70m,hendrycksTest-management,5-shot,acc_norm,0.18446601941747573,0.03840423627288276
EleutherAI/pythia-70m,hendrycksTest-marketing,5-shot,accuracy,0.19658119658119658,0.02603538609895129
EleutherAI/pythia-70m,hendrycksTest-marketing,5-shot,acc_norm,0.19658119658119658,0.02603538609895129
EleutherAI/pythia-70m,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-70m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/pythia-70m,hendrycksTest-miscellaneous,5-shot,accuracy,0.21711366538952745,0.014743125394823276
EleutherAI/pythia-70m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.21711366538952745,0.014743125394823276
EleutherAI/pythia-70m,hendrycksTest-moral_disputes,5-shot,accuracy,0.24566473988439305,0.02317629820399201
EleutherAI/pythia-70m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24566473988439305,0.02317629820399201
EleutherAI/pythia-70m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2324022346368715,0.01412596875467341
EleutherAI/pythia-70m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2324022346368715,0.01412596875467341
EleutherAI/pythia-70m,hendrycksTest-nutrition,5-shot,accuracy,0.24836601307189543,0.024739981355113592
EleutherAI/pythia-70m,hendrycksTest-nutrition,5-shot,acc_norm,0.24836601307189543,0.024739981355113592
EleutherAI/pythia-70m,hendrycksTest-philosophy,5-shot,accuracy,0.21864951768488747,0.0234755814178611
EleutherAI/pythia-70m,hendrycksTest-philosophy,5-shot,acc_norm,0.21864951768488747,0.0234755814178611
EleutherAI/pythia-70m,hendrycksTest-prehistory,5-shot,accuracy,0.24074074074074073,0.02378858355165854
EleutherAI/pythia-70m,hendrycksTest-prehistory,5-shot,acc_norm,0.24074074074074073,0.02378858355165854
EleutherAI/pythia-70m,hendrycksTest-professional_accounting,5-shot,accuracy,0.21631205673758866,0.024561720560562793
EleutherAI/pythia-70m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.21631205673758866,0.024561720560562793
EleutherAI/pythia-70m,hendrycksTest-professional_law,5-shot,accuracy,0.23468057366362452,0.010824026872449335
EleutherAI/pythia-70m,hendrycksTest-professional_law,5-shot,acc_norm,0.23468057366362452,0.010824026872449335
EleutherAI/pythia-70m,hendrycksTest-professional_medicine,5-shot,accuracy,0.34191176470588236,0.02881472242225418
EleutherAI/pythia-70m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.34191176470588236,0.02881472242225418
EleutherAI/pythia-70m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2173202614379085,0.016684820929148605
EleutherAI/pythia-70m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2173202614379085,0.016684820929148605
EleutherAI/pythia-70m,hendrycksTest-public_relations,5-shot,accuracy,0.2636363636363636,0.04220224692971987
EleutherAI/pythia-70m,hendrycksTest-public_relations,5-shot,acc_norm,0.2636363636363636,0.04220224692971987
EleutherAI/pythia-70m,hendrycksTest-security_studies,5-shot,accuracy,0.27346938775510204,0.02853556033712845
EleutherAI/pythia-70m,hendrycksTest-security_studies,5-shot,acc_norm,0.27346938775510204,0.02853556033712845
EleutherAI/pythia-70m,hendrycksTest-sociology,5-shot,accuracy,0.27860696517412936,0.031700561834973086
EleutherAI/pythia-70m,hendrycksTest-sociology,5-shot,acc_norm,0.27860696517412936,0.031700561834973086
EleutherAI/pythia-70m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.22,0.041633319989322695
EleutherAI/pythia-70m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.22,0.041633319989322695
EleutherAI/pythia-70m,hendrycksTest-virology,5-shot,accuracy,0.24096385542168675,0.033293941190735296
EleutherAI/pythia-70m,hendrycksTest-virology,5-shot,acc_norm,0.24096385542168675,0.033293941190735296
EleutherAI/pythia-70m,hendrycksTest-world_religions,5-shot,accuracy,0.2222222222222222,0.03188578017686398
EleutherAI/pythia-70m,hendrycksTest-world_religions,5-shot,acc_norm,0.2222222222222222,0.03188578017686398
EleutherAI/pythia-70m,truthfulqa:mc,0-shot,mc1,0.23745410036719705,0.014896277441041836
EleutherAI/pythia-70m,truthfulqa:mc,0-shot,mc2,0.47064644683796225,0.015503754703352232
meta-llama/Llama-2-70b-chat-hf,drop,3-shot,accuracy,0.040373322147651006,0.0020157564185176837
meta-llama/Llama-2-70b-chat-hf,drop,3-shot,f1,0.1050272651006715,0.0023756238577676155
meta-llama/Llama-2-70b-chat-hf,gsm8k,5-shot,accuracy,0.266868840030326,0.012183780551887957
meta-llama/Llama-2-70b-chat-hf,winogrande,5-shot,accuracy,0.8050513022888713,0.011134099415938268
huggyllama/llama-7b,arc:challenge,25-shot,accuracy,0.47696245733788395,0.014595873205358267
huggyllama/llama-7b,arc:challenge,25-shot,acc_norm,0.5093856655290102,0.014608816322065
huggyllama/llama-7b,hellaswag,10-shot,accuracy,0.5753833897629954,0.004932745013072713
huggyllama/llama-7b,hellaswag,10-shot,acc_norm,0.7781318462457678,0.004146537488135709
huggyllama/llama-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.26,0.04408440022768081
huggyllama/llama-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.26,0.04408440022768081
huggyllama/llama-7b,hendrycksTest-anatomy,5-shot,accuracy,0.3851851851851852,0.042039210401562783
huggyllama/llama-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.3851851851851852,0.042039210401562783
huggyllama/llama-7b,hendrycksTest-astronomy,5-shot,accuracy,0.34210526315789475,0.03860731599316092
huggyllama/llama-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.34210526315789475,0.03860731599316092
huggyllama/llama-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.41,0.049431107042371025
huggyllama/llama-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.41,0.049431107042371025
huggyllama/llama-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.35471698113207545,0.02944517532819959
huggyllama/llama-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.35471698113207545,0.02944517532819959
huggyllama/llama-7b,hendrycksTest-college_biology,5-shot,accuracy,0.375,0.04048439222695598
huggyllama/llama-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.375,0.04048439222695598
huggyllama/llama-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.3,0.046056618647183814
huggyllama/llama-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.3,0.046056618647183814
huggyllama/llama-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.31,0.04648231987117316
huggyllama/llama-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.31,0.04648231987117316
huggyllama/llama-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.33,0.04725815626252604
huggyllama/llama-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.33,0.04725815626252604
huggyllama/llama-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.3352601156069364,0.03599586301247078
huggyllama/llama-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.3352601156069364,0.03599586301247078
huggyllama/llama-7b,hendrycksTest-college_physics,5-shot,accuracy,0.23529411764705882,0.04220773659171451
huggyllama/llama-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.23529411764705882,0.04220773659171451
huggyllama/llama-7b,hendrycksTest-computer_security,5-shot,accuracy,0.45,0.05
huggyllama/llama-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.45,0.05
huggyllama/llama-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3702127659574468,0.03156564682236785
huggyllama/llama-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3702127659574468,0.03156564682236785
huggyllama/llama-7b,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.04142439719489362
huggyllama/llama-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.04142439719489362
huggyllama/llama-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.22758620689655173,0.03493950380131184
huggyllama/llama-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.22758620689655173,0.03493950380131184
huggyllama/llama-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2619047619047619,0.022644212615525214
huggyllama/llama-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2619047619047619,0.022644212615525214
huggyllama/llama-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.2619047619047619,0.03932537680392871
huggyllama/llama-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2619047619047619,0.03932537680392871
huggyllama/llama-7b,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
huggyllama/llama-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
huggyllama/llama-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.33225806451612905,0.0267955608481228
huggyllama/llama-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.33225806451612905,0.0267955608481228
huggyllama/llama-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.30049261083743845,0.03225799476233485
huggyllama/llama-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.30049261083743845,0.03225799476233485
huggyllama/llama-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.04725815626252605
huggyllama/llama-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.04725815626252605
huggyllama/llama-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.43636363636363634,0.03872592983524754
huggyllama/llama-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.43636363636363634,0.03872592983524754
huggyllama/llama-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.3333333333333333,0.03358618145732522
huggyllama/llama-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3333333333333333,0.03358618145732522
huggyllama/llama-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.44559585492227977,0.0358701498607566
huggyllama/llama-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.44559585492227977,0.0358701498607566
huggyllama/llama-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3435897435897436,0.024078696580635477
huggyllama/llama-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3435897435897436,0.024078696580635477
huggyllama/llama-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.026719240783712173
huggyllama/llama-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25925925925925924,0.026719240783712173
huggyllama/llama-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.33613445378151263,0.030684737115135363
huggyllama/llama-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.33613445378151263,0.030684737115135363
huggyllama/llama-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.26490066225165565,0.036030385453603854
huggyllama/llama-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.26490066225165565,0.036030385453603854
huggyllama/llama-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.47339449541284406,0.02140695268815159
huggyllama/llama-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.47339449541284406,0.02140695268815159
huggyllama/llama-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3055555555555556,0.031415546294025445
huggyllama/llama-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3055555555555556,0.031415546294025445
huggyllama/llama-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.35784313725490197,0.03364487286088299
huggyllama/llama-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.35784313725490197,0.03364487286088299
huggyllama/llama-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.43037974683544306,0.03223017195937598
huggyllama/llama-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.43037974683544306,0.03223017195937598
huggyllama/llama-7b,hendrycksTest-human_aging,5-shot,accuracy,0.39461883408071746,0.03280400504755291
huggyllama/llama-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.39461883408071746,0.03280400504755291
huggyllama/llama-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.3511450381679389,0.0418644516301375
huggyllama/llama-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3511450381679389,0.0418644516301375
huggyllama/llama-7b,hendrycksTest-international_law,5-shot,accuracy,0.5206611570247934,0.04560456086387235
huggyllama/llama-7b,hendrycksTest-international_law,5-shot,acc_norm,0.5206611570247934,0.04560456086387235
huggyllama/llama-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.4166666666666667,0.04766075165356461
huggyllama/llama-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.4166666666666667,0.04766075165356461
huggyllama/llama-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.4294478527607362,0.038890666191127216
huggyllama/llama-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.4294478527607362,0.038890666191127216
huggyllama/llama-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.2767857142857143,0.042466243366976256
huggyllama/llama-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.2767857142857143,0.042466243366976256
huggyllama/llama-7b,hendrycksTest-management,5-shot,accuracy,0.3300970873786408,0.046561471100123514
huggyllama/llama-7b,hendrycksTest-management,5-shot,acc_norm,0.3300970873786408,0.046561471100123514
huggyllama/llama-7b,hendrycksTest-marketing,5-shot,accuracy,0.47863247863247865,0.03272616447634954
huggyllama/llama-7b,hendrycksTest-marketing,5-shot,acc_norm,0.47863247863247865,0.03272616447634954
huggyllama/llama-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.37,0.048523658709391
huggyllama/llama-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.37,0.048523658709391
huggyllama/llama-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.42528735632183906,0.017679225489431447
huggyllama/llama-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.42528735632183906,0.017679225489431447
huggyllama/llama-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.3959537572254335,0.02632981334194625
huggyllama/llama-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3959537572254335,0.02632981334194625
huggyllama/llama-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
huggyllama/llama-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
huggyllama/llama-7b,hendrycksTest-nutrition,5-shot,accuracy,0.3888888888888889,0.02791405551046801
huggyllama/llama-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.3888888888888889,0.02791405551046801
huggyllama/llama-7b,hendrycksTest-philosophy,5-shot,accuracy,0.3954983922829582,0.027770918531427834
huggyllama/llama-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.3954983922829582,0.027770918531427834
huggyllama/llama-7b,hendrycksTest-prehistory,5-shot,accuracy,0.345679012345679,0.026462487777001883
huggyllama/llama-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.345679012345679,0.026462487777001883
huggyllama/llama-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2695035460992908,0.026469036818590624
huggyllama/llama-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2695035460992908,0.026469036818590624
huggyllama/llama-7b,hendrycksTest-professional_law,5-shot,accuracy,0.29595827900912647,0.011658518525277054
huggyllama/llama-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.29595827900912647,0.011658518525277054
huggyllama/llama-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.4411764705882353,0.030161911930767102
huggyllama/llama-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4411764705882353,0.030161911930767102
huggyllama/llama-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.35784313725490197,0.019393058402355442
huggyllama/llama-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.35784313725490197,0.019393058402355442
huggyllama/llama-7b,hendrycksTest-public_relations,5-shot,accuracy,0.41818181818181815,0.0472457740573157
huggyllama/llama-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.41818181818181815,0.0472457740573157
huggyllama/llama-7b,hendrycksTest-security_studies,5-shot,accuracy,0.34285714285714286,0.030387262919547728
huggyllama/llama-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.34285714285714286,0.030387262919547728
huggyllama/llama-7b,hendrycksTest-sociology,5-shot,accuracy,0.46766169154228854,0.035281314729336065
huggyllama/llama-7b,hendrycksTest-sociology,5-shot,acc_norm,0.46766169154228854,0.035281314729336065
huggyllama/llama-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.43,0.049756985195624284
huggyllama/llama-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.43,0.049756985195624284
huggyllama/llama-7b,hendrycksTest-virology,5-shot,accuracy,0.3433734939759036,0.03696584317010601
huggyllama/llama-7b,hendrycksTest-virology,5-shot,acc_norm,0.3433734939759036,0.03696584317010601
huggyllama/llama-7b,hendrycksTest-world_religions,5-shot,accuracy,0.4853801169590643,0.038331852752130205
huggyllama/llama-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.4853801169590643,0.038331852752130205
huggyllama/llama-7b,truthfulqa:mc,0-shot,mc1,0.2215422276621787,0.014537867601301137
huggyllama/llama-7b,truthfulqa:mc,0-shot,mc2,0.3432793294414406,0.01318846106276968
huggyllama/llama-7b,winogrande,5-shot,accuracy,0.7142857142857143,0.012696531870038616
huggyllama/llama-7b,gsm8k,5-shot,accuracy,0.08718726307808947,0.007770691416783579
huggyllama/llama-7b,minerva_math_precalc,5-shot,accuracy,0.023809523809523808,0.006530469219761487
huggyllama/llama-7b,minerva_math_prealgebra,5-shot,accuracy,0.05855338691159587,0.007960021356233738
huggyllama/llama-7b,minerva_math_num_theory,5-shot,accuracy,0.016666666666666666,0.005514172815089617
huggyllama/llama-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.02768549280177187,0.005462937642242674
huggyllama/llama-7b,minerva_math_geometry,5-shot,accuracy,0.03549060542797495,0.008462447487439958
huggyllama/llama-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.03375527426160337,0.008303932653126971
huggyllama/llama-7b,minerva_math_algebra,5-shot,accuracy,0.02948609941027801,0.0049120999853740025
huggyllama/llama-7b,fld_default,0-shot,accuracy,0.0,
huggyllama/llama-7b,fld_star,0-shot,accuracy,0.0,
huggyllama/llama-7b,arithmetic_3da,5-shot,accuracy,0.671,0.010508792383460743
huggyllama/llama-7b,arithmetic_3ds,5-shot,accuracy,0.4205,0.011040870681821411
huggyllama/llama-7b,arithmetic_4da,5-shot,accuracy,0.3535,0.010692335480100337
huggyllama/llama-7b,arithmetic_2ds,5-shot,accuracy,0.4915,0.011181519941139164
huggyllama/llama-7b,arithmetic_5ds,5-shot,accuracy,0.1135,0.007094648829999192
huggyllama/llama-7b,arithmetic_5da,5-shot,accuracy,0.105,0.0068564572122015
huggyllama/llama-7b,arithmetic_1dc,5-shot,accuracy,0.17,0.008401505379771036
huggyllama/llama-7b,arithmetic_4ds,5-shot,accuracy,0.3045,0.01029285414368695
huggyllama/llama-7b,arithmetic_2dm,5-shot,accuracy,0.1865,0.008711878294128388
huggyllama/llama-7b,arithmetic_2da,5-shot,accuracy,0.7645,0.009490253287121995
huggyllama/llama-7b,gsm8k_cot,5-shot,accuracy,0.10993176648976498,0.008616195587865418
huggyllama/llama-7b,anli_r2,0-shot,brier_score,0.7348861281471742,
huggyllama/llama-7b,anli_r3,0-shot,brier_score,0.7095518208768032,
huggyllama/llama-7b,anli_r1,0-shot,brier_score,0.7461478989427204,
huggyllama/llama-7b,xnli_eu,0-shot,brier_score,1.0296807787278208,
huggyllama/llama-7b,xnli_vi,0-shot,brier_score,0.9782764908278533,
huggyllama/llama-7b,xnli_ru,0-shot,brier_score,0.8046437633798106,
huggyllama/llama-7b,xnli_zh,0-shot,brier_score,0.9663395791429601,
huggyllama/llama-7b,xnli_tr,0-shot,brier_score,0.8179342178015766,
huggyllama/llama-7b,xnli_fr,0-shot,brier_score,0.7545951761984023,
huggyllama/llama-7b,xnli_en,0-shot,brier_score,0.6399015782286974,
huggyllama/llama-7b,xnli_ur,0-shot,brier_score,1.18515651443942,
huggyllama/llama-7b,xnli_ar,0-shot,brier_score,1.2101674335547863,
huggyllama/llama-7b,xnli_de,0-shot,brier_score,0.7906547361490429,
huggyllama/llama-7b,xnli_hi,0-shot,brier_score,0.8598989915709084,
huggyllama/llama-7b,xnli_es,0-shot,brier_score,0.8477390395566538,
huggyllama/llama-7b,xnli_bg,0-shot,brier_score,0.8810810022273962,
huggyllama/llama-7b,xnli_sw,0-shot,brier_score,1.0933436482571928,
huggyllama/llama-7b,xnli_el,0-shot,brier_score,0.8910593589743593,
huggyllama/llama-7b,xnli_th,0-shot,brier_score,1.0236076918742028,
huggyllama/llama-7b,logiqa2,0-shot,brier_score,0.9998570403139664,
huggyllama/llama-7b,mathqa,5-shot,brier_score,0.9133227108274022,
huggyllama/llama-7b,lambada_standard,0-shot,perplexity,4.405370129844651,0.08983307496604127
huggyllama/llama-7b,lambada_standard,0-shot,accuracy,0.6780516204152921,0.0065093342627460075
huggyllama/llama-7b,lambada_openai,0-shot,perplexity,3.488115332748822,0.06835244562396407
huggyllama/llama-7b,lambada_openai,0-shot,accuracy,0.7351057636328352,0.006147849695828234
froggeric/WestLake-10.7B-v2,minerva_math_precalc,5-shot,accuracy,0.038461538461538464,0.008237556478425377
froggeric/WestLake-10.7B-v2,minerva_math_prealgebra,5-shot,accuracy,0.27898966704936856,0.015205656567297112
froggeric/WestLake-10.7B-v2,minerva_math_num_theory,5-shot,accuracy,0.08148148148148149,0.011783628281121663
froggeric/WestLake-10.7B-v2,minerva_math_intermediate_algebra,5-shot,accuracy,0.03765227021040975,0.006338088917730661
froggeric/WestLake-10.7B-v2,minerva_math_geometry,5-shot,accuracy,0.081419624217119,0.012508613685688272
froggeric/WestLake-10.7B-v2,minerva_math_counting_and_prob,5-shot,accuracy,0.0970464135021097,0.013611058115372832
froggeric/WestLake-10.7B-v2,minerva_math_algebra,5-shot,accuracy,0.1979780960404381,0.011570692228998754
froggeric/WestLake-10.7B-v2,fld_default,0-shot,accuracy,0.0002,0.000199999999999999
froggeric/WestLake-10.7B-v2,fld_star,0-shot,accuracy,0.0002,0.00019999999999999963
froggeric/WestLake-10.7B-v2,arithmetic_3da,5-shot,accuracy,0.954,0.004685400355171849
froggeric/WestLake-10.7B-v2,arithmetic_3ds,5-shot,accuracy,0.975,0.0034919331033682467
froggeric/WestLake-10.7B-v2,arithmetic_4da,5-shot,accuracy,0.8725,0.007459872643009688
froggeric/WestLake-10.7B-v2,arithmetic_2ds,5-shot,accuracy,0.99,0.0022254159696827483
froggeric/WestLake-10.7B-v2,arithmetic_5ds,5-shot,accuracy,0.8575,0.007818403847292685
froggeric/WestLake-10.7B-v2,arithmetic_5da,5-shot,accuracy,0.81,0.008774308761784195
froggeric/WestLake-10.7B-v2,arithmetic_1dc,5-shot,accuracy,0.6025,0.010945628277499658
froggeric/WestLake-10.7B-v2,arithmetic_4ds,5-shot,accuracy,0.92,0.006067817499282812
froggeric/WestLake-10.7B-v2,arithmetic_2dm,5-shot,accuracy,0.5035,0.011182862030875628
froggeric/WestLake-10.7B-v2,arithmetic_2da,5-shot,accuracy,0.982,0.0029736208922129122
froggeric/WestLake-10.7B-v2,gsm8k_cot,5-shot,accuracy,0.6550416982562547,0.013093630133666224
froggeric/WestLake-10.7B-v2,gsm8k,5-shot,accuracy,0.6209249431387415,0.013363630295088347
froggeric/WestLake-10.7B-v2,anli_r2,0-shot,brier_score,0.8472615638756087,
froggeric/WestLake-10.7B-v2,anli_r3,0-shot,brier_score,0.7853073888110543,
froggeric/WestLake-10.7B-v2,anli_r1,0-shot,brier_score,0.6792771671674599,
froggeric/WestLake-10.7B-v2,xnli_eu,0-shot,brier_score,1.065526750088975,
froggeric/WestLake-10.7B-v2,xnli_vi,0-shot,brier_score,1.0439012064803812,
froggeric/WestLake-10.7B-v2,xnli_ru,0-shot,brier_score,0.9928432729414078,
froggeric/WestLake-10.7B-v2,xnli_zh,0-shot,brier_score,1.1708548798400407,
froggeric/WestLake-10.7B-v2,xnli_tr,0-shot,brier_score,1.0415841664185104,
froggeric/WestLake-10.7B-v2,xnli_fr,0-shot,brier_score,1.0902927110654848,
froggeric/WestLake-10.7B-v2,xnli_en,0-shot,brier_score,0.7999870468682789,
froggeric/WestLake-10.7B-v2,xnli_ur,0-shot,brier_score,1.203193103469732,
froggeric/WestLake-10.7B-v2,xnli_ar,0-shot,brier_score,1.218269698711459,
froggeric/WestLake-10.7B-v2,xnli_de,0-shot,brier_score,0.9566540615741678,
froggeric/WestLake-10.7B-v2,xnli_hi,0-shot,brier_score,0.9931017430900577,
froggeric/WestLake-10.7B-v2,xnli_es,0-shot,brier_score,1.0462014404303681,
froggeric/WestLake-10.7B-v2,xnli_bg,0-shot,brier_score,0.9533770617239662,
froggeric/WestLake-10.7B-v2,xnli_sw,0-shot,brier_score,1.039675312596033,
froggeric/WestLake-10.7B-v2,xnli_el,0-shot,brier_score,0.9991586293399103,
froggeric/WestLake-10.7B-v2,xnli_th,0-shot,brier_score,1.2343337415593545,
froggeric/WestLake-10.7B-v2,logiqa2,0-shot,brier_score,0.9135478490056801,
froggeric/WestLake-10.7B-v2,mathqa,5-shot,brier_score,1.0613930870257773,
froggeric/WestLake-10.7B-v2,lambada_standard,0-shot,perplexity,7.47557563678433,0.24808836554648117
froggeric/WestLake-10.7B-v2,lambada_standard,0-shot,accuracy,0.5486124587618862,0.006932975888368619
froggeric/WestLake-10.7B-v2,lambada_openai,0-shot,perplexity,5.457072232577165,0.16173449273656423
froggeric/WestLake-10.7B-v2,lambada_openai,0-shot,accuracy,0.6037259848631865,0.006814434238262821
froggeric/WestLake-10.7B-v2,mmlu_world_religions,0-shot,accuracy,0.8070175438596491,0.030267457554898465
froggeric/WestLake-10.7B-v2,mmlu_formal_logic,0-shot,accuracy,0.4523809523809524,0.044518079590553275
froggeric/WestLake-10.7B-v2,mmlu_prehistory,0-shot,accuracy,0.7376543209876543,0.024477222856135104
froggeric/WestLake-10.7B-v2,mmlu_moral_scenarios,0-shot,accuracy,0.33519553072625696,0.015788007190185888
froggeric/WestLake-10.7B-v2,mmlu_high_school_world_history,0-shot,accuracy,0.810126582278481,0.025530100460233497
froggeric/WestLake-10.7B-v2,mmlu_moral_disputes,0-shot,accuracy,0.7052023121387283,0.024547617794803838
froggeric/WestLake-10.7B-v2,mmlu_professional_law,0-shot,accuracy,0.47979139504563234,0.012759801427767562
froggeric/WestLake-10.7B-v2,mmlu_logical_fallacies,0-shot,accuracy,0.7484662576687117,0.034089978868575295
froggeric/WestLake-10.7B-v2,mmlu_high_school_us_history,0-shot,accuracy,0.7941176470588235,0.028379449451588667
froggeric/WestLake-10.7B-v2,mmlu_philosophy,0-shot,accuracy,0.684887459807074,0.026385273703464496
froggeric/WestLake-10.7B-v2,mmlu_jurisprudence,0-shot,accuracy,0.8055555555555556,0.03826076324884864
froggeric/WestLake-10.7B-v2,mmlu_international_law,0-shot,accuracy,0.7768595041322314,0.03800754475228733
froggeric/WestLake-10.7B-v2,mmlu_high_school_european_history,0-shot,accuracy,0.7575757575757576,0.03346409881055953
froggeric/WestLake-10.7B-v2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8911917098445595,0.022473253332768763
froggeric/WestLake-10.7B-v2,mmlu_high_school_microeconomics,0-shot,accuracy,0.6554621848739496,0.030868682604121626
froggeric/WestLake-10.7B-v2,mmlu_high_school_geography,0-shot,accuracy,0.7575757575757576,0.030532892233932026
froggeric/WestLake-10.7B-v2,mmlu_high_school_psychology,0-shot,accuracy,0.8348623853211009,0.015919557829976037
froggeric/WestLake-10.7B-v2,mmlu_public_relations,0-shot,accuracy,0.6636363636363637,0.04525393596302506
froggeric/WestLake-10.7B-v2,mmlu_us_foreign_policy,0-shot,accuracy,0.86,0.03487350880197769
froggeric/WestLake-10.7B-v2,mmlu_sociology,0-shot,accuracy,0.8407960199004975,0.02587064676616914
froggeric/WestLake-10.7B-v2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6384615384615384,0.024359581465396987
froggeric/WestLake-10.7B-v2,mmlu_security_studies,0-shot,accuracy,0.710204081632653,0.02904308868330434
froggeric/WestLake-10.7B-v2,mmlu_professional_psychology,0-shot,accuracy,0.6928104575163399,0.01866335967146366
froggeric/WestLake-10.7B-v2,mmlu_human_sexuality,0-shot,accuracy,0.732824427480916,0.03880848301082396
froggeric/WestLake-10.7B-v2,mmlu_econometrics,0-shot,accuracy,0.5087719298245614,0.04702880432049615
froggeric/WestLake-10.7B-v2,mmlu_miscellaneous,0-shot,accuracy,0.8186462324393359,0.013778693778464102
froggeric/WestLake-10.7B-v2,mmlu_marketing,0-shot,accuracy,0.8717948717948718,0.02190190511507333
froggeric/WestLake-10.7B-v2,mmlu_management,0-shot,accuracy,0.7572815533980582,0.04245022486384495
froggeric/WestLake-10.7B-v2,mmlu_nutrition,0-shot,accuracy,0.6830065359477124,0.02664327847450875
froggeric/WestLake-10.7B-v2,mmlu_medical_genetics,0-shot,accuracy,0.77,0.04229525846816505
froggeric/WestLake-10.7B-v2,mmlu_human_aging,0-shot,accuracy,0.7309417040358744,0.029763779406874975
froggeric/WestLake-10.7B-v2,mmlu_professional_medicine,0-shot,accuracy,0.6580882352941176,0.028814722422254174
froggeric/WestLake-10.7B-v2,mmlu_college_medicine,0-shot,accuracy,0.6416184971098265,0.036563436533531606
froggeric/WestLake-10.7B-v2,mmlu_business_ethics,0-shot,accuracy,0.67,0.04725815626252609
froggeric/WestLake-10.7B-v2,mmlu_clinical_knowledge,0-shot,accuracy,0.6641509433962264,0.029067220146644826
froggeric/WestLake-10.7B-v2,mmlu_global_facts,0-shot,accuracy,0.4,0.04923659639173309
froggeric/WestLake-10.7B-v2,mmlu_virology,0-shot,accuracy,0.5301204819277109,0.03885425420866768
froggeric/WestLake-10.7B-v2,mmlu_professional_accounting,0-shot,accuracy,0.475177304964539,0.029790719243829714
froggeric/WestLake-10.7B-v2,mmlu_college_physics,0-shot,accuracy,0.37254901960784315,0.04810840148082633
froggeric/WestLake-10.7B-v2,mmlu_high_school_physics,0-shot,accuracy,0.40397350993377484,0.040064856853653415
froggeric/WestLake-10.7B-v2,mmlu_high_school_biology,0-shot,accuracy,0.7677419354838709,0.024022256130308235
froggeric/WestLake-10.7B-v2,mmlu_college_biology,0-shot,accuracy,0.7222222222222222,0.037455547914624555
froggeric/WestLake-10.7B-v2,mmlu_anatomy,0-shot,accuracy,0.562962962962963,0.04284958639753401
froggeric/WestLake-10.7B-v2,mmlu_college_chemistry,0-shot,accuracy,0.43,0.04975698519562429
froggeric/WestLake-10.7B-v2,mmlu_computer_security,0-shot,accuracy,0.73,0.044619604333847394
froggeric/WestLake-10.7B-v2,mmlu_college_computer_science,0-shot,accuracy,0.57,0.049756985195624284
froggeric/WestLake-10.7B-v2,mmlu_astronomy,0-shot,accuracy,0.7039473684210527,0.037150621549989056
froggeric/WestLake-10.7B-v2,mmlu_college_mathematics,0-shot,accuracy,0.28,0.04512608598542128
froggeric/WestLake-10.7B-v2,mmlu_conceptual_physics,0-shot,accuracy,0.574468085106383,0.03232146916224468
froggeric/WestLake-10.7B-v2,mmlu_abstract_algebra,0-shot,accuracy,0.34,0.04760952285695235
froggeric/WestLake-10.7B-v2,mmlu_high_school_computer_science,0-shot,accuracy,0.69,0.04648231987117316
froggeric/WestLake-10.7B-v2,mmlu_machine_learning,0-shot,accuracy,0.5,0.04745789978762494
froggeric/WestLake-10.7B-v2,mmlu_high_school_chemistry,0-shot,accuracy,0.5073891625615764,0.035176035403610105
froggeric/WestLake-10.7B-v2,mmlu_high_school_statistics,0-shot,accuracy,0.5185185185185185,0.03407632093854051
froggeric/WestLake-10.7B-v2,mmlu_elementary_mathematics,0-shot,accuracy,0.4312169312169312,0.025506481698138204
froggeric/WestLake-10.7B-v2,mmlu_electrical_engineering,0-shot,accuracy,0.5655172413793104,0.041307408795554966
froggeric/WestLake-10.7B-v2,mmlu_high_school_mathematics,0-shot,accuracy,0.3333333333333333,0.028742040903948482
froggeric/WestLake-10.7B-v2,arc_challenge,25-shot,accuracy,0.6672354948805461,0.013769863046192307
froggeric/WestLake-10.7B-v2,arc_challenge,25-shot,acc_norm,0.6953924914675768,0.01344952210993249
froggeric/WestLake-10.7B-v2,hellaswag,10-shot,accuracy,0.7130053774148576,0.004514345547780335
froggeric/WestLake-10.7B-v2,hellaswag,10-shot,acc_norm,0.8808006373232424,0.0032336074238900206
froggeric/WestLake-10.7B-v2,truthfulqa_mc2,0-shot,accuracy,0.6502221311834233,0.015728866880290463
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,bleu_max,10.118772746100687,0.4033244046848448
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,bleu_acc,0.4602203182374541,0.017448017223960884
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,bleu_diff,0.23678007300279724,0.25435457707661413
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge1_max,32.93967882301825,0.6250936418056136
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge1_acc,0.4883720930232558,0.017498767175740088
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge1_diff,0.3499431820705031,0.43627397939161733
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge2_max,18.885790230324695,0.621447224079988
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge2_acc,0.38310893512851896,0.017018461679389862
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rouge2_diff,-0.4377782614406173,0.4605252038625926
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rougeL_max,28.92110479821841,0.6163965826709437
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rougeL_acc,0.46266829865361075,0.017454645150970588
froggeric/WestLake-10.7B-v2,truthfulqa_gen,0-shot,rougeL_diff,-0.21060189672916219,0.4195858986205983
froggeric/WestLake-10.7B-v2,truthfulqa_mc1,0-shot,accuracy,0.5116279069767442,0.017498767175740084
froggeric/WestLake-10.7B-v2,winogrande,5-shot,accuracy,0.8634569850039463,0.009650242900291605
IEITYuan/Yuan2-2B-hf,minerva_math_precalc,5-shot,accuracy,0.01282051282051282,0.004818950982487626
IEITYuan/Yuan2-2B-hf,minerva_math_prealgebra,5-shot,accuracy,0.06199770378874857,0.008175797512062155
IEITYuan/Yuan2-2B-hf,minerva_math_num_theory,5-shot,accuracy,0.014814814814814815,0.005203704987512651
IEITYuan/Yuan2-2B-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.004429678848283499,0.0022111531423787936
IEITYuan/Yuan2-2B-hf,minerva_math_geometry,5-shot,accuracy,0.029227557411273485,0.007704439205471374
IEITYuan/Yuan2-2B-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.014767932489451477,0.005546238589668469
IEITYuan/Yuan2-2B-hf,minerva_math_algebra,5-shot,accuracy,0.03369839932603201,0.0052398474232848045
IEITYuan/Yuan2-2B-hf,fld_default,0-shot,accuracy,0.0,
IEITYuan/Yuan2-2B-hf,fld_star,0-shot,accuracy,0.0,
IEITYuan/Yuan2-2B-hf,arithmetic_3da,5-shot,accuracy,0.3295,0.010512855704685485
IEITYuan/Yuan2-2B-hf,arithmetic_3ds,5-shot,accuracy,0.124,0.007371510671822562
IEITYuan/Yuan2-2B-hf,arithmetic_4da,5-shot,accuracy,0.112,0.007053571892184738
IEITYuan/Yuan2-2B-hf,arithmetic_2ds,5-shot,accuracy,0.231,0.00942676678219962
IEITYuan/Yuan2-2B-hf,arithmetic_5ds,5-shot,accuracy,0.006,0.0017272787111155172
IEITYuan/Yuan2-2B-hf,arithmetic_5da,5-shot,accuracy,0.0495,0.004851457855290592
IEITYuan/Yuan2-2B-hf,arithmetic_1dc,5-shot,accuracy,0.303,0.010278537063322019
IEITYuan/Yuan2-2B-hf,arithmetic_4ds,5-shot,accuracy,0.039,0.004329997048176563
IEITYuan/Yuan2-2B-hf,arithmetic_2dm,5-shot,accuracy,0.0695,0.005687798389997827
IEITYuan/Yuan2-2B-hf,arithmetic_2da,5-shot,accuracy,0.5045,0.011182683094883916
IEITYuan/Yuan2-2B-hf,gsm8k_cot,5-shot,accuracy,0.09249431387414708,0.007980396874560166
IEITYuan/Yuan2-2B-hf,gsm8k,5-shot,accuracy,0.09249431387414708,0.007980396874560166
IEITYuan/Yuan2-2B-hf,anli_r2,0-shot,brier_score,0.9021877709220847,
IEITYuan/Yuan2-2B-hf,anli_r3,0-shot,brier_score,0.8960203323957571,
IEITYuan/Yuan2-2B-hf,anli_r1,0-shot,brier_score,0.8980224868789101,
IEITYuan/Yuan2-2B-hf,xnli_eu,0-shot,brier_score,1.1726851242898388,
IEITYuan/Yuan2-2B-hf,xnli_vi,0-shot,brier_score,1.0551222712344248,
IEITYuan/Yuan2-2B-hf,xnli_ru,0-shot,brier_score,1.0838363628503949,
IEITYuan/Yuan2-2B-hf,xnli_zh,0-shot,brier_score,1.0730192701838772,
IEITYuan/Yuan2-2B-hf,xnli_tr,0-shot,brier_score,1.1096395679638529,
IEITYuan/Yuan2-2B-hf,xnli_fr,0-shot,brier_score,1.0979677062130364,
IEITYuan/Yuan2-2B-hf,xnli_en,0-shot,brier_score,0.7874104438895939,
IEITYuan/Yuan2-2B-hf,xnli_ur,0-shot,brier_score,1.3108842358000463,
IEITYuan/Yuan2-2B-hf,xnli_ar,0-shot,brier_score,1.0708782622925244,
IEITYuan/Yuan2-2B-hf,xnli_de,0-shot,brier_score,0.9569650860704157,
IEITYuan/Yuan2-2B-hf,xnli_hi,0-shot,brier_score,1.2300842352985888,
IEITYuan/Yuan2-2B-hf,xnli_es,0-shot,brier_score,1.045692200792037,
IEITYuan/Yuan2-2B-hf,xnli_bg,0-shot,brier_score,1.1627810243907892,
IEITYuan/Yuan2-2B-hf,xnli_sw,0-shot,brier_score,1.088251408596032,
IEITYuan/Yuan2-2B-hf,xnli_el,0-shot,brier_score,1.1639414398316317,
IEITYuan/Yuan2-2B-hf,xnli_th,0-shot,brier_score,1.3070235732345081,
IEITYuan/Yuan2-2B-hf,logiqa2,0-shot,brier_score,1.2490949064547225,
IEITYuan/Yuan2-2B-hf,mathqa,5-shot,brier_score,0.990379580332815,
IEITYuan/Yuan2-2B-hf,lambada_standard,0-shot,perplexity,523.2726641428518,28.91907158439682
IEITYuan/Yuan2-2B-hf,lambada_standard,0-shot,accuracy,0.1622355909179119,0.005136249280239084
IEITYuan/Yuan2-2B-hf,lambada_openai,0-shot,perplexity,75.6433032154668,3.790131948633793
IEITYuan/Yuan2-2B-hf,lambada_openai,0-shot,accuracy,0.29283912284106345,0.006339948563069486
IEITYuan/Yuan2-2B-hf,mmlu_world_religions,0-shot,accuracy,0.21052631578947367,0.031267817146631786
IEITYuan/Yuan2-2B-hf,mmlu_formal_logic,0-shot,accuracy,0.20634920634920634,0.036196045241242536
IEITYuan/Yuan2-2B-hf,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.024748624490537382
IEITYuan/Yuan2-2B-hf,mmlu_moral_scenarios,0-shot,accuracy,0.2659217877094972,0.014776765066438892
IEITYuan/Yuan2-2B-hf,mmlu_high_school_world_history,0-shot,accuracy,0.26582278481012656,0.02875679962965834
IEITYuan/Yuan2-2B-hf,mmlu_moral_disputes,0-shot,accuracy,0.27167630057803466,0.023948512905468348
IEITYuan/Yuan2-2B-hf,mmlu_professional_law,0-shot,accuracy,0.2666232073011734,0.01129383603161214
IEITYuan/Yuan2-2B-hf,mmlu_logical_fallacies,0-shot,accuracy,0.2147239263803681,0.03226219377286774
IEITYuan/Yuan2-2B-hf,mmlu_high_school_us_history,0-shot,accuracy,0.21568627450980393,0.028867431449849303
IEITYuan/Yuan2-2B-hf,mmlu_philosophy,0-shot,accuracy,0.2540192926045016,0.024723861504771693
IEITYuan/Yuan2-2B-hf,mmlu_jurisprudence,0-shot,accuracy,0.23148148148148148,0.04077494709252627
IEITYuan/Yuan2-2B-hf,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.03941897526516302
IEITYuan/Yuan2-2B-hf,mmlu_high_school_european_history,0-shot,accuracy,0.24848484848484848,0.03374402644139405
IEITYuan/Yuan2-2B-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2849740932642487,0.032577140777096614
IEITYuan/Yuan2-2B-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.2184873949579832,0.026841514322958934
IEITYuan/Yuan2-2B-hf,mmlu_high_school_geography,0-shot,accuracy,0.22727272727272727,0.029857515673386396
IEITYuan/Yuan2-2B-hf,mmlu_high_school_psychology,0-shot,accuracy,0.21651376146788992,0.017658710594443138
IEITYuan/Yuan2-2B-hf,mmlu_public_relations,0-shot,accuracy,0.24545454545454545,0.04122066502878284
IEITYuan/Yuan2-2B-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.32,0.04688261722621505
IEITYuan/Yuan2-2B-hf,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296014
IEITYuan/Yuan2-2B-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2230769230769231,0.021107730127244
IEITYuan/Yuan2-2B-hf,mmlu_security_studies,0-shot,accuracy,0.22040816326530613,0.026537045312145298
IEITYuan/Yuan2-2B-hf,mmlu_professional_psychology,0-shot,accuracy,0.26633986928104575,0.0178831881346672
IEITYuan/Yuan2-2B-hf,mmlu_human_sexuality,0-shot,accuracy,0.22900763358778625,0.036853466317118506
IEITYuan/Yuan2-2B-hf,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.04142439719489362
IEITYuan/Yuan2-2B-hf,mmlu_miscellaneous,0-shot,accuracy,0.2796934865900383,0.016050792148036546
IEITYuan/Yuan2-2B-hf,mmlu_marketing,0-shot,accuracy,0.23504273504273504,0.027778835904935437
IEITYuan/Yuan2-2B-hf,mmlu_management,0-shot,accuracy,0.2621359223300971,0.04354631077260595
IEITYuan/Yuan2-2B-hf,mmlu_nutrition,0-shot,accuracy,0.238562091503268,0.02440439492808787
IEITYuan/Yuan2-2B-hf,mmlu_medical_genetics,0-shot,accuracy,0.23,0.04229525846816506
IEITYuan/Yuan2-2B-hf,mmlu_human_aging,0-shot,accuracy,0.2914798206278027,0.030500283176545906
IEITYuan/Yuan2-2B-hf,mmlu_professional_medicine,0-shot,accuracy,0.18382352941176472,0.023529242185193106
IEITYuan/Yuan2-2B-hf,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
IEITYuan/Yuan2-2B-hf,mmlu_business_ethics,0-shot,accuracy,0.25,0.04351941398892446
IEITYuan/Yuan2-2B-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.29056603773584905,0.02794321998933716
IEITYuan/Yuan2-2B-hf,mmlu_global_facts,0-shot,accuracy,0.33,0.04725815626252605
IEITYuan/Yuan2-2B-hf,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.03610805018031023
IEITYuan/Yuan2-2B-hf,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539005,0.025892151156709405
IEITYuan/Yuan2-2B-hf,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.041583075330832865
IEITYuan/Yuan2-2B-hf,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.03297986648473835
IEITYuan/Yuan2-2B-hf,mmlu_high_school_biology,0-shot,accuracy,0.2,0.02275520495954294
IEITYuan/Yuan2-2B-hf,mmlu_college_biology,0-shot,accuracy,0.19444444444444445,0.03309615177059006
IEITYuan/Yuan2-2B-hf,mmlu_anatomy,0-shot,accuracy,0.23703703703703705,0.03673731683969506
IEITYuan/Yuan2-2B-hf,mmlu_college_chemistry,0-shot,accuracy,0.22,0.04163331998932269
IEITYuan/Yuan2-2B-hf,mmlu_computer_security,0-shot,accuracy,0.29,0.045604802157206845
IEITYuan/Yuan2-2B-hf,mmlu_college_computer_science,0-shot,accuracy,0.15,0.0358870281282637
IEITYuan/Yuan2-2B-hf,mmlu_astronomy,0-shot,accuracy,0.18421052631578946,0.031546980450822305
IEITYuan/Yuan2-2B-hf,mmlu_college_mathematics,0-shot,accuracy,0.25,0.04351941398892446
IEITYuan/Yuan2-2B-hf,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.030472973363380052
IEITYuan/Yuan2-2B-hf,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.041633319989322695
IEITYuan/Yuan2-2B-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
IEITYuan/Yuan2-2B-hf,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.04287858751340456
IEITYuan/Yuan2-2B-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.2413793103448276,0.030108330718011625
IEITYuan/Yuan2-2B-hf,mmlu_high_school_statistics,0-shot,accuracy,0.33796296296296297,0.032259413526312945
IEITYuan/Yuan2-2B-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.25396825396825395,0.022418042891113946
IEITYuan/Yuan2-2B-hf,mmlu_electrical_engineering,0-shot,accuracy,0.30344827586206896,0.038312260488503336
IEITYuan/Yuan2-2B-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.24074074074074073,0.026067159222275788
IEITYuan/Yuan2-2B-hf,arc_challenge,25-shot,accuracy,0.19880546075085323,0.01166285019817553
IEITYuan/Yuan2-2B-hf,arc_challenge,25-shot,acc_norm,0.23976109215017063,0.012476304127453947
IEITYuan/Yuan2-2B-hf,hellaswag,10-shot,accuracy,0.30541724756024696,0.004596426220000867
IEITYuan/Yuan2-2B-hf,hellaswag,10-shot,acc_norm,0.3333001394144593,0.004704293898729931
IEITYuan/Yuan2-2B-hf,truthfulqa_mc2,0-shot,accuracy,0.4250838081980288,0.01582787673797006
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,bleu_max,15.197501509988685,0.544225214068991
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,bleu_acc,0.3047735618115055,0.016114124156882417
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,bleu_diff,-2.903206350824163,0.44799349927215737
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge1_max,40.224440846013266,0.7259309767822962
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge1_acc,0.31211750305997554,0.01622075676952096
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge1_diff,-4.514331396454585,0.635813668954008
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge2_max,23.95451031471122,0.7969484153233573
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge2_acc,0.24969400244798043,0.015152286907148125
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rouge2_diff,-5.21174195263994,0.7068689979410017
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rougeL_max,36.89784769319415,0.7270599149793284
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rougeL_acc,0.30966952264381886,0.016185744355144936
IEITYuan/Yuan2-2B-hf,truthfulqa_gen,0-shot,rougeL_diff,-4.171471272218275,0.6150306439924023
IEITYuan/Yuan2-2B-hf,truthfulqa_mc1,0-shot,accuracy,0.25458996328029376,0.015250117079156507
IEITYuan/Yuan2-2B-hf,winogrande,5-shot,accuracy,0.5074980268350434,0.01405090552122858
openlm-research/open_llama_3b_v2,arc:challenge,25-shot,accuracy,0.35580204778157,0.013990571137918762
openlm-research/open_llama_3b_v2,arc:challenge,25-shot,acc_norm,0.40273037542662116,0.014332236306790145
openlm-research/open_llama_3b_v2,hellaswag,10-shot,accuracy,0.5309699263095001,0.0049802004518516765
openlm-research/open_llama_3b_v2,hellaswag,10-shot,acc_norm,0.7155945030870344,0.00450208828747007
openlm-research/open_llama_3b_v2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.044619604333847415
openlm-research/open_llama_3b_v2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.044619604333847415
openlm-research/open_llama_3b_v2,hendrycksTest-anatomy,5-shot,accuracy,0.23703703703703705,0.03673731683969506
openlm-research/open_llama_3b_v2,hendrycksTest-anatomy,5-shot,acc_norm,0.23703703703703705,0.03673731683969506
openlm-research/open_llama_3b_v2,hendrycksTest-astronomy,5-shot,accuracy,0.27631578947368424,0.03639057569952924
openlm-research/open_llama_3b_v2,hendrycksTest-astronomy,5-shot,acc_norm,0.27631578947368424,0.03639057569952924
openlm-research/open_llama_3b_v2,hendrycksTest-business_ethics,5-shot,accuracy,0.4,0.04923659639173309
openlm-research/open_llama_3b_v2,hendrycksTest-business_ethics,5-shot,acc_norm,0.4,0.04923659639173309
openlm-research/open_llama_3b_v2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.027257260322494845
openlm-research/open_llama_3b_v2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.027257260322494845
openlm-research/open_llama_3b_v2,hendrycksTest-college_biology,5-shot,accuracy,0.24305555555555555,0.0358687928008034
openlm-research/open_llama_3b_v2,hendrycksTest-college_biology,5-shot,acc_norm,0.24305555555555555,0.0358687928008034
openlm-research/open_llama_3b_v2,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909284
openlm-research/open_llama_3b_v2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909284
openlm-research/open_llama_3b_v2,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.0446196043338474
openlm-research/open_llama_3b_v2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.0446196043338474
openlm-research/open_llama_3b_v2,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
openlm-research/open_llama_3b_v2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
openlm-research/open_llama_3b_v2,hendrycksTest-college_medicine,5-shot,accuracy,0.23699421965317918,0.03242414757483098
openlm-research/open_llama_3b_v2,hendrycksTest-college_medicine,5-shot,acc_norm,0.23699421965317918,0.03242414757483098
openlm-research/open_llama_3b_v2,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617749
openlm-research/open_llama_3b_v2,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617749
openlm-research/open_llama_3b_v2,hendrycksTest-computer_security,5-shot,accuracy,0.32,0.04688261722621505
openlm-research/open_llama_3b_v2,hendrycksTest-computer_security,5-shot,acc_norm,0.32,0.04688261722621505
openlm-research/open_llama_3b_v2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.32340425531914896,0.030579442773610337
openlm-research/open_llama_3b_v2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.32340425531914896,0.030579442773610337
openlm-research/open_llama_3b_v2,hendrycksTest-econometrics,5-shot,accuracy,0.24561403508771928,0.04049339297748143
openlm-research/open_llama_3b_v2,hendrycksTest-econometrics,5-shot,acc_norm,0.24561403508771928,0.04049339297748143
openlm-research/open_llama_3b_v2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.20689655172413793,0.03375672449560554
openlm-research/open_llama_3b_v2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.20689655172413793,0.03375672449560554
openlm-research/open_llama_3b_v2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.291005291005291,0.023393826500484875
openlm-research/open_llama_3b_v2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.291005291005291,0.023393826500484875
openlm-research/open_llama_3b_v2,hendrycksTest-formal_logic,5-shot,accuracy,0.24603174603174602,0.03852273364924318
openlm-research/open_llama_3b_v2,hendrycksTest-formal_logic,5-shot,acc_norm,0.24603174603174602,0.03852273364924318
openlm-research/open_llama_3b_v2,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
openlm-research/open_llama_3b_v2,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_biology,5-shot,accuracy,0.23548387096774193,0.024137632429337707
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.23548387096774193,0.024137632429337707
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.21674876847290642,0.028990331252516235
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.21674876847290642,0.028990331252516235
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.042923469599092816
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.042923469599092816
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.24848484848484848,0.03374402644139404
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.24848484848484848,0.03374402644139404
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_geography,5-shot,accuracy,0.20202020202020202,0.02860620428922988
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.20202020202020202,0.02860620428922988
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.23834196891191708,0.03074890536390988
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.23834196891191708,0.03074890536390988
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2564102564102564,0.02213908110397155
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2564102564102564,0.02213908110397155
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.23333333333333334,0.025787874220959316
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.23333333333333334,0.025787874220959316
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2773109243697479,0.02907937453948001
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2773109243697479,0.02907937453948001
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_physics,5-shot,accuracy,0.33112582781456956,0.038425817186598696
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.33112582781456956,0.038425817186598696
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.24770642201834864,0.018508143602547815
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.24770642201834864,0.018508143602547815
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.20833333333333334,0.02769691071309394
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.20833333333333334,0.02769691071309394
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.23529411764705882,0.029771775228145638
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.23529411764705882,0.029771775228145638
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25316455696202533,0.028304657943035293
openlm-research/open_llama_3b_v2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25316455696202533,0.028304657943035293
openlm-research/open_llama_3b_v2,hendrycksTest-human_aging,5-shot,accuracy,0.3991031390134529,0.03286745312567961
openlm-research/open_llama_3b_v2,hendrycksTest-human_aging,5-shot,acc_norm,0.3991031390134529,0.03286745312567961
openlm-research/open_llama_3b_v2,hendrycksTest-human_sexuality,5-shot,accuracy,0.25190839694656486,0.03807387116306086
openlm-research/open_llama_3b_v2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.25190839694656486,0.03807387116306086
openlm-research/open_llama_3b_v2,hendrycksTest-international_law,5-shot,accuracy,0.2975206611570248,0.04173349148083498
openlm-research/open_llama_3b_v2,hendrycksTest-international_law,5-shot,acc_norm,0.2975206611570248,0.04173349148083498
openlm-research/open_llama_3b_v2,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.044143436668549335
openlm-research/open_llama_3b_v2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.044143436668549335
openlm-research/open_llama_3b_v2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2147239263803681,0.03226219377286774
openlm-research/open_llama_3b_v2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2147239263803681,0.03226219377286774
openlm-research/open_llama_3b_v2,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.043270409325787296
openlm-research/open_llama_3b_v2,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.043270409325787296
openlm-research/open_llama_3b_v2,hendrycksTest-management,5-shot,accuracy,0.27184466019417475,0.044052680241409216
openlm-research/open_llama_3b_v2,hendrycksTest-management,5-shot,acc_norm,0.27184466019417475,0.044052680241409216
openlm-research/open_llama_3b_v2,hendrycksTest-marketing,5-shot,accuracy,0.2863247863247863,0.029614323690456648
openlm-research/open_llama_3b_v2,hendrycksTest-marketing,5-shot,acc_norm,0.2863247863247863,0.029614323690456648
openlm-research/open_llama_3b_v2,hendrycksTest-medical_genetics,5-shot,accuracy,0.23,0.042295258468165044
openlm-research/open_llama_3b_v2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.23,0.042295258468165044
openlm-research/open_llama_3b_v2,hendrycksTest-miscellaneous,5-shot,accuracy,0.280970625798212,0.016073127851221246
openlm-research/open_llama_3b_v2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.280970625798212,0.016073127851221246
openlm-research/open_llama_3b_v2,hendrycksTest-moral_disputes,5-shot,accuracy,0.25722543352601157,0.023532925431044287
openlm-research/open_llama_3b_v2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.25722543352601157,0.023532925431044287
openlm-research/open_llama_3b_v2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24022346368715083,0.014288343803925312
openlm-research/open_llama_3b_v2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24022346368715083,0.014288343803925312
openlm-research/open_llama_3b_v2,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.025261691219729494
openlm-research/open_llama_3b_v2,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.025261691219729494
openlm-research/open_llama_3b_v2,hendrycksTest-philosophy,5-shot,accuracy,0.2765273311897106,0.025403832978179625
openlm-research/open_llama_3b_v2,hendrycksTest-philosophy,5-shot,acc_norm,0.2765273311897106,0.025403832978179625
openlm-research/open_llama_3b_v2,hendrycksTest-prehistory,5-shot,accuracy,0.2993827160493827,0.025483115601195462
openlm-research/open_llama_3b_v2,hendrycksTest-prehistory,5-shot,acc_norm,0.2993827160493827,0.025483115601195462
openlm-research/open_llama_3b_v2,hendrycksTest-professional_accounting,5-shot,accuracy,0.3120567375886525,0.02764012054516993
openlm-research/open_llama_3b_v2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3120567375886525,0.02764012054516993
openlm-research/open_llama_3b_v2,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.01099615663514269
openlm-research/open_llama_3b_v2,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.01099615663514269
openlm-research/open_llama_3b_v2,hendrycksTest-professional_medicine,5-shot,accuracy,0.21323529411764705,0.024880971512294275
openlm-research/open_llama_3b_v2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.21323529411764705,0.024880971512294275
openlm-research/open_llama_3b_v2,hendrycksTest-professional_psychology,5-shot,accuracy,0.2647058823529412,0.017848089574913226
openlm-research/open_llama_3b_v2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2647058823529412,0.017848089574913226
openlm-research/open_llama_3b_v2,hendrycksTest-public_relations,5-shot,accuracy,0.32727272727272727,0.04494290866252088
openlm-research/open_llama_3b_v2,hendrycksTest-public_relations,5-shot,acc_norm,0.32727272727272727,0.04494290866252088
openlm-research/open_llama_3b_v2,hendrycksTest-security_studies,5-shot,accuracy,0.3346938775510204,0.030209235226242307
openlm-research/open_llama_3b_v2,hendrycksTest-security_studies,5-shot,acc_norm,0.3346938775510204,0.030209235226242307
openlm-research/open_llama_3b_v2,hendrycksTest-sociology,5-shot,accuracy,0.27860696517412936,0.031700561834973086
openlm-research/open_llama_3b_v2,hendrycksTest-sociology,5-shot,acc_norm,0.27860696517412936,0.031700561834973086
openlm-research/open_llama_3b_v2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.36,0.048241815132442176
openlm-research/open_llama_3b_v2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.36,0.048241815132442176
openlm-research/open_llama_3b_v2,hendrycksTest-virology,5-shot,accuracy,0.3373493975903614,0.03680783690727581
openlm-research/open_llama_3b_v2,hendrycksTest-virology,5-shot,acc_norm,0.3373493975903614,0.03680783690727581
openlm-research/open_llama_3b_v2,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.03508771929824563
openlm-research/open_llama_3b_v2,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.03508771929824563
openlm-research/open_llama_3b_v2,truthfulqa:mc,0-shot,mc1,0.2178702570379437,0.014450846714123897
openlm-research/open_llama_3b_v2,truthfulqa:mc,0-shot,mc2,0.3477745324693538,0.013261749316970146
openlm-research/open_llama_3b_v2,drop,3-shot,accuracy,0.001153523489932886,0.0003476179896857095
openlm-research/open_llama_3b_v2,drop,3-shot,f1,0.05134962248322172,0.0012730168443049574
openlm-research/open_llama_3b_v2,gsm8k,5-shot,accuracy,0.039423805913570885,0.005360280030342438
openlm-research/open_llama_3b_v2,winogrande,5-shot,accuracy,0.6677190213101816,0.013238316554236533
EleutherAI/pythia-2.8b-deduped,drop,3-shot,accuracy,0.0012583892617449664,0.00036305608931190916
EleutherAI/pythia-2.8b-deduped,drop,3-shot,f1,0.0446549916107384,0.0011620582208289672
EleutherAI/pythia-2.8b-deduped,gsm8k,5-shot,accuracy,0.008339651250947688,0.002504942226860519
EleutherAI/pythia-2.8b-deduped,winogrande,5-shot,accuracy,0.6022099447513812,0.013755743513749023
EleutherAI/pythia-2.8b-deduped,arc:challenge,25-shot,accuracy,0.32593856655290104,0.013697432466693244
EleutherAI/pythia-2.8b-deduped,arc:challenge,25-shot,acc_norm,0.3626279863481229,0.014049106564955014
EleutherAI/pythia-2.8b-deduped,hellaswag,10-shot,accuracy,0.45160326628161723,0.004966351835028203
EleutherAI/pythia-2.8b-deduped,hellaswag,10-shot,acc_norm,0.6065524795857399,0.004875162699121662
EleutherAI/pythia-2.8b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.044619604333847415
EleutherAI/pythia-2.8b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.044619604333847415
EleutherAI/pythia-2.8b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.2962962962962963,0.03944624162501117
EleutherAI/pythia-2.8b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.2962962962962963,0.03944624162501117
EleutherAI/pythia-2.8b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.20394736842105263,0.03279000406310051
EleutherAI/pythia-2.8b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.20394736842105263,0.03279000406310051
EleutherAI/pythia-2.8b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-2.8b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.04163331998932269
EleutherAI/pythia-2.8b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.027257260322494845
EleutherAI/pythia-2.8b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.027257260322494845
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2361111111111111,0.03551446610810826
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2361111111111111,0.03551446610810826
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.24,0.042923469599092816
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.24,0.042923469599092816
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.23699421965317918,0.03242414757483099
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.23699421965317918,0.03242414757483099
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.14705882352941177,0.035240689515674474
EleutherAI/pythia-2.8b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.14705882352941177,0.035240689515674474
EleutherAI/pythia-2.8b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/pythia-2.8b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.04512608598542128
EleutherAI/pythia-2.8b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.28085106382978725,0.02937917046412483
EleutherAI/pythia-2.8b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.28085106382978725,0.02937917046412483
EleutherAI/pythia-2.8b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.21929824561403508,0.038924311065187546
EleutherAI/pythia-2.8b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.21929824561403508,0.038924311065187546
EleutherAI/pythia-2.8b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.27586206896551724,0.037245636197746325
EleutherAI/pythia-2.8b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.27586206896551724,0.037245636197746325
EleutherAI/pythia-2.8b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.26455026455026454,0.022717467897708614
EleutherAI/pythia-2.8b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.26455026455026454,0.022717467897708614
EleutherAI/pythia-2.8b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.037184890068181146
EleutherAI/pythia-2.8b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.037184890068181146
EleutherAI/pythia-2.8b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526045
EleutherAI/pythia-2.8b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526045
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.24838709677419354,0.024580028921481
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.24838709677419354,0.024580028921481
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2315270935960591,0.02967833314144444
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2315270935960591,0.02967833314144444
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.23,0.04229525846816506
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.23,0.04229525846816506
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.28484848484848485,0.035243908445117836
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.28484848484848485,0.035243908445117836
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2777777777777778,0.03191178226713548
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2777777777777778,0.03191178226713548
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2694300518134715,0.032018671228777947
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2694300518134715,0.032018671228777947
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.32051282051282054,0.023661296393964273
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.32051282051282054,0.023661296393964273
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.02684205787383371
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.02684205787383371
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.24789915966386555,0.028047967224176892
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.24789915966386555,0.028047967224176892
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.31788079470198677,0.038020397601079024
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.31788079470198677,0.038020397601079024
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.23302752293577983,0.018125669180861503
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.23302752293577983,0.018125669180861503
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.44907407407407407,0.03392238405321617
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.44907407407407407,0.03392238405321617
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.21568627450980393,0.028867431449849316
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.21568627450980393,0.028867431449849316
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2320675105485232,0.02747974455080852
EleutherAI/pythia-2.8b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2320675105485232,0.02747974455080852
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.34977578475336324,0.03200736719484503
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.34977578475336324,0.03200736719484503
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.22900763358778625,0.036853466317118506
EleutherAI/pythia-2.8b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.22900763358778625,0.036853466317118506
EleutherAI/pythia-2.8b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.34710743801652894,0.04345724570292534
EleutherAI/pythia-2.8b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.34710743801652894,0.04345724570292534
EleutherAI/pythia-2.8b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.23148148148148148,0.04077494709252627
EleutherAI/pythia-2.8b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.23148148148148148,0.04077494709252627
EleutherAI/pythia-2.8b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26993865030674846,0.034878251684978906
EleutherAI/pythia-2.8b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26993865030674846,0.034878251684978906
EleutherAI/pythia-2.8b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.26785714285714285,0.04203277291467763
EleutherAI/pythia-2.8b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.26785714285714285,0.04203277291467763
EleutherAI/pythia-2.8b-deduped,hendrycksTest-management,5-shot,accuracy,0.23300970873786409,0.041858325989283136
EleutherAI/pythia-2.8b-deduped,hendrycksTest-management,5-shot,acc_norm,0.23300970873786409,0.041858325989283136
EleutherAI/pythia-2.8b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.24358974358974358,0.02812096650391439
EleutherAI/pythia-2.8b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.24358974358974358,0.02812096650391439
EleutherAI/pythia-2.8b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.04292346959909282
EleutherAI/pythia-2.8b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.04292346959909282
EleutherAI/pythia-2.8b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.280970625798212,0.016073127851221246
EleutherAI/pythia-2.8b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.280970625798212,0.016073127851221246
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2861271676300578,0.02433214677913413
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2861271676300578,0.02433214677913413
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.22569832402234638,0.013981395058455054
EleutherAI/pythia-2.8b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.22569832402234638,0.013981395058455054
EleutherAI/pythia-2.8b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.025261691219729494
EleutherAI/pythia-2.8b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.025261691219729494
EleutherAI/pythia-2.8b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2765273311897106,0.02540383297817962
EleutherAI/pythia-2.8b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2765273311897106,0.02540383297817962
EleutherAI/pythia-2.8b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2777777777777778,0.024922001168886338
EleutherAI/pythia-2.8b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2777777777777778,0.024922001168886338
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.02601199293090202
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.02601199293090202
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.258148631029987,0.011176923719313394
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.258148631029987,0.011176923719313394
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.4338235294117647,0.030105636570016647
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4338235294117647,0.030105636570016647
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.27450980392156865,0.018054027458815198
EleutherAI/pythia-2.8b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.27450980392156865,0.018054027458815198
EleutherAI/pythia-2.8b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.35454545454545455,0.04582004841505417
EleutherAI/pythia-2.8b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.35454545454545455,0.04582004841505417
EleutherAI/pythia-2.8b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.19591836734693877,0.02540930195322568
EleutherAI/pythia-2.8b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.19591836734693877,0.02540930195322568
EleutherAI/pythia-2.8b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.03076944496729602
EleutherAI/pythia-2.8b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.03076944496729602
EleutherAI/pythia-2.8b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-2.8b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.2,0.040201512610368445
EleutherAI/pythia-2.8b-deduped,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.035915667978246635
EleutherAI/pythia-2.8b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.035915667978246635
EleutherAI/pythia-2.8b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.03508771929824565
EleutherAI/pythia-2.8b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.03508771929824565
EleutherAI/pythia-2.8b-deduped,truthfulqa:mc,0-shot,mc1,0.2141982864137087,0.014362148155690466
EleutherAI/pythia-2.8b-deduped,truthfulqa:mc,0-shot,mc2,0.3555978234501585,0.01359490561334657
allenai/tulu-2-dpo-70b,arc:challenge,25-shot,accuracy,0.6825938566552902,0.013602239088038167
allenai/tulu-2-dpo-70b,arc:challenge,25-shot,acc_norm,0.7209897610921502,0.01310678488360134
allenai/tulu-2-dpo-70b,hellaswag,10-shot,accuracy,0.7082254530969926,0.004536500714147989
allenai/tulu-2-dpo-70b,hellaswag,10-shot,acc_norm,0.8898625771758614,0.0031242116171988606
allenai/tulu-2-dpo-70b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.046056618647183814
allenai/tulu-2-dpo-70b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.046056618647183814
allenai/tulu-2-dpo-70b,hendrycksTest-anatomy,5-shot,accuracy,0.6148148148148148,0.04203921040156279
allenai/tulu-2-dpo-70b,hendrycksTest-anatomy,5-shot,acc_norm,0.6148148148148148,0.04203921040156279
allenai/tulu-2-dpo-70b,hendrycksTest-astronomy,5-shot,accuracy,0.7828947368421053,0.03355045304882924
allenai/tulu-2-dpo-70b,hendrycksTest-astronomy,5-shot,acc_norm,0.7828947368421053,0.03355045304882924
allenai/tulu-2-dpo-70b,hendrycksTest-business_ethics,5-shot,accuracy,0.73,0.0446196043338474
allenai/tulu-2-dpo-70b,hendrycksTest-business_ethics,5-shot,acc_norm,0.73,0.0446196043338474
allenai/tulu-2-dpo-70b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7509433962264151,0.02661648298050171
allenai/tulu-2-dpo-70b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7509433962264151,0.02661648298050171
allenai/tulu-2-dpo-70b,hendrycksTest-college_biology,5-shot,accuracy,0.8263888888888888,0.03167473383795718
allenai/tulu-2-dpo-70b,hendrycksTest-college_biology,5-shot,acc_norm,0.8263888888888888,0.03167473383795718
allenai/tulu-2-dpo-70b,hendrycksTest-college_chemistry,5-shot,accuracy,0.51,0.05024183937956911
allenai/tulu-2-dpo-70b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.51,0.05024183937956911
allenai/tulu-2-dpo-70b,hendrycksTest-college_computer_science,5-shot,accuracy,0.59,0.04943110704237102
allenai/tulu-2-dpo-70b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.59,0.04943110704237102
allenai/tulu-2-dpo-70b,hendrycksTest-college_mathematics,5-shot,accuracy,0.39,0.049020713000019756
allenai/tulu-2-dpo-70b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.39,0.049020713000019756
allenai/tulu-2-dpo-70b,hendrycksTest-college_medicine,5-shot,accuracy,0.7225433526011561,0.03414014007044037
allenai/tulu-2-dpo-70b,hendrycksTest-college_medicine,5-shot,acc_norm,0.7225433526011561,0.03414014007044037
allenai/tulu-2-dpo-70b,hendrycksTest-college_physics,5-shot,accuracy,0.43137254901960786,0.04928099597287534
allenai/tulu-2-dpo-70b,hendrycksTest-college_physics,5-shot,acc_norm,0.43137254901960786,0.04928099597287534
allenai/tulu-2-dpo-70b,hendrycksTest-computer_security,5-shot,accuracy,0.72,0.045126085985421276
allenai/tulu-2-dpo-70b,hendrycksTest-computer_security,5-shot,acc_norm,0.72,0.045126085985421276
allenai/tulu-2-dpo-70b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6978723404255319,0.03001755447188056
allenai/tulu-2-dpo-70b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6978723404255319,0.03001755447188056
allenai/tulu-2-dpo-70b,hendrycksTest-econometrics,5-shot,accuracy,0.4473684210526316,0.04677473004491199
allenai/tulu-2-dpo-70b,hendrycksTest-econometrics,5-shot,acc_norm,0.4473684210526316,0.04677473004491199
allenai/tulu-2-dpo-70b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5793103448275863,0.04113914981189261
allenai/tulu-2-dpo-70b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5793103448275863,0.04113914981189261
allenai/tulu-2-dpo-70b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.455026455026455,0.025646928361049398
allenai/tulu-2-dpo-70b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.455026455026455,0.025646928361049398
allenai/tulu-2-dpo-70b,hendrycksTest-formal_logic,5-shot,accuracy,0.48412698412698413,0.04469881854072606
allenai/tulu-2-dpo-70b,hendrycksTest-formal_logic,5-shot,acc_norm,0.48412698412698413,0.04469881854072606
allenai/tulu-2-dpo-70b,hendrycksTest-global_facts,5-shot,accuracy,0.52,0.050211673156867795
allenai/tulu-2-dpo-70b,hendrycksTest-global_facts,5-shot,acc_norm,0.52,0.050211673156867795
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_biology,5-shot,accuracy,0.7935483870967742,0.023025899617188716
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7935483870967742,0.023025899617188716
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.03512819077876106
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.03512819077876106
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.78,0.04163331998932261
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.78,0.04163331998932261
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8363636363636363,0.02888787239548795
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8363636363636363,0.02888787239548795
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_geography,5-shot,accuracy,0.8888888888888888,0.02239078763821676
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8888888888888888,0.02239078763821676
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9378238341968912,0.01742697415424052
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9378238341968912,0.01742697415424052
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7333333333333333,0.022421273612923707
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.7333333333333333,0.022421273612923707
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3592592592592593,0.029252905927251976
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3592592592592593,0.029252905927251976
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8025210084033614,0.025859164122051453
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8025210084033614,0.025859164122051453
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_physics,5-shot,accuracy,0.47019867549668876,0.040752249922169775
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.47019867549668876,0.040752249922169775
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8935779816513761,0.013221554674594372
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8935779816513761,0.013221554674594372
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6064814814814815,0.03331747876370312
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6064814814814815,0.03331747876370312
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9166666666666666,0.019398452135813905
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9166666666666666,0.019398452135813905
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8607594936708861,0.022535526352692705
allenai/tulu-2-dpo-70b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8607594936708861,0.022535526352692705
allenai/tulu-2-dpo-70b,hendrycksTest-human_aging,5-shot,accuracy,0.7668161434977578,0.028380391147094713
allenai/tulu-2-dpo-70b,hendrycksTest-human_aging,5-shot,acc_norm,0.7668161434977578,0.028380391147094713
allenai/tulu-2-dpo-70b,hendrycksTest-human_sexuality,5-shot,accuracy,0.8320610687022901,0.03278548537343138
allenai/tulu-2-dpo-70b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8320610687022901,0.03278548537343138
allenai/tulu-2-dpo-70b,hendrycksTest-international_law,5-shot,accuracy,0.8677685950413223,0.0309227883204458
allenai/tulu-2-dpo-70b,hendrycksTest-international_law,5-shot,acc_norm,0.8677685950413223,0.0309227883204458
allenai/tulu-2-dpo-70b,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.035207039905179635
allenai/tulu-2-dpo-70b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.035207039905179635
allenai/tulu-2-dpo-70b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8282208588957055,0.029634717272371037
allenai/tulu-2-dpo-70b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8282208588957055,0.029634717272371037
allenai/tulu-2-dpo-70b,hendrycksTest-machine_learning,5-shot,accuracy,0.5,0.04745789978762494
allenai/tulu-2-dpo-70b,hendrycksTest-machine_learning,5-shot,acc_norm,0.5,0.04745789978762494
allenai/tulu-2-dpo-70b,hendrycksTest-management,5-shot,accuracy,0.8155339805825242,0.03840423627288276
allenai/tulu-2-dpo-70b,hendrycksTest-management,5-shot,acc_norm,0.8155339805825242,0.03840423627288276
allenai/tulu-2-dpo-70b,hendrycksTest-marketing,5-shot,accuracy,0.8974358974358975,0.019875655027867457
allenai/tulu-2-dpo-70b,hendrycksTest-marketing,5-shot,acc_norm,0.8974358974358975,0.019875655027867457
allenai/tulu-2-dpo-70b,hendrycksTest-medical_genetics,5-shot,accuracy,0.69,0.04648231987117316
allenai/tulu-2-dpo-70b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.69,0.04648231987117316
allenai/tulu-2-dpo-70b,hendrycksTest-miscellaneous,5-shot,accuracy,0.8531289910600255,0.012658201736147288
allenai/tulu-2-dpo-70b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8531289910600255,0.012658201736147288
allenai/tulu-2-dpo-70b,hendrycksTest-moral_disputes,5-shot,accuracy,0.7658959537572254,0.022797110278071124
allenai/tulu-2-dpo-70b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7658959537572254,0.022797110278071124
allenai/tulu-2-dpo-70b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.511731843575419,0.016717897676932162
allenai/tulu-2-dpo-70b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.511731843575419,0.016717897676932162
allenai/tulu-2-dpo-70b,hendrycksTest-nutrition,5-shot,accuracy,0.7843137254901961,0.02355083135199509
allenai/tulu-2-dpo-70b,hendrycksTest-nutrition,5-shot,acc_norm,0.7843137254901961,0.02355083135199509
allenai/tulu-2-dpo-70b,hendrycksTest-philosophy,5-shot,accuracy,0.7620578778135049,0.02418515064781871
allenai/tulu-2-dpo-70b,hendrycksTest-philosophy,5-shot,acc_norm,0.7620578778135049,0.02418515064781871
allenai/tulu-2-dpo-70b,hendrycksTest-prehistory,5-shot,accuracy,0.8209876543209876,0.02133086876212706
allenai/tulu-2-dpo-70b,hendrycksTest-prehistory,5-shot,acc_norm,0.8209876543209876,0.02133086876212706
allenai/tulu-2-dpo-70b,hendrycksTest-professional_accounting,5-shot,accuracy,0.574468085106383,0.02949482760014436
allenai/tulu-2-dpo-70b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.574468085106383,0.02949482760014436
allenai/tulu-2-dpo-70b,hendrycksTest-professional_law,5-shot,accuracy,0.546284224250326,0.012715404841277752
allenai/tulu-2-dpo-70b,hendrycksTest-professional_law,5-shot,acc_norm,0.546284224250326,0.012715404841277752
allenai/tulu-2-dpo-70b,hendrycksTest-professional_medicine,5-shot,accuracy,0.75,0.026303648393696036
allenai/tulu-2-dpo-70b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.75,0.026303648393696036
allenai/tulu-2-dpo-70b,hendrycksTest-professional_psychology,5-shot,accuracy,0.7598039215686274,0.01728276069516741
allenai/tulu-2-dpo-70b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7598039215686274,0.01728276069516741
allenai/tulu-2-dpo-70b,hendrycksTest-public_relations,5-shot,accuracy,0.7727272727272727,0.04013964554072775
allenai/tulu-2-dpo-70b,hendrycksTest-public_relations,5-shot,acc_norm,0.7727272727272727,0.04013964554072775
allenai/tulu-2-dpo-70b,hendrycksTest-security_studies,5-shot,accuracy,0.7673469387755102,0.027049257915896175
allenai/tulu-2-dpo-70b,hendrycksTest-security_studies,5-shot,acc_norm,0.7673469387755102,0.027049257915896175
allenai/tulu-2-dpo-70b,hendrycksTest-sociology,5-shot,accuracy,0.8756218905472637,0.023335401790166327
allenai/tulu-2-dpo-70b,hendrycksTest-sociology,5-shot,acc_norm,0.8756218905472637,0.023335401790166327
allenai/tulu-2-dpo-70b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.88,0.03265986323710906
allenai/tulu-2-dpo-70b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.88,0.03265986323710906
allenai/tulu-2-dpo-70b,hendrycksTest-virology,5-shot,accuracy,0.5120481927710844,0.03891364495835817
allenai/tulu-2-dpo-70b,hendrycksTest-virology,5-shot,acc_norm,0.5120481927710844,0.03891364495835817
allenai/tulu-2-dpo-70b,hendrycksTest-world_religions,5-shot,accuracy,0.8713450292397661,0.02567934272327691
allenai/tulu-2-dpo-70b,hendrycksTest-world_religions,5-shot,acc_norm,0.8713450292397661,0.02567934272327691
allenai/tulu-2-dpo-70b,truthfulqa:mc,0-shot,mc1,0.4675642594859241,0.017466632149577613
allenai/tulu-2-dpo-70b,truthfulqa:mc,0-shot,mc2,0.6577655722264159,0.014903281756393213
allenai/tulu-2-dpo-70b,winogrande,5-shot,accuracy,0.8326756116811366,0.010490608806828079
allenai/tulu-2-dpo-70b,gsm8k,5-shot,accuracy,0.6262319939347991,0.013326342860737007
Salesforce/codegen-350M-mono,minerva_math_precalc,5-shot,accuracy,0.0018315018315018315,0.0018315018315018335
Salesforce/codegen-350M-mono,minerva_math_prealgebra,5-shot,accuracy,0.004592422502870264,0.0022922488477037863
Salesforce/codegen-350M-mono,minerva_math_num_theory,5-shot,accuracy,0.001851851851851852,0.0018518518518518504
Salesforce/codegen-350M-mono,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417496,0.0015652595934070638
Salesforce/codegen-350M-mono,minerva_math_geometry,5-shot,accuracy,0.0020876826722338203,0.002087682672233835
Salesforce/codegen-350M-mono,minerva_math_counting_and_prob,5-shot,accuracy,0.006329113924050633,0.003646382041065048
Salesforce/codegen-350M-mono,minerva_math_algebra,5-shot,accuracy,0.006739679865206402,0.0023757942810498744
Salesforce/codegen-350M-mono,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
Salesforce/codegen-350M-mono,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521431
Salesforce/codegen-350M-mono,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
Salesforce/codegen-350M-mono,arithmetic_2ds,5-shot,accuracy,0.0055,0.0016541593398342208
Salesforce/codegen-350M-mono,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-350M-mono,arithmetic_1dc,5-shot,accuracy,0.0525,0.004988418302285789
Salesforce/codegen-350M-mono,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005000000000000043
Salesforce/codegen-350M-mono,arithmetic_2dm,5-shot,accuracy,0.0205,0.0031693686198869895
Salesforce/codegen-350M-mono,arithmetic_2da,5-shot,accuracy,0.013,0.002533517190523328
Salesforce/codegen-350M-mono,gsm8k_cot,5-shot,accuracy,0.019711902956785442,0.0038289829787357165
Salesforce/codegen-350M-mono,gsm8k,5-shot,accuracy,0.017437452615617893,0.0036054868679982334
Salesforce/codegen-350M-mono,anli_r2,0-shot,brier_score,0.8673139893251594,
Salesforce/codegen-350M-mono,anli_r3,0-shot,brier_score,0.8478090822960272,
Salesforce/codegen-350M-mono,anli_r1,0-shot,brier_score,0.8706127110280617,
Salesforce/codegen-350M-mono,xnli_eu,0-shot,brier_score,1.2682920496708858,
Salesforce/codegen-350M-mono,xnli_vi,0-shot,brier_score,1.2752175230857816,
Salesforce/codegen-350M-mono,xnli_ru,0-shot,brier_score,0.8045751050884926,
Salesforce/codegen-350M-mono,xnli_zh,0-shot,brier_score,1.2459315046432844,
Salesforce/codegen-350M-mono,xnli_tr,0-shot,brier_score,1.1658554315468868,
Salesforce/codegen-350M-mono,xnli_fr,0-shot,brier_score,1.1841733511077182,
Salesforce/codegen-350M-mono,xnli_en,0-shot,brier_score,0.7473727928562213,
Salesforce/codegen-350M-mono,xnli_ur,0-shot,brier_score,1.2271500572114633,
Salesforce/codegen-350M-mono,xnli_ar,0-shot,brier_score,1.0042596344795447,
Salesforce/codegen-350M-mono,xnli_de,0-shot,brier_score,1.014094216947774,
Salesforce/codegen-350M-mono,xnli_hi,0-shot,brier_score,1.1401151042767916,
Salesforce/codegen-350M-mono,xnli_es,0-shot,brier_score,1.0787926125248246,
Salesforce/codegen-350M-mono,xnli_bg,0-shot,brier_score,1.1483978030694448,
Salesforce/codegen-350M-mono,xnli_sw,0-shot,brier_score,0.9825310566958355,
Salesforce/codegen-350M-mono,xnli_el,0-shot,brier_score,1.0833010621729644,
Salesforce/codegen-350M-mono,xnli_th,0-shot,brier_score,1.0701844940260412,
Salesforce/codegen-350M-mono,logiqa2,0-shot,brier_score,1.2332295009469143,
Salesforce/codegen-350M-mono,mathqa,5-shot,brier_score,0.9914231048948398,
Salesforce/codegen-350M-mono,lambada_standard,0-shot,perplexity,647.5605869289026,29.442408532236634
Salesforce/codegen-350M-mono,lambada_standard,0-shot,accuracy,0.1340966427323889,0.004747399033321078
Salesforce/codegen-350M-mono,lambada_openai,0-shot,perplexity,727.0942907672784,37.34894050370883
Salesforce/codegen-350M-mono,lambada_openai,0-shot,accuracy,0.14147098777411216,0.004855380319784803
Salesforce/codegen-350M-mono,mmlu_world_religions,0-shot,accuracy,0.29239766081871343,0.03488647713457922
Salesforce/codegen-350M-mono,mmlu_formal_logic,0-shot,accuracy,0.21428571428571427,0.036700664510471825
Salesforce/codegen-350M-mono,mmlu_prehistory,0-shot,accuracy,0.20679012345679013,0.022535006705942818
Salesforce/codegen-350M-mono,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
Salesforce/codegen-350M-mono,mmlu_high_school_world_history,0-shot,accuracy,0.2320675105485232,0.027479744550808514
Salesforce/codegen-350M-mono,mmlu_moral_disputes,0-shot,accuracy,0.18497109826589594,0.020903975842083033
Salesforce/codegen-350M-mono,mmlu_professional_law,0-shot,accuracy,0.24902216427640156,0.01104489226404077
Salesforce/codegen-350M-mono,mmlu_logical_fallacies,0-shot,accuracy,0.25766871165644173,0.03436150827846917
Salesforce/codegen-350M-mono,mmlu_high_school_us_history,0-shot,accuracy,0.25980392156862747,0.030778554678693264
Salesforce/codegen-350M-mono,mmlu_philosophy,0-shot,accuracy,0.24115755627009647,0.02429659403476343
Salesforce/codegen-350M-mono,mmlu_jurisprudence,0-shot,accuracy,0.24074074074074073,0.04133119440243838
Salesforce/codegen-350M-mono,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.03941897526516303
Salesforce/codegen-350M-mono,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
Salesforce/codegen-350M-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.03469713791704372
Salesforce/codegen-350M-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.2647058823529412,0.028657491285071973
Salesforce/codegen-350M-mono,mmlu_high_school_geography,0-shot,accuracy,0.35353535353535354,0.03406086723547153
Salesforce/codegen-350M-mono,mmlu_high_school_psychology,0-shot,accuracy,0.30091743119266057,0.019664751366802114
Salesforce/codegen-350M-mono,mmlu_public_relations,0-shot,accuracy,0.20909090909090908,0.03895091015724138
Salesforce/codegen-350M-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.0446196043338474
Salesforce/codegen-350M-mono,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409224
Salesforce/codegen-350M-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.33589743589743587,0.023946724741563976
Salesforce/codegen-350M-mono,mmlu_security_studies,0-shot,accuracy,0.27346938775510204,0.02853556033712845
Salesforce/codegen-350M-mono,mmlu_professional_psychology,0-shot,accuracy,0.2434640522875817,0.017362473762146644
Salesforce/codegen-350M-mono,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596918
Salesforce/codegen-350M-mono,mmlu_econometrics,0-shot,accuracy,0.21052631578947367,0.0383515395439942
Salesforce/codegen-350M-mono,mmlu_miscellaneous,0-shot,accuracy,0.25287356321839083,0.01554337731371968
Salesforce/codegen-350M-mono,mmlu_marketing,0-shot,accuracy,0.23931623931623933,0.027951826808924333
Salesforce/codegen-350M-mono,mmlu_management,0-shot,accuracy,0.20388349514563106,0.03989139859531772
Salesforce/codegen-350M-mono,mmlu_nutrition,0-shot,accuracy,0.2549019607843137,0.024954184324879912
Salesforce/codegen-350M-mono,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
Salesforce/codegen-350M-mono,mmlu_human_aging,0-shot,accuracy,0.21973094170403587,0.027790177064383602
Salesforce/codegen-350M-mono,mmlu_professional_medicine,0-shot,accuracy,0.3602941176470588,0.029163128570670733
Salesforce/codegen-350M-mono,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
Salesforce/codegen-350M-mono,mmlu_business_ethics,0-shot,accuracy,0.33,0.04725815626252604
Salesforce/codegen-350M-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.2792452830188679,0.027611163402399715
Salesforce/codegen-350M-mono,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
Salesforce/codegen-350M-mono,mmlu_virology,0-shot,accuracy,0.1927710843373494,0.03070982405056527
Salesforce/codegen-350M-mono,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.026684564340460997
Salesforce/codegen-350M-mono,mmlu_college_physics,0-shot,accuracy,0.20588235294117646,0.040233822736177476
Salesforce/codegen-350M-mono,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
Salesforce/codegen-350M-mono,mmlu_high_school_biology,0-shot,accuracy,0.29354838709677417,0.02590608702131929
Salesforce/codegen-350M-mono,mmlu_college_biology,0-shot,accuracy,0.25,0.03621034121889507
Salesforce/codegen-350M-mono,mmlu_anatomy,0-shot,accuracy,0.24444444444444444,0.037125378336148665
Salesforce/codegen-350M-mono,mmlu_college_chemistry,0-shot,accuracy,0.24,0.042923469599092816
Salesforce/codegen-350M-mono,mmlu_computer_security,0-shot,accuracy,0.21,0.040936018074033256
Salesforce/codegen-350M-mono,mmlu_college_computer_science,0-shot,accuracy,0.38,0.048783173121456316
Salesforce/codegen-350M-mono,mmlu_astronomy,0-shot,accuracy,0.18421052631578946,0.0315469804508223
Salesforce/codegen-350M-mono,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-350M-mono,mmlu_conceptual_physics,0-shot,accuracy,0.33617021276595743,0.030881618520676942
Salesforce/codegen-350M-mono,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.040936018074033256
Salesforce/codegen-350M-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.28,0.04512608598542127
Salesforce/codegen-350M-mono,mmlu_machine_learning,0-shot,accuracy,0.25892857142857145,0.04157751539865629
Salesforce/codegen-350M-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.28078817733990147,0.03161856335358609
Salesforce/codegen-350M-mono,mmlu_high_school_statistics,0-shot,accuracy,0.4444444444444444,0.03388857118502326
Salesforce/codegen-350M-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.25132275132275134,0.022340482339643895
Salesforce/codegen-350M-mono,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.03600105692727771
Salesforce/codegen-350M-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.026466117538959905
Salesforce/codegen-350M-mono,arc_challenge,25-shot,accuracy,0.17491467576791808,0.011101562501828222
Salesforce/codegen-350M-mono,arc_challenge,25-shot,acc_norm,0.2090443686006826,0.011882746987406451
Salesforce/codegen-350M-mono,hellaswag,10-shot,accuracy,0.27185819557857,0.004440079173277
Salesforce/codegen-350M-mono,hellaswag,10-shot,acc_norm,0.2898824935271858,0.004527804016253785
Salesforce/codegen-350M-mono,truthfulqa_mc2,0-shot,accuracy,0.46361368860111973,0.015624637210450354
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,bleu_max,17.866027572769028,0.6178956466724292
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,bleu_acc,0.33414932680538556,0.016512530677150555
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,bleu_diff,-2.1708615982970616,0.4770014144933768
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge1_max,40.82248996851368,0.78495594086117
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge1_acc,0.3047735618115055,0.01611412415688242
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge1_diff,-4.957623701292929,0.6248240530678496
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge2_max,24.375831054930796,0.8575180753868562
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge2_acc,0.23011015911872704,0.014734557959807763
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rouge2_diff,-4.351761806675208,0.6492003146837445
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rougeL_max,37.79851538845665,0.7858244663149364
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rougeL_acc,0.3023255813953488,0.01607750926613303
Salesforce/codegen-350M-mono,truthfulqa_gen,0-shot,rougeL_diff,-4.666501260483323,0.6089020246878603
Salesforce/codegen-350M-mono,truthfulqa_mc1,0-shot,accuracy,0.2521419828641371,0.015201522246299956
Salesforce/codegen-350M-mono,winogrande,5-shot,accuracy,0.5193370165745856,0.014041972733712976
EleutherAI/gpt-neo-2.7B,arc:challenge,25-shot,accuracy,0.310580204778157,0.013522292098053048
EleutherAI/gpt-neo-2.7B,arc:challenge,25-shot,acc_norm,0.33361774744027306,0.013778687054176536
EleutherAI/gpt-neo-2.7B,hellaswag,10-shot,accuracy,0.4266082453694483,0.004935735300348859
EleutherAI/gpt-neo-2.7B,hellaswag,10-shot,acc_norm,0.561840270862378,0.004951470301995878
EleutherAI/gpt-neo-2.7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-neo-2.7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/gpt-neo-2.7B,hendrycksTest-anatomy,5-shot,accuracy,0.2074074074074074,0.03502553170678318
EleutherAI/gpt-neo-2.7B,hendrycksTest-anatomy,5-shot,acc_norm,0.2074074074074074,0.03502553170678318
EleutherAI/gpt-neo-2.7B,hendrycksTest-astronomy,5-shot,accuracy,0.19078947368421054,0.031975658210325
EleutherAI/gpt-neo-2.7B,hendrycksTest-astronomy,5-shot,acc_norm,0.19078947368421054,0.031975658210325
EleutherAI/gpt-neo-2.7B,hendrycksTest-business_ethics,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-neo-2.7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/gpt-neo-2.7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2641509433962264,0.027134291628741702
EleutherAI/gpt-neo-2.7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2641509433962264,0.027134291628741702
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.03685651095897532
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.23,0.04229525846816506
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.23,0.04229525846816506
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.044084400227680794
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.044084400227680794
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.04512608598542127
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.04512608598542127
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_physics,5-shot,accuracy,0.22549019607843138,0.041583075330832865
EleutherAI/gpt-neo-2.7B,hendrycksTest-college_physics,5-shot,acc_norm,0.22549019607843138,0.041583075330832865
EleutherAI/gpt-neo-2.7B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.045126085985421255
EleutherAI/gpt-neo-2.7B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.045126085985421255
EleutherAI/gpt-neo-2.7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.02924188386962881
EleutherAI/gpt-neo-2.7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.02924188386962881
EleutherAI/gpt-neo-2.7B,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.041424397194893624
EleutherAI/gpt-neo-2.7B,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.041424397194893624
EleutherAI/gpt-neo-2.7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.25517241379310346,0.03632984052707842
EleutherAI/gpt-neo-2.7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.25517241379310346,0.03632984052707842
EleutherAI/gpt-neo-2.7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.291005291005291,0.023393826500484865
EleutherAI/gpt-neo-2.7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.291005291005291,0.023393826500484865
EleutherAI/gpt-neo-2.7B,hendrycksTest-formal_logic,5-shot,accuracy,0.1746031746031746,0.033954900208561116
EleutherAI/gpt-neo-2.7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.1746031746031746,0.033954900208561116
EleutherAI/gpt-neo-2.7B,hendrycksTest-global_facts,5-shot,accuracy,0.22,0.04163331998932268
EleutherAI/gpt-neo-2.7B,hendrycksTest-global_facts,5-shot,acc_norm,0.22,0.04163331998932268
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.23548387096774193,0.02413763242933771
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.23548387096774193,0.02413763242933771
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.24630541871921183,0.030315099285617732
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.24630541871921183,0.030315099285617732
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.35,0.047937248544110196
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.35,0.047937248544110196
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.24242424242424243,0.03346409881055953
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.24242424242424243,0.03346409881055953
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.3333333333333333,0.03358618145732523
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3333333333333333,0.03358618145732523
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.25906735751295334,0.03161877917935409
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.25906735751295334,0.03161877917935409
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3435897435897436,0.024078696580635477
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3435897435897436,0.024078696580635477
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.23703703703703705,0.025928876132766128
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.23703703703703705,0.025928876132766128
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.25630252100840334,0.02835962087053395
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.25630252100840334,0.02835962087053395
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.23178807947019867,0.03445406271987054
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23178807947019867,0.03445406271987054
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3100917431192661,0.019830849684439746
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3100917431192661,0.019830849684439746
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.39351851851851855,0.03331747876370312
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.39351851851851855,0.03331747876370312
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.19607843137254902,0.027865942286639318
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.19607843137254902,0.027865942286639318
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.22362869198312235,0.02712329820522997
EleutherAI/gpt-neo-2.7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.22362869198312235,0.02712329820522997
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_aging,5-shot,accuracy,0.18385650224215247,0.02599837909235651
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_aging,5-shot,acc_norm,0.18385650224215247,0.02599837909235651
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2824427480916031,0.03948406125768361
EleutherAI/gpt-neo-2.7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2824427480916031,0.03948406125768361
EleutherAI/gpt-neo-2.7B,hendrycksTest-international_law,5-shot,accuracy,0.23140495867768596,0.0384985609879409
EleutherAI/gpt-neo-2.7B,hendrycksTest-international_law,5-shot,acc_norm,0.23140495867768596,0.0384985609879409
EleutherAI/gpt-neo-2.7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.28703703703703703,0.043733130409147614
EleutherAI/gpt-neo-2.7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.28703703703703703,0.043733130409147614
EleutherAI/gpt-neo-2.7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25766871165644173,0.03436150827846917
EleutherAI/gpt-neo-2.7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25766871165644173,0.03436150827846917
EleutherAI/gpt-neo-2.7B,hendrycksTest-machine_learning,5-shot,accuracy,0.17857142857142858,0.03635209121577806
EleutherAI/gpt-neo-2.7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.17857142857142858,0.03635209121577806
EleutherAI/gpt-neo-2.7B,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.04354631077260595
EleutherAI/gpt-neo-2.7B,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.04354631077260595
EleutherAI/gpt-neo-2.7B,hendrycksTest-marketing,5-shot,accuracy,0.2692307692307692,0.02905858830374885
EleutherAI/gpt-neo-2.7B,hendrycksTest-marketing,5-shot,acc_norm,0.2692307692307692,0.02905858830374885
EleutherAI/gpt-neo-2.7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.04512608598542127
EleutherAI/gpt-neo-2.7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.04512608598542127
EleutherAI/gpt-neo-2.7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2388250319284802,0.015246803197398682
EleutherAI/gpt-neo-2.7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2388250319284802,0.015246803197398682
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24855491329479767,0.023267528432100174
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.264804469273743,0.014756906483260664
EleutherAI/gpt-neo-2.7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.264804469273743,0.014756906483260664
EleutherAI/gpt-neo-2.7B,hendrycksTest-nutrition,5-shot,accuracy,0.31699346405228757,0.026643278474508755
EleutherAI/gpt-neo-2.7B,hendrycksTest-nutrition,5-shot,acc_norm,0.31699346405228757,0.026643278474508755
EleutherAI/gpt-neo-2.7B,hendrycksTest-philosophy,5-shot,accuracy,0.31511254019292606,0.026385273703464485
EleutherAI/gpt-neo-2.7B,hendrycksTest-philosophy,5-shot,acc_norm,0.31511254019292606,0.026385273703464485
EleutherAI/gpt-neo-2.7B,hendrycksTest-prehistory,5-shot,accuracy,0.30864197530864196,0.02570264026060375
EleutherAI/gpt-neo-2.7B,hendrycksTest-prehistory,5-shot,acc_norm,0.30864197530864196,0.02570264026060375
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902006
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902006
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_law,5-shot,accuracy,0.2438070404172099,0.010966507972178477
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_law,5-shot,acc_norm,0.2438070404172099,0.010966507972178477
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.43014705882352944,0.030074971917302875
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.43014705882352944,0.030074971917302875
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.27450980392156865,0.0180540274588152
EleutherAI/gpt-neo-2.7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.27450980392156865,0.0180540274588152
EleutherAI/gpt-neo-2.7B,hendrycksTest-public_relations,5-shot,accuracy,0.19090909090909092,0.03764425585984924
EleutherAI/gpt-neo-2.7B,hendrycksTest-public_relations,5-shot,acc_norm,0.19090909090909092,0.03764425585984924
EleutherAI/gpt-neo-2.7B,hendrycksTest-security_studies,5-shot,accuracy,0.2857142857142857,0.028920583220675585
EleutherAI/gpt-neo-2.7B,hendrycksTest-security_studies,5-shot,acc_norm,0.2857142857142857,0.028920583220675585
EleutherAI/gpt-neo-2.7B,hendrycksTest-sociology,5-shot,accuracy,0.22388059701492538,0.029475250236017183
EleutherAI/gpt-neo-2.7B,hendrycksTest-sociology,5-shot,acc_norm,0.22388059701492538,0.029475250236017183
EleutherAI/gpt-neo-2.7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neo-2.7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/gpt-neo-2.7B,hendrycksTest-virology,5-shot,accuracy,0.3132530120481928,0.03610805018031023
EleutherAI/gpt-neo-2.7B,hendrycksTest-virology,5-shot,acc_norm,0.3132530120481928,0.03610805018031023
EleutherAI/gpt-neo-2.7B,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.034462962170884265
EleutherAI/gpt-neo-2.7B,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.034462962170884265
EleutherAI/gpt-neo-2.7B,truthfulqa:mc,0-shot,mc1,0.23990208078335373,0.014948812679062133
EleutherAI/gpt-neo-2.7B,truthfulqa:mc,0-shot,mc2,0.39777959908780214,0.014038895665663543
EleutherAI/gpt-neo-2.7B,drop,3-shot,accuracy,0.001363255033557047,0.0003778609196460643
EleutherAI/gpt-neo-2.7B,drop,3-shot,f1,0.04774853187919481,0.0012502430800989544
EleutherAI/gpt-neo-2.7B,gsm8k,5-shot,accuracy,0.02047005307050796,0.0039004133859157192
EleutherAI/gpt-neo-2.7B,winogrande,5-shot,accuracy,0.5777426992896606,0.013881582030658557
CohereForAI/c4ai-command-r-plus,arc:challenge,25-shot,accuracy,0.659556313993174,0.013847460518892978
CohereForAI/c4ai-command-r-plus,arc:challenge,25-shot,acc_norm,0.7039249146757679,0.01334091608524626
CohereForAI/c4ai-command-r-plus,hellaswag,10-shot,accuracy,0.6927902808205537,0.004603942439861571
CohereForAI/c4ai-command-r-plus,hellaswag,10-shot,acc_norm,0.8796056562437762,0.00324757033045692
CohereForAI/c4ai-command-r-plus,hendrycksTest-abstract_algebra,5-shot,accuracy,0.44,0.04988876515698589
CohereForAI/c4ai-command-r-plus,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.44,0.04988876515698589
CohereForAI/c4ai-command-r-plus,hendrycksTest-anatomy,5-shot,accuracy,0.7481481481481481,0.03749850709174021
CohereForAI/c4ai-command-r-plus,hendrycksTest-anatomy,5-shot,acc_norm,0.7481481481481481,0.03749850709174021
CohereForAI/c4ai-command-r-plus,hendrycksTest-astronomy,5-shot,accuracy,0.8486842105263158,0.029162631596843975
CohereForAI/c4ai-command-r-plus,hendrycksTest-astronomy,5-shot,acc_norm,0.8486842105263158,0.029162631596843975
CohereForAI/c4ai-command-r-plus,hendrycksTest-business_ethics,5-shot,accuracy,0.79,0.040936018074033256
CohereForAI/c4ai-command-r-plus,hendrycksTest-business_ethics,5-shot,acc_norm,0.79,0.040936018074033256
CohereForAI/c4ai-command-r-plus,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.769811320754717,0.025907897122408173
CohereForAI/c4ai-command-r-plus,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.769811320754717,0.025907897122408173
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_biology,5-shot,accuracy,0.8611111111111112,0.028919802956134912
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_biology,5-shot,acc_norm,0.8611111111111112,0.028919802956134912
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_chemistry,5-shot,accuracy,0.49,0.05024183937956913
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_chemistry,5-shot,acc_norm,0.49,0.05024183937956913
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_computer_science,5-shot,accuracy,0.61,0.04902071300001975
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_computer_science,5-shot,acc_norm,0.61,0.04902071300001975
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_mathematics,5-shot,accuracy,0.39,0.04902071300001974
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_mathematics,5-shot,acc_norm,0.39,0.04902071300001974
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_medicine,5-shot,accuracy,0.7398843930635838,0.033450369167889904
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_medicine,5-shot,acc_norm,0.7398843930635838,0.033450369167889904
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_physics,5-shot,accuracy,0.5098039215686274,0.04974229460422817
CohereForAI/c4ai-command-r-plus,hendrycksTest-college_physics,5-shot,acc_norm,0.5098039215686274,0.04974229460422817
CohereForAI/c4ai-command-r-plus,hendrycksTest-computer_security,5-shot,accuracy,0.84,0.03684529491774708
CohereForAI/c4ai-command-r-plus,hendrycksTest-computer_security,5-shot,acc_norm,0.84,0.03684529491774708
CohereForAI/c4ai-command-r-plus,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7191489361702128,0.02937917046412482
CohereForAI/c4ai-command-r-plus,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7191489361702128,0.02937917046412482
CohereForAI/c4ai-command-r-plus,hendrycksTest-econometrics,5-shot,accuracy,0.6052631578947368,0.04598188057816542
CohereForAI/c4ai-command-r-plus,hendrycksTest-econometrics,5-shot,acc_norm,0.6052631578947368,0.04598188057816542
CohereForAI/c4ai-command-r-plus,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7241379310344828,0.037245636197746325
CohereForAI/c4ai-command-r-plus,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7241379310344828,0.037245636197746325
CohereForAI/c4ai-command-r-plus,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.5661375661375662,0.025525034382474887
CohereForAI/c4ai-command-r-plus,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.5661375661375662,0.025525034382474887
CohereForAI/c4ai-command-r-plus,hendrycksTest-formal_logic,5-shot,accuracy,0.5634920634920635,0.04435932892851466
CohereForAI/c4ai-command-r-plus,hendrycksTest-formal_logic,5-shot,acc_norm,0.5634920634920635,0.04435932892851466
CohereForAI/c4ai-command-r-plus,hendrycksTest-global_facts,5-shot,accuracy,0.6,0.049236596391733084
CohereForAI/c4ai-command-r-plus,hendrycksTest-global_facts,5-shot,acc_norm,0.6,0.049236596391733084
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_biology,5-shot,accuracy,0.8258064516129032,0.021576248184514573
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8258064516129032,0.021576248184514573
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6403940886699507,0.03376458246509567
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6403940886699507,0.03376458246509567
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.81,0.03942772444036624
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.81,0.03942772444036624
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8606060606060606,0.02704594882586535
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8606060606060606,0.02704594882586535
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_geography,5-shot,accuracy,0.9090909090909091,0.020482086775424204
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9090909090909091,0.020482086775424204
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9378238341968912,0.017426974154240535
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9378238341968912,0.017426974154240535
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7128205128205128,0.022939925418530616
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.7128205128205128,0.022939925418530616
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3888888888888889,0.029723278961476668
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3888888888888889,0.029723278961476668
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7983193277310925,0.026064313406304534
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7983193277310925,0.026064313406304534
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_physics,5-shot,accuracy,0.5099337748344371,0.04081677107248437
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5099337748344371,0.04081677107248437
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9064220183486239,0.012486841824601963
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9064220183486239,0.012486841824601963
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_statistics,5-shot,accuracy,0.625,0.033016908987210894
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.625,0.033016908987210894
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8970588235294118,0.02132833757080438
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8970588235294118,0.02132833757080438
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9071729957805907,0.01888975055095671
CohereForAI/c4ai-command-r-plus,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9071729957805907,0.01888975055095671
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_aging,5-shot,accuracy,0.820627802690583,0.0257498195691928
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_aging,5-shot,acc_norm,0.820627802690583,0.0257498195691928
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_sexuality,5-shot,accuracy,0.8473282442748091,0.031545216720054704
CohereForAI/c4ai-command-r-plus,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8473282442748091,0.031545216720054704
CohereForAI/c4ai-command-r-plus,hendrycksTest-international_law,5-shot,accuracy,0.9008264462809917,0.02728524631275896
CohereForAI/c4ai-command-r-plus,hendrycksTest-international_law,5-shot,acc_norm,0.9008264462809917,0.02728524631275896
CohereForAI/c4ai-command-r-plus,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.03520703990517964
CohereForAI/c4ai-command-r-plus,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.03520703990517964
CohereForAI/c4ai-command-r-plus,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8404907975460123,0.02876748172598387
CohereForAI/c4ai-command-r-plus,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8404907975460123,0.02876748172598387
CohereForAI/c4ai-command-r-plus,hendrycksTest-machine_learning,5-shot,accuracy,0.5178571428571429,0.047427623612430116
CohereForAI/c4ai-command-r-plus,hendrycksTest-machine_learning,5-shot,acc_norm,0.5178571428571429,0.047427623612430116
CohereForAI/c4ai-command-r-plus,hendrycksTest-management,5-shot,accuracy,0.8543689320388349,0.034926064766237906
CohereForAI/c4ai-command-r-plus,hendrycksTest-management,5-shot,acc_norm,0.8543689320388349,0.034926064766237906
CohereForAI/c4ai-command-r-plus,hendrycksTest-marketing,5-shot,accuracy,0.9017094017094017,0.019503444900757567
CohereForAI/c4ai-command-r-plus,hendrycksTest-marketing,5-shot,acc_norm,0.9017094017094017,0.019503444900757567
CohereForAI/c4ai-command-r-plus,hendrycksTest-medical_genetics,5-shot,accuracy,0.83,0.0377525168068637
CohereForAI/c4ai-command-r-plus,hendrycksTest-medical_genetics,5-shot,acc_norm,0.83,0.0377525168068637
CohereForAI/c4ai-command-r-plus,hendrycksTest-miscellaneous,5-shot,accuracy,0.879948914431673,0.011622736692041256
CohereForAI/c4ai-command-r-plus,hendrycksTest-miscellaneous,5-shot,acc_norm,0.879948914431673,0.011622736692041256
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_disputes,5-shot,accuracy,0.7745664739884393,0.022497230190967554
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7745664739884393,0.022497230190967554
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6312849162011173,0.016135759015030122
CohereForAI/c4ai-command-r-plus,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6312849162011173,0.016135759015030122
CohereForAI/c4ai-command-r-plus,hendrycksTest-nutrition,5-shot,accuracy,0.7973856209150327,0.023015446877985686
CohereForAI/c4ai-command-r-plus,hendrycksTest-nutrition,5-shot,acc_norm,0.7973856209150327,0.023015446877985686
CohereForAI/c4ai-command-r-plus,hendrycksTest-philosophy,5-shot,accuracy,0.77491961414791,0.023720088516179027
CohereForAI/c4ai-command-r-plus,hendrycksTest-philosophy,5-shot,acc_norm,0.77491961414791,0.023720088516179027
CohereForAI/c4ai-command-r-plus,hendrycksTest-prehistory,5-shot,accuracy,0.8611111111111112,0.019242526226544536
CohereForAI/c4ai-command-r-plus,hendrycksTest-prehistory,5-shot,acc_norm,0.8611111111111112,0.019242526226544536
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_accounting,5-shot,accuracy,0.5780141843971631,0.029462189233370593
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5780141843971631,0.029462189233370593
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_law,5-shot,accuracy,0.590612777053455,0.012558780895570755
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_law,5-shot,acc_norm,0.590612777053455,0.012558780895570755
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_medicine,5-shot,accuracy,0.75,0.026303648393696036
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_medicine,5-shot,acc_norm,0.75,0.026303648393696036
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_psychology,5-shot,accuracy,0.7973856209150327,0.016261055283746127
CohereForAI/c4ai-command-r-plus,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7973856209150327,0.016261055283746127
CohereForAI/c4ai-command-r-plus,hendrycksTest-public_relations,5-shot,accuracy,0.7909090909090909,0.038950910157241364
CohereForAI/c4ai-command-r-plus,hendrycksTest-public_relations,5-shot,acc_norm,0.7909090909090909,0.038950910157241364
CohereForAI/c4ai-command-r-plus,hendrycksTest-security_studies,5-shot,accuracy,0.8244897959183674,0.02435280072297001
CohereForAI/c4ai-command-r-plus,hendrycksTest-security_studies,5-shot,acc_norm,0.8244897959183674,0.02435280072297001
CohereForAI/c4ai-command-r-plus,hendrycksTest-sociology,5-shot,accuracy,0.8855721393034826,0.022509345325101713
CohereForAI/c4ai-command-r-plus,hendrycksTest-sociology,5-shot,acc_norm,0.8855721393034826,0.022509345325101713
CohereForAI/c4ai-command-r-plus,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.027265992434429093
CohereForAI/c4ai-command-r-plus,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.027265992434429093
CohereForAI/c4ai-command-r-plus,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721015
CohereForAI/c4ai-command-r-plus,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721015
CohereForAI/c4ai-command-r-plus,hendrycksTest-world_religions,5-shot,accuracy,0.8830409356725146,0.024648068961366152
CohereForAI/c4ai-command-r-plus,hendrycksTest-world_religions,5-shot,acc_norm,0.8830409356725146,0.024648068961366152
CohereForAI/c4ai-command-r-plus,truthfulqa:mc,0-shot,mc1,0.39657282741738065,0.017124930942023518
CohereForAI/c4ai-command-r-plus,truthfulqa:mc,0-shot,mc2,0.5695167541939289,0.015126847126703044
CohereForAI/c4ai-command-r-plus,winogrande,5-shot,accuracy,0.8382004735595896,0.010350128010292406
CohereForAI/c4ai-command-r-plus,gsm8k,5-shot,accuracy,0.47308567096285065,0.013752517189717465
mistralai/Mistral-7B-v0.1,drop,3-shot,accuracy,0.001572986577181208,0.00040584511324177333
mistralai/Mistral-7B-v0.1,drop,3-shot,f1,0.06143666107382555,0.0013713061256604275
mistralai/Mistral-7B-v0.1,gsm8k,5-shot,accuracy,0.3707354056103108,0.013304267705458433
mistralai/Mistral-7B-v0.1,winogrande,5-shot,accuracy,0.7861089187056038,0.011524466954090254
mistralai/Mistral-7B-v0.1,arc:challenge,25-shot,accuracy,0.568259385665529,0.014474591427196202
mistralai/Mistral-7B-v0.1,arc:challenge,25-shot,acc_norm,0.5998293515358362,0.014317197787809172
mistralai/Mistral-7B-v0.1,hellaswag,10-shot,accuracy,0.6294562836088429,0.00481963366883254
mistralai/Mistral-7B-v0.1,hellaswag,10-shot,acc_norm,0.8331009759012149,0.0037212361965025162
mistralai/Mistral-7B-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.29,0.045604802157206845
mistralai/Mistral-7B-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.29,0.045604802157206845
mistralai/Mistral-7B-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.6296296296296297,0.041716541613545426
mistralai/Mistral-7B-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.6296296296296297,0.041716541613545426
mistralai/Mistral-7B-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.6578947368421053,0.03860731599316091
mistralai/Mistral-7B-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.6578947368421053,0.03860731599316091
mistralai/Mistral-7B-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.57,0.049756985195624284
mistralai/Mistral-7B-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.57,0.049756985195624284
mistralai/Mistral-7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.6943396226415094,0.028353298073322663
mistralai/Mistral-7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.6943396226415094,0.028353298073322663
mistralai/Mistral-7B-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.7291666666666666,0.03716177437566017
mistralai/Mistral-7B-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.7291666666666666,0.03716177437566017
mistralai/Mistral-7B-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.52,0.050211673156867795
mistralai/Mistral-7B-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.52,0.050211673156867795
mistralai/Mistral-7B-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.52,0.050211673156867795
mistralai/Mistral-7B-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.52,0.050211673156867795
mistralai/Mistral-7B-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.4,0.04923659639173309
mistralai/Mistral-7B-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.4,0.04923659639173309
mistralai/Mistral-7B-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.6647398843930635,0.03599586301247077
mistralai/Mistral-7B-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.6647398843930635,0.03599586301247077
mistralai/Mistral-7B-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.39215686274509803,0.04858083574266346
mistralai/Mistral-7B-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.39215686274509803,0.04858083574266346
mistralai/Mistral-7B-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.77,0.042295258468165065
mistralai/Mistral-7B-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.77,0.042295258468165065
mistralai/Mistral-7B-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.574468085106383,0.03232146916224468
mistralai/Mistral-7B-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.574468085106383,0.03232146916224468
mistralai/Mistral-7B-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.5,0.047036043419179864
mistralai/Mistral-7B-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.5,0.047036043419179864
mistralai/Mistral-7B-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5724137931034483,0.04122737111370332
mistralai/Mistral-7B-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5724137931034483,0.04122737111370332
mistralai/Mistral-7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.373015873015873,0.02490699045899257
mistralai/Mistral-7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.373015873015873,0.02490699045899257
mistralai/Mistral-7B-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.4126984126984127,0.04403438954768177
mistralai/Mistral-7B-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.4126984126984127,0.04403438954768177
mistralai/Mistral-7B-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.37,0.04852365870939099
mistralai/Mistral-7B-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.37,0.04852365870939099
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.7709677419354839,0.023904914311782648
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7709677419354839,0.023904914311782648
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.03512819077876106
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.03512819077876106
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.68,0.04688261722621504
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.68,0.04688261722621504
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7818181818181819,0.032250781083062896
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7818181818181819,0.032250781083062896
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.7727272727272727,0.029857515673386417
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.7727272727272727,0.029857515673386417
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.8652849740932642,0.02463978909770944
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.8652849740932642,0.02463978909770944
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6666666666666666,0.023901157979402534
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6666666666666666,0.023901157979402534
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.337037037037037,0.028820884666253255
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.337037037037037,0.028820884666253255
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.6596638655462185,0.030778057422931673
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.6596638655462185,0.030778057422931673
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.32450331125827814,0.038227469376587525
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.32450331125827814,0.038227469376587525
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8238532110091743,0.016332882393431385
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8238532110091743,0.016332882393431385
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5740740740740741,0.03372343271653062
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5740740740740741,0.03372343271653062
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.7990196078431373,0.028125972265654373
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.7990196078431373,0.028125972265654373
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.7721518987341772,0.027303484599069436
mistralai/Mistral-7B-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.7721518987341772,0.027303484599069436
mistralai/Mistral-7B-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.7040358744394619,0.030636591348699803
mistralai/Mistral-7B-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.7040358744394619,0.030636591348699803
mistralai/Mistral-7B-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.7938931297709924,0.03547771004159463
mistralai/Mistral-7B-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7938931297709924,0.03547771004159463
mistralai/Mistral-7B-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.7768595041322314,0.03800754475228732
mistralai/Mistral-7B-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.7768595041322314,0.03800754475228732
mistralai/Mistral-7B-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.7777777777777778,0.040191074725573483
mistralai/Mistral-7B-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7777777777777778,0.040191074725573483
mistralai/Mistral-7B-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7914110429447853,0.031921934489347235
mistralai/Mistral-7B-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7914110429447853,0.031921934489347235
mistralai/Mistral-7B-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.48214285714285715,0.047427623612430116
mistralai/Mistral-7B-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.48214285714285715,0.047427623612430116
mistralai/Mistral-7B-v0.1,hendrycksTest-management,5-shot,accuracy,0.8155339805825242,0.03840423627288276
mistralai/Mistral-7B-v0.1,hendrycksTest-management,5-shot,acc_norm,0.8155339805825242,0.03840423627288276
mistralai/Mistral-7B-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.8717948717948718,0.02190190511507333
mistralai/Mistral-7B-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.8717948717948718,0.02190190511507333
mistralai/Mistral-7B-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.74,0.04408440022768078
mistralai/Mistral-7B-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.74,0.04408440022768078
mistralai/Mistral-7B-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.8173690932311622,0.013816335389973136
mistralai/Mistral-7B-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8173690932311622,0.013816335389973136
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.7109826589595376,0.02440517393578323
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7109826589595376,0.02440517393578323
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.32513966480446926,0.01566654278505355
mistralai/Mistral-7B-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.32513966480446926,0.01566654278505355
mistralai/Mistral-7B-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.7581699346405228,0.024518195641879334
mistralai/Mistral-7B-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.7581699346405228,0.024518195641879334
mistralai/Mistral-7B-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.6977491961414791,0.026082700695399665
mistralai/Mistral-7B-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.6977491961414791,0.026082700695399665
mistralai/Mistral-7B-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.7345679012345679,0.024569223600460845
mistralai/Mistral-7B-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.7345679012345679,0.024569223600460845
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.4858156028368794,0.02981549448368206
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4858156028368794,0.02981549448368206
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.44784876140808344,0.01270058240476822
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.44784876140808344,0.01270058240476822
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.6911764705882353,0.02806499816704009
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.6911764705882353,0.02806499816704009
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.6813725490196079,0.01885008469646872
mistralai/Mistral-7B-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.6813725490196079,0.01885008469646872
mistralai/Mistral-7B-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.6727272727272727,0.0449429086625209
mistralai/Mistral-7B-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.6727272727272727,0.0449429086625209
mistralai/Mistral-7B-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.726530612244898,0.028535560337128448
mistralai/Mistral-7B-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.726530612244898,0.028535560337128448
mistralai/Mistral-7B-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.8308457711442786,0.026508590656233264
mistralai/Mistral-7B-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.8308457711442786,0.026508590656233264
mistralai/Mistral-7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.034873508801977704
mistralai/Mistral-7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.034873508801977704
mistralai/Mistral-7B-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.03869543323472101
mistralai/Mistral-7B-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.03869543323472101
mistralai/Mistral-7B-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8304093567251462,0.02878210810540171
mistralai/Mistral-7B-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8304093567251462,0.02878210810540171
mistralai/Mistral-7B-v0.1,truthfulqa:mc,0-shot,mc1,0.2802937576499388,0.015723139524608763
mistralai/Mistral-7B-v0.1,truthfulqa:mc,0-shot,mc2,0.4215317106968115,0.014138129483133954
mistralai/Mistral-7B-v0.1,minerva_math_precalc,5-shot,accuracy,0.04212454212454213,0.008604464925374512
mistralai/Mistral-7B-v0.1,minerva_math_prealgebra,5-shot,accuracy,0.23765786452353616,0.014430834004369359
mistralai/Mistral-7B-v0.1,minerva_math_num_theory,5-shot,accuracy,0.08888888888888889,0.012257870465567287
mistralai/Mistral-7B-v0.1,minerva_math_intermediate_algebra,5-shot,accuracy,0.05426356589147287,0.007542858423834505
mistralai/Mistral-7B-v0.1,minerva_math_geometry,5-shot,accuracy,0.11064718162839249,0.014348062924241815
mistralai/Mistral-7B-v0.1,minerva_math_counting_and_prob,5-shot,accuracy,0.12025316455696203,0.014955348691492934
mistralai/Mistral-7B-v0.1,minerva_math_algebra,5-shot,accuracy,0.18281381634372368,0.011223354238019886
mistralai/Mistral-7B-v0.1,fld_default,0-shot,accuracy,0.0,
mistralai/Mistral-7B-v0.1,fld_star,0-shot,accuracy,0.0,
mistralai/Mistral-7B-v0.1,arithmetic_3da,5-shot,accuracy,0.982,0.0029736208922129417
mistralai/Mistral-7B-v0.1,arithmetic_3ds,5-shot,accuracy,0.9885,0.0023846841214675823
mistralai/Mistral-7B-v0.1,arithmetic_4da,5-shot,accuracy,0.954,0.004685400355171836
mistralai/Mistral-7B-v0.1,arithmetic_2ds,5-shot,accuracy,0.998,0.00099924934306949
mistralai/Mistral-7B-v0.1,arithmetic_5ds,5-shot,accuracy,0.8715,0.007484776946774901
mistralai/Mistral-7B-v0.1,arithmetic_5da,5-shot,accuracy,0.909,0.0064327435900281065
mistralai/Mistral-7B-v0.1,arithmetic_1dc,5-shot,accuracy,0.6455,0.010699164035359294
mistralai/Mistral-7B-v0.1,arithmetic_4ds,5-shot,accuracy,0.945,0.005099068566917326
mistralai/Mistral-7B-v0.1,arithmetic_2dm,5-shot,accuracy,0.7115,0.010133371482818627
mistralai/Mistral-7B-v0.1,arithmetic_2da,5-shot,accuracy,0.9985,0.0008655920660521451
mistralai/Mistral-7B-v0.1,gsm8k_cot,5-shot,accuracy,0.41925701288855194,0.013591720959042111
mistralai/Mistral-7B-v0.1,anli_r2,0-shot,brier_score,0.7950624544675337,
mistralai/Mistral-7B-v0.1,anli_r3,0-shot,brier_score,0.7365319523049788,
mistralai/Mistral-7B-v0.1,anli_r1,0-shot,brier_score,0.824029011811061,
mistralai/Mistral-7B-v0.1,xnli_eu,0-shot,brier_score,0.9652658697204612,
mistralai/Mistral-7B-v0.1,xnli_vi,0-shot,brier_score,0.8766059818111679,
mistralai/Mistral-7B-v0.1,xnli_ru,0-shot,brier_score,0.7413504110189894,
mistralai/Mistral-7B-v0.1,xnli_zh,0-shot,brier_score,0.9609280684971038,
mistralai/Mistral-7B-v0.1,xnli_tr,0-shot,brier_score,0.8729771059898165,
mistralai/Mistral-7B-v0.1,xnli_fr,0-shot,brier_score,0.7650067503932926,
mistralai/Mistral-7B-v0.1,xnli_en,0-shot,brier_score,0.6632403264489223,
mistralai/Mistral-7B-v0.1,xnli_ur,0-shot,brier_score,1.2217226967863828,
mistralai/Mistral-7B-v0.1,xnli_ar,0-shot,brier_score,1.2723443593223103,
mistralai/Mistral-7B-v0.1,xnli_de,0-shot,brier_score,0.8115761663377433,
mistralai/Mistral-7B-v0.1,xnli_hi,0-shot,brier_score,0.8465004556210892,
mistralai/Mistral-7B-v0.1,xnli_es,0-shot,brier_score,0.8695701092578179,
mistralai/Mistral-7B-v0.1,xnli_bg,0-shot,brier_score,0.84232405643235,
mistralai/Mistral-7B-v0.1,xnli_sw,0-shot,brier_score,0.8955590100538611,
mistralai/Mistral-7B-v0.1,xnli_el,0-shot,brier_score,0.7705875419124961,
mistralai/Mistral-7B-v0.1,xnli_th,0-shot,brier_score,0.9058259849840192,
mistralai/Mistral-7B-v0.1,logiqa2,0-shot,brier_score,0.8994132704589166,
mistralai/Mistral-7B-v0.1,mathqa,5-shot,brier_score,0.8214663265573888,
mistralai/Mistral-7B-v0.1,lambada_standard,0-shot,perplexity,3.776806210204293,0.0726222723228476
mistralai/Mistral-7B-v0.1,lambada_standard,0-shot,accuracy,0.694352804191733,0.006418187162765859
mistralai/Mistral-7B-v0.1,lambada_openai,0-shot,perplexity,3.1806434293677626,0.058187731736352985
mistralai/Mistral-7B-v0.1,lambada_openai,0-shot,accuracy,0.7568406753347564,0.0059766767751295215
jisukim8873/falcon-7B-case-5,arc:challenge,25-shot,accuracy,0.4462457337883959,0.014526705548539982
jisukim8873/falcon-7B-case-5,arc:challenge,25-shot,acc_norm,0.48378839590443684,0.014603708567414947
jisukim8873/falcon-7B-case-5,hellaswag,10-shot,accuracy,0.5970922127066322,0.004894801119898607
jisukim8873/falcon-7B-case-5,hellaswag,10-shot,acc_norm,0.7851025692093209,0.004099117122280895
jisukim8873/falcon-7B-case-5,hendrycksTest-abstract_algebra,5-shot,accuracy,0.34,0.04760952285695236
jisukim8873/falcon-7B-case-5,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.34,0.04760952285695236
jisukim8873/falcon-7B-case-5,hendrycksTest-anatomy,5-shot,accuracy,0.31851851851851853,0.040247784019771096
jisukim8873/falcon-7B-case-5,hendrycksTest-anatomy,5-shot,acc_norm,0.31851851851851853,0.040247784019771096
jisukim8873/falcon-7B-case-5,hendrycksTest-astronomy,5-shot,accuracy,0.23684210526315788,0.03459777606810536
jisukim8873/falcon-7B-case-5,hendrycksTest-astronomy,5-shot,acc_norm,0.23684210526315788,0.03459777606810536
jisukim8873/falcon-7B-case-5,hendrycksTest-business_ethics,5-shot,accuracy,0.19,0.039427724440366234
jisukim8873/falcon-7B-case-5,hendrycksTest-business_ethics,5-shot,acc_norm,0.19,0.039427724440366234
jisukim8873/falcon-7B-case-5,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3169811320754717,0.028637235639800935
jisukim8873/falcon-7B-case-5,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3169811320754717,0.028637235639800935
jisukim8873/falcon-7B-case-5,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.03716177437566017
jisukim8873/falcon-7B-case-5,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.03716177437566017
jisukim8873/falcon-7B-case-5,hendrycksTest-college_chemistry,5-shot,accuracy,0.16,0.03684529491774709
jisukim8873/falcon-7B-case-5,hendrycksTest-college_chemistry,5-shot,acc_norm,0.16,0.03684529491774709
jisukim8873/falcon-7B-case-5,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.04512608598542127
jisukim8873/falcon-7B-case-5,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.04512608598542127
jisukim8873/falcon-7B-case-5,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.044084400227680794
jisukim8873/falcon-7B-case-5,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.044084400227680794
jisukim8873/falcon-7B-case-5,hendrycksTest-college_medicine,5-shot,accuracy,0.2832369942196532,0.034355680560478746
jisukim8873/falcon-7B-case-5,hendrycksTest-college_medicine,5-shot,acc_norm,0.2832369942196532,0.034355680560478746
jisukim8873/falcon-7B-case-5,hendrycksTest-college_physics,5-shot,accuracy,0.16666666666666666,0.03708284662416545
jisukim8873/falcon-7B-case-5,hendrycksTest-college_physics,5-shot,acc_norm,0.16666666666666666,0.03708284662416545
jisukim8873/falcon-7B-case-5,hendrycksTest-computer_security,5-shot,accuracy,0.33,0.047258156262526045
jisukim8873/falcon-7B-case-5,hendrycksTest-computer_security,5-shot,acc_norm,0.33,0.047258156262526045
jisukim8873/falcon-7B-case-5,hendrycksTest-conceptual_physics,5-shot,accuracy,0.34893617021276596,0.031158522131357783
jisukim8873/falcon-7B-case-5,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.34893617021276596,0.031158522131357783
jisukim8873/falcon-7B-case-5,hendrycksTest-econometrics,5-shot,accuracy,0.24561403508771928,0.040493392977481425
jisukim8873/falcon-7B-case-5,hendrycksTest-econometrics,5-shot,acc_norm,0.24561403508771928,0.040493392977481425
jisukim8873/falcon-7B-case-5,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2689655172413793,0.036951833116502325
jisukim8873/falcon-7B-case-5,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2689655172413793,0.036951833116502325
jisukim8873/falcon-7B-case-5,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2671957671957672,0.02278967314577657
jisukim8873/falcon-7B-case-5,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2671957671957672,0.02278967314577657
jisukim8873/falcon-7B-case-5,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.03567016675276864
jisukim8873/falcon-7B-case-5,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.03567016675276864
jisukim8873/falcon-7B-case-5,hendrycksTest-global_facts,5-shot,accuracy,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-5,hendrycksTest-global_facts,5-shot,acc_norm,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_biology,5-shot,accuracy,0.3419354838709677,0.02698528957655274
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3419354838709677,0.02698528957655274
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.31527093596059114,0.03269080871970187
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.31527093596059114,0.03269080871970187
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.39,0.04902071300001975
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.39,0.04902071300001975
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_european_history,5-shot,accuracy,0.34545454545454546,0.037131580674819135
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.34545454545454546,0.037131580674819135
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_geography,5-shot,accuracy,0.2878787878787879,0.03225883512300993
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2878787878787879,0.03225883512300993
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.03257714077709662
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.03257714077709662
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.26153846153846155,0.02228214120420442
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.26153846153846155,0.02228214120420442
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25555555555555554,0.02659393910184408
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25555555555555554,0.02659393910184408
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2857142857142857,0.02934457250063435
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2857142857142857,0.02934457250063435
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.03631329803969654
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.03631329803969654
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_psychology,5-shot,accuracy,0.30091743119266057,0.019664751366802114
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.30091743119266057,0.019664751366802114
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2037037037037037,0.027467401804057986
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2037037037037037,0.027467401804057986
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_us_history,5-shot,accuracy,0.29411764705882354,0.03198001660115071
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.29411764705882354,0.03198001660115071
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3333333333333333,0.0306858205966108
jisukim8873/falcon-7B-case-5,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3333333333333333,0.0306858205966108
jisukim8873/falcon-7B-case-5,hendrycksTest-human_aging,5-shot,accuracy,0.4260089686098655,0.033188332862172806
jisukim8873/falcon-7B-case-5,hendrycksTest-human_aging,5-shot,acc_norm,0.4260089686098655,0.033188332862172806
jisukim8873/falcon-7B-case-5,hendrycksTest-human_sexuality,5-shot,accuracy,0.2824427480916031,0.03948406125768361
jisukim8873/falcon-7B-case-5,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2824427480916031,0.03948406125768361
jisukim8873/falcon-7B-case-5,hendrycksTest-international_law,5-shot,accuracy,0.36363636363636365,0.043913262867240704
jisukim8873/falcon-7B-case-5,hendrycksTest-international_law,5-shot,acc_norm,0.36363636363636365,0.043913262867240704
jisukim8873/falcon-7B-case-5,hendrycksTest-jurisprudence,5-shot,accuracy,0.3055555555555556,0.044531975073749834
jisukim8873/falcon-7B-case-5,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3055555555555556,0.044531975073749834
jisukim8873/falcon-7B-case-5,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25153374233128833,0.03408997886857529
jisukim8873/falcon-7B-case-5,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25153374233128833,0.03408997886857529
jisukim8873/falcon-7B-case-5,hendrycksTest-machine_learning,5-shot,accuracy,0.33035714285714285,0.04464285714285713
jisukim8873/falcon-7B-case-5,hendrycksTest-machine_learning,5-shot,acc_norm,0.33035714285714285,0.04464285714285713
jisukim8873/falcon-7B-case-5,hendrycksTest-management,5-shot,accuracy,0.30097087378640774,0.045416094465039476
jisukim8873/falcon-7B-case-5,hendrycksTest-management,5-shot,acc_norm,0.30097087378640774,0.045416094465039476
jisukim8873/falcon-7B-case-5,hendrycksTest-marketing,5-shot,accuracy,0.36752136752136755,0.03158539157745637
jisukim8873/falcon-7B-case-5,hendrycksTest-marketing,5-shot,acc_norm,0.36752136752136755,0.03158539157745637
jisukim8873/falcon-7B-case-5,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-5,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-5,hendrycksTest-miscellaneous,5-shot,accuracy,0.3614303959131545,0.017179601328900736
jisukim8873/falcon-7B-case-5,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3614303959131545,0.017179601328900736
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_disputes,5-shot,accuracy,0.3699421965317919,0.025992472029306383
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3699421965317919,0.025992472029306383
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24916201117318434,0.014465893829859924
jisukim8873/falcon-7B-case-5,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24916201117318434,0.014465893829859924
jisukim8873/falcon-7B-case-5,hendrycksTest-nutrition,5-shot,accuracy,0.3104575163398693,0.02649303322514589
jisukim8873/falcon-7B-case-5,hendrycksTest-nutrition,5-shot,acc_norm,0.3104575163398693,0.02649303322514589
jisukim8873/falcon-7B-case-5,hendrycksTest-philosophy,5-shot,accuracy,0.33762057877813506,0.026858825879488558
jisukim8873/falcon-7B-case-5,hendrycksTest-philosophy,5-shot,acc_norm,0.33762057877813506,0.026858825879488558
jisukim8873/falcon-7B-case-5,hendrycksTest-prehistory,5-shot,accuracy,0.3055555555555556,0.025630824975621344
jisukim8873/falcon-7B-case-5,hendrycksTest-prehistory,5-shot,acc_norm,0.3055555555555556,0.025630824975621344
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_accounting,5-shot,accuracy,0.24822695035460993,0.025770015644290392
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24822695035460993,0.025770015644290392
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_law,5-shot,accuracy,0.2907431551499348,0.011598062372851988
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_law,5-shot,acc_norm,0.2907431551499348,0.011598062372851988
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_medicine,5-shot,accuracy,0.17647058823529413,0.023157468308559356
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_medicine,5-shot,acc_norm,0.17647058823529413,0.023157468308559356
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_psychology,5-shot,accuracy,0.28921568627450983,0.018342529845275908
jisukim8873/falcon-7B-case-5,hendrycksTest-professional_psychology,5-shot,acc_norm,0.28921568627450983,0.018342529845275908
jisukim8873/falcon-7B-case-5,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.041723430387053825
jisukim8873/falcon-7B-case-5,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.041723430387053825
jisukim8873/falcon-7B-case-5,hendrycksTest-security_studies,5-shot,accuracy,0.2653061224489796,0.028263889943784596
jisukim8873/falcon-7B-case-5,hendrycksTest-security_studies,5-shot,acc_norm,0.2653061224489796,0.028263889943784596
jisukim8873/falcon-7B-case-5,hendrycksTest-sociology,5-shot,accuracy,0.31343283582089554,0.03280188205348643
jisukim8873/falcon-7B-case-5,hendrycksTest-sociology,5-shot,acc_norm,0.31343283582089554,0.03280188205348643
jisukim8873/falcon-7B-case-5,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.42,0.049604496374885836
jisukim8873/falcon-7B-case-5,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.42,0.049604496374885836
jisukim8873/falcon-7B-case-5,hendrycksTest-virology,5-shot,accuracy,0.37349397590361444,0.037658451171688624
jisukim8873/falcon-7B-case-5,hendrycksTest-virology,5-shot,acc_norm,0.37349397590361444,0.037658451171688624
jisukim8873/falcon-7B-case-5,hendrycksTest-world_religions,5-shot,accuracy,0.3567251461988304,0.03674013002860954
jisukim8873/falcon-7B-case-5,hendrycksTest-world_religions,5-shot,acc_norm,0.3567251461988304,0.03674013002860954
jisukim8873/falcon-7B-case-5,truthfulqa:mc,0-shot,mc1,0.2484700122399021,0.01512742709652069
jisukim8873/falcon-7B-case-5,truthfulqa:mc,0-shot,mc2,0.36034965375475575,0.01417459244672947
jisukim8873/falcon-7B-case-5,winogrande,5-shot,accuracy,0.7182320441988951,0.012643326011852946
jisukim8873/falcon-7B-case-5,gsm8k,5-shot,accuracy,0.07960576194086429,0.007455924338676244
jisukim8873/falcon-7B-case-5,minerva_math_precalc,5-shot,accuracy,0.007326007326007326,0.0036529080893830334
jisukim8873/falcon-7B-case-5,minerva_math_prealgebra,5-shot,accuracy,0.03559127439724455,0.006281201252709593
jisukim8873/falcon-7B-case-5,minerva_math_num_theory,5-shot,accuracy,0.018518518518518517,0.005806972807912272
jisukim8873/falcon-7B-case-5,minerva_math_intermediate_algebra,5-shot,accuracy,0.01107419712070875,0.0034844537978317943
jisukim8873/falcon-7B-case-5,minerva_math_geometry,5-shot,accuracy,0.006263048016701462,0.003608399732887869
jisukim8873/falcon-7B-case-5,minerva_math_counting_and_prob,5-shot,accuracy,0.008438818565400843,0.004206007207713057
jisukim8873/falcon-7B-case-5,minerva_math_algebra,5-shot,accuracy,0.011794439764111205,0.003134873065301043
jisukim8873/falcon-7B-case-5,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
jisukim8873/falcon-7B-case-5,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339484
jisukim8873/falcon-7B-case-5,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
jisukim8873/falcon-7B-case-5,arithmetic_2ds,5-shot,accuracy,0.0125,0.00248494717876267
jisukim8873/falcon-7B-case-5,arithmetic_5ds,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_1dc,5-shot,accuracy,0.0565,0.0051640302675624965
jisukim8873/falcon-7B-case-5,arithmetic_4ds,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-5,arithmetic_2dm,5-shot,accuracy,0.041,0.004435012363831016
jisukim8873/falcon-7B-case-5,arithmetic_2da,5-shot,accuracy,0.0095,0.002169614853910038
jisukim8873/falcon-7B-case-5,gsm8k_cot,5-shot,accuracy,0.10614101592115238,0.008484346948434586
jisukim8873/falcon-7B-case-5,anli_r2,0-shot,brier_score,0.9513281547700652,
jisukim8873/falcon-7B-case-5,anli_r3,0-shot,brier_score,0.8632624385858723,
jisukim8873/falcon-7B-case-5,anli_r1,0-shot,brier_score,0.9747999915331574,
jisukim8873/falcon-7B-case-5,xnli_eu,0-shot,brier_score,1.0516363750130937,
jisukim8873/falcon-7B-case-5,xnli_vi,0-shot,brier_score,0.9989937908225948,
jisukim8873/falcon-7B-case-5,xnli_ru,0-shot,brier_score,0.8313811181787544,
jisukim8873/falcon-7B-case-5,xnli_zh,0-shot,brier_score,0.9903379657251044,
jisukim8873/falcon-7B-case-5,xnli_tr,0-shot,brier_score,0.9686053353297328,
jisukim8873/falcon-7B-case-5,xnli_fr,0-shot,brier_score,0.7336352662626028,
jisukim8873/falcon-7B-case-5,xnli_en,0-shot,brier_score,0.6661677143555013,
jisukim8873/falcon-7B-case-5,xnli_ur,0-shot,brier_score,1.2696545502356362,
jisukim8873/falcon-7B-case-5,xnli_ar,0-shot,brier_score,1.2884254322323176,
jisukim8873/falcon-7B-case-5,xnli_de,0-shot,brier_score,0.8278491965645761,
jisukim8873/falcon-7B-case-5,xnli_hi,0-shot,brier_score,1.0473020786937401,
jisukim8873/falcon-7B-case-5,xnli_es,0-shot,brier_score,0.8100639118042829,
jisukim8873/falcon-7B-case-5,xnli_bg,0-shot,brier_score,0.9677419215546682,
jisukim8873/falcon-7B-case-5,xnli_sw,0-shot,brier_score,1.1267158887404765,
jisukim8873/falcon-7B-case-5,xnli_el,0-shot,brier_score,0.9376379182808555,
jisukim8873/falcon-7B-case-5,xnli_th,0-shot,brier_score,0.9937338487237533,
jisukim8873/falcon-7B-case-5,logiqa2,0-shot,brier_score,1.07732917197947,
jisukim8873/falcon-7B-case-5,mathqa,5-shot,brier_score,0.9433681905807897,
jisukim8873/falcon-7B-case-5,lambada_standard,0-shot,perplexity,4.154513384117848,0.09112783779974745
jisukim8873/falcon-7B-case-5,lambada_standard,0-shot,accuracy,0.6679604114108286,0.006561186280791164
jisukim8873/falcon-7B-case-5,lambada_openai,0-shot,perplexity,3.231212255722818,0.06690211574319113
jisukim8873/falcon-7B-case-5,lambada_openai,0-shot,accuracy,0.7393751212885697,0.006115788029333531
facebook/xglm-4.5B,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_prealgebra,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0011074197120708748,0.0011074197120708796
facebook/xglm-4.5B,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-4.5B,minerva_math_algebra,5-shot,accuracy,0.0,
facebook/xglm-4.5B,fld_default,0-shot,accuracy,0.0,
facebook/xglm-4.5B,fld_star,0-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_3da,5-shot,accuracy,0.0005,0.0005000000000000061
facebook/xglm-4.5B,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005000000000000116
facebook/xglm-4.5B,arithmetic_4da,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_2ds,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339508
facebook/xglm-4.5B,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-4.5B,arithmetic_2dm,5-shot,accuracy,0.006,0.0017272787111155192
facebook/xglm-4.5B,arithmetic_2da,5-shot,accuracy,0.0,
facebook/xglm-4.5B,gsm8k_cot,5-shot,accuracy,0.021986353297952996,0.004039162758110061
facebook/xglm-4.5B,gsm8k,5-shot,accuracy,0.002274450341167551,0.001312157814867431
facebook/xglm-4.5B,anli_r2,0-shot,brier_score,0.7423986272054851,
facebook/xglm-4.5B,anli_r3,0-shot,brier_score,0.753360865009507,
facebook/xglm-4.5B,anli_r1,0-shot,brier_score,0.7489020744444143,
facebook/xglm-4.5B,xnli_eu,0-shot,brier_score,0.8835776224752929,
facebook/xglm-4.5B,xnli_vi,0-shot,brier_score,0.734074056481032,
facebook/xglm-4.5B,xnli_ru,0-shot,brier_score,0.7752082188185605,
facebook/xglm-4.5B,xnli_zh,0-shot,brier_score,1.059743376070012,
facebook/xglm-4.5B,xnli_tr,0-shot,brier_score,0.7912014592670232,
facebook/xglm-4.5B,xnli_fr,0-shot,brier_score,0.7417811076459814,
facebook/xglm-4.5B,xnli_en,0-shot,brier_score,0.6323125293224192,
facebook/xglm-4.5B,xnli_ur,0-shot,brier_score,0.9364267567095893,
facebook/xglm-4.5B,xnli_ar,0-shot,brier_score,1.2526435424482893,
facebook/xglm-4.5B,xnli_de,0-shot,brier_score,0.8248883014366909,
facebook/xglm-4.5B,xnli_hi,0-shot,brier_score,0.7748000452700087,
facebook/xglm-4.5B,xnli_es,0-shot,brier_score,0.824750043726708,
facebook/xglm-4.5B,xnli_bg,0-shot,brier_score,0.7293483146313945,
facebook/xglm-4.5B,xnli_sw,0-shot,brier_score,0.7765272974570628,
facebook/xglm-4.5B,xnli_el,0-shot,brier_score,0.813931199828111,
facebook/xglm-4.5B,xnli_th,0-shot,brier_score,0.8560226403534275,
facebook/xglm-4.5B,logiqa2,0-shot,brier_score,1.1510780077217264,
facebook/xglm-4.5B,mathqa,5-shot,brier_score,0.9914166377643306,
facebook/xglm-4.5B,lambada_standard,0-shot,perplexity,9.728468384838203,0.27282567798166885
facebook/xglm-4.5B,lambada_standard,0-shot,accuracy,0.5152338443625073,0.006962743717451541
facebook/xglm-4.5B,lambada_openai,0-shot,perplexity,8.474112679424595,0.23447686069986642
facebook/xglm-4.5B,lambada_openai,0-shot,accuracy,0.526877547059965,0.00695590589621272
facebook/xglm-4.5B,mmlu_world_religions,0-shot,accuracy,0.21052631578947367,0.031267817146631786
facebook/xglm-4.5B,mmlu_formal_logic,0-shot,accuracy,0.1746031746031746,0.033954900208561116
facebook/xglm-4.5B,mmlu_prehistory,0-shot,accuracy,0.2654320987654321,0.024569223600460845
facebook/xglm-4.5B,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.01424263007057487
facebook/xglm-4.5B,mmlu_high_school_world_history,0-shot,accuracy,0.2320675105485232,0.027479744550808514
facebook/xglm-4.5B,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763006,0.023868003262500107
facebook/xglm-4.5B,mmlu_professional_law,0-shot,accuracy,0.26988265971316816,0.011337381084250416
facebook/xglm-4.5B,mmlu_logical_fallacies,0-shot,accuracy,0.3006134969325153,0.03602511318806771
facebook/xglm-4.5B,mmlu_high_school_us_history,0-shot,accuracy,0.2107843137254902,0.028626547912437378
facebook/xglm-4.5B,mmlu_philosophy,0-shot,accuracy,0.3022508038585209,0.026082700695399662
facebook/xglm-4.5B,mmlu_jurisprudence,0-shot,accuracy,0.23148148148148148,0.04077494709252626
facebook/xglm-4.5B,mmlu_international_law,0-shot,accuracy,0.34710743801652894,0.04345724570292534
facebook/xglm-4.5B,mmlu_high_school_european_history,0-shot,accuracy,0.21212121212121213,0.031922715695482995
facebook/xglm-4.5B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2538860103626943,0.03141024780565317
facebook/xglm-4.5B,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.027553614467863786
facebook/xglm-4.5B,mmlu_high_school_geography,0-shot,accuracy,0.2676767676767677,0.03154449888270286
facebook/xglm-4.5B,mmlu_high_school_psychology,0-shot,accuracy,0.25504587155963304,0.018688500856535836
facebook/xglm-4.5B,mmlu_public_relations,0-shot,accuracy,0.23636363636363636,0.04069306319721377
facebook/xglm-4.5B,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.04512608598542127
facebook/xglm-4.5B,mmlu_sociology,0-shot,accuracy,0.27860696517412936,0.0317005618349731
facebook/xglm-4.5B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24358974358974358,0.021763733684173937
facebook/xglm-4.5B,mmlu_security_studies,0-shot,accuracy,0.23673469387755103,0.027212835884073153
facebook/xglm-4.5B,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612379
facebook/xglm-4.5B,mmlu_human_sexuality,0-shot,accuracy,0.26717557251908397,0.03880848301082397
facebook/xglm-4.5B,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.041857744240220554
facebook/xglm-4.5B,mmlu_miscellaneous,0-shot,accuracy,0.20689655172413793,0.014485656041669173
facebook/xglm-4.5B,mmlu_marketing,0-shot,accuracy,0.23931623931623933,0.027951826808924333
facebook/xglm-4.5B,mmlu_management,0-shot,accuracy,0.2912621359223301,0.044986763205729224
facebook/xglm-4.5B,mmlu_nutrition,0-shot,accuracy,0.2581699346405229,0.025058503316958147
facebook/xglm-4.5B,mmlu_medical_genetics,0-shot,accuracy,0.26,0.04408440022768078
facebook/xglm-4.5B,mmlu_human_aging,0-shot,accuracy,0.21524663677130046,0.027584066602208274
facebook/xglm-4.5B,mmlu_professional_medicine,0-shot,accuracy,0.3272058823529412,0.02850145286039657
facebook/xglm-4.5B,mmlu_college_medicine,0-shot,accuracy,0.27167630057803466,0.03391750322321659
facebook/xglm-4.5B,mmlu_business_ethics,0-shot,accuracy,0.23,0.04229525846816505
facebook/xglm-4.5B,mmlu_clinical_knowledge,0-shot,accuracy,0.22641509433962265,0.025757559893106727
facebook/xglm-4.5B,mmlu_global_facts,0-shot,accuracy,0.34,0.047609522856952344
facebook/xglm-4.5B,mmlu_virology,0-shot,accuracy,0.1927710843373494,0.03070982405056527
facebook/xglm-4.5B,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.026684564340460997
facebook/xglm-4.5B,mmlu_college_physics,0-shot,accuracy,0.19607843137254902,0.03950581861179962
facebook/xglm-4.5B,mmlu_high_school_physics,0-shot,accuracy,0.26490066225165565,0.03603038545360385
facebook/xglm-4.5B,mmlu_high_school_biology,0-shot,accuracy,0.2645161290322581,0.02509189237885928
facebook/xglm-4.5B,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.037161774375660164
facebook/xglm-4.5B,mmlu_anatomy,0-shot,accuracy,0.32592592592592595,0.040491220417025055
facebook/xglm-4.5B,mmlu_college_chemistry,0-shot,accuracy,0.24,0.04292346959909283
facebook/xglm-4.5B,mmlu_computer_security,0-shot,accuracy,0.29,0.04560480215720684
facebook/xglm-4.5B,mmlu_college_computer_science,0-shot,accuracy,0.28,0.04512608598542128
facebook/xglm-4.5B,mmlu_astronomy,0-shot,accuracy,0.2236842105263158,0.033911609343436025
facebook/xglm-4.5B,mmlu_college_mathematics,0-shot,accuracy,0.28,0.045126085985421296
facebook/xglm-4.5B,mmlu_conceptual_physics,0-shot,accuracy,0.2425531914893617,0.028020226271200217
facebook/xglm-4.5B,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.04229525846816505
facebook/xglm-4.5B,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.045604802157206845
facebook/xglm-4.5B,mmlu_machine_learning,0-shot,accuracy,0.25,0.04109974682633932
facebook/xglm-4.5B,mmlu_high_school_chemistry,0-shot,accuracy,0.24630541871921183,0.030315099285617722
facebook/xglm-4.5B,mmlu_high_school_statistics,0-shot,accuracy,0.2777777777777778,0.03054674526495319
facebook/xglm-4.5B,mmlu_elementary_mathematics,0-shot,accuracy,0.25396825396825395,0.022418042891113946
facebook/xglm-4.5B,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
facebook/xglm-4.5B,mmlu_high_school_mathematics,0-shot,accuracy,0.24814814814814815,0.0263357394040558
facebook/xglm-4.5B,arc_challenge,25-shot,accuracy,0.2977815699658703,0.013363080107244485
facebook/xglm-4.5B,arc_challenge,25-shot,acc_norm,0.31313993174061433,0.0135526715436235
facebook/xglm-4.5B,hellaswag,10-shot,accuracy,0.4334793865763792,0.004945424771611598
facebook/xglm-4.5B,hellaswag,10-shot,acc_norm,0.5794662417845051,0.004926358564494565
facebook/xglm-4.5B,truthfulqa_mc2,0-shot,accuracy,0.3582664656425214,0.013774296665210408
facebook/xglm-4.5B,truthfulqa_gen,0-shot,bleu_max,11.647008551534103,0.5837506738382987
facebook/xglm-4.5B,truthfulqa_gen,0-shot,bleu_acc,0.2533659730722154,0.015225899340826847
facebook/xglm-4.5B,truthfulqa_gen,0-shot,bleu_diff,-3.651974700389702,0.47117969919233177
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge1_max,29.74676225067295,0.8122994874104966
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge1_acc,0.2766217870257038,0.015659605755326933
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge1_diff,-5.319592251781975,0.5675727855582745
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge2_max,17.451517261760348,0.7677122688036538
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge2_acc,0.18727050183598531,0.013657229868067026
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rouge2_diff,-6.283551976079485,0.6225195424994728
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rougeL_max,27.33216522895946,0.7816913352147911
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rougeL_acc,0.2741738066095471,0.01561651849721937
facebook/xglm-4.5B,truthfulqa_gen,0-shot,rougeL_diff,-5.342717008492413,0.5591802833523164
facebook/xglm-4.5B,truthfulqa_mc1,0-shot,accuracy,0.20807833537331702,0.014210503473576613
facebook/xglm-4.5B,winogrande,5-shot,accuracy,0.5493291239147593,0.013983928869040239
meta-llama/Llama-2-7b-hf,minerva_math_precalc,5-shot,accuracy,0.018315018315018316,0.00574369673165366
meta-llama/Llama-2-7b-hf,minerva_math_prealgebra,5-shot,accuracy,0.0711825487944891,0.008717507390607558
meta-llama/Llama-2-7b-hf,minerva_math_num_theory,5-shot,accuracy,0.018518518518518517,0.005806972807912266
meta-llama/Llama-2-7b-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.021040974529346623,0.004778723623319674
meta-llama/Llama-2-7b-hf,minerva_math_geometry,5-shot,accuracy,0.037578288100208766,0.008698357509032981
meta-llama/Llama-2-7b-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.02531645569620253,0.007222751926043707
meta-llama/Llama-2-7b-hf,minerva_math_algebra,5-shot,accuracy,0.03201347935973041,0.005111622218276094
meta-llama/Llama-2-7b-hf,fld_default,0-shot,accuracy,0.0,
meta-llama/Llama-2-7b-hf,fld_star,0-shot,accuracy,0.0,
meta-llama/Llama-2-7b-hf,arithmetic_3da,5-shot,accuracy,0.8525,0.007931161747394468
meta-llama/Llama-2-7b-hf,arithmetic_3ds,5-shot,accuracy,0.439,0.011099599116647338
meta-llama/Llama-2-7b-hf,arithmetic_4da,5-shot,accuracy,0.6725,0.010496521494368456
meta-llama/Llama-2-7b-hf,arithmetic_2ds,5-shot,accuracy,0.5025,0.01118299623099078
meta-llama/Llama-2-7b-hf,arithmetic_5ds,5-shot,accuracy,0.2475,0.00965238101349167
meta-llama/Llama-2-7b-hf,arithmetic_5da,5-shot,accuracy,0.405,0.010979425025334411
meta-llama/Llama-2-7b-hf,arithmetic_1dc,5-shot,accuracy,0.1845,0.008675684915577382
meta-llama/Llama-2-7b-hf,arithmetic_4ds,5-shot,accuracy,0.363,0.010755153958374458
meta-llama/Llama-2-7b-hf,arithmetic_2dm,5-shot,accuracy,0.1545,0.008083783073189481
meta-llama/Llama-2-7b-hf,arithmetic_2da,5-shot,accuracy,0.8885,0.007039790787172794
meta-llama/Llama-2-7b-hf,gsm8k_cot,5-shot,accuracy,0.14404852160727824,0.009672110973065277
meta-llama/Llama-2-7b-hf,gsm8k,5-shot,accuracy,0.14025777103866566,0.009565108281428644
meta-llama/Llama-2-7b-hf,anli_r2,0-shot,brier_score,0.6728312849024087,
meta-llama/Llama-2-7b-hf,anli_r3,0-shot,brier_score,0.675924163095268,
meta-llama/Llama-2-7b-hf,anli_r1,0-shot,brier_score,0.680913669643727,
meta-llama/Llama-2-7b-hf,xnli_eu,0-shot,brier_score,1.0405008687131228,
meta-llama/Llama-2-7b-hf,xnli_vi,0-shot,brier_score,0.9640582331183833,
meta-llama/Llama-2-7b-hf,xnli_ru,0-shot,brier_score,0.8785393767177124,
meta-llama/Llama-2-7b-hf,xnli_zh,0-shot,brier_score,0.9451672393270157,
meta-llama/Llama-2-7b-hf,xnli_tr,0-shot,brier_score,0.8861324289870143,
meta-llama/Llama-2-7b-hf,xnli_fr,0-shot,brier_score,0.7371020830089063,
meta-llama/Llama-2-7b-hf,xnli_en,0-shot,brier_score,0.6309192441010487,
meta-llama/Llama-2-7b-hf,xnli_ur,0-shot,brier_score,1.2513457477843288,
meta-llama/Llama-2-7b-hf,xnli_ar,0-shot,brier_score,1.0275160054419725,
meta-llama/Llama-2-7b-hf,xnli_de,0-shot,brier_score,0.7952906725023694,
meta-llama/Llama-2-7b-hf,xnli_hi,0-shot,brier_score,0.9207229324542658,
meta-llama/Llama-2-7b-hf,xnli_es,0-shot,brier_score,0.9614303254112365,
meta-llama/Llama-2-7b-hf,xnli_bg,0-shot,brier_score,0.8810276928274537,
meta-llama/Llama-2-7b-hf,xnli_sw,0-shot,brier_score,0.8940776492925108,
meta-llama/Llama-2-7b-hf,xnli_el,0-shot,brier_score,0.8013356806216894,
meta-llama/Llama-2-7b-hf,xnli_th,0-shot,brier_score,0.9641360198328498,
meta-llama/Llama-2-7b-hf,logiqa2,0-shot,brier_score,0.9310549072453368,
meta-llama/Llama-2-7b-hf,mathqa,5-shot,brier_score,0.8984744744150647,
meta-llama/Llama-2-7b-hf,lambada_standard,0-shot,perplexity,4.129570848882444,0.08224461333530494
meta-llama/Llama-2-7b-hf,lambada_standard,0-shot,accuracy,0.6821269163594023,0.006487412955192984
meta-llama/Llama-2-7b-hf,lambada_openai,0-shot,perplexity,3.395129491760032,0.06674310180079371
meta-llama/Llama-2-7b-hf,lambada_openai,0-shot,accuracy,0.7391810595769455,0.006117261570238603
meta-llama/Llama-2-7b-hf,mmlu_world_religions,0-shot,accuracy,0.695906432748538,0.03528211258245232
meta-llama/Llama-2-7b-hf,mmlu_formal_logic,0-shot,accuracy,0.29365079365079366,0.04073524322147126
meta-llama/Llama-2-7b-hf,mmlu_prehistory,0-shot,accuracy,0.5030864197530864,0.02782021415859437
meta-llama/Llama-2-7b-hf,mmlu_moral_scenarios,0-shot,accuracy,0.24022346368715083,0.014288343803925308
meta-llama/Llama-2-7b-hf,mmlu_high_school_world_history,0-shot,accuracy,0.6371308016877637,0.031299208255302136
meta-llama/Llama-2-7b-hf,mmlu_moral_disputes,0-shot,accuracy,0.5057803468208093,0.026917296179149123
meta-llama/Llama-2-7b-hf,mmlu_professional_law,0-shot,accuracy,0.3670143415906128,0.012310264244842129
meta-llama/Llama-2-7b-hf,mmlu_logical_fallacies,0-shot,accuracy,0.5153374233128835,0.03926522378708843
meta-llama/Llama-2-7b-hf,mmlu_high_school_us_history,0-shot,accuracy,0.5441176470588235,0.03495624522015478
meta-llama/Llama-2-7b-hf,mmlu_philosophy,0-shot,accuracy,0.5916398713826366,0.027917050748484634
meta-llama/Llama-2-7b-hf,mmlu_jurisprudence,0-shot,accuracy,0.5277777777777778,0.04826217294139894
meta-llama/Llama-2-7b-hf,mmlu_international_law,0-shot,accuracy,0.6528925619834711,0.043457245702925335
meta-llama/Llama-2-7b-hf,mmlu_high_school_european_history,0-shot,accuracy,0.6,0.03825460278380026
meta-llama/Llama-2-7b-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6787564766839378,0.033699508685490674
meta-llama/Llama-2-7b-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.4327731092436975,0.03218358107742613
meta-llama/Llama-2-7b-hf,mmlu_high_school_geography,0-shot,accuracy,0.494949494949495,0.035621707606254015
meta-llama/Llama-2-7b-hf,mmlu_high_school_psychology,0-shot,accuracy,0.6238532110091743,0.020769231968205074
meta-llama/Llama-2-7b-hf,mmlu_public_relations,0-shot,accuracy,0.5636363636363636,0.04750185058907296
meta-llama/Llama-2-7b-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.62,0.04878317312145633
meta-llama/Llama-2-7b-hf,mmlu_sociology,0-shot,accuracy,0.6517412935323383,0.033687874661154596
meta-llama/Llama-2-7b-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4512820512820513,0.025230381238934833
meta-llama/Llama-2-7b-hf,mmlu_security_studies,0-shot,accuracy,0.47346938775510206,0.03196412734523272
meta-llama/Llama-2-7b-hf,mmlu_professional_psychology,0-shot,accuracy,0.4444444444444444,0.02010258389588718
meta-llama/Llama-2-7b-hf,mmlu_human_sexuality,0-shot,accuracy,0.549618320610687,0.04363643698524779
meta-llama/Llama-2-7b-hf,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.04185774424022056
meta-llama/Llama-2-7b-hf,mmlu_miscellaneous,0-shot,accuracy,0.6436781609195402,0.017125853762755886
meta-llama/Llama-2-7b-hf,mmlu_marketing,0-shot,accuracy,0.6965811965811965,0.030118210106942666
meta-llama/Llama-2-7b-hf,mmlu_management,0-shot,accuracy,0.5436893203883495,0.049318019942204146
meta-llama/Llama-2-7b-hf,mmlu_nutrition,0-shot,accuracy,0.49673202614379086,0.02862930519400354
meta-llama/Llama-2-7b-hf,mmlu_medical_genetics,0-shot,accuracy,0.51,0.05024183937956911
meta-llama/Llama-2-7b-hf,mmlu_human_aging,0-shot,accuracy,0.5650224215246636,0.03327283370271345
meta-llama/Llama-2-7b-hf,mmlu_professional_medicine,0-shot,accuracy,0.5183823529411765,0.03035230339535196
meta-llama/Llama-2-7b-hf,mmlu_college_medicine,0-shot,accuracy,0.41040462427745666,0.03750757044895537
meta-llama/Llama-2-7b-hf,mmlu_business_ethics,0-shot,accuracy,0.53,0.05016135580465919
meta-llama/Llama-2-7b-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.46037735849056605,0.030676096599389188
meta-llama/Llama-2-7b-hf,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621505
meta-llama/Llama-2-7b-hf,mmlu_virology,0-shot,accuracy,0.42168674698795183,0.038444531817709175
meta-llama/Llama-2-7b-hf,mmlu_professional_accounting,0-shot,accuracy,0.3475177304964539,0.02840662780959095
meta-llama/Llama-2-7b-hf,mmlu_college_physics,0-shot,accuracy,0.24509803921568626,0.04280105837364395
meta-llama/Llama-2-7b-hf,mmlu_high_school_physics,0-shot,accuracy,0.31788079470198677,0.03802039760107903
meta-llama/Llama-2-7b-hf,mmlu_high_school_biology,0-shot,accuracy,0.4967741935483871,0.02844341422643833
meta-llama/Llama-2-7b-hf,mmlu_college_biology,0-shot,accuracy,0.4722222222222222,0.04174752578923185
meta-llama/Llama-2-7b-hf,mmlu_anatomy,0-shot,accuracy,0.4666666666666667,0.043097329010363554
meta-llama/Llama-2-7b-hf,mmlu_college_chemistry,0-shot,accuracy,0.34,0.04760952285695236
meta-llama/Llama-2-7b-hf,mmlu_computer_security,0-shot,accuracy,0.61,0.04902071300001975
meta-llama/Llama-2-7b-hf,mmlu_college_computer_science,0-shot,accuracy,0.34,0.047609522856952344
meta-llama/Llama-2-7b-hf,mmlu_astronomy,0-shot,accuracy,0.3881578947368421,0.03965842097512744
meta-llama/Llama-2-7b-hf,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215034
meta-llama/Llama-2-7b-hf,mmlu_conceptual_physics,0-shot,accuracy,0.4127659574468085,0.03218471141400351
meta-llama/Llama-2-7b-hf,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.04560480215720683
meta-llama/Llama-2-7b-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.39,0.049020713000019756
meta-llama/Llama-2-7b-hf,mmlu_machine_learning,0-shot,accuracy,0.375,0.04595091388086298
meta-llama/Llama-2-7b-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.37438423645320196,0.03405155380561952
meta-llama/Llama-2-7b-hf,mmlu_high_school_statistics,0-shot,accuracy,0.2638888888888889,0.030058202704309846
meta-llama/Llama-2-7b-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.2671957671957672,0.022789673145776564
meta-llama/Llama-2-7b-hf,mmlu_electrical_engineering,0-shot,accuracy,0.47586206896551725,0.041618085035015295
meta-llama/Llama-2-7b-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.3,0.02794045713622841
meta-llama/Llama-2-7b-hf,arc_challenge,25-shot,accuracy,0.4854948805460751,0.014605241081370053
meta-llama/Llama-2-7b-hf,arc_challenge,25-shot,acc_norm,0.5290102389078498,0.014586776355294323
meta-llama/Llama-2-7b-hf,hellaswag,10-shot,accuracy,0.5868352917745469,0.004913955705080136
meta-llama/Llama-2-7b-hf,hellaswag,10-shot,acc_norm,0.7866958773152758,0.004088034745195409
meta-llama/Llama-2-7b-hf,truthfulqa_mc2,0-shot,accuracy,0.38965409945411716,0.013577349838309216
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,bleu_max,30.794481532169428,0.8287919046619883
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,bleu_acc,0.34394124847001223,0.016629087514276816
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,bleu_diff,-5.993291829792142,0.9641520859323807
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge1_max,56.463597234122524,0.8547674057001485
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge1_acc,0.3219094247246022,0.01635556761196043
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge1_diff,-7.067120288688261,1.0624470832704904
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge2_max,42.08844049121037,1.020295723228426
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge2_acc,0.30599755201958384,0.01613222972815502
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rouge2_diff,-8.293387411017884,1.2550691006847554
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rougeL_max,53.62297062326941,0.8821248084779468
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rougeL_acc,0.3243574051407589,0.01638797677964794
meta-llama/Llama-2-7b-hf,truthfulqa_gen,0-shot,rougeL_diff,-7.257582732253812,1.0716212651200012
meta-llama/Llama-2-7b-hf,truthfulqa_mc1,0-shot,accuracy,0.2521419828641371,0.015201522246299962
meta-llama/Llama-2-7b-hf,winogrande,5-shot,accuracy,0.7458563535911602,0.012236307219708269
cerebras/Cerebras-GPT-1.3B,minerva_math_precalc,5-shot,accuracy,0.009157509157509158,0.004080306065048968
cerebras/Cerebras-GPT-1.3B,minerva_math_prealgebra,5-shot,accuracy,0.012629161882893225,0.0037858882182630017
cerebras/Cerebras-GPT-1.3B,minerva_math_num_theory,5-shot,accuracy,0.012962962962962963,0.004872192984581503
cerebras/Cerebras-GPT-1.3B,minerva_math_intermediate_algebra,5-shot,accuracy,0.015503875968992248,0.004113617238360454
cerebras/Cerebras-GPT-1.3B,minerva_math_geometry,5-shot,accuracy,0.006263048016701462,0.003608399732887888
cerebras/Cerebras-GPT-1.3B,minerva_math_counting_and_prob,5-shot,accuracy,0.010548523206751054,0.004697453735376145
cerebras/Cerebras-GPT-1.3B,minerva_math_algebra,5-shot,accuracy,0.009267059814658803,0.0027823191184887844
cerebras/Cerebras-GPT-1.3B,fld_default,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,fld_star,0-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
cerebras/Cerebras-GPT-1.3B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
cerebras/Cerebras-GPT-1.3B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
cerebras/Cerebras-GPT-1.3B,arithmetic_2ds,5-shot,accuracy,0.013,0.0025335171905233197
cerebras/Cerebras-GPT-1.3B,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_1dc,5-shot,accuracy,0.0275,0.0036576719757437743
cerebras/Cerebras-GPT-1.3B,arithmetic_4ds,5-shot,accuracy,0.0,
cerebras/Cerebras-GPT-1.3B,arithmetic_2dm,5-shot,accuracy,0.022,0.0032807593162018913
cerebras/Cerebras-GPT-1.3B,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237512
cerebras/Cerebras-GPT-1.3B,gsm8k_cot,5-shot,accuracy,0.014404852160727824,0.003282055917136958
cerebras/Cerebras-GPT-1.3B,gsm8k,5-shot,accuracy,0.002274450341167551,0.0013121578148673984
cerebras/Cerebras-GPT-1.3B,anli_r2,0-shot,brier_score,0.8160206698968121,
cerebras/Cerebras-GPT-1.3B,anli_r3,0-shot,brier_score,0.7564486854420894,
cerebras/Cerebras-GPT-1.3B,anli_r1,0-shot,brier_score,0.8453036236123399,
cerebras/Cerebras-GPT-1.3B,xnli_eu,0-shot,brier_score,1.203122941977538,
cerebras/Cerebras-GPT-1.3B,xnli_vi,0-shot,brier_score,0.9581888474081791,
cerebras/Cerebras-GPT-1.3B,xnli_ru,0-shot,brier_score,0.8555274102354892,
cerebras/Cerebras-GPT-1.3B,xnli_zh,0-shot,brier_score,0.9525134852831153,
cerebras/Cerebras-GPT-1.3B,xnli_tr,0-shot,brier_score,0.8488424294763287,
cerebras/Cerebras-GPT-1.3B,xnli_fr,0-shot,brier_score,0.9004072134991292,
cerebras/Cerebras-GPT-1.3B,xnli_en,0-shot,brier_score,0.676735865099788,
cerebras/Cerebras-GPT-1.3B,xnli_ur,0-shot,brier_score,1.119118044439167,
cerebras/Cerebras-GPT-1.3B,xnli_ar,0-shot,brier_score,1.0334678129216435,
cerebras/Cerebras-GPT-1.3B,xnli_de,0-shot,brier_score,0.8840107844662859,
cerebras/Cerebras-GPT-1.3B,xnli_hi,0-shot,brier_score,0.9489724257727796,
cerebras/Cerebras-GPT-1.3B,xnli_es,0-shot,brier_score,0.9940572286363858,
cerebras/Cerebras-GPT-1.3B,xnli_bg,0-shot,brier_score,0.7972760255131902,
cerebras/Cerebras-GPT-1.3B,xnli_sw,0-shot,brier_score,0.9917977124323483,
cerebras/Cerebras-GPT-1.3B,xnli_el,0-shot,brier_score,1.2932784189714346,
cerebras/Cerebras-GPT-1.3B,xnli_th,0-shot,brier_score,0.7809459010580413,
cerebras/Cerebras-GPT-1.3B,logiqa2,0-shot,brier_score,1.2019264190394812,
cerebras/Cerebras-GPT-1.3B,mathqa,5-shot,brier_score,0.9930432217011608,
cerebras/Cerebras-GPT-1.3B,lambada_standard,0-shot,perplexity,30.331273764614334,1.0890794817806768
cerebras/Cerebras-GPT-1.3B,lambada_standard,0-shot,accuracy,0.3584319813700757,0.006680928173680374
cerebras/Cerebras-GPT-1.3B,lambada_openai,0-shot,perplexity,14.11714896411139,0.4545475556587265
cerebras/Cerebras-GPT-1.3B,lambada_openai,0-shot,accuracy,0.4614787502425771,0.006945273445805861
cerebras/Cerebras-GPT-1.3B,mmlu_world_religions,0-shot,accuracy,0.30409356725146197,0.03528211258245231
cerebras/Cerebras-GPT-1.3B,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.039325376803928724
cerebras/Cerebras-GPT-1.3B,mmlu_prehistory,0-shot,accuracy,0.2623456790123457,0.024477222856135124
cerebras/Cerebras-GPT-1.3B,mmlu_moral_scenarios,0-shot,accuracy,0.23687150837988827,0.014219570788103982
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.029041333510598046
cerebras/Cerebras-GPT-1.3B,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763006,0.02386800326250011
cerebras/Cerebras-GPT-1.3B,mmlu_professional_law,0-shot,accuracy,0.2379400260756193,0.01087570078769423
cerebras/Cerebras-GPT-1.3B,mmlu_logical_fallacies,0-shot,accuracy,0.3006134969325153,0.03602511318806771
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_us_history,0-shot,accuracy,0.22549019607843138,0.029331162294251728
cerebras/Cerebras-GPT-1.3B,mmlu_philosophy,0-shot,accuracy,0.26688102893890675,0.025122637608816643
cerebras/Cerebras-GPT-1.3B,mmlu_jurisprudence,0-shot,accuracy,0.24074074074074073,0.041331194402438376
cerebras/Cerebras-GPT-1.3B,mmlu_international_law,0-shot,accuracy,0.2727272727272727,0.04065578140908705
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_european_history,0-shot,accuracy,0.26666666666666666,0.03453131801885415
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.34196891191709844,0.03423465100104282
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_microeconomics,0-shot,accuracy,0.22268907563025211,0.027025433498882357
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_geography,0-shot,accuracy,0.31313131313131315,0.03304205087813652
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_psychology,0-shot,accuracy,0.3431192660550459,0.020354777736086037
cerebras/Cerebras-GPT-1.3B,mmlu_public_relations,0-shot,accuracy,0.16363636363636364,0.035434330542986794
cerebras/Cerebras-GPT-1.3B,mmlu_us_foreign_policy,0-shot,accuracy,0.2,0.04020151261036843
cerebras/Cerebras-GPT-1.3B,mmlu_sociology,0-shot,accuracy,0.22885572139303484,0.02970528405677245
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3230769230769231,0.023710888501970565
cerebras/Cerebras-GPT-1.3B,mmlu_security_studies,0-shot,accuracy,0.23265306122448978,0.02704925791589618
cerebras/Cerebras-GPT-1.3B,mmlu_professional_psychology,0-shot,accuracy,0.272875816993464,0.01802047414839358
cerebras/Cerebras-GPT-1.3B,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596918
cerebras/Cerebras-GPT-1.3B,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748141
cerebras/Cerebras-GPT-1.3B,mmlu_miscellaneous,0-shot,accuracy,0.24776500638569604,0.015438083080568965
cerebras/Cerebras-GPT-1.3B,mmlu_marketing,0-shot,accuracy,0.2905982905982906,0.029745048572674057
cerebras/Cerebras-GPT-1.3B,mmlu_management,0-shot,accuracy,0.20388349514563106,0.0398913985953177
cerebras/Cerebras-GPT-1.3B,mmlu_nutrition,0-shot,accuracy,0.2973856209150327,0.02617390850671858
cerebras/Cerebras-GPT-1.3B,mmlu_medical_genetics,0-shot,accuracy,0.28,0.04512608598542127
cerebras/Cerebras-GPT-1.3B,mmlu_human_aging,0-shot,accuracy,0.242152466367713,0.028751392398694755
cerebras/Cerebras-GPT-1.3B,mmlu_professional_medicine,0-shot,accuracy,0.41911764705882354,0.02997280717046463
cerebras/Cerebras-GPT-1.3B,mmlu_college_medicine,0-shot,accuracy,0.2774566473988439,0.034140140070440354
cerebras/Cerebras-GPT-1.3B,mmlu_business_ethics,0-shot,accuracy,0.22,0.041633319989322716
cerebras/Cerebras-GPT-1.3B,mmlu_clinical_knowledge,0-shot,accuracy,0.27169811320754716,0.027377706624670713
cerebras/Cerebras-GPT-1.3B,mmlu_global_facts,0-shot,accuracy,0.31,0.04648231987117316
cerebras/Cerebras-GPT-1.3B,mmlu_virology,0-shot,accuracy,0.3313253012048193,0.03664314777288085
cerebras/Cerebras-GPT-1.3B,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.026684564340461008
cerebras/Cerebras-GPT-1.3B,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.04158307533083286
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649006,0.0347918557259966
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_biology,0-shot,accuracy,0.2032258064516129,0.022891687984554963
cerebras/Cerebras-GPT-1.3B,mmlu_college_biology,0-shot,accuracy,0.2361111111111111,0.03551446610810826
cerebras/Cerebras-GPT-1.3B,mmlu_anatomy,0-shot,accuracy,0.2074074074074074,0.03502553170678317
cerebras/Cerebras-GPT-1.3B,mmlu_college_chemistry,0-shot,accuracy,0.24,0.04292346959909284
cerebras/Cerebras-GPT-1.3B,mmlu_computer_security,0-shot,accuracy,0.21,0.04093601807403325
cerebras/Cerebras-GPT-1.3B,mmlu_college_computer_science,0-shot,accuracy,0.34,0.04760952285695235
cerebras/Cerebras-GPT-1.3B,mmlu_astronomy,0-shot,accuracy,0.21052631578947367,0.033176727875331574
cerebras/Cerebras-GPT-1.3B,mmlu_college_mathematics,0-shot,accuracy,0.31,0.04648231987117316
cerebras/Cerebras-GPT-1.3B,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.02951319662553935
cerebras/Cerebras-GPT-1.3B,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.04560480215720684
cerebras/Cerebras-GPT-1.3B,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.04327040932578728
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_chemistry,0-shot,accuracy,0.23645320197044334,0.029896114291733552
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
cerebras/Cerebras-GPT-1.3B,mmlu_elementary_mathematics,0-shot,accuracy,0.24074074074074073,0.0220190800122179
cerebras/Cerebras-GPT-1.3B,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
cerebras/Cerebras-GPT-1.3B,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.026842057873833706
cerebras/Cerebras-GPT-1.3B,arc_challenge,25-shot,accuracy,0.24744027303754265,0.01261035266329267
cerebras/Cerebras-GPT-1.3B,arc_challenge,25-shot,acc_norm,0.2738907849829352,0.013032004972989503
cerebras/Cerebras-GPT-1.3B,hellaswag,10-shot,accuracy,0.32901812387970525,0.004688963175758139
cerebras/Cerebras-GPT-1.3B,hellaswag,10-shot,acc_norm,0.385381398127863,0.004856906473719379
cerebras/Cerebras-GPT-1.3B,truthfulqa_mc2,0-shot,accuracy,0.4270182875007654,0.014896443872403905
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,bleu_max,15.75896364576763,0.5364357683647447
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,bleu_acc,0.3598531211750306,0.016801860466677174
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,bleu_diff,-3.3445863219845346,0.5222101463879798
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge1_max,36.903769503650345,0.7998635375855746
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge1_acc,0.2913096695226438,0.01590598704818482
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge1_diff,-6.460703210665295,0.7328034463145772
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge2_max,19.326584406586324,0.8077506474590068
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge2_acc,0.18237454100367198,0.013518055636187212
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rouge2_diff,-6.646991226024485,0.7429900455011789
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rougeL_max,33.790056148847405,0.78334334845043
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rougeL_acc,0.28518971848225216,0.015805827874454892
cerebras/Cerebras-GPT-1.3B,truthfulqa_gen,0-shot,rougeL_diff,-6.286118787315412,0.7141317626988594
cerebras/Cerebras-GPT-1.3B,truthfulqa_mc1,0-shot,accuracy,0.24479804161566707,0.015051869486714997
cerebras/Cerebras-GPT-1.3B,winogrande,5-shot,accuracy,0.5343330702446725,0.014019317531542565
mistralai/Mixtral-8x7B-Instruct-v0.1,arc:challenge,25-shot,accuracy,0.6655290102389079,0.013787460322441377
mistralai/Mixtral-8x7B-Instruct-v0.1,arc:challenge,25-shot,acc_norm,0.7013651877133106,0.013374078615068738
mistralai/Mixtral-8x7B-Instruct-v0.1,hellaswag,10-shot,accuracy,0.6858195578570006,0.004632399677490809
mistralai/Mixtral-8x7B-Instruct-v0.1,hellaswag,10-shot,acc_norm,0.8755228042222665,0.003294504807555227
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.42,0.049604496374885836
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.42,0.049604496374885836
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.6666666666666666,0.04072314811876837
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.6666666666666666,0.04072314811876837
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.7894736842105263,0.03317672787533157
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.7894736842105263,0.03317672787533157
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.73,0.044619604333847394
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.73,0.044619604333847394
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7773584905660378,0.025604233470899098
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7773584905660378,0.025604233470899098
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.8263888888888888,0.03167473383795718
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.8263888888888888,0.03167473383795718
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.5,0.050251890762960605
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.5,0.050251890762960605
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.66,0.04760952285695237
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.66,0.04760952285695237
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.46,0.05009082659620332
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.46,0.05009082659620332
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.7572254335260116,0.0326926380614177
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.7572254335260116,0.0326926380614177
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.43137254901960786,0.04928099597287534
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.43137254901960786,0.04928099597287534
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.81,0.039427724440366234
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.81,0.039427724440366234
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6680851063829787,0.03078373675774564
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6680851063829787,0.03078373675774564
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.6140350877192983,0.04579639422070434
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.6140350877192983,0.04579639422070434
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6482758620689655,0.0397923663749741
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6482758620689655,0.0397923663749741
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.47883597883597884,0.025728230952130726
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.47883597883597884,0.025728230952130726
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.5238095238095238,0.04467062628403273
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.5238095238095238,0.04467062628403273
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.42,0.049604496374885836
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.42,0.049604496374885836
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.8516129032258064,0.020222737554330378
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8516129032258064,0.020222737554330378
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6206896551724138,0.034139638059062345
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6206896551724138,0.034139638059062345
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.78,0.041633319989322626
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.78,0.041633319989322626
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8,0.03123475237772117
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8,0.03123475237772117
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.8686868686868687,0.024063156416822523
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8686868686868687,0.024063156416822523
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9585492227979274,0.01438543285747646
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9585492227979274,0.01438543285747646
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6974358974358974,0.02329088805377272
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6974358974358974,0.02329088805377272
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3888888888888889,0.029723278961476664
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3888888888888889,0.029723278961476664
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8025210084033614,0.02585916412205145
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8025210084033614,0.02585916412205145
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.47019867549668876,0.040752249922169775
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.47019867549668876,0.040752249922169775
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8844036697247707,0.013708749534172636
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8844036697247707,0.013708749534172636
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5972222222222222,0.03344887382997866
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5972222222222222,0.03344887382997866
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8529411764705882,0.024857478080250447
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8529411764705882,0.024857478080250447
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8481012658227848,0.023363878096632446
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8481012658227848,0.023363878096632446
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.757847533632287,0.028751392398694755
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.757847533632287,0.028751392398694755
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.8091603053435115,0.034465133507525975
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8091603053435115,0.034465133507525975
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.8760330578512396,0.030083098716035202
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.8760330578512396,0.030083098716035202
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.03520703990517963
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.03520703990517963
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8159509202453987,0.030446777687971716
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8159509202453987,0.030446777687971716
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.5714285714285714,0.04697113923010213
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.5714285714285714,0.04697113923010213
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-management,5-shot,accuracy,0.8446601941747572,0.035865947385739734
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-management,5-shot,acc_norm,0.8446601941747572,0.035865947385739734
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.9230769230769231,0.017456987872436193
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.9230769230769231,0.017456987872436193
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.77,0.042295258468165065
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.77,0.042295258468165065
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.879948914431673,0.011622736692041287
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.879948914431673,0.011622736692041287
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.7803468208092486,0.022289638852617897
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7803468208092486,0.022289638852617897
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.46033519553072627,0.016669799592112032
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.46033519553072627,0.016669799592112032
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.8202614379084967,0.02198603218206415
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.8202614379084967,0.02198603218206415
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.797427652733119,0.022827317491059686
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.797427652733119,0.022827317491059686
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.8333333333333334,0.020736358408060006
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.8333333333333334,0.020736358408060006
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.5531914893617021,0.029658235097666907
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5531914893617021,0.029658235097666907
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.5443285528031291,0.012719949543032228
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.5443285528031291,0.012719949543032228
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.7941176470588235,0.02456220431414231
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.7941176470588235,0.02456220431414231
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.7647058823529411,0.01716058723504635
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7647058823529411,0.01716058723504635
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.7090909090909091,0.04350271442923243
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.7090909090909091,0.04350271442923243
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.7714285714285715,0.02688214492230774
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.7714285714285715,0.02688214492230774
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.02207632610182466
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.02207632610182466
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.9,0.030151134457776334
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.9,0.030151134457776334
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5060240963855421,0.03892212195333045
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5060240963855421,0.03892212195333045
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.02517298435015577
mistralai/Mixtral-8x7B-Instruct-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.02517298435015577
mistralai/Mixtral-8x7B-Instruct-v0.1,truthfulqa:mc,0-shot,mc1,0.5006119951040392,0.01750348793889251
mistralai/Mixtral-8x7B-Instruct-v0.1,truthfulqa:mc,0-shot,mc2,0.649788114114722,0.015119260704075871
mistralai/Mixtral-8x7B-Instruct-v0.1,winogrande,5-shot,accuracy,0.8105761641673244,0.011012790432989247
mistralai/Mixtral-8x7B-Instruct-v0.1,gsm8k,5-shot,accuracy,0.6110689916603488,0.01342838248127424
jb723/cross_lingual_epoch2,arc:challenge,25-shot,accuracy,0.3293515358361775,0.013734057652635474
jb723/cross_lingual_epoch2,arc:challenge,25-shot,acc_norm,0.3924914675767918,0.014269634635670714
jb723/cross_lingual_epoch2,hellaswag,10-shot,accuracy,0.3392750448117905,0.004724956665879986
jb723/cross_lingual_epoch2,hellaswag,10-shot,acc_norm,0.47918741286596295,0.004985456752161
jb723/cross_lingual_epoch2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.04512608598542128
jb723/cross_lingual_epoch2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.04512608598542128
jb723/cross_lingual_epoch2,hendrycksTest-anatomy,5-shot,accuracy,0.4148148148148148,0.04256193767901407
jb723/cross_lingual_epoch2,hendrycksTest-anatomy,5-shot,acc_norm,0.4148148148148148,0.04256193767901407
jb723/cross_lingual_epoch2,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.037385206761196686
jb723/cross_lingual_epoch2,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.037385206761196686
jb723/cross_lingual_epoch2,hendrycksTest-business_ethics,5-shot,accuracy,0.4,0.04923659639173309
jb723/cross_lingual_epoch2,hendrycksTest-business_ethics,5-shot,acc_norm,0.4,0.04923659639173309
jb723/cross_lingual_epoch2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4339622641509434,0.030503292013342596
jb723/cross_lingual_epoch2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4339622641509434,0.030503292013342596
jb723/cross_lingual_epoch2,hendrycksTest-college_biology,5-shot,accuracy,0.3125,0.038760854559127644
jb723/cross_lingual_epoch2,hendrycksTest-college_biology,5-shot,acc_norm,0.3125,0.038760854559127644
jb723/cross_lingual_epoch2,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.038612291966536955
jb723/cross_lingual_epoch2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.038612291966536955
jb723/cross_lingual_epoch2,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.04688261722621504
jb723/cross_lingual_epoch2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.04688261722621504
jb723/cross_lingual_epoch2,hendrycksTest-college_mathematics,5-shot,accuracy,0.29,0.045604802157206845
jb723/cross_lingual_epoch2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.29,0.045604802157206845
jb723/cross_lingual_epoch2,hendrycksTest-college_medicine,5-shot,accuracy,0.3583815028901734,0.03656343653353158
jb723/cross_lingual_epoch2,hendrycksTest-college_medicine,5-shot,acc_norm,0.3583815028901734,0.03656343653353158
jb723/cross_lingual_epoch2,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617747
jb723/cross_lingual_epoch2,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617747
jb723/cross_lingual_epoch2,hendrycksTest-computer_security,5-shot,accuracy,0.48,0.05021167315686781
jb723/cross_lingual_epoch2,hendrycksTest-computer_security,5-shot,acc_norm,0.48,0.05021167315686781
jb723/cross_lingual_epoch2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.030363582197238167
jb723/cross_lingual_epoch2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.030363582197238167
jb723/cross_lingual_epoch2,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.04096985139843671
jb723/cross_lingual_epoch2,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.04096985139843671
jb723/cross_lingual_epoch2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.38620689655172413,0.04057324734419035
jb723/cross_lingual_epoch2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.38620689655172413,0.04057324734419035
jb723/cross_lingual_epoch2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.022494510767503154
jb723/cross_lingual_epoch2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.022494510767503154
jb723/cross_lingual_epoch2,hendrycksTest-formal_logic,5-shot,accuracy,0.30158730158730157,0.04104947269903394
jb723/cross_lingual_epoch2,hendrycksTest-formal_logic,5-shot,acc_norm,0.30158730158730157,0.04104947269903394
jb723/cross_lingual_epoch2,hendrycksTest-global_facts,5-shot,accuracy,0.29,0.045604802157206824
jb723/cross_lingual_epoch2,hendrycksTest-global_facts,5-shot,acc_norm,0.29,0.045604802157206824
jb723/cross_lingual_epoch2,hendrycksTest-high_school_biology,5-shot,accuracy,0.4032258064516129,0.027906150826041143
jb723/cross_lingual_epoch2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4032258064516129,0.027906150826041143
jb723/cross_lingual_epoch2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.24630541871921183,0.03031509928561773
jb723/cross_lingual_epoch2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.24630541871921183,0.03031509928561773
jb723/cross_lingual_epoch2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.35,0.0479372485441102
jb723/cross_lingual_epoch2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
jb723/cross_lingual_epoch2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3696969696969697,0.03769430314512568
jb723/cross_lingual_epoch2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3696969696969697,0.03769430314512568
jb723/cross_lingual_epoch2,hendrycksTest-high_school_geography,5-shot,accuracy,0.3787878787878788,0.03456088731993747
jb723/cross_lingual_epoch2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3787878787878788,0.03456088731993747
jb723/cross_lingual_epoch2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.49222797927461137,0.03608003225569653
jb723/cross_lingual_epoch2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.49222797927461137,0.03608003225569653
jb723/cross_lingual_epoch2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2923076923076923,0.023060438380857744
jb723/cross_lingual_epoch2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2923076923076923,0.023060438380857744
jb723/cross_lingual_epoch2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2074074074074074,0.02472071319395216
jb723/cross_lingual_epoch2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2074074074074074,0.02472071319395216
jb723/cross_lingual_epoch2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.36134453781512604,0.031204691225150023
jb723/cross_lingual_epoch2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.36134453781512604,0.031204691225150023
jb723/cross_lingual_epoch2,hendrycksTest-high_school_physics,5-shot,accuracy,0.23841059602649006,0.0347918557259966
jb723/cross_lingual_epoch2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23841059602649006,0.0347918557259966
jb723/cross_lingual_epoch2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.43853211009174314,0.021274713073954572
jb723/cross_lingual_epoch2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.43853211009174314,0.021274713073954572
jb723/cross_lingual_epoch2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.18981481481481483,0.026744714834691926
jb723/cross_lingual_epoch2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.18981481481481483,0.026744714834691926
jb723/cross_lingual_epoch2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.37745098039215685,0.03402272044340703
jb723/cross_lingual_epoch2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.37745098039215685,0.03402272044340703
jb723/cross_lingual_epoch2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.4388185654008439,0.032302649315470375
jb723/cross_lingual_epoch2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.4388185654008439,0.032302649315470375
jb723/cross_lingual_epoch2,hendrycksTest-human_aging,5-shot,accuracy,0.49327354260089684,0.03355476596234354
jb723/cross_lingual_epoch2,hendrycksTest-human_aging,5-shot,acc_norm,0.49327354260089684,0.03355476596234354
jb723/cross_lingual_epoch2,hendrycksTest-human_sexuality,5-shot,accuracy,0.40458015267175573,0.043046937953806645
jb723/cross_lingual_epoch2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.40458015267175573,0.043046937953806645
jb723/cross_lingual_epoch2,hendrycksTest-international_law,5-shot,accuracy,0.5206611570247934,0.04560456086387235
jb723/cross_lingual_epoch2,hendrycksTest-international_law,5-shot,acc_norm,0.5206611570247934,0.04560456086387235
jb723/cross_lingual_epoch2,hendrycksTest-jurisprudence,5-shot,accuracy,0.4444444444444444,0.04803752235190193
jb723/cross_lingual_epoch2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.4444444444444444,0.04803752235190193
jb723/cross_lingual_epoch2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3558282208588957,0.03761521380046734
jb723/cross_lingual_epoch2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3558282208588957,0.03761521380046734
jb723/cross_lingual_epoch2,hendrycksTest-machine_learning,5-shot,accuracy,0.3125,0.043994650575715215
jb723/cross_lingual_epoch2,hendrycksTest-machine_learning,5-shot,acc_norm,0.3125,0.043994650575715215
jb723/cross_lingual_epoch2,hendrycksTest-management,5-shot,accuracy,0.36893203883495146,0.0477761518115674
jb723/cross_lingual_epoch2,hendrycksTest-management,5-shot,acc_norm,0.36893203883495146,0.0477761518115674
jb723/cross_lingual_epoch2,hendrycksTest-marketing,5-shot,accuracy,0.6196581196581197,0.031804252043840985
jb723/cross_lingual_epoch2,hendrycksTest-marketing,5-shot,acc_norm,0.6196581196581197,0.031804252043840985
jb723/cross_lingual_epoch2,hendrycksTest-medical_genetics,5-shot,accuracy,0.39,0.04902071300001975
jb723/cross_lingual_epoch2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.39,0.04902071300001975
jb723/cross_lingual_epoch2,hendrycksTest-miscellaneous,5-shot,accuracy,0.4725415070242657,0.017852981266633955
jb723/cross_lingual_epoch2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.4725415070242657,0.017852981266633955
jb723/cross_lingual_epoch2,hendrycksTest-moral_disputes,5-shot,accuracy,0.4046242774566474,0.02642481659400985
jb723/cross_lingual_epoch2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.4046242774566474,0.02642481659400985
jb723/cross_lingual_epoch2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23798882681564246,0.014242630070574915
jb723/cross_lingual_epoch2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23798882681564246,0.014242630070574915
jb723/cross_lingual_epoch2,hendrycksTest-nutrition,5-shot,accuracy,0.42810457516339867,0.028332397483664274
jb723/cross_lingual_epoch2,hendrycksTest-nutrition,5-shot,acc_norm,0.42810457516339867,0.028332397483664274
jb723/cross_lingual_epoch2,hendrycksTest-philosophy,5-shot,accuracy,0.4758842443729904,0.028365041542564584
jb723/cross_lingual_epoch2,hendrycksTest-philosophy,5-shot,acc_norm,0.4758842443729904,0.028365041542564584
jb723/cross_lingual_epoch2,hendrycksTest-prehistory,5-shot,accuracy,0.4537037037037037,0.027701228468542602
jb723/cross_lingual_epoch2,hendrycksTest-prehistory,5-shot,acc_norm,0.4537037037037037,0.027701228468542602
jb723/cross_lingual_epoch2,hendrycksTest-professional_accounting,5-shot,accuracy,0.2765957446808511,0.026684564340461
jb723/cross_lingual_epoch2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2765957446808511,0.026684564340461
jb723/cross_lingual_epoch2,hendrycksTest-professional_law,5-shot,accuracy,0.2985658409387223,0.011688060141794228
jb723/cross_lingual_epoch2,hendrycksTest-professional_law,5-shot,acc_norm,0.2985658409387223,0.011688060141794228
jb723/cross_lingual_epoch2,hendrycksTest-professional_medicine,5-shot,accuracy,0.25735294117647056,0.026556519470041506
jb723/cross_lingual_epoch2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.25735294117647056,0.026556519470041506
jb723/cross_lingual_epoch2,hendrycksTest-professional_psychology,5-shot,accuracy,0.369281045751634,0.019524316744866356
jb723/cross_lingual_epoch2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.369281045751634,0.019524316744866356
jb723/cross_lingual_epoch2,hendrycksTest-public_relations,5-shot,accuracy,0.4636363636363636,0.04776449162396197
jb723/cross_lingual_epoch2,hendrycksTest-public_relations,5-shot,acc_norm,0.4636363636363636,0.04776449162396197
jb723/cross_lingual_epoch2,hendrycksTest-security_studies,5-shot,accuracy,0.4122448979591837,0.03151236044674281
jb723/cross_lingual_epoch2,hendrycksTest-security_studies,5-shot,acc_norm,0.4122448979591837,0.03151236044674281
jb723/cross_lingual_epoch2,hendrycksTest-sociology,5-shot,accuracy,0.472636815920398,0.03530235517334682
jb723/cross_lingual_epoch2,hendrycksTest-sociology,5-shot,acc_norm,0.472636815920398,0.03530235517334682
jb723/cross_lingual_epoch2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.57,0.049756985195624284
jb723/cross_lingual_epoch2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.57,0.049756985195624284
jb723/cross_lingual_epoch2,hendrycksTest-virology,5-shot,accuracy,0.3855421686746988,0.03789134424611548
jb723/cross_lingual_epoch2,hendrycksTest-virology,5-shot,acc_norm,0.3855421686746988,0.03789134424611548
jb723/cross_lingual_epoch2,hendrycksTest-world_religions,5-shot,accuracy,0.43859649122807015,0.038057975055904594
jb723/cross_lingual_epoch2,hendrycksTest-world_religions,5-shot,acc_norm,0.43859649122807015,0.038057975055904594
jb723/cross_lingual_epoch2,truthfulqa:mc,0-shot,mc1,0.23745410036719705,0.01489627744104185
jb723/cross_lingual_epoch2,truthfulqa:mc,0-shot,mc2,0.4789867119861502,0.016540775343672782
jb723/cross_lingual_epoch2,winogrande,5-shot,accuracy,0.6211523283346487,0.013633724603180335
jb723/cross_lingual_epoch2,drop,3-shot,accuracy,0.049601510067114093,0.0022235145171999363
jb723/cross_lingual_epoch2,drop,3-shot,f1,0.07294463087248305,0.002421427712218101
jb723/cross_lingual_epoch2,gsm8k,5-shot,accuracy,0.009855951478392721,0.00272107657704166
jb723/cross_lingual_epoch2,minerva_math_precalc,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_prealgebra,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_num_theory,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_geometry,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,minerva_math_algebra,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,fld_default,0-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,fld_star,0-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_3da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_3ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_4da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_2ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_5ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_5da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_1dc,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_4ds,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_2dm,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,arithmetic_2da,5-shot,accuracy,0.0,
jb723/cross_lingual_epoch2,gsm8k_cot,5-shot,accuracy,0.009097801364670205,0.0026153265107756725
jb723/cross_lingual_epoch2,anli_r2,0-shot,brier_score,0.874037701565474,
jb723/cross_lingual_epoch2,anli_r3,0-shot,brier_score,0.8702105204736998,
jb723/cross_lingual_epoch2,anli_r1,0-shot,brier_score,0.8702617825185673,
jb723/cross_lingual_epoch2,xnli_eu,0-shot,brier_score,1.3253474537453493,
jb723/cross_lingual_epoch2,xnli_vi,0-shot,brier_score,1.3276025007815453,
jb723/cross_lingual_epoch2,xnli_ru,0-shot,brier_score,1.2227407590964208,
jb723/cross_lingual_epoch2,xnli_zh,0-shot,brier_score,1.2264156406113862,
jb723/cross_lingual_epoch2,xnli_tr,0-shot,brier_score,1.2800307161337745,
jb723/cross_lingual_epoch2,xnli_fr,0-shot,brier_score,1.1092515762780133,
jb723/cross_lingual_epoch2,xnli_en,0-shot,brier_score,0.8103522850538913,
jb723/cross_lingual_epoch2,xnli_ur,0-shot,brier_score,1.1631258828395,
jb723/cross_lingual_epoch2,xnli_ar,0-shot,brier_score,1.274604804567007,
jb723/cross_lingual_epoch2,xnli_de,0-shot,brier_score,0.9515056234633059,
jb723/cross_lingual_epoch2,xnli_hi,0-shot,brier_score,1.3329920595410927,
jb723/cross_lingual_epoch2,xnli_es,0-shot,brier_score,1.1512902734559032,
jb723/cross_lingual_epoch2,xnli_bg,0-shot,brier_score,1.0172978900397067,
jb723/cross_lingual_epoch2,xnli_sw,0-shot,brier_score,0.9811618617891729,
jb723/cross_lingual_epoch2,xnli_el,0-shot,brier_score,1.2011718465559706,
jb723/cross_lingual_epoch2,xnli_th,0-shot,brier_score,1.332573065123302,
jb723/cross_lingual_epoch2,logiqa2,0-shot,brier_score,1.5381466250180293,
jb723/cross_lingual_epoch2,mathqa,5-shot,brier_score,1.0660116386252705,
jb723/cross_lingual_epoch2,lambada_standard,0-shot,perplexity,354.9488617195369,25.870375094453767
jb723/cross_lingual_epoch2,lambada_standard,0-shot,accuracy,0.24704055889772947,0.006008720389692815
jb723/cross_lingual_epoch2,lambada_openai,0-shot,perplexity,76.30917472321947,3.6122799594811053
jb723/cross_lingual_epoch2,lambada_openai,0-shot,accuracy,0.23539685620027168,0.0059105846518009274
HuggingFaceTB/SmolLM-360M,mmlu_world_religions,0-shot,accuracy,0.28654970760233917,0.03467826685703826
HuggingFaceTB/SmolLM-360M,mmlu_formal_logic,0-shot,accuracy,0.1746031746031746,0.03395490020856113
HuggingFaceTB/SmolLM-360M,mmlu_prehistory,0-shot,accuracy,0.2808641975308642,0.025006469755799208
HuggingFaceTB/SmolLM-360M,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.014422292204808838
HuggingFaceTB/SmolLM-360M,mmlu_high_school_world_history,0-shot,accuracy,0.27848101265822783,0.029178682304842555
HuggingFaceTB/SmolLM-360M,mmlu_moral_disputes,0-shot,accuracy,0.2832369942196532,0.024257901705323374
HuggingFaceTB/SmolLM-360M,mmlu_professional_law,0-shot,accuracy,0.23989569752281617,0.010906282617981636
HuggingFaceTB/SmolLM-360M,mmlu_logical_fallacies,0-shot,accuracy,0.294478527607362,0.03581165790474082
HuggingFaceTB/SmolLM-360M,mmlu_high_school_us_history,0-shot,accuracy,0.19117647058823528,0.027599174300640773
HuggingFaceTB/SmolLM-360M,mmlu_philosophy,0-shot,accuracy,0.2861736334405145,0.025670259242188936
HuggingFaceTB/SmolLM-360M,mmlu_jurisprudence,0-shot,accuracy,0.18518518518518517,0.03755265865037183
HuggingFaceTB/SmolLM-360M,mmlu_international_law,0-shot,accuracy,0.38016528925619836,0.04431324501968432
HuggingFaceTB/SmolLM-360M,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
HuggingFaceTB/SmolLM-360M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.25906735751295334,0.0316187791793541
HuggingFaceTB/SmolLM-360M,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.02755361446786381
HuggingFaceTB/SmolLM-360M,mmlu_high_school_geography,0-shot,accuracy,0.23737373737373738,0.030313710538198896
HuggingFaceTB/SmolLM-360M,mmlu_high_school_psychology,0-shot,accuracy,0.23669724770642203,0.018224078117299078
HuggingFaceTB/SmolLM-360M,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.04265792110940588
HuggingFaceTB/SmolLM-360M,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.04292346959909282
HuggingFaceTB/SmolLM-360M,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296007
HuggingFaceTB/SmolLM-360M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2282051282051282,0.02127839386358628
HuggingFaceTB/SmolLM-360M,mmlu_security_studies,0-shot,accuracy,0.20408163265306123,0.0258012834750905
HuggingFaceTB/SmolLM-360M,mmlu_professional_psychology,0-shot,accuracy,0.25163398692810457,0.017555818091322284
HuggingFaceTB/SmolLM-360M,mmlu_human_sexuality,0-shot,accuracy,0.20610687022900764,0.035477710041594626
HuggingFaceTB/SmolLM-360M,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.041424397194893624
HuggingFaceTB/SmolLM-360M,mmlu_miscellaneous,0-shot,accuracy,0.2784163473818646,0.01602829518899246
HuggingFaceTB/SmolLM-360M,mmlu_marketing,0-shot,accuracy,0.2222222222222222,0.027236013946196704
HuggingFaceTB/SmolLM-360M,mmlu_management,0-shot,accuracy,0.1553398058252427,0.03586594738573973
HuggingFaceTB/SmolLM-360M,mmlu_nutrition,0-shot,accuracy,0.22875816993464052,0.02405102973991225
HuggingFaceTB/SmolLM-360M,mmlu_medical_genetics,0-shot,accuracy,0.27,0.044619604333847394
HuggingFaceTB/SmolLM-360M,mmlu_human_aging,0-shot,accuracy,0.32286995515695066,0.03138147637575498
HuggingFaceTB/SmolLM-360M,mmlu_professional_medicine,0-shot,accuracy,0.39338235294117646,0.02967428828131118
HuggingFaceTB/SmolLM-360M,mmlu_college_medicine,0-shot,accuracy,0.19653179190751446,0.030299574664788147
HuggingFaceTB/SmolLM-360M,mmlu_business_ethics,0-shot,accuracy,0.24,0.04292346959909283
HuggingFaceTB/SmolLM-360M,mmlu_clinical_knowledge,0-shot,accuracy,0.2641509433962264,0.027134291628741713
HuggingFaceTB/SmolLM-360M,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621505
HuggingFaceTB/SmolLM-360M,mmlu_virology,0-shot,accuracy,0.30120481927710846,0.03571609230053481
HuggingFaceTB/SmolLM-360M,mmlu_professional_accounting,0-shot,accuracy,0.25886524822695034,0.026129572527180848
HuggingFaceTB/SmolLM-360M,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237654
HuggingFaceTB/SmolLM-360M,mmlu_high_school_physics,0-shot,accuracy,0.2119205298013245,0.03336767086567977
HuggingFaceTB/SmolLM-360M,mmlu_high_school_biology,0-shot,accuracy,0.22258064516129034,0.023664216671642518
HuggingFaceTB/SmolLM-360M,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.03476590104304134
HuggingFaceTB/SmolLM-360M,mmlu_anatomy,0-shot,accuracy,0.3111111111111111,0.03999262876617723
HuggingFaceTB/SmolLM-360M,mmlu_college_chemistry,0-shot,accuracy,0.15,0.03588702812826369
HuggingFaceTB/SmolLM-360M,mmlu_computer_security,0-shot,accuracy,0.23,0.042295258468165065
HuggingFaceTB/SmolLM-360M,mmlu_college_computer_science,0-shot,accuracy,0.3,0.046056618647183814
HuggingFaceTB/SmolLM-360M,mmlu_astronomy,0-shot,accuracy,0.23026315789473684,0.03426059424403165
HuggingFaceTB/SmolLM-360M,mmlu_college_mathematics,0-shot,accuracy,0.29,0.045604802157206845
HuggingFaceTB/SmolLM-360M,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.030472973363380056
HuggingFaceTB/SmolLM-360M,mmlu_abstract_algebra,0-shot,accuracy,0.33,0.04725815626252605
HuggingFaceTB/SmolLM-360M,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.047609522856952365
HuggingFaceTB/SmolLM-360M,mmlu_machine_learning,0-shot,accuracy,0.22321428571428573,0.03952301967702511
HuggingFaceTB/SmolLM-360M,mmlu_high_school_chemistry,0-shot,accuracy,0.29064039408866993,0.03194740072265541
HuggingFaceTB/SmolLM-360M,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
HuggingFaceTB/SmolLM-360M,mmlu_elementary_mathematics,0-shot,accuracy,0.26455026455026454,0.02271746789770862
HuggingFaceTB/SmolLM-360M,mmlu_electrical_engineering,0-shot,accuracy,0.2689655172413793,0.03695183311650232
HuggingFaceTB/SmolLM-360M,mmlu_high_school_mathematics,0-shot,accuracy,0.2814814814814815,0.027420019350945273
HuggingFaceTB/SmolLM-360M,arc_challenge,25-shot,accuracy,0.3651877133105802,0.014070265519268802
HuggingFaceTB/SmolLM-360M,arc_challenge,25-shot,acc_norm,0.386518771331058,0.014230084761910478
HuggingFaceTB/SmolLM-360M,hellaswag,10-shot,accuracy,0.41505676160127464,0.004917248150601852
HuggingFaceTB/SmolLM-360M,hellaswag,10-shot,acc_norm,0.5423222465644294,0.004971874159777681
HuggingFaceTB/SmolLM-360M,truthfulqa_mc2,0-shot,accuracy,0.3793017405276516,0.014255545371148842
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,bleu_max,23.51716504412742,0.7176975216279505
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,bleu_acc,0.2864137086903305,0.01582614243950238
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,bleu_diff,-7.083041541904997,0.7077513549659533
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge1_max,48.70952743752934,0.8300761683331853
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge1_acc,0.27539779681762544,0.01563813566777552
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge1_diff,-9.635957207171685,0.7485923569688212
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge2_max,32.566501096217806,0.9326341598087236
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge2_acc,0.23623011015911874,0.014869755015871107
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rouge2_diff,-10.895866455590797,0.9226904079378292
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rougeL_max,45.55323568193951,0.8340494740388494
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rougeL_acc,0.2607099143206854,0.015368841620766372
HuggingFaceTB/SmolLM-360M,truthfulqa_gen,0-shot,rougeL_diff,-9.756463734915389,0.7580939692627898
HuggingFaceTB/SmolLM-360M,truthfulqa_mc1,0-shot,accuracy,0.25091799265605874,0.015176985027707698
HuggingFaceTB/SmolLM-360M,winogrande,5-shot,accuracy,0.5572217837411207,0.01396015735078497
HuggingFaceTB/SmolLM-360M,gsm8k,5-shot,accuracy,0.013646702047005308,0.0031957470754808027
meta-llama/Meta-Llama-3-8B-Instruct,arc:challenge,25-shot,accuracy,0.5716723549488054,0.01446049636759902
meta-llama/Meta-Llama-3-8B-Instruct,arc:challenge,25-shot,acc_norm,0.6075085324232082,0.014269634635670722
meta-llama/Meta-Llama-3-8B-Instruct,hellaswag,10-shot,accuracy,0.5898227444732125,0.004908604732082822
meta-llama/Meta-Llama-3-8B-Instruct,hellaswag,10-shot,acc_norm,0.7898824935271859,0.004065592811695992
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215034
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215034
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-anatomy,5-shot,accuracy,0.6518518518518519,0.041153246103369526
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-anatomy,5-shot,acc_norm,0.6518518518518519,0.041153246103369526
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-astronomy,5-shot,accuracy,0.7039473684210527,0.03715062154998905
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-astronomy,5-shot,acc_norm,0.7039473684210527,0.03715062154998905
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-business_ethics,5-shot,accuracy,0.69,0.04648231987117316
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-business_ethics,5-shot,acc_norm,0.69,0.04648231987117316
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7471698113207547,0.02674989977124121
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7471698113207547,0.02674989977124121
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_biology,5-shot,accuracy,0.7986111111111112,0.03353647469713839
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_biology,5-shot,acc_norm,0.7986111111111112,0.03353647469713839
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_chemistry,5-shot,accuracy,0.47,0.050161355804659205
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_chemistry,5-shot,acc_norm,0.47,0.050161355804659205
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_computer_science,5-shot,accuracy,0.59,0.04943110704237102
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_computer_science,5-shot,acc_norm,0.59,0.04943110704237102
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_mathematics,5-shot,accuracy,0.39,0.04902071300001975
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_mathematics,5-shot,acc_norm,0.39,0.04902071300001975
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_medicine,5-shot,accuracy,0.6358381502890174,0.03669072477416907
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_medicine,5-shot,acc_norm,0.6358381502890174,0.03669072477416907
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_physics,5-shot,accuracy,0.5,0.04975185951049946
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-college_physics,5-shot,acc_norm,0.5,0.04975185951049946
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-computer_security,5-shot,accuracy,0.77,0.04229525846816506
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-computer_security,5-shot,acc_norm,0.77,0.04229525846816506
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6042553191489362,0.03196758697835363
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6042553191489362,0.03196758697835363
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-econometrics,5-shot,accuracy,0.6052631578947368,0.04598188057816542
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-econometrics,5-shot,acc_norm,0.6052631578947368,0.04598188057816542
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6275862068965518,0.04028731532947559
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6275862068965518,0.04028731532947559
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4444444444444444,0.025591857761382182
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4444444444444444,0.025591857761382182
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-formal_logic,5-shot,accuracy,0.48412698412698413,0.04469881854072606
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-formal_logic,5-shot,acc_norm,0.48412698412698413,0.04469881854072606
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-global_facts,5-shot,accuracy,0.4,0.04923659639173309
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-global_facts,5-shot,acc_norm,0.4,0.04923659639173309
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_biology,5-shot,accuracy,0.7838709677419354,0.02341529343356853
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7838709677419354,0.02341529343356853
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5073891625615764,0.035176035403610105
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5073891625615764,0.035176035403610105
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.75,0.04351941398892446
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.75,0.04351941398892446
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7454545454545455,0.03401506715249039
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7454545454545455,0.03401506715249039
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_geography,5-shot,accuracy,0.8434343434343434,0.025890520358141454
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8434343434343434,0.025890520358141454
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9119170984455959,0.02045374660160103
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9119170984455959,0.02045374660160103
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.658974358974359,0.02403548967633507
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.658974358974359,0.02403548967633507
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4,0.029869605095316904
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4,0.029869605095316904
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7647058823529411,0.027553614467863814
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7647058823529411,0.027553614467863814
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_physics,5-shot,accuracy,0.44370860927152317,0.04056527902281731
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_physics,5-shot,acc_norm,0.44370860927152317,0.04056527902281731
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8366972477064221,0.01584825580650155
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8366972477064221,0.01584825580650155
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5370370370370371,0.03400603625538272
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5370370370370371,0.03400603625538272
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8578431372549019,0.0245098039215686
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8578431372549019,0.0245098039215686
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8438818565400844,0.023627159460318667
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8438818565400844,0.023627159460318667
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_aging,5-shot,accuracy,0.7219730941704036,0.030069584874494036
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_aging,5-shot,acc_norm,0.7219730941704036,0.030069584874494036
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_sexuality,5-shot,accuracy,0.7786259541984732,0.0364129708131373
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7786259541984732,0.0364129708131373
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-international_law,5-shot,accuracy,0.8181818181818182,0.03520893951097653
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-international_law,5-shot,acc_norm,0.8181818181818182,0.03520893951097653
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-jurisprudence,5-shot,accuracy,0.7777777777777778,0.0401910747255735
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7777777777777778,0.0401910747255735
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7668711656441718,0.0332201579577674
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7668711656441718,0.0332201579577674
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-machine_learning,5-shot,accuracy,0.5446428571428571,0.04726835553719097
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-machine_learning,5-shot,acc_norm,0.5446428571428571,0.04726835553719097
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-management,5-shot,accuracy,0.7864077669902912,0.040580420156460344
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-management,5-shot,acc_norm,0.7864077669902912,0.040580420156460344
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-marketing,5-shot,accuracy,0.905982905982906,0.019119892798924974
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-marketing,5-shot,acc_norm,0.905982905982906,0.019119892798924974
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-medical_genetics,5-shot,accuracy,0.8,0.04020151261036845
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-medical_genetics,5-shot,acc_norm,0.8,0.04020151261036845
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-miscellaneous,5-shot,accuracy,0.7982120051085568,0.014351702181636857
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-miscellaneous,5-shot,acc_norm,0.7982120051085568,0.014351702181636857
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_disputes,5-shot,accuracy,0.7485549132947977,0.02335736578587403
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7485549132947977,0.02335736578587403
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_scenarios,5-shot,accuracy,0.43687150837988825,0.01658868086453062
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.43687150837988825,0.01658868086453062
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-nutrition,5-shot,accuracy,0.7450980392156863,0.024954184324879905
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-nutrition,5-shot,acc_norm,0.7450980392156863,0.024954184324879905
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-philosophy,5-shot,accuracy,0.7138263665594855,0.025670259242188933
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-philosophy,5-shot,acc_norm,0.7138263665594855,0.025670259242188933
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-prehistory,5-shot,accuracy,0.7407407407407407,0.024383665531035454
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-prehistory,5-shot,acc_norm,0.7407407407407407,0.024383665531035454
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_accounting,5-shot,accuracy,0.5283687943262412,0.029779450957303055
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5283687943262412,0.029779450957303055
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_law,5-shot,accuracy,0.4784876140808344,0.012758410941038916
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_law,5-shot,acc_norm,0.4784876140808344,0.012758410941038916
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_medicine,5-shot,accuracy,0.7169117647058824,0.02736586113151381
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_medicine,5-shot,acc_norm,0.7169117647058824,0.02736586113151381
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_psychology,5-shot,accuracy,0.7058823529411765,0.0184334276494019
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7058823529411765,0.0184334276494019
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-public_relations,5-shot,accuracy,0.6454545454545455,0.045820048415054174
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-public_relations,5-shot,acc_norm,0.6454545454545455,0.045820048415054174
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-security_studies,5-shot,accuracy,0.7387755102040816,0.028123429335142773
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-security_studies,5-shot,acc_norm,0.7387755102040816,0.028123429335142773
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-sociology,5-shot,accuracy,0.8656716417910447,0.024112678240900794
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-sociology,5-shot,acc_norm,0.8656716417910447,0.024112678240900794
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.034873508801977704
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.034873508801977704
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-virology,5-shot,accuracy,0.5120481927710844,0.03891364495835816
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-virology,5-shot,acc_norm,0.5120481927710844,0.03891364495835816
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-world_religions,5-shot,accuracy,0.7777777777777778,0.03188578017686398
meta-llama/Meta-Llama-3-8B-Instruct,hendrycksTest-world_religions,5-shot,acc_norm,0.7777777777777778,0.03188578017686398
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa:mc,0-shot,mc1,0.3623011015911873,0.016826646897262258
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa:mc,0-shot,mc2,0.5164972283615512,0.01519689881872357
meta-llama/Meta-Llama-3-8B-Instruct,winogrande,5-shot,accuracy,0.7561168113654302,0.01206892327890818
meta-llama/Meta-Llama-3-8B-Instruct,gsm8k,5-shot,accuracy,0.755117513267627,0.011844819027863667
openlm-research/open_llama_3b_v2,minerva_math_precalc,5-shot,accuracy,0.009157509157509158,0.004080306065048996
openlm-research/open_llama_3b_v2,minerva_math_prealgebra,5-shot,accuracy,0.04362801377726751,0.006925266930505693
openlm-research/open_llama_3b_v2,minerva_math_num_theory,5-shot,accuracy,0.012962962962962963,0.004872192984581458
openlm-research/open_llama_3b_v2,minerva_math_intermediate_algebra,5-shot,accuracy,0.015503875968992248,0.0041136172383604485
openlm-research/open_llama_3b_v2,minerva_math_geometry,5-shot,accuracy,0.014613778705636743,0.0054887134436863135
openlm-research/open_llama_3b_v2,minerva_math_counting_and_prob,5-shot,accuracy,0.014767932489451477,0.00554623858966847
openlm-research/open_llama_3b_v2,minerva_math_algebra,5-shot,accuracy,0.02106149957877001,0.004169461854206043
openlm-research/open_llama_3b_v2,fld_default,0-shot,accuracy,0.0,
openlm-research/open_llama_3b_v2,fld_star,0-shot,accuracy,0.0,
openlm-research/open_llama_3b_v2,arithmetic_3da,5-shot,accuracy,0.284,0.010085775202269382
openlm-research/open_llama_3b_v2,arithmetic_3ds,5-shot,accuracy,0.2605,0.009816723436540222
openlm-research/open_llama_3b_v2,arithmetic_4da,5-shot,accuracy,0.0825,0.0061535199604740055
openlm-research/open_llama_3b_v2,arithmetic_2ds,5-shot,accuracy,0.4195,0.011037245371590673
openlm-research/open_llama_3b_v2,arithmetic_5ds,5-shot,accuracy,0.015,0.0027186753387999575
openlm-research/open_llama_3b_v2,arithmetic_5da,5-shot,accuracy,0.015,0.002718675338799954
openlm-research/open_llama_3b_v2,arithmetic_1dc,5-shot,accuracy,0.0955,0.006573544001554193
openlm-research/open_llama_3b_v2,arithmetic_4ds,5-shot,accuracy,0.101,0.006739600218525675
openlm-research/open_llama_3b_v2,arithmetic_2dm,5-shot,accuracy,0.1655,0.00831200455424887
openlm-research/open_llama_3b_v2,arithmetic_2da,5-shot,accuracy,0.4295,0.01107141197325543
openlm-research/open_llama_3b_v2,gsm8k_cot,5-shot,accuracy,0.04852160727824109,0.005918468618921081
openlm-research/open_llama_3b_v2,anli_r2,0-shot,brier_score,0.707706480342601,
openlm-research/open_llama_3b_v2,anli_r3,0-shot,brier_score,0.679183075689064,
openlm-research/open_llama_3b_v2,anli_r1,0-shot,brier_score,0.7235055238380341,
openlm-research/open_llama_3b_v2,xnli_eu,0-shot,brier_score,1.132908017291774,
openlm-research/open_llama_3b_v2,xnli_vi,0-shot,brier_score,0.971548338532371,
openlm-research/open_llama_3b_v2,xnli_ru,0-shot,brier_score,0.7523375161853497,
openlm-research/open_llama_3b_v2,xnli_zh,0-shot,brier_score,0.9867010701190115,
openlm-research/open_llama_3b_v2,xnli_tr,0-shot,brier_score,0.8808176289678832,
openlm-research/open_llama_3b_v2,xnli_fr,0-shot,brier_score,0.7460738931677325,
openlm-research/open_llama_3b_v2,xnli_en,0-shot,brier_score,0.6474238073900174,
openlm-research/open_llama_3b_v2,xnli_ur,0-shot,brier_score,1.3250861980436117,
openlm-research/open_llama_3b_v2,xnli_ar,0-shot,brier_score,1.234167496082821,
openlm-research/open_llama_3b_v2,xnli_de,0-shot,brier_score,0.812354579101372,
openlm-research/open_llama_3b_v2,xnli_hi,0-shot,brier_score,1.1542572623687906,
openlm-research/open_llama_3b_v2,xnli_es,0-shot,brier_score,0.8910867470387117,
openlm-research/open_llama_3b_v2,xnli_bg,0-shot,brier_score,0.859452606954678,
openlm-research/open_llama_3b_v2,xnli_sw,0-shot,brier_score,1.0713476113363065,
openlm-research/open_llama_3b_v2,xnli_el,0-shot,brier_score,1.1450771586115809,
openlm-research/open_llama_3b_v2,xnli_th,0-shot,brier_score,0.9689833936635035,
openlm-research/open_llama_3b_v2,logiqa2,0-shot,brier_score,1.0179439999333861,
openlm-research/open_llama_3b_v2,mathqa,5-shot,brier_score,0.9356941842152771,
openlm-research/open_llama_3b_v2,lambada_standard,0-shot,perplexity,5.818821278870909,0.13816721772720136
openlm-research/open_llama_3b_v2,lambada_standard,0-shot,accuracy,0.5951872695517174,0.006838580607651546
openlm-research/open_llama_3b_v2,lambada_openai,0-shot,perplexity,4.561675691644536,0.10330641094995072
openlm-research/open_llama_3b_v2,lambada_openai,0-shot,accuracy,0.6735882010479333,0.006532692754359034
openlm-research/open_llama_3b_v2,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.03565079670708312
openlm-research/open_llama_3b_v2,mmlu_formal_logic,0-shot,accuracy,0.23809523809523808,0.03809523809523809
openlm-research/open_llama_3b_v2,mmlu_prehistory,0-shot,accuracy,0.2839506172839506,0.02508947852376513
openlm-research/open_llama_3b_v2,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.01435591196476786
openlm-research/open_llama_3b_v2,mmlu_high_school_world_history,0-shot,accuracy,0.2489451476793249,0.028146970599422644
openlm-research/open_llama_3b_v2,mmlu_moral_disputes,0-shot,accuracy,0.26011560693641617,0.023618678310069374
openlm-research/open_llama_3b_v2,mmlu_professional_law,0-shot,accuracy,0.2470664928292047,0.01101575225527935
openlm-research/open_llama_3b_v2,mmlu_logical_fallacies,0-shot,accuracy,0.2085889570552147,0.031921934489347235
openlm-research/open_llama_3b_v2,mmlu_high_school_us_history,0-shot,accuracy,0.23039215686274508,0.029554292605695066
openlm-research/open_llama_3b_v2,mmlu_philosophy,0-shot,accuracy,0.27009646302250806,0.025218040373410622
openlm-research/open_llama_3b_v2,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.04414343666854933
openlm-research/open_llama_3b_v2,mmlu_international_law,0-shot,accuracy,0.2809917355371901,0.04103203830514511
openlm-research/open_llama_3b_v2,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.033464098810559534
openlm-research/open_llama_3b_v2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.23316062176165803,0.030516111371476008
openlm-research/open_llama_3b_v2,mmlu_high_school_microeconomics,0-shot,accuracy,0.28991596638655465,0.029472485833136098
openlm-research/open_llama_3b_v2,mmlu_high_school_geography,0-shot,accuracy,0.21717171717171718,0.02937661648494563
openlm-research/open_llama_3b_v2,mmlu_high_school_psychology,0-shot,accuracy,0.24036697247706423,0.01832060732096407
openlm-research/open_llama_3b_v2,mmlu_public_relations,0-shot,accuracy,0.3181818181818182,0.04461272175910507
openlm-research/open_llama_3b_v2,mmlu_us_foreign_policy,0-shot,accuracy,0.34,0.04760952285695235
openlm-research/open_llama_3b_v2,mmlu_sociology,0-shot,accuracy,0.25870646766169153,0.030965903123573033
openlm-research/open_llama_3b_v2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24871794871794872,0.021916957709213803
openlm-research/open_llama_3b_v2,mmlu_security_studies,0-shot,accuracy,0.3224489795918367,0.02992310056368391
openlm-research/open_llama_3b_v2,mmlu_professional_psychology,0-shot,accuracy,0.27124183006535946,0.017986615304030316
openlm-research/open_llama_3b_v2,mmlu_human_sexuality,0-shot,accuracy,0.24427480916030533,0.03768335959728742
openlm-research/open_llama_3b_v2,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748142
openlm-research/open_llama_3b_v2,mmlu_miscellaneous,0-shot,accuracy,0.2835249042145594,0.016117318166832276
openlm-research/open_llama_3b_v2,mmlu_marketing,0-shot,accuracy,0.29914529914529914,0.02999695185834948
openlm-research/open_llama_3b_v2,mmlu_management,0-shot,accuracy,0.2524271844660194,0.04301250399690878
openlm-research/open_llama_3b_v2,mmlu_nutrition,0-shot,accuracy,0.26143790849673204,0.025160998214292456
openlm-research/open_llama_3b_v2,mmlu_medical_genetics,0-shot,accuracy,0.22,0.04163331998932269
openlm-research/open_llama_3b_v2,mmlu_human_aging,0-shot,accuracy,0.3811659192825112,0.03259625118416827
openlm-research/open_llama_3b_v2,mmlu_professional_medicine,0-shot,accuracy,0.21691176470588236,0.025035845227711274
openlm-research/open_llama_3b_v2,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.03186209851641143
openlm-research/open_llama_3b_v2,mmlu_business_ethics,0-shot,accuracy,0.41,0.049431107042371025
openlm-research/open_llama_3b_v2,mmlu_clinical_knowledge,0-shot,accuracy,0.29056603773584905,0.027943219989337145
openlm-research/open_llama_3b_v2,mmlu_global_facts,0-shot,accuracy,0.31,0.04648231987117316
openlm-research/open_llama_3b_v2,mmlu_virology,0-shot,accuracy,0.3373493975903614,0.0368078369072758
openlm-research/open_llama_3b_v2,mmlu_professional_accounting,0-shot,accuracy,0.30141843971631205,0.02737412888263115
openlm-research/open_llama_3b_v2,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237654
openlm-research/open_llama_3b_v2,mmlu_high_school_physics,0-shot,accuracy,0.32450331125827814,0.03822746937658753
openlm-research/open_llama_3b_v2,mmlu_high_school_biology,0-shot,accuracy,0.23548387096774193,0.024137632429337717
openlm-research/open_llama_3b_v2,mmlu_college_biology,0-shot,accuracy,0.24305555555555555,0.03586879280080341
openlm-research/open_llama_3b_v2,mmlu_anatomy,0-shot,accuracy,0.23703703703703705,0.03673731683969506
openlm-research/open_llama_3b_v2,mmlu_college_chemistry,0-shot,accuracy,0.24,0.04292346959909284
openlm-research/open_llama_3b_v2,mmlu_computer_security,0-shot,accuracy,0.32,0.04688261722621505
openlm-research/open_llama_3b_v2,mmlu_college_computer_science,0-shot,accuracy,0.23,0.04229525846816507
openlm-research/open_llama_3b_v2,mmlu_astronomy,0-shot,accuracy,0.26973684210526316,0.036117805602848975
openlm-research/open_llama_3b_v2,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215034
openlm-research/open_llama_3b_v2,mmlu_conceptual_physics,0-shot,accuracy,0.3446808510638298,0.031068985963122145
openlm-research/open_llama_3b_v2,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768079
openlm-research/open_llama_3b_v2,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.0440844002276808
openlm-research/open_llama_3b_v2,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.0432704093257873
openlm-research/open_llama_3b_v2,mmlu_high_school_chemistry,0-shot,accuracy,0.2413793103448276,0.030108330718011625
openlm-research/open_llama_3b_v2,mmlu_high_school_statistics,0-shot,accuracy,0.19444444444444445,0.026991454502036726
openlm-research/open_llama_3b_v2,mmlu_elementary_mathematics,0-shot,accuracy,0.29365079365079366,0.02345603738398203
openlm-research/open_llama_3b_v2,mmlu_electrical_engineering,0-shot,accuracy,0.20689655172413793,0.03375672449560553
openlm-research/open_llama_3b_v2,mmlu_high_school_mathematics,0-shot,accuracy,0.2222222222222222,0.025348097468097845
openlm-research/open_llama_3b_v2,arc_challenge,25-shot,accuracy,0.36006825938566556,0.014027516814585186
openlm-research/open_llama_3b_v2,arc_challenge,25-shot,acc_norm,0.40784982935153585,0.014361097288449701
openlm-research/open_llama_3b_v2,truthfulqa_mc2,0-shot,accuracy,0.34584348807121285,0.013212582599603815
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,bleu_max,25.09984177135623,0.7599305897286084
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,bleu_acc,0.3011015911872705,0.01605899902610059
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,bleu_diff,-7.793264166700625,0.8253857817947617
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge1_max,50.570610391811464,0.859536655414322
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge1_acc,0.2668298653610771,0.015483691939237258
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge1_diff,-10.097862717598343,0.9307007939492593
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge2_max,34.0661565486233,0.9907235368522948
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge2_acc,0.23990208078335373,0.014948812679062142
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rouge2_diff,-11.881434651179656,1.0800608172022408
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rougeL_max,47.56194685447575,0.8711921370362652
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rougeL_acc,0.2631578947368421,0.015415241740237002
openlm-research/open_llama_3b_v2,truthfulqa_gen,0-shot,rougeL_diff,-10.243475244546104,0.9305534307619345
openlm-research/open_llama_3b_v2,truthfulqa_mc1,0-shot,accuracy,0.2141982864137087,0.014362148155690466
EleutherAI/gpt-neo-125M,minerva_math_precalc,5-shot,accuracy,0.003663003663003663,0.002587757368193456
EleutherAI/gpt-neo-125M,minerva_math_prealgebra,5-shot,accuracy,0.002296211251435132,0.00162273313693462
EleutherAI/gpt-neo-125M,minerva_math_num_theory,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417496,0.0015652595934070484
EleutherAI/gpt-neo-125M,minerva_math_geometry,5-shot,accuracy,0.0041753653444676405,0.002949339217075629
EleutherAI/gpt-neo-125M,minerva_math_counting_and_prob,5-shot,accuracy,0.002109704641350211,0.002109704641350211
EleutherAI/gpt-neo-125M,minerva_math_algebra,5-shot,accuracy,0.003369839932603201,0.001682787605228352
EleutherAI/gpt-neo-125M,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_3da,5-shot,accuracy,0.0005,0.0005000000000000061
EleutherAI/gpt-neo-125M,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339515
EleutherAI/gpt-neo-125M,arithmetic_4da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_2ds,5-shot,accuracy,0.001,0.000706929893933945
EleutherAI/gpt-neo-125M,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_1dc,5-shot,accuracy,0.0025,0.0011169148353275264
EleutherAI/gpt-neo-125M,arithmetic_4ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-125M,arithmetic_2dm,5-shot,accuracy,0.0285,0.0037216663472429377
EleutherAI/gpt-neo-125M,arithmetic_2da,5-shot,accuracy,0.001,0.0007069298939339552
EleutherAI/gpt-neo-125M,gsm8k_cot,5-shot,accuracy,0.015163002274450341,0.003366022949726344
EleutherAI/gpt-neo-125M,gsm8k,5-shot,accuracy,0.017437452615617893,0.003605486867998238
EleutherAI/gpt-neo-125M,anli_r2,0-shot,brier_score,0.9548365401800128,
EleutherAI/gpt-neo-125M,anli_r3,0-shot,brier_score,0.8891583885006976,
EleutherAI/gpt-neo-125M,anli_r1,0-shot,brier_score,0.9622490181407919,
EleutherAI/gpt-neo-125M,xnli_eu,0-shot,brier_score,1.1177099043871799,
EleutherAI/gpt-neo-125M,xnli_vi,0-shot,brier_score,0.849920028552591,
EleutherAI/gpt-neo-125M,xnli_ru,0-shot,brier_score,0.9576388869814577,
EleutherAI/gpt-neo-125M,xnli_zh,0-shot,brier_score,0.9929665020833541,
EleutherAI/gpt-neo-125M,xnli_tr,0-shot,brier_score,0.9095949998078355,
EleutherAI/gpt-neo-125M,xnli_fr,0-shot,brier_score,0.9206902731267517,
EleutherAI/gpt-neo-125M,xnli_en,0-shot,brier_score,0.7190039933226706,
EleutherAI/gpt-neo-125M,xnli_ur,0-shot,brier_score,1.3284008366257785,
EleutherAI/gpt-neo-125M,xnli_ar,0-shot,brier_score,0.8957307885455548,
EleutherAI/gpt-neo-125M,xnli_de,0-shot,brier_score,0.9295157814794096,
EleutherAI/gpt-neo-125M,xnli_hi,0-shot,brier_score,0.789158069353759,
EleutherAI/gpt-neo-125M,xnli_es,0-shot,brier_score,1.042790212974061,
EleutherAI/gpt-neo-125M,xnli_bg,0-shot,brier_score,1.2387012114897136,
EleutherAI/gpt-neo-125M,xnli_sw,0-shot,brier_score,1.281388698211314,
EleutherAI/gpt-neo-125M,xnli_el,0-shot,brier_score,1.2797796563123813,
EleutherAI/gpt-neo-125M,xnli_th,0-shot,brier_score,1.0078771683411278,
EleutherAI/gpt-neo-125M,logiqa2,0-shot,brier_score,1.1460935685457103,
EleutherAI/gpt-neo-125M,mathqa,5-shot,brier_score,1.0224773820452193,
EleutherAI/gpt-neo-125M,lambada_standard,0-shot,perplexity,111.82437441637022,5.021215660258462
EleutherAI/gpt-neo-125M,lambada_standard,0-shot,accuracy,0.26062487871143025,0.006115788029333536
EleutherAI/gpt-neo-125M,lambada_openai,0-shot,perplexity,30.265471083236193,1.1271375050920782
EleutherAI/gpt-neo-125M,lambada_openai,0-shot,accuracy,0.3735687948767708,0.006739599048608376
jb723/LLaMA2-en-ko-7B-model,minerva_math_precalc,5-shot,accuracy,0.02564102564102564,0.006770627800780461
jb723/LLaMA2-en-ko-7B-model,minerva_math_prealgebra,5-shot,accuracy,0.06314580941446613,0.008246100866669394
jb723/LLaMA2-en-ko-7B-model,minerva_math_num_theory,5-shot,accuracy,0.044444444444444446,0.008876511687867027
jb723/LLaMA2-en-ko-7B-model,minerva_math_intermediate_algebra,5-shot,accuracy,0.04318936877076412,0.006768589184759898
jb723/LLaMA2-en-ko-7B-model,minerva_math_geometry,5-shot,accuracy,0.031315240083507306,0.00796627249945704
jb723/LLaMA2-en-ko-7B-model,minerva_math_counting_and_prob,5-shot,accuracy,0.0379746835443038,0.008788398915918344
jb723/LLaMA2-en-ko-7B-model,minerva_math_algebra,5-shot,accuracy,0.03622577927548441,0.005425680006601671
jb723/LLaMA2-en-ko-7B-model,fld_default,0-shot,accuracy,0.0,
jb723/LLaMA2-en-ko-7B-model,fld_star,0-shot,accuracy,0.0,
jb723/LLaMA2-en-ko-7B-model,arithmetic_3da,5-shot,accuracy,0.812,0.008738774690512815
jb723/LLaMA2-en-ko-7B-model,arithmetic_3ds,5-shot,accuracy,0.6985,0.010264090353040862
jb723/LLaMA2-en-ko-7B-model,arithmetic_4da,5-shot,accuracy,0.445,0.011115272135099212
jb723/LLaMA2-en-ko-7B-model,arithmetic_2ds,5-shot,accuracy,0.888,0.007053571892184729
jb723/LLaMA2-en-ko-7B-model,arithmetic_5ds,5-shot,accuracy,0.1365,0.0076787601003246685
jb723/LLaMA2-en-ko-7B-model,arithmetic_5da,5-shot,accuracy,0.1975,0.00890429774092991
jb723/LLaMA2-en-ko-7B-model,arithmetic_1dc,5-shot,accuracy,0.1985,0.008921248193760077
jb723/LLaMA2-en-ko-7B-model,arithmetic_4ds,5-shot,accuracy,0.324,0.010467415315716553
jb723/LLaMA2-en-ko-7B-model,arithmetic_2dm,5-shot,accuracy,0.2485,0.009665432493822849
jb723/LLaMA2-en-ko-7B-model,arithmetic_2da,5-shot,accuracy,0.913,0.0063035995814963814
jb723/LLaMA2-en-ko-7B-model,gsm8k_cot,5-shot,accuracy,0.10462471569370735,0.008430668082029294
jb723/LLaMA2-en-ko-7B-model,gsm8k,5-shot,accuracy,0.0803639120545868,0.007488258573239077
jb723/LLaMA2-en-ko-7B-model,anli_r2,0-shot,brier_score,0.7571293238745885,
jb723/LLaMA2-en-ko-7B-model,anli_r3,0-shot,brier_score,0.7917361451062274,
jb723/LLaMA2-en-ko-7B-model,anli_r1,0-shot,brier_score,0.790322608509962,
jb723/LLaMA2-en-ko-7B-model,xnli_eu,0-shot,brier_score,1.1616926264930671,
jb723/LLaMA2-en-ko-7B-model,xnli_vi,0-shot,brier_score,1.281568581631835,
jb723/LLaMA2-en-ko-7B-model,xnli_ru,0-shot,brier_score,1.0656470677761996,
jb723/LLaMA2-en-ko-7B-model,xnli_zh,0-shot,brier_score,1.2528903645729945,
jb723/LLaMA2-en-ko-7B-model,xnli_tr,0-shot,brier_score,1.1733266271961436,
jb723/LLaMA2-en-ko-7B-model,xnli_fr,0-shot,brier_score,0.9317029900667162,
jb723/LLaMA2-en-ko-7B-model,xnli_en,0-shot,brier_score,0.7583959203972753,
jb723/LLaMA2-en-ko-7B-model,xnli_ur,0-shot,brier_score,1.2177675272267834,
jb723/LLaMA2-en-ko-7B-model,xnli_ar,0-shot,brier_score,1.2410505846779556,
jb723/LLaMA2-en-ko-7B-model,xnli_de,0-shot,brier_score,1.0386205045640462,
jb723/LLaMA2-en-ko-7B-model,xnli_hi,0-shot,brier_score,1.293753345210115,
jb723/LLaMA2-en-ko-7B-model,xnli_es,0-shot,brier_score,1.1731481806637558,
jb723/LLaMA2-en-ko-7B-model,xnli_bg,0-shot,brier_score,1.2046814609921392,
jb723/LLaMA2-en-ko-7B-model,xnli_sw,0-shot,brier_score,0.9985997618380864,
jb723/LLaMA2-en-ko-7B-model,xnli_el,0-shot,brier_score,1.2034257796980916,
jb723/LLaMA2-en-ko-7B-model,xnli_th,0-shot,brier_score,1.1949904072552373,
jb723/LLaMA2-en-ko-7B-model,logiqa2,0-shot,brier_score,1.062929306890202,
jb723/LLaMA2-en-ko-7B-model,mathqa,5-shot,brier_score,0.9595240469786607,
jb723/LLaMA2-en-ko-7B-model,lambada_standard,0-shot,perplexity,13.492654865070445,0.760087072196989
jb723/LLaMA2-en-ko-7B-model,lambada_standard,0-shot,accuracy,0.5649136425383272,0.006907021966670063
jb723/LLaMA2-en-ko-7B-model,lambada_openai,0-shot,perplexity,5.252813024296853,0.1956035665911124
jb723/LLaMA2-en-ko-7B-model,lambada_openai,0-shot,accuracy,0.6510770424995148,0.006640381581831476
jb723/LLaMA2-en-ko-7B-model,mmlu_world_religions,0-shot,accuracy,0.672514619883041,0.035993357714560276
jb723/LLaMA2-en-ko-7B-model,mmlu_formal_logic,0-shot,accuracy,0.24603174603174602,0.038522733649243156
jb723/LLaMA2-en-ko-7B-model,mmlu_prehistory,0-shot,accuracy,0.5154320987654321,0.027807490044276184
jb723/LLaMA2-en-ko-7B-model,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_world_history,0-shot,accuracy,0.6751054852320675,0.030486039389105307
jb723/LLaMA2-en-ko-7B-model,mmlu_moral_disputes,0-shot,accuracy,0.5260115606936416,0.02688264343402289
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_law,0-shot,accuracy,0.3546284224250326,0.012218576439090174
jb723/LLaMA2-en-ko-7B-model,mmlu_logical_fallacies,0-shot,accuracy,0.4785276073619632,0.0392474687675113
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_us_history,0-shot,accuracy,0.5637254901960784,0.03480693138457038
jb723/LLaMA2-en-ko-7B-model,mmlu_philosophy,0-shot,accuracy,0.5594855305466238,0.02819640057419743
jb723/LLaMA2-en-ko-7B-model,mmlu_jurisprudence,0-shot,accuracy,0.49074074074074076,0.04832853553437055
jb723/LLaMA2-en-ko-7B-model,mmlu_international_law,0-shot,accuracy,0.6033057851239669,0.04465869780531009
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_european_history,0-shot,accuracy,0.5757575757575758,0.038592681420702636
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6787564766839378,0.033699508685490674
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_microeconomics,0-shot,accuracy,0.41596638655462187,0.03201650100739615
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_geography,0-shot,accuracy,0.5909090909090909,0.03502975799413007
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_psychology,0-shot,accuracy,0.6201834862385321,0.020808825617866244
jb723/LLaMA2-en-ko-7B-model,mmlu_public_relations,0-shot,accuracy,0.5545454545454546,0.047605488214603246
jb723/LLaMA2-en-ko-7B-model,mmlu_us_foreign_policy,0-shot,accuracy,0.64,0.04824181513244218
jb723/LLaMA2-en-ko-7B-model,mmlu_sociology,0-shot,accuracy,0.5870646766169154,0.03481520803367348
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_macroeconomics,0-shot,accuracy,0.44358974358974357,0.025189149894764194
jb723/LLaMA2-en-ko-7B-model,mmlu_security_studies,0-shot,accuracy,0.4489795918367347,0.03184213866687579
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_psychology,0-shot,accuracy,0.4477124183006536,0.020116925347422425
jb723/LLaMA2-en-ko-7B-model,mmlu_human_sexuality,0-shot,accuracy,0.549618320610687,0.04363643698524779
jb723/LLaMA2-en-ko-7B-model,mmlu_econometrics,0-shot,accuracy,0.32456140350877194,0.04404556157374767
jb723/LLaMA2-en-ko-7B-model,mmlu_miscellaneous,0-shot,accuracy,0.6360153256704981,0.017205684809032232
jb723/LLaMA2-en-ko-7B-model,mmlu_marketing,0-shot,accuracy,0.6623931623931624,0.030980296992618558
jb723/LLaMA2-en-ko-7B-model,mmlu_management,0-shot,accuracy,0.5825242718446602,0.048828405482122375
jb723/LLaMA2-en-ko-7B-model,mmlu_nutrition,0-shot,accuracy,0.4803921568627451,0.028607893699576063
jb723/LLaMA2-en-ko-7B-model,mmlu_medical_genetics,0-shot,accuracy,0.56,0.049888765156985884
jb723/LLaMA2-en-ko-7B-model,mmlu_human_aging,0-shot,accuracy,0.5336322869955157,0.03348180017060306
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_medicine,0-shot,accuracy,0.4338235294117647,0.030105636570016636
jb723/LLaMA2-en-ko-7B-model,mmlu_college_medicine,0-shot,accuracy,0.43352601156069365,0.03778621079092056
jb723/LLaMA2-en-ko-7B-model,mmlu_business_ethics,0-shot,accuracy,0.48,0.050211673156867795
jb723/LLaMA2-en-ko-7B-model,mmlu_clinical_knowledge,0-shot,accuracy,0.4679245283018868,0.030709486992556552
jb723/LLaMA2-en-ko-7B-model,mmlu_global_facts,0-shot,accuracy,0.39,0.04902071300001974
jb723/LLaMA2-en-ko-7B-model,mmlu_virology,0-shot,accuracy,0.4397590361445783,0.03864139923699122
jb723/LLaMA2-en-ko-7B-model,mmlu_professional_accounting,0-shot,accuracy,0.3617021276595745,0.028663820147199492
jb723/LLaMA2-en-ko-7B-model,mmlu_college_physics,0-shot,accuracy,0.14705882352941177,0.035240689515674495
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_physics,0-shot,accuracy,0.31788079470198677,0.038020397601079024
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_biology,0-shot,accuracy,0.5,0.028444006199428714
jb723/LLaMA2-en-ko-7B-model,mmlu_college_biology,0-shot,accuracy,0.4652777777777778,0.04171115858181618
jb723/LLaMA2-en-ko-7B-model,mmlu_anatomy,0-shot,accuracy,0.4444444444444444,0.04292596718256981
jb723/LLaMA2-en-ko-7B-model,mmlu_college_chemistry,0-shot,accuracy,0.37,0.048523658709391
jb723/LLaMA2-en-ko-7B-model,mmlu_computer_security,0-shot,accuracy,0.51,0.05024183937956911
jb723/LLaMA2-en-ko-7B-model,mmlu_college_computer_science,0-shot,accuracy,0.35,0.047937248544110196
jb723/LLaMA2-en-ko-7B-model,mmlu_astronomy,0-shot,accuracy,0.4407894736842105,0.04040311062490437
jb723/LLaMA2-en-ko-7B-model,mmlu_college_mathematics,0-shot,accuracy,0.33,0.04725815626252604
jb723/LLaMA2-en-ko-7B-model,mmlu_conceptual_physics,0-shot,accuracy,0.4,0.03202563076101735
jb723/LLaMA2-en-ko-7B-model,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.046056618647183814
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_computer_science,0-shot,accuracy,0.41,0.04943110704237102
jb723/LLaMA2-en-ko-7B-model,mmlu_machine_learning,0-shot,accuracy,0.32142857142857145,0.04432804055291518
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_chemistry,0-shot,accuracy,0.32019704433497537,0.032826493853041504
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_statistics,0-shot,accuracy,0.2824074074074074,0.03070137211151092
jb723/LLaMA2-en-ko-7B-model,mmlu_elementary_mathematics,0-shot,accuracy,0.31746031746031744,0.023973861998992072
jb723/LLaMA2-en-ko-7B-model,mmlu_electrical_engineering,0-shot,accuracy,0.47586206896551725,0.0416180850350153
jb723/LLaMA2-en-ko-7B-model,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.026466117538959912
jb723/LLaMA2-en-ko-7B-model,arc_challenge,25-shot,accuracy,0.5136518771331058,0.014605943429860942
jb723/LLaMA2-en-ko-7B-model,arc_challenge,25-shot,acc_norm,0.53839590443686,0.014568245550296365
jb723/LLaMA2-en-ko-7B-model,hellaswag,10-shot,accuracy,0.6069508066122287,0.004874293964843518
jb723/LLaMA2-en-ko-7B-model,hellaswag,10-shot,acc_norm,0.7893845847440749,0.004069123905324906
jb723/LLaMA2-en-ko-7B-model,truthfulqa_mc2,0-shot,accuracy,0.4095607803946665,0.015544245050976095
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,bleu_max,20.89104935353322,0.690580917763846
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,bleu_acc,0.34394124847001223,0.016629087514276816
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,bleu_diff,-4.982714805919928,0.6525563274154834
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge1_max,46.0179025495015,0.8157872400574628
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge1_acc,0.34149326805385555,0.016600688619950833
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge1_diff,-7.817434391713708,0.7537330659697064
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge2_max,30.284883181541517,0.885344085472441
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge2_acc,0.2913096695226438,0.015905987048184824
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rouge2_diff,-8.624090028516706,0.8953473056334376
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rougeL_max,42.39510902130912,0.8131112049425046
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rougeL_acc,0.31334149326805383,0.016238065069059573
jb723/LLaMA2-en-ko-7B-model,truthfulqa_gen,0-shot,rougeL_diff,-8.169098689998256,0.7490350953113237
jb723/LLaMA2-en-ko-7B-model,truthfulqa_mc1,0-shot,accuracy,0.2876376988984088,0.015846315101394795
jb723/LLaMA2-en-ko-7B-model,winogrande,5-shot,accuracy,0.7000789265982637,0.012878347526636072
EleutherAI/pythia-410m-deduped,drop,3-shot,accuracy,0.0014681208053691276,0.0003921042190298293
EleutherAI/pythia-410m-deduped,drop,3-shot,f1,0.042572357382550524,0.0011637772390608397
EleutherAI/pythia-410m-deduped,gsm8k,5-shot,accuracy,0.003032600454890068,0.0015145735612245436
EleutherAI/pythia-410m-deduped,winogrande,5-shot,accuracy,0.5438042620363063,0.013998453610924331
EleutherAI/pythia-410m-deduped,arc:challenge,25-shot,accuracy,0.22610921501706485,0.012224202097063295
EleutherAI/pythia-410m-deduped,arc:challenge,25-shot,acc_norm,0.24829351535836178,0.012624912868089762
EleutherAI/pythia-410m-deduped,hellaswag,10-shot,accuracy,0.344353714399522,0.004741859753178411
EleutherAI/pythia-410m-deduped,hellaswag,10-shot,acc_norm,0.41286596295558653,0.00491342901055906
EleutherAI/pythia-410m-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.18,0.03861229196653697
EleutherAI/pythia-410m-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.18,0.03861229196653697
EleutherAI/pythia-410m-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.2740740740740741,0.03853254836552003
EleutherAI/pythia-410m-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.2740740740740741,0.03853254836552003
EleutherAI/pythia-410m-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.1513157894736842,0.029162631596843975
EleutherAI/pythia-410m-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.1513157894736842,0.029162631596843975
EleutherAI/pythia-410m-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-410m-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-410m-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2490566037735849,0.026616482980501715
EleutherAI/pythia-410m-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2490566037735849,0.026616482980501715
EleutherAI/pythia-410m-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.3263888888888889,0.03921067198982266
EleutherAI/pythia-410m-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.3263888888888889,0.03921067198982266
EleutherAI/pythia-410m-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-410m-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-410m-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.22,0.04163331998932268
EleutherAI/pythia-410m-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.22,0.04163331998932268
EleutherAI/pythia-410m-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-410m-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.044619604333847394
EleutherAI/pythia-410m-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2023121387283237,0.030631145539198823
EleutherAI/pythia-410m-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2023121387283237,0.030631145539198823
EleutherAI/pythia-410m-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.23529411764705882,0.04220773659171452
EleutherAI/pythia-410m-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.23529411764705882,0.04220773659171452
EleutherAI/pythia-410m-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-410m-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.044619604333847394
EleutherAI/pythia-410m-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.027501752944412424
EleutherAI/pythia-410m-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.027501752944412424
EleutherAI/pythia-410m-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.21052631578947367,0.038351539543994194
EleutherAI/pythia-410m-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.21052631578947367,0.038351539543994194
EleutherAI/pythia-410m-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.036001056927277716
EleutherAI/pythia-410m-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.036001056927277716
EleutherAI/pythia-410m-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2619047619047619,0.022644212615525218
EleutherAI/pythia-410m-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2619047619047619,0.022644212615525218
EleutherAI/pythia-410m-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.30952380952380953,0.04134913018303316
EleutherAI/pythia-410m-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.30952380952380953,0.04134913018303316
EleutherAI/pythia-410m-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.24,0.04292346959909281
EleutherAI/pythia-410m-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.24,0.04292346959909281
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.25161290322580643,0.024685979286239952
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.25161290322580643,0.024685979286239952
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.030516530732694436
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.030516530732694436
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.04292346959909284
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.04292346959909284
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.22424242424242424,0.032568666616811015
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.22424242424242424,0.032568666616811015
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.23232323232323232,0.030088629490217483
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.23232323232323232,0.030088629490217483
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.22797927461139897,0.030276909945178256
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.22797927461139897,0.030276909945178256
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3128205128205128,0.02350757902064535
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3128205128205128,0.02350757902064535
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26666666666666666,0.026962424325073828
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26666666666666666,0.026962424325073828
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.20168067226890757,0.02606431340630452
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.20168067226890757,0.02606431340630452
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.25165562913907286,0.035433042343899844
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.25165562913907286,0.035433042343899844
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.21651376146788992,0.01765871059444315
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.21651376146788992,0.01765871059444315
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.23529411764705882,0.029771775228145628
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.23529411764705882,0.029771775228145628
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.27848101265822783,0.029178682304842565
EleutherAI/pythia-410m-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.27848101265822783,0.029178682304842565
EleutherAI/pythia-410m-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.336322869955157,0.031708824268455
EleutherAI/pythia-410m-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.336322869955157,0.031708824268455
EleutherAI/pythia-410m-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.24427480916030533,0.03768335959728743
EleutherAI/pythia-410m-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.24427480916030533,0.03768335959728743
EleutherAI/pythia-410m-deduped,hendrycksTest-international_law,5-shot,accuracy,0.32231404958677684,0.04266416363352167
EleutherAI/pythia-410m-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.32231404958677684,0.04266416363352167
EleutherAI/pythia-410m-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.044143436668549335
EleutherAI/pythia-410m-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.044143436668549335
EleutherAI/pythia-410m-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.22699386503067484,0.032910995786157686
EleutherAI/pythia-410m-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.22699386503067484,0.032910995786157686
EleutherAI/pythia-410m-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.3392857142857143,0.04493949068613539
EleutherAI/pythia-410m-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.3392857142857143,0.04493949068613539
EleutherAI/pythia-410m-deduped,hendrycksTest-management,5-shot,accuracy,0.2815533980582524,0.044532548363264673
EleutherAI/pythia-410m-deduped,hendrycksTest-management,5-shot,acc_norm,0.2815533980582524,0.044532548363264673
EleutherAI/pythia-410m-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2264957264957265,0.027421007295392916
EleutherAI/pythia-410m-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2264957264957265,0.027421007295392916
EleutherAI/pythia-410m-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-410m-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-410m-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.26181353767560667,0.01572083867844526
EleutherAI/pythia-410m-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.26181353767560667,0.01572083867844526
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2514450867052023,0.02335736578587404
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2514450867052023,0.02335736578587404
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24134078212290502,0.014310999547961459
EleutherAI/pythia-410m-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24134078212290502,0.014310999547961459
EleutherAI/pythia-410m-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.22875816993464052,0.024051029739912255
EleutherAI/pythia-410m-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.22875816993464052,0.024051029739912255
EleutherAI/pythia-410m-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.022122439772480774
EleutherAI/pythia-410m-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.022122439772480774
EleutherAI/pythia-410m-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2037037037037037,0.022409674547304168
EleutherAI/pythia-410m-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2037037037037037,0.022409674547304168
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.24468085106382978,0.02564555362226673
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24468085106382978,0.02564555362226673
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.23989569752281617,0.010906282617981653
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.23989569752281617,0.010906282617981653
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.3235294117647059,0.02841820861940679
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3235294117647059,0.02841820861940679
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.24509803921568626,0.01740181671142766
EleutherAI/pythia-410m-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.24509803921568626,0.01740181671142766
EleutherAI/pythia-410m-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.04350271442923243
EleutherAI/pythia-410m-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.04350271442923243
EleutherAI/pythia-410m-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.3510204081632653,0.03055531675557364
EleutherAI/pythia-410m-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.3510204081632653,0.03055531675557364
EleutherAI/pythia-410m-deduped,hendrycksTest-sociology,5-shot,accuracy,0.27860696517412936,0.031700561834973086
EleutherAI/pythia-410m-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.27860696517412936,0.031700561834973086
EleutherAI/pythia-410m-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-410m-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.040936018074033256
EleutherAI/pythia-410m-deduped,hendrycksTest-virology,5-shot,accuracy,0.26506024096385544,0.03436024037944968
EleutherAI/pythia-410m-deduped,hendrycksTest-virology,5-shot,acc_norm,0.26506024096385544,0.03436024037944968
EleutherAI/pythia-410m-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.28654970760233917,0.03467826685703826
EleutherAI/pythia-410m-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.28654970760233917,0.03467826685703826
EleutherAI/pythia-410m-deduped,truthfulqa:mc,0-shot,mc1,0.23745410036719705,0.01489627744104184
EleutherAI/pythia-410m-deduped,truthfulqa:mc,0-shot,mc2,0.40947578592206774,0.014566854286767683
AbacusResearch/Jallabi-34B,arc:challenge,25-shot,accuracy,0.6348122866894198,0.014070265519268804
AbacusResearch/Jallabi-34B,arc:challenge,25-shot,acc_norm,0.6604095563139932,0.01383903976282017
AbacusResearch/Jallabi-34B,hellaswag,10-shot,accuracy,0.6382194781915953,0.004795337009118205
AbacusResearch/Jallabi-34B,hellaswag,10-shot,acc_norm,0.8380800637323242,0.0036762448867232664
AbacusResearch/Jallabi-34B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.51,0.05024183937956911
AbacusResearch/Jallabi-34B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.51,0.05024183937956911
AbacusResearch/Jallabi-34B,hendrycksTest-anatomy,5-shot,accuracy,0.7037037037037037,0.03944624162501116
AbacusResearch/Jallabi-34B,hendrycksTest-anatomy,5-shot,acc_norm,0.7037037037037037,0.03944624162501116
AbacusResearch/Jallabi-34B,hendrycksTest-astronomy,5-shot,accuracy,0.8552631578947368,0.028631951845930384
AbacusResearch/Jallabi-34B,hendrycksTest-astronomy,5-shot,acc_norm,0.8552631578947368,0.028631951845930384
AbacusResearch/Jallabi-34B,hendrycksTest-business_ethics,5-shot,accuracy,0.84,0.03684529491774709
AbacusResearch/Jallabi-34B,hendrycksTest-business_ethics,5-shot,acc_norm,0.84,0.03684529491774709
AbacusResearch/Jallabi-34B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8188679245283019,0.023702963526757798
AbacusResearch/Jallabi-34B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8188679245283019,0.023702963526757798
AbacusResearch/Jallabi-34B,hendrycksTest-college_biology,5-shot,accuracy,0.9166666666666666,0.023112508176051233
AbacusResearch/Jallabi-34B,hendrycksTest-college_biology,5-shot,acc_norm,0.9166666666666666,0.023112508176051233
AbacusResearch/Jallabi-34B,hendrycksTest-college_chemistry,5-shot,accuracy,0.53,0.050161355804659205
AbacusResearch/Jallabi-34B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.53,0.050161355804659205
AbacusResearch/Jallabi-34B,hendrycksTest-college_computer_science,5-shot,accuracy,0.65,0.047937248544110196
AbacusResearch/Jallabi-34B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.65,0.047937248544110196
AbacusResearch/Jallabi-34B,hendrycksTest-college_mathematics,5-shot,accuracy,0.45,0.04999999999999999
AbacusResearch/Jallabi-34B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.45,0.04999999999999999
AbacusResearch/Jallabi-34B,hendrycksTest-college_medicine,5-shot,accuracy,0.7052023121387283,0.034765996075164785
AbacusResearch/Jallabi-34B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7052023121387283,0.034765996075164785
AbacusResearch/Jallabi-34B,hendrycksTest-college_physics,5-shot,accuracy,0.5196078431372549,0.04971358884367405
AbacusResearch/Jallabi-34B,hendrycksTest-college_physics,5-shot,acc_norm,0.5196078431372549,0.04971358884367405
AbacusResearch/Jallabi-34B,hendrycksTest-computer_security,5-shot,accuracy,0.85,0.03588702812826369
AbacusResearch/Jallabi-34B,hendrycksTest-computer_security,5-shot,acc_norm,0.85,0.03588702812826369
AbacusResearch/Jallabi-34B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7659574468085106,0.027678452578212387
AbacusResearch/Jallabi-34B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7659574468085106,0.027678452578212387
AbacusResearch/Jallabi-34B,hendrycksTest-econometrics,5-shot,accuracy,0.5614035087719298,0.04668000738510455
AbacusResearch/Jallabi-34B,hendrycksTest-econometrics,5-shot,acc_norm,0.5614035087719298,0.04668000738510455
AbacusResearch/Jallabi-34B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7586206896551724,0.03565998174135302
AbacusResearch/Jallabi-34B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7586206896551724,0.03565998174135302
AbacusResearch/Jallabi-34B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.7407407407407407,0.022569897074918435
AbacusResearch/Jallabi-34B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.7407407407407407,0.022569897074918435
AbacusResearch/Jallabi-34B,hendrycksTest-formal_logic,5-shot,accuracy,0.5396825396825397,0.04458029125470973
AbacusResearch/Jallabi-34B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5396825396825397,0.04458029125470973
AbacusResearch/Jallabi-34B,hendrycksTest-global_facts,5-shot,accuracy,0.55,0.05
AbacusResearch/Jallabi-34B,hendrycksTest-global_facts,5-shot,acc_norm,0.55,0.05
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_biology,5-shot,accuracy,0.9032258064516129,0.016818943416345197
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.9032258064516129,0.016818943416345197
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6157635467980296,0.0342239856565755
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6157635467980296,0.0342239856565755
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.8,0.04020151261036846
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.8,0.04020151261036846
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8727272727272727,0.026024657651656187
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8727272727272727,0.026024657651656187
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_geography,5-shot,accuracy,0.9090909090909091,0.020482086775424218
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9090909090909091,0.020482086775424218
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9689119170984456,0.01252531062552703
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9689119170984456,0.01252531062552703
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8102564102564103,0.019880165406588792
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8102564102564103,0.019880165406588792
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.44074074074074077,0.030270671157284067
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.44074074074074077,0.030270671157284067
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8613445378151261,0.02244826447683258
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8613445378151261,0.02244826447683258
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_physics,5-shot,accuracy,0.4966887417218543,0.04082393379449654
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4966887417218543,0.04082393379449654
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9192660550458716,0.011680172292862076
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9192660550458716,0.011680172292862076
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6342592592592593,0.032847388576472056
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6342592592592593,0.032847388576472056
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9215686274509803,0.01886951464665893
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9215686274509803,0.01886951464665893
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.919831223628692,0.01767667999189163
AbacusResearch/Jallabi-34B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.919831223628692,0.01767667999189163
AbacusResearch/Jallabi-34B,hendrycksTest-human_aging,5-shot,accuracy,0.7757847533632287,0.027991534258519513
AbacusResearch/Jallabi-34B,hendrycksTest-human_aging,5-shot,acc_norm,0.7757847533632287,0.027991534258519513
AbacusResearch/Jallabi-34B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8549618320610687,0.03088466108951538
AbacusResearch/Jallabi-34B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8549618320610687,0.03088466108951538
AbacusResearch/Jallabi-34B,hendrycksTest-international_law,5-shot,accuracy,0.9173553719008265,0.02513538235660422
AbacusResearch/Jallabi-34B,hendrycksTest-international_law,5-shot,acc_norm,0.9173553719008265,0.02513538235660422
AbacusResearch/Jallabi-34B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8981481481481481,0.029239272675632748
AbacusResearch/Jallabi-34B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8981481481481481,0.029239272675632748
AbacusResearch/Jallabi-34B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8650306748466258,0.02684576505455386
AbacusResearch/Jallabi-34B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8650306748466258,0.02684576505455386
AbacusResearch/Jallabi-34B,hendrycksTest-machine_learning,5-shot,accuracy,0.6428571428571429,0.04547960999764376
AbacusResearch/Jallabi-34B,hendrycksTest-machine_learning,5-shot,acc_norm,0.6428571428571429,0.04547960999764376
AbacusResearch/Jallabi-34B,hendrycksTest-management,5-shot,accuracy,0.9029126213592233,0.02931596291881348
AbacusResearch/Jallabi-34B,hendrycksTest-management,5-shot,acc_norm,0.9029126213592233,0.02931596291881348
AbacusResearch/Jallabi-34B,hendrycksTest-marketing,5-shot,accuracy,0.9145299145299145,0.018315891685625845
AbacusResearch/Jallabi-34B,hendrycksTest-marketing,5-shot,acc_norm,0.9145299145299145,0.018315891685625845
AbacusResearch/Jallabi-34B,hendrycksTest-medical_genetics,5-shot,accuracy,0.89,0.03144660377352202
AbacusResearch/Jallabi-34B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.89,0.03144660377352202
AbacusResearch/Jallabi-34B,hendrycksTest-miscellaneous,5-shot,accuracy,0.913154533844189,0.010070298377747786
AbacusResearch/Jallabi-34B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.913154533844189,0.010070298377747786
AbacusResearch/Jallabi-34B,hendrycksTest-moral_disputes,5-shot,accuracy,0.8034682080924855,0.021393961404363847
AbacusResearch/Jallabi-34B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8034682080924855,0.021393961404363847
AbacusResearch/Jallabi-34B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6703910614525139,0.01572153107518387
AbacusResearch/Jallabi-34B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6703910614525139,0.01572153107518387
AbacusResearch/Jallabi-34B,hendrycksTest-nutrition,5-shot,accuracy,0.8431372549019608,0.02082375883758091
AbacusResearch/Jallabi-34B,hendrycksTest-nutrition,5-shot,acc_norm,0.8431372549019608,0.02082375883758091
AbacusResearch/Jallabi-34B,hendrycksTest-philosophy,5-shot,accuracy,0.819935691318328,0.02182342285774494
AbacusResearch/Jallabi-34B,hendrycksTest-philosophy,5-shot,acc_norm,0.819935691318328,0.02182342285774494
AbacusResearch/Jallabi-34B,hendrycksTest-prehistory,5-shot,accuracy,0.8672839506172839,0.01887735383957184
AbacusResearch/Jallabi-34B,hendrycksTest-prehistory,5-shot,acc_norm,0.8672839506172839,0.01887735383957184
AbacusResearch/Jallabi-34B,hendrycksTest-professional_accounting,5-shot,accuracy,0.6276595744680851,0.028838921471251458
AbacusResearch/Jallabi-34B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6276595744680851,0.028838921471251458
AbacusResearch/Jallabi-34B,hendrycksTest-professional_law,5-shot,accuracy,0.5775749674054759,0.012615600475734921
AbacusResearch/Jallabi-34B,hendrycksTest-professional_law,5-shot,acc_norm,0.5775749674054759,0.012615600475734921
AbacusResearch/Jallabi-34B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8272058823529411,0.022966067585581784
AbacusResearch/Jallabi-34B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8272058823529411,0.022966067585581784
AbacusResearch/Jallabi-34B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8104575163398693,0.015856152189980263
AbacusResearch/Jallabi-34B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8104575163398693,0.015856152189980263
AbacusResearch/Jallabi-34B,hendrycksTest-public_relations,5-shot,accuracy,0.7,0.04389311454644287
AbacusResearch/Jallabi-34B,hendrycksTest-public_relations,5-shot,acc_norm,0.7,0.04389311454644287
AbacusResearch/Jallabi-34B,hendrycksTest-security_studies,5-shot,accuracy,0.8408163265306122,0.023420972069166344
AbacusResearch/Jallabi-34B,hendrycksTest-security_studies,5-shot,acc_norm,0.8408163265306122,0.023420972069166344
AbacusResearch/Jallabi-34B,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.02207632610182466
AbacusResearch/Jallabi-34B,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.02207632610182466
AbacusResearch/Jallabi-34B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.9,0.030151134457776334
AbacusResearch/Jallabi-34B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.9,0.030151134457776334
AbacusResearch/Jallabi-34B,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721015
AbacusResearch/Jallabi-34B,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721015
AbacusResearch/Jallabi-34B,hendrycksTest-world_religions,5-shot,accuracy,0.9005847953216374,0.022949025579355027
AbacusResearch/Jallabi-34B,hendrycksTest-world_religions,5-shot,acc_norm,0.9005847953216374,0.022949025579355027
AbacusResearch/Jallabi-34B,truthfulqa:mc,0-shot,mc1,0.3659730722154223,0.016862941684088365
AbacusResearch/Jallabi-34B,truthfulqa:mc,0-shot,mc2,0.5146389940410719,0.015020552354313921
AbacusResearch/Jallabi-34B,winogrande,5-shot,accuracy,0.8145224940805051,0.010923965303140505
AbacusResearch/Jallabi-34B,gsm8k,5-shot,accuracy,0.6520090978013646,0.013120581030382132
mosaicml/mpt-7b-storywriter,arc:challenge,25-shot,accuracy,0.17320819112627986,0.011058694183280334
mosaicml/mpt-7b-storywriter,arc:challenge,25-shot,acc_norm,0.20392491467576793,0.011774262478702249
mosaicml/mpt-7b-storywriter,hellaswag,10-shot,accuracy,0.5461063533160725,0.004968521608065447
mosaicml/mpt-7b-storywriter,hellaswag,10-shot,acc_norm,0.7425811591316471,0.004363185172047184
mosaicml/mpt-7b-storywriter,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-anatomy,5-shot,accuracy,0.32592592592592595,0.040491220417025055
mosaicml/mpt-7b-storywriter,hendrycksTest-anatomy,5-shot,acc_norm,0.32592592592592595,0.040491220417025055
mosaicml/mpt-7b-storywriter,hendrycksTest-astronomy,5-shot,accuracy,0.18421052631578946,0.0315469804508223
mosaicml/mpt-7b-storywriter,hendrycksTest-astronomy,5-shot,acc_norm,0.18421052631578946,0.0315469804508223
mosaicml/mpt-7b-storywriter,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.044619604333847394
mosaicml/mpt-7b-storywriter,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.044619604333847394
mosaicml/mpt-7b-storywriter,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.22264150943396227,0.0256042334708991
mosaicml/mpt-7b-storywriter,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.22264150943396227,0.0256042334708991
mosaicml/mpt-7b-storywriter,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.034765901043041336
mosaicml/mpt-7b-storywriter,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.034765901043041336
mosaicml/mpt-7b-storywriter,hendrycksTest-college_chemistry,5-shot,accuracy,0.28,0.04512608598542127
mosaicml/mpt-7b-storywriter,hendrycksTest-college_chemistry,5-shot,acc_norm,0.28,0.04512608598542127
mosaicml/mpt-7b-storywriter,hendrycksTest-college_computer_science,5-shot,accuracy,0.15,0.03588702812826372
mosaicml/mpt-7b-storywriter,hendrycksTest-college_computer_science,5-shot,acc_norm,0.15,0.03588702812826372
mosaicml/mpt-7b-storywriter,hendrycksTest-college_mathematics,5-shot,accuracy,0.2,0.04020151261036846
mosaicml/mpt-7b-storywriter,hendrycksTest-college_mathematics,5-shot,acc_norm,0.2,0.04020151261036846
mosaicml/mpt-7b-storywriter,hendrycksTest-college_medicine,5-shot,accuracy,0.2138728323699422,0.03126511206173043
mosaicml/mpt-7b-storywriter,hendrycksTest-college_medicine,5-shot,acc_norm,0.2138728323699422,0.03126511206173043
mosaicml/mpt-7b-storywriter,hendrycksTest-college_physics,5-shot,accuracy,0.28431372549019607,0.04488482852329017
mosaicml/mpt-7b-storywriter,hendrycksTest-college_physics,5-shot,acc_norm,0.28431372549019607,0.04488482852329017
mosaicml/mpt-7b-storywriter,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.04229525846816505
mosaicml/mpt-7b-storywriter,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.04229525846816505
mosaicml/mpt-7b-storywriter,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.027501752944412424
mosaicml/mpt-7b-storywriter,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.027501752944412424
mosaicml/mpt-7b-storywriter,hendrycksTest-econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
mosaicml/mpt-7b-storywriter,hendrycksTest-econometrics,5-shot,acc_norm,0.22807017543859648,0.03947152782669415
mosaicml/mpt-7b-storywriter,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.03664666337225257
mosaicml/mpt-7b-storywriter,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.03664666337225257
mosaicml/mpt-7b-storywriter,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918424
mosaicml/mpt-7b-storywriter,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.022569897074918424
mosaicml/mpt-7b-storywriter,hendrycksTest-formal_logic,5-shot,accuracy,0.20634920634920634,0.036196045241242515
mosaicml/mpt-7b-storywriter,hendrycksTest-formal_logic,5-shot,acc_norm,0.20634920634920634,0.036196045241242515
mosaicml/mpt-7b-storywriter,hendrycksTest-global_facts,5-shot,accuracy,0.21,0.040936018074033256
mosaicml/mpt-7b-storywriter,hendrycksTest-global_facts,5-shot,acc_norm,0.21,0.040936018074033256
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_biology,5-shot,accuracy,0.3096774193548387,0.026302774983517418
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3096774193548387,0.026302774983517418
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.030516530732694436
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.030516530732694436
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.2,0.04020151261036845
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.2,0.04020151261036845
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23636363636363636,0.03317505930009179
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23636363636363636,0.03317505930009179
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_geography,5-shot,accuracy,0.2474747474747475,0.030746300742124488
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2474747474747475,0.030746300742124488
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.22797927461139897,0.030276909945178256
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.22797927461139897,0.030276909945178256
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.20512820512820512,0.02047323317355198
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.20512820512820512,0.02047323317355198
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24074074074074073,0.026067159222275788
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24074074074074073,0.026067159222275788
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23109243697478993,0.027381406927868966
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23109243697478993,0.027381406927868966
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.03297986648473835
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.03297986648473835
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_psychology,5-shot,accuracy,0.20917431192660552,0.017437937173343226
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.20917431192660552,0.017437937173343226
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4398148148148148,0.03385177976044811
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4398148148148148,0.03385177976044811
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_us_history,5-shot,accuracy,0.24509803921568626,0.030190282453501947
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.24509803921568626,0.030190282453501947
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_world_history,5-shot,accuracy,0.23628691983122363,0.027652153144159263
mosaicml/mpt-7b-storywriter,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.23628691983122363,0.027652153144159263
mosaicml/mpt-7b-storywriter,hendrycksTest-human_aging,5-shot,accuracy,0.2242152466367713,0.027991534258519527
mosaicml/mpt-7b-storywriter,hendrycksTest-human_aging,5-shot,acc_norm,0.2242152466367713,0.027991534258519527
mosaicml/mpt-7b-storywriter,hendrycksTest-human_sexuality,5-shot,accuracy,0.25190839694656486,0.03807387116306086
mosaicml/mpt-7b-storywriter,hendrycksTest-human_sexuality,5-shot,acc_norm,0.25190839694656486,0.03807387116306086
mosaicml/mpt-7b-storywriter,hendrycksTest-international_law,5-shot,accuracy,0.24793388429752067,0.03941897526516303
mosaicml/mpt-7b-storywriter,hendrycksTest-international_law,5-shot,acc_norm,0.24793388429752067,0.03941897526516303
mosaicml/mpt-7b-storywriter,hendrycksTest-jurisprudence,5-shot,accuracy,0.21296296296296297,0.0395783547198098
mosaicml/mpt-7b-storywriter,hendrycksTest-jurisprudence,5-shot,acc_norm,0.21296296296296297,0.0395783547198098
mosaicml/mpt-7b-storywriter,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25153374233128833,0.034089978868575295
mosaicml/mpt-7b-storywriter,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25153374233128833,0.034089978868575295
mosaicml/mpt-7b-storywriter,hendrycksTest-machine_learning,5-shot,accuracy,0.19642857142857142,0.03770970049347018
mosaicml/mpt-7b-storywriter,hendrycksTest-machine_learning,5-shot,acc_norm,0.19642857142857142,0.03770970049347018
mosaicml/mpt-7b-storywriter,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.03916667762822585
mosaicml/mpt-7b-storywriter,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.03916667762822585
mosaicml/mpt-7b-storywriter,hendrycksTest-marketing,5-shot,accuracy,0.23504273504273504,0.02777883590493543
mosaicml/mpt-7b-storywriter,hendrycksTest-marketing,5-shot,acc_norm,0.23504273504273504,0.02777883590493543
mosaicml/mpt-7b-storywriter,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.0446196043338474
mosaicml/mpt-7b-storywriter,hendrycksTest-miscellaneous,5-shot,accuracy,0.25925925925925924,0.01567100600933957
mosaicml/mpt-7b-storywriter,hendrycksTest-miscellaneous,5-shot,acc_norm,0.25925925925925924,0.01567100600933957
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_disputes,5-shot,accuracy,0.21676300578034682,0.022183477668412856
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_disputes,5-shot,acc_norm,0.21676300578034682,0.022183477668412856
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23798882681564246,0.014242630070574915
mosaicml/mpt-7b-storywriter,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23798882681564246,0.014242630070574915
mosaicml/mpt-7b-storywriter,hendrycksTest-nutrition,5-shot,accuracy,0.22549019607843138,0.02392915551735129
mosaicml/mpt-7b-storywriter,hendrycksTest-nutrition,5-shot,acc_norm,0.22549019607843138,0.02392915551735129
mosaicml/mpt-7b-storywriter,hendrycksTest-philosophy,5-shot,accuracy,0.22508038585209003,0.023720088516179027
mosaicml/mpt-7b-storywriter,hendrycksTest-philosophy,5-shot,acc_norm,0.22508038585209003,0.023720088516179027
mosaicml/mpt-7b-storywriter,hendrycksTest-prehistory,5-shot,accuracy,0.25,0.02409347123262133
mosaicml/mpt-7b-storywriter,hendrycksTest-prehistory,5-shot,acc_norm,0.25,0.02409347123262133
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_accounting,5-shot,accuracy,0.2375886524822695,0.025389512552729903
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2375886524822695,0.025389512552729903
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_law,5-shot,accuracy,0.24967405475880053,0.011054538377832324
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_law,5-shot,acc_norm,0.24967405475880053,0.011054538377832324
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_medicine,5-shot,accuracy,0.24632352941176472,0.02617343857052
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_medicine,5-shot,acc_norm,0.24632352941176472,0.02617343857052
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_psychology,5-shot,accuracy,0.25980392156862747,0.01774089950917779
mosaicml/mpt-7b-storywriter,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25980392156862747,0.01774089950917779
mosaicml/mpt-7b-storywriter,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.04172343038705383
mosaicml/mpt-7b-storywriter,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.04172343038705383
mosaicml/mpt-7b-storywriter,hendrycksTest-security_studies,5-shot,accuracy,0.2653061224489796,0.028263889943784593
mosaicml/mpt-7b-storywriter,hendrycksTest-security_studies,5-shot,acc_norm,0.2653061224489796,0.028263889943784593
mosaicml/mpt-7b-storywriter,hendrycksTest-sociology,5-shot,accuracy,0.27860696517412936,0.031700561834973086
mosaicml/mpt-7b-storywriter,hendrycksTest-sociology,5-shot,acc_norm,0.27860696517412936,0.031700561834973086
mosaicml/mpt-7b-storywriter,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.27,0.044619604333847394
mosaicml/mpt-7b-storywriter,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.27,0.044619604333847394
mosaicml/mpt-7b-storywriter,hendrycksTest-virology,5-shot,accuracy,0.2469879518072289,0.03357351982064536
mosaicml/mpt-7b-storywriter,hendrycksTest-virology,5-shot,acc_norm,0.2469879518072289,0.03357351982064536
mosaicml/mpt-7b-storywriter,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.03446296217088427
mosaicml/mpt-7b-storywriter,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.03446296217088427
mosaicml/mpt-7b-storywriter,truthfulqa:mc,0-shot,mc1,0.23623011015911874,0.014869755015871091
mosaicml/mpt-7b-storywriter,truthfulqa:mc,0-shot,mc2,0.48439801230953305,0.016279319966019105
mosaicml/mpt-7b-storywriter,drop,3-shot,accuracy,0.0006291946308724832,0.00025680027497237983
mosaicml/mpt-7b-storywriter,drop,3-shot,f1,0.0032026006711409396,0.0005040610386397096
mosaicml/mpt-7b-storywriter,gsm8k,5-shot,accuracy,0.05155420773313116,0.00609088795526282
mosaicml/mpt-7b-storywriter,winogrande,5-shot,accuracy,0.6827150749802684,0.01308059841133212
meta-llama/Meta-Llama-3-8B,arc:challenge,25-shot,accuracy,0.5409556313993175,0.014562291073601226
meta-llama/Meta-Llama-3-8B,arc:challenge,25-shot,acc_norm,0.5921501706484642,0.014361097288449701
meta-llama/Meta-Llama-3-8B,hellaswag,10-shot,accuracy,0.6176060545708026,0.004849788423944358
meta-llama/Meta-Llama-3-8B,hellaswag,10-shot,acc_norm,0.8201553475403306,0.003832731017592122
meta-llama/Meta-Llama-3-8B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.046056618647183814
meta-llama/Meta-Llama-3-8B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.046056618647183814
meta-llama/Meta-Llama-3-8B,hendrycksTest-anatomy,5-shot,accuracy,0.6888888888888889,0.039992628766177214
meta-llama/Meta-Llama-3-8B,hendrycksTest-anatomy,5-shot,acc_norm,0.6888888888888889,0.039992628766177214
meta-llama/Meta-Llama-3-8B,hendrycksTest-astronomy,5-shot,accuracy,0.6842105263157895,0.037827289808654685
meta-llama/Meta-Llama-3-8B,hendrycksTest-astronomy,5-shot,acc_norm,0.6842105263157895,0.037827289808654685
meta-llama/Meta-Llama-3-8B,hendrycksTest-business_ethics,5-shot,accuracy,0.62,0.048783173121456316
meta-llama/Meta-Llama-3-8B,hendrycksTest-business_ethics,5-shot,acc_norm,0.62,0.048783173121456316
meta-llama/Meta-Llama-3-8B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7471698113207547,0.02674989977124121
meta-llama/Meta-Llama-3-8B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7471698113207547,0.02674989977124121
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_biology,5-shot,accuracy,0.7777777777777778,0.03476590104304134
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_biology,5-shot,acc_norm,0.7777777777777778,0.03476590104304134
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_chemistry,5-shot,accuracy,0.46,0.05009082659620332
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.46,0.05009082659620332
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_computer_science,5-shot,accuracy,0.5,0.050251890762960605
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.5,0.050251890762960605
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_mathematics,5-shot,accuracy,0.34,0.04760952285695235
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.34,0.04760952285695235
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_medicine,5-shot,accuracy,0.653179190751445,0.036291466701596636
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_medicine,5-shot,acc_norm,0.653179190751445,0.036291466701596636
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_physics,5-shot,accuracy,0.5098039215686274,0.04974229460422817
meta-llama/Meta-Llama-3-8B,hendrycksTest-college_physics,5-shot,acc_norm,0.5098039215686274,0.04974229460422817
meta-llama/Meta-Llama-3-8B,hendrycksTest-computer_security,5-shot,accuracy,0.81,0.03942772444036624
meta-llama/Meta-Llama-3-8B,hendrycksTest-computer_security,5-shot,acc_norm,0.81,0.03942772444036624
meta-llama/Meta-Llama-3-8B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.5829787234042553,0.03223276266711712
meta-llama/Meta-Llama-3-8B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.5829787234042553,0.03223276266711712
meta-llama/Meta-Llama-3-8B,hendrycksTest-econometrics,5-shot,accuracy,0.49122807017543857,0.047028804320496165
meta-llama/Meta-Llama-3-8B,hendrycksTest-econometrics,5-shot,acc_norm,0.49122807017543857,0.047028804320496165
meta-llama/Meta-Llama-3-8B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6413793103448275,0.039966295748767186
meta-llama/Meta-Llama-3-8B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6413793103448275,0.039966295748767186
meta-llama/Meta-Llama-3-8B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.42592592592592593,0.025467149045469543
meta-llama/Meta-Llama-3-8B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.42592592592592593,0.025467149045469543
meta-llama/Meta-Llama-3-8B,hendrycksTest-formal_logic,5-shot,accuracy,0.48412698412698413,0.04469881854072606
meta-llama/Meta-Llama-3-8B,hendrycksTest-formal_logic,5-shot,acc_norm,0.48412698412698413,0.04469881854072606
meta-llama/Meta-Llama-3-8B,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.048241815132442176
meta-llama/Meta-Llama-3-8B,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.048241815132442176
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_biology,5-shot,accuracy,0.7774193548387097,0.023664216671642507
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7774193548387097,0.023664216671642507
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.03512819077876106
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.03512819077876106
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.67,0.04725815626252609
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.67,0.04725815626252609
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7696969696969697,0.0328766675860349
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7696969696969697,0.0328766675860349
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_geography,5-shot,accuracy,0.797979797979798,0.028606204289229862
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.797979797979798,0.028606204289229862
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.8911917098445595,0.022473253332768766
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.8911917098445595,0.022473253332768766
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6461538461538462,0.024243783994062164
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6461538461538462,0.024243783994062164
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3888888888888889,0.029723278961476664
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3888888888888889,0.029723278961476664
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.726890756302521,0.028942004040998167
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.726890756302521,0.028942004040998167
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_physics,5-shot,accuracy,0.45695364238410596,0.04067325174247443
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.45695364238410596,0.04067325174247443
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8385321100917431,0.01577623925616325
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8385321100917431,0.01577623925616325
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5416666666666666,0.03398110890294636
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5416666666666666,0.03398110890294636
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8529411764705882,0.024857478080250475
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8529411764705882,0.024857478080250475
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8185654008438819,0.02508596114457965
meta-llama/Meta-Llama-3-8B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8185654008438819,0.02508596114457965
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_aging,5-shot,accuracy,0.7130044843049327,0.03036037971029195
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_aging,5-shot,acc_norm,0.7130044843049327,0.03036037971029195
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_sexuality,5-shot,accuracy,0.7938931297709924,0.03547771004159463
meta-llama/Meta-Llama-3-8B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7938931297709924,0.03547771004159463
meta-llama/Meta-Llama-3-8B,hendrycksTest-international_law,5-shot,accuracy,0.8512396694214877,0.03248470083807194
meta-llama/Meta-Llama-3-8B,hendrycksTest-international_law,5-shot,acc_norm,0.8512396694214877,0.03248470083807194
meta-llama/Meta-Llama-3-8B,hendrycksTest-jurisprudence,5-shot,accuracy,0.7592592592592593,0.04133119440243839
meta-llama/Meta-Llama-3-8B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7592592592592593,0.04133119440243839
meta-llama/Meta-Llama-3-8B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7361963190184049,0.03462419931615623
meta-llama/Meta-Llama-3-8B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7361963190184049,0.03462419931615623
meta-llama/Meta-Llama-3-8B,hendrycksTest-machine_learning,5-shot,accuracy,0.5178571428571429,0.047427623612430116
meta-llama/Meta-Llama-3-8B,hendrycksTest-machine_learning,5-shot,acc_norm,0.5178571428571429,0.047427623612430116
meta-llama/Meta-Llama-3-8B,hendrycksTest-management,5-shot,accuracy,0.8737864077669902,0.03288180278808628
meta-llama/Meta-Llama-3-8B,hendrycksTest-management,5-shot,acc_norm,0.8737864077669902,0.03288180278808628
meta-llama/Meta-Llama-3-8B,hendrycksTest-marketing,5-shot,accuracy,0.8931623931623932,0.020237149008990932
meta-llama/Meta-Llama-3-8B,hendrycksTest-marketing,5-shot,acc_norm,0.8931623931623932,0.020237149008990932
meta-llama/Meta-Llama-3-8B,hendrycksTest-medical_genetics,5-shot,accuracy,0.77,0.04229525846816506
meta-llama/Meta-Llama-3-8B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.77,0.04229525846816506
meta-llama/Meta-Llama-3-8B,hendrycksTest-miscellaneous,5-shot,accuracy,0.8365261813537676,0.013223928616741609
meta-llama/Meta-Llama-3-8B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8365261813537676,0.013223928616741609
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_disputes,5-shot,accuracy,0.7398843930635838,0.023618678310069356
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7398843930635838,0.023618678310069356
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.423463687150838,0.016525425898773507
meta-llama/Meta-Llama-3-8B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.423463687150838,0.016525425898773507
meta-llama/Meta-Llama-3-8B,hendrycksTest-nutrition,5-shot,accuracy,0.7679738562091504,0.02417084087934084
meta-llama/Meta-Llama-3-8B,hendrycksTest-nutrition,5-shot,acc_norm,0.7679738562091504,0.02417084087934084
meta-llama/Meta-Llama-3-8B,hendrycksTest-philosophy,5-shot,accuracy,0.7459807073954984,0.024723861504771696
meta-llama/Meta-Llama-3-8B,hendrycksTest-philosophy,5-shot,acc_norm,0.7459807073954984,0.024723861504771696
meta-llama/Meta-Llama-3-8B,hendrycksTest-prehistory,5-shot,accuracy,0.7253086419753086,0.02483605786829468
meta-llama/Meta-Llama-3-8B,hendrycksTest-prehistory,5-shot,acc_norm,0.7253086419753086,0.02483605786829468
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_accounting,5-shot,accuracy,0.4858156028368794,0.02981549448368206
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4858156028368794,0.02981549448368206
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_law,5-shot,accuracy,0.46284224250325945,0.012734923579532072
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_law,5-shot,acc_norm,0.46284224250325945,0.012734923579532072
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_medicine,5-shot,accuracy,0.7242647058823529,0.027146271936625162
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.7242647058823529,0.027146271936625162
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_psychology,5-shot,accuracy,0.7238562091503268,0.018087276935663137
meta-llama/Meta-Llama-3-8B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7238562091503268,0.018087276935663137
meta-llama/Meta-Llama-3-8B,hendrycksTest-public_relations,5-shot,accuracy,0.7,0.04389311454644287
meta-llama/Meta-Llama-3-8B,hendrycksTest-public_relations,5-shot,acc_norm,0.7,0.04389311454644287
meta-llama/Meta-Llama-3-8B,hendrycksTest-security_studies,5-shot,accuracy,0.7510204081632653,0.02768297952296022
meta-llama/Meta-Llama-3-8B,hendrycksTest-security_studies,5-shot,acc_norm,0.7510204081632653,0.02768297952296022
meta-llama/Meta-Llama-3-8B,hendrycksTest-sociology,5-shot,accuracy,0.8656716417910447,0.024112678240900836
meta-llama/Meta-Llama-3-8B,hendrycksTest-sociology,5-shot,acc_norm,0.8656716417910447,0.024112678240900836
meta-llama/Meta-Llama-3-8B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.87,0.033799766898963086
meta-llama/Meta-Llama-3-8B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.87,0.033799766898963086
meta-llama/Meta-Llama-3-8B,hendrycksTest-virology,5-shot,accuracy,0.5542168674698795,0.038695433234721015
meta-llama/Meta-Llama-3-8B,hendrycksTest-virology,5-shot,acc_norm,0.5542168674698795,0.038695433234721015
meta-llama/Meta-Llama-3-8B,hendrycksTest-world_religions,5-shot,accuracy,0.8362573099415205,0.028380919596145866
meta-llama/Meta-Llama-3-8B,hendrycksTest-world_religions,5-shot,acc_norm,0.8362573099415205,0.028380919596145866
meta-llama/Meta-Llama-3-8B,truthfulqa:mc,0-shot,mc1,0.26805385556915545,0.015506204722834557
meta-llama/Meta-Llama-3-8B,truthfulqa:mc,0-shot,mc2,0.4395226511050948,0.0139343637688421
meta-llama/Meta-Llama-3-8B,winogrande,5-shot,accuracy,0.771112865035517,0.011807360224025397
meta-llama/Meta-Llama-3-8B,gsm8k,5-shot,accuracy,0.5018953752843063,0.013772385765569753
meta-llama/Meta-Llama-3-8B,minerva_math_precalc,5-shot,accuracy,0.06227106227106227,0.010351029472773795
meta-llama/Meta-Llama-3-8B,minerva_math_prealgebra,5-shot,accuracy,0.2973593570608496,0.01549700637866032
meta-llama/Meta-Llama-3-8B,minerva_math_num_theory,5-shot,accuracy,0.07962962962962963,0.011660690804827262
meta-llama/Meta-Llama-3-8B,minerva_math_intermediate_algebra,5-shot,accuracy,0.05647840531561462,0.007686238678355193
meta-llama/Meta-Llama-3-8B,minerva_math_geometry,5-shot,accuracy,0.1315240083507307,0.015458504556847509
meta-llama/Meta-Llama-3-8B,minerva_math_counting_and_prob,5-shot,accuracy,0.13924050632911392,0.015918169955367448
meta-llama/Meta-Llama-3-8B,minerva_math_algebra,5-shot,accuracy,0.2333614153327717,0.012281955435280458
meta-llama/Meta-Llama-3-8B,fld_default,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B,fld_star,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B,arithmetic_3da,5-shot,accuracy,0.999,0.0007069298939339491
meta-llama/Meta-Llama-3-8B,arithmetic_3ds,5-shot,accuracy,0.827,0.008459981420950169
meta-llama/Meta-Llama-3-8B,arithmetic_4da,5-shot,accuracy,0.8865,0.007094648829999189
meta-llama/Meta-Llama-3-8B,arithmetic_2ds,5-shot,accuracy,0.998,0.0009992493430694847
meta-llama/Meta-Llama-3-8B,arithmetic_5ds,5-shot,accuracy,0.716,0.010085775202269404
meta-llama/Meta-Llama-3-8B,arithmetic_5da,5-shot,accuracy,0.7945,0.009037461637895063
meta-llama/Meta-Llama-3-8B,arithmetic_1dc,5-shot,accuracy,0.8715,0.007484776946774906
meta-llama/Meta-Llama-3-8B,arithmetic_4ds,5-shot,accuracy,0.798,0.008979884139540956
meta-llama/Meta-Llama-3-8B,arithmetic_2dm,5-shot,accuracy,0.798,0.008979884139540966
meta-llama/Meta-Llama-3-8B,arithmetic_2da,5-shot,accuracy,1.0,
meta-llama/Meta-Llama-3-8B,gsm8k_cot,5-shot,accuracy,0.5420773313115997,0.01372362964984407
meta-llama/Meta-Llama-3-8B,anli_r2,0-shot,brier_score,0.7340467171859161,
meta-llama/Meta-Llama-3-8B,anli_r3,0-shot,brier_score,0.7121953651588336,
meta-llama/Meta-Llama-3-8B,anli_r1,0-shot,brier_score,0.7541976473288209,
meta-llama/Meta-Llama-3-8B,xnli_eu,0-shot,brier_score,0.8161378857768329,
meta-llama/Meta-Llama-3-8B,xnli_vi,0-shot,brier_score,0.7012948256730949,
meta-llama/Meta-Llama-3-8B,xnli_ru,0-shot,brier_score,0.7041160968968688,
meta-llama/Meta-Llama-3-8B,xnli_zh,0-shot,brier_score,0.8941874011270395,
meta-llama/Meta-Llama-3-8B,xnli_tr,0-shot,brier_score,0.7914917696056338,
meta-llama/Meta-Llama-3-8B,xnli_fr,0-shot,brier_score,0.744870423375577,
meta-llama/Meta-Llama-3-8B,xnli_en,0-shot,brier_score,0.6630590579212835,
meta-llama/Meta-Llama-3-8B,xnli_ur,0-shot,brier_score,1.2437930525145167,
meta-llama/Meta-Llama-3-8B,xnli_ar,0-shot,brier_score,1.2746409939019057,
meta-llama/Meta-Llama-3-8B,xnli_de,0-shot,brier_score,0.7785218513263985,
meta-llama/Meta-Llama-3-8B,xnli_hi,0-shot,brier_score,0.7929776988678373,
meta-llama/Meta-Llama-3-8B,xnli_es,0-shot,brier_score,0.8005855709367697,
meta-llama/Meta-Llama-3-8B,xnli_bg,0-shot,brier_score,0.8622344515881246,
meta-llama/Meta-Llama-3-8B,xnli_sw,0-shot,brier_score,0.84665245616744,
meta-llama/Meta-Llama-3-8B,xnli_el,0-shot,brier_score,0.929237628843266,
meta-llama/Meta-Llama-3-8B,xnli_th,0-shot,brier_score,0.7752077729549335,
meta-llama/Meta-Llama-3-8B,logiqa2,0-shot,brier_score,0.8788764591649608,
meta-llama/Meta-Llama-3-8B,mathqa,5-shot,brier_score,0.7661715680467305,
meta-llama/Meta-Llama-3-8B,lambada_standard,0-shot,perplexity,3.8644129999654666,0.0767209246494085
meta-llama/Meta-Llama-3-8B,lambada_standard,0-shot,accuracy,0.6877547059965069,0.006456197525328597
meta-llama/Meta-Llama-3-8B,lambada_openai,0-shot,perplexity,3.092042982548172,0.05686731066947785
meta-llama/Meta-Llama-3-8B,lambada_openai,0-shot,accuracy,0.7578109838928779,0.0059685624476109225
facebook/opt-350m,minerva_math_precalc,5-shot,accuracy,0.003663003663003663,0.0025877573681934458
facebook/opt-350m,minerva_math_prealgebra,5-shot,accuracy,0.021814006888633754,0.00495243536887352
facebook/opt-350m,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/opt-350m,minerva_math_intermediate_algebra,5-shot,accuracy,0.0033222591362126247,0.0019159795218657448
facebook/opt-350m,minerva_math_geometry,5-shot,accuracy,0.0020876826722338203,0.0020876826722338315
facebook/opt-350m,minerva_math_counting_and_prob,5-shot,accuracy,0.002109704641350211,0.0021097046413502104
facebook/opt-350m,minerva_math_algebra,5-shot,accuracy,0.015164279696714406,0.0035485460431325393
facebook/opt-350m,fld_default,0-shot,accuracy,0.0,
facebook/opt-350m,fld_star,0-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
facebook/opt-350m,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
facebook/opt-350m,arithmetic_4da,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_2ds,5-shot,accuracy,0.0095,0.00216961485391003
facebook/opt-350m,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_1dc,5-shot,accuracy,0.002,0.0009992493430694997
facebook/opt-350m,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-350m,arithmetic_2dm,5-shot,accuracy,0.0185,0.0030138707185866456
facebook/opt-350m,arithmetic_2da,5-shot,accuracy,0.0045,0.0014969954902233223
facebook/opt-350m,gsm8k_cot,5-shot,accuracy,0.016679302501895376,0.0035275958887224556
facebook/opt-350m,gsm8k,5-shot,accuracy,0.003032600454890068,0.0015145735612245468
facebook/opt-350m,anli_r2,0-shot,brier_score,0.8622047830185462,
facebook/opt-350m,anli_r3,0-shot,brier_score,0.8573734867626938,
facebook/opt-350m,anli_r1,0-shot,brier_score,0.875763881825301,
facebook/opt-350m,xnli_eu,0-shot,brier_score,1.2350745588223613,
facebook/opt-350m,xnli_vi,0-shot,brier_score,1.187962184278827,
facebook/opt-350m,xnli_ru,0-shot,brier_score,0.9510442144388731,
facebook/opt-350m,xnli_zh,0-shot,brier_score,1.2604084317893178,
facebook/opt-350m,xnli_tr,0-shot,brier_score,0.8578078728939282,
facebook/opt-350m,xnli_fr,0-shot,brier_score,0.8643029753225795,
facebook/opt-350m,xnli_en,0-shot,brier_score,0.7004467657014397,
facebook/opt-350m,xnli_ur,0-shot,brier_score,1.294615717341209,
facebook/opt-350m,xnli_ar,0-shot,brier_score,0.9077213776188938,
facebook/opt-350m,xnli_de,0-shot,brier_score,0.9042343874682501,
facebook/opt-350m,xnli_hi,0-shot,brier_score,1.2717666974720738,
facebook/opt-350m,xnli_es,0-shot,brier_score,0.9307819515890197,
facebook/opt-350m,xnli_bg,0-shot,brier_score,1.2325643903521082,
facebook/opt-350m,xnli_sw,0-shot,brier_score,1.141700829535198,
facebook/opt-350m,xnli_el,0-shot,brier_score,1.0571822860452535,
facebook/opt-350m,xnli_th,0-shot,brier_score,1.294957782543984,
facebook/opt-350m,logiqa2,0-shot,brier_score,1.1601959747659683,
facebook/opt-350m,mathqa,5-shot,brier_score,1.0103387666530812,
facebook/opt-350m,lambada_standard,0-shot,perplexity,38.15763443103525,1.5528711068483714
facebook/opt-350m,lambada_standard,0-shot,accuracy,0.3578497962352028,0.00667852832588857
facebook/opt-350m,lambada_openai,0-shot,perplexity,16.398847797098593,0.555542932914263
facebook/opt-350m,lambada_openai,0-shot,accuracy,0.4513875412381137,0.00693297588836862
facebook/opt-350m,mmlu_world_religions,0-shot,accuracy,0.18128654970760233,0.02954774168764002
facebook/opt-350m,mmlu_formal_logic,0-shot,accuracy,0.23015873015873015,0.03764950879790605
facebook/opt-350m,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.024748624490537382
facebook/opt-350m,mmlu_moral_scenarios,0-shot,accuracy,0.24022346368715083,0.014288343803925307
facebook/opt-350m,mmlu_high_school_world_history,0-shot,accuracy,0.20675105485232068,0.0263616516683891
facebook/opt-350m,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763006,0.023868003262500107
facebook/opt-350m,mmlu_professional_law,0-shot,accuracy,0.23989569752281617,0.010906282617981634
facebook/opt-350m,mmlu_logical_fallacies,0-shot,accuracy,0.24539877300613497,0.03380939813943354
facebook/opt-350m,mmlu_high_school_us_history,0-shot,accuracy,0.25980392156862747,0.03077855467869328
facebook/opt-350m,mmlu_philosophy,0-shot,accuracy,0.18971061093247588,0.022268196258783218
facebook/opt-350m,mmlu_jurisprudence,0-shot,accuracy,0.2222222222222222,0.040191074725573483
facebook/opt-350m,mmlu_international_law,0-shot,accuracy,0.371900826446281,0.04412015806624504
facebook/opt-350m,mmlu_high_school_european_history,0-shot,accuracy,0.26666666666666666,0.03453131801885415
facebook/opt-350m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3160621761658031,0.03355397369686173
facebook/opt-350m,mmlu_high_school_microeconomics,0-shot,accuracy,0.2773109243697479,0.029079374539480007
facebook/opt-350m,mmlu_high_school_geography,0-shot,accuracy,0.35858585858585856,0.03416903640391521
facebook/opt-350m,mmlu_high_school_psychology,0-shot,accuracy,0.3431192660550459,0.02035477773608604
facebook/opt-350m,mmlu_public_relations,0-shot,accuracy,0.20909090909090908,0.038950910157241364
facebook/opt-350m,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
facebook/opt-350m,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296004
facebook/opt-350m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2794871794871795,0.022752388839776826
facebook/opt-350m,mmlu_security_studies,0-shot,accuracy,0.4,0.031362502409358936
facebook/opt-350m,mmlu_professional_psychology,0-shot,accuracy,0.23529411764705882,0.017160587235046345
facebook/opt-350m,mmlu_human_sexuality,0-shot,accuracy,0.26717557251908397,0.038808483010823944
facebook/opt-350m,mmlu_econometrics,0-shot,accuracy,0.22807017543859648,0.03947152782669415
facebook/opt-350m,mmlu_miscellaneous,0-shot,accuracy,0.20945083014048532,0.0145513105681437
facebook/opt-350m,mmlu_marketing,0-shot,accuracy,0.20512820512820512,0.026453508054040332
facebook/opt-350m,mmlu_management,0-shot,accuracy,0.1941747572815534,0.03916667762822584
facebook/opt-350m,mmlu_nutrition,0-shot,accuracy,0.2581699346405229,0.025058503316958147
facebook/opt-350m,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
facebook/opt-350m,mmlu_human_aging,0-shot,accuracy,0.16143497757847533,0.024693957899128472
facebook/opt-350m,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.030211479609121593
facebook/opt-350m,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
facebook/opt-350m,mmlu_business_ethics,0-shot,accuracy,0.21,0.040936018074033256
facebook/opt-350m,mmlu_clinical_knowledge,0-shot,accuracy,0.26037735849056604,0.02700876609070808
facebook/opt-350m,mmlu_global_facts,0-shot,accuracy,0.19,0.039427724440366234
facebook/opt-350m,mmlu_virology,0-shot,accuracy,0.18674698795180722,0.030338749144500594
facebook/opt-350m,mmlu_professional_accounting,0-shot,accuracy,0.24468085106382978,0.025645553622266733
facebook/opt-350m,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.04158307533083286
facebook/opt-350m,mmlu_high_school_physics,0-shot,accuracy,0.33774834437086093,0.03861557546255168
facebook/opt-350m,mmlu_high_school_biology,0-shot,accuracy,0.2967741935483871,0.02598850079241189
facebook/opt-350m,mmlu_college_biology,0-shot,accuracy,0.20833333333333334,0.033961162058453336
facebook/opt-350m,mmlu_anatomy,0-shot,accuracy,0.26666666666666666,0.038201699145179055
facebook/opt-350m,mmlu_college_chemistry,0-shot,accuracy,0.35,0.0479372485441102
facebook/opt-350m,mmlu_computer_security,0-shot,accuracy,0.18,0.03861229196653697
facebook/opt-350m,mmlu_college_computer_science,0-shot,accuracy,0.29,0.045604802157206845
facebook/opt-350m,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
facebook/opt-350m,mmlu_college_mathematics,0-shot,accuracy,0.27,0.044619604333847394
facebook/opt-350m,mmlu_conceptual_physics,0-shot,accuracy,0.30638297872340425,0.030135906478517563
facebook/opt-350m,mmlu_abstract_algebra,0-shot,accuracy,0.22,0.04163331998932269
facebook/opt-350m,mmlu_high_school_computer_science,0-shot,accuracy,0.19,0.03942772444036625
facebook/opt-350m,mmlu_machine_learning,0-shot,accuracy,0.23214285714285715,0.04007341809755805
facebook/opt-350m,mmlu_high_school_chemistry,0-shot,accuracy,0.2955665024630542,0.032104944337514575
facebook/opt-350m,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-350m,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.022494510767503154
facebook/opt-350m,mmlu_electrical_engineering,0-shot,accuracy,0.31724137931034485,0.038783523721386236
facebook/opt-350m,mmlu_high_school_mathematics,0-shot,accuracy,0.27037037037037037,0.027080372815145658
facebook/opt-350m,arc_challenge,25-shot,accuracy,0.20733788395904437,0.011846905782971335
facebook/opt-350m,arc_challenge,25-shot,acc_norm,0.23976109215017063,0.01247630412745394
facebook/opt-350m,hellaswag,10-shot,accuracy,0.3212507468631747,0.00466002527081701
facebook/opt-350m,hellaswag,10-shot,acc_norm,0.367257518422625,0.00481072310837822
facebook/opt-350m,truthfulqa_mc2,0-shot,accuracy,0.4101397354990324,0.014705665829993402
facebook/opt-350m,truthfulqa_gen,0-shot,bleu_max,16.445503288865865,0.5837064468134793
facebook/opt-350m,truthfulqa_gen,0-shot,bleu_acc,0.390452876376989,0.017078230743431462
facebook/opt-350m,truthfulqa_gen,0-shot,bleu_diff,-2.0622603654652294,0.549702898772285
facebook/opt-350m,truthfulqa_gen,0-shot,rouge1_max,35.862459464442495,0.8635314811502294
facebook/opt-350m,truthfulqa_gen,0-shot,rouge1_acc,0.2839657282741738,0.015785370858396756
facebook/opt-350m,truthfulqa_gen,0-shot,rouge1_diff,-5.37391209618839,0.7800863393842812
facebook/opt-350m,truthfulqa_gen,0-shot,rouge2_max,18.920614186005043,0.8752002965989032
facebook/opt-350m,truthfulqa_gen,0-shot,rouge2_acc,0.18115055079559364,0.013482697187817888
facebook/opt-350m,truthfulqa_gen,0-shot,rouge2_diff,-4.905442542549504,0.7931073647795669
facebook/opt-350m,truthfulqa_gen,0-shot,rougeL_max,33.326011225852476,0.8434189971154012
facebook/opt-350m,truthfulqa_gen,0-shot,rougeL_acc,0.2962056303549572,0.015983595101811385
facebook/opt-350m,truthfulqa_gen,0-shot,rougeL_diff,-4.789359756965765,0.7670328005263544
facebook/opt-350m,truthfulqa_mc1,0-shot,accuracy,0.2350061199510404,0.014843061507731606
facebook/opt-350m,winogrande,5-shot,accuracy,0.526440410418311,0.014032823874407229
aisingapore/sea-lion-7b-instruct,minerva_math_precalc,5-shot,accuracy,0.01282051282051282,0.004818950982487616
aisingapore/sea-lion-7b-instruct,minerva_math_prealgebra,5-shot,accuracy,0.02066590126291619,0.004823174633923075
aisingapore/sea-lion-7b-instruct,minerva_math_num_theory,5-shot,accuracy,0.018518518518518517,0.005806972807912275
aisingapore/sea-lion-7b-instruct,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.00490009308861579
aisingapore/sea-lion-7b-instruct,minerva_math_geometry,5-shot,accuracy,0.031315240083507306,0.007966272499457003
aisingapore/sea-lion-7b-instruct,minerva_math_counting_and_prob,5-shot,accuracy,0.023206751054852322,0.006922738487143304
aisingapore/sea-lion-7b-instruct,minerva_math_algebra,5-shot,accuracy,0.019376579612468407,0.004002647498105356
aisingapore/sea-lion-7b-instruct,fld_default,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,fld_star,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_3da,5-shot,accuracy,0.018,0.0029736208922129218
aisingapore/sea-lion-7b-instruct,arithmetic_3ds,5-shot,accuracy,0.015,0.002718675338799953
aisingapore/sea-lion-7b-instruct,arithmetic_4da,5-shot,accuracy,0.001,0.000706929893933947
aisingapore/sea-lion-7b-instruct,arithmetic_2ds,5-shot,accuracy,0.105,0.006856457212201529
aisingapore/sea-lion-7b-instruct,arithmetic_5ds,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_5da,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_1dc,5-shot,accuracy,0.0635,0.005454241411478469
aisingapore/sea-lion-7b-instruct,arithmetic_4ds,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b-instruct,arithmetic_2dm,5-shot,accuracy,0.0905,0.006416810947142402
aisingapore/sea-lion-7b-instruct,arithmetic_2da,5-shot,accuracy,0.125,0.0073969491973863355
aisingapore/sea-lion-7b-instruct,gsm8k_cot,5-shot,accuracy,0.030326004548900682,0.004723487465514785
aisingapore/sea-lion-7b-instruct,gsm8k,5-shot,accuracy,0.03639120545868082,0.005158113489231192
aisingapore/sea-lion-7b-instruct,anli_r2,0-shot,brier_score,0.822362171539859,
aisingapore/sea-lion-7b-instruct,anli_r3,0-shot,brier_score,0.806465692022114,
aisingapore/sea-lion-7b-instruct,anli_r1,0-shot,brier_score,0.8656927316527164,
aisingapore/sea-lion-7b-instruct,xnli_eu,0-shot,brier_score,1.0647285791530434,
aisingapore/sea-lion-7b-instruct,xnli_vi,0-shot,brier_score,0.7605131981756559,
aisingapore/sea-lion-7b-instruct,xnli_ru,0-shot,brier_score,0.895224913639339,
aisingapore/sea-lion-7b-instruct,xnli_zh,0-shot,brier_score,1.0934355350658187,
aisingapore/sea-lion-7b-instruct,xnli_tr,0-shot,brier_score,0.9075091571950322,
aisingapore/sea-lion-7b-instruct,xnli_fr,0-shot,brier_score,0.864102393355566,
aisingapore/sea-lion-7b-instruct,xnli_en,0-shot,brier_score,0.6608052556326952,
aisingapore/sea-lion-7b-instruct,xnli_ur,0-shot,brier_score,1.308169358746148,
aisingapore/sea-lion-7b-instruct,xnli_ar,0-shot,brier_score,0.8825032212383281,
aisingapore/sea-lion-7b-instruct,xnli_de,0-shot,brier_score,0.8773428304376831,
aisingapore/sea-lion-7b-instruct,xnli_hi,0-shot,brier_score,0.9773682473278797,
aisingapore/sea-lion-7b-instruct,xnli_es,0-shot,brier_score,0.9557450117318852,
aisingapore/sea-lion-7b-instruct,xnli_bg,0-shot,brier_score,0.8633761295842486,
aisingapore/sea-lion-7b-instruct,xnli_sw,0-shot,brier_score,0.94799257162619,
aisingapore/sea-lion-7b-instruct,xnli_el,0-shot,brier_score,1.2823572586721372,
aisingapore/sea-lion-7b-instruct,xnli_th,0-shot,brier_score,0.806377402982851,
aisingapore/sea-lion-7b-instruct,logiqa2,0-shot,brier_score,1.134869575549907,
aisingapore/sea-lion-7b-instruct,mathqa,5-shot,brier_score,1.0274994164407336,
aisingapore/sea-lion-7b-instruct,lambada_standard,0-shot,perplexity,6.148366872576256,0.1664108176034716
aisingapore/sea-lion-7b-instruct,lambada_standard,0-shot,accuracy,0.594216960993596,0.006841188231378082
aisingapore/sea-lion-7b-instruct,lambada_openai,0-shot,perplexity,4.7435583183157215,0.1180869043800317
aisingapore/sea-lion-7b-instruct,lambada_openai,0-shot,accuracy,0.6539879681738793,0.0066273903700315336
aisingapore/sea-lion-7b-instruct,mmlu_world_religions,0-shot,accuracy,0.3742690058479532,0.03711601185389483
aisingapore/sea-lion-7b-instruct,mmlu_formal_logic,0-shot,accuracy,0.1984126984126984,0.03567016675276865
aisingapore/sea-lion-7b-instruct,mmlu_prehistory,0-shot,accuracy,0.24382716049382716,0.02389187954195961
aisingapore/sea-lion-7b-instruct,mmlu_moral_scenarios,0-shot,accuracy,0.23910614525139665,0.014265554192331154
aisingapore/sea-lion-7b-instruct,mmlu_high_school_world_history,0-shot,accuracy,0.29535864978902954,0.02969633871342289
aisingapore/sea-lion-7b-instruct,mmlu_moral_disputes,0-shot,accuracy,0.2832369942196532,0.024257901705323378
aisingapore/sea-lion-7b-instruct,mmlu_professional_law,0-shot,accuracy,0.26140808344198174,0.01122252816977131
aisingapore/sea-lion-7b-instruct,mmlu_logical_fallacies,0-shot,accuracy,0.19631901840490798,0.031207970394709218
aisingapore/sea-lion-7b-instruct,mmlu_high_school_us_history,0-shot,accuracy,0.27941176470588236,0.031493281045079556
aisingapore/sea-lion-7b-instruct,mmlu_philosophy,0-shot,accuracy,0.26688102893890675,0.02512263760881664
aisingapore/sea-lion-7b-instruct,mmlu_jurisprudence,0-shot,accuracy,0.26851851851851855,0.04284467968052192
aisingapore/sea-lion-7b-instruct,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.039418975265163025
aisingapore/sea-lion-7b-instruct,mmlu_high_school_european_history,0-shot,accuracy,0.23636363636363636,0.033175059300091805
aisingapore/sea-lion-7b-instruct,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3471502590673575,0.03435696168361355
aisingapore/sea-lion-7b-instruct,mmlu_high_school_microeconomics,0-shot,accuracy,0.33613445378151263,0.03068473711513537
aisingapore/sea-lion-7b-instruct,mmlu_high_school_geography,0-shot,accuracy,0.35353535353535354,0.03406086723547153
aisingapore/sea-lion-7b-instruct,mmlu_high_school_psychology,0-shot,accuracy,0.27706422018348625,0.019188482590169538
aisingapore/sea-lion-7b-instruct,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.041723430387053825
aisingapore/sea-lion-7b-instruct,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.04648231987117316
aisingapore/sea-lion-7b-instruct,mmlu_sociology,0-shot,accuracy,0.22885572139303484,0.029705284056772422
aisingapore/sea-lion-7b-instruct,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2948717948717949,0.023119362758232283
aisingapore/sea-lion-7b-instruct,mmlu_security_studies,0-shot,accuracy,0.40816326530612246,0.03146465712827424
aisingapore/sea-lion-7b-instruct,mmlu_professional_psychology,0-shot,accuracy,0.2696078431372549,0.017952449196987862
aisingapore/sea-lion-7b-instruct,mmlu_human_sexuality,0-shot,accuracy,0.29770992366412213,0.04010358942462203
aisingapore/sea-lion-7b-instruct,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.03999423879281334
aisingapore/sea-lion-7b-instruct,mmlu_miscellaneous,0-shot,accuracy,0.24776500638569604,0.015438083080568956
aisingapore/sea-lion-7b-instruct,mmlu_marketing,0-shot,accuracy,0.2606837606837607,0.028760348956523414
aisingapore/sea-lion-7b-instruct,mmlu_management,0-shot,accuracy,0.20388349514563106,0.03989139859531771
aisingapore/sea-lion-7b-instruct,mmlu_nutrition,0-shot,accuracy,0.3202614379084967,0.026716118380156858
aisingapore/sea-lion-7b-instruct,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
aisingapore/sea-lion-7b-instruct,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.03114679648297246
aisingapore/sea-lion-7b-instruct,mmlu_professional_medicine,0-shot,accuracy,0.38235294117647056,0.02952009569768775
aisingapore/sea-lion-7b-instruct,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.03186209851641144
aisingapore/sea-lion-7b-instruct,mmlu_business_ethics,0-shot,accuracy,0.36,0.048241815132442176
aisingapore/sea-lion-7b-instruct,mmlu_clinical_knowledge,0-shot,accuracy,0.2830188679245283,0.0277242364927009
aisingapore/sea-lion-7b-instruct,mmlu_global_facts,0-shot,accuracy,0.18,0.03861229196653696
aisingapore/sea-lion-7b-instruct,mmlu_virology,0-shot,accuracy,0.27710843373493976,0.034843315926805875
aisingapore/sea-lion-7b-instruct,mmlu_professional_accounting,0-shot,accuracy,0.22695035460992907,0.024987106365642983
aisingapore/sea-lion-7b-instruct,mmlu_college_physics,0-shot,accuracy,0.24509803921568626,0.04280105837364395
aisingapore/sea-lion-7b-instruct,mmlu_high_school_physics,0-shot,accuracy,0.26490066225165565,0.036030385453603826
aisingapore/sea-lion-7b-instruct,mmlu_high_school_biology,0-shot,accuracy,0.23548387096774193,0.024137632429337707
aisingapore/sea-lion-7b-instruct,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.03685651095897532
aisingapore/sea-lion-7b-instruct,mmlu_anatomy,0-shot,accuracy,0.2222222222222222,0.035914440841969694
aisingapore/sea-lion-7b-instruct,mmlu_college_chemistry,0-shot,accuracy,0.26,0.04408440022768079
aisingapore/sea-lion-7b-instruct,mmlu_computer_security,0-shot,accuracy,0.32,0.04688261722621504
aisingapore/sea-lion-7b-instruct,mmlu_college_computer_science,0-shot,accuracy,0.27,0.044619604333847394
aisingapore/sea-lion-7b-instruct,mmlu_astronomy,0-shot,accuracy,0.2565789473684211,0.0355418036802569
aisingapore/sea-lion-7b-instruct,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816503
aisingapore/sea-lion-7b-instruct,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.029513196625539355
aisingapore/sea-lion-7b-instruct,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.04512608598542127
aisingapore/sea-lion-7b-instruct,mmlu_high_school_computer_science,0-shot,accuracy,0.27,0.0446196043338474
aisingapore/sea-lion-7b-instruct,mmlu_machine_learning,0-shot,accuracy,0.16964285714285715,0.03562367850095391
aisingapore/sea-lion-7b-instruct,mmlu_high_school_chemistry,0-shot,accuracy,0.17733990147783252,0.02687433727680835
aisingapore/sea-lion-7b-instruct,mmlu_high_school_statistics,0-shot,accuracy,0.4074074074074074,0.03350991604696043
aisingapore/sea-lion-7b-instruct,mmlu_elementary_mathematics,0-shot,accuracy,0.23015873015873015,0.021679219663693135
aisingapore/sea-lion-7b-instruct,mmlu_electrical_engineering,0-shot,accuracy,0.21379310344827587,0.03416520447747549
aisingapore/sea-lion-7b-instruct,mmlu_high_school_mathematics,0-shot,accuracy,0.27037037037037037,0.02708037281514565
aisingapore/sea-lion-7b-instruct,arc_challenge,25-shot,accuracy,0.36860068259385664,0.014097810678042198
aisingapore/sea-lion-7b-instruct,arc_challenge,25-shot,acc_norm,0.4052901023890785,0.014346869060229328
aisingapore/sea-lion-7b-instruct,hellaswag,10-shot,accuracy,0.5070703047201752,0.004989282516055396
aisingapore/sea-lion-7b-instruct,hellaswag,10-shot,acc_norm,0.68442541326429,0.004637944965914668
aisingapore/sea-lion-7b-instruct,truthfulqa_mc2,0-shot,accuracy,0.36251426266425163,0.013785301832598731
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,bleu_max,22.470062578446793,0.7295090033700214
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,bleu_acc,0.31701346389228885,0.016289203374403382
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,bleu_diff,-5.418278156499666,0.6905915780852375
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge1_max,46.358554536831306,0.8882483041479635
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge1_acc,0.2998776009791922,0.016040352966713634
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge1_diff,-8.48027229679748,0.8198393069016621
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge2_max,29.876694665726628,0.9732956920229631
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge2_acc,0.22766217870257038,0.014679255032111068
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rouge2_diff,-8.531092668075676,0.9069547026809286
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rougeL_max,43.379773048465026,0.8897140288128731
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rougeL_acc,0.2876376988984088,0.015846315101394805
aisingapore/sea-lion-7b-instruct,truthfulqa_gen,0-shot,rougeL_diff,-8.622396199581418,0.8195677121259433
aisingapore/sea-lion-7b-instruct,truthfulqa_mc1,0-shot,accuracy,0.20930232558139536,0.014241219434785828
aisingapore/sea-lion-7b-instruct,winogrande,5-shot,accuracy,0.6156274664561957,0.01367156760083619
Salesforce/codegen-2B-multi,minerva_math_precalc,5-shot,accuracy,0.016483516483516484,0.005454029764766761
Salesforce/codegen-2B-multi,minerva_math_prealgebra,5-shot,accuracy,0.009184845005740528,0.003234242725762958
Salesforce/codegen-2B-multi,minerva_math_num_theory,5-shot,accuracy,0.014814814814814815,0.005203704987512652
Salesforce/codegen-2B-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.018826135105204873,0.004525330498668474
Salesforce/codegen-2B-multi,minerva_math_geometry,5-shot,accuracy,0.010438413361169102,0.004648627117184649
Salesforce/codegen-2B-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.006329113924050633,0.003646382041065055
Salesforce/codegen-2B-multi,minerva_math_algebra,5-shot,accuracy,0.016849199663016005,0.0037372948497597118
Salesforce/codegen-2B-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521547
Salesforce/codegen-2B-multi,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646984
Salesforce/codegen-2B-multi,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
Salesforce/codegen-2B-multi,arithmetic_2ds,5-shot,accuracy,0.0145,0.002673658397142754
Salesforce/codegen-2B-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_1dc,5-shot,accuracy,0.021,0.0032069677767574555
Salesforce/codegen-2B-multi,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-multi,arithmetic_2dm,5-shot,accuracy,0.017,0.0028913110935905456
Salesforce/codegen-2B-multi,arithmetic_2da,5-shot,accuracy,0.0105,0.002279796863070998
Salesforce/codegen-2B-multi,gsm8k_cot,5-shot,accuracy,0.024260803639120546,0.004238007900001408
Salesforce/codegen-2B-multi,gsm8k,5-shot,accuracy,0.025018953752843062,0.00430204504656429
Salesforce/codegen-2B-multi,anli_r2,0-shot,brier_score,0.9748658390993673,
Salesforce/codegen-2B-multi,anli_r3,0-shot,brier_score,0.9159103324247587,
Salesforce/codegen-2B-multi,anli_r1,0-shot,brier_score,1.0092709408874003,
Salesforce/codegen-2B-multi,xnli_eu,0-shot,brier_score,1.1666516618470189,
Salesforce/codegen-2B-multi,xnli_vi,0-shot,brier_score,1.2843093361039912,
Salesforce/codegen-2B-multi,xnli_ru,0-shot,brier_score,0.8090664913187244,
Salesforce/codegen-2B-multi,xnli_zh,0-shot,brier_score,0.904579300460954,
Salesforce/codegen-2B-multi,xnli_tr,0-shot,brier_score,0.9205007916072985,
Salesforce/codegen-2B-multi,xnli_fr,0-shot,brier_score,0.9220325735050247,
Salesforce/codegen-2B-multi,xnli_en,0-shot,brier_score,0.7458236186385612,
Salesforce/codegen-2B-multi,xnli_ur,0-shot,brier_score,1.3227919367050145,
Salesforce/codegen-2B-multi,xnli_ar,0-shot,brier_score,0.908786661893419,
Salesforce/codegen-2B-multi,xnli_de,0-shot,brier_score,0.9684063717218064,
Salesforce/codegen-2B-multi,xnli_hi,0-shot,brier_score,1.071163113090109,
Salesforce/codegen-2B-multi,xnli_es,0-shot,brier_score,1.0508250865994877,
Salesforce/codegen-2B-multi,xnli_bg,0-shot,brier_score,1.0973174685369405,
Salesforce/codegen-2B-multi,xnli_sw,0-shot,brier_score,1.0296151222717675,
Salesforce/codegen-2B-multi,xnli_el,0-shot,brier_score,0.9165722128481477,
Salesforce/codegen-2B-multi,xnli_th,0-shot,brier_score,1.0251103514516042,
Salesforce/codegen-2B-multi,logiqa2,0-shot,brier_score,1.1763968660378155,
Salesforce/codegen-2B-multi,mathqa,5-shot,brier_score,0.9772584388033301,
Salesforce/codegen-2B-multi,lambada_standard,0-shot,perplexity,81.4029030188419,3.4255201511790587
Salesforce/codegen-2B-multi,lambada_standard,0-shot,accuracy,0.263729866097419,0.006139179363569852
Salesforce/codegen-2B-multi,lambada_openai,0-shot,perplexity,71.22943112687902,3.023615035564398
Salesforce/codegen-2B-multi,lambada_openai,0-shot,accuracy,0.27440326023675526,0.006216620663857009
Salesforce/codegen-2B-multi,mmlu_world_religions,0-shot,accuracy,0.23391812865497075,0.03246721765117826
Salesforce/codegen-2B-multi,mmlu_formal_logic,0-shot,accuracy,0.12698412698412698,0.029780417522688434
Salesforce/codegen-2B-multi,mmlu_prehistory,0-shot,accuracy,0.23148148148148148,0.023468429832451166
Salesforce/codegen-2B-multi,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
Salesforce/codegen-2B-multi,mmlu_high_school_world_history,0-shot,accuracy,0.2109704641350211,0.02655837250266192
Salesforce/codegen-2B-multi,mmlu_moral_disputes,0-shot,accuracy,0.2543352601156069,0.02344582627654555
Salesforce/codegen-2B-multi,mmlu_professional_law,0-shot,accuracy,0.24902216427640156,0.01104489226404077
Salesforce/codegen-2B-multi,mmlu_logical_fallacies,0-shot,accuracy,0.26993865030674846,0.03487825168497892
Salesforce/codegen-2B-multi,mmlu_high_school_us_history,0-shot,accuracy,0.24509803921568626,0.030190282453501954
Salesforce/codegen-2B-multi,mmlu_philosophy,0-shot,accuracy,0.3022508038585209,0.02608270069539966
Salesforce/codegen-2B-multi,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094635
Salesforce/codegen-2B-multi,mmlu_international_law,0-shot,accuracy,0.35537190082644626,0.0436923632657398
Salesforce/codegen-2B-multi,mmlu_high_school_european_history,0-shot,accuracy,0.23030303030303031,0.032876667586034886
Salesforce/codegen-2B-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.34196891191709844,0.03423465100104284
Salesforce/codegen-2B-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
Salesforce/codegen-2B-multi,mmlu_high_school_geography,0-shot,accuracy,0.23232323232323232,0.030088629490217487
Salesforce/codegen-2B-multi,mmlu_high_school_psychology,0-shot,accuracy,0.21467889908256882,0.01760430414925649
Salesforce/codegen-2B-multi,mmlu_public_relations,0-shot,accuracy,0.22727272727272727,0.04013964554072774
Salesforce/codegen-2B-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.045604802157206845
Salesforce/codegen-2B-multi,mmlu_sociology,0-shot,accuracy,0.22885572139303484,0.02970528405677245
Salesforce/codegen-2B-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.26666666666666666,0.022421273612923707
Salesforce/codegen-2B-multi,mmlu_security_studies,0-shot,accuracy,0.24081632653061225,0.027372942201788163
Salesforce/codegen-2B-multi,mmlu_professional_psychology,0-shot,accuracy,0.26143790849673204,0.017776947157528037
Salesforce/codegen-2B-multi,mmlu_human_sexuality,0-shot,accuracy,0.20610687022900764,0.03547771004159463
Salesforce/codegen-2B-multi,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.04096985139843671
Salesforce/codegen-2B-multi,mmlu_miscellaneous,0-shot,accuracy,0.27330779054916987,0.015936681062628556
Salesforce/codegen-2B-multi,mmlu_marketing,0-shot,accuracy,0.31196581196581197,0.030351527323344944
Salesforce/codegen-2B-multi,mmlu_management,0-shot,accuracy,0.1262135922330097,0.03288180278808628
Salesforce/codegen-2B-multi,mmlu_nutrition,0-shot,accuracy,0.238562091503268,0.02440439492808787
Salesforce/codegen-2B-multi,mmlu_medical_genetics,0-shot,accuracy,0.23,0.04229525846816505
Salesforce/codegen-2B-multi,mmlu_human_aging,0-shot,accuracy,0.32286995515695066,0.03138147637575498
Salesforce/codegen-2B-multi,mmlu_professional_medicine,0-shot,accuracy,0.35661764705882354,0.029097209568411955
Salesforce/codegen-2B-multi,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
Salesforce/codegen-2B-multi,mmlu_business_ethics,0-shot,accuracy,0.21,0.040936018074033256
Salesforce/codegen-2B-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.026880647889051982
Salesforce/codegen-2B-multi,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621505
Salesforce/codegen-2B-multi,mmlu_virology,0-shot,accuracy,0.29518072289156627,0.035509201856896294
Salesforce/codegen-2B-multi,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.02668456434046098
Salesforce/codegen-2B-multi,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
Salesforce/codegen-2B-multi,mmlu_high_school_physics,0-shot,accuracy,0.26490066225165565,0.036030385453603826
Salesforce/codegen-2B-multi,mmlu_high_school_biology,0-shot,accuracy,0.3258064516129032,0.026662010578567104
Salesforce/codegen-2B-multi,mmlu_college_biology,0-shot,accuracy,0.2638888888888889,0.03685651095897532
Salesforce/codegen-2B-multi,mmlu_anatomy,0-shot,accuracy,0.3037037037037037,0.039725528847851375
Salesforce/codegen-2B-multi,mmlu_college_chemistry,0-shot,accuracy,0.13,0.03379976689896309
Salesforce/codegen-2B-multi,mmlu_computer_security,0-shot,accuracy,0.23,0.04229525846816507
Salesforce/codegen-2B-multi,mmlu_college_computer_science,0-shot,accuracy,0.37,0.048523658709391
Salesforce/codegen-2B-multi,mmlu_astronomy,0-shot,accuracy,0.19078947368421054,0.031975658210325
Salesforce/codegen-2B-multi,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816506
Salesforce/codegen-2B-multi,mmlu_conceptual_physics,0-shot,accuracy,0.2553191489361702,0.028504856470514203
Salesforce/codegen-2B-multi,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.040936018074033256
Salesforce/codegen-2B-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.04725815626252605
Salesforce/codegen-2B-multi,mmlu_machine_learning,0-shot,accuracy,0.22321428571428573,0.039523019677025116
Salesforce/codegen-2B-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.22167487684729065,0.029225575892489614
Salesforce/codegen-2B-multi,mmlu_high_school_statistics,0-shot,accuracy,0.44907407407407407,0.03392238405321616
Salesforce/codegen-2B-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.2619047619047619,0.022644212615525218
Salesforce/codegen-2B-multi,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.03752833958003336
Salesforce/codegen-2B-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.27037037037037037,0.027080372815145665
Salesforce/codegen-2B-multi,arc_challenge,25-shot,accuracy,0.22184300341296928,0.012141659068147884
Salesforce/codegen-2B-multi,arc_challenge,25-shot,acc_norm,0.257679180887372,0.012780770562768393
Salesforce/codegen-2B-multi,hellaswag,10-shot,accuracy,0.31428002389962156,0.004632797375289776
Salesforce/codegen-2B-multi,hellaswag,10-shot,acc_norm,0.36666002788289187,0.004809077205343483
Salesforce/codegen-2B-multi,truthfulqa_mc2,0-shot,accuracy,0.4459179118660402,0.01522157614487195
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,bleu_max,12.757546154780197,0.4498040933988642
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,bleu_acc,0.379436964504284,0.016987039266142954
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,bleu_diff,-1.8337630237130322,0.4401683091426479
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge1_max,30.34079217924218,0.759556986026027
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge1_acc,0.2766217870257038,0.015659605755326936
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge1_diff,-7.075553576988576,0.7258364783365392
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge2_max,13.032258561545998,0.7083547799321905
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge2_acc,0.1346389228886169,0.011949202293705486
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rouge2_diff,-6.167131616599388,0.6346861400524839
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rougeL_max,27.669505162919336,0.7249567476362944
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rougeL_acc,0.27906976744186046,0.015702107090627915
Salesforce/codegen-2B-multi,truthfulqa_gen,0-shot,rougeL_diff,-6.591009170287162,0.7117399236362356
Salesforce/codegen-2B-multi,truthfulqa_mc1,0-shot,accuracy,0.25458996328029376,0.015250117079156496
Salesforce/codegen-2B-multi,winogrande,5-shot,accuracy,0.5501183898973955,0.013981711904049735
jisukim8873/falcon-7B-case-1,arc:challenge,25-shot,accuracy,0.439419795221843,0.014503747823580127
jisukim8873/falcon-7B-case-1,arc:challenge,25-shot,acc_norm,0.4761092150170648,0.014594701798071654
jisukim8873/falcon-7B-case-1,hellaswag,10-shot,accuracy,0.5987851025692094,0.004891426533390627
jisukim8873/falcon-7B-case-1,hellaswag,10-shot,acc_norm,0.7868950408285202,0.004086642984916037
jisukim8873/falcon-7B-case-1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.04688261722621504
jisukim8873/falcon-7B-case-1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.04688261722621504
jisukim8873/falcon-7B-case-1,hendrycksTest-anatomy,5-shot,accuracy,0.28888888888888886,0.03915450630414251
jisukim8873/falcon-7B-case-1,hendrycksTest-anatomy,5-shot,acc_norm,0.28888888888888886,0.03915450630414251
jisukim8873/falcon-7B-case-1,hendrycksTest-astronomy,5-shot,accuracy,0.23684210526315788,0.034597776068105365
jisukim8873/falcon-7B-case-1,hendrycksTest-astronomy,5-shot,acc_norm,0.23684210526315788,0.034597776068105365
jisukim8873/falcon-7B-case-1,hendrycksTest-business_ethics,5-shot,accuracy,0.18,0.038612291966536934
jisukim8873/falcon-7B-case-1,hendrycksTest-business_ethics,5-shot,acc_norm,0.18,0.038612291966536934
jisukim8873/falcon-7B-case-1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.30566037735849055,0.028353298073322663
jisukim8873/falcon-7B-case-1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.30566037735849055,0.028353298073322663
jisukim8873/falcon-7B-case-1,hendrycksTest-college_biology,5-shot,accuracy,0.25,0.03621034121889507
jisukim8873/falcon-7B-case-1,hendrycksTest-college_biology,5-shot,acc_norm,0.25,0.03621034121889507
jisukim8873/falcon-7B-case-1,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.03775251680686371
jisukim8873/falcon-7B-case-1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.03775251680686371
jisukim8873/falcon-7B-case-1,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-1,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.04408440022768078
jisukim8873/falcon-7B-case-1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.04408440022768078
jisukim8873/falcon-7B-case-1,hendrycksTest-college_medicine,5-shot,accuracy,0.27167630057803466,0.033917503223216586
jisukim8873/falcon-7B-case-1,hendrycksTest-college_medicine,5-shot,acc_norm,0.27167630057803466,0.033917503223216586
jisukim8873/falcon-7B-case-1,hendrycksTest-college_physics,5-shot,accuracy,0.19607843137254902,0.03950581861179961
jisukim8873/falcon-7B-case-1,hendrycksTest-college_physics,5-shot,acc_norm,0.19607843137254902,0.03950581861179961
jisukim8873/falcon-7B-case-1,hendrycksTest-computer_security,5-shot,accuracy,0.39,0.04902071300001974
jisukim8873/falcon-7B-case-1,hendrycksTest-computer_security,5-shot,acc_norm,0.39,0.04902071300001974
jisukim8873/falcon-7B-case-1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.030363582197238174
jisukim8873/falcon-7B-case-1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.030363582197238174
jisukim8873/falcon-7B-case-1,hendrycksTest-econometrics,5-shot,accuracy,0.30701754385964913,0.0433913832257986
jisukim8873/falcon-7B-case-1,hendrycksTest-econometrics,5-shot,acc_norm,0.30701754385964913,0.0433913832257986
jisukim8873/falcon-7B-case-1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.03600105692727771
jisukim8873/falcon-7B-case-1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.03600105692727771
jisukim8873/falcon-7B-case-1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2804232804232804,0.02313528797432563
jisukim8873/falcon-7B-case-1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2804232804232804,0.02313528797432563
jisukim8873/falcon-7B-case-1,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.037184890068181146
jisukim8873/falcon-7B-case-1,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.037184890068181146
jisukim8873/falcon-7B-case-1,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-1,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_biology,5-shot,accuracy,0.3032258064516129,0.026148685930671753
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3032258064516129,0.026148685930671753
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.32019704433497537,0.032826493853041504
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.32019704433497537,0.032826493853041504
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.047609522856952344
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.047609522856952344
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2787878787878788,0.03501438706296781
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2787878787878788,0.03501438706296781
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_geography,5-shot,accuracy,0.26262626262626265,0.03135305009533086
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.26262626262626265,0.03135305009533086
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.03257714077709662
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.03257714077709662
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2717948717948718,0.022556551010132354
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2717948717948718,0.022556551010132354
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.02671924078371218
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25925925925925924,0.02671924078371218
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.028801392193631273
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.028801392193631273
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.03684881521389023
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.03684881521389023
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.30091743119266057,0.019664751366802114
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.30091743119266057,0.019664751366802114
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.18981481481481483,0.026744714834691943
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.18981481481481483,0.026744714834691943
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.27450980392156865,0.031321798030832904
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.27450980392156865,0.031321798030832904
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-1,hendrycksTest-human_aging,5-shot,accuracy,0.4125560538116592,0.03304062175449297
jisukim8873/falcon-7B-case-1,hendrycksTest-human_aging,5-shot,acc_norm,0.4125560538116592,0.03304062175449297
jisukim8873/falcon-7B-case-1,hendrycksTest-human_sexuality,5-shot,accuracy,0.25190839694656486,0.03807387116306086
jisukim8873/falcon-7B-case-1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.25190839694656486,0.03807387116306086
jisukim8873/falcon-7B-case-1,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.04294340845212094
jisukim8873/falcon-7B-case-1,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.04294340845212094
jisukim8873/falcon-7B-case-1,hendrycksTest-jurisprudence,5-shot,accuracy,0.32407407407407407,0.04524596007030048
jisukim8873/falcon-7B-case-1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.32407407407407407,0.04524596007030048
jisukim8873/falcon-7B-case-1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.27607361963190186,0.03512385283705051
jisukim8873/falcon-7B-case-1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.27607361963190186,0.03512385283705051
jisukim8873/falcon-7B-case-1,hendrycksTest-machine_learning,5-shot,accuracy,0.3482142857142857,0.04521829902833585
jisukim8873/falcon-7B-case-1,hendrycksTest-machine_learning,5-shot,acc_norm,0.3482142857142857,0.04521829902833585
jisukim8873/falcon-7B-case-1,hendrycksTest-management,5-shot,accuracy,0.3106796116504854,0.04582124160161552
jisukim8873/falcon-7B-case-1,hendrycksTest-management,5-shot,acc_norm,0.3106796116504854,0.04582124160161552
jisukim8873/falcon-7B-case-1,hendrycksTest-marketing,5-shot,accuracy,0.32905982905982906,0.030782321577688166
jisukim8873/falcon-7B-case-1,hendrycksTest-marketing,5-shot,acc_norm,0.32905982905982906,0.030782321577688166
jisukim8873/falcon-7B-case-1,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.047937248544110196
jisukim8873/falcon-7B-case-1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.047937248544110196
jisukim8873/falcon-7B-case-1,hendrycksTest-miscellaneous,5-shot,accuracy,0.367816091954023,0.017243828891846273
jisukim8873/falcon-7B-case-1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.367816091954023,0.017243828891846273
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_disputes,5-shot,accuracy,0.33236994219653176,0.025361168749688235
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.33236994219653176,0.025361168749688235
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.26256983240223464,0.014716824273017768
jisukim8873/falcon-7B-case-1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.26256983240223464,0.014716824273017768
jisukim8873/falcon-7B-case-1,hendrycksTest-nutrition,5-shot,accuracy,0.3366013071895425,0.027057974624494382
jisukim8873/falcon-7B-case-1,hendrycksTest-nutrition,5-shot,acc_norm,0.3366013071895425,0.027057974624494382
jisukim8873/falcon-7B-case-1,hendrycksTest-philosophy,5-shot,accuracy,0.31511254019292606,0.02638527370346447
jisukim8873/falcon-7B-case-1,hendrycksTest-philosophy,5-shot,acc_norm,0.31511254019292606,0.02638527370346447
jisukim8873/falcon-7B-case-1,hendrycksTest-prehistory,5-shot,accuracy,0.3055555555555556,0.025630824975621344
jisukim8873/falcon-7B-case-1,hendrycksTest-prehistory,5-shot,acc_norm,0.3055555555555556,0.025630824975621344
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_accounting,5-shot,accuracy,0.25886524822695034,0.026129572527180848
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25886524822695034,0.026129572527180848
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_law,5-shot,accuracy,0.2607561929595828,0.01121347155960232
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_law,5-shot,acc_norm,0.2607561929595828,0.01121347155960232
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.023709788253811766
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.023709788253811766
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_psychology,5-shot,accuracy,0.25980392156862747,0.017740899509177795
jisukim8873/falcon-7B-case-1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25980392156862747,0.017740899509177795
jisukim8873/falcon-7B-case-1,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.04309118709946458
jisukim8873/falcon-7B-case-1,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.04309118709946458
jisukim8873/falcon-7B-case-1,hendrycksTest-security_studies,5-shot,accuracy,0.20408163265306123,0.02580128347509051
jisukim8873/falcon-7B-case-1,hendrycksTest-security_studies,5-shot,acc_norm,0.20408163265306123,0.02580128347509051
jisukim8873/falcon-7B-case-1,hendrycksTest-sociology,5-shot,accuracy,0.30845771144278605,0.03265819588512697
jisukim8873/falcon-7B-case-1,hendrycksTest-sociology,5-shot,acc_norm,0.30845771144278605,0.03265819588512697
jisukim8873/falcon-7B-case-1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.43,0.049756985195624284
jisukim8873/falcon-7B-case-1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.43,0.049756985195624284
jisukim8873/falcon-7B-case-1,hendrycksTest-virology,5-shot,accuracy,0.3433734939759036,0.03696584317010601
jisukim8873/falcon-7B-case-1,hendrycksTest-virology,5-shot,acc_norm,0.3433734939759036,0.03696584317010601
jisukim8873/falcon-7B-case-1,hendrycksTest-world_religions,5-shot,accuracy,0.3684210526315789,0.036996580176568775
jisukim8873/falcon-7B-case-1,hendrycksTest-world_religions,5-shot,acc_norm,0.3684210526315789,0.036996580176568775
jisukim8873/falcon-7B-case-1,truthfulqa:mc,0-shot,mc1,0.25703794369645044,0.01529807750948508
jisukim8873/falcon-7B-case-1,truthfulqa:mc,0-shot,mc2,0.3778933788894488,0.014398186673209289
jisukim8873/falcon-7B-case-1,winogrande,5-shot,accuracy,0.7166535122336227,0.012664751735505323
jisukim8873/falcon-7B-case-1,gsm8k,5-shot,accuracy,0.09401061410159212,0.008038819818872478
jisukim8873/falcon-7B-case-1,minerva_math_precalc,5-shot,accuracy,0.007326007326007326,0.0036529080893830325
jisukim8873/falcon-7B-case-1,minerva_math_prealgebra,5-shot,accuracy,0.02640642939150402,0.005436057762573987
jisukim8873/falcon-7B-case-1,minerva_math_num_theory,5-shot,accuracy,0.012962962962962963,0.004872192984581494
jisukim8873/falcon-7B-case-1,minerva_math_intermediate_algebra,5-shot,accuracy,0.013289036544850499,0.0038127511080199464
jisukim8873/falcon-7B-case-1,minerva_math_geometry,5-shot,accuracy,0.020876826722338204,0.006539385795813939
jisukim8873/falcon-7B-case-1,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.006275362513989589
jisukim8873/falcon-7B-case-1,minerva_math_algebra,5-shot,accuracy,0.018534119629317607,0.0039163476763639645
jisukim8873/falcon-7B-case-1,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-1,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-1,arithmetic_3da,5-shot,accuracy,0.202,0.008979884139540944
jisukim8873/falcon-7B-case-1,arithmetic_3ds,5-shot,accuracy,0.3245,0.010471614123485492
jisukim8873/falcon-7B-case-1,arithmetic_4da,5-shot,accuracy,0.0055,0.0016541593398342208
jisukim8873/falcon-7B-case-1,arithmetic_2ds,5-shot,accuracy,0.4725,0.011166208716863541
jisukim8873/falcon-7B-case-1,arithmetic_5ds,5-shot,accuracy,0.035,0.004110468096699785
jisukim8873/falcon-7B-case-1,arithmetic_5da,5-shot,accuracy,0.0005,0.0005000000000000087
jisukim8873/falcon-7B-case-1,arithmetic_1dc,5-shot,accuracy,0.091,0.006432743590028116
jisukim8873/falcon-7B-case-1,arithmetic_4ds,5-shot,accuracy,0.084,0.006204131335071231
jisukim8873/falcon-7B-case-1,arithmetic_2dm,5-shot,accuracy,0.2645,0.00986501567495644
jisukim8873/falcon-7B-case-1,arithmetic_2da,5-shot,accuracy,0.7235,0.010003694915178819
jisukim8873/falcon-7B-case-1,gsm8k_cot,5-shot,accuracy,0.09628506444275967,0.00812526412821589
jisukim8873/falcon-7B-case-1,anli_r2,0-shot,brier_score,0.9413381511880419,
jisukim8873/falcon-7B-case-1,anli_r3,0-shot,brier_score,0.87237900881463,
jisukim8873/falcon-7B-case-1,anli_r1,0-shot,brier_score,0.9857934321426033,
jisukim8873/falcon-7B-case-1,xnli_eu,0-shot,brier_score,1.026246542896589,
jisukim8873/falcon-7B-case-1,xnli_vi,0-shot,brier_score,1.0279853059907973,
jisukim8873/falcon-7B-case-1,xnli_ru,0-shot,brier_score,0.8097107614311067,
jisukim8873/falcon-7B-case-1,xnli_zh,0-shot,brier_score,1.0156490749806204,
jisukim8873/falcon-7B-case-1,xnli_tr,0-shot,brier_score,0.9756836472864571,
jisukim8873/falcon-7B-case-1,xnli_fr,0-shot,brier_score,0.7603981109729621,
jisukim8873/falcon-7B-case-1,xnli_en,0-shot,brier_score,0.662787162600886,
jisukim8873/falcon-7B-case-1,xnli_ur,0-shot,brier_score,1.31954291668707,
jisukim8873/falcon-7B-case-1,xnli_ar,0-shot,brier_score,1.2990240082694595,
jisukim8873/falcon-7B-case-1,xnli_de,0-shot,brier_score,0.849940839182899,
jisukim8873/falcon-7B-case-1,xnli_hi,0-shot,brier_score,1.1637487081246372,
jisukim8873/falcon-7B-case-1,xnli_es,0-shot,brier_score,0.8194931547365195,
jisukim8873/falcon-7B-case-1,xnli_bg,0-shot,brier_score,0.9860919344218411,
jisukim8873/falcon-7B-case-1,xnli_sw,0-shot,brier_score,1.082079686536352,
jisukim8873/falcon-7B-case-1,xnli_el,0-shot,brier_score,0.8625720197689004,
jisukim8873/falcon-7B-case-1,xnli_th,0-shot,brier_score,0.9548472798777402,
jisukim8873/falcon-7B-case-1,logiqa2,0-shot,brier_score,1.05514711694456,
jisukim8873/falcon-7B-case-1,mathqa,5-shot,brier_score,0.9309727513874027,
jisukim8873/falcon-7B-case-1,lambada_standard,0-shot,perplexity,4.1704788868142115,0.09171832314219872
jisukim8873/falcon-7B-case-1,lambada_standard,0-shot,accuracy,0.6625266834853484,0.006587694938528704
jisukim8873/falcon-7B-case-1,lambada_openai,0-shot,perplexity,3.3182500746294745,0.06880896651957533
jisukim8873/falcon-7B-case-1,lambada_openai,0-shot,accuracy,0.7343295167863381,0.006153599372822545
AbacusResearch/haLLawa4-7b,minerva_math_precalc,5-shot,accuracy,0.08791208791208792,0.012129541821143602
AbacusResearch/haLLawa4-7b,minerva_math_prealgebra,5-shot,accuracy,0.40642939150401836,0.016652104255062996
AbacusResearch/haLLawa4-7b,minerva_math_num_theory,5-shot,accuracy,0.15,0.01538015491211301
AbacusResearch/haLLawa4-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.09302325581395349,0.009671427372356392
AbacusResearch/haLLawa4-7b,minerva_math_geometry,5-shot,accuracy,0.17745302713987474,0.01747463495280595
AbacusResearch/haLLawa4-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.21729957805907174,0.01896254634032935
AbacusResearch/haLLawa4-7b,minerva_math_algebra,5-shot,accuracy,0.32940185341196293,0.013647460597468906
AbacusResearch/haLLawa4-7b,fld_default,0-shot,accuracy,0.0,
AbacusResearch/haLLawa4-7b,fld_star,0-shot,accuracy,0.0,
AbacusResearch/haLLawa4-7b,arithmetic_3da,5-shot,accuracy,0.9845,0.002762913651550293
AbacusResearch/haLLawa4-7b,arithmetic_3ds,5-shot,accuracy,0.985,0.002718675338799954
AbacusResearch/haLLawa4-7b,arithmetic_4da,5-shot,accuracy,0.948,0.004965916850399518
AbacusResearch/haLLawa4-7b,arithmetic_2ds,5-shot,accuracy,0.9955,0.0014969954902233232
AbacusResearch/haLLawa4-7b,arithmetic_5ds,5-shot,accuracy,0.901,0.006679955905951291
AbacusResearch/haLLawa4-7b,arithmetic_5da,5-shot,accuracy,0.916,0.006204131335071236
AbacusResearch/haLLawa4-7b,arithmetic_1dc,5-shot,accuracy,0.757,0.009592784306726521
AbacusResearch/haLLawa4-7b,arithmetic_4ds,5-shot,accuracy,0.951,0.004828162753862956
AbacusResearch/haLLawa4-7b,arithmetic_2dm,5-shot,accuracy,0.6645,0.010560569957105102
AbacusResearch/haLLawa4-7b,arithmetic_2da,5-shot,accuracy,1.0,
AbacusResearch/haLLawa4-7b,gsm8k_cot,5-shot,accuracy,0.7581501137225171,0.011794861371318702
AbacusResearch/haLLawa4-7b,gsm8k,5-shot,accuracy,0.7429871114480667,0.012036781757428677
AbacusResearch/haLLawa4-7b,anli_r2,0-shot,brier_score,0.7666504741725055,
AbacusResearch/haLLawa4-7b,anli_r3,0-shot,brier_score,0.8663025386598154,
AbacusResearch/haLLawa4-7b,anli_r1,0-shot,brier_score,0.6573805818915781,
AbacusResearch/haLLawa4-7b,xnli_eu,0-shot,brier_score,1.1139982065153322,
AbacusResearch/haLLawa4-7b,xnli_vi,0-shot,brier_score,0.9999180808359713,
AbacusResearch/haLLawa4-7b,xnli_ru,0-shot,brier_score,0.9538905561506925,
AbacusResearch/haLLawa4-7b,xnli_zh,0-shot,brier_score,1.0601931277652965,
AbacusResearch/haLLawa4-7b,xnli_tr,0-shot,brier_score,1.0467850390267222,
AbacusResearch/haLLawa4-7b,xnli_fr,0-shot,brier_score,0.9334503115142729,
AbacusResearch/haLLawa4-7b,xnli_en,0-shot,brier_score,0.7398112895767275,
AbacusResearch/haLLawa4-7b,xnli_ur,0-shot,brier_score,1.2069939556444398,
AbacusResearch/haLLawa4-7b,xnli_ar,0-shot,brier_score,1.1995508224164844,
AbacusResearch/haLLawa4-7b,xnli_de,0-shot,brier_score,0.9119484180329919,
AbacusResearch/haLLawa4-7b,xnli_hi,0-shot,brier_score,0.9461357371948407,
AbacusResearch/haLLawa4-7b,xnli_es,0-shot,brier_score,0.9585943286675973,
AbacusResearch/haLLawa4-7b,xnli_bg,0-shot,brier_score,0.9511880273157712,
AbacusResearch/haLLawa4-7b,xnli_sw,0-shot,brier_score,1.0399624826911418,
AbacusResearch/haLLawa4-7b,xnli_el,0-shot,brier_score,0.9893964628647417,
AbacusResearch/haLLawa4-7b,xnli_th,0-shot,brier_score,1.1061854373450353,
AbacusResearch/haLLawa4-7b,logiqa2,0-shot,brier_score,0.9626222319450617,
AbacusResearch/haLLawa4-7b,mathqa,5-shot,brier_score,0.9820782094042059,
AbacusResearch/haLLawa4-7b,lambada_standard,0-shot,perplexity,4.423249636812455,0.11448351786225823
AbacusResearch/haLLawa4-7b,lambada_standard,0-shot,accuracy,0.6367164758393169,0.00670051166747681
AbacusResearch/haLLawa4-7b,lambada_openai,0-shot,perplexity,3.629649594000675,0.08434195320266337
AbacusResearch/haLLawa4-7b,lambada_openai,0-shot,accuracy,0.6959052978847273,0.006409019178962834
AbacusResearch/haLLawa4-7b,mmlu_world_religions,0-shot,accuracy,0.8304093567251462,0.028782108105401712
AbacusResearch/haLLawa4-7b,mmlu_formal_logic,0-shot,accuracy,0.48412698412698413,0.04469881854072606
AbacusResearch/haLLawa4-7b,mmlu_prehistory,0-shot,accuracy,0.7253086419753086,0.024836057868294674
AbacusResearch/haLLawa4-7b,mmlu_moral_scenarios,0-shot,accuracy,0.42905027932960893,0.01655328786311604
AbacusResearch/haLLawa4-7b,mmlu_high_school_world_history,0-shot,accuracy,0.8059071729957806,0.025744902532290927
AbacusResearch/haLLawa4-7b,mmlu_moral_disputes,0-shot,accuracy,0.7312138728323699,0.023868003262500118
AbacusResearch/haLLawa4-7b,mmlu_professional_law,0-shot,accuracy,0.470013037809648,0.012747248967079055
AbacusResearch/haLLawa4-7b,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.033220157957767414
AbacusResearch/haLLawa4-7b,mmlu_high_school_us_history,0-shot,accuracy,0.8627450980392157,0.024152225962801584
AbacusResearch/haLLawa4-7b,mmlu_philosophy,0-shot,accuracy,0.7009646302250804,0.026003301117885142
AbacusResearch/haLLawa4-7b,mmlu_jurisprudence,0-shot,accuracy,0.7870370370370371,0.039578354719809784
AbacusResearch/haLLawa4-7b,mmlu_international_law,0-shot,accuracy,0.768595041322314,0.038498560987940904
AbacusResearch/haLLawa4-7b,mmlu_high_school_european_history,0-shot,accuracy,0.7636363636363637,0.03317505930009181
AbacusResearch/haLLawa4-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9015544041450777,0.021500249576033467
AbacusResearch/haLLawa4-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.680672268907563,0.0302839955258844
AbacusResearch/haLLawa4-7b,mmlu_high_school_geography,0-shot,accuracy,0.7929292929292929,0.028869778460267073
AbacusResearch/haLLawa4-7b,mmlu_high_school_psychology,0-shot,accuracy,0.8311926605504587,0.016060056268530364
AbacusResearch/haLLawa4-7b,mmlu_public_relations,0-shot,accuracy,0.6818181818181818,0.04461272175910508
AbacusResearch/haLLawa4-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.86,0.03487350880197768
AbacusResearch/haLLawa4-7b,mmlu_sociology,0-shot,accuracy,0.8407960199004975,0.02587064676616913
AbacusResearch/haLLawa4-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6692307692307692,0.02385479568097112
AbacusResearch/haLLawa4-7b,mmlu_security_studies,0-shot,accuracy,0.7387755102040816,0.028123429335142783
AbacusResearch/haLLawa4-7b,mmlu_professional_psychology,0-shot,accuracy,0.6748366013071896,0.01895088677080631
AbacusResearch/haLLawa4-7b,mmlu_human_sexuality,0-shot,accuracy,0.8091603053435115,0.03446513350752599
AbacusResearch/haLLawa4-7b,mmlu_econometrics,0-shot,accuracy,0.4824561403508772,0.04700708033551038
AbacusResearch/haLLawa4-7b,mmlu_miscellaneous,0-shot,accuracy,0.8314176245210728,0.013387895731543602
AbacusResearch/haLLawa4-7b,mmlu_marketing,0-shot,accuracy,0.8760683760683761,0.021586494001281376
AbacusResearch/haLLawa4-7b,mmlu_management,0-shot,accuracy,0.7766990291262136,0.04123553189891431
AbacusResearch/haLLawa4-7b,mmlu_nutrition,0-shot,accuracy,0.7320261437908496,0.025360603796242557
AbacusResearch/haLLawa4-7b,mmlu_medical_genetics,0-shot,accuracy,0.7,0.046056618647183814
AbacusResearch/haLLawa4-7b,mmlu_human_aging,0-shot,accuracy,0.6816143497757847,0.03126580522513713
AbacusResearch/haLLawa4-7b,mmlu_professional_medicine,0-shot,accuracy,0.6727941176470589,0.02850145286039657
AbacusResearch/haLLawa4-7b,mmlu_college_medicine,0-shot,accuracy,0.6589595375722543,0.036146654241808254
AbacusResearch/haLLawa4-7b,mmlu_business_ethics,0-shot,accuracy,0.63,0.048523658709390974
AbacusResearch/haLLawa4-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.6830188679245283,0.028637235639800893
AbacusResearch/haLLawa4-7b,mmlu_global_facts,0-shot,accuracy,0.36,0.04824181513244218
AbacusResearch/haLLawa4-7b,mmlu_virology,0-shot,accuracy,0.5180722891566265,0.038899512528272166
AbacusResearch/haLLawa4-7b,mmlu_professional_accounting,0-shot,accuracy,0.4929078014184397,0.02982449855912901
AbacusResearch/haLLawa4-7b,mmlu_college_physics,0-shot,accuracy,0.45098039215686275,0.04951218252396262
AbacusResearch/haLLawa4-7b,mmlu_high_school_physics,0-shot,accuracy,0.37748344370860926,0.0395802723112157
AbacusResearch/haLLawa4-7b,mmlu_high_school_biology,0-shot,accuracy,0.7806451612903226,0.0235407993587233
AbacusResearch/haLLawa4-7b,mmlu_college_biology,0-shot,accuracy,0.75,0.03621034121889507
AbacusResearch/haLLawa4-7b,mmlu_anatomy,0-shot,accuracy,0.6148148148148148,0.042039210401562783
AbacusResearch/haLLawa4-7b,mmlu_college_chemistry,0-shot,accuracy,0.45,0.049999999999999996
AbacusResearch/haLLawa4-7b,mmlu_computer_security,0-shot,accuracy,0.75,0.04351941398892446
AbacusResearch/haLLawa4-7b,mmlu_college_computer_science,0-shot,accuracy,0.59,0.04943110704237101
AbacusResearch/haLLawa4-7b,mmlu_astronomy,0-shot,accuracy,0.7105263157894737,0.036906779861372814
AbacusResearch/haLLawa4-7b,mmlu_college_mathematics,0-shot,accuracy,0.29,0.045604802157206845
AbacusResearch/haLLawa4-7b,mmlu_conceptual_physics,0-shot,accuracy,0.5659574468085107,0.032400380867927465
AbacusResearch/haLLawa4-7b,mmlu_abstract_algebra,0-shot,accuracy,0.35,0.0479372485441102
AbacusResearch/haLLawa4-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.7,0.046056618647183814
AbacusResearch/haLLawa4-7b,mmlu_machine_learning,0-shot,accuracy,0.4017857142857143,0.04653333146973646
AbacusResearch/haLLawa4-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.5123152709359606,0.035169204442208966
AbacusResearch/haLLawa4-7b,mmlu_high_school_statistics,0-shot,accuracy,0.5138888888888888,0.034086558679777494
AbacusResearch/haLLawa4-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.3968253968253968,0.025197101074246487
AbacusResearch/haLLawa4-7b,mmlu_electrical_engineering,0-shot,accuracy,0.5793103448275863,0.0411391498118926
AbacusResearch/haLLawa4-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.31851851851851853,0.02840653309060846
AbacusResearch/haLLawa4-7b,arc_challenge,25-shot,accuracy,0.7030716723549488,0.013352025976725223
AbacusResearch/haLLawa4-7b,arc_challenge,25-shot,acc_norm,0.7235494880546075,0.013069662474252423
AbacusResearch/haLLawa4-7b,hellaswag,10-shot,accuracy,0.7102170882294364,0.0045273436511307445
AbacusResearch/haLLawa4-7b,hellaswag,10-shot,acc_norm,0.8841864170483967,0.003193472530282367
AbacusResearch/haLLawa4-7b,truthfulqa_mc2,0-shot,accuracy,0.7478686028704679,0.014178044720511106
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,bleu_max,15.495195703574323,0.5680370072696724
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,bleu_acc,0.5263157894736842,0.01747924116197544
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,bleu_diff,2.2914445163776342,0.44265660441282384
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge1_max,41.476176909090015,0.7163761236938211
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge1_acc,0.5703794369645043,0.01732923458040911
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge1_diff,4.5715005574824925,0.6975679934842908
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge2_max,26.293465437291108,0.7848741256847009
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge2_acc,0.4638922888616891,0.01745780042226862
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rouge2_diff,3.935472582571117,0.7546985023903435
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rougeL_max,37.83478014787854,0.7279950256366058
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rougeL_acc,0.5385556915544676,0.017451384104637452
AbacusResearch/haLLawa4-7b,truthfulqa_gen,0-shot,rougeL_diff,4.12214109311066,0.7002705028145758
AbacusResearch/haLLawa4-7b,truthfulqa_mc1,0-shot,accuracy,0.5936352509179926,0.017193835812093907
AbacusResearch/haLLawa4-7b,winogrande,5-shot,accuracy,0.8263614838200474,0.010646116480331005
allenai/OLMo-7B-hf,minerva_math_precalc,5-shot,accuracy,0.016483516483516484,0.005454029764766746
allenai/OLMo-7B-hf,minerva_math_prealgebra,5-shot,accuracy,0.03559127439724455,0.006281201252709582
allenai/OLMo-7B-hf,minerva_math_num_theory,5-shot,accuracy,0.014814814814814815,0.0052037049875126515
allenai/OLMo-7B-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.018826135105204873,0.004525330498668475
allenai/OLMo-7B-hf,minerva_math_geometry,5-shot,accuracy,0.018789144050104383,0.006210416427997403
allenai/OLMo-7B-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.023206751054852322,0.006922738487143303
allenai/OLMo-7B-hf,minerva_math_algebra,5-shot,accuracy,0.012636899747262006,0.003243518444352171
allenai/OLMo-7B-hf,fld_default,0-shot,accuracy,0.0,
allenai/OLMo-7B-hf,fld_star,0-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521528
allenai/OLMo-7B-hf,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
allenai/OLMo-7B-hf,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
allenai/OLMo-7B-hf,arithmetic_2ds,5-shot,accuracy,0.0155,0.002762913651550328
allenai/OLMo-7B-hf,arithmetic_5ds,5-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_5da,5-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_1dc,5-shot,accuracy,0.0065,0.0017973564602277768
allenai/OLMo-7B-hf,arithmetic_4ds,5-shot,accuracy,0.0,
allenai/OLMo-7B-hf,arithmetic_2dm,5-shot,accuracy,0.029,0.0037532044004605267
allenai/OLMo-7B-hf,arithmetic_2da,5-shot,accuracy,0.016,0.002806410156941529
allenai/OLMo-7B-hf,gsm8k_cot,5-shot,accuracy,0.07733131159969674,0.007357713523222347
allenai/OLMo-7B-hf,gsm8k,5-shot,accuracy,0.04700530705079606,0.005829898355937165
allenai/OLMo-7B-hf,anli_r2,0-shot,brier_score,0.727415360551483,
allenai/OLMo-7B-hf,anli_r3,0-shot,brier_score,0.7329111882888333,
allenai/OLMo-7B-hf,anli_r1,0-shot,brier_score,0.7459395081085325,
allenai/OLMo-7B-hf,xnli_eu,0-shot,brier_score,0.8502840947105932,
allenai/OLMo-7B-hf,xnli_vi,0-shot,brier_score,0.8697900492853695,
allenai/OLMo-7B-hf,xnli_ru,0-shot,brier_score,0.7888266397325421,
allenai/OLMo-7B-hf,xnli_zh,0-shot,brier_score,0.9170790959685605,
allenai/OLMo-7B-hf,xnli_tr,0-shot,brier_score,0.9126604079971902,
allenai/OLMo-7B-hf,xnli_fr,0-shot,brier_score,0.6871829801737326,
allenai/OLMo-7B-hf,xnli_en,0-shot,brier_score,0.6203086496227678,
allenai/OLMo-7B-hf,xnli_ur,0-shot,brier_score,1.1489733890229683,
allenai/OLMo-7B-hf,xnli_ar,0-shot,brier_score,1.2297706224773113,
allenai/OLMo-7B-hf,xnli_de,0-shot,brier_score,0.8009175976298082,
allenai/OLMo-7B-hf,xnli_hi,0-shot,brier_score,0.8077524633855024,
allenai/OLMo-7B-hf,xnli_es,0-shot,brier_score,0.8277268635765521,
allenai/OLMo-7B-hf,xnli_bg,0-shot,brier_score,0.7504606344491516,
allenai/OLMo-7B-hf,xnli_sw,0-shot,brier_score,1.02550711834348,
allenai/OLMo-7B-hf,xnli_el,0-shot,brier_score,0.9957388418764128,
allenai/OLMo-7B-hf,xnli_th,0-shot,brier_score,1.0487110744801207,
allenai/OLMo-7B-hf,logiqa2,0-shot,brier_score,0.9608102017283243,
allenai/OLMo-7B-hf,mathqa,5-shot,brier_score,0.9668035927315314,
allenai/OLMo-7B-hf,lambada_standard,0-shot,perplexity,5.030584263765433,0.11130666457513855
allenai/OLMo-7B-hf,lambada_standard,0-shot,accuracy,0.6405977100718029,0.0066849041360329976
allenai/OLMo-7B-hf,lambada_openai,0-shot,perplexity,4.123391391255741,0.08680886025197333
allenai/OLMo-7B-hf,lambada_openai,0-shot,accuracy,0.6941587424801087,0.006419327115892601
allenai/OLMo-7B-hf,mmlu_world_religions,0-shot,accuracy,0.2631578947368421,0.033773102522091945
allenai/OLMo-7B-hf,mmlu_formal_logic,0-shot,accuracy,0.1984126984126984,0.03567016675276864
allenai/OLMo-7B-hf,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.025773111169630436
allenai/OLMo-7B-hf,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574903
allenai/OLMo-7B-hf,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
allenai/OLMo-7B-hf,mmlu_moral_disputes,0-shot,accuracy,0.2658959537572254,0.023786203255508297
allenai/OLMo-7B-hf,mmlu_professional_law,0-shot,accuracy,0.2607561929595828,0.011213471559602313
allenai/OLMo-7B-hf,mmlu_logical_fallacies,0-shot,accuracy,0.2392638036809816,0.033519538795212696
allenai/OLMo-7B-hf,mmlu_high_school_us_history,0-shot,accuracy,0.22058823529411764,0.02910225438967408
allenai/OLMo-7B-hf,mmlu_philosophy,0-shot,accuracy,0.3247588424437299,0.026596782287697043
allenai/OLMo-7B-hf,mmlu_jurisprudence,0-shot,accuracy,0.21296296296296297,0.039578354719809784
allenai/OLMo-7B-hf,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.04026187527591206
allenai/OLMo-7B-hf,mmlu_high_school_european_history,0-shot,accuracy,0.26666666666666666,0.03453131801885415
allenai/OLMo-7B-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3316062176165803,0.03397636541089116
allenai/OLMo-7B-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.3235294117647059,0.03038835355188684
allenai/OLMo-7B-hf,mmlu_high_school_geography,0-shot,accuracy,0.23232323232323232,0.030088629490217483
allenai/OLMo-7B-hf,mmlu_high_school_psychology,0-shot,accuracy,0.24770642201834864,0.018508143602547815
allenai/OLMo-7B-hf,mmlu_public_relations,0-shot,accuracy,0.32727272727272727,0.04494290866252089
allenai/OLMo-7B-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.04292346959909283
allenai/OLMo-7B-hf,mmlu_sociology,0-shot,accuracy,0.3582089552238806,0.03390393042268814
allenai/OLMo-7B-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3435897435897436,0.024078696580635467
allenai/OLMo-7B-hf,mmlu_security_studies,0-shot,accuracy,0.4204081632653061,0.03160106993449604
allenai/OLMo-7B-hf,mmlu_professional_psychology,0-shot,accuracy,0.22712418300653595,0.01694985327921238
allenai/OLMo-7B-hf,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.03915345408847836
allenai/OLMo-7B-hf,mmlu_econometrics,0-shot,accuracy,0.2982456140350877,0.04303684033537316
allenai/OLMo-7B-hf,mmlu_miscellaneous,0-shot,accuracy,0.2950191570881226,0.016308363772932724
allenai/OLMo-7B-hf,mmlu_marketing,0-shot,accuracy,0.2863247863247863,0.02961432369045666
allenai/OLMo-7B-hf,mmlu_management,0-shot,accuracy,0.27184466019417475,0.044052680241409216
allenai/OLMo-7B-hf,mmlu_nutrition,0-shot,accuracy,0.33986928104575165,0.027121956071388852
allenai/OLMo-7B-hf,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
allenai/OLMo-7B-hf,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.03149384670994131
allenai/OLMo-7B-hf,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091216
allenai/OLMo-7B-hf,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
allenai/OLMo-7B-hf,mmlu_business_ethics,0-shot,accuracy,0.21,0.040936018074033256
allenai/OLMo-7B-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.27169811320754716,0.027377706624670713
allenai/OLMo-7B-hf,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621504
allenai/OLMo-7B-hf,mmlu_virology,0-shot,accuracy,0.4036144578313253,0.03819486140758398
allenai/OLMo-7B-hf,mmlu_professional_accounting,0-shot,accuracy,0.22340425531914893,0.024847921358063962
allenai/OLMo-7B-hf,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237654
allenai/OLMo-7B-hf,mmlu_high_school_physics,0-shot,accuracy,0.2980132450331126,0.037345356767871984
allenai/OLMo-7B-hf,mmlu_high_school_biology,0-shot,accuracy,0.34516129032258064,0.027045746573534327
allenai/OLMo-7B-hf,mmlu_college_biology,0-shot,accuracy,0.24305555555555555,0.03586879280080341
allenai/OLMo-7B-hf,mmlu_anatomy,0-shot,accuracy,0.26666666666666666,0.03820169914517905
allenai/OLMo-7B-hf,mmlu_college_chemistry,0-shot,accuracy,0.29,0.045604802157206845
allenai/OLMo-7B-hf,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
allenai/OLMo-7B-hf,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526045
allenai/OLMo-7B-hf,mmlu_astronomy,0-shot,accuracy,0.26973684210526316,0.03611780560284898
allenai/OLMo-7B-hf,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276808
allenai/OLMo-7B-hf,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.029513196625539345
allenai/OLMo-7B-hf,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
allenai/OLMo-7B-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.2,0.04020151261036845
allenai/OLMo-7B-hf,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.04287858751340456
allenai/OLMo-7B-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.32019704433497537,0.032826493853041504
allenai/OLMo-7B-hf,mmlu_high_school_statistics,0-shot,accuracy,0.4537037037037037,0.03395322726375797
allenai/OLMo-7B-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.25396825396825395,0.02241804289111394
allenai/OLMo-7B-hf,mmlu_electrical_engineering,0-shot,accuracy,0.2896551724137931,0.03780019230438014
allenai/OLMo-7B-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.27037037037037037,0.027080372815145644
allenai/OLMo-7B-hf,arc_challenge,25-shot,accuracy,0.43430034129692835,0.014484703048857364
allenai/OLMo-7B-hf,arc_challenge,25-shot,acc_norm,0.4616040955631399,0.01456824555029636
allenai/OLMo-7B-hf,hellaswag,10-shot,accuracy,0.5726946823341964,0.004936762568217072
allenai/OLMo-7B-hf,hellaswag,10-shot,acc_norm,0.7694682334196375,0.00420312448903718
allenai/OLMo-7B-hf,truthfulqa_mc2,0-shot,accuracy,0.3585058314358672,0.013802749984870202
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,bleu_max,26.539400833287374,0.7779688490400837
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,bleu_acc,0.2937576499388005,0.015945068581236614
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,bleu_diff,-9.216867205020309,0.7821192582719276
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge1_max,51.946561103999834,0.8300725318977166
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge1_acc,0.31334149326805383,0.016238065069059605
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge1_diff,-11.3371350794366,0.8136895635471025
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge2_max,36.413967516398216,0.9679506184935703
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge2_acc,0.2582619339045288,0.015321821688476187
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rouge2_diff,-13.565291787723291,1.009721378967521
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rougeL_max,49.13417294832966,0.8355931924931606
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rougeL_acc,0.2937576499388005,0.015945068581236614
allenai/OLMo-7B-hf,truthfulqa_gen,0-shot,rougeL_diff,-11.47267969384019,0.8120028631669012
allenai/OLMo-7B-hf,truthfulqa_mc1,0-shot,accuracy,0.24479804161566707,0.015051869486715
allenai/OLMo-7B-hf,winogrande,5-shot,accuracy,0.6953433307024467,0.012935646499325312
Salesforce/codegen-6B-mono,minerva_math_precalc,5-shot,accuracy,0.016483516483516484,0.005454029764766752
Salesforce/codegen-6B-mono,minerva_math_prealgebra,5-shot,accuracy,0.011481056257175661,0.0036118008378658315
Salesforce/codegen-6B-mono,minerva_math_num_theory,5-shot,accuracy,0.014814814814814815,0.005203704987512651
Salesforce/codegen-6B-mono,minerva_math_intermediate_algebra,5-shot,accuracy,0.006644518272425249,0.0027050844483854065
Salesforce/codegen-6B-mono,minerva_math_geometry,5-shot,accuracy,0.006263048016701462,0.0036083997328878823
Salesforce/codegen-6B-mono,minerva_math_counting_and_prob,5-shot,accuracy,0.012658227848101266,0.005140313889578834
Salesforce/codegen-6B-mono,minerva_math_algebra,5-shot,accuracy,0.010109519797809604,0.0029048017186508964
Salesforce/codegen-6B-mono,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_3da,5-shot,accuracy,0.0035,0.0013208888574315787
Salesforce/codegen-6B-mono,arithmetic_3ds,5-shot,accuracy,0.01,0.0022254159696827483
Salesforce/codegen-6B-mono,arithmetic_4da,5-shot,accuracy,0.001,0.0007069298939339444
Salesforce/codegen-6B-mono,arithmetic_2ds,5-shot,accuracy,0.0865,0.006287180554084609
Salesforce/codegen-6B-mono,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_1dc,5-shot,accuracy,0.112,0.007053571892184721
Salesforce/codegen-6B-mono,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-6B-mono,arithmetic_2dm,5-shot,accuracy,0.061,0.005352926948264491
Salesforce/codegen-6B-mono,arithmetic_2da,5-shot,accuracy,0.064,0.005474210764278855
Salesforce/codegen-6B-mono,gsm8k_cot,5-shot,accuracy,0.028051554207733132,0.004548229533836338
Salesforce/codegen-6B-mono,gsm8k,5-shot,accuracy,0.026535253980288095,0.004427045987265169
Salesforce/codegen-6B-mono,anli_r2,0-shot,brier_score,0.8876594910752693,
Salesforce/codegen-6B-mono,anli_r3,0-shot,brier_score,0.9495729853127675,
Salesforce/codegen-6B-mono,anli_r1,0-shot,brier_score,0.9382038423017862,
Salesforce/codegen-6B-mono,xnli_eu,0-shot,brier_score,1.2520379152089613,
Salesforce/codegen-6B-mono,xnli_vi,0-shot,brier_score,1.1171458537014292,
Salesforce/codegen-6B-mono,xnli_ru,0-shot,brier_score,0.8728299042741055,
Salesforce/codegen-6B-mono,xnli_zh,0-shot,brier_score,0.921242092190122,
Salesforce/codegen-6B-mono,xnli_tr,0-shot,brier_score,0.9486527913660737,
Salesforce/codegen-6B-mono,xnli_fr,0-shot,brier_score,0.9557543141847247,
Salesforce/codegen-6B-mono,xnli_en,0-shot,brier_score,0.7389286092568025,
Salesforce/codegen-6B-mono,xnli_ur,0-shot,brier_score,1.3111515844555548,
Salesforce/codegen-6B-mono,xnli_ar,0-shot,brier_score,1.1548071078890445,
Salesforce/codegen-6B-mono,xnli_de,0-shot,brier_score,0.9638964115536973,
Salesforce/codegen-6B-mono,xnli_hi,0-shot,brier_score,0.7796514751173046,
Salesforce/codegen-6B-mono,xnli_es,0-shot,brier_score,1.0355571799263816,
Salesforce/codegen-6B-mono,xnli_bg,0-shot,brier_score,0.9432814063972743,
Salesforce/codegen-6B-mono,xnli_sw,0-shot,brier_score,1.1131222005854853,
Salesforce/codegen-6B-mono,xnli_el,0-shot,brier_score,0.8760007535828703,
Salesforce/codegen-6B-mono,xnli_th,0-shot,brier_score,0.9361718072793239,
Salesforce/codegen-6B-mono,logiqa2,0-shot,brier_score,1.132872371789095,
Salesforce/codegen-6B-mono,mathqa,5-shot,brier_score,0.9827803171269597,
Salesforce/codegen-6B-mono,lambada_standard,0-shot,perplexity,87.13890878986679,3.482329775710741
Salesforce/codegen-6B-mono,lambada_standard,0-shot,accuracy,0.2342324859305259,0.005900436024282866
Salesforce/codegen-6B-mono,lambada_openai,0-shot,perplexity,67.5368425039748,2.8888291352437236
Salesforce/codegen-6B-mono,lambada_openai,0-shot,accuracy,0.27983698816223557,0.006254319132119315
Salesforce/codegen-6B-mono,mmlu_world_religions,0-shot,accuracy,0.29239766081871343,0.034886477134579215
Salesforce/codegen-6B-mono,mmlu_formal_logic,0-shot,accuracy,0.29365079365079366,0.04073524322147127
Salesforce/codegen-6B-mono,mmlu_prehistory,0-shot,accuracy,0.2808641975308642,0.025006469755799208
Salesforce/codegen-6B-mono,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.014422292204808836
Salesforce/codegen-6B-mono,mmlu_high_school_world_history,0-shot,accuracy,0.25738396624472576,0.028458820991460285
Salesforce/codegen-6B-mono,mmlu_moral_disputes,0-shot,accuracy,0.25722543352601157,0.023532925431044283
Salesforce/codegen-6B-mono,mmlu_professional_law,0-shot,accuracy,0.24185136897001303,0.010936550813827054
Salesforce/codegen-6B-mono,mmlu_logical_fallacies,0-shot,accuracy,0.27607361963190186,0.0351238528370505
Salesforce/codegen-6B-mono,mmlu_high_school_us_history,0-shot,accuracy,0.24019607843137256,0.02998373305591361
Salesforce/codegen-6B-mono,mmlu_philosophy,0-shot,accuracy,0.2282958199356913,0.02383930331139822
Salesforce/codegen-6B-mono,mmlu_jurisprudence,0-shot,accuracy,0.28703703703703703,0.043733130409147614
Salesforce/codegen-6B-mono,mmlu_international_law,0-shot,accuracy,0.256198347107438,0.03984979653302872
Salesforce/codegen-6B-mono,mmlu_high_school_european_history,0-shot,accuracy,0.23636363636363636,0.033175059300091805
Salesforce/codegen-6B-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.23834196891191708,0.03074890536390989
Salesforce/codegen-6B-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.3277310924369748,0.03048991141767323
Salesforce/codegen-6B-mono,mmlu_high_school_geography,0-shot,accuracy,0.26262626262626265,0.031353050095330834
Salesforce/codegen-6B-mono,mmlu_high_school_psychology,0-shot,accuracy,0.23302752293577983,0.018125669180861496
Salesforce/codegen-6B-mono,mmlu_public_relations,0-shot,accuracy,0.19090909090909092,0.03764425585984926
Salesforce/codegen-6B-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.33,0.04725815626252604
Salesforce/codegen-6B-mono,mmlu_sociology,0-shot,accuracy,0.263681592039801,0.03115715086935557
Salesforce/codegen-6B-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.28717948717948716,0.022939925418530616
Salesforce/codegen-6B-mono,mmlu_security_studies,0-shot,accuracy,0.2857142857142857,0.028920583220675578
Salesforce/codegen-6B-mono,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594726
Salesforce/codegen-6B-mono,mmlu_human_sexuality,0-shot,accuracy,0.25190839694656486,0.03807387116306087
Salesforce/codegen-6B-mono,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748142
Salesforce/codegen-6B-mono,mmlu_miscellaneous,0-shot,accuracy,0.2681992337164751,0.015842430835269428
Salesforce/codegen-6B-mono,mmlu_marketing,0-shot,accuracy,0.25213675213675213,0.02844796547623101
Salesforce/codegen-6B-mono,mmlu_management,0-shot,accuracy,0.1553398058252427,0.035865947385739734
Salesforce/codegen-6B-mono,mmlu_nutrition,0-shot,accuracy,0.2549019607843137,0.02495418432487991
Salesforce/codegen-6B-mono,mmlu_medical_genetics,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-6B-mono,mmlu_human_aging,0-shot,accuracy,0.25112107623318386,0.02910522083322461
Salesforce/codegen-6B-mono,mmlu_professional_medicine,0-shot,accuracy,0.3786764705882353,0.029465133639776125
Salesforce/codegen-6B-mono,mmlu_college_medicine,0-shot,accuracy,0.2023121387283237,0.030631145539198823
Salesforce/codegen-6B-mono,mmlu_business_ethics,0-shot,accuracy,0.35,0.047937248544110196
Salesforce/codegen-6B-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.2339622641509434,0.02605529690115292
Salesforce/codegen-6B-mono,mmlu_global_facts,0-shot,accuracy,0.2,0.04020151261036844
Salesforce/codegen-6B-mono,mmlu_virology,0-shot,accuracy,0.2710843373493976,0.034605799075530276
Salesforce/codegen-6B-mono,mmlu_professional_accounting,0-shot,accuracy,0.25886524822695034,0.026129572527180848
Salesforce/codegen-6B-mono,mmlu_college_physics,0-shot,accuracy,0.16666666666666666,0.03708284662416545
Salesforce/codegen-6B-mono,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
Salesforce/codegen-6B-mono,mmlu_high_school_biology,0-shot,accuracy,0.20967741935483872,0.02315787934908353
Salesforce/codegen-6B-mono,mmlu_college_biology,0-shot,accuracy,0.25,0.03621034121889507
Salesforce/codegen-6B-mono,mmlu_anatomy,0-shot,accuracy,0.1925925925925926,0.034065420585026526
Salesforce/codegen-6B-mono,mmlu_college_chemistry,0-shot,accuracy,0.24,0.042923469599092816
Salesforce/codegen-6B-mono,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
Salesforce/codegen-6B-mono,mmlu_college_computer_science,0-shot,accuracy,0.32,0.04688261722621504
Salesforce/codegen-6B-mono,mmlu_astronomy,0-shot,accuracy,0.19736842105263158,0.03238981601699397
Salesforce/codegen-6B-mono,mmlu_college_mathematics,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-6B-mono,mmlu_conceptual_physics,0-shot,accuracy,0.30638297872340425,0.030135906478517563
Salesforce/codegen-6B-mono,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768077
Salesforce/codegen-6B-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-6B-mono,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.0432704093257873
Salesforce/codegen-6B-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.27586206896551724,0.031447125816782405
Salesforce/codegen-6B-mono,mmlu_high_school_statistics,0-shot,accuracy,0.375,0.033016908987210894
Salesforce/codegen-6B-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.021765961672154537
Salesforce/codegen-6B-mono,mmlu_electrical_engineering,0-shot,accuracy,0.20689655172413793,0.03375672449560553
Salesforce/codegen-6B-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.2777777777777778,0.027309140588230172
Salesforce/codegen-6B-mono,arc_challenge,25-shot,accuracy,0.21416382252559726,0.011988383205966513
Salesforce/codegen-6B-mono,arc_challenge,25-shot,acc_norm,0.24061433447098976,0.012491468532390582
Salesforce/codegen-6B-mono,hellaswag,10-shot,accuracy,0.311292571200956,0.004620758579628644
Salesforce/codegen-6B-mono,hellaswag,10-shot,acc_norm,0.358195578570006,0.004784901248558693
Salesforce/codegen-6B-mono,truthfulqa_mc2,0-shot,accuracy,0.4328537953113665,0.015447836565101179
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,bleu_max,17.25064603206814,0.6130969229491668
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,bleu_acc,0.34149326805385555,0.016600688619950836
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,bleu_diff,-1.7382338560986075,0.5783719458598973
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge1_max,39.52252821322913,0.8483702442864192
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge1_acc,0.3108935128518972,0.016203316673559696
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge1_diff,-3.784504356456098,0.8036994596352391
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge2_max,22.32239340912172,0.8946073218671986
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge2_acc,0.204406364749082,0.014117174337432621
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rouge2_diff,-3.2879573679240153,0.8115132417345173
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rougeL_max,36.318101595189205,0.8312620370594777
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rougeL_acc,0.29865361077111385,0.01602157061376854
Salesforce/codegen-6B-mono,truthfulqa_gen,0-shot,rougeL_diff,-3.7476560334607485,0.7943779862877182
Salesforce/codegen-6B-mono,truthfulqa_mc1,0-shot,accuracy,0.2631578947368421,0.015415241740237009
Salesforce/codegen-6B-mono,winogrande,5-shot,accuracy,0.5390686661404893,0.014009521680980314
jisukim8873/falcon-7B-case-4,arc:challenge,25-shot,accuracy,0.439419795221843,0.014503747823580127
jisukim8873/falcon-7B-case-4,arc:challenge,25-shot,acc_norm,0.4761092150170648,0.014594701798071654
jisukim8873/falcon-7B-case-4,hellaswag,10-shot,accuracy,0.5990838478390759,0.0048908247185303
jisukim8873/falcon-7B-case-4,hellaswag,10-shot,acc_norm,0.7893845847440749,0.004069123905324906
jisukim8873/falcon-7B-case-4,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.04688261722621504
jisukim8873/falcon-7B-case-4,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.04688261722621504
jisukim8873/falcon-7B-case-4,hendrycksTest-anatomy,5-shot,accuracy,0.28888888888888886,0.03915450630414251
jisukim8873/falcon-7B-case-4,hendrycksTest-anatomy,5-shot,acc_norm,0.28888888888888886,0.03915450630414251
jisukim8873/falcon-7B-case-4,hendrycksTest-astronomy,5-shot,accuracy,0.23684210526315788,0.034597776068105365
jisukim8873/falcon-7B-case-4,hendrycksTest-astronomy,5-shot,acc_norm,0.23684210526315788,0.034597776068105365
jisukim8873/falcon-7B-case-4,hendrycksTest-business_ethics,5-shot,accuracy,0.18,0.038612291966536934
jisukim8873/falcon-7B-case-4,hendrycksTest-business_ethics,5-shot,acc_norm,0.18,0.038612291966536934
jisukim8873/falcon-7B-case-4,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.30566037735849055,0.028353298073322663
jisukim8873/falcon-7B-case-4,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.30566037735849055,0.028353298073322663
jisukim8873/falcon-7B-case-4,hendrycksTest-college_biology,5-shot,accuracy,0.25,0.03621034121889507
jisukim8873/falcon-7B-case-4,hendrycksTest-college_biology,5-shot,acc_norm,0.25,0.03621034121889507
jisukim8873/falcon-7B-case-4,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.03775251680686371
jisukim8873/falcon-7B-case-4,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.03775251680686371
jisukim8873/falcon-7B-case-4,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-4,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-4,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.04408440022768078
jisukim8873/falcon-7B-case-4,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.04408440022768078
jisukim8873/falcon-7B-case-4,hendrycksTest-college_medicine,5-shot,accuracy,0.27167630057803466,0.033917503223216586
jisukim8873/falcon-7B-case-4,hendrycksTest-college_medicine,5-shot,acc_norm,0.27167630057803466,0.033917503223216586
jisukim8873/falcon-7B-case-4,hendrycksTest-college_physics,5-shot,accuracy,0.19607843137254902,0.03950581861179961
jisukim8873/falcon-7B-case-4,hendrycksTest-college_physics,5-shot,acc_norm,0.19607843137254902,0.03950581861179961
jisukim8873/falcon-7B-case-4,hendrycksTest-computer_security,5-shot,accuracy,0.39,0.04902071300001974
jisukim8873/falcon-7B-case-4,hendrycksTest-computer_security,5-shot,acc_norm,0.39,0.04902071300001974
jisukim8873/falcon-7B-case-4,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.030363582197238174
jisukim8873/falcon-7B-case-4,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.030363582197238174
jisukim8873/falcon-7B-case-4,hendrycksTest-econometrics,5-shot,accuracy,0.30701754385964913,0.0433913832257986
jisukim8873/falcon-7B-case-4,hendrycksTest-econometrics,5-shot,acc_norm,0.30701754385964913,0.0433913832257986
jisukim8873/falcon-7B-case-4,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.03600105692727771
jisukim8873/falcon-7B-case-4,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.03600105692727771
jisukim8873/falcon-7B-case-4,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2804232804232804,0.02313528797432563
jisukim8873/falcon-7B-case-4,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2804232804232804,0.02313528797432563
jisukim8873/falcon-7B-case-4,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.037184890068181146
jisukim8873/falcon-7B-case-4,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.037184890068181146
jisukim8873/falcon-7B-case-4,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-4,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_biology,5-shot,accuracy,0.3032258064516129,0.026148685930671753
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3032258064516129,0.026148685930671753
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.32019704433497537,0.032826493853041504
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.32019704433497537,0.032826493853041504
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.047609522856952344
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.047609522856952344
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2787878787878788,0.03501438706296781
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2787878787878788,0.03501438706296781
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_geography,5-shot,accuracy,0.26262626262626265,0.03135305009533086
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_geography,5-shot,acc_norm,0.26262626262626265,0.03135305009533086
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.03257714077709662
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.03257714077709662
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2717948717948718,0.022556551010132354
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2717948717948718,0.022556551010132354
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.02671924078371218
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25925925925925924,0.02671924078371218
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.028801392193631273
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.028801392193631273
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.03684881521389023
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.03684881521389023
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_psychology,5-shot,accuracy,0.30091743119266057,0.019664751366802114
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.30091743119266057,0.019664751366802114
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_statistics,5-shot,accuracy,0.18981481481481483,0.026744714834691943
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.18981481481481483,0.026744714834691943
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_us_history,5-shot,accuracy,0.27450980392156865,0.031321798030832904
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.27450980392156865,0.031321798030832904
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-4,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3037974683544304,0.0299366963871386
jisukim8873/falcon-7B-case-4,hendrycksTest-human_aging,5-shot,accuracy,0.4125560538116592,0.03304062175449297
jisukim8873/falcon-7B-case-4,hendrycksTest-human_aging,5-shot,acc_norm,0.4125560538116592,0.03304062175449297
jisukim8873/falcon-7B-case-4,hendrycksTest-human_sexuality,5-shot,accuracy,0.25190839694656486,0.03807387116306086
jisukim8873/falcon-7B-case-4,hendrycksTest-human_sexuality,5-shot,acc_norm,0.25190839694656486,0.03807387116306086
jisukim8873/falcon-7B-case-4,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.04294340845212094
jisukim8873/falcon-7B-case-4,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.04294340845212094
jisukim8873/falcon-7B-case-4,hendrycksTest-jurisprudence,5-shot,accuracy,0.32407407407407407,0.04524596007030048
jisukim8873/falcon-7B-case-4,hendrycksTest-jurisprudence,5-shot,acc_norm,0.32407407407407407,0.04524596007030048
jisukim8873/falcon-7B-case-4,hendrycksTest-logical_fallacies,5-shot,accuracy,0.27607361963190186,0.03512385283705051
jisukim8873/falcon-7B-case-4,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.27607361963190186,0.03512385283705051
jisukim8873/falcon-7B-case-4,hendrycksTest-machine_learning,5-shot,accuracy,0.3482142857142857,0.04521829902833585
jisukim8873/falcon-7B-case-4,hendrycksTest-machine_learning,5-shot,acc_norm,0.3482142857142857,0.04521829902833585
jisukim8873/falcon-7B-case-4,hendrycksTest-management,5-shot,accuracy,0.3106796116504854,0.04582124160161552
jisukim8873/falcon-7B-case-4,hendrycksTest-management,5-shot,acc_norm,0.3106796116504854,0.04582124160161552
jisukim8873/falcon-7B-case-4,hendrycksTest-marketing,5-shot,accuracy,0.32905982905982906,0.030782321577688166
jisukim8873/falcon-7B-case-4,hendrycksTest-marketing,5-shot,acc_norm,0.32905982905982906,0.030782321577688166
jisukim8873/falcon-7B-case-4,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.047937248544110196
jisukim8873/falcon-7B-case-4,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.047937248544110196
jisukim8873/falcon-7B-case-4,hendrycksTest-miscellaneous,5-shot,accuracy,0.367816091954023,0.017243828891846273
jisukim8873/falcon-7B-case-4,hendrycksTest-miscellaneous,5-shot,acc_norm,0.367816091954023,0.017243828891846273
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_disputes,5-shot,accuracy,0.33236994219653176,0.025361168749688235
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_disputes,5-shot,acc_norm,0.33236994219653176,0.025361168749688235
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_scenarios,5-shot,accuracy,0.26256983240223464,0.014716824273017768
jisukim8873/falcon-7B-case-4,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.26256983240223464,0.014716824273017768
jisukim8873/falcon-7B-case-4,hendrycksTest-nutrition,5-shot,accuracy,0.3366013071895425,0.027057974624494382
jisukim8873/falcon-7B-case-4,hendrycksTest-nutrition,5-shot,acc_norm,0.3366013071895425,0.027057974624494382
jisukim8873/falcon-7B-case-4,hendrycksTest-philosophy,5-shot,accuracy,0.31511254019292606,0.02638527370346447
jisukim8873/falcon-7B-case-4,hendrycksTest-philosophy,5-shot,acc_norm,0.31511254019292606,0.02638527370346447
jisukim8873/falcon-7B-case-4,hendrycksTest-prehistory,5-shot,accuracy,0.3055555555555556,0.025630824975621344
jisukim8873/falcon-7B-case-4,hendrycksTest-prehistory,5-shot,acc_norm,0.3055555555555556,0.025630824975621344
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_accounting,5-shot,accuracy,0.25886524822695034,0.026129572527180848
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25886524822695034,0.026129572527180848
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_law,5-shot,accuracy,0.2607561929595828,0.01121347155960232
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_law,5-shot,acc_norm,0.2607561929595828,0.01121347155960232
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.023709788253811766
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.023709788253811766
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_psychology,5-shot,accuracy,0.25980392156862747,0.017740899509177795
jisukim8873/falcon-7B-case-4,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25980392156862747,0.017740899509177795
jisukim8873/falcon-7B-case-4,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.04309118709946458
jisukim8873/falcon-7B-case-4,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.04309118709946458
jisukim8873/falcon-7B-case-4,hendrycksTest-security_studies,5-shot,accuracy,0.20408163265306123,0.02580128347509051
jisukim8873/falcon-7B-case-4,hendrycksTest-security_studies,5-shot,acc_norm,0.20408163265306123,0.02580128347509051
jisukim8873/falcon-7B-case-4,hendrycksTest-sociology,5-shot,accuracy,0.30845771144278605,0.03265819588512697
jisukim8873/falcon-7B-case-4,hendrycksTest-sociology,5-shot,acc_norm,0.30845771144278605,0.03265819588512697
jisukim8873/falcon-7B-case-4,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.43,0.049756985195624284
jisukim8873/falcon-7B-case-4,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.43,0.049756985195624284
jisukim8873/falcon-7B-case-4,hendrycksTest-virology,5-shot,accuracy,0.3433734939759036,0.03696584317010601
jisukim8873/falcon-7B-case-4,hendrycksTest-virology,5-shot,acc_norm,0.3433734939759036,0.03696584317010601
jisukim8873/falcon-7B-case-4,hendrycksTest-world_religions,5-shot,accuracy,0.3684210526315789,0.036996580176568775
jisukim8873/falcon-7B-case-4,hendrycksTest-world_religions,5-shot,acc_norm,0.3684210526315789,0.036996580176568775
jisukim8873/falcon-7B-case-4,truthfulqa:mc,0-shot,mc1,0.25703794369645044,0.01529807750948508
jisukim8873/falcon-7B-case-4,truthfulqa:mc,0-shot,mc2,0.3778933788894488,0.014398186673209289
jisukim8873/falcon-7B-case-4,winogrande,5-shot,accuracy,0.7016574585635359,0.012858885010030425
jisukim8873/falcon-7B-case-4,gsm8k,5-shot,accuracy,0.07808946171341925,0.007390654481108223
jisukim8873/falcon-7B-case-4,minerva_math_precalc,5-shot,accuracy,0.007326007326007326,0.0036529080893830325
jisukim8873/falcon-7B-case-4,minerva_math_prealgebra,5-shot,accuracy,0.02640642939150402,0.005436057762573987
jisukim8873/falcon-7B-case-4,minerva_math_num_theory,5-shot,accuracy,0.012962962962962963,0.004872192984581494
jisukim8873/falcon-7B-case-4,minerva_math_intermediate_algebra,5-shot,accuracy,0.013289036544850499,0.0038127511080199464
jisukim8873/falcon-7B-case-4,minerva_math_geometry,5-shot,accuracy,0.020876826722338204,0.006539385795813939
jisukim8873/falcon-7B-case-4,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.006275362513989589
jisukim8873/falcon-7B-case-4,minerva_math_algebra,5-shot,accuracy,0.018534119629317607,0.0039163476763639645
jisukim8873/falcon-7B-case-4,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-4,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-4,arithmetic_3da,5-shot,accuracy,0.202,0.008979884139540944
jisukim8873/falcon-7B-case-4,arithmetic_3ds,5-shot,accuracy,0.3245,0.010471614123485492
jisukim8873/falcon-7B-case-4,arithmetic_4da,5-shot,accuracy,0.0055,0.0016541593398342208
jisukim8873/falcon-7B-case-4,arithmetic_2ds,5-shot,accuracy,0.4725,0.011166208716863541
jisukim8873/falcon-7B-case-4,arithmetic_5ds,5-shot,accuracy,0.035,0.004110468096699785
jisukim8873/falcon-7B-case-4,arithmetic_5da,5-shot,accuracy,0.0005,0.0005000000000000087
jisukim8873/falcon-7B-case-4,arithmetic_1dc,5-shot,accuracy,0.091,0.006432743590028116
jisukim8873/falcon-7B-case-4,arithmetic_4ds,5-shot,accuracy,0.084,0.006204131335071231
jisukim8873/falcon-7B-case-4,arithmetic_2dm,5-shot,accuracy,0.2645,0.00986501567495644
jisukim8873/falcon-7B-case-4,arithmetic_2da,5-shot,accuracy,0.7235,0.010003694915178819
jisukim8873/falcon-7B-case-4,gsm8k_cot,5-shot,accuracy,0.09628506444275967,0.00812526412821589
jisukim8873/falcon-7B-case-4,anli_r2,0-shot,brier_score,0.9413381511880419,
jisukim8873/falcon-7B-case-4,anli_r3,0-shot,brier_score,0.87237900881463,
jisukim8873/falcon-7B-case-4,anli_r1,0-shot,brier_score,0.9857934321426033,
jisukim8873/falcon-7B-case-4,xnli_eu,0-shot,brier_score,1.026246542896589,
jisukim8873/falcon-7B-case-4,xnli_vi,0-shot,brier_score,1.0279853059907973,
jisukim8873/falcon-7B-case-4,xnli_ru,0-shot,brier_score,0.8097107614311067,
jisukim8873/falcon-7B-case-4,xnli_zh,0-shot,brier_score,1.0156490749806204,
jisukim8873/falcon-7B-case-4,xnli_tr,0-shot,brier_score,0.9756836472864571,
jisukim8873/falcon-7B-case-4,xnli_fr,0-shot,brier_score,0.7603981109729621,
jisukim8873/falcon-7B-case-4,xnli_en,0-shot,brier_score,0.662787162600886,
jisukim8873/falcon-7B-case-4,xnli_ur,0-shot,brier_score,1.31954291668707,
jisukim8873/falcon-7B-case-4,xnli_ar,0-shot,brier_score,1.2990240082694595,
jisukim8873/falcon-7B-case-4,xnli_de,0-shot,brier_score,0.849940839182899,
jisukim8873/falcon-7B-case-4,xnli_hi,0-shot,brier_score,1.1637487081246372,
jisukim8873/falcon-7B-case-4,xnli_es,0-shot,brier_score,0.8194931547365195,
jisukim8873/falcon-7B-case-4,xnli_bg,0-shot,brier_score,0.9860919344218411,
jisukim8873/falcon-7B-case-4,xnli_sw,0-shot,brier_score,1.082079686536352,
jisukim8873/falcon-7B-case-4,xnli_el,0-shot,brier_score,0.8625720197689004,
jisukim8873/falcon-7B-case-4,xnli_th,0-shot,brier_score,0.9548472798777402,
jisukim8873/falcon-7B-case-4,logiqa2,0-shot,brier_score,1.05514711694456,
jisukim8873/falcon-7B-case-4,mathqa,5-shot,brier_score,0.9309727513874027,
jisukim8873/falcon-7B-case-4,lambada_standard,0-shot,perplexity,4.1704788868142115,0.09171832314219872
jisukim8873/falcon-7B-case-4,lambada_standard,0-shot,accuracy,0.6625266834853484,0.006587694938528704
jisukim8873/falcon-7B-case-4,lambada_openai,0-shot,perplexity,3.3182500746294745,0.06880896651957533
jisukim8873/falcon-7B-case-4,lambada_openai,0-shot,accuracy,0.7343295167863381,0.006153599372822545
jisukim8873/falcon-7B-case-4,mmlu_world_religions,0-shot,accuracy,0.36257309941520466,0.0368713061556206
jisukim8873/falcon-7B-case-4,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.03718489006818114
jisukim8873/falcon-7B-case-4,mmlu_prehistory,0-shot,accuracy,0.30246913580246915,0.025557653981868052
jisukim8873/falcon-7B-case-4,mmlu_moral_scenarios,0-shot,accuracy,0.26033519553072626,0.014676252009319471
jisukim8873/falcon-7B-case-4,mmlu_high_school_world_history,0-shot,accuracy,0.3037974683544304,0.02993669638713861
jisukim8873/falcon-7B-case-4,mmlu_moral_disputes,0-shot,accuracy,0.32947976878612717,0.025305258131879727
jisukim8873/falcon-7B-case-4,mmlu_professional_law,0-shot,accuracy,0.2633637548891786,0.011249506403605287
jisukim8873/falcon-7B-case-4,mmlu_logical_fallacies,0-shot,accuracy,0.27607361963190186,0.03512385283705051
jisukim8873/falcon-7B-case-4,mmlu_high_school_us_history,0-shot,accuracy,0.27941176470588236,0.031493281045079556
jisukim8873/falcon-7B-case-4,mmlu_philosophy,0-shot,accuracy,0.3086816720257235,0.02623696588115326
jisukim8873/falcon-7B-case-4,mmlu_jurisprudence,0-shot,accuracy,0.35185185185185186,0.04616631111801715
jisukim8873/falcon-7B-case-4,mmlu_international_law,0-shot,accuracy,0.3305785123966942,0.04294340845212094
jisukim8873/falcon-7B-case-4,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.03546563019624335
jisukim8873/falcon-7B-case-4,mmlu_high_school_government_and_politics,0-shot,accuracy,0.27979274611398963,0.032396370467357015
jisukim8873/falcon-7B-case-4,mmlu_high_school_microeconomics,0-shot,accuracy,0.2647058823529412,0.028657491285071963
jisukim8873/falcon-7B-case-4,mmlu_high_school_geography,0-shot,accuracy,0.2777777777777778,0.03191178226713547
jisukim8873/falcon-7B-case-4,mmlu_high_school_psychology,0-shot,accuracy,0.30642201834862387,0.019765517220458523
jisukim8873/falcon-7B-case-4,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.04265792110940588
jisukim8873/falcon-7B-case-4,mmlu_us_foreign_policy,0-shot,accuracy,0.42,0.049604496374885836
jisukim8873/falcon-7B-case-4,mmlu_sociology,0-shot,accuracy,0.3034825870646766,0.032510068164586195
jisukim8873/falcon-7B-case-4,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2743589743589744,0.022622765767493214
jisukim8873/falcon-7B-case-4,mmlu_security_studies,0-shot,accuracy,0.20816326530612245,0.025991117672813296
jisukim8873/falcon-7B-case-4,mmlu_professional_psychology,0-shot,accuracy,0.2581699346405229,0.017704531653250064
jisukim8873/falcon-7B-case-4,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
jisukim8873/falcon-7B-case-4,mmlu_econometrics,0-shot,accuracy,0.3157894736842105,0.04372748290278008
jisukim8873/falcon-7B-case-4,mmlu_miscellaneous,0-shot,accuracy,0.3652618135376756,0.01721853002883864
jisukim8873/falcon-7B-case-4,mmlu_marketing,0-shot,accuracy,0.3333333333333333,0.030882736974138656
jisukim8873/falcon-7B-case-4,mmlu_management,0-shot,accuracy,0.33980582524271846,0.04689765937278135
jisukim8873/falcon-7B-case-4,mmlu_nutrition,0-shot,accuracy,0.3300653594771242,0.026925654653615693
jisukim8873/falcon-7B-case-4,mmlu_medical_genetics,0-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-4,mmlu_human_aging,0-shot,accuracy,0.4170403587443946,0.03309266936071721
jisukim8873/falcon-7B-case-4,mmlu_professional_medicine,0-shot,accuracy,0.1948529411764706,0.024060599423487414
jisukim8873/falcon-7B-case-4,mmlu_college_medicine,0-shot,accuracy,0.27167630057803466,0.03391750322321659
jisukim8873/falcon-7B-case-4,mmlu_business_ethics,0-shot,accuracy,0.2,0.040201512610368466
jisukim8873/falcon-7B-case-4,mmlu_clinical_knowledge,0-shot,accuracy,0.30566037735849055,0.028353298073322666
jisukim8873/falcon-7B-case-4,mmlu_global_facts,0-shot,accuracy,0.37,0.04852365870939099
jisukim8873/falcon-7B-case-4,mmlu_virology,0-shot,accuracy,0.3313253012048193,0.03664314777288086
jisukim8873/falcon-7B-case-4,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539005,0.025892151156709405
jisukim8873/falcon-7B-case-4,mmlu_college_physics,0-shot,accuracy,0.19607843137254902,0.03950581861179963
jisukim8873/falcon-7B-case-4,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.03684881521389023
jisukim8873/falcon-7B-case-4,mmlu_high_school_biology,0-shot,accuracy,0.3096774193548387,0.026302774983517414
jisukim8873/falcon-7B-case-4,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
jisukim8873/falcon-7B-case-4,mmlu_anatomy,0-shot,accuracy,0.2962962962962963,0.03944624162501116
jisukim8873/falcon-7B-case-4,mmlu_college_chemistry,0-shot,accuracy,0.17,0.037752516806863715
jisukim8873/falcon-7B-case-4,mmlu_computer_security,0-shot,accuracy,0.36,0.048241815132442176
jisukim8873/falcon-7B-case-4,mmlu_college_computer_science,0-shot,accuracy,0.35,0.047937248544110196
jisukim8873/falcon-7B-case-4,mmlu_astronomy,0-shot,accuracy,0.23026315789473684,0.034260594244031654
jisukim8873/falcon-7B-case-4,mmlu_college_mathematics,0-shot,accuracy,0.27,0.0446196043338474
jisukim8873/falcon-7B-case-4,mmlu_conceptual_physics,0-shot,accuracy,0.31063829787234043,0.03025123757921317
jisukim8873/falcon-7B-case-4,mmlu_abstract_algebra,0-shot,accuracy,0.33,0.04725815626252604
jisukim8873/falcon-7B-case-4,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-4,mmlu_machine_learning,0-shot,accuracy,0.3482142857142857,0.045218299028335865
jisukim8873/falcon-7B-case-4,mmlu_high_school_chemistry,0-shot,accuracy,0.32019704433497537,0.032826493853041504
jisukim8873/falcon-7B-case-4,mmlu_high_school_statistics,0-shot,accuracy,0.18518518518518517,0.026491914727355143
jisukim8873/falcon-7B-case-4,mmlu_elementary_mathematics,0-shot,accuracy,0.2777777777777778,0.023068188848261107
jisukim8873/falcon-7B-case-4,mmlu_electrical_engineering,0-shot,accuracy,0.2620689655172414,0.036646663372252565
jisukim8873/falcon-7B-case-4,mmlu_high_school_mathematics,0-shot,accuracy,0.2777777777777778,0.027309140588230186
jisukim8873/falcon-7B-case-4,arc_challenge,25-shot,accuracy,0.4453924914675768,0.01452398763834407
jisukim8873/falcon-7B-case-4,arc_challenge,25-shot,acc_norm,0.4735494880546075,0.014590931358120165
jisukim8873/falcon-7B-case-4,truthfulqa_mc2,0-shot,accuracy,0.37834578161036186,0.01440719551886384
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,bleu_max,25.491727663339983,0.7876825292141532
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,bleu_acc,0.2974296205630355,0.016002651487360974
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,bleu_diff,-7.301689206799327,0.7522941543496632
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge1_max,51.09004512811695,0.8130980891432411
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge1_acc,0.3011015911872705,0.01605899902610058
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge1_diff,-9.608532182458044,0.7813130202579182
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge2_max,35.83529943497114,0.9345260348229253
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge2_acc,0.2692778457772338,0.015528566637087253
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rouge2_diff,-10.914652626830508,0.943625020821263
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rougeL_max,47.82154692891093,0.8290897847436749
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rougeL_acc,0.3023255813953488,0.016077509266133033
jisukim8873/falcon-7B-case-4,truthfulqa_gen,0-shot,rougeL_diff,-9.662039631078054,0.7901057890971249
jisukim8873/falcon-7B-case-4,truthfulqa_mc1,0-shot,accuracy,0.2594859241126071,0.015345409485557996
frank098/orca_mini_3b_juniper,minerva_math_precalc,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_prealgebra,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_num_theory,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_geometry,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,minerva_math_algebra,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,fld_default,0-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,fld_star,0-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_3da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_3ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_4da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_2ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_5ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_5da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_1dc,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_4ds,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_2dm,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,arithmetic_2da,5-shot,accuracy,0.0,
frank098/orca_mini_3b_juniper,gsm8k_cot,5-shot,accuracy,0.012130401819560273,0.0030152942428909512
frank098/orca_mini_3b_juniper,gsm8k,5-shot,accuracy,0.01288855193328279,0.003106901266499648
frank098/orca_mini_3b_juniper,anli_r2,0-shot,brier_score,0.9441265812558454,
frank098/orca_mini_3b_juniper,anli_r3,0-shot,brier_score,0.9703161154980474,
frank098/orca_mini_3b_juniper,anli_r1,0-shot,brier_score,0.9194398384386011,
frank098/orca_mini_3b_juniper,xnli_eu,0-shot,brier_score,1.2999740747701036,
frank098/orca_mini_3b_juniper,xnli_vi,0-shot,brier_score,1.0362620270912626,
frank098/orca_mini_3b_juniper,xnli_ru,0-shot,brier_score,1.0761920507221865,
frank098/orca_mini_3b_juniper,xnli_zh,0-shot,brier_score,1.14055510107051,
frank098/orca_mini_3b_juniper,xnli_tr,0-shot,brier_score,1.0280785216228991,
frank098/orca_mini_3b_juniper,xnli_fr,0-shot,brier_score,1.0706704552738813,
frank098/orca_mini_3b_juniper,xnli_en,0-shot,brier_score,0.9206631257574506,
frank098/orca_mini_3b_juniper,xnli_ur,0-shot,brier_score,1.2254000998967638,
frank098/orca_mini_3b_juniper,xnli_ar,0-shot,brier_score,1.022517089906402,
frank098/orca_mini_3b_juniper,xnli_de,0-shot,brier_score,0.9989803721354296,
frank098/orca_mini_3b_juniper,xnli_hi,0-shot,brier_score,0.9769413427617684,
frank098/orca_mini_3b_juniper,xnli_es,0-shot,brier_score,1.1242290380210678,
frank098/orca_mini_3b_juniper,xnli_bg,0-shot,brier_score,1.068234109399256,
frank098/orca_mini_3b_juniper,xnli_sw,0-shot,brier_score,1.11414698513575,
frank098/orca_mini_3b_juniper,xnli_el,0-shot,brier_score,1.2572221142753026,
frank098/orca_mini_3b_juniper,xnli_th,0-shot,brier_score,0.8811067149691696,
frank098/orca_mini_3b_juniper,logiqa2,0-shot,brier_score,1.5569003832753865,
frank098/orca_mini_3b_juniper,mathqa,5-shot,brier_score,1.177340881036448,
frank098/orca_mini_3b_juniper,lambada_standard,0-shot,perplexity,4.4069819807293254e+29,2.995589241082757e+29
frank098/orca_mini_3b_juniper,lambada_standard,0-shot,accuracy,0.048903551329322725,0.0030046545800346733
frank098/orca_mini_3b_juniper,lambada_openai,0-shot,perplexity,161290686836746.56,87566827028421.67
frank098/orca_mini_3b_juniper,lambada_openai,0-shot,accuracy,0.15893654182029884,0.005093758311628701
frank098/orca_mini_3b_juniper,mmlu_world_religions,0-shot,accuracy,0.2573099415204678,0.03352799844161865
frank098/orca_mini_3b_juniper,mmlu_formal_logic,0-shot,accuracy,0.19047619047619047,0.035122074123020534
frank098/orca_mini_3b_juniper,mmlu_prehistory,0-shot,accuracy,0.24382716049382716,0.023891879541959614
frank098/orca_mini_3b_juniper,mmlu_moral_scenarios,0-shot,accuracy,0.25027932960893856,0.014487500852850414
frank098/orca_mini_3b_juniper,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.029041333510598025
frank098/orca_mini_3b_juniper,mmlu_moral_disputes,0-shot,accuracy,0.28901734104046245,0.02440517393578323
frank098/orca_mini_3b_juniper,mmlu_professional_law,0-shot,accuracy,0.24445893089960888,0.010976425013113886
frank098/orca_mini_3b_juniper,mmlu_logical_fallacies,0-shot,accuracy,0.2085889570552147,0.03192193448934724
frank098/orca_mini_3b_juniper,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.030587591351604246
frank098/orca_mini_3b_juniper,mmlu_philosophy,0-shot,accuracy,0.3054662379421222,0.02616058445014048
frank098/orca_mini_3b_juniper,mmlu_jurisprudence,0-shot,accuracy,0.24074074074074073,0.04133119440243838
frank098/orca_mini_3b_juniper,mmlu_international_law,0-shot,accuracy,0.35537190082644626,0.0436923632657398
frank098/orca_mini_3b_juniper,mmlu_high_school_european_history,0-shot,accuracy,0.26666666666666666,0.034531318018854146
frank098/orca_mini_3b_juniper,mmlu_high_school_government_and_politics,0-shot,accuracy,0.26424870466321243,0.03182155050916648
frank098/orca_mini_3b_juniper,mmlu_high_school_microeconomics,0-shot,accuracy,0.21008403361344538,0.026461398717471874
frank098/orca_mini_3b_juniper,mmlu_high_school_geography,0-shot,accuracy,0.24242424242424243,0.030532892233932026
frank098/orca_mini_3b_juniper,mmlu_high_school_psychology,0-shot,accuracy,0.25688073394495414,0.018732492928342444
frank098/orca_mini_3b_juniper,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.041723430387053825
frank098/orca_mini_3b_juniper,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.04512608598542126
frank098/orca_mini_3b_juniper,mmlu_sociology,0-shot,accuracy,0.25870646766169153,0.030965903123573026
frank098/orca_mini_3b_juniper,mmlu_high_school_macroeconomics,0-shot,accuracy,0.258974358974359,0.022211106810061665
frank098/orca_mini_3b_juniper,mmlu_security_studies,0-shot,accuracy,0.2612244897959184,0.028123429335142783
frank098/orca_mini_3b_juniper,mmlu_professional_psychology,0-shot,accuracy,0.27450980392156865,0.018054027458815198
frank098/orca_mini_3b_juniper,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596918
frank098/orca_mini_3b_juniper,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.03999423879281335
frank098/orca_mini_3b_juniper,mmlu_miscellaneous,0-shot,accuracy,0.29246487867177523,0.01626700068459864
frank098/orca_mini_3b_juniper,mmlu_marketing,0-shot,accuracy,0.3076923076923077,0.030236389942173092
frank098/orca_mini_3b_juniper,mmlu_management,0-shot,accuracy,0.2815533980582524,0.044532548363264673
frank098/orca_mini_3b_juniper,mmlu_nutrition,0-shot,accuracy,0.24509803921568626,0.02463004897982478
frank098/orca_mini_3b_juniper,mmlu_medical_genetics,0-shot,accuracy,0.19,0.03942772444036624
frank098/orca_mini_3b_juniper,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.03126580522513713
frank098/orca_mini_3b_juniper,mmlu_professional_medicine,0-shot,accuracy,0.1875,0.023709788253811766
frank098/orca_mini_3b_juniper,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
frank098/orca_mini_3b_juniper,mmlu_business_ethics,0-shot,accuracy,0.26,0.04408440022768079
frank098/orca_mini_3b_juniper,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.02688064788905198
frank098/orca_mini_3b_juniper,mmlu_global_facts,0-shot,accuracy,0.28,0.04512608598542127
frank098/orca_mini_3b_juniper,mmlu_virology,0-shot,accuracy,0.2710843373493976,0.03460579907553027
frank098/orca_mini_3b_juniper,mmlu_professional_accounting,0-shot,accuracy,0.24468085106382978,0.025645553622266736
frank098/orca_mini_3b_juniper,mmlu_college_physics,0-shot,accuracy,0.1568627450980392,0.03618664819936249
frank098/orca_mini_3b_juniper,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.037101857261199946
frank098/orca_mini_3b_juniper,mmlu_high_school_biology,0-shot,accuracy,0.22903225806451613,0.023904914311782648
frank098/orca_mini_3b_juniper,mmlu_college_biology,0-shot,accuracy,0.2847222222222222,0.037738099906869355
frank098/orca_mini_3b_juniper,mmlu_anatomy,0-shot,accuracy,0.32592592592592595,0.040491220417025055
frank098/orca_mini_3b_juniper,mmlu_college_chemistry,0-shot,accuracy,0.18,0.038612291966536955
frank098/orca_mini_3b_juniper,mmlu_computer_security,0-shot,accuracy,0.26,0.04408440022768078
frank098/orca_mini_3b_juniper,mmlu_college_computer_science,0-shot,accuracy,0.3,0.04605661864718381
frank098/orca_mini_3b_juniper,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.035834961763610625
frank098/orca_mini_3b_juniper,mmlu_college_mathematics,0-shot,accuracy,0.26,0.04408440022768079
frank098/orca_mini_3b_juniper,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.02951319662553935
frank098/orca_mini_3b_juniper,mmlu_abstract_algebra,0-shot,accuracy,0.33,0.047258156262526045
frank098/orca_mini_3b_juniper,mmlu_high_school_computer_science,0-shot,accuracy,0.32,0.04688261722621504
frank098/orca_mini_3b_juniper,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.042878587513404544
frank098/orca_mini_3b_juniper,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.030516530732694436
frank098/orca_mini_3b_juniper,mmlu_high_school_statistics,0-shot,accuracy,0.21296296296296297,0.02792096314799366
frank098/orca_mini_3b_juniper,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.021765961672154523
frank098/orca_mini_3b_juniper,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.03752833958003336
frank098/orca_mini_3b_juniper,mmlu_high_school_mathematics,0-shot,accuracy,0.26666666666666666,0.02696242432507384
frank098/orca_mini_3b_juniper,arc_challenge,25-shot,accuracy,0.20819112627986347,0.011864866118448064
frank098/orca_mini_3b_juniper,arc_challenge,25-shot,acc_norm,0.2721843003412969,0.013006600406423702
frank098/orca_mini_3b_juniper,hellaswag,10-shot,accuracy,0.2951603266281617,0.004551826272978069
frank098/orca_mini_3b_juniper,hellaswag,10-shot,acc_norm,0.3249352718581956,0.004673934837150445
frank098/orca_mini_3b_juniper,truthfulqa_mc2,0-shot,accuracy,nan,nan
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,bleu_max,2.121881599584391,0.019092051524966364
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,bleu_acc,0.2864137086903305,0.01582614243950238
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,bleu_diff,-0.011217465344358236,0.014945493640817572
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge1_max,9.582083815386978,0.2347410115210295
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge1_acc,0.3659730722154223,0.0168629416840884
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge1_diff,-0.33342510959799404,0.24223368193993938
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge2_max,0.15461298329759457,0.035411667590005365
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge2_acc,0.014687882496940025,0.0042113508796163095
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rouge2_diff,-0.06323759927762475,0.047484334407979674
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rougeL_max,9.028388478244057,0.21328918229746682
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rougeL_acc,0.36474908200734396,0.016850961061720144
frank098/orca_mini_3b_juniper,truthfulqa_gen,0-shot,rougeL_diff,-0.18092953651312477,0.22234608369714223
frank098/orca_mini_3b_juniper,truthfulqa_mc1,0-shot,accuracy,0.23990208078335373,0.014948812679062133
frank098/orca_mini_3b_juniper,winogrande,5-shot,accuracy,0.5406471981057617,0.01400597382382514
AbacusResearch/jaLLAbi,minerva_math_precalc,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_prealgebra,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_num_theory,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_geometry,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,minerva_math_algebra,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,fld_default,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,fld_star,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_3da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_3ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_4da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_2ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_5ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_5da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_1dc,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_4ds,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_2dm,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,arithmetic_2da,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,gsm8k_cot,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,gsm8k,5-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,anli_r2,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,anli_r3,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,anli_r1,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_eu,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_vi,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_ru,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_zh,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_tr,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_fr,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_en,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_ur,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_ar,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_de,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_hi,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_es,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_bg,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_sw,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_el,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,xnli_th,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,logiqa2,0-shot,brier_score,nan,
AbacusResearch/jaLLAbi,mathqa,5-shot,brier_score,nan,
AbacusResearch/jaLLAbi,lambada_standard,0-shot,perplexity,nan,nan
AbacusResearch/jaLLAbi,lambada_standard,0-shot,accuracy,0.0,
AbacusResearch/jaLLAbi,lambada_openai,0-shot,perplexity,nan,nan
AbacusResearch/jaLLAbi,lambada_openai,0-shot,accuracy,0.0,
kevin009/flyingllama-v2,arc:challenge,25-shot,accuracy,0.2158703071672355,0.012022975360030672
kevin009/flyingllama-v2,arc:challenge,25-shot,acc_norm,0.24744027303754265,0.01261035266329267
kevin009/flyingllama-v2,hellaswag,10-shot,accuracy,0.32732523401712804,0.004682780790508346
kevin009/flyingllama-v2,hellaswag,10-shot,acc_norm,0.3843855805616411,0.004854555294017559
kevin009/flyingllama-v2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.19,0.039427724440366234
kevin009/flyingllama-v2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.19,0.039427724440366234
kevin009/flyingllama-v2,hendrycksTest-anatomy,5-shot,accuracy,0.26666666666666666,0.03820169914517904
kevin009/flyingllama-v2,hendrycksTest-anatomy,5-shot,acc_norm,0.26666666666666666,0.03820169914517904
kevin009/flyingllama-v2,hendrycksTest-astronomy,5-shot,accuracy,0.19736842105263158,0.03238981601699397
kevin009/flyingllama-v2,hendrycksTest-astronomy,5-shot,acc_norm,0.19736842105263158,0.03238981601699397
kevin009/flyingllama-v2,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.04163331998932268
kevin009/flyingllama-v2,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.04163331998932268
kevin009/flyingllama-v2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.24150943396226415,0.02634148037111836
kevin009/flyingllama-v2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.24150943396226415,0.02634148037111836
kevin009/flyingllama-v2,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.03653946969442099
kevin009/flyingllama-v2,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.03653946969442099
kevin009/flyingllama-v2,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036845
kevin009/flyingllama-v2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036845
kevin009/flyingllama-v2,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.047937248544110196
kevin009/flyingllama-v2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.047937248544110196
kevin009/flyingllama-v2,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.04605661864718381
kevin009/flyingllama-v2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.04605661864718381
kevin009/flyingllama-v2,hendrycksTest-college_medicine,5-shot,accuracy,0.23699421965317918,0.03242414757483099
kevin009/flyingllama-v2,hendrycksTest-college_medicine,5-shot,acc_norm,0.23699421965317918,0.03242414757483099
kevin009/flyingllama-v2,hendrycksTest-college_physics,5-shot,accuracy,0.3137254901960784,0.04617034827006717
kevin009/flyingllama-v2,hendrycksTest-college_physics,5-shot,acc_norm,0.3137254901960784,0.04617034827006717
kevin009/flyingllama-v2,hendrycksTest-computer_security,5-shot,accuracy,0.22,0.0416333199893227
kevin009/flyingllama-v2,hendrycksTest-computer_security,5-shot,acc_norm,0.22,0.0416333199893227
kevin009/flyingllama-v2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.19574468085106383,0.025937853139977148
kevin009/flyingllama-v2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.19574468085106383,0.025937853139977148
kevin009/flyingllama-v2,hendrycksTest-econometrics,5-shot,accuracy,0.23684210526315788,0.039994238792813344
kevin009/flyingllama-v2,hendrycksTest-econometrics,5-shot,acc_norm,0.23684210526315788,0.039994238792813344
kevin009/flyingllama-v2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.036001056927277716
kevin009/flyingllama-v2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.036001056927277716
kevin009/flyingllama-v2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24867724867724866,0.022261817692400175
kevin009/flyingllama-v2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24867724867724866,0.022261817692400175
kevin009/flyingllama-v2,hendrycksTest-formal_logic,5-shot,accuracy,0.23809523809523808,0.03809523809523811
kevin009/flyingllama-v2,hendrycksTest-formal_logic,5-shot,acc_norm,0.23809523809523808,0.03809523809523811
kevin009/flyingllama-v2,hendrycksTest-global_facts,5-shot,accuracy,0.21,0.040936018074033256
kevin009/flyingllama-v2,hendrycksTest-global_facts,5-shot,acc_norm,0.21,0.040936018074033256
kevin009/flyingllama-v2,hendrycksTest-high_school_biology,5-shot,accuracy,0.2903225806451613,0.025822106119415888
kevin009/flyingllama-v2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2903225806451613,0.025822106119415888
kevin009/flyingllama-v2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.27586206896551724,0.03144712581678241
kevin009/flyingllama-v2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.27586206896551724,0.03144712581678241
kevin009/flyingllama-v2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.3,0.046056618647183814
kevin009/flyingllama-v2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.3,0.046056618647183814
kevin009/flyingllama-v2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23030303030303031,0.03287666758603488
kevin009/flyingllama-v2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23030303030303031,0.03287666758603488
kevin009/flyingllama-v2,hendrycksTest-high_school_geography,5-shot,accuracy,0.3484848484848485,0.033948539651564025
kevin009/flyingllama-v2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3484848484848485,0.033948539651564025
kevin009/flyingllama-v2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.33678756476683935,0.034107802518361825
kevin009/flyingllama-v2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.33678756476683935,0.034107802518361825
kevin009/flyingllama-v2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.33589743589743587,0.023946724741563976
kevin009/flyingllama-v2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.33589743589743587,0.023946724741563976
kevin009/flyingllama-v2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.27037037037037037,0.027080372815145668
kevin009/flyingllama-v2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.27037037037037037,0.027080372815145668
kevin009/flyingllama-v2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.02684151432295895
kevin009/flyingllama-v2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2184873949579832,0.02684151432295895
kevin009/flyingllama-v2,hendrycksTest-high_school_physics,5-shot,accuracy,0.33112582781456956,0.038425817186598696
kevin009/flyingllama-v2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.33112582781456956,0.038425817186598696
kevin009/flyingllama-v2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.29724770642201837,0.019595707224643544
kevin009/flyingllama-v2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.29724770642201837,0.019595707224643544
kevin009/flyingllama-v2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
kevin009/flyingllama-v2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
kevin009/flyingllama-v2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.23039215686274508,0.029554292605695053
kevin009/flyingllama-v2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.23039215686274508,0.029554292605695053
kevin009/flyingllama-v2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2489451476793249,0.028146970599422644
kevin009/flyingllama-v2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2489451476793249,0.028146970599422644
kevin009/flyingllama-v2,hendrycksTest-human_aging,5-shot,accuracy,0.16591928251121077,0.024967553196547136
kevin009/flyingllama-v2,hendrycksTest-human_aging,5-shot,acc_norm,0.16591928251121077,0.024967553196547136
kevin009/flyingllama-v2,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
kevin009/flyingllama-v2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
kevin009/flyingllama-v2,hendrycksTest-international_law,5-shot,accuracy,0.32231404958677684,0.042664163633521664
kevin009/flyingllama-v2,hendrycksTest-international_law,5-shot,acc_norm,0.32231404958677684,0.042664163633521664
kevin009/flyingllama-v2,hendrycksTest-jurisprudence,5-shot,accuracy,0.26851851851851855,0.04284467968052192
kevin009/flyingllama-v2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.26851851851851855,0.04284467968052192
kevin009/flyingllama-v2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26380368098159507,0.03462419931615624
kevin009/flyingllama-v2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26380368098159507,0.03462419931615624
kevin009/flyingllama-v2,hendrycksTest-machine_learning,5-shot,accuracy,0.1875,0.0370468111477387
kevin009/flyingllama-v2,hendrycksTest-machine_learning,5-shot,acc_norm,0.1875,0.0370468111477387
kevin009/flyingllama-v2,hendrycksTest-management,5-shot,accuracy,0.17475728155339806,0.03760178006026621
kevin009/flyingllama-v2,hendrycksTest-management,5-shot,acc_norm,0.17475728155339806,0.03760178006026621
kevin009/flyingllama-v2,hendrycksTest-marketing,5-shot,accuracy,0.2222222222222222,0.027236013946196663
kevin009/flyingllama-v2,hendrycksTest-marketing,5-shot,acc_norm,0.2222222222222222,0.027236013946196663
kevin009/flyingllama-v2,hendrycksTest-medical_genetics,5-shot,accuracy,0.22,0.041633319989322695
kevin009/flyingllama-v2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.22,0.041633319989322695
kevin009/flyingllama-v2,hendrycksTest-miscellaneous,5-shot,accuracy,0.2656449553001277,0.015794302487888726
kevin009/flyingllama-v2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2656449553001277,0.015794302487888726
kevin009/flyingllama-v2,hendrycksTest-moral_disputes,5-shot,accuracy,0.23121387283236994,0.022698657167855716
kevin009/flyingllama-v2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.23121387283236994,0.022698657167855716
kevin009/flyingllama-v2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24692737430167597,0.014422292204808835
kevin009/flyingllama-v2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24692737430167597,0.014422292204808835
kevin009/flyingllama-v2,hendrycksTest-nutrition,5-shot,accuracy,0.2777777777777778,0.02564686309713791
kevin009/flyingllama-v2,hendrycksTest-nutrition,5-shot,acc_norm,0.2777777777777778,0.02564686309713791
kevin009/flyingllama-v2,hendrycksTest-philosophy,5-shot,accuracy,0.19614147909967847,0.022552447780478026
kevin009/flyingllama-v2,hendrycksTest-philosophy,5-shot,acc_norm,0.19614147909967847,0.022552447780478026
kevin009/flyingllama-v2,hendrycksTest-prehistory,5-shot,accuracy,0.2222222222222222,0.023132376234543346
kevin009/flyingllama-v2,hendrycksTest-prehistory,5-shot,acc_norm,0.2222222222222222,0.023132376234543346
kevin009/flyingllama-v2,hendrycksTest-professional_accounting,5-shot,accuracy,0.24822695035460993,0.02577001564429039
kevin009/flyingllama-v2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24822695035460993,0.02577001564429039
kevin009/flyingllama-v2,hendrycksTest-professional_law,5-shot,accuracy,0.27249022164276404,0.011371658294311525
kevin009/flyingllama-v2,hendrycksTest-professional_law,5-shot,acc_norm,0.27249022164276404,0.011371658294311525
kevin009/flyingllama-v2,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121593
kevin009/flyingllama-v2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.030211479609121593
kevin009/flyingllama-v2,hendrycksTest-professional_psychology,5-shot,accuracy,0.23039215686274508,0.017035229258034044
kevin009/flyingllama-v2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.23039215686274508,0.017035229258034044
kevin009/flyingllama-v2,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.041723430387053825
kevin009/flyingllama-v2,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.041723430387053825
kevin009/flyingllama-v2,hendrycksTest-security_studies,5-shot,accuracy,0.32653061224489793,0.030021056238440317
kevin009/flyingllama-v2,hendrycksTest-security_studies,5-shot,acc_norm,0.32653061224489793,0.030021056238440317
kevin009/flyingllama-v2,hendrycksTest-sociology,5-shot,accuracy,0.27860696517412936,0.031700561834973086
kevin009/flyingllama-v2,hendrycksTest-sociology,5-shot,acc_norm,0.27860696517412936,0.031700561834973086
kevin009/flyingllama-v2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.045604802157206845
kevin009/flyingllama-v2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.045604802157206845
kevin009/flyingllama-v2,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.0362933532994786
kevin009/flyingllama-v2,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.0362933532994786
kevin009/flyingllama-v2,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.034462962170884265
kevin009/flyingllama-v2,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.034462962170884265
kevin009/flyingllama-v2,truthfulqa:mc,0-shot,mc1,0.24357405140758873,0.015026354824910782
kevin009/flyingllama-v2,truthfulqa:mc,0-shot,mc2,0.41299297017962017,0.014938905945440792
kevin009/flyingllama-v2,winogrande,5-shot,accuracy,0.5027624309392266,0.014052271211616438
kevin009/flyingllama-v2,gsm8k,5-shot,accuracy,0.0,
kevin009/flyingllama-v2,anli_r2,0-shot,brier_score,1.1735330591237014,
kevin009/flyingllama-v2,anli_r3,0-shot,brier_score,1.1385554531177986,
kevin009/flyingllama-v2,anli_r1,0-shot,brier_score,1.19478847252206,
kevin009/flyingllama-v2,xnli_eu,0-shot,brier_score,1.2452653292406854,
kevin009/flyingllama-v2,xnli_vi,0-shot,brier_score,1.3055660713550172,
kevin009/flyingllama-v2,xnli_ru,0-shot,brier_score,0.8653403177730274,
kevin009/flyingllama-v2,xnli_zh,0-shot,brier_score,1.2545233907564624,
kevin009/flyingllama-v2,xnli_tr,0-shot,brier_score,0.9473726872904683,
kevin009/flyingllama-v2,xnli_fr,0-shot,brier_score,0.8657109098182422,
kevin009/flyingllama-v2,xnli_en,0-shot,brier_score,0.7220869891227981,
kevin009/flyingllama-v2,xnli_ur,0-shot,brier_score,1.2730658722542465,
kevin009/flyingllama-v2,xnli_ar,0-shot,brier_score,1.0108144142280742,
kevin009/flyingllama-v2,xnli_de,0-shot,brier_score,0.8540767754991446,
kevin009/flyingllama-v2,xnli_hi,0-shot,brier_score,1.012100384865812,
kevin009/flyingllama-v2,xnli_es,0-shot,brier_score,0.9670230383724598,
kevin009/flyingllama-v2,xnli_bg,0-shot,brier_score,0.9439847791011455,
kevin009/flyingllama-v2,xnli_sw,0-shot,brier_score,1.0034169805409079,
kevin009/flyingllama-v2,xnli_el,0-shot,brier_score,1.107850707104091,
kevin009/flyingllama-v2,xnli_th,0-shot,brier_score,1.1658594816739027,
kevin009/flyingllama-v2,logiqa2,0-shot,brier_score,1.1554316673601366,
kevin009/flyingllama-v2,mathqa,5-shot,brier_score,1.0239110371606883,
kevin009/flyingllama-v2,lambada_standard,0-shot,perplexity,62.83149180042299,2.838643274776153
kevin009/flyingllama-v2,lambada_standard,0-shot,accuracy,0.3114690471569959,0.006451805320261249
kevin009/flyingllama-v2,lambada_openai,0-shot,perplexity,19.64791044748978,0.7536882287369607
kevin009/flyingllama-v2,lambada_openai,0-shot,accuracy,0.42577139530370656,0.006888786490936493
EleutherAI/pythia-70m-deduped,drop,3-shot,accuracy,0.0012583892617449664,0.0003630560893119184
EleutherAI/pythia-70m-deduped,drop,3-shot,f1,0.023000209731543642,0.0009427318515971101
EleutherAI/pythia-70m-deduped,gsm8k,5-shot,accuracy,0.009097801364670205,0.002615326510775671
EleutherAI/pythia-70m-deduped,winogrande,5-shot,accuracy,0.4925019731649566,0.014050905521228573
EleutherAI/pythia-70m-deduped,arc:challenge,25-shot,accuracy,0.18344709897610922,0.011310170179554536
EleutherAI/pythia-70m-deduped,arc:challenge,25-shot,acc_norm,0.21075085324232082,0.011918271754852197
EleutherAI/pythia-70m-deduped,hellaswag,10-shot,accuracy,0.26857199761003786,0.004423109313298973
EleutherAI/pythia-70m-deduped,hellaswag,10-shot,acc_norm,0.27504481179047996,0.004456242601950572
EleutherAI/pythia-70m-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.24,0.042923469599092816
EleutherAI/pythia-70m-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.24,0.042923469599092816
EleutherAI/pythia-70m-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.35555555555555557,0.04135176749720385
EleutherAI/pythia-70m-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.35555555555555557,0.04135176749720385
EleutherAI/pythia-70m-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.18421052631578946,0.0315469804508223
EleutherAI/pythia-70m-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.18421052631578946,0.0315469804508223
EleutherAI/pythia-70m-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.04229525846816505
EleutherAI/pythia-70m-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.04229525846816505
EleutherAI/pythia-70m-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2339622641509434,0.026055296901152922
EleutherAI/pythia-70m-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2339622641509434,0.026055296901152922
EleutherAI/pythia-70m-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.22916666666666666,0.03514697467862388
EleutherAI/pythia-70m-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.22916666666666666,0.03514697467862388
EleutherAI/pythia-70m-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909282
EleutherAI/pythia-70m-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909282
EleutherAI/pythia-70m-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-70m-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.045126085985421276
EleutherAI/pythia-70m-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-70m-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.04163331998932269
EleutherAI/pythia-70m-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2023121387283237,0.03063114553919882
EleutherAI/pythia-70m-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2023121387283237,0.03063114553919882
EleutherAI/pythia-70m-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.22549019607843138,0.041583075330832865
EleutherAI/pythia-70m-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.22549019607843138,0.041583075330832865
EleutherAI/pythia-70m-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-70m-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-70m-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.28936170212765955,0.02964400657700962
EleutherAI/pythia-70m-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.28936170212765955,0.02964400657700962
EleutherAI/pythia-70m-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.0409698513984367
EleutherAI/pythia-70m-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.0409698513984367
EleutherAI/pythia-70m-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.20689655172413793,0.03375672449560554
EleutherAI/pythia-70m-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.20689655172413793,0.03375672449560554
EleutherAI/pythia-70m-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25396825396825395,0.022418042891113942
EleutherAI/pythia-70m-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25396825396825395,0.022418042891113942
EleutherAI/pythia-70m-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.20634920634920634,0.036196045241242515
EleutherAI/pythia-70m-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.20634920634920634,0.036196045241242515
EleutherAI/pythia-70m-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-70m-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.3258064516129032,0.0266620105785671
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3258064516129032,0.0266620105785671
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.28078817733990147,0.0316185633535861
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.28078817733990147,0.0316185633535861
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.22,0.04163331998932269
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.24242424242424243,0.03346409881055953
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.24242424242424243,0.03346409881055953
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.02962022787479048
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.02962022787479048
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.35233160621761656,0.03447478286414359
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.35233160621761656,0.03447478286414359
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2846153846153846,0.022878322799706283
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2846153846153846,0.022878322799706283
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.026466117538959916
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.026466117538959916
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23109243697478993,0.027381406927868963
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23109243697478993,0.027381406927868963
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.03297986648473835
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.03297986648473835
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.24036697247706423,0.01832060732096407
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.24036697247706423,0.01832060732096407
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4583333333333333,0.033981108902946366
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4583333333333333,0.033981108902946366
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25980392156862747,0.03077855467869326
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25980392156862747,0.03077855467869326
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.28270042194092826,0.029312814153955934
EleutherAI/pythia-70m-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.28270042194092826,0.029312814153955934
EleutherAI/pythia-70m-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.2556053811659193,0.029275891003969927
EleutherAI/pythia-70m-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.2556053811659193,0.029275891003969927
EleutherAI/pythia-70m-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2900763358778626,0.03980066246467765
EleutherAI/pythia-70m-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2900763358778626,0.03980066246467765
EleutherAI/pythia-70m-deduped,hendrycksTest-international_law,5-shot,accuracy,0.19834710743801653,0.036401182719909456
EleutherAI/pythia-70m-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.19834710743801653,0.036401182719909456
EleutherAI/pythia-70m-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2037037037037037,0.038935425188248475
EleutherAI/pythia-70m-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2037037037037037,0.038935425188248475
EleutherAI/pythia-70m-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25153374233128833,0.03408997886857529
EleutherAI/pythia-70m-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25153374233128833,0.03408997886857529
EleutherAI/pythia-70m-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.17857142857142858,0.03635209121577806
EleutherAI/pythia-70m-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.17857142857142858,0.03635209121577806
EleutherAI/pythia-70m-deduped,hendrycksTest-management,5-shot,accuracy,0.18446601941747573,0.03840423627288276
EleutherAI/pythia-70m-deduped,hendrycksTest-management,5-shot,acc_norm,0.18446601941747573,0.03840423627288276
EleutherAI/pythia-70m-deduped,hendrycksTest-marketing,5-shot,accuracy,0.20085470085470086,0.02624677294689048
EleutherAI/pythia-70m-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.20085470085470086,0.02624677294689048
EleutherAI/pythia-70m-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.34,0.04760952285695235
EleutherAI/pythia-70m-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.34,0.04760952285695235
EleutherAI/pythia-70m-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.23754789272030652,0.01521873304615019
EleutherAI/pythia-70m-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.23754789272030652,0.01521873304615019
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2658959537572254,0.023786203255508283
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2658959537572254,0.023786203255508283
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2670391061452514,0.014796502622562555
EleutherAI/pythia-70m-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2670391061452514,0.014796502622562555
EleutherAI/pythia-70m-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.025261691219729484
EleutherAI/pythia-70m-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.025261691219729484
EleutherAI/pythia-70m-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.19614147909967847,0.022552447780478043
EleutherAI/pythia-70m-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.19614147909967847,0.022552447780478043
EleutherAI/pythia-70m-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2345679012345679,0.023576881744005716
EleutherAI/pythia-70m-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2345679012345679,0.023576881744005716
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.25886524822695034,0.026129572527180848
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25886524822695034,0.026129572527180848
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.2392438070404172,0.010896123652676655
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.2392438070404172,0.010896123652676655
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.41544117647058826,0.02993534270787775
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.41544117647058826,0.02993534270787775
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.24836601307189543,0.017479487001364764
EleutherAI/pythia-70m-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.24836601307189543,0.017479487001364764
EleutherAI/pythia-70m-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-70m-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-70m-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.22857142857142856,0.026882144922307748
EleutherAI/pythia-70m-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.22857142857142856,0.026882144922307748
EleutherAI/pythia-70m-deduped,hendrycksTest-sociology,5-shot,accuracy,0.24875621890547264,0.030567675938916707
EleutherAI/pythia-70m-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.24875621890547264,0.030567675938916707
EleutherAI/pythia-70m-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.32,0.046882617226215034
EleutherAI/pythia-70m-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.32,0.046882617226215034
EleutherAI/pythia-70m-deduped,hendrycksTest-virology,5-shot,accuracy,0.1927710843373494,0.030709824050565274
EleutherAI/pythia-70m-deduped,hendrycksTest-virology,5-shot,acc_norm,0.1927710843373494,0.030709824050565274
EleutherAI/pythia-70m-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.30994152046783624,0.03546976959393163
EleutherAI/pythia-70m-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.30994152046783624,0.03546976959393163
EleutherAI/pythia-70m-deduped,truthfulqa:mc,0-shot,mc1,0.2521419828641371,0.015201522246299962
EleutherAI/pythia-70m-deduped,truthfulqa:mc,0-shot,mc2,0.47514385475605914,0.01572213315211734
EleutherAI/pythia-70m-deduped,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.035825294425731215
EleutherAI/pythia-70m-deduped,mmlu_formal_logic,0-shot,accuracy,0.21428571428571427,0.03670066451047182
EleutherAI/pythia-70m-deduped,mmlu_prehistory,0-shot,accuracy,0.25308641975308643,0.024191808600713
EleutherAI/pythia-70m-deduped,mmlu_moral_scenarios,0-shot,accuracy,0.2581005586592179,0.014635185616527832
EleutherAI/pythia-70m-deduped,mmlu_high_school_world_history,0-shot,accuracy,0.29535864978902954,0.02969633871342289
EleutherAI/pythia-70m-deduped,mmlu_moral_disputes,0-shot,accuracy,0.26011560693641617,0.023618678310069363
EleutherAI/pythia-70m-deduped,mmlu_professional_law,0-shot,accuracy,0.23663624511082137,0.010855137351572739
EleutherAI/pythia-70m-deduped,mmlu_logical_fallacies,0-shot,accuracy,0.27607361963190186,0.0351238528370505
EleutherAI/pythia-70m-deduped,mmlu_high_school_us_history,0-shot,accuracy,0.29411764705882354,0.03198001660115072
EleutherAI/pythia-70m-deduped,mmlu_philosophy,0-shot,accuracy,0.19292604501607716,0.022411516780911363
EleutherAI/pythia-70m-deduped,mmlu_jurisprudence,0-shot,accuracy,0.21296296296296297,0.0395783547198098
EleutherAI/pythia-70m-deduped,mmlu_international_law,0-shot,accuracy,0.256198347107438,0.03984979653302871
EleutherAI/pythia-70m-deduped,mmlu_high_school_european_history,0-shot,accuracy,0.2606060606060606,0.034277431758165236
EleutherAI/pythia-70m-deduped,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.034697137917043715
EleutherAI/pythia-70m-deduped,mmlu_high_school_microeconomics,0-shot,accuracy,0.22268907563025211,0.027025433498882367
EleutherAI/pythia-70m-deduped,mmlu_high_school_geography,0-shot,accuracy,0.20202020202020202,0.028606204289229865
EleutherAI/pythia-70m-deduped,mmlu_high_school_psychology,0-shot,accuracy,0.23486238532110093,0.01817511051034359
EleutherAI/pythia-70m-deduped,mmlu_public_relations,0-shot,accuracy,0.20909090909090908,0.03895091015724137
EleutherAI/pythia-70m-deduped,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-70m-deduped,mmlu_sociology,0-shot,accuracy,0.24875621890547264,0.030567675938916718
EleutherAI/pythia-70m-deduped,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2948717948717949,0.023119362758232277
EleutherAI/pythia-70m-deduped,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.02635891633490401
EleutherAI/pythia-70m-deduped,mmlu_professional_psychology,0-shot,accuracy,0.24836601307189543,0.017479487001364764
EleutherAI/pythia-70m-deduped,mmlu_human_sexuality,0-shot,accuracy,0.2900763358778626,0.03980066246467765
EleutherAI/pythia-70m-deduped,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.04185774424022056
EleutherAI/pythia-70m-deduped,mmlu_miscellaneous,0-shot,accuracy,0.24393358876117496,0.015357212665829489
EleutherAI/pythia-70m-deduped,mmlu_marketing,0-shot,accuracy,0.20085470085470086,0.02624677294689048
EleutherAI/pythia-70m-deduped,mmlu_management,0-shot,accuracy,0.1941747572815534,0.03916667762822584
EleutherAI/pythia-70m-deduped,mmlu_nutrition,0-shot,accuracy,0.2875816993464052,0.02591780611714716
EleutherAI/pythia-70m-deduped,mmlu_medical_genetics,0-shot,accuracy,0.33,0.047258156262526045
EleutherAI/pythia-70m-deduped,mmlu_human_aging,0-shot,accuracy,0.2600896860986547,0.029442495585857487
EleutherAI/pythia-70m-deduped,mmlu_professional_medicine,0-shot,accuracy,0.41911764705882354,0.02997280717046463
EleutherAI/pythia-70m-deduped,mmlu_college_medicine,0-shot,accuracy,0.19653179190751446,0.030299574664788137
EleutherAI/pythia-70m-deduped,mmlu_business_ethics,0-shot,accuracy,0.2,0.04020151261036845
EleutherAI/pythia-70m-deduped,mmlu_clinical_knowledge,0-shot,accuracy,0.2528301886792453,0.026749899771241235
EleutherAI/pythia-70m-deduped,mmlu_global_facts,0-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-70m-deduped,mmlu_virology,0-shot,accuracy,0.1746987951807229,0.029560326211256833
EleutherAI/pythia-70m-deduped,mmlu_professional_accounting,0-shot,accuracy,0.24468085106382978,0.025645553622266733
EleutherAI/pythia-70m-deduped,mmlu_college_physics,0-shot,accuracy,0.23529411764705882,0.04220773659171452
EleutherAI/pythia-70m-deduped,mmlu_high_school_physics,0-shot,accuracy,0.17880794701986755,0.03128744850600724
EleutherAI/pythia-70m-deduped,mmlu_high_school_biology,0-shot,accuracy,0.3161290322580645,0.02645087448904277
EleutherAI/pythia-70m-deduped,mmlu_college_biology,0-shot,accuracy,0.2152777777777778,0.034370793441061344
EleutherAI/pythia-70m-deduped,mmlu_anatomy,0-shot,accuracy,0.32592592592592595,0.040491220417025055
EleutherAI/pythia-70m-deduped,mmlu_college_chemistry,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-70m-deduped,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-70m-deduped,mmlu_college_computer_science,0-shot,accuracy,0.32,0.046882617226215034
EleutherAI/pythia-70m-deduped,mmlu_astronomy,0-shot,accuracy,0.19078947368421054,0.031975658210325
EleutherAI/pythia-70m-deduped,mmlu_college_mathematics,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-70m-deduped,mmlu_conceptual_physics,0-shot,accuracy,0.28936170212765955,0.02964400657700962
EleutherAI/pythia-70m-deduped,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-70m-deduped,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.04408440022768078
EleutherAI/pythia-70m-deduped,mmlu_machine_learning,0-shot,accuracy,0.22321428571428573,0.039523019677025116
EleutherAI/pythia-70m-deduped,mmlu_high_school_chemistry,0-shot,accuracy,0.27586206896551724,0.03144712581678241
EleutherAI/pythia-70m-deduped,mmlu_high_school_statistics,0-shot,accuracy,0.46296296296296297,0.03400603625538271
EleutherAI/pythia-70m-deduped,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.022494510767503154
EleutherAI/pythia-70m-deduped,mmlu_electrical_engineering,0-shot,accuracy,0.20689655172413793,0.03375672449560553
EleutherAI/pythia-70m-deduped,mmlu_high_school_mathematics,0-shot,accuracy,0.22592592592592592,0.025497532639609553
EleutherAI/pythia-70m-deduped,arc_challenge,25-shot,accuracy,0.1697952218430034,0.010971775157784204
EleutherAI/pythia-70m-deduped,arc_challenge,25-shot,acc_norm,0.21331058020477817,0.011970971742326334
EleutherAI/pythia-70m-deduped,truthfulqa_mc2,0-shot,accuracy,0.4767651653188276,0.015673835636167466
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,bleu_max,46.031535492951335,0.8673187818828474
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,bleu_acc,0.795593635250918,0.014117174337432615
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,bleu_diff,39.793171656866846,0.9094412465216882
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge1_max,69.81726690246458,1.1081734621228823
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge1_acc,0.799265605875153,0.01402204571748216
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge1_diff,56.95442784511964,1.3527087958065618
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge2_max,63.72306529658852,1.2425333117054245
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge2_acc,0.7552019583843329,0.015051869486714985
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rouge2_diff,60.61075595094692,1.3860618275238243
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rougeL_max,69.31439415404608,1.1276933183097786
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rougeL_acc,0.8102815177478581,0.013725485265185087
EleutherAI/pythia-70m-deduped,truthfulqa_gen,0-shot,rougeL_diff,57.784969619564976,1.3395186035921138
EleutherAI/pythia-70m-deduped,truthfulqa_mc1,0-shot,accuracy,0.2582619339045288,0.015321821688476183
Salesforce/codegen-16B-multi,minerva_math_precalc,5-shot,accuracy,0.0347985347985348,0.00785038966769786
Salesforce/codegen-16B-multi,minerva_math_prealgebra,5-shot,accuracy,0.03788748564867968,0.006472934284600699
Salesforce/codegen-16B-multi,minerva_math_num_theory,5-shot,accuracy,0.022222222222222223,0.006349206349206319
Salesforce/codegen-16B-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.024363233665559248,0.005133437461128035
Salesforce/codegen-16B-multi,minerva_math_geometry,5-shot,accuracy,0.022964509394572025,0.006851249878769252
Salesforce/codegen-16B-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.016877637130801686,0.0059228268948526815
Salesforce/codegen-16B-multi,minerva_math_algebra,5-shot,accuracy,0.02695871946082561,0.00470297768200653
Salesforce/codegen-16B-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,arithmetic_3da,5-shot,accuracy,0.022,0.0032807593162018905
Salesforce/codegen-16B-multi,arithmetic_3ds,5-shot,accuracy,0.0145,0.0026736583971427824
Salesforce/codegen-16B-multi,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521479
Salesforce/codegen-16B-multi,arithmetic_2ds,5-shot,accuracy,0.166,0.008322056735817098
Salesforce/codegen-16B-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-16B-multi,arithmetic_1dc,5-shot,accuracy,0.147,0.007920029256998883
Salesforce/codegen-16B-multi,arithmetic_4ds,5-shot,accuracy,0.001,0.0007069298939339487
Salesforce/codegen-16B-multi,arithmetic_2dm,5-shot,accuracy,0.0975,0.006634672896399609
Salesforce/codegen-16B-multi,arithmetic_2da,5-shot,accuracy,0.1715,0.008430860852092992
Salesforce/codegen-16B-multi,gsm8k_cot,5-shot,accuracy,0.03184230477634572,0.004836348558260953
Salesforce/codegen-16B-multi,gsm8k,5-shot,accuracy,0.03411675511751327,0.0050002126007732545
Salesforce/codegen-16B-multi,anli_r2,0-shot,brier_score,0.7344945652955857,
Salesforce/codegen-16B-multi,anli_r3,0-shot,brier_score,0.7512937712393907,
Salesforce/codegen-16B-multi,anli_r1,0-shot,brier_score,0.7409617387492144,
Salesforce/codegen-16B-multi,xnli_eu,0-shot,brier_score,1.0499948811805855,
Salesforce/codegen-16B-multi,xnli_vi,0-shot,brier_score,1.0494397425378459,
Salesforce/codegen-16B-multi,xnli_ru,0-shot,brier_score,0.9528824583761204,
Salesforce/codegen-16B-multi,xnli_zh,0-shot,brier_score,1.046794144815665,
Salesforce/codegen-16B-multi,xnli_tr,0-shot,brier_score,0.9505776051805006,
Salesforce/codegen-16B-multi,xnli_fr,0-shot,brier_score,0.8746123551516378,
Salesforce/codegen-16B-multi,xnli_en,0-shot,brier_score,0.7168910989376758,
Salesforce/codegen-16B-multi,xnli_ur,0-shot,brier_score,1.2504543392394984,
Salesforce/codegen-16B-multi,xnli_ar,0-shot,brier_score,1.0299487744752638,
Salesforce/codegen-16B-multi,xnli_de,0-shot,brier_score,0.9175188876319201,
Salesforce/codegen-16B-multi,xnli_hi,0-shot,brier_score,1.0205470500057452,
Salesforce/codegen-16B-multi,xnli_es,0-shot,brier_score,0.9198569977209587,
Salesforce/codegen-16B-multi,xnli_bg,0-shot,brier_score,0.8427184705601842,
Salesforce/codegen-16B-multi,xnli_sw,0-shot,brier_score,0.9401072122195173,
Salesforce/codegen-16B-multi,xnli_el,0-shot,brier_score,0.9761407972947893,
Salesforce/codegen-16B-multi,xnli_th,0-shot,brier_score,0.8572219481864177,
Salesforce/codegen-16B-multi,logiqa2,0-shot,brier_score,1.1546592524731565,
Salesforce/codegen-16B-multi,mathqa,5-shot,brier_score,0.953439313800392,
Salesforce/codegen-16B-multi,lambada_standard,0-shot,perplexity,25.851477547441906,0.9461878766495387
Salesforce/codegen-16B-multi,lambada_standard,0-shot,accuracy,0.3512516980399767,0.006650578225573468
Salesforce/codegen-16B-multi,lambada_openai,0-shot,perplexity,15.404938907332895,0.5077743811614407
Salesforce/codegen-16B-multi,lambada_openai,0-shot,accuracy,0.4255773335920823,0.006888380073136036
Salesforce/codegen-16B-multi,mmlu_world_religions,0-shot,accuracy,0.32748538011695905,0.035993357714560276
Salesforce/codegen-16B-multi,mmlu_formal_logic,0-shot,accuracy,0.30158730158730157,0.04104947269903394
Salesforce/codegen-16B-multi,mmlu_prehistory,0-shot,accuracy,0.30864197530864196,0.025702640260603756
Salesforce/codegen-16B-multi,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.014422292204808864
Salesforce/codegen-16B-multi,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
Salesforce/codegen-16B-multi,mmlu_moral_disputes,0-shot,accuracy,0.2254335260115607,0.022497230190967547
Salesforce/codegen-16B-multi,mmlu_professional_law,0-shot,accuracy,0.2588005215123859,0.011186109046564613
Salesforce/codegen-16B-multi,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.033220157957767414
Salesforce/codegen-16B-multi,mmlu_high_school_us_history,0-shot,accuracy,0.27450980392156865,0.031321798030832904
Salesforce/codegen-16B-multi,mmlu_philosophy,0-shot,accuracy,0.2733118971061093,0.025311765975426115
Salesforce/codegen-16B-multi,mmlu_jurisprudence,0-shot,accuracy,0.19444444444444445,0.03826076324884863
Salesforce/codegen-16B-multi,mmlu_international_law,0-shot,accuracy,0.2727272727272727,0.04065578140908705
Salesforce/codegen-16B-multi,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.03546563019624336
Salesforce/codegen-16B-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.25906735751295334,0.031618779179354135
Salesforce/codegen-16B-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.29411764705882354,0.02959732973097809
Salesforce/codegen-16B-multi,mmlu_high_school_geography,0-shot,accuracy,0.24242424242424243,0.030532892233932026
Salesforce/codegen-16B-multi,mmlu_high_school_psychology,0-shot,accuracy,0.21834862385321102,0.017712600528722717
Salesforce/codegen-16B-multi,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
Salesforce/codegen-16B-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.33,0.04725815626252604
Salesforce/codegen-16B-multi,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296024
Salesforce/codegen-16B-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.26153846153846155,0.022282141204204423
Salesforce/codegen-16B-multi,mmlu_security_studies,0-shot,accuracy,0.3795918367346939,0.031067211262872478
Salesforce/codegen-16B-multi,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
Salesforce/codegen-16B-multi,mmlu_human_sexuality,0-shot,accuracy,0.33587786259541985,0.04142313771996665
Salesforce/codegen-16B-multi,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813365
Salesforce/codegen-16B-multi,mmlu_miscellaneous,0-shot,accuracy,0.2669220945083014,0.01581845089477755
Salesforce/codegen-16B-multi,mmlu_marketing,0-shot,accuracy,0.27350427350427353,0.029202540153431177
Salesforce/codegen-16B-multi,mmlu_management,0-shot,accuracy,0.1941747572815534,0.03916667762822585
Salesforce/codegen-16B-multi,mmlu_nutrition,0-shot,accuracy,0.25163398692810457,0.024848018263875192
Salesforce/codegen-16B-multi,mmlu_medical_genetics,0-shot,accuracy,0.35,0.04793724854411018
Salesforce/codegen-16B-multi,mmlu_human_aging,0-shot,accuracy,0.3183856502242152,0.03126580522513713
Salesforce/codegen-16B-multi,mmlu_professional_medicine,0-shot,accuracy,0.2610294117647059,0.02667925227010312
Salesforce/codegen-16B-multi,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.03186209851641143
Salesforce/codegen-16B-multi,mmlu_business_ethics,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-16B-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.24528301886792453,0.02648035717989571
Salesforce/codegen-16B-multi,mmlu_global_facts,0-shot,accuracy,0.18,0.03861229196653694
Salesforce/codegen-16B-multi,mmlu_virology,0-shot,accuracy,0.26506024096385544,0.03436024037944967
Salesforce/codegen-16B-multi,mmlu_professional_accounting,0-shot,accuracy,0.1879432624113475,0.023305230769714243
Salesforce/codegen-16B-multi,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.041583075330832865
Salesforce/codegen-16B-multi,mmlu_high_school_physics,0-shot,accuracy,0.33774834437086093,0.0386155754625517
Salesforce/codegen-16B-multi,mmlu_high_school_biology,0-shot,accuracy,0.3032258064516129,0.026148685930671753
Salesforce/codegen-16B-multi,mmlu_college_biology,0-shot,accuracy,0.3055555555555556,0.03852084696008534
Salesforce/codegen-16B-multi,mmlu_anatomy,0-shot,accuracy,0.28888888888888886,0.0391545063041425
Salesforce/codegen-16B-multi,mmlu_college_chemistry,0-shot,accuracy,0.39,0.04902071300001975
Salesforce/codegen-16B-multi,mmlu_computer_security,0-shot,accuracy,0.33,0.04725815626252605
Salesforce/codegen-16B-multi,mmlu_college_computer_science,0-shot,accuracy,0.24,0.04292346959909283
Salesforce/codegen-16B-multi,mmlu_astronomy,0-shot,accuracy,0.27631578947368424,0.03639057569952925
Salesforce/codegen-16B-multi,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215034
Salesforce/codegen-16B-multi,mmlu_conceptual_physics,0-shot,accuracy,0.2680851063829787,0.028957342788342347
Salesforce/codegen-16B-multi,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.04229525846816505
Salesforce/codegen-16B-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.046056618647183814
Salesforce/codegen-16B-multi,mmlu_machine_learning,0-shot,accuracy,0.2767857142857143,0.04246624336697626
Salesforce/codegen-16B-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.030516530732694436
Salesforce/codegen-16B-multi,mmlu_high_school_statistics,0-shot,accuracy,0.375,0.033016908987210894
Salesforce/codegen-16B-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.26455026455026454,0.02271746789770862
Salesforce/codegen-16B-multi,mmlu_electrical_engineering,0-shot,accuracy,0.3310344827586207,0.039215453124671215
Salesforce/codegen-16B-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.026842057873833706
Salesforce/codegen-16B-multi,arc_challenge,25-shot,accuracy,0.28498293515358364,0.013191348179838793
Salesforce/codegen-16B-multi,arc_challenge,25-shot,acc_norm,0.3370307167235495,0.01381347665290228
Salesforce/codegen-16B-multi,hellaswag,10-shot,accuracy,0.39364668392750446,0.004875595792850675
Salesforce/codegen-16B-multi,hellaswag,10-shot,acc_norm,0.5127464648476399,0.004988159744742506
Salesforce/codegen-16B-multi,truthfulqa_mc2,0-shot,accuracy,0.43278625644567936,0.01474151113675408
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,bleu_max,17.563201921535452,0.6084253447461239
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,bleu_acc,0.3769889840881273,0.016965517578930358
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,bleu_diff,-2.9441042316118144,0.6521697109915429
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge1_max,40.112597617421514,0.8426548513555856
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge1_acc,0.3353733170134639,0.01652753403966899
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge1_diff,-5.161390138276828,0.9272668238645785
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge2_max,21.567769834989484,0.9243570922115246
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge2_acc,0.1909424724602203,0.013759285842685714
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rouge2_diff,-7.056555663275975,0.9635864851509317
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rougeL_max,37.40770287988807,0.8348939987940525
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rougeL_acc,0.32068543451652387,0.016339170373280917
Salesforce/codegen-16B-multi,truthfulqa_gen,0-shot,rougeL_diff,-5.184697052173686,0.9244121757045393
Salesforce/codegen-16B-multi,truthfulqa_mc1,0-shot,accuracy,0.25703794369645044,0.015298077509485083
Salesforce/codegen-16B-multi,winogrande,5-shot,accuracy,0.55327545382794,0.013972488371616687
facebook/opt-6.7b,arc:challenge,25-shot,accuracy,0.34726962457337884,0.013913034529620434
facebook/opt-6.7b,arc:challenge,25-shot,acc_norm,0.3916382252559727,0.014264122124938215
facebook/opt-6.7b,hellaswag,10-shot,accuracy,0.5072694682334197,0.004989254011895759
facebook/opt-6.7b,hellaswag,10-shot,acc_norm,0.6890061740689106,0.004619542392006366
facebook/opt-6.7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
facebook/opt-6.7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.04351941398892446
facebook/opt-6.7b,hendrycksTest-anatomy,5-shot,accuracy,0.34814814814814815,0.041153246103369526
facebook/opt-6.7b,hendrycksTest-anatomy,5-shot,acc_norm,0.34814814814814815,0.041153246103369526
facebook/opt-6.7b,hendrycksTest-astronomy,5-shot,accuracy,0.14473684210526316,0.028631951845930367
facebook/opt-6.7b,hendrycksTest-astronomy,5-shot,acc_norm,0.14473684210526316,0.028631951845930367
facebook/opt-6.7b,hendrycksTest-business_ethics,5-shot,accuracy,0.16,0.03684529491774707
facebook/opt-6.7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.16,0.03684529491774707
facebook/opt-6.7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2037735849056604,0.0247907845017754
facebook/opt-6.7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2037735849056604,0.0247907845017754
facebook/opt-6.7b,hendrycksTest-college_biology,5-shot,accuracy,0.22916666666666666,0.035146974678623884
facebook/opt-6.7b,hendrycksTest-college_biology,5-shot,acc_norm,0.22916666666666666,0.035146974678623884
facebook/opt-6.7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909282
facebook/opt-6.7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909282
facebook/opt-6.7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.04688261722621504
facebook/opt-6.7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.04688261722621504
facebook/opt-6.7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.0446196043338474
facebook/opt-6.7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.0446196043338474
facebook/opt-6.7b,hendrycksTest-college_medicine,5-shot,accuracy,0.23121387283236994,0.032147373020294696
facebook/opt-6.7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.23121387283236994,0.032147373020294696
facebook/opt-6.7b,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617749
facebook/opt-6.7b,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617749
facebook/opt-6.7b,hendrycksTest-computer_security,5-shot,accuracy,0.24,0.042923469599092816
facebook/opt-6.7b,hendrycksTest-computer_security,5-shot,acc_norm,0.24,0.042923469599092816
facebook/opt-6.7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.028957342788342347
facebook/opt-6.7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.028957342788342347
facebook/opt-6.7b,hendrycksTest-econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
facebook/opt-6.7b,hendrycksTest-econometrics,5-shot,acc_norm,0.22807017543859648,0.03947152782669415
facebook/opt-6.7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.27586206896551724,0.037245636197746325
facebook/opt-6.7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.27586206896551724,0.037245636197746325
facebook/opt-6.7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.21428571428571427,0.021132859182754447
facebook/opt-6.7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.21428571428571427,0.021132859182754447
facebook/opt-6.7b,hendrycksTest-formal_logic,5-shot,accuracy,0.16666666666666666,0.03333333333333337
facebook/opt-6.7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.16666666666666666,0.03333333333333337
facebook/opt-6.7b,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.046056618647183814
facebook/opt-6.7b,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.046056618647183814
facebook/opt-6.7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.22258064516129034,0.023664216671642525
facebook/opt-6.7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.22258064516129034,0.023664216671642525
facebook/opt-6.7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.24630541871921183,0.03031509928561774
facebook/opt-6.7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.24630541871921183,0.03031509928561774
facebook/opt-6.7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.23,0.04229525846816506
facebook/opt-6.7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.23,0.04229525846816506
facebook/opt-6.7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23636363636363636,0.033175059300091805
facebook/opt-6.7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23636363636363636,0.033175059300091805
facebook/opt-6.7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.19696969696969696,0.028335609732463348
facebook/opt-6.7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.19696969696969696,0.028335609732463348
facebook/opt-6.7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.24352331606217617,0.03097543638684543
facebook/opt-6.7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.24352331606217617,0.03097543638684543
facebook/opt-6.7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2230769230769231,0.02110773012724399
facebook/opt-6.7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2230769230769231,0.02110773012724399
facebook/opt-6.7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.026842057873833706
facebook/opt-6.7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.026842057873833706
facebook/opt-6.7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.18487394957983194,0.025215992877954205
facebook/opt-6.7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.18487394957983194,0.025215992877954205
facebook/opt-6.7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.18543046357615894,0.03173284384294284
facebook/opt-6.7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.18543046357615894,0.03173284384294284
facebook/opt-6.7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.25871559633027524,0.018776052319619624
facebook/opt-6.7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.25871559633027524,0.018776052319619624
facebook/opt-6.7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2037037037037037,0.027467401804058024
facebook/opt-6.7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2037037037037037,0.027467401804058024
facebook/opt-6.7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.24019607843137256,0.02998373305591361
facebook/opt-6.7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.24019607843137256,0.02998373305591361
facebook/opt-6.7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2616033755274262,0.028609516716994934
facebook/opt-6.7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2616033755274262,0.028609516716994934
facebook/opt-6.7b,hendrycksTest-human_aging,5-shot,accuracy,0.3452914798206278,0.03191100192835795
facebook/opt-6.7b,hendrycksTest-human_aging,5-shot,acc_norm,0.3452914798206278,0.03191100192835795
facebook/opt-6.7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.21374045801526717,0.0359546161177469
facebook/opt-6.7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.21374045801526717,0.0359546161177469
facebook/opt-6.7b,hendrycksTest-international_law,5-shot,accuracy,0.23140495867768596,0.03849856098794089
facebook/opt-6.7b,hendrycksTest-international_law,5-shot,acc_norm,0.23140495867768596,0.03849856098794089
facebook/opt-6.7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.25925925925925924,0.04236511258094634
facebook/opt-6.7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25925925925925924,0.04236511258094634
facebook/opt-6.7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25153374233128833,0.034089978868575295
facebook/opt-6.7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25153374233128833,0.034089978868575295
facebook/opt-6.7b,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.0432704093257873
facebook/opt-6.7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.0432704093257873
facebook/opt-6.7b,hendrycksTest-management,5-shot,accuracy,0.23300970873786409,0.041858325989283164
facebook/opt-6.7b,hendrycksTest-management,5-shot,acc_norm,0.23300970873786409,0.041858325989283164
facebook/opt-6.7b,hendrycksTest-marketing,5-shot,accuracy,0.3034188034188034,0.030118210106942656
facebook/opt-6.7b,hendrycksTest-marketing,5-shot,acc_norm,0.3034188034188034,0.030118210106942656
facebook/opt-6.7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
facebook/opt-6.7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.046056618647183814
facebook/opt-6.7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.27586206896551724,0.01598281477469563
facebook/opt-6.7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.27586206896551724,0.01598281477469563
facebook/opt-6.7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.23410404624277456,0.02279711027807113
facebook/opt-6.7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.23410404624277456,0.02279711027807113
facebook/opt-6.7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
facebook/opt-6.7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
facebook/opt-6.7b,hendrycksTest-nutrition,5-shot,accuracy,0.22875816993464052,0.024051029739912258
facebook/opt-6.7b,hendrycksTest-nutrition,5-shot,acc_norm,0.22875816993464052,0.024051029739912258
facebook/opt-6.7b,hendrycksTest-philosophy,5-shot,accuracy,0.24758842443729903,0.024513879973621967
facebook/opt-6.7b,hendrycksTest-philosophy,5-shot,acc_norm,0.24758842443729903,0.024513879973621967
facebook/opt-6.7b,hendrycksTest-prehistory,5-shot,accuracy,0.2839506172839506,0.025089478523765134
facebook/opt-6.7b,hendrycksTest-prehistory,5-shot,acc_norm,0.2839506172839506,0.025089478523765134
facebook/opt-6.7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.02678917235114025
facebook/opt-6.7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.02678917235114025
facebook/opt-6.7b,hendrycksTest-professional_law,5-shot,accuracy,0.24119947848761408,0.010926496102034966
facebook/opt-6.7b,hendrycksTest-professional_law,5-shot,acc_norm,0.24119947848761408,0.010926496102034966
facebook/opt-6.7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.27205882352941174,0.027033041151681456
facebook/opt-6.7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.27205882352941174,0.027033041151681456
facebook/opt-6.7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.25326797385620914,0.017593486895366835
facebook/opt-6.7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25326797385620914,0.017593486895366835
facebook/opt-6.7b,hendrycksTest-public_relations,5-shot,accuracy,0.33636363636363636,0.04525393596302505
facebook/opt-6.7b,hendrycksTest-public_relations,5-shot,acc_norm,0.33636363636363636,0.04525393596302505
facebook/opt-6.7b,hendrycksTest-security_studies,5-shot,accuracy,0.19591836734693877,0.02540930195322568
facebook/opt-6.7b,hendrycksTest-security_studies,5-shot,acc_norm,0.19591836734693877,0.02540930195322568
facebook/opt-6.7b,hendrycksTest-sociology,5-shot,accuracy,0.22388059701492538,0.02947525023601719
facebook/opt-6.7b,hendrycksTest-sociology,5-shot,acc_norm,0.22388059701492538,0.02947525023601719
facebook/opt-6.7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.04093601807403326
facebook/opt-6.7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.04093601807403326
facebook/opt-6.7b,hendrycksTest-virology,5-shot,accuracy,0.3253012048192771,0.03647168523683227
facebook/opt-6.7b,hendrycksTest-virology,5-shot,acc_norm,0.3253012048192771,0.03647168523683227
facebook/opt-6.7b,hendrycksTest-world_religions,5-shot,accuracy,0.22807017543859648,0.03218093795602357
facebook/opt-6.7b,hendrycksTest-world_religions,5-shot,acc_norm,0.22807017543859648,0.03218093795602357
facebook/opt-6.7b,truthfulqa:mc,0-shot,mc1,0.211750305997552,0.014302068353925609
facebook/opt-6.7b,truthfulqa:mc,0-shot,mc2,0.3512214978869498,0.013507132529353398
facebook/opt-6.7b,drop,3-shot,accuracy,0.001153523489932886,0.0003476179896857095
facebook/opt-6.7b,drop,3-shot,f1,0.04860633389261755,0.0011917611903016134
facebook/opt-6.7b,gsm8k,5-shot,accuracy,0.02577710386656558,0.00436504295362182
facebook/opt-6.7b,winogrande,5-shot,accuracy,0.654301499605367,0.013366596951934375
cerebras/Cerebras-GPT-13B,arc:challenge,25-shot,accuracy,0.3378839590443686,0.013822047922283505
cerebras/Cerebras-GPT-13B,arc:challenge,25-shot,acc_norm,0.38139931740614336,0.014194389086685256
cerebras/Cerebras-GPT-13B,hellaswag,10-shot,accuracy,0.44981079466241786,0.00496457968571244
cerebras/Cerebras-GPT-13B,hellaswag,10-shot,acc_norm,0.6000796654052978,0.004888805003103053
cerebras/Cerebras-GPT-13B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.042295258468165065
cerebras/Cerebras-GPT-13B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.042295258468165065
cerebras/Cerebras-GPT-13B,hendrycksTest-anatomy,5-shot,accuracy,0.25925925925925924,0.03785714465066656
cerebras/Cerebras-GPT-13B,hendrycksTest-anatomy,5-shot,acc_norm,0.25925925925925924,0.03785714465066656
cerebras/Cerebras-GPT-13B,hendrycksTest-astronomy,5-shot,accuracy,0.21710526315789475,0.03355045304882923
cerebras/Cerebras-GPT-13B,hendrycksTest-astronomy,5-shot,acc_norm,0.21710526315789475,0.03355045304882923
cerebras/Cerebras-GPT-13B,hendrycksTest-business_ethics,5-shot,accuracy,0.19,0.03942772444036624
cerebras/Cerebras-GPT-13B,hendrycksTest-business_ethics,5-shot,acc_norm,0.19,0.03942772444036624
cerebras/Cerebras-GPT-13B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2528301886792453,0.026749899771241235
cerebras/Cerebras-GPT-13B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2528301886792453,0.026749899771241235
cerebras/Cerebras-GPT-13B,hendrycksTest-college_biology,5-shot,accuracy,0.2916666666666667,0.03800968060554859
cerebras/Cerebras-GPT-13B,hendrycksTest-college_biology,5-shot,acc_norm,0.2916666666666667,0.03800968060554859
cerebras/Cerebras-GPT-13B,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.0416333199893227
cerebras/Cerebras-GPT-13B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.0416333199893227
cerebras/Cerebras-GPT-13B,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.045126085985421276
cerebras/Cerebras-GPT-13B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.045126085985421276
cerebras/Cerebras-GPT-13B,hendrycksTest-college_mathematics,5-shot,accuracy,0.34,0.04760952285695235
cerebras/Cerebras-GPT-13B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.34,0.04760952285695235
cerebras/Cerebras-GPT-13B,hendrycksTest-college_medicine,5-shot,accuracy,0.23121387283236994,0.0321473730202947
cerebras/Cerebras-GPT-13B,hendrycksTest-college_medicine,5-shot,acc_norm,0.23121387283236994,0.0321473730202947
cerebras/Cerebras-GPT-13B,hendrycksTest-college_physics,5-shot,accuracy,0.19607843137254902,0.03950581861179963
cerebras/Cerebras-GPT-13B,hendrycksTest-college_physics,5-shot,acc_norm,0.19607843137254902,0.03950581861179963
cerebras/Cerebras-GPT-13B,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
cerebras/Cerebras-GPT-13B,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
cerebras/Cerebras-GPT-13B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2553191489361702,0.028504856470514175
cerebras/Cerebras-GPT-13B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2553191489361702,0.028504856470514175
cerebras/Cerebras-GPT-13B,hendrycksTest-econometrics,5-shot,accuracy,0.23684210526315788,0.03999423879281336
cerebras/Cerebras-GPT-13B,hendrycksTest-econometrics,5-shot,acc_norm,0.23684210526315788,0.03999423879281336
cerebras/Cerebras-GPT-13B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.03600105692727771
cerebras/Cerebras-GPT-13B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.03600105692727771
cerebras/Cerebras-GPT-13B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24338624338624337,0.022101128787415422
cerebras/Cerebras-GPT-13B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24338624338624337,0.022101128787415422
cerebras/Cerebras-GPT-13B,hendrycksTest-formal_logic,5-shot,accuracy,0.30158730158730157,0.04104947269903394
cerebras/Cerebras-GPT-13B,hendrycksTest-formal_logic,5-shot,acc_norm,0.30158730158730157,0.04104947269903394
cerebras/Cerebras-GPT-13B,hendrycksTest-global_facts,5-shot,accuracy,0.2,0.04020151261036846
cerebras/Cerebras-GPT-13B,hendrycksTest-global_facts,5-shot,acc_norm,0.2,0.04020151261036846
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_biology,5-shot,accuracy,0.24838709677419354,0.02458002892148101
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.24838709677419354,0.02458002892148101
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.22660098522167488,0.029454863835292982
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.22660098522167488,0.029454863835292982
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.23,0.042295258468165065
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.23,0.042295258468165065
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2606060606060606,0.034277431758165236
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2606060606060606,0.034277431758165236
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_geography,5-shot,accuracy,0.23737373737373738,0.030313710538198896
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.23737373737373738,0.030313710538198896
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.19689119170984457,0.028697873971860674
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.19689119170984457,0.028697873971860674
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.31794871794871793,0.02361088430892786
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.31794871794871793,0.02361088430892786
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24444444444444444,0.026202766534652148
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24444444444444444,0.026202766534652148
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2773109243697479,0.029079374539480007
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2773109243697479,0.029079374539480007
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_physics,5-shot,accuracy,0.23178807947019867,0.034454062719870546
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23178807947019867,0.034454062719870546
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.28256880733944956,0.019304243497707152
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.28256880733944956,0.019304243497707152
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4537037037037037,0.03395322726375798
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4537037037037037,0.03395322726375798
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.030587591351604243
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.030587591351604243
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.27848101265822783,0.029178682304842548
cerebras/Cerebras-GPT-13B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.27848101265822783,0.029178682304842548
cerebras/Cerebras-GPT-13B,hendrycksTest-human_aging,5-shot,accuracy,0.25112107623318386,0.029105220833224622
cerebras/Cerebras-GPT-13B,hendrycksTest-human_aging,5-shot,acc_norm,0.25112107623318386,0.029105220833224622
cerebras/Cerebras-GPT-13B,hendrycksTest-human_sexuality,5-shot,accuracy,0.22900763358778625,0.036853466317118506
cerebras/Cerebras-GPT-13B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.22900763358778625,0.036853466317118506
cerebras/Cerebras-GPT-13B,hendrycksTest-international_law,5-shot,accuracy,0.2644628099173554,0.04026187527591205
cerebras/Cerebras-GPT-13B,hendrycksTest-international_law,5-shot,acc_norm,0.2644628099173554,0.04026187527591205
cerebras/Cerebras-GPT-13B,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.04186091791394607
cerebras/Cerebras-GPT-13B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.04186091791394607
cerebras/Cerebras-GPT-13B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.24539877300613497,0.03380939813943354
cerebras/Cerebras-GPT-13B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.24539877300613497,0.03380939813943354
cerebras/Cerebras-GPT-13B,hendrycksTest-machine_learning,5-shot,accuracy,0.2767857142857143,0.04246624336697625
cerebras/Cerebras-GPT-13B,hendrycksTest-machine_learning,5-shot,acc_norm,0.2767857142857143,0.04246624336697625
cerebras/Cerebras-GPT-13B,hendrycksTest-management,5-shot,accuracy,0.24271844660194175,0.04245022486384493
cerebras/Cerebras-GPT-13B,hendrycksTest-management,5-shot,acc_norm,0.24271844660194175,0.04245022486384493
cerebras/Cerebras-GPT-13B,hendrycksTest-marketing,5-shot,accuracy,0.25213675213675213,0.02844796547623102
cerebras/Cerebras-GPT-13B,hendrycksTest-marketing,5-shot,acc_norm,0.25213675213675213,0.02844796547623102
cerebras/Cerebras-GPT-13B,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.045126085985421276
cerebras/Cerebras-GPT-13B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.045126085985421276
cerebras/Cerebras-GPT-13B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2835249042145594,0.016117318166832272
cerebras/Cerebras-GPT-13B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2835249042145594,0.016117318166832272
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.023703099525258172
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.023703099525258172
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.27262569832402234,0.01489339173524959
cerebras/Cerebras-GPT-13B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.27262569832402234,0.01489339173524959
cerebras/Cerebras-GPT-13B,hendrycksTest-nutrition,5-shot,accuracy,0.2581699346405229,0.025058503316958157
cerebras/Cerebras-GPT-13B,hendrycksTest-nutrition,5-shot,acc_norm,0.2581699346405229,0.025058503316958157
cerebras/Cerebras-GPT-13B,hendrycksTest-philosophy,5-shot,accuracy,0.27009646302250806,0.025218040373410622
cerebras/Cerebras-GPT-13B,hendrycksTest-philosophy,5-shot,acc_norm,0.27009646302250806,0.025218040373410622
cerebras/Cerebras-GPT-13B,hendrycksTest-prehistory,5-shot,accuracy,0.24382716049382716,0.02389187954195961
cerebras/Cerebras-GPT-13B,hendrycksTest-prehistory,5-shot,acc_norm,0.24382716049382716,0.02389187954195961
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_accounting,5-shot,accuracy,0.24822695035460993,0.025770015644290385
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24822695035460993,0.025770015644290385
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_law,5-shot,accuracy,0.2516297262059974,0.011083276280441898
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_law,5-shot,acc_norm,0.2516297262059974,0.011083276280441898
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_medicine,5-shot,accuracy,0.19117647058823528,0.023886881922440362
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.19117647058823528,0.023886881922440362
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_psychology,5-shot,accuracy,0.24183006535947713,0.017322789207784326
cerebras/Cerebras-GPT-13B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.24183006535947713,0.017322789207784326
cerebras/Cerebras-GPT-13B,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.04350271442923243
cerebras/Cerebras-GPT-13B,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.04350271442923243
cerebras/Cerebras-GPT-13B,hendrycksTest-security_studies,5-shot,accuracy,0.3551020408163265,0.030635655150387638
cerebras/Cerebras-GPT-13B,hendrycksTest-security_studies,5-shot,acc_norm,0.3551020408163265,0.030635655150387638
cerebras/Cerebras-GPT-13B,hendrycksTest-sociology,5-shot,accuracy,0.25870646766169153,0.030965903123573005
cerebras/Cerebras-GPT-13B,hendrycksTest-sociology,5-shot,acc_norm,0.25870646766169153,0.030965903123573005
cerebras/Cerebras-GPT-13B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.24,0.04292346959909283
cerebras/Cerebras-GPT-13B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.24,0.04292346959909283
cerebras/Cerebras-GPT-13B,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.03591566797824662
cerebras/Cerebras-GPT-13B,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.03591566797824662
cerebras/Cerebras-GPT-13B,hendrycksTest-world_religions,5-shot,accuracy,0.30409356725146197,0.03528211258245232
cerebras/Cerebras-GPT-13B,hendrycksTest-world_religions,5-shot,acc_norm,0.30409356725146197,0.03528211258245232
cerebras/Cerebras-GPT-13B,truthfulqa:mc,0-shot,mc1,0.22766217870257038,0.01467925503211107
cerebras/Cerebras-GPT-13B,truthfulqa:mc,0-shot,mc2,0.39185464744654125,0.013884078720404066
cerebras/Cerebras-GPT-13B,drop,3-shot,accuracy,0.0003145973154362416,0.0001816137946883952
cerebras/Cerebras-GPT-13B,drop,3-shot,f1,0.043891568791946466,0.0011058022021902458
cerebras/Cerebras-GPT-13B,gsm8k,5-shot,accuracy,0.01288855193328279,0.003106901266499662
cerebras/Cerebras-GPT-13B,winogrande,5-shot,accuracy,0.5982636148382005,0.013778439266649479
facebook/opt-2.7b,drop,3-shot,accuracy,0.0010486577181208054,0.0003314581465219369
facebook/opt-2.7b,drop,3-shot,f1,0.04767407718120815,0.0011986644527763738
facebook/opt-2.7b,gsm8k,5-shot,accuracy,0.021986353297952996,0.004039162758110046
facebook/opt-2.7b,winogrande,5-shot,accuracy,0.6164167324388319,0.013666275889539017
facebook/opt-2.7b,arc:challenge,25-shot,accuracy,0.3097269624573379,0.01351205841523836
facebook/opt-2.7b,arc:challenge,25-shot,acc_norm,0.3395904436860068,0.01383903976282016
facebook/opt-2.7b,hellaswag,10-shot,accuracy,0.4598685520812587,0.004973683026202175
facebook/opt-2.7b,hellaswag,10-shot,acc_norm,0.6177056363274248,0.004849547819134493
facebook/opt-2.7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.2,0.04020151261036846
facebook/opt-2.7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.2,0.04020151261036846
facebook/opt-2.7b,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.037498507091740234
facebook/opt-2.7b,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.037498507091740234
facebook/opt-2.7b,hendrycksTest-astronomy,5-shot,accuracy,0.17105263157894737,0.030643607071677084
facebook/opt-2.7b,hendrycksTest-astronomy,5-shot,acc_norm,0.17105263157894737,0.030643607071677084
facebook/opt-2.7b,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.040936018074033256
facebook/opt-2.7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.040936018074033256
facebook/opt-2.7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.22264150943396227,0.025604233470899098
facebook/opt-2.7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.22264150943396227,0.025604233470899098
facebook/opt-2.7b,hendrycksTest-college_biology,5-shot,accuracy,0.20833333333333334,0.03396116205845334
facebook/opt-2.7b,hendrycksTest-college_biology,5-shot,acc_norm,0.20833333333333334,0.03396116205845334
facebook/opt-2.7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.19,0.03942772444036625
facebook/opt-2.7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.19,0.03942772444036625
facebook/opt-2.7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.29,0.045604802157206845
facebook/opt-2.7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.29,0.045604802157206845
facebook/opt-2.7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.23,0.04229525846816505
facebook/opt-2.7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.23,0.04229525846816505
facebook/opt-2.7b,hendrycksTest-college_medicine,5-shot,accuracy,0.24855491329479767,0.03295304696818317
facebook/opt-2.7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.24855491329479767,0.03295304696818317
facebook/opt-2.7b,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237656
facebook/opt-2.7b,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237656
facebook/opt-2.7b,hendrycksTest-computer_security,5-shot,accuracy,0.26,0.04408440022768078
facebook/opt-2.7b,hendrycksTest-computer_security,5-shot,acc_norm,0.26,0.04408440022768078
facebook/opt-2.7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.225531914893617,0.027321078417387536
facebook/opt-2.7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.225531914893617,0.027321078417387536
facebook/opt-2.7b,hendrycksTest-econometrics,5-shot,accuracy,0.24561403508771928,0.04049339297748141
facebook/opt-2.7b,hendrycksTest-econometrics,5-shot,acc_norm,0.24561403508771928,0.04049339297748141
facebook/opt-2.7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2827586206896552,0.03752833958003336
facebook/opt-2.7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2827586206896552,0.03752833958003336
facebook/opt-2.7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.22486772486772486,0.02150209607822914
facebook/opt-2.7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.22486772486772486,0.02150209607822914
facebook/opt-2.7b,hendrycksTest-formal_logic,5-shot,accuracy,0.19047619047619047,0.03512207412302054
facebook/opt-2.7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.19047619047619047,0.03512207412302054
facebook/opt-2.7b,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
facebook/opt-2.7b,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
facebook/opt-2.7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2161290322580645,0.023415293433568525
facebook/opt-2.7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2161290322580645,0.023415293433568525
facebook/opt-2.7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.03051653073269444
facebook/opt-2.7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.03051653073269444
facebook/opt-2.7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526045
facebook/opt-2.7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526045
facebook/opt-2.7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23636363636363636,0.03317505930009181
facebook/opt-2.7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23636363636363636,0.03317505930009181
facebook/opt-2.7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.21717171717171718,0.029376616484945633
facebook/opt-2.7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.21717171717171718,0.029376616484945633
facebook/opt-2.7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.33678756476683935,0.03410780251836182
facebook/opt-2.7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.33678756476683935,0.03410780251836182
facebook/opt-2.7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3487179487179487,0.02416278028401772
facebook/opt-2.7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3487179487179487,0.02416278028401772
facebook/opt-2.7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2814814814814815,0.027420019350945277
facebook/opt-2.7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2814814814814815,0.027420019350945277
facebook/opt-2.7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.21428571428571427,0.02665353159671548
facebook/opt-2.7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.21428571428571427,0.02665353159671548
facebook/opt-2.7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.31788079470198677,0.03802039760107903
facebook/opt-2.7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.31788079470198677,0.03802039760107903
facebook/opt-2.7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.326605504587156,0.020106990889937303
facebook/opt-2.7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.326605504587156,0.020106990889937303
facebook/opt-2.7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4074074074074074,0.033509916046960436
facebook/opt-2.7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4074074074074074,0.033509916046960436
facebook/opt-2.7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.03039153369274154
facebook/opt-2.7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.03039153369274154
facebook/opt-2.7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.23628691983122363,0.02765215314415927
facebook/opt-2.7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.23628691983122363,0.02765215314415927
facebook/opt-2.7b,hendrycksTest-human_aging,5-shot,accuracy,0.20179372197309417,0.02693611191280226
facebook/opt-2.7b,hendrycksTest-human_aging,5-shot,acc_norm,0.20179372197309417,0.02693611191280226
facebook/opt-2.7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.1984732824427481,0.03498149385462472
facebook/opt-2.7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.1984732824427481,0.03498149385462472
facebook/opt-2.7b,hendrycksTest-international_law,5-shot,accuracy,0.24793388429752067,0.03941897526516302
facebook/opt-2.7b,hendrycksTest-international_law,5-shot,acc_norm,0.24793388429752067,0.03941897526516302
facebook/opt-2.7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.28703703703703703,0.043733130409147614
facebook/opt-2.7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.28703703703703703,0.043733130409147614
facebook/opt-2.7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.22699386503067484,0.03291099578615769
facebook/opt-2.7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.22699386503067484,0.03291099578615769
facebook/opt-2.7b,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.04287858751340456
facebook/opt-2.7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.04287858751340456
facebook/opt-2.7b,hendrycksTest-management,5-shot,accuracy,0.36893203883495146,0.047776151811567386
facebook/opt-2.7b,hendrycksTest-management,5-shot,acc_norm,0.36893203883495146,0.047776151811567386
facebook/opt-2.7b,hendrycksTest-marketing,5-shot,accuracy,0.23931623931623933,0.027951826808924333
facebook/opt-2.7b,hendrycksTest-marketing,5-shot,acc_norm,0.23931623931623933,0.027951826808924333
facebook/opt-2.7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.047937248544110196
facebook/opt-2.7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.047937248544110196
facebook/opt-2.7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.24648786717752236,0.01541130876968693
facebook/opt-2.7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.24648786717752236,0.01541130876968693
facebook/opt-2.7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.24277456647398843,0.023083658586984204
facebook/opt-2.7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24277456647398843,0.023083658586984204
facebook/opt-2.7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23575418994413408,0.014196375686290804
facebook/opt-2.7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23575418994413408,0.014196375686290804
facebook/opt-2.7b,hendrycksTest-nutrition,5-shot,accuracy,0.19281045751633988,0.02258931888817676
facebook/opt-2.7b,hendrycksTest-nutrition,5-shot,acc_norm,0.19281045751633988,0.02258931888817676
facebook/opt-2.7b,hendrycksTest-philosophy,5-shot,accuracy,0.31511254019292606,0.026385273703464482
facebook/opt-2.7b,hendrycksTest-philosophy,5-shot,acc_norm,0.31511254019292606,0.026385273703464482
facebook/opt-2.7b,hendrycksTest-prehistory,5-shot,accuracy,0.24074074074074073,0.02378858355165854
facebook/opt-2.7b,hendrycksTest-prehistory,5-shot,acc_norm,0.24074074074074073,0.02378858355165854
facebook/opt-2.7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2978723404255319,0.02728160834446942
facebook/opt-2.7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2978723404255319,0.02728160834446942
facebook/opt-2.7b,hendrycksTest-professional_law,5-shot,accuracy,0.2522816166883963,0.011092789056875243
facebook/opt-2.7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2522816166883963,0.011092789056875243
facebook/opt-2.7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.31985294117647056,0.028332959514031218
facebook/opt-2.7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.31985294117647056,0.028332959514031218
facebook/opt-2.7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2581699346405229,0.01770453165325007
facebook/opt-2.7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2581699346405229,0.01770453165325007
facebook/opt-2.7b,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
facebook/opt-2.7b,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
facebook/opt-2.7b,hendrycksTest-security_studies,5-shot,accuracy,0.20816326530612245,0.025991117672813296
facebook/opt-2.7b,hendrycksTest-security_studies,5-shot,acc_norm,0.20816326530612245,0.025991117672813296
facebook/opt-2.7b,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.03076944496729602
facebook/opt-2.7b,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.03076944496729602
facebook/opt-2.7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.25,0.04351941398892446
facebook/opt-2.7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.25,0.04351941398892446
facebook/opt-2.7b,hendrycksTest-virology,5-shot,accuracy,0.23493975903614459,0.03300533186128922
facebook/opt-2.7b,hendrycksTest-virology,5-shot,acc_norm,0.23493975903614459,0.03300533186128922
facebook/opt-2.7b,hendrycksTest-world_religions,5-shot,accuracy,0.17543859649122806,0.029170885500727654
facebook/opt-2.7b,hendrycksTest-world_religions,5-shot,acc_norm,0.17543859649122806,0.029170885500727654
facebook/opt-2.7b,truthfulqa:mc,0-shot,mc1,0.22766217870257038,0.01467925503211107
facebook/opt-2.7b,truthfulqa:mc,0-shot,mc2,0.37425417990290183,0.013823032731766478
01-ai/Yi-9B,mmlu_world_religions,0-shot,accuracy,0.8421052631578947,0.027966785859160865
01-ai/Yi-9B,mmlu_formal_logic,0-shot,accuracy,0.5793650793650794,0.04415438226743745
01-ai/Yi-9B,mmlu_prehistory,0-shot,accuracy,0.7716049382716049,0.023358211840626267
01-ai/Yi-9B,mmlu_moral_scenarios,0-shot,accuracy,0.39553072625698327,0.016353415410075768
01-ai/Yi-9B,mmlu_high_school_world_history,0-shot,accuracy,0.8227848101265823,0.024856364184503224
01-ai/Yi-9B,mmlu_moral_disputes,0-shot,accuracy,0.7658959537572254,0.022797110278071128
01-ai/Yi-9B,mmlu_professional_law,0-shot,accuracy,0.4941329856584094,0.012769356925216526
01-ai/Yi-9B,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.033220157957767414
01-ai/Yi-9B,mmlu_high_school_us_history,0-shot,accuracy,0.8676470588235294,0.023784297520918856
01-ai/Yi-9B,mmlu_philosophy,0-shot,accuracy,0.77491961414791,0.023720088516179034
01-ai/Yi-9B,mmlu_jurisprudence,0-shot,accuracy,0.7870370370370371,0.039578354719809784
01-ai/Yi-9B,mmlu_international_law,0-shot,accuracy,0.8264462809917356,0.0345727283691767
01-ai/Yi-9B,mmlu_high_school_european_history,0-shot,accuracy,0.7696969696969697,0.0328766675860349
01-ai/Yi-9B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9326424870466321,0.018088393839078912
01-ai/Yi-9B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8235294117647058,0.024762902678057926
01-ai/Yi-9B,mmlu_high_school_geography,0-shot,accuracy,0.8535353535353535,0.02519092111460393
01-ai/Yi-9B,mmlu_high_school_psychology,0-shot,accuracy,0.8660550458715597,0.014602811435592635
01-ai/Yi-9B,mmlu_public_relations,0-shot,accuracy,0.7363636363636363,0.04220224692971987
01-ai/Yi-9B,mmlu_us_foreign_policy,0-shot,accuracy,0.9,0.030151134457776348
01-ai/Yi-9B,mmlu_sociology,0-shot,accuracy,0.8606965174129353,0.02448448716291397
01-ai/Yi-9B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.764102564102564,0.021525965407408726
01-ai/Yi-9B,mmlu_security_studies,0-shot,accuracy,0.763265306122449,0.02721283588407314
01-ai/Yi-9B,mmlu_professional_psychology,0-shot,accuracy,0.7058823529411765,0.018433427649401896
01-ai/Yi-9B,mmlu_human_sexuality,0-shot,accuracy,0.7862595419847328,0.0359546161177469
01-ai/Yi-9B,mmlu_econometrics,0-shot,accuracy,0.5263157894736842,0.046970851366478626
01-ai/Yi-9B,mmlu_miscellaneous,0-shot,accuracy,0.8390804597701149,0.01314022551561173
01-ai/Yi-9B,mmlu_marketing,0-shot,accuracy,0.905982905982906,0.019119892798924974
01-ai/Yi-9B,mmlu_management,0-shot,accuracy,0.8252427184466019,0.03760178006026621
01-ai/Yi-9B,mmlu_nutrition,0-shot,accuracy,0.7549019607843137,0.02463004897982476
01-ai/Yi-9B,mmlu_medical_genetics,0-shot,accuracy,0.75,0.04351941398892446
01-ai/Yi-9B,mmlu_human_aging,0-shot,accuracy,0.7488789237668162,0.02910522083322462
01-ai/Yi-9B,mmlu_professional_medicine,0-shot,accuracy,0.7058823529411765,0.0276784686421447
01-ai/Yi-9B,mmlu_college_medicine,0-shot,accuracy,0.7283236994219653,0.0339175032232166
01-ai/Yi-9B,mmlu_business_ethics,0-shot,accuracy,0.79,0.040936018074033256
01-ai/Yi-9B,mmlu_clinical_knowledge,0-shot,accuracy,0.7283018867924528,0.027377706624670713
01-ai/Yi-9B,mmlu_global_facts,0-shot,accuracy,0.34,0.04760952285695235
01-ai/Yi-9B,mmlu_virology,0-shot,accuracy,0.5060240963855421,0.03892212195333045
01-ai/Yi-9B,mmlu_professional_accounting,0-shot,accuracy,0.5602836879432624,0.029609912075594106
01-ai/Yi-9B,mmlu_college_physics,0-shot,accuracy,0.46078431372549017,0.049598599663841815
01-ai/Yi-9B,mmlu_high_school_physics,0-shot,accuracy,0.4105960264900662,0.04016689594849929
01-ai/Yi-9B,mmlu_high_school_biology,0-shot,accuracy,0.8483870967741935,0.02040261665441674
01-ai/Yi-9B,mmlu_college_biology,0-shot,accuracy,0.8263888888888888,0.03167473383795718
01-ai/Yi-9B,mmlu_anatomy,0-shot,accuracy,0.5851851851851851,0.04256193767901408
01-ai/Yi-9B,mmlu_college_chemistry,0-shot,accuracy,0.53,0.050161355804659205
01-ai/Yi-9B,mmlu_computer_security,0-shot,accuracy,0.82,0.038612291966536934
01-ai/Yi-9B,mmlu_college_computer_science,0-shot,accuracy,0.61,0.04902071300001974
01-ai/Yi-9B,mmlu_astronomy,0-shot,accuracy,0.756578947368421,0.034923496688842384
01-ai/Yi-9B,mmlu_college_mathematics,0-shot,accuracy,0.44,0.049888765156985884
01-ai/Yi-9B,mmlu_conceptual_physics,0-shot,accuracy,0.7106382978723405,0.02964400657700962
01-ai/Yi-9B,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.046056618647183814
01-ai/Yi-9B,mmlu_high_school_computer_science,0-shot,accuracy,0.85,0.035887028128263686
01-ai/Yi-9B,mmlu_machine_learning,0-shot,accuracy,0.5714285714285714,0.04697113923010212
01-ai/Yi-9B,mmlu_high_school_chemistry,0-shot,accuracy,0.5862068965517241,0.03465304488406796
01-ai/Yi-9B,mmlu_high_school_statistics,0-shot,accuracy,0.6805555555555556,0.03179876342176851
01-ai/Yi-9B,mmlu_elementary_mathematics,0-shot,accuracy,0.5978835978835979,0.02525303255499769
01-ai/Yi-9B,mmlu_electrical_engineering,0-shot,accuracy,0.7310344827586207,0.03695183311650232
01-ai/Yi-9B,mmlu_high_school_mathematics,0-shot,accuracy,0.40370370370370373,0.029914812342227627
01-ai/Yi-9B,arc_challenge,25-shot,accuracy,0.5708191126279863,0.01446408589487065
01-ai/Yi-9B,arc_challenge,25-shot,acc_norm,0.6092150170648464,0.014258563880513778
01-ai/Yi-9B,hellaswag,10-shot,accuracy,0.5907189802828122,0.004906962980328275
01-ai/Yi-9B,hellaswag,10-shot,acc_norm,0.7898824935271859,0.004065592811696024
01-ai/Yi-9B,truthfulqa_mc2,0-shot,accuracy,0.42418949208379386,0.01470465519411602
01-ai/Yi-9B,truthfulqa_gen,0-shot,bleu_max,30.339360489971618,0.8638727006346777
01-ai/Yi-9B,truthfulqa_gen,0-shot,bleu_acc,0.4418604651162791,0.017384767478986204
01-ai/Yi-9B,truthfulqa_gen,0-shot,bleu_diff,1.3722910316943433,1.0157369467932418
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge1_max,54.26710006444303,0.972813269778497
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge1_acc,0.423500611995104,0.01729742144853477
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge1_diff,2.2642753980731283,1.2861505543673575
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge2_max,39.64525782768942,1.1477096591774218
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge2_acc,0.3574051407588739,0.016776599676729422
01-ai/Yi-9B,truthfulqa_gen,0-shot,rouge2_diff,1.7760823421690082,1.4195555435665703
01-ai/Yi-9B,truthfulqa_gen,0-shot,rougeL_max,51.8047383804989,0.9914125732871117
01-ai/Yi-9B,truthfulqa_gen,0-shot,rougeL_acc,0.42472460220318237,0.01730400095716747
01-ai/Yi-9B,truthfulqa_gen,0-shot,rougeL_diff,1.766099304090031,1.3064312031215068
01-ai/Yi-9B,truthfulqa_mc1,0-shot,accuracy,0.2778457772337821,0.015680929364024654
01-ai/Yi-9B,winogrande,5-shot,accuracy,0.7655880031570639,0.01190613010623799
01-ai/Yi-9B,gsm8k,5-shot,accuracy,0.5072024260803639,0.013771055751972875
facebook/opt-125m,drop,3-shot,accuracy,0.0009437919463087249,0.0003144653119413193
facebook/opt-125m,drop,3-shot,f1,0.0336220637583893,0.001037070791466854
facebook/opt-125m,gsm8k,5-shot,accuracy,0.01819560272934041,0.0036816118940738705
facebook/opt-125m,winogrande,5-shot,accuracy,0.5090765588003157,0.014050170094497704
facebook/opt-125m,arc:challenge,25-shot,accuracy,0.20392491467576793,0.011774262478702256
facebook/opt-125m,arc:challenge,25-shot,acc_norm,0.22866894197952217,0.012272853582540792
facebook/opt-125m,hellaswag,10-shot,accuracy,0.29107747460665206,0.0045333077585213545
facebook/opt-125m,hellaswag,10-shot,acc_norm,0.31756622186815375,0.004645783048004614
facebook/opt-125m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.044619604333847415
facebook/opt-125m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.044619604333847415
facebook/opt-125m,hendrycksTest-anatomy,5-shot,accuracy,0.24444444444444444,0.03712537833614868
facebook/opt-125m,hendrycksTest-anatomy,5-shot,acc_norm,0.24444444444444444,0.03712537833614868
facebook/opt-125m,hendrycksTest-astronomy,5-shot,accuracy,0.26973684210526316,0.03611780560284898
facebook/opt-125m,hendrycksTest-astronomy,5-shot,acc_norm,0.26973684210526316,0.03611780560284898
facebook/opt-125m,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.040936018074033256
facebook/opt-125m,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.040936018074033256
facebook/opt-125m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.21509433962264152,0.025288394502891363
facebook/opt-125m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.21509433962264152,0.025288394502891363
facebook/opt-125m,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.03476590104304134
facebook/opt-125m,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.03476590104304134
facebook/opt-125m,hendrycksTest-college_chemistry,5-shot,accuracy,0.28,0.04512608598542127
facebook/opt-125m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.28,0.04512608598542127
facebook/opt-125m,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.04725815626252604
facebook/opt-125m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.04725815626252604
facebook/opt-125m,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.04408440022768077
facebook/opt-125m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.04408440022768077
facebook/opt-125m,hendrycksTest-college_medicine,5-shot,accuracy,0.2023121387283237,0.03063114553919882
facebook/opt-125m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2023121387283237,0.03063114553919882
facebook/opt-125m,hendrycksTest-college_physics,5-shot,accuracy,0.37254901960784315,0.04810840148082633
facebook/opt-125m,hendrycksTest-college_physics,5-shot,acc_norm,0.37254901960784315,0.04810840148082633
facebook/opt-125m,hendrycksTest-computer_security,5-shot,accuracy,0.18,0.038612291966536955
facebook/opt-125m,hendrycksTest-computer_security,5-shot,acc_norm,0.18,0.038612291966536955
facebook/opt-125m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3191489361702128,0.030472973363380045
facebook/opt-125m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3191489361702128,0.030472973363380045
facebook/opt-125m,hendrycksTest-econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
facebook/opt-125m,hendrycksTest-econometrics,5-shot,acc_norm,0.22807017543859648,0.03947152782669415
facebook/opt-125m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.0360010569272777
facebook/opt-125m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.0360010569272777
facebook/opt-125m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.022494510767503154
facebook/opt-125m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.022494510767503154
facebook/opt-125m,hendrycksTest-formal_logic,5-shot,accuracy,0.14285714285714285,0.03129843185743809
facebook/opt-125m,hendrycksTest-formal_logic,5-shot,acc_norm,0.14285714285714285,0.03129843185743809
facebook/opt-125m,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.038612291966536934
facebook/opt-125m,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.038612291966536934
facebook/opt-125m,hendrycksTest-high_school_biology,5-shot,accuracy,0.3161290322580645,0.02645087448904277
facebook/opt-125m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3161290322580645,0.02645087448904277
facebook/opt-125m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2955665024630542,0.032104944337514575
facebook/opt-125m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2955665024630542,0.032104944337514575
facebook/opt-125m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.039427724440366234
facebook/opt-125m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.039427724440366234
facebook/opt-125m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.21212121212121213,0.03192271569548299
facebook/opt-125m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.21212121212121213,0.03192271569548299
facebook/opt-125m,hendrycksTest-high_school_geography,5-shot,accuracy,0.2727272727272727,0.03173071239071724
facebook/opt-125m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2727272727272727,0.03173071239071724
facebook/opt-125m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.36787564766839376,0.03480175668466036
facebook/opt-125m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.36787564766839376,0.03480175668466036
facebook/opt-125m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3435897435897436,0.02407869658063547
facebook/opt-125m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3435897435897436,0.02407869658063547
facebook/opt-125m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.026842057873833706
facebook/opt-125m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.026842057873833706
facebook/opt-125m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.35294117647058826,0.031041941304059288
facebook/opt-125m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.35294117647058826,0.031041941304059288
facebook/opt-125m,hendrycksTest-high_school_physics,5-shot,accuracy,0.31125827814569534,0.03780445850526733
facebook/opt-125m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.31125827814569534,0.03780445850526733
facebook/opt-125m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.22752293577981653,0.017974463578776502
facebook/opt-125m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.22752293577981653,0.017974463578776502
facebook/opt-125m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-125m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
facebook/opt-125m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25980392156862747,0.03077855467869326
facebook/opt-125m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25980392156862747,0.03077855467869326
facebook/opt-125m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25738396624472576,0.02845882099146031
facebook/opt-125m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25738396624472576,0.02845882099146031
facebook/opt-125m,hendrycksTest-human_aging,5-shot,accuracy,0.20179372197309417,0.026936111912802273
facebook/opt-125m,hendrycksTest-human_aging,5-shot,acc_norm,0.20179372197309417,0.026936111912802273
facebook/opt-125m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.03727673575596918
facebook/opt-125m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.03727673575596918
facebook/opt-125m,hendrycksTest-international_law,5-shot,accuracy,0.38016528925619836,0.04431324501968432
facebook/opt-125m,hendrycksTest-international_law,5-shot,acc_norm,0.38016528925619836,0.04431324501968432
facebook/opt-125m,hendrycksTest-jurisprudence,5-shot,accuracy,0.21296296296296297,0.0395783547198098
facebook/opt-125m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.21296296296296297,0.0395783547198098
facebook/opt-125m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.22085889570552147,0.032591773927421776
facebook/opt-125m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.22085889570552147,0.032591773927421776
facebook/opt-125m,hendrycksTest-machine_learning,5-shot,accuracy,0.15178571428571427,0.034057028381856924
facebook/opt-125m,hendrycksTest-machine_learning,5-shot,acc_norm,0.15178571428571427,0.034057028381856924
facebook/opt-125m,hendrycksTest-management,5-shot,accuracy,0.18446601941747573,0.03840423627288276
facebook/opt-125m,hendrycksTest-management,5-shot,acc_norm,0.18446601941747573,0.03840423627288276
facebook/opt-125m,hendrycksTest-marketing,5-shot,accuracy,0.19658119658119658,0.02603538609895129
facebook/opt-125m,hendrycksTest-marketing,5-shot,acc_norm,0.19658119658119658,0.02603538609895129
facebook/opt-125m,hendrycksTest-medical_genetics,5-shot,accuracy,0.34,0.047609522856952344
facebook/opt-125m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.34,0.047609522856952344
facebook/opt-125m,hendrycksTest-miscellaneous,5-shot,accuracy,0.2541507024265645,0.015569254692045764
facebook/opt-125m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2541507024265645,0.015569254692045764
facebook/opt-125m,hendrycksTest-moral_disputes,5-shot,accuracy,0.23121387283236994,0.02269865716785571
facebook/opt-125m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.23121387283236994,0.02269865716785571
facebook/opt-125m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
facebook/opt-125m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
facebook/opt-125m,hendrycksTest-nutrition,5-shot,accuracy,0.27124183006535946,0.02545775669666788
facebook/opt-125m,hendrycksTest-nutrition,5-shot,acc_norm,0.27124183006535946,0.02545775669666788
facebook/opt-125m,hendrycksTest-philosophy,5-shot,accuracy,0.2379421221864952,0.024185150647818707
facebook/opt-125m,hendrycksTest-philosophy,5-shot,acc_norm,0.2379421221864952,0.024185150647818707
facebook/opt-125m,hendrycksTest-prehistory,5-shot,accuracy,0.2932098765432099,0.025329888171900926
facebook/opt-125m,hendrycksTest-prehistory,5-shot,acc_norm,0.2932098765432099,0.025329888171900926
facebook/opt-125m,hendrycksTest-professional_accounting,5-shot,accuracy,0.26595744680851063,0.026358065698880592
facebook/opt-125m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.26595744680851063,0.026358065698880592
facebook/opt-125m,hendrycksTest-professional_law,5-shot,accuracy,0.2529335071707953,0.011102268713839989
facebook/opt-125m,hendrycksTest-professional_law,5-shot,acc_norm,0.2529335071707953,0.011102268713839989
facebook/opt-125m,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121593
facebook/opt-125m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.030211479609121593
facebook/opt-125m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2222222222222222,0.016819028375736383
facebook/opt-125m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2222222222222222,0.016819028375736383
facebook/opt-125m,hendrycksTest-public_relations,5-shot,accuracy,0.22727272727272727,0.04013964554072774
facebook/opt-125m,hendrycksTest-public_relations,5-shot,acc_norm,0.22727272727272727,0.04013964554072774
facebook/opt-125m,hendrycksTest-security_studies,5-shot,accuracy,0.24897959183673468,0.027682979522960234
facebook/opt-125m,hendrycksTest-security_studies,5-shot,acc_norm,0.24897959183673468,0.027682979522960234
facebook/opt-125m,hendrycksTest-sociology,5-shot,accuracy,0.23383084577114427,0.029929415408348398
facebook/opt-125m,hendrycksTest-sociology,5-shot,acc_norm,0.23383084577114427,0.029929415408348398
facebook/opt-125m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.32,0.046882617226215034
facebook/opt-125m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.32,0.046882617226215034
facebook/opt-125m,hendrycksTest-virology,5-shot,accuracy,0.20481927710843373,0.03141784291663926
facebook/opt-125m,hendrycksTest-virology,5-shot,acc_norm,0.20481927710843373,0.03141784291663926
facebook/opt-125m,hendrycksTest-world_religions,5-shot,accuracy,0.17543859649122806,0.029170885500727654
facebook/opt-125m,hendrycksTest-world_religions,5-shot,acc_norm,0.17543859649122806,0.029170885500727654
facebook/opt-125m,truthfulqa:mc,0-shot,mc1,0.23990208078335373,0.014948812679062133
facebook/opt-125m,truthfulqa:mc,0-shot,mc2,0.4286875802615585,0.015058469591263904
jisukim8873/falcon-7B-case-0,arc:challenge,25-shot,accuracy,0.45051194539249145,0.014539646098471627
jisukim8873/falcon-7B-case-0,arc:challenge,25-shot,acc_norm,0.49146757679180886,0.014609263165632186
jisukim8873/falcon-7B-case-0,hellaswag,10-shot,accuracy,0.5967934674367655,0.004895390341445623
jisukim8873/falcon-7B-case-0,hellaswag,10-shot,acc_norm,0.782513443537144,0.004116931383157345
jisukim8873/falcon-7B-case-0,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.04688261722621504
jisukim8873/falcon-7B-case-0,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.04688261722621504
jisukim8873/falcon-7B-case-0,hendrycksTest-anatomy,5-shot,accuracy,0.3851851851851852,0.042039210401562783
jisukim8873/falcon-7B-case-0,hendrycksTest-anatomy,5-shot,acc_norm,0.3851851851851852,0.042039210401562783
jisukim8873/falcon-7B-case-0,hendrycksTest-astronomy,5-shot,accuracy,0.23026315789473684,0.03426059424403165
jisukim8873/falcon-7B-case-0,hendrycksTest-astronomy,5-shot,acc_norm,0.23026315789473684,0.03426059424403165
jisukim8873/falcon-7B-case-0,hendrycksTest-business_ethics,5-shot,accuracy,0.25,0.04351941398892446
jisukim8873/falcon-7B-case-0,hendrycksTest-business_ethics,5-shot,acc_norm,0.25,0.04351941398892446
jisukim8873/falcon-7B-case-0,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2943396226415094,0.028049186315695245
jisukim8873/falcon-7B-case-0,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2943396226415094,0.028049186315695245
jisukim8873/falcon-7B-case-0,hendrycksTest-college_biology,5-shot,accuracy,0.2777777777777778,0.037455547914624576
jisukim8873/falcon-7B-case-0,hendrycksTest-college_biology,5-shot,acc_norm,0.2777777777777778,0.037455547914624576
jisukim8873/falcon-7B-case-0,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-0,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.0377525168068637
jisukim8873/falcon-7B-case-0,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.04512608598542128
jisukim8873/falcon-7B-case-0,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.04512608598542128
jisukim8873/falcon-7B-case-0,hendrycksTest-college_mathematics,5-shot,accuracy,0.21,0.040936018074033256
jisukim8873/falcon-7B-case-0,hendrycksTest-college_mathematics,5-shot,acc_norm,0.21,0.040936018074033256
jisukim8873/falcon-7B-case-0,hendrycksTest-college_medicine,5-shot,accuracy,0.23121387283236994,0.0321473730202947
jisukim8873/falcon-7B-case-0,hendrycksTest-college_medicine,5-shot,acc_norm,0.23121387283236994,0.0321473730202947
jisukim8873/falcon-7B-case-0,hendrycksTest-college_physics,5-shot,accuracy,0.17647058823529413,0.03793281185307809
jisukim8873/falcon-7B-case-0,hendrycksTest-college_physics,5-shot,acc_norm,0.17647058823529413,0.03793281185307809
jisukim8873/falcon-7B-case-0,hendrycksTest-computer_security,5-shot,accuracy,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-0,hendrycksTest-computer_security,5-shot,acc_norm,0.35,0.0479372485441102
jisukim8873/falcon-7B-case-0,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3446808510638298,0.03106898596312215
jisukim8873/falcon-7B-case-0,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3446808510638298,0.03106898596312215
jisukim8873/falcon-7B-case-0,hendrycksTest-econometrics,5-shot,accuracy,0.3508771929824561,0.044895393502706986
jisukim8873/falcon-7B-case-0,hendrycksTest-econometrics,5-shot,acc_norm,0.3508771929824561,0.044895393502706986
jisukim8873/falcon-7B-case-0,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2689655172413793,0.036951833116502325
jisukim8873/falcon-7B-case-0,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2689655172413793,0.036951833116502325
jisukim8873/falcon-7B-case-0,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.02256989707491842
jisukim8873/falcon-7B-case-0,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.02256989707491842
jisukim8873/falcon-7B-case-0,hendrycksTest-formal_logic,5-shot,accuracy,0.16666666666666666,0.033333333333333375
jisukim8873/falcon-7B-case-0,hendrycksTest-formal_logic,5-shot,acc_norm,0.16666666666666666,0.033333333333333375
jisukim8873/falcon-7B-case-0,hendrycksTest-global_facts,5-shot,accuracy,0.27,0.0446196043338474
jisukim8873/falcon-7B-case-0,hendrycksTest-global_facts,5-shot,acc_norm,0.27,0.0446196043338474
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_biology,5-shot,accuracy,0.3258064516129032,0.0266620105785671
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3258064516129032,0.0266620105785671
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.28078817733990147,0.03161856335358609
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.28078817733990147,0.03161856335358609
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.045126085985421276
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.045126085985421276
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3515151515151515,0.037282069986826503
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3515151515151515,0.037282069986826503
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_geography,5-shot,accuracy,0.2727272727272727,0.03173071239071724
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2727272727272727,0.03173071239071724
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.25906735751295334,0.03161877917935411
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.25906735751295334,0.03161877917935411
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2282051282051282,0.02127839386358628
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2282051282051282,0.02127839386358628
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24444444444444444,0.02620276653465215
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24444444444444444,0.02620276653465215
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.24369747899159663,0.02788682807838057
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.24369747899159663,0.02788682807838057
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_physics,5-shot,accuracy,0.26490066225165565,0.03603038545360384
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_physics,5-shot,acc_norm,0.26490066225165565,0.03603038545360384
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_psychology,5-shot,accuracy,0.28073394495412846,0.019266055045871616
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.28073394495412846,0.019266055045871616
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_statistics,5-shot,accuracy,0.18981481481481483,0.026744714834691933
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.18981481481481483,0.026744714834691933
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_us_history,5-shot,accuracy,0.27941176470588236,0.03149328104507956
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.27941176470588236,0.03149328104507956
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3080168776371308,0.030052389335605705
jisukim8873/falcon-7B-case-0,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3080168776371308,0.030052389335605705
jisukim8873/falcon-7B-case-0,hendrycksTest-human_aging,5-shot,accuracy,0.4170403587443946,0.03309266936071721
jisukim8873/falcon-7B-case-0,hendrycksTest-human_aging,5-shot,acc_norm,0.4170403587443946,0.03309266936071721
jisukim8873/falcon-7B-case-0,hendrycksTest-human_sexuality,5-shot,accuracy,0.24427480916030533,0.037683359597287434
jisukim8873/falcon-7B-case-0,hendrycksTest-human_sexuality,5-shot,acc_norm,0.24427480916030533,0.037683359597287434
jisukim8873/falcon-7B-case-0,hendrycksTest-international_law,5-shot,accuracy,0.35537190082644626,0.04369236326573981
jisukim8873/falcon-7B-case-0,hendrycksTest-international_law,5-shot,acc_norm,0.35537190082644626,0.04369236326573981
jisukim8873/falcon-7B-case-0,hendrycksTest-jurisprudence,5-shot,accuracy,0.3333333333333333,0.04557239513497752
jisukim8873/falcon-7B-case-0,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3333333333333333,0.04557239513497752
jisukim8873/falcon-7B-case-0,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26380368098159507,0.03462419931615625
jisukim8873/falcon-7B-case-0,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26380368098159507,0.03462419931615625
jisukim8873/falcon-7B-case-0,hendrycksTest-machine_learning,5-shot,accuracy,0.36607142857142855,0.04572372358737431
jisukim8873/falcon-7B-case-0,hendrycksTest-machine_learning,5-shot,acc_norm,0.36607142857142855,0.04572372358737431
jisukim8873/falcon-7B-case-0,hendrycksTest-management,5-shot,accuracy,0.32038834951456313,0.0462028408228004
jisukim8873/falcon-7B-case-0,hendrycksTest-management,5-shot,acc_norm,0.32038834951456313,0.0462028408228004
jisukim8873/falcon-7B-case-0,hendrycksTest-marketing,5-shot,accuracy,0.3333333333333333,0.030882736974138677
jisukim8873/falcon-7B-case-0,hendrycksTest-marketing,5-shot,acc_norm,0.3333333333333333,0.030882736974138677
jisukim8873/falcon-7B-case-0,hendrycksTest-medical_genetics,5-shot,accuracy,0.36,0.048241815132442176
jisukim8873/falcon-7B-case-0,hendrycksTest-medical_genetics,5-shot,acc_norm,0.36,0.048241815132442176
jisukim8873/falcon-7B-case-0,hendrycksTest-miscellaneous,5-shot,accuracy,0.36270753512132825,0.0171927086746023
jisukim8873/falcon-7B-case-0,hendrycksTest-miscellaneous,5-shot,acc_norm,0.36270753512132825,0.0171927086746023
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_disputes,5-shot,accuracy,0.3236994219653179,0.025190181327608425
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3236994219653179,0.025190181327608425
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_scenarios,5-shot,accuracy,0.25921787709497207,0.014655780837497743
jisukim8873/falcon-7B-case-0,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.25921787709497207,0.014655780837497743
jisukim8873/falcon-7B-case-0,hendrycksTest-nutrition,5-shot,accuracy,0.3137254901960784,0.02656892101545715
jisukim8873/falcon-7B-case-0,hendrycksTest-nutrition,5-shot,acc_norm,0.3137254901960784,0.02656892101545715
jisukim8873/falcon-7B-case-0,hendrycksTest-philosophy,5-shot,accuracy,0.3054662379421222,0.026160584450140488
jisukim8873/falcon-7B-case-0,hendrycksTest-philosophy,5-shot,acc_norm,0.3054662379421222,0.026160584450140488
jisukim8873/falcon-7B-case-0,hendrycksTest-prehistory,5-shot,accuracy,0.2808641975308642,0.02500646975579921
jisukim8873/falcon-7B-case-0,hendrycksTest-prehistory,5-shot,acc_norm,0.2808641975308642,0.02500646975579921
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_accounting,5-shot,accuracy,0.26595744680851063,0.026358065698880592
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_accounting,5-shot,acc_norm,0.26595744680851063,0.026358065698880592
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_law,5-shot,accuracy,0.27640156453715775,0.01142215319455357
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_law,5-shot,acc_norm,0.27640156453715775,0.01142215319455357
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_medicine,5-shot,accuracy,0.1801470588235294,0.02334516361654488
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1801470588235294,0.02334516361654488
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_psychology,5-shot,accuracy,0.315359477124183,0.01879808628488689
jisukim8873/falcon-7B-case-0,hendrycksTest-professional_psychology,5-shot,acc_norm,0.315359477124183,0.01879808628488689
jisukim8873/falcon-7B-case-0,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.04350271442923243
jisukim8873/falcon-7B-case-0,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.04350271442923243
jisukim8873/falcon-7B-case-0,hendrycksTest-security_studies,5-shot,accuracy,0.23673469387755103,0.02721283588407316
jisukim8873/falcon-7B-case-0,hendrycksTest-security_studies,5-shot,acc_norm,0.23673469387755103,0.02721283588407316
jisukim8873/falcon-7B-case-0,hendrycksTest-sociology,5-shot,accuracy,0.2736318407960199,0.03152439186555401
jisukim8873/falcon-7B-case-0,hendrycksTest-sociology,5-shot,acc_norm,0.2736318407960199,0.03152439186555401
jisukim8873/falcon-7B-case-0,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.44,0.049888765156985884
jisukim8873/falcon-7B-case-0,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.44,0.049888765156985884
jisukim8873/falcon-7B-case-0,hendrycksTest-virology,5-shot,accuracy,0.3132530120481928,0.036108050180310235
jisukim8873/falcon-7B-case-0,hendrycksTest-virology,5-shot,acc_norm,0.3132530120481928,0.036108050180310235
jisukim8873/falcon-7B-case-0,hendrycksTest-world_religions,5-shot,accuracy,0.391812865497076,0.037439798259264016
jisukim8873/falcon-7B-case-0,hendrycksTest-world_religions,5-shot,acc_norm,0.391812865497076,0.037439798259264016
jisukim8873/falcon-7B-case-0,truthfulqa:mc,0-shot,mc1,0.24724602203182375,0.01510240479735965
jisukim8873/falcon-7B-case-0,truthfulqa:mc,0-shot,mc2,0.3617782578587819,0.014303952786254175
jisukim8873/falcon-7B-case-0,winogrande,5-shot,accuracy,0.7182320441988951,0.012643326011852946
jisukim8873/falcon-7B-case-0,gsm8k,5-shot,accuracy,0.08642911296436695,0.007740044337103763
jisukim8873/falcon-7B-case-0,minerva_math_precalc,5-shot,accuracy,0.007326007326007326,0.003652908089383034
jisukim8873/falcon-7B-case-0,minerva_math_prealgebra,5-shot,accuracy,0.03444316877152698,0.006182738010487289
jisukim8873/falcon-7B-case-0,minerva_math_num_theory,5-shot,accuracy,0.022222222222222223,0.006349206349206316
jisukim8873/falcon-7B-case-0,minerva_math_intermediate_algebra,5-shot,accuracy,0.01107419712070875,0.003484453797831771
jisukim8873/falcon-7B-case-0,minerva_math_geometry,5-shot,accuracy,0.020876826722338204,0.006539385795813932
jisukim8873/falcon-7B-case-0,minerva_math_counting_and_prob,5-shot,accuracy,0.012658227848101266,0.005140313889578838
jisukim8873/falcon-7B-case-0,minerva_math_algebra,5-shot,accuracy,0.015164279696714406,0.003548546043132551
jisukim8873/falcon-7B-case-0,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-0,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-0,arithmetic_3da,5-shot,accuracy,0.041,0.004435012363831009
jisukim8873/falcon-7B-case-0,arithmetic_3ds,5-shot,accuracy,0.105,0.0068564572122015175
jisukim8873/falcon-7B-case-0,arithmetic_4da,5-shot,accuracy,0.002,0.0009992493430694884
jisukim8873/falcon-7B-case-0,arithmetic_2ds,5-shot,accuracy,0.261,0.009822817511892339
jisukim8873/falcon-7B-case-0,arithmetic_5ds,5-shot,accuracy,0.0085,0.002053285901060983
jisukim8873/falcon-7B-case-0,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-0,arithmetic_1dc,5-shot,accuracy,0.095,0.006558125075221681
jisukim8873/falcon-7B-case-0,arithmetic_4ds,5-shot,accuracy,0.0595,0.005290923542820115
jisukim8873/falcon-7B-case-0,arithmetic_2dm,5-shot,accuracy,0.0805,0.006085095660266759
jisukim8873/falcon-7B-case-0,arithmetic_2da,5-shot,accuracy,0.3425,0.010613821253479093
jisukim8873/falcon-7B-case-0,gsm8k_cot,5-shot,accuracy,0.10841546626231995,0.008563852506627485
jisukim8873/falcon-7B-case-0,anli_r2,0-shot,brier_score,0.990088442664038,
jisukim8873/falcon-7B-case-0,anli_r3,0-shot,brier_score,0.899582509397577,
jisukim8873/falcon-7B-case-0,anli_r1,0-shot,brier_score,1.0281641108435757,
jisukim8873/falcon-7B-case-0,xnli_eu,0-shot,brier_score,1.0537250229688973,
jisukim8873/falcon-7B-case-0,xnli_vi,0-shot,brier_score,0.9839825713551124,
jisukim8873/falcon-7B-case-0,xnli_ru,0-shot,brier_score,0.8064365352145315,
jisukim8873/falcon-7B-case-0,xnli_zh,0-shot,brier_score,1.0075338003232164,
jisukim8873/falcon-7B-case-0,xnli_tr,0-shot,brier_score,1.0171633908679167,
jisukim8873/falcon-7B-case-0,xnli_fr,0-shot,brier_score,0.7287345172911353,
jisukim8873/falcon-7B-case-0,xnli_en,0-shot,brier_score,0.6626521979549853,
jisukim8873/falcon-7B-case-0,xnli_ur,0-shot,brier_score,1.2911106062968751,
jisukim8873/falcon-7B-case-0,xnli_ar,0-shot,brier_score,1.3000289724546177,
jisukim8873/falcon-7B-case-0,xnli_de,0-shot,brier_score,0.8378848288444989,
jisukim8873/falcon-7B-case-0,xnli_hi,0-shot,brier_score,1.200629112469935,
jisukim8873/falcon-7B-case-0,xnli_es,0-shot,brier_score,0.8086580822433482,
jisukim8873/falcon-7B-case-0,xnli_bg,0-shot,brier_score,0.9255584740609321,
jisukim8873/falcon-7B-case-0,xnli_sw,0-shot,brier_score,0.9589266465708508,
jisukim8873/falcon-7B-case-0,xnli_el,0-shot,brier_score,0.8660838695165314,
jisukim8873/falcon-7B-case-0,xnli_th,0-shot,brier_score,0.9644722384510833,
jisukim8873/falcon-7B-case-0,logiqa2,0-shot,brier_score,1.0462410059838376,
jisukim8873/falcon-7B-case-0,mathqa,5-shot,brier_score,0.9489730788685253,
jisukim8873/falcon-7B-case-0,lambada_standard,0-shot,perplexity,4.28074845242095,0.09514152737212433
jisukim8873/falcon-7B-case-0,lambada_standard,0-shot,accuracy,0.6621385600620997,0.006589551054654385
jisukim8873/falcon-7B-case-0,lambada_openai,0-shot,perplexity,3.416541255218945,0.07243298727752308
jisukim8873/falcon-7B-case-0,lambada_openai,0-shot,accuracy,0.7288957888608577,0.006193168913891796
mosaicml/mpt-7b-instruct,minerva_math_precalc,5-shot,accuracy,0.029304029304029304,0.007224487305459686
mosaicml/mpt-7b-instruct,minerva_math_prealgebra,5-shot,accuracy,0.0677382319173364,0.008519737992709985
mosaicml/mpt-7b-instruct,minerva_math_num_theory,5-shot,accuracy,0.044444444444444446,0.00887651168786704
mosaicml/mpt-7b-instruct,minerva_math_intermediate_algebra,5-shot,accuracy,0.03211517165005537,0.005870345955796336
mosaicml/mpt-7b-instruct,minerva_math_geometry,5-shot,accuracy,0.05010438413361169,0.009978421784323616
mosaicml/mpt-7b-instruct,minerva_math_counting_and_prob,5-shot,accuracy,0.05907172995780591,0.010840205941655379
mosaicml/mpt-7b-instruct,minerva_math_algebra,5-shot,accuracy,0.040438079191238416,0.005719912921553342
mosaicml/mpt-7b-instruct,fld_default,0-shot,accuracy,0.0,
mosaicml/mpt-7b-instruct,fld_star,0-shot,accuracy,0.0,
mosaicml/mpt-7b-instruct,arithmetic_3da,5-shot,accuracy,0.0505,0.004897639067368751
mosaicml/mpt-7b-instruct,arithmetic_3ds,5-shot,accuracy,0.0445,0.004611996341621302
mosaicml/mpt-7b-instruct,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521504
mosaicml/mpt-7b-instruct,arithmetic_2ds,5-shot,accuracy,0.2525,0.009716948314273846
mosaicml/mpt-7b-instruct,arithmetic_5ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b-instruct,arithmetic_5da,5-shot,accuracy,0.0005,0.0005000000000000026
mosaicml/mpt-7b-instruct,arithmetic_1dc,5-shot,accuracy,0.0135,0.002581124968507327
mosaicml/mpt-7b-instruct,arithmetic_4ds,5-shot,accuracy,0.002,0.00099924934306949
mosaicml/mpt-7b-instruct,arithmetic_2dm,5-shot,accuracy,0.067,0.005592060046868742
mosaicml/mpt-7b-instruct,arithmetic_2da,5-shot,accuracy,0.1985,0.008921248193760107
mosaicml/mpt-7b-instruct,gsm8k_cot,5-shot,accuracy,0.06974981046247157,0.007016389571013853
mosaicml/mpt-7b-instruct,gsm8k,5-shot,accuracy,0.028051554207733132,0.0045482295338363475
mosaicml/mpt-7b-instruct,anli_r2,0-shot,brier_score,0.7896846312992308,
mosaicml/mpt-7b-instruct,anli_r3,0-shot,brier_score,0.7829638521132672,
mosaicml/mpt-7b-instruct,anli_r1,0-shot,brier_score,0.815948761399128,
mosaicml/mpt-7b-instruct,xnli_eu,0-shot,brier_score,1.0863038087250385,
mosaicml/mpt-7b-instruct,xnli_vi,0-shot,brier_score,1.0752130995950049,
mosaicml/mpt-7b-instruct,xnli_ru,0-shot,brier_score,0.8232916030715194,
mosaicml/mpt-7b-instruct,xnli_zh,0-shot,brier_score,1.0965246448802113,
mosaicml/mpt-7b-instruct,xnli_tr,0-shot,brier_score,0.9706318085768089,
mosaicml/mpt-7b-instruct,xnli_fr,0-shot,brier_score,0.7747493883608759,
mosaicml/mpt-7b-instruct,xnli_en,0-shot,brier_score,0.6437717014643014,
mosaicml/mpt-7b-instruct,xnli_ur,0-shot,brier_score,1.31440196299888,
mosaicml/mpt-7b-instruct,xnli_ar,0-shot,brier_score,1.264810737749815,
mosaicml/mpt-7b-instruct,xnli_de,0-shot,brier_score,0.8408303311766104,
mosaicml/mpt-7b-instruct,xnli_hi,0-shot,brier_score,1.0110715039028983,
mosaicml/mpt-7b-instruct,xnli_es,0-shot,brier_score,0.8694036206904238,
mosaicml/mpt-7b-instruct,xnli_bg,0-shot,brier_score,0.880489846093007,
mosaicml/mpt-7b-instruct,xnli_sw,0-shot,brier_score,1.1727384594477277,
mosaicml/mpt-7b-instruct,xnli_el,0-shot,brier_score,1.037015650900093,
mosaicml/mpt-7b-instruct,xnli_th,0-shot,brier_score,0.9186444184570115,
mosaicml/mpt-7b-instruct,logiqa2,0-shot,brier_score,1.0058818905060056,
mosaicml/mpt-7b-instruct,mathqa,5-shot,brier_score,0.9402290653146276,
mosaicml/mpt-7b-instruct,lambada_standard,0-shot,perplexity,4.596967497234068,0.11312431016784812
mosaicml/mpt-7b-instruct,lambada_standard,0-shot,accuracy,0.6262371434116049,0.0067403050865517955
mosaicml/mpt-7b-instruct,lambada_openai,0-shot,perplexity,3.6623178667968417,0.08650055440703999
mosaicml/mpt-7b-instruct,lambada_openai,0-shot,accuracy,0.673394139336309,0.0065336930212616965
mosaicml/mpt-7b-instruct,mmlu_world_religions,0-shot,accuracy,0.2222222222222222,0.03188578017686398
mosaicml/mpt-7b-instruct,mmlu_formal_logic,0-shot,accuracy,0.20634920634920634,0.03619604524124252
mosaicml/mpt-7b-instruct,mmlu_prehistory,0-shot,accuracy,0.32407407407407407,0.02604176620271716
mosaicml/mpt-7b-instruct,mmlu_moral_scenarios,0-shot,accuracy,0.24134078212290502,0.01431099954796146
mosaicml/mpt-7b-instruct,mmlu_high_school_world_history,0-shot,accuracy,0.24472573839662448,0.02798569938703643
mosaicml/mpt-7b-instruct,mmlu_moral_disputes,0-shot,accuracy,0.3208092485549133,0.025131000233647904
mosaicml/mpt-7b-instruct,mmlu_professional_law,0-shot,accuracy,0.2842242503259452,0.011519880596516078
mosaicml/mpt-7b-instruct,mmlu_logical_fallacies,0-shot,accuracy,0.3128834355828221,0.036429145782924055
mosaicml/mpt-7b-instruct,mmlu_high_school_us_history,0-shot,accuracy,0.24509803921568626,0.030190282453501954
mosaicml/mpt-7b-instruct,mmlu_philosophy,0-shot,accuracy,0.31511254019292606,0.026385273703464492
mosaicml/mpt-7b-instruct,mmlu_jurisprudence,0-shot,accuracy,0.3148148148148148,0.04489931073591311
mosaicml/mpt-7b-instruct,mmlu_international_law,0-shot,accuracy,0.32231404958677684,0.042664163633521664
mosaicml/mpt-7b-instruct,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.03546563019624336
mosaicml/mpt-7b-instruct,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.03469713791704372
mosaicml/mpt-7b-instruct,mmlu_high_school_microeconomics,0-shot,accuracy,0.3319327731092437,0.030588697013783667
mosaicml/mpt-7b-instruct,mmlu_high_school_geography,0-shot,accuracy,0.3434343434343434,0.03383201223244443
mosaicml/mpt-7b-instruct,mmlu_high_school_psychology,0-shot,accuracy,0.3321100917431193,0.020192682985423357
mosaicml/mpt-7b-instruct,mmlu_public_relations,0-shot,accuracy,0.37272727272727274,0.04631381319425464
mosaicml/mpt-7b-instruct,mmlu_us_foreign_policy,0-shot,accuracy,0.36,0.04824181513244218
mosaicml/mpt-7b-instruct,mmlu_sociology,0-shot,accuracy,0.24875621890547264,0.030567675938916707
mosaicml/mpt-7b-instruct,mmlu_high_school_macroeconomics,0-shot,accuracy,0.35384615384615387,0.024243783994062153
mosaicml/mpt-7b-instruct,mmlu_security_studies,0-shot,accuracy,0.42857142857142855,0.031680911612338825
mosaicml/mpt-7b-instruct,mmlu_professional_psychology,0-shot,accuracy,0.315359477124183,0.018798086284886887
mosaicml/mpt-7b-instruct,mmlu_human_sexuality,0-shot,accuracy,0.3969465648854962,0.04291135671009224
mosaicml/mpt-7b-instruct,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.04185774424022056
mosaicml/mpt-7b-instruct,mmlu_miscellaneous,0-shot,accuracy,0.334610472541507,0.016873468641592157
mosaicml/mpt-7b-instruct,mmlu_marketing,0-shot,accuracy,0.33760683760683763,0.030980296992618558
mosaicml/mpt-7b-instruct,mmlu_management,0-shot,accuracy,0.27184466019417475,0.044052680241409216
mosaicml/mpt-7b-instruct,mmlu_nutrition,0-shot,accuracy,0.31699346405228757,0.02664327847450875
mosaicml/mpt-7b-instruct,mmlu_medical_genetics,0-shot,accuracy,0.37,0.04852365870939099
mosaicml/mpt-7b-instruct,mmlu_human_aging,0-shot,accuracy,0.34080717488789236,0.031811497470553604
mosaicml/mpt-7b-instruct,mmlu_professional_medicine,0-shot,accuracy,0.23897058823529413,0.025905280644893006
mosaicml/mpt-7b-instruct,mmlu_college_medicine,0-shot,accuracy,0.3179190751445087,0.03550683989165582
mosaicml/mpt-7b-instruct,mmlu_business_ethics,0-shot,accuracy,0.29,0.04560480215720684
mosaicml/mpt-7b-instruct,mmlu_clinical_knowledge,0-shot,accuracy,0.35471698113207545,0.02944517532819959
mosaicml/mpt-7b-instruct,mmlu_global_facts,0-shot,accuracy,0.29,0.045604802157206845
mosaicml/mpt-7b-instruct,mmlu_virology,0-shot,accuracy,0.3614457831325301,0.037400593820293204
mosaicml/mpt-7b-instruct,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539005,0.025892151156709405
mosaicml/mpt-7b-instruct,mmlu_college_physics,0-shot,accuracy,0.24509803921568626,0.04280105837364395
mosaicml/mpt-7b-instruct,mmlu_high_school_physics,0-shot,accuracy,0.33774834437086093,0.03861557546255168
mosaicml/mpt-7b-instruct,mmlu_high_school_biology,0-shot,accuracy,0.3870967741935484,0.02770935967503249
mosaicml/mpt-7b-instruct,mmlu_college_biology,0-shot,accuracy,0.3263888888888889,0.03921067198982266
mosaicml/mpt-7b-instruct,mmlu_anatomy,0-shot,accuracy,0.28888888888888886,0.0391545063041425
mosaicml/mpt-7b-instruct,mmlu_college_chemistry,0-shot,accuracy,0.36,0.048241815132442176
mosaicml/mpt-7b-instruct,mmlu_computer_security,0-shot,accuracy,0.37,0.04852365870939099
mosaicml/mpt-7b-instruct,mmlu_college_computer_science,0-shot,accuracy,0.37,0.04852365870939099
mosaicml/mpt-7b-instruct,mmlu_astronomy,0-shot,accuracy,0.3157894736842105,0.037827289808654685
mosaicml/mpt-7b-instruct,mmlu_college_mathematics,0-shot,accuracy,0.32,0.04688261722621504
mosaicml/mpt-7b-instruct,mmlu_conceptual_physics,0-shot,accuracy,0.34893617021276596,0.031158522131357783
mosaicml/mpt-7b-instruct,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768078
mosaicml/mpt-7b-instruct,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.04760952285695235
mosaicml/mpt-7b-instruct,mmlu_machine_learning,0-shot,accuracy,0.33035714285714285,0.04464285714285714
mosaicml/mpt-7b-instruct,mmlu_high_school_chemistry,0-shot,accuracy,0.2857142857142857,0.0317852971064275
mosaicml/mpt-7b-instruct,mmlu_high_school_statistics,0-shot,accuracy,0.375,0.033016908987210894
mosaicml/mpt-7b-instruct,mmlu_elementary_mathematics,0-shot,accuracy,0.2777777777777778,0.02306818884826111
mosaicml/mpt-7b-instruct,mmlu_electrical_engineering,0-shot,accuracy,0.35172413793103446,0.03979236637497412
mosaicml/mpt-7b-instruct,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.026842057873833713
mosaicml/mpt-7b-instruct,arc_challenge,25-shot,accuracy,0.44795221843003413,0.01453201149821167
mosaicml/mpt-7b-instruct,arc_challenge,25-shot,acc_norm,0.49658703071672355,0.014611050403244084
mosaicml/mpt-7b-instruct,hellaswag,10-shot,accuracy,0.58105954989046,0.004923772581848499
mosaicml/mpt-7b-instruct,hellaswag,10-shot,acc_norm,0.7791276638119896,0.004139867975116299
mosaicml/mpt-7b-instruct,truthfulqa_mc2,0-shot,accuracy,0.35155810772535084,0.013782411103738377
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,bleu_max,22.020255818812327,0.7190998542826786
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,bleu_acc,0.3072215422276622,0.016150201321323047
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,bleu_diff,-7.310042815538973,0.7057991080780003
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge1_max,45.74195109027613,0.8459789920841821
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge1_acc,0.2962056303549572,0.01598359510181139
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge1_diff,-10.270084976230358,0.8292877682668228
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge2_max,29.204194016485438,0.9485549352289394
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge2_acc,0.22276621787025705,0.01456650696139674
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rouge2_diff,-11.797169376853871,0.9223640474246654
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rougeL_max,42.95963549103364,0.8505381892909781
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rougeL_acc,0.2864137086903305,0.015826142439502377
mosaicml/mpt-7b-instruct,truthfulqa_gen,0-shot,rougeL_diff,-10.523682874537876,0.8392765224735843
mosaicml/mpt-7b-instruct,truthfulqa_mc1,0-shot,accuracy,0.22643818849449204,0.014651337324602576
mosaicml/mpt-7b-instruct,winogrande,5-shot,accuracy,0.7048145224940805,0.012819410741754772
EleutherAI/pythia-410M,anli,0-shot,accuracy,0.3381,0.0148
EleutherAI/pythia-410M,logiqa2,0-shot,accuracy,0.2252,0.0105
EleutherAI/pythia-410M,mathqa,5-shot,accuracy,0.2379,0.0078
EleutherAI/pythia-410M,asdiv,5-shot,accuracy,0.0017,0.0009
EleutherAI/pythia-410M,hellaswag,10-shot,accuracy,0.3372,0.0047
EleutherAI/pythia-410M,xnli,0-shot,accuracy,0.3699,0.0318
EleutherAI/pythia-410M,arithmetic,5-shot,accuracy,0.008,0.0086
EleutherAI/gpt-j-6b,minerva_math_precalc,5-shot,accuracy,0.018315018315018316,0.005743696731653661
EleutherAI/gpt-j-6b,minerva_math_prealgebra,5-shot,accuracy,0.04477611940298507,0.007011584710623335
EleutherAI/gpt-j-6b,minerva_math_num_theory,5-shot,accuracy,0.022222222222222223,0.0063492063492063145
EleutherAI/gpt-j-6b,minerva_math_intermediate_algebra,5-shot,accuracy,0.023255813953488372,0.0050182572516275815
EleutherAI/gpt-j-6b,minerva_math_geometry,5-shot,accuracy,0.022964509394572025,0.006851249878769254
EleutherAI/gpt-j-6b,minerva_math_counting_and_prob,5-shot,accuracy,0.029535864978902954,0.007784559126948299
EleutherAI/gpt-j-6b,minerva_math_algebra,5-shot,accuracy,0.02358887952822241,0.004406843931023534
EleutherAI/gpt-j-6b,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-j-6b,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-j-6b,arithmetic_3da,5-shot,accuracy,0.087,0.006303599581496389
EleutherAI/gpt-j-6b,arithmetic_3ds,5-shot,accuracy,0.0465,0.004709561018023942
EleutherAI/gpt-j-6b,arithmetic_4da,5-shot,accuracy,0.007,0.0018647355360237453
EleutherAI/gpt-j-6b,arithmetic_2ds,5-shot,accuracy,0.2175,0.009227103810100017
EleutherAI/gpt-j-6b,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-j-6b,arithmetic_5da,5-shot,accuracy,0.0005,0.0005000000000000127
EleutherAI/gpt-j-6b,arithmetic_1dc,5-shot,accuracy,0.089,0.006368656050529475
EleutherAI/gpt-j-6b,arithmetic_4ds,5-shot,accuracy,0.0055,0.0016541593398342208
EleutherAI/gpt-j-6b,arithmetic_2dm,5-shot,accuracy,0.1395,0.007749187050909062
EleutherAI/gpt-j-6b,arithmetic_2da,5-shot,accuracy,0.24,0.009552257472001268
EleutherAI/gpt-j-6b,gsm8k_cot,5-shot,accuracy,0.03335860500379075,0.004946282649173773
EleutherAI/gpt-j-6b,gsm8k,5-shot,accuracy,0.029567854435178165,0.004665893134220772
EleutherAI/gpt-j-6b,anli_r2,0-shot,brier_score,0.7625067534704538,
EleutherAI/gpt-j-6b,anli_r3,0-shot,brier_score,0.7268456375297242,
EleutherAI/gpt-j-6b,anli_r1,0-shot,brier_score,0.7767404198801905,
EleutherAI/gpt-j-6b,xnli_eu,0-shot,brier_score,0.840514896839387,
EleutherAI/gpt-j-6b,xnli_vi,0-shot,brier_score,0.758218907162874,
EleutherAI/gpt-j-6b,xnli_ru,0-shot,brier_score,0.7743489919871677,
EleutherAI/gpt-j-6b,xnli_zh,0-shot,brier_score,0.9538503257434168,
EleutherAI/gpt-j-6b,xnli_tr,0-shot,brier_score,0.8753439505463679,
EleutherAI/gpt-j-6b,xnli_fr,0-shot,brier_score,0.7600564801060831,
EleutherAI/gpt-j-6b,xnli_en,0-shot,brier_score,0.631686417358844,
EleutherAI/gpt-j-6b,xnli_ur,0-shot,brier_score,0.9619292774250547,
EleutherAI/gpt-j-6b,xnli_ar,0-shot,brier_score,1.1116475345057533,
EleutherAI/gpt-j-6b,xnli_de,0-shot,brier_score,0.8152549958871562,
EleutherAI/gpt-j-6b,xnli_hi,0-shot,brier_score,0.7460150666217235,
EleutherAI/gpt-j-6b,xnli_es,0-shot,brier_score,0.7914512101723771,
EleutherAI/gpt-j-6b,xnli_bg,0-shot,brier_score,0.7416509603093395,
EleutherAI/gpt-j-6b,xnli_sw,0-shot,brier_score,0.8512933047395764,
EleutherAI/gpt-j-6b,xnli_el,0-shot,brier_score,1.089738388519557,
EleutherAI/gpt-j-6b,xnli_th,0-shot,brier_score,0.828907727940754,
EleutherAI/gpt-j-6b,logiqa2,0-shot,brier_score,1.1310390010190252,
EleutherAI/gpt-j-6b,mathqa,5-shot,brier_score,0.9512998262714331,
EleutherAI/gpt-j-6b,lambada_standard,0-shot,perplexity,5.680920517213677,0.13320913025647432
EleutherAI/gpt-j-6b,lambada_standard,0-shot,accuracy,0.6136231321560256,0.006783729046949487
EleutherAI/gpt-j-6b,lambada_openai,0-shot,perplexity,4.1024027240281375,0.08833542272706026
EleutherAI/gpt-j-6b,lambada_openai,0-shot,accuracy,0.6830972249175238,0.00648210937056637
EleutherAI/gpt-j-6b,mmlu_world_religions,0-shot,accuracy,0.34502923976608185,0.036459813773888065
EleutherAI/gpt-j-6b,mmlu_formal_logic,0-shot,accuracy,0.18253968253968253,0.03455071019102151
EleutherAI/gpt-j-6b,mmlu_prehistory,0-shot,accuracy,0.30864197530864196,0.02570264026060376
EleutherAI/gpt-j-6b,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217892
EleutherAI/gpt-j-6b,mmlu_high_school_world_history,0-shot,accuracy,0.2869198312236287,0.029443773022594693
EleutherAI/gpt-j-6b,mmlu_moral_disputes,0-shot,accuracy,0.26878612716763006,0.02386800326250011
EleutherAI/gpt-j-6b,mmlu_professional_law,0-shot,accuracy,0.288135593220339,0.011567140661324568
EleutherAI/gpt-j-6b,mmlu_logical_fallacies,0-shot,accuracy,0.24539877300613497,0.03380939813943353
EleutherAI/gpt-j-6b,mmlu_high_school_us_history,0-shot,accuracy,0.28431372549019607,0.03166009679399813
EleutherAI/gpt-j-6b,mmlu_philosophy,0-shot,accuracy,0.26688102893890675,0.02512263760881664
EleutherAI/gpt-j-6b,mmlu_jurisprudence,0-shot,accuracy,0.26851851851851855,0.04284467968052191
EleutherAI/gpt-j-6b,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.03941897526516303
EleutherAI/gpt-j-6b,mmlu_high_school_european_history,0-shot,accuracy,0.2787878787878788,0.035014387062967806
EleutherAI/gpt-j-6b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.21761658031088082,0.02977866303775296
EleutherAI/gpt-j-6b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2647058823529412,0.02865749128507196
EleutherAI/gpt-j-6b,mmlu_high_school_geography,0-shot,accuracy,0.23232323232323232,0.030088629490217483
EleutherAI/gpt-j-6b,mmlu_high_school_psychology,0-shot,accuracy,0.23302752293577983,0.018125669180861507
EleutherAI/gpt-j-6b,mmlu_public_relations,0-shot,accuracy,0.34545454545454546,0.04554619617541054
EleutherAI/gpt-j-6b,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-j-6b,mmlu_sociology,0-shot,accuracy,0.2885572139303483,0.03203841040213322
EleutherAI/gpt-j-6b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2358974358974359,0.021525965407408726
EleutherAI/gpt-j-6b,mmlu_security_studies,0-shot,accuracy,0.35918367346938773,0.03071356045510849
EleutherAI/gpt-j-6b,mmlu_professional_psychology,0-shot,accuracy,0.27941176470588236,0.018152871051538826
EleutherAI/gpt-j-6b,mmlu_human_sexuality,0-shot,accuracy,0.22137404580152673,0.03641297081313729
EleutherAI/gpt-j-6b,mmlu_econometrics,0-shot,accuracy,0.2807017543859649,0.042270544512322
EleutherAI/gpt-j-6b,mmlu_miscellaneous,0-shot,accuracy,0.3128991060025543,0.016580935940304055
EleutherAI/gpt-j-6b,mmlu_marketing,0-shot,accuracy,0.27350427350427353,0.02920254015343117
EleutherAI/gpt-j-6b,mmlu_management,0-shot,accuracy,0.20388349514563106,0.039891398595317706
EleutherAI/gpt-j-6b,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.025261691219729494
EleutherAI/gpt-j-6b,mmlu_medical_genetics,0-shot,accuracy,0.29,0.04560480215720684
EleutherAI/gpt-j-6b,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.03149384670994131
EleutherAI/gpt-j-6b,mmlu_professional_medicine,0-shot,accuracy,0.23897058823529413,0.025905280644893006
EleutherAI/gpt-j-6b,mmlu_college_medicine,0-shot,accuracy,0.27167630057803466,0.03391750322321659
EleutherAI/gpt-j-6b,mmlu_business_ethics,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-j-6b,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.02688064788905199
EleutherAI/gpt-j-6b,mmlu_global_facts,0-shot,accuracy,0.22,0.0416333199893227
EleutherAI/gpt-j-6b,mmlu_virology,0-shot,accuracy,0.3373493975903614,0.03680783690727581
EleutherAI/gpt-j-6b,mmlu_professional_accounting,0-shot,accuracy,0.2765957446808511,0.026684564340461025
EleutherAI/gpt-j-6b,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237654
EleutherAI/gpt-j-6b,mmlu_high_school_physics,0-shot,accuracy,0.26490066225165565,0.03603038545360384
EleutherAI/gpt-j-6b,mmlu_high_school_biology,0-shot,accuracy,0.20967741935483872,0.023157879349083515
EleutherAI/gpt-j-6b,mmlu_college_biology,0-shot,accuracy,0.25,0.03621034121889507
EleutherAI/gpt-j-6b,mmlu_anatomy,0-shot,accuracy,0.2740740740740741,0.03853254836552003
EleutherAI/gpt-j-6b,mmlu_college_chemistry,0-shot,accuracy,0.18,0.03861229196653694
EleutherAI/gpt-j-6b,mmlu_computer_security,0-shot,accuracy,0.41,0.049431107042371025
EleutherAI/gpt-j-6b,mmlu_college_computer_science,0-shot,accuracy,0.23,0.042295258468165044
EleutherAI/gpt-j-6b,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.03583496176361064
EleutherAI/gpt-j-6b,mmlu_college_mathematics,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/gpt-j-6b,mmlu_conceptual_physics,0-shot,accuracy,0.33617021276595743,0.030881618520676942
EleutherAI/gpt-j-6b,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-j-6b,mmlu_high_school_computer_science,0-shot,accuracy,0.15,0.03588702812826373
EleutherAI/gpt-j-6b,mmlu_machine_learning,0-shot,accuracy,0.38392857142857145,0.04616143075028547
EleutherAI/gpt-j-6b,mmlu_high_school_chemistry,0-shot,accuracy,0.24630541871921183,0.03031509928561773
EleutherAI/gpt-j-6b,mmlu_high_school_statistics,0-shot,accuracy,0.1574074074074074,0.02483717351824239
EleutherAI/gpt-j-6b,mmlu_elementary_mathematics,0-shot,accuracy,0.23544973544973544,0.021851509822031708
EleutherAI/gpt-j-6b,mmlu_electrical_engineering,0-shot,accuracy,0.2896551724137931,0.037800192304380135
EleutherAI/gpt-j-6b,mmlu_high_school_mathematics,0-shot,accuracy,0.2851851851851852,0.027528599210340492
EleutherAI/gpt-j-6b,arc_challenge,25-shot,accuracy,0.36177474402730375,0.014041957945038076
EleutherAI/gpt-j-6b,arc_challenge,25-shot,acc_norm,0.4087030716723549,0.014365750345427008
EleutherAI/gpt-j-6b,hellaswag,10-shot,accuracy,0.4945230033857797,0.00498948204061011
EleutherAI/gpt-j-6b,hellaswag,10-shot,acc_norm,0.675363473411671,0.004672819355838537
EleutherAI/gpt-j-6b,truthfulqa_mc2,0-shot,accuracy,0.35957103209275876,0.013461019906422903
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,bleu_max,22.80807652676079,0.729394081143316
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,bleu_acc,0.2998776009791922,0.01604035296671363
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,bleu_diff,-7.044836167856834,0.7649652886092126
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge1_max,46.0394772961559,0.8908991054603245
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge1_acc,0.2558139534883721,0.01527417621928336
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge1_diff,-10.481586263318324,0.9003692764067813
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge2_max,29.614709663039555,0.9851662196235105
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge2_acc,0.20318237454100369,0.014085666526340879
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rouge2_diff,-11.404292789473761,1.0230238475013214
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rougeL_max,43.38528981216948,0.8926391261722091
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rougeL_acc,0.24357405140758873,0.01502635482491078
EleutherAI/gpt-j-6b,truthfulqa_gen,0-shot,rougeL_diff,-10.689938323819645,0.9132036865292477
EleutherAI/gpt-j-6b,truthfulqa_mc1,0-shot,accuracy,0.20195838433292534,0.014053957441512346
EleutherAI/gpt-j-6b,winogrande,5-shot,accuracy,0.65982636148382,0.0133152187624174
bigscience/bloom-1b7,gsm8k,5-shot,accuracy,0.002274450341167551,0.0013121578148674335
bigscience/bloom-1b7,arc:challenge,25-shot,accuracy,0.2721843003412969,0.013006600406423707
bigscience/bloom-1b7,arc:challenge,25-shot,acc_norm,0.30631399317406144,0.013470584417276511
bigscience/bloom-1b7,hellaswag,10-shot,accuracy,0.37621987651862177,0.004834461997944859
bigscience/bloom-1b7,hellaswag,10-shot,acc_norm,0.4794861581358295,0.004985580065946455
bigscience/bloom-1b7,hendrycksTest-abstract_algebra,5-shot,accuracy,0.18,0.03861229196653697
bigscience/bloom-1b7,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.18,0.03861229196653697
bigscience/bloom-1b7,hendrycksTest-anatomy,5-shot,accuracy,0.24444444444444444,0.03712537833614867
bigscience/bloom-1b7,hendrycksTest-anatomy,5-shot,acc_norm,0.24444444444444444,0.03712537833614867
bigscience/bloom-1b7,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.035834961763610645
bigscience/bloom-1b7,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.035834961763610645
bigscience/bloom-1b7,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.040936018074033256
bigscience/bloom-1b7,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.040936018074033256
bigscience/bloom-1b7,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2981132075471698,0.028152837942493857
bigscience/bloom-1b7,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2981132075471698,0.028152837942493857
bigscience/bloom-1b7,hendrycksTest-college_biology,5-shot,accuracy,0.2777777777777778,0.037455547914624576
bigscience/bloom-1b7,hendrycksTest-college_biology,5-shot,acc_norm,0.2777777777777778,0.037455547914624576
bigscience/bloom-1b7,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.041633319989322695
bigscience/bloom-1b7,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.041633319989322695
bigscience/bloom-1b7,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.04725815626252604
bigscience/bloom-1b7,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.04725815626252604
bigscience/bloom-1b7,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
bigscience/bloom-1b7,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
bigscience/bloom-1b7,hendrycksTest-college_medicine,5-shot,accuracy,0.24277456647398843,0.0326926380614177
bigscience/bloom-1b7,hendrycksTest-college_medicine,5-shot,acc_norm,0.24277456647398843,0.0326926380614177
bigscience/bloom-1b7,hendrycksTest-college_physics,5-shot,accuracy,0.2647058823529412,0.04389869956808779
bigscience/bloom-1b7,hendrycksTest-college_physics,5-shot,acc_norm,0.2647058823529412,0.04389869956808779
bigscience/bloom-1b7,hendrycksTest-computer_security,5-shot,accuracy,0.21,0.04093601807403326
bigscience/bloom-1b7,hendrycksTest-computer_security,5-shot,acc_norm,0.21,0.04093601807403326
bigscience/bloom-1b7,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.02750175294441242
bigscience/bloom-1b7,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.02750175294441242
bigscience/bloom-1b7,hendrycksTest-econometrics,5-shot,accuracy,0.23684210526315788,0.039994238792813344
bigscience/bloom-1b7,hendrycksTest-econometrics,5-shot,acc_norm,0.23684210526315788,0.039994238792813344
bigscience/bloom-1b7,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.036001056927277716
bigscience/bloom-1b7,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.036001056927277716
bigscience/bloom-1b7,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2328042328042328,0.02176596167215454
bigscience/bloom-1b7,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2328042328042328,0.02176596167215454
bigscience/bloom-1b7,hendrycksTest-formal_logic,5-shot,accuracy,0.35714285714285715,0.04285714285714281
bigscience/bloom-1b7,hendrycksTest-formal_logic,5-shot,acc_norm,0.35714285714285715,0.04285714285714281
bigscience/bloom-1b7,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
bigscience/bloom-1b7,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
bigscience/bloom-1b7,hendrycksTest-high_school_biology,5-shot,accuracy,0.2870967741935484,0.02573654274559452
bigscience/bloom-1b7,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2870967741935484,0.02573654274559452
bigscience/bloom-1b7,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.28078817733990147,0.03161856335358609
bigscience/bloom-1b7,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.28078817733990147,0.03161856335358609
bigscience/bloom-1b7,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.22,0.04163331998932269
bigscience/bloom-1b7,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.22,0.04163331998932269
bigscience/bloom-1b7,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3090909090909091,0.03608541011573967
bigscience/bloom-1b7,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3090909090909091,0.03608541011573967
bigscience/bloom-1b7,hendrycksTest-high_school_geography,5-shot,accuracy,0.36363636363636365,0.034273086529999344
bigscience/bloom-1b7,hendrycksTest-high_school_geography,5-shot,acc_norm,0.36363636363636365,0.034273086529999344
bigscience/bloom-1b7,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.38341968911917096,0.03508984236295341
bigscience/bloom-1b7,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.38341968911917096,0.03508984236295341
bigscience/bloom-1b7,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.358974358974359,0.024321738484602354
bigscience/bloom-1b7,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.358974358974359,0.024321738484602354
bigscience/bloom-1b7,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25555555555555554,0.02659393910184407
bigscience/bloom-1b7,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25555555555555554,0.02659393910184407
bigscience/bloom-1b7,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2857142857142857,0.02934457250063434
bigscience/bloom-1b7,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2857142857142857,0.02934457250063434
bigscience/bloom-1b7,hendrycksTest-high_school_physics,5-shot,accuracy,0.33112582781456956,0.038425817186598696
bigscience/bloom-1b7,hendrycksTest-high_school_physics,5-shot,acc_norm,0.33112582781456956,0.038425817186598696
bigscience/bloom-1b7,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3486238532110092,0.020431254090714328
bigscience/bloom-1b7,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3486238532110092,0.020431254090714328
bigscience/bloom-1b7,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
bigscience/bloom-1b7,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
bigscience/bloom-1b7,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.030587591351604246
bigscience/bloom-1b7,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.030587591351604246
bigscience/bloom-1b7,hendrycksTest-high_school_world_history,5-shot,accuracy,0.23628691983122363,0.027652153144159267
bigscience/bloom-1b7,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.23628691983122363,0.027652153144159267
bigscience/bloom-1b7,hendrycksTest-human_aging,5-shot,accuracy,0.13004484304932734,0.02257451942417487
bigscience/bloom-1b7,hendrycksTest-human_aging,5-shot,acc_norm,0.13004484304932734,0.02257451942417487
bigscience/bloom-1b7,hendrycksTest-human_sexuality,5-shot,accuracy,0.22900763358778625,0.036853466317118506
bigscience/bloom-1b7,hendrycksTest-human_sexuality,5-shot,acc_norm,0.22900763358778625,0.036853466317118506
bigscience/bloom-1b7,hendrycksTest-international_law,5-shot,accuracy,0.21487603305785125,0.037494924487096966
bigscience/bloom-1b7,hendrycksTest-international_law,5-shot,acc_norm,0.21487603305785125,0.037494924487096966
bigscience/bloom-1b7,hendrycksTest-jurisprudence,5-shot,accuracy,0.24074074074074073,0.041331194402438376
bigscience/bloom-1b7,hendrycksTest-jurisprudence,5-shot,acc_norm,0.24074074074074073,0.041331194402438376
bigscience/bloom-1b7,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26993865030674846,0.034878251684978906
bigscience/bloom-1b7,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26993865030674846,0.034878251684978906
bigscience/bloom-1b7,hendrycksTest-machine_learning,5-shot,accuracy,0.17857142857142858,0.036352091215778065
bigscience/bloom-1b7,hendrycksTest-machine_learning,5-shot,acc_norm,0.17857142857142858,0.036352091215778065
bigscience/bloom-1b7,hendrycksTest-management,5-shot,accuracy,0.3883495145631068,0.0482572933735639
bigscience/bloom-1b7,hendrycksTest-management,5-shot,acc_norm,0.3883495145631068,0.0482572933735639
bigscience/bloom-1b7,hendrycksTest-marketing,5-shot,accuracy,0.23931623931623933,0.027951826808924333
bigscience/bloom-1b7,hendrycksTest-marketing,5-shot,acc_norm,0.23931623931623933,0.027951826808924333
bigscience/bloom-1b7,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.04292346959909283
bigscience/bloom-1b7,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.04292346959909283
bigscience/bloom-1b7,hendrycksTest-miscellaneous,5-shot,accuracy,0.20561941251596424,0.014452500456785825
bigscience/bloom-1b7,hendrycksTest-miscellaneous,5-shot,acc_norm,0.20561941251596424,0.014452500456785825
bigscience/bloom-1b7,hendrycksTest-moral_disputes,5-shot,accuracy,0.26878612716763006,0.023868003262500104
bigscience/bloom-1b7,hendrycksTest-moral_disputes,5-shot,acc_norm,0.26878612716763006,0.023868003262500104
bigscience/bloom-1b7,hendrycksTest-moral_scenarios,5-shot,accuracy,0.27262569832402234,0.014893391735249588
bigscience/bloom-1b7,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.27262569832402234,0.014893391735249588
bigscience/bloom-1b7,hendrycksTest-nutrition,5-shot,accuracy,0.2777777777777778,0.025646863097137897
bigscience/bloom-1b7,hendrycksTest-nutrition,5-shot,acc_norm,0.2777777777777778,0.025646863097137897
bigscience/bloom-1b7,hendrycksTest-philosophy,5-shot,accuracy,0.2829581993569132,0.025583062489984813
bigscience/bloom-1b7,hendrycksTest-philosophy,5-shot,acc_norm,0.2829581993569132,0.025583062489984813
bigscience/bloom-1b7,hendrycksTest-prehistory,5-shot,accuracy,0.22530864197530864,0.023246202647819746
bigscience/bloom-1b7,hendrycksTest-prehistory,5-shot,acc_norm,0.22530864197530864,0.023246202647819746
bigscience/bloom-1b7,hendrycksTest-professional_accounting,5-shot,accuracy,0.2695035460992908,0.026469036818590634
bigscience/bloom-1b7,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2695035460992908,0.026469036818590634
bigscience/bloom-1b7,hendrycksTest-professional_law,5-shot,accuracy,0.2692307692307692,0.01132873440314032
bigscience/bloom-1b7,hendrycksTest-professional_law,5-shot,acc_norm,0.2692307692307692,0.01132873440314032
bigscience/bloom-1b7,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121593
bigscience/bloom-1b7,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.030211479609121593
bigscience/bloom-1b7,hendrycksTest-professional_psychology,5-shot,accuracy,0.2875816993464052,0.018311653053648222
bigscience/bloom-1b7,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2875816993464052,0.018311653053648222
bigscience/bloom-1b7,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.04350271442923243
bigscience/bloom-1b7,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.04350271442923243
bigscience/bloom-1b7,hendrycksTest-security_studies,5-shot,accuracy,0.4,0.031362502409358936
bigscience/bloom-1b7,hendrycksTest-security_studies,5-shot,acc_norm,0.4,0.031362502409358936
bigscience/bloom-1b7,hendrycksTest-sociology,5-shot,accuracy,0.263681592039801,0.03115715086935555
bigscience/bloom-1b7,hendrycksTest-sociology,5-shot,acc_norm,0.263681592039801,0.03115715086935555
bigscience/bloom-1b7,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.04408440022768078
bigscience/bloom-1b7,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.04408440022768078
bigscience/bloom-1b7,hendrycksTest-virology,5-shot,accuracy,0.19879518072289157,0.031069390260789437
bigscience/bloom-1b7,hendrycksTest-virology,5-shot,acc_norm,0.19879518072289157,0.031069390260789437
bigscience/bloom-1b7,hendrycksTest-world_religions,5-shot,accuracy,0.2573099415204678,0.03352799844161865
bigscience/bloom-1b7,hendrycksTest-world_religions,5-shot,acc_norm,0.2573099415204678,0.03352799844161865
bigscience/bloom-1b7,truthfulqa:mc,0-shot,mc1,0.24479804161566707,0.015051869486715006
bigscience/bloom-1b7,truthfulqa:mc,0-shot,mc2,0.41309062711992767,0.014435926003938946
bigscience/bloom-1b7,drop,3-shot,accuracy,0.0009437919463087249,0.000314465311941353
bigscience/bloom-1b7,drop,3-shot,f1,0.050256921140939666,0.0012661427361730828
bigscience/bloom-1b7,winogrande,5-shot,accuracy,0.5445935280189423,0.013996485037729775
EleutherAI/pythia-6.9b-deduped,arc:challenge,25-shot,accuracy,0.3728668941979522,0.014131176760131162
EleutherAI/pythia-6.9b-deduped,arc:challenge,25-shot,acc_norm,0.4129692832764505,0.014388344935398326
EleutherAI/pythia-6.9b-deduped,hellaswag,10-shot,accuracy,0.492531368253336,0.004989224715784536
EleutherAI/pythia-6.9b-deduped,hellaswag,10-shot,acc_norm,0.6704839673371839,0.004690768393854471
EleutherAI/pythia-6.9b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.044619604333847415
EleutherAI/pythia-6.9b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.044619604333847415
EleutherAI/pythia-6.9b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.23703703703703705,0.03673731683969506
EleutherAI/pythia-6.9b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.23703703703703705,0.03673731683969506
EleutherAI/pythia-6.9b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.23684210526315788,0.03459777606810535
EleutherAI/pythia-6.9b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.23684210526315788,0.03459777606810535
EleutherAI/pythia-6.9b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-6.9b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.2,0.040201512610368445
EleutherAI/pythia-6.9b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2528301886792453,0.026749899771241238
EleutherAI/pythia-6.9b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2528301886792453,0.026749899771241238
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.03653946969442099
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.03653946969442099
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.03861229196653697
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.03861229196653697
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.047937248544110196
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.047937248544110196
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.19653179190751446,0.03029957466478814
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.19653179190751446,0.03029957466478814
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617748
EleutherAI/pythia-6.9b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617748
EleutherAI/pythia-6.9b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-6.9b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.24,0.04292346959909283
EleutherAI/pythia-6.9b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2936170212765957,0.029771642712491227
EleutherAI/pythia-6.9b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2936170212765957,0.029771642712491227
EleutherAI/pythia-6.9b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
EleutherAI/pythia-6.9b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.22807017543859648,0.03947152782669415
EleutherAI/pythia-6.9b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.30344827586206896,0.038312260488503336
EleutherAI/pythia-6.9b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.30344827586206896,0.038312260488503336
EleutherAI/pythia-6.9b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24074074074074073,0.0220190800122179
EleutherAI/pythia-6.9b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24074074074074073,0.0220190800122179
EleutherAI/pythia-6.9b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.21428571428571427,0.03670066451047181
EleutherAI/pythia-6.9b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.21428571428571427,0.03670066451047181
EleutherAI/pythia-6.9b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-6.9b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.23870967741935484,0.024251071262208837
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.23870967741935484,0.024251071262208837
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3054187192118227,0.03240661565868408
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3054187192118227,0.03240661565868408
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.039427724440366255
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.039427724440366255
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23636363636363636,0.033175059300091805
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23636363636363636,0.033175059300091805
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.23737373737373738,0.030313710538198892
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.23737373737373738,0.030313710538198892
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2538860103626943,0.0314102478056532
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2538860103626943,0.0314102478056532
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.25384615384615383,0.022066054378726257
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.25384615384615383,0.022066054378726257
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26666666666666666,0.026962424325073828
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26666666666666666,0.026962424325073828
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.026841514322958938
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2184873949579832,0.026841514322958938
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.31125827814569534,0.03780445850526731
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.31125827814569534,0.03780445850526731
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.21834862385321102,0.017712600528722727
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.21834862385321102,0.017712600528722727
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.44907407407407407,0.03392238405321617
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.44907407407407407,0.03392238405321617
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.030587591351604246
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.030587591351604246
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.22784810126582278,0.027303484599069432
EleutherAI/pythia-6.9b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.22784810126582278,0.027303484599069432
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.37668161434977576,0.03252113489929188
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.37668161434977576,0.03252113489929188
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.03727673575596918
EleutherAI/pythia-6.9b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.03727673575596918
EleutherAI/pythia-6.9b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.04294340845212095
EleutherAI/pythia-6.9b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.04294340845212095
EleutherAI/pythia-6.9b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.28703703703703703,0.043733130409147614
EleutherAI/pythia-6.9b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.28703703703703703,0.043733130409147614
EleutherAI/pythia-6.9b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3128834355828221,0.036429145782924055
EleutherAI/pythia-6.9b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3128834355828221,0.036429145782924055
EleutherAI/pythia-6.9b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.33035714285714285,0.04464285714285713
EleutherAI/pythia-6.9b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.33035714285714285,0.04464285714285713
EleutherAI/pythia-6.9b-deduped,hendrycksTest-management,5-shot,accuracy,0.2815533980582524,0.04453254836326468
EleutherAI/pythia-6.9b-deduped,hendrycksTest-management,5-shot,acc_norm,0.2815533980582524,0.04453254836326468
EleutherAI/pythia-6.9b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.24786324786324787,0.028286324075564397
EleutherAI/pythia-6.9b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.24786324786324787,0.028286324075564397
EleutherAI/pythia-6.9b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-6.9b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.04292346959909283
EleutherAI/pythia-6.9b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.28991060025542786,0.016225017944770954
EleutherAI/pythia-6.9b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.28991060025542786,0.016225017944770954
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2138728323699422,0.02207570925175718
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2138728323699422,0.02207570925175718
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.014355911964767865
EleutherAI/pythia-6.9b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.014355911964767865
EleutherAI/pythia-6.9b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.25163398692810457,0.024848018263875202
EleutherAI/pythia-6.9b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.25163398692810457,0.024848018263875202
EleutherAI/pythia-6.9b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2861736334405145,0.02567025924218894
EleutherAI/pythia-6.9b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2861736334405145,0.02567025924218894
EleutherAI/pythia-6.9b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.024569223600460842
EleutherAI/pythia-6.9b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.024569223600460842
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.026789172351140242
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.026789172351140242
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.25554106910039115,0.011139857833598521
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.25554106910039115,0.011139857833598521
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.21323529411764705,0.024880971512294278
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.21323529411764705,0.024880971512294278
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2777777777777778,0.01812022425148458
EleutherAI/pythia-6.9b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2777777777777778,0.01812022425148458
EleutherAI/pythia-6.9b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-6.9b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-6.9b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.31020408163265306,0.029613459872484378
EleutherAI/pythia-6.9b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.31020408163265306,0.029613459872484378
EleutherAI/pythia-6.9b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409217
EleutherAI/pythia-6.9b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.23880597014925373,0.030147775935409217
EleutherAI/pythia-6.9b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.04512608598542127
EleutherAI/pythia-6.9b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.04512608598542127
EleutherAI/pythia-6.9b-deduped,hendrycksTest-virology,5-shot,accuracy,0.3493975903614458,0.03711725190740751
EleutherAI/pythia-6.9b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.3493975903614458,0.03711725190740751
EleutherAI/pythia-6.9b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.2982456140350877,0.03508771929824563
EleutherAI/pythia-6.9b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.2982456140350877,0.03508771929824563
EleutherAI/pythia-6.9b-deduped,truthfulqa:mc,0-shot,mc1,0.21542227662178703,0.014391902652427683
EleutherAI/pythia-6.9b-deduped,truthfulqa:mc,0-shot,mc2,0.3519458488266109,0.013230951607864657
EleutherAI/pythia-6.9b-deduped,drop,3-shot,accuracy,0.0007340604026845638,0.0002773614457335642
EleutherAI/pythia-6.9b-deduped,drop,3-shot,f1,0.04495805369127533,0.0011424943224633687
EleutherAI/pythia-6.9b-deduped,gsm8k,5-shot,accuracy,0.016679302501895376,0.003527595888722438
EleutherAI/pythia-6.9b-deduped,winogrande,5-shot,accuracy,0.6408839779005525,0.013483115202120236
Qwen/Qwen-7B,arc:challenge,25-shot,accuracy,0.49658703071672355,0.014611050403244081
Qwen/Qwen-7B,arc:challenge,25-shot,acc_norm,0.5136518771331058,0.014605943429860954
Qwen/Qwen-7B,hellaswag,10-shot,accuracy,0.5759808803027285,0.004931831953800039
Qwen/Qwen-7B,hellaswag,10-shot,acc_norm,0.7847042421828321,0.004101873407354696
Qwen/Qwen-7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.04648231987117316
Qwen/Qwen-7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.04648231987117316
Qwen/Qwen-7B,hendrycksTest-anatomy,5-shot,accuracy,0.5037037037037037,0.04319223625811331
Qwen/Qwen-7B,hendrycksTest-anatomy,5-shot,acc_norm,0.5037037037037037,0.04319223625811331
Qwen/Qwen-7B,hendrycksTest-astronomy,5-shot,accuracy,0.6447368421052632,0.038947344870133176
Qwen/Qwen-7B,hendrycksTest-astronomy,5-shot,acc_norm,0.6447368421052632,0.038947344870133176
Qwen/Qwen-7B,hendrycksTest-business_ethics,5-shot,accuracy,0.68,0.046882617226215034
Qwen/Qwen-7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.68,0.046882617226215034
Qwen/Qwen-7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.630188679245283,0.029711421880107933
Qwen/Qwen-7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.630188679245283,0.029711421880107933
Qwen/Qwen-7B,hendrycksTest-college_biology,5-shot,accuracy,0.6805555555555556,0.038990736873573344
Qwen/Qwen-7B,hendrycksTest-college_biology,5-shot,acc_norm,0.6805555555555556,0.038990736873573344
Qwen/Qwen-7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.48,0.050211673156867795
Qwen/Qwen-7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.48,0.050211673156867795
Qwen/Qwen-7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.53,0.050161355804659205
Qwen/Qwen-7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.53,0.050161355804659205
Qwen/Qwen-7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
Qwen/Qwen-7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.046056618647183814
Qwen/Qwen-7B,hendrycksTest-college_medicine,5-shot,accuracy,0.5838150289017341,0.03758517775404947
Qwen/Qwen-7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.5838150289017341,0.03758517775404947
Qwen/Qwen-7B,hendrycksTest-college_physics,5-shot,accuracy,0.37254901960784315,0.04810840148082635
Qwen/Qwen-7B,hendrycksTest-college_physics,5-shot,acc_norm,0.37254901960784315,0.04810840148082635
Qwen/Qwen-7B,hendrycksTest-computer_security,5-shot,accuracy,0.73,0.044619604333847394
Qwen/Qwen-7B,hendrycksTest-computer_security,5-shot,acc_norm,0.73,0.044619604333847394
Qwen/Qwen-7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.5319148936170213,0.03261936918467381
Qwen/Qwen-7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.5319148936170213,0.03261936918467381
Qwen/Qwen-7B,hendrycksTest-econometrics,5-shot,accuracy,0.34210526315789475,0.04462917535336937
Qwen/Qwen-7B,hendrycksTest-econometrics,5-shot,acc_norm,0.34210526315789475,0.04462917535336937
Qwen/Qwen-7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.593103448275862,0.04093793981266236
Qwen/Qwen-7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.593103448275862,0.04093793981266236
Qwen/Qwen-7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.42857142857142855,0.02548718714785938
Qwen/Qwen-7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.42857142857142855,0.02548718714785938
Qwen/Qwen-7B,hendrycksTest-formal_logic,5-shot,accuracy,0.42063492063492064,0.04415438226743744
Qwen/Qwen-7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.42063492063492064,0.04415438226743744
Qwen/Qwen-7B,hendrycksTest-global_facts,5-shot,accuracy,0.38,0.048783173121456316
Qwen/Qwen-7B,hendrycksTest-global_facts,5-shot,acc_norm,0.38,0.048783173121456316
Qwen/Qwen-7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.7193548387096774,0.02556060472102289
Qwen/Qwen-7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7193548387096774,0.02556060472102289
Qwen/Qwen-7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5123152709359606,0.035169204442208966
Qwen/Qwen-7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5123152709359606,0.035169204442208966
Qwen/Qwen-7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.67,0.04725815626252607
Qwen/Qwen-7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.67,0.04725815626252607
Qwen/Qwen-7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.6666666666666666,0.036810508691615486
Qwen/Qwen-7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.6666666666666666,0.036810508691615486
Qwen/Qwen-7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.7525252525252525,0.030746300742124484
Qwen/Qwen-7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.7525252525252525,0.030746300742124484
Qwen/Qwen-7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.844559585492228,0.026148483469153303
Qwen/Qwen-7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.844559585492228,0.026148483469153303
Qwen/Qwen-7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6102564102564103,0.024726967886647074
Qwen/Qwen-7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6102564102564103,0.024726967886647074
Qwen/Qwen-7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3,0.027940457136228423
Qwen/Qwen-7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3,0.027940457136228423
Qwen/Qwen-7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.5840336134453782,0.03201650100739612
Qwen/Qwen-7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.5840336134453782,0.03201650100739612
Qwen/Qwen-7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.36423841059602646,0.03929111781242741
Qwen/Qwen-7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.36423841059602646,0.03929111781242741
Qwen/Qwen-7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.781651376146789,0.017712600528722724
Qwen/Qwen-7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.781651376146789,0.017712600528722724
Qwen/Qwen-7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4212962962962963,0.03367462138896078
Qwen/Qwen-7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4212962962962963,0.03367462138896078
Qwen/Qwen-7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.7401960784313726,0.03077855467869326
Qwen/Qwen-7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.7401960784313726,0.03077855467869326
Qwen/Qwen-7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.759493670886076,0.027820781981149685
Qwen/Qwen-7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.759493670886076,0.027820781981149685
Qwen/Qwen-7B,hendrycksTest-human_aging,5-shot,accuracy,0.6367713004484304,0.03227790442850499
Qwen/Qwen-7B,hendrycksTest-human_aging,5-shot,acc_norm,0.6367713004484304,0.03227790442850499
Qwen/Qwen-7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.7022900763358778,0.040103589424622034
Qwen/Qwen-7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7022900763358778,0.040103589424622034
Qwen/Qwen-7B,hendrycksTest-international_law,5-shot,accuracy,0.7768595041322314,0.03800754475228733
Qwen/Qwen-7B,hendrycksTest-international_law,5-shot,acc_norm,0.7768595041322314,0.03800754475228733
Qwen/Qwen-7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.7407407407407407,0.042365112580946315
Qwen/Qwen-7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.7407407407407407,0.042365112580946315
Qwen/Qwen-7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.6625766871165644,0.03714908409935573
Qwen/Qwen-7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.6625766871165644,0.03714908409935573
Qwen/Qwen-7B,hendrycksTest-machine_learning,5-shot,accuracy,0.375,0.04595091388086298
Qwen/Qwen-7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.375,0.04595091388086298
Qwen/Qwen-7B,hendrycksTest-management,5-shot,accuracy,0.7766990291262136,0.04123553189891431
Qwen/Qwen-7B,hendrycksTest-management,5-shot,acc_norm,0.7766990291262136,0.04123553189891431
Qwen/Qwen-7B,hendrycksTest-marketing,5-shot,accuracy,0.8333333333333334,0.024414947304543674
Qwen/Qwen-7B,hendrycksTest-marketing,5-shot,acc_norm,0.8333333333333334,0.024414947304543674
Qwen/Qwen-7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.71,0.04560480215720685
Qwen/Qwen-7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.71,0.04560480215720685
Qwen/Qwen-7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.7841634738186463,0.014711684386139953
Qwen/Qwen-7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.7841634738186463,0.014711684386139953
Qwen/Qwen-7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.6676300578034682,0.025361168749688225
Qwen/Qwen-7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.6676300578034682,0.025361168749688225
Qwen/Qwen-7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.31508379888268156,0.01553685085247363
Qwen/Qwen-7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.31508379888268156,0.01553685085247363
Qwen/Qwen-7B,hendrycksTest-nutrition,5-shot,accuracy,0.6764705882352942,0.026787453111906494
Qwen/Qwen-7B,hendrycksTest-nutrition,5-shot,acc_norm,0.6764705882352942,0.026787453111906494
Qwen/Qwen-7B,hendrycksTest-philosophy,5-shot,accuracy,0.6752411575562701,0.026596782287697043
Qwen/Qwen-7B,hendrycksTest-philosophy,5-shot,acc_norm,0.6752411575562701,0.026596782287697043
Qwen/Qwen-7B,hendrycksTest-prehistory,5-shot,accuracy,0.6882716049382716,0.025773111169630457
Qwen/Qwen-7B,hendrycksTest-prehistory,5-shot,acc_norm,0.6882716049382716,0.025773111169630457
Qwen/Qwen-7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.42907801418439717,0.029525914302558555
Qwen/Qwen-7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.42907801418439717,0.029525914302558555
Qwen/Qwen-7B,hendrycksTest-professional_law,5-shot,accuracy,0.4198174706649283,0.01260496081608737
Qwen/Qwen-7B,hendrycksTest-professional_law,5-shot,acc_norm,0.4198174706649283,0.01260496081608737
Qwen/Qwen-7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.5735294117647058,0.030042615832714867
Qwen/Qwen-7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.5735294117647058,0.030042615832714867
Qwen/Qwen-7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.5931372549019608,0.019873802005061173
Qwen/Qwen-7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.5931372549019608,0.019873802005061173
Qwen/Qwen-7B,hendrycksTest-public_relations,5-shot,accuracy,0.6545454545454545,0.04554619617541054
Qwen/Qwen-7B,hendrycksTest-public_relations,5-shot,acc_norm,0.6545454545454545,0.04554619617541054
Qwen/Qwen-7B,hendrycksTest-security_studies,5-shot,accuracy,0.7061224489795919,0.02916273841024977
Qwen/Qwen-7B,hendrycksTest-security_studies,5-shot,acc_norm,0.7061224489795919,0.02916273841024977
Qwen/Qwen-7B,hendrycksTest-sociology,5-shot,accuracy,0.8059701492537313,0.0279626776047689
Qwen/Qwen-7B,hendrycksTest-sociology,5-shot,acc_norm,0.8059701492537313,0.0279626776047689
Qwen/Qwen-7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.8,0.04020151261036845
Qwen/Qwen-7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.8,0.04020151261036845
Qwen/Qwen-7B,hendrycksTest-virology,5-shot,accuracy,0.4819277108433735,0.038899512528272166
Qwen/Qwen-7B,hendrycksTest-virology,5-shot,acc_norm,0.4819277108433735,0.038899512528272166
Qwen/Qwen-7B,hendrycksTest-world_religions,5-shot,accuracy,0.7543859649122807,0.03301405946987251
Qwen/Qwen-7B,hendrycksTest-world_religions,5-shot,acc_norm,0.7543859649122807,0.03301405946987251
Qwen/Qwen-7B,truthfulqa:mc,0-shot,mc1,0.3243574051407589,0.016387976779647935
Qwen/Qwen-7B,truthfulqa:mc,0-shot,mc2,0.4778593054989757,0.015018777696051509
Qwen/Qwen-7B,drop,3-shot,accuracy,0.04163171140939597,0.0020455872163586053
Qwen/Qwen-7B,drop,3-shot,f1,0.0925429949664433,0.002229779833344395
Qwen/Qwen-7B,gsm8k,5-shot,accuracy,0.4495830174374526,0.013702290047884744
Qwen/Qwen-7B,winogrande,5-shot,accuracy,0.7269139700078927,0.012522020105869457
01-ai/Yi-34B,arc:challenge,25-shot,accuracy,0.6160409556313993,0.01421244498065189
01-ai/Yi-34B,arc:challenge,25-shot,acc_norm,0.6459044368600683,0.01397545412275656
01-ai/Yi-34B,hellaswag,10-shot,accuracy,0.6568412666799442,0.004737936758047645
01-ai/Yi-34B,hellaswag,10-shot,acc_norm,0.8558056164110734,0.0035056879433872637
01-ai/Yi-34B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.45,0.049999999999999996
01-ai/Yi-34B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.45,0.049999999999999996
01-ai/Yi-34B,hendrycksTest-anatomy,5-shot,accuracy,0.7481481481481481,0.03749850709174021
01-ai/Yi-34B,hendrycksTest-anatomy,5-shot,acc_norm,0.7481481481481481,0.03749850709174021
01-ai/Yi-34B,hendrycksTest-astronomy,5-shot,accuracy,0.9013157894736842,0.024270227737522715
01-ai/Yi-34B,hendrycksTest-astronomy,5-shot,acc_norm,0.9013157894736842,0.024270227737522715
01-ai/Yi-34B,hendrycksTest-business_ethics,5-shot,accuracy,0.79,0.040936018074033256
01-ai/Yi-34B,hendrycksTest-business_ethics,5-shot,acc_norm,0.79,0.040936018074033256
01-ai/Yi-34B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7924528301886793,0.02495991802891127
01-ai/Yi-34B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7924528301886793,0.02495991802891127
01-ai/Yi-34B,hendrycksTest-college_biology,5-shot,accuracy,0.8819444444444444,0.026983346503309354
01-ai/Yi-34B,hendrycksTest-college_biology,5-shot,acc_norm,0.8819444444444444,0.026983346503309354
01-ai/Yi-34B,hendrycksTest-college_chemistry,5-shot,accuracy,0.49,0.05024183937956912
01-ai/Yi-34B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.49,0.05024183937956912
01-ai/Yi-34B,hendrycksTest-college_computer_science,5-shot,accuracy,0.65,0.04793724854411019
01-ai/Yi-34B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.65,0.04793724854411019
01-ai/Yi-34B,hendrycksTest-college_mathematics,5-shot,accuracy,0.48,0.050211673156867795
01-ai/Yi-34B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.48,0.050211673156867795
01-ai/Yi-34B,hendrycksTest-college_medicine,5-shot,accuracy,0.7109826589595376,0.03456425745086999
01-ai/Yi-34B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7109826589595376,0.03456425745086999
01-ai/Yi-34B,hendrycksTest-college_physics,5-shot,accuracy,0.5,0.04975185951049946
01-ai/Yi-34B,hendrycksTest-college_physics,5-shot,acc_norm,0.5,0.04975185951049946
01-ai/Yi-34B,hendrycksTest-computer_security,5-shot,accuracy,0.82,0.03861229196653694
01-ai/Yi-34B,hendrycksTest-computer_security,5-shot,acc_norm,0.82,0.03861229196653694
01-ai/Yi-34B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.7702127659574468,0.02750175294441242
01-ai/Yi-34B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.7702127659574468,0.02750175294441242
01-ai/Yi-34B,hendrycksTest-econometrics,5-shot,accuracy,0.5526315789473685,0.04677473004491199
01-ai/Yi-34B,hendrycksTest-econometrics,5-shot,acc_norm,0.5526315789473685,0.04677473004491199
01-ai/Yi-34B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.8,0.0333333333333333
01-ai/Yi-34B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.8,0.0333333333333333
01-ai/Yi-34B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.656084656084656,0.024464426625596437
01-ai/Yi-34B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.656084656084656,0.024464426625596437
01-ai/Yi-34B,hendrycksTest-formal_logic,5-shot,accuracy,0.5634920634920635,0.04435932892851466
01-ai/Yi-34B,hendrycksTest-formal_logic,5-shot,acc_norm,0.5634920634920635,0.04435932892851466
01-ai/Yi-34B,hendrycksTest-global_facts,5-shot,accuracy,0.52,0.050211673156867795
01-ai/Yi-34B,hendrycksTest-global_facts,5-shot,acc_norm,0.52,0.050211673156867795
01-ai/Yi-34B,hendrycksTest-high_school_biology,5-shot,accuracy,0.8806451612903226,0.018443411325315393
01-ai/Yi-34B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8806451612903226,0.018443411325315393
01-ai/Yi-34B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.645320197044335,0.03366124489051449
01-ai/Yi-34B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.645320197044335,0.03366124489051449
01-ai/Yi-34B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.82,0.038612291966536955
01-ai/Yi-34B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.82,0.038612291966536955
01-ai/Yi-34B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8666666666666667,0.026544435312706473
01-ai/Yi-34B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8666666666666667,0.026544435312706473
01-ai/Yi-34B,hendrycksTest-high_school_geography,5-shot,accuracy,0.8939393939393939,0.021938047738853106
01-ai/Yi-34B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8939393939393939,0.021938047738853106
01-ai/Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9792746113989638,0.010281417011909042
01-ai/Yi-34B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9792746113989638,0.010281417011909042
01-ai/Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8,0.020280805062535726
01-ai/Yi-34B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8,0.020280805062535726
01-ai/Yi-34B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4444444444444444,0.030296771286067323
01-ai/Yi-34B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4444444444444444,0.030296771286067323
01-ai/Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8571428571428571,0.02273020811930654
01-ai/Yi-34B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8571428571428571,0.02273020811930654
01-ai/Yi-34B,hendrycksTest-high_school_physics,5-shot,accuracy,0.5165562913907285,0.04080244185628972
01-ai/Yi-34B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5165562913907285,0.04080244185628972
01-ai/Yi-34B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9155963302752294,0.011918819327334877
01-ai/Yi-34B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9155963302752294,0.011918819327334877
01-ai/Yi-34B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6388888888888888,0.032757734861009996
01-ai/Yi-34B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6388888888888888,0.032757734861009996
01-ai/Yi-34B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9166666666666666,0.019398452135813905
01-ai/Yi-34B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9166666666666666,0.019398452135813905
01-ai/Yi-34B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.919831223628692,0.017676679991891625
01-ai/Yi-34B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.919831223628692,0.017676679991891625
01-ai/Yi-34B,hendrycksTest-human_aging,5-shot,accuracy,0.7937219730941704,0.027157150479563824
01-ai/Yi-34B,hendrycksTest-human_aging,5-shot,acc_norm,0.7937219730941704,0.027157150479563824
01-ai/Yi-34B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8625954198473282,0.030194823996804475
01-ai/Yi-34B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8625954198473282,0.030194823996804475
01-ai/Yi-34B,hendrycksTest-international_law,5-shot,accuracy,0.9090909090909091,0.02624319405407388
01-ai/Yi-34B,hendrycksTest-international_law,5-shot,acc_norm,0.9090909090909091,0.02624319405407388
01-ai/Yi-34B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8888888888888888,0.03038159675665167
01-ai/Yi-34B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8888888888888888,0.03038159675665167
01-ai/Yi-34B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8834355828220859,0.025212327210507108
01-ai/Yi-34B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8834355828220859,0.025212327210507108
01-ai/Yi-34B,hendrycksTest-machine_learning,5-shot,accuracy,0.5982142857142857,0.04653333146973647
01-ai/Yi-34B,hendrycksTest-machine_learning,5-shot,acc_norm,0.5982142857142857,0.04653333146973647
01-ai/Yi-34B,hendrycksTest-management,5-shot,accuracy,0.912621359223301,0.027960689125970654
01-ai/Yi-34B,hendrycksTest-management,5-shot,acc_norm,0.912621359223301,0.027960689125970654
01-ai/Yi-34B,hendrycksTest-marketing,5-shot,accuracy,0.9316239316239316,0.01653462768431136
01-ai/Yi-34B,hendrycksTest-marketing,5-shot,acc_norm,0.9316239316239316,0.01653462768431136
01-ai/Yi-34B,hendrycksTest-medical_genetics,5-shot,accuracy,0.87,0.033799766898963086
01-ai/Yi-34B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.87,0.033799766898963086
01-ai/Yi-34B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9054916985951469,0.01046101533819307
01-ai/Yi-34B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9054916985951469,0.01046101533819307
01-ai/Yi-34B,hendrycksTest-moral_disputes,5-shot,accuracy,0.8294797687861272,0.020247961569303728
01-ai/Yi-34B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8294797687861272,0.020247961569303728
01-ai/Yi-34B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6446927374301676,0.016006989934803192
01-ai/Yi-34B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6446927374301676,0.016006989934803192
01-ai/Yi-34B,hendrycksTest-nutrition,5-shot,accuracy,0.8627450980392157,0.01970403918385981
01-ai/Yi-34B,hendrycksTest-nutrition,5-shot,acc_norm,0.8627450980392157,0.01970403918385981
01-ai/Yi-34B,hendrycksTest-philosophy,5-shot,accuracy,0.8392282958199357,0.020862388082391888
01-ai/Yi-34B,hendrycksTest-philosophy,5-shot,acc_norm,0.8392282958199357,0.020862388082391888
01-ai/Yi-34B,hendrycksTest-prehistory,5-shot,accuracy,0.8827160493827161,0.01790311261528112
01-ai/Yi-34B,hendrycksTest-prehistory,5-shot,acc_norm,0.8827160493827161,0.01790311261528112
01-ai/Yi-34B,hendrycksTest-professional_accounting,5-shot,accuracy,0.6702127659574468,0.02804594694204241
01-ai/Yi-34B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6702127659574468,0.02804594694204241
01-ai/Yi-34B,hendrycksTest-professional_law,5-shot,accuracy,0.6049543676662321,0.01248572781325157
01-ai/Yi-34B,hendrycksTest-professional_law,5-shot,acc_norm,0.6049543676662321,0.01248572781325157
01-ai/Yi-34B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8125,0.023709788253811766
01-ai/Yi-34B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8125,0.023709788253811766
01-ai/Yi-34B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8186274509803921,0.015588643495370457
01-ai/Yi-34B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8186274509803921,0.015588643495370457
01-ai/Yi-34B,hendrycksTest-public_relations,5-shot,accuracy,0.7363636363636363,0.04220224692971987
01-ai/Yi-34B,hendrycksTest-public_relations,5-shot,acc_norm,0.7363636363636363,0.04220224692971987
01-ai/Yi-34B,hendrycksTest-security_studies,5-shot,accuracy,0.8448979591836735,0.0231747988612186
01-ai/Yi-34B,hendrycksTest-security_studies,5-shot,acc_norm,0.8448979591836735,0.0231747988612186
01-ai/Yi-34B,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.022076326101824657
01-ai/Yi-34B,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.022076326101824657
01-ai/Yi-34B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.9,0.030151134457776334
01-ai/Yi-34B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.9,0.030151134457776334
01-ai/Yi-34B,hendrycksTest-virology,5-shot,accuracy,0.5783132530120482,0.038444531817709175
01-ai/Yi-34B,hendrycksTest-virology,5-shot,acc_norm,0.5783132530120482,0.038444531817709175
01-ai/Yi-34B,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.02517298435015578
01-ai/Yi-34B,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.02517298435015578
01-ai/Yi-34B,truthfulqa:mc,0-shot,mc1,0.40758873929008566,0.017201949234553107
01-ai/Yi-34B,truthfulqa:mc,0-shot,mc2,0.5623083932983032,0.015165963671039869
01-ai/Yi-34B,drop,3-shot,accuracy,0.6081166107382551,0.004999326629880105
01-ai/Yi-34B,drop,3-shot,f1,0.6419882550335565,0.004748239351156368
01-ai/Yi-34B,gsm8k,5-shot,accuracy,0.6557998483699773,0.013086800426693782
01-ai/Yi-34B,winogrande,5-shot,accuracy,0.8287292817679558,0.010588417294962526
01-ai/Yi-34B,mmlu_world_religions,0-shot,accuracy,0.8654970760233918,0.026168221344662294
01-ai/Yi-34B,mmlu_formal_logic,0-shot,accuracy,0.5555555555555556,0.04444444444444449
01-ai/Yi-34B,mmlu_prehistory,0-shot,accuracy,0.8734567901234568,0.018498600558790917
01-ai/Yi-34B,mmlu_moral_scenarios,0-shot,accuracy,0.6458100558659218,0.015995644947299225
01-ai/Yi-34B,mmlu_high_school_world_history,0-shot,accuracy,0.9240506329113924,0.017244633251065688
01-ai/Yi-34B,mmlu_moral_disputes,0-shot,accuracy,0.8323699421965318,0.02011057991973484
01-ai/Yi-34B,mmlu_professional_law,0-shot,accuracy,0.5971316818774446,0.01252695557711801
01-ai/Yi-34B,mmlu_logical_fallacies,0-shot,accuracy,0.8834355828220859,0.025212327210507094
01-ai/Yi-34B,mmlu_high_school_us_history,0-shot,accuracy,0.9117647058823529,0.019907399791316942
01-ai/Yi-34B,mmlu_philosophy,0-shot,accuracy,0.8392282958199357,0.02086238808239191
01-ai/Yi-34B,mmlu_jurisprudence,0-shot,accuracy,0.8981481481481481,0.02923927267563274
01-ai/Yi-34B,mmlu_international_law,0-shot,accuracy,0.9173553719008265,0.02513538235660422
01-ai/Yi-34B,mmlu_high_school_european_history,0-shot,accuracy,0.8666666666666667,0.026544435312706473
01-ai/Yi-34B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9792746113989638,0.010281417011909025
01-ai/Yi-34B,mmlu_high_school_microeconomics,0-shot,accuracy,0.8529411764705882,0.023005459446673957
01-ai/Yi-34B,mmlu_high_school_geography,0-shot,accuracy,0.8888888888888888,0.022390787638216763
01-ai/Yi-34B,mmlu_high_school_psychology,0-shot,accuracy,0.9155963302752294,0.011918819327334889
01-ai/Yi-34B,mmlu_public_relations,0-shot,accuracy,0.7363636363636363,0.04220224692971987
01-ai/Yi-34B,mmlu_us_foreign_policy,0-shot,accuracy,0.91,0.028762349126466125
01-ai/Yi-34B,mmlu_sociology,0-shot,accuracy,0.8855721393034826,0.022509345325101703
01-ai/Yi-34B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7871794871794872,0.020752423722128013
01-ai/Yi-34B,mmlu_security_studies,0-shot,accuracy,0.8530612244897959,0.02266540041721763
01-ai/Yi-34B,mmlu_professional_psychology,0-shot,accuracy,0.8235294117647058,0.015422512066262547
01-ai/Yi-34B,mmlu_human_sexuality,0-shot,accuracy,0.8625954198473282,0.030194823996804475
01-ai/Yi-34B,mmlu_econometrics,0-shot,accuracy,0.5526315789473685,0.04677473004491199
01-ai/Yi-34B,mmlu_miscellaneous,0-shot,accuracy,0.9054916985951469,0.010461015338193071
01-ai/Yi-34B,mmlu_marketing,0-shot,accuracy,0.9273504273504274,0.017004368568132353
01-ai/Yi-34B,mmlu_management,0-shot,accuracy,0.912621359223301,0.027960689125970658
01-ai/Yi-34B,mmlu_nutrition,0-shot,accuracy,0.869281045751634,0.019301873624215298
01-ai/Yi-34B,mmlu_medical_genetics,0-shot,accuracy,0.87,0.03379976689896309
01-ai/Yi-34B,mmlu_human_aging,0-shot,accuracy,0.7892376681614349,0.027373095500540186
01-ai/Yi-34B,mmlu_professional_medicine,0-shot,accuracy,0.8235294117647058,0.02315746830855936
01-ai/Yi-34B,mmlu_college_medicine,0-shot,accuracy,0.7052023121387283,0.03476599607516478
01-ai/Yi-34B,mmlu_business_ethics,0-shot,accuracy,0.79,0.040936018074033256
01-ai/Yi-34B,mmlu_clinical_knowledge,0-shot,accuracy,0.7924528301886793,0.02495991802891127
01-ai/Yi-34B,mmlu_global_facts,0-shot,accuracy,0.52,0.050211673156867795
01-ai/Yi-34B,mmlu_virology,0-shot,accuracy,0.572289156626506,0.038515976837185335
01-ai/Yi-34B,mmlu_professional_accounting,0-shot,accuracy,0.6631205673758865,0.028195534873966734
01-ai/Yi-34B,mmlu_college_physics,0-shot,accuracy,0.4803921568627451,0.04971358884367406
01-ai/Yi-34B,mmlu_high_school_physics,0-shot,accuracy,0.5231788079470199,0.04078093859163084
01-ai/Yi-34B,mmlu_high_school_biology,0-shot,accuracy,0.8806451612903226,0.01844341132531541
01-ai/Yi-34B,mmlu_college_biology,0-shot,accuracy,0.875,0.02765610492929436
01-ai/Yi-34B,mmlu_anatomy,0-shot,accuracy,0.7481481481481481,0.037498507091740206
01-ai/Yi-34B,mmlu_college_chemistry,0-shot,accuracy,0.48,0.050211673156867795
01-ai/Yi-34B,mmlu_computer_security,0-shot,accuracy,0.82,0.03861229196653695
01-ai/Yi-34B,mmlu_college_computer_science,0-shot,accuracy,0.64,0.04824181513244218
01-ai/Yi-34B,mmlu_astronomy,0-shot,accuracy,0.9013157894736842,0.024270227737522732
01-ai/Yi-34B,mmlu_college_mathematics,0-shot,accuracy,0.48,0.050211673156867795
01-ai/Yi-34B,mmlu_conceptual_physics,0-shot,accuracy,0.774468085106383,0.02732107841738753
01-ai/Yi-34B,mmlu_abstract_algebra,0-shot,accuracy,0.44,0.04988876515698589
01-ai/Yi-34B,mmlu_high_school_computer_science,0-shot,accuracy,0.82,0.038612291966536934
01-ai/Yi-34B,mmlu_machine_learning,0-shot,accuracy,0.6160714285714286,0.04616143075028547
01-ai/Yi-34B,mmlu_high_school_chemistry,0-shot,accuracy,0.6403940886699507,0.03376458246509567
01-ai/Yi-34B,mmlu_high_school_statistics,0-shot,accuracy,0.6481481481481481,0.032568505702936464
01-ai/Yi-34B,mmlu_elementary_mathematics,0-shot,accuracy,0.6507936507936508,0.02455229220934265
01-ai/Yi-34B,mmlu_electrical_engineering,0-shot,accuracy,0.8,0.033333333333333305
01-ai/Yi-34B,mmlu_high_school_mathematics,0-shot,accuracy,0.4666666666666667,0.03041771696171748
01-ai/Yi-34B,arc_challenge,25-shot,accuracy,0.6049488054607508,0.014285898292938163
01-ai/Yi-34B,arc_challenge,25-shot,acc_norm,0.6459044368600683,0.013975454122756555
01-ai/Yi-34B,truthfulqa_mc2,0-shot,accuracy,0.562494191768382,0.015156214731267605
01-ai/Yi-34B,truthfulqa_gen,0-shot,bleu_max,20.330647080409832,0.7024796684315535
01-ai/Yi-34B,truthfulqa_gen,0-shot,bleu_acc,0.48714810281517745,0.01749771794429982
01-ai/Yi-34B,truthfulqa_gen,0-shot,bleu_diff,1.2034859228193233,0.588166050547877
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge1_max,45.71688577672627,0.8227479818577124
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge1_acc,0.5140758873929009,0.017496563717042796
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge1_diff,2.5276609445648797,0.781731669558233
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge2_max,29.910346423305118,0.9393337389294315
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge2_acc,0.3769889840881273,0.016965517578930358
01-ai/Yi-34B,truthfulqa_gen,0-shot,rouge2_diff,0.485430202097819,0.8945392225724594
01-ai/Yi-34B,truthfulqa_gen,0-shot,rougeL_max,42.9851750782942,0.8216423235693433
01-ai/Yi-34B,truthfulqa_gen,0-shot,rougeL_acc,0.5361077111383109,0.017457800422268622
01-ai/Yi-34B,truthfulqa_gen,0-shot,rougeL_diff,2.5484074342195404,0.7888693977638205
01-ai/Yi-34B,truthfulqa_mc1,0-shot,accuracy,0.40758873929008566,0.01720194923455311
Salesforce/codegen-6B-multi,minerva_math_precalc,5-shot,accuracy,0.01098901098901099,0.004465618427331412
Salesforce/codegen-6B-multi,minerva_math_prealgebra,5-shot,accuracy,0.001148105625717566,0.00114810562571757
Salesforce/codegen-6B-multi,minerva_math_num_theory,5-shot,accuracy,0.001851851851851852,0.001851851851851851
Salesforce/codegen-6B-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.009966777408637873,0.0033074934669720295
Salesforce/codegen-6B-multi,minerva_math_geometry,5-shot,accuracy,0.0020876826722338203,0.0020876826722338333
Salesforce/codegen-6B-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.006329113924050633,0.0036463820410650577
Salesforce/codegen-6B-multi,minerva_math_algebra,5-shot,accuracy,0.008424599831508003,0.0026539648581165158
Salesforce/codegen-6B-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,arithmetic_3da,5-shot,accuracy,0.004,0.0014117352790976934
Salesforce/codegen-6B-multi,arithmetic_3ds,5-shot,accuracy,0.009,0.002112280962711353
Salesforce/codegen-6B-multi,arithmetic_4da,5-shot,accuracy,0.002,0.0009992493430694928
Salesforce/codegen-6B-multi,arithmetic_2ds,5-shot,accuracy,0.0615,0.005373389214995324
Salesforce/codegen-6B-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-6B-multi,arithmetic_1dc,5-shot,accuracy,0.093,0.0064958908780204825
Salesforce/codegen-6B-multi,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005000000000000068
Salesforce/codegen-6B-multi,arithmetic_2dm,5-shot,accuracy,0.039,0.0043299970481765525
Salesforce/codegen-6B-multi,arithmetic_2da,5-shot,accuracy,0.052,0.004965916850399523
Salesforce/codegen-6B-multi,gsm8k_cot,5-shot,accuracy,0.026535253980288095,0.004427045987265165
Salesforce/codegen-6B-multi,gsm8k,5-shot,accuracy,0.009855951478392721,0.0027210765770416634
Salesforce/codegen-6B-multi,anli_r2,0-shot,brier_score,0.7472883683198149,
Salesforce/codegen-6B-multi,anli_r3,0-shot,brier_score,0.7776246952766512,
Salesforce/codegen-6B-multi,anli_r1,0-shot,brier_score,0.7736621012350583,
Salesforce/codegen-6B-multi,xnli_eu,0-shot,brier_score,1.0388889959896963,
Salesforce/codegen-6B-multi,xnli_vi,0-shot,brier_score,1.2154173472405334,
Salesforce/codegen-6B-multi,xnli_ru,0-shot,brier_score,0.8730313226525406,
Salesforce/codegen-6B-multi,xnli_zh,0-shot,brier_score,0.9828620736086736,
Salesforce/codegen-6B-multi,xnli_tr,0-shot,brier_score,1.0519111336327807,
Salesforce/codegen-6B-multi,xnli_fr,0-shot,brier_score,1.0820785121586356,
Salesforce/codegen-6B-multi,xnli_en,0-shot,brier_score,0.7349387098851814,
Salesforce/codegen-6B-multi,xnli_ur,0-shot,brier_score,1.3175247071589906,
Salesforce/codegen-6B-multi,xnli_ar,0-shot,brier_score,1.0337470838746723,
Salesforce/codegen-6B-multi,xnli_de,0-shot,brier_score,0.9493973793623973,
Salesforce/codegen-6B-multi,xnli_hi,0-shot,brier_score,0.9824796835327766,
Salesforce/codegen-6B-multi,xnli_es,0-shot,brier_score,1.2007293365919631,
Salesforce/codegen-6B-multi,xnli_bg,0-shot,brier_score,0.9574865362674579,
Salesforce/codegen-6B-multi,xnli_sw,0-shot,brier_score,0.8975406853203322,
Salesforce/codegen-6B-multi,xnli_el,0-shot,brier_score,0.8985200188087877,
Salesforce/codegen-6B-multi,xnli_th,0-shot,brier_score,0.9106758374214105,
Salesforce/codegen-6B-multi,logiqa2,0-shot,brier_score,1.1702528650156314,
Salesforce/codegen-6B-multi,mathqa,5-shot,brier_score,0.959189809900026,
Salesforce/codegen-6B-multi,lambada_standard,0-shot,perplexity,71.09167247462723,2.933823524943788
Salesforce/codegen-6B-multi,lambada_standard,0-shot,accuracy,0.25441490393945276,0.006067809764031531
Salesforce/codegen-6B-multi,lambada_openai,0-shot,perplexity,46.16956426063179,1.8273643949515241
Salesforce/codegen-6B-multi,lambada_openai,0-shot,accuracy,0.3027362701339026,0.006400926467529961
Salesforce/codegen-6B-multi,mmlu_world_religions,0-shot,accuracy,0.2807017543859649,0.03446296217088427
Salesforce/codegen-6B-multi,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.03718489006818114
Salesforce/codegen-6B-multi,mmlu_prehistory,0-shot,accuracy,0.23765432098765432,0.02368359183700856
Salesforce/codegen-6B-multi,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.01435591196476786
Salesforce/codegen-6B-multi,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
Salesforce/codegen-6B-multi,mmlu_moral_disputes,0-shot,accuracy,0.2861271676300578,0.02433214677913413
Salesforce/codegen-6B-multi,mmlu_professional_law,0-shot,accuracy,0.258148631029987,0.011176923719313394
Salesforce/codegen-6B-multi,mmlu_logical_fallacies,0-shot,accuracy,0.27607361963190186,0.03512385283705051
Salesforce/codegen-6B-multi,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
Salesforce/codegen-6B-multi,mmlu_philosophy,0-shot,accuracy,0.3022508038585209,0.02608270069539966
Salesforce/codegen-6B-multi,mmlu_jurisprudence,0-shot,accuracy,0.23148148148148148,0.04077494709252627
Salesforce/codegen-6B-multi,mmlu_international_law,0-shot,accuracy,0.3305785123966942,0.04294340845212095
Salesforce/codegen-6B-multi,mmlu_high_school_european_history,0-shot,accuracy,0.2787878787878788,0.03501438706296781
Salesforce/codegen-6B-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20725388601036268,0.029252823291803627
Salesforce/codegen-6B-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.25630252100840334,0.02835962087053395
Salesforce/codegen-6B-multi,mmlu_high_school_geography,0-shot,accuracy,0.20707070707070707,0.028869778460267063
Salesforce/codegen-6B-multi,mmlu_high_school_psychology,0-shot,accuracy,0.25321100917431194,0.018644073041375046
Salesforce/codegen-6B-multi,mmlu_public_relations,0-shot,accuracy,0.3181818181818182,0.04461272175910509
Salesforce/codegen-6B-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.18,0.03861229196653697
Salesforce/codegen-6B-multi,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409217
Salesforce/codegen-6B-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2205128205128205,0.021020672680827912
Salesforce/codegen-6B-multi,mmlu_security_studies,0-shot,accuracy,0.17551020408163265,0.02435280072297001
Salesforce/codegen-6B-multi,mmlu_professional_psychology,0-shot,accuracy,0.2565359477124183,0.017667841612378988
Salesforce/codegen-6B-multi,mmlu_human_sexuality,0-shot,accuracy,0.1984732824427481,0.034981493854624714
Salesforce/codegen-6B-multi,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.04185774424022056
Salesforce/codegen-6B-multi,mmlu_miscellaneous,0-shot,accuracy,0.28607918263090676,0.01616087140512753
Salesforce/codegen-6B-multi,mmlu_marketing,0-shot,accuracy,0.2264957264957265,0.02742100729539292
Salesforce/codegen-6B-multi,mmlu_management,0-shot,accuracy,0.17475728155339806,0.0376017800602662
Salesforce/codegen-6B-multi,mmlu_nutrition,0-shot,accuracy,0.26143790849673204,0.025160998214292456
Salesforce/codegen-6B-multi,mmlu_medical_genetics,0-shot,accuracy,0.26,0.04408440022768078
Salesforce/codegen-6B-multi,mmlu_human_aging,0-shot,accuracy,0.3901345291479821,0.03273766725459157
Salesforce/codegen-6B-multi,mmlu_professional_medicine,0-shot,accuracy,0.19117647058823528,0.023886881922440328
Salesforce/codegen-6B-multi,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
Salesforce/codegen-6B-multi,mmlu_business_ethics,0-shot,accuracy,0.26,0.044084400227680794
Salesforce/codegen-6B-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.27169811320754716,0.027377706624670713
Salesforce/codegen-6B-multi,mmlu_global_facts,0-shot,accuracy,0.24,0.042923469599092816
Salesforce/codegen-6B-multi,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.036108050180310235
Salesforce/codegen-6B-multi,mmlu_professional_accounting,0-shot,accuracy,0.22695035460992907,0.024987106365642973
Salesforce/codegen-6B-multi,mmlu_college_physics,0-shot,accuracy,0.19607843137254902,0.0395058186117996
Salesforce/codegen-6B-multi,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.03684881521389024
Salesforce/codegen-6B-multi,mmlu_high_school_biology,0-shot,accuracy,0.23548387096774193,0.024137632429337717
Salesforce/codegen-6B-multi,mmlu_college_biology,0-shot,accuracy,0.2847222222222222,0.03773809990686934
Salesforce/codegen-6B-multi,mmlu_anatomy,0-shot,accuracy,0.28888888888888886,0.0391545063041425
Salesforce/codegen-6B-multi,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
Salesforce/codegen-6B-multi,mmlu_computer_security,0-shot,accuracy,0.3,0.046056618647183814
Salesforce/codegen-6B-multi,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-6B-multi,mmlu_astronomy,0-shot,accuracy,0.21710526315789475,0.03355045304882924
Salesforce/codegen-6B-multi,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816506
Salesforce/codegen-6B-multi,mmlu_conceptual_physics,0-shot,accuracy,0.3446808510638298,0.031068985963122145
Salesforce/codegen-6B-multi,mmlu_abstract_algebra,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-6B-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-6B-multi,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.04287858751340456
Salesforce/codegen-6B-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.030516530732694436
Salesforce/codegen-6B-multi,mmlu_high_school_statistics,0-shot,accuracy,0.26851851851851855,0.030225226160012404
Salesforce/codegen-6B-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.24074074074074073,0.02201908001221789
Salesforce/codegen-6B-multi,mmlu_electrical_engineering,0-shot,accuracy,0.3103448275862069,0.03855289616378948
Salesforce/codegen-6B-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.24444444444444444,0.026202766534652148
Salesforce/codegen-6B-multi,arc_challenge,25-shot,accuracy,0.22440273037542663,0.012191404938603838
Salesforce/codegen-6B-multi,arc_challenge,25-shot,acc_norm,0.26791808873720135,0.012942030195136433
Salesforce/codegen-6B-multi,hellaswag,10-shot,accuracy,0.3350926110336586,0.004710581496639355
Salesforce/codegen-6B-multi,hellaswag,10-shot,acc_norm,0.41107349133638715,0.004910229643262719
Salesforce/codegen-6B-multi,truthfulqa_mc2,0-shot,accuracy,0.456190954016092,0.015179710279065738
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,bleu_max,18.4162863206651,0.6674446234293927
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,bleu_acc,0.4186046511627907,0.017270015284476865
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,bleu_diff,2.0276870793796506,0.7080695144029784
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge1_max,39.01657959182641,0.9129251342011889
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge1_acc,0.3671970624235006,0.01687480500145318
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge1_diff,0.7733335673342009,1.0566035740226651
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge2_max,21.648507625466433,1.002315340981092
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge2_acc,0.23745410036719705,0.014896277441041845
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rouge2_diff,1.2461063185143297,1.0682476326083845
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rougeL_max,36.58025015195491,0.9084853751956918
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rougeL_acc,0.3671970624235006,0.01687480500145318
Salesforce/codegen-6B-multi,truthfulqa_gen,0-shot,rougeL_diff,1.0039000965278462,1.069567876870668
Salesforce/codegen-6B-multi,truthfulqa_mc1,0-shot,accuracy,0.26805385556915545,0.015506204722834545
Salesforce/codegen-6B-multi,winogrande,5-shot,accuracy,0.5390686661404893,0.014009521680980306
meta-llama/Meta-Llama-3-70B,arc:challenge,25-shot,accuracy,0.6510238907849829,0.0139289334613825
meta-llama/Meta-Llama-3-70B,arc:challenge,25-shot,acc_norm,0.6877133105802048,0.013542598541688067
meta-llama/Meta-Llama-3-70B,hellaswag,10-shot,accuracy,0.6951802429794861,0.004593902601979336
meta-llama/Meta-Llama-3-70B,hellaswag,10-shot,acc_norm,0.8798048197570205,0.003245250394565294
meta-llama/Meta-Llama-3-70B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.48,0.05021167315686779
meta-llama/Meta-Llama-3-70B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.48,0.05021167315686779
meta-llama/Meta-Llama-3-70B,hendrycksTest-anatomy,5-shot,accuracy,0.7851851851851852,0.035478541985608236
meta-llama/Meta-Llama-3-70B,hendrycksTest-anatomy,5-shot,acc_norm,0.7851851851851852,0.035478541985608236
meta-llama/Meta-Llama-3-70B,hendrycksTest-astronomy,5-shot,accuracy,0.9210526315789473,0.021944342818247926
meta-llama/Meta-Llama-3-70B,hendrycksTest-astronomy,5-shot,acc_norm,0.9210526315789473,0.021944342818247926
meta-llama/Meta-Llama-3-70B,hendrycksTest-business_ethics,5-shot,accuracy,0.82,0.038612291966536934
meta-llama/Meta-Llama-3-70B,hendrycksTest-business_ethics,5-shot,acc_norm,0.82,0.038612291966536934
meta-llama/Meta-Llama-3-70B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8566037735849057,0.02157033497662495
meta-llama/Meta-Llama-3-70B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8566037735849057,0.02157033497662495
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_biology,5-shot,accuracy,0.9375,0.02024219611347799
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_biology,5-shot,acc_norm,0.9375,0.02024219611347799
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_chemistry,5-shot,accuracy,0.62,0.04878317312145632
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.62,0.04878317312145632
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_computer_science,5-shot,accuracy,0.73,0.044619604333847394
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.73,0.044619604333847394
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_mathematics,5-shot,accuracy,0.52,0.050211673156867795
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.52,0.050211673156867795
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_medicine,5-shot,accuracy,0.7976878612716763,0.030631145539198816
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_medicine,5-shot,acc_norm,0.7976878612716763,0.030631145539198816
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_physics,5-shot,accuracy,0.49019607843137253,0.04974229460422817
meta-llama/Meta-Llama-3-70B,hendrycksTest-college_physics,5-shot,acc_norm,0.49019607843137253,0.04974229460422817
meta-llama/Meta-Llama-3-70B,hendrycksTest-computer_security,5-shot,accuracy,0.83,0.03775251680686371
meta-llama/Meta-Llama-3-70B,hendrycksTest-computer_security,5-shot,acc_norm,0.83,0.03775251680686371
meta-llama/Meta-Llama-3-70B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.825531914893617,0.02480944233550398
meta-llama/Meta-Llama-3-70B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.825531914893617,0.02480944233550398
meta-llama/Meta-Llama-3-70B,hendrycksTest-econometrics,5-shot,accuracy,0.7280701754385965,0.04185774424022056
meta-llama/Meta-Llama-3-70B,hendrycksTest-econometrics,5-shot,acc_norm,0.7280701754385965,0.04185774424022056
meta-llama/Meta-Llama-3-70B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7517241379310344,0.03600105692727771
meta-llama/Meta-Llama-3-70B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7517241379310344,0.03600105692727771
meta-llama/Meta-Llama-3-70B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.626984126984127,0.02490699045899257
meta-llama/Meta-Llama-3-70B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.626984126984127,0.02490699045899257
meta-llama/Meta-Llama-3-70B,hendrycksTest-formal_logic,5-shot,accuracy,0.6507936507936508,0.04263906892795131
meta-llama/Meta-Llama-3-70B,hendrycksTest-formal_logic,5-shot,acc_norm,0.6507936507936508,0.04263906892795131
meta-llama/Meta-Llama-3-70B,hendrycksTest-global_facts,5-shot,accuracy,0.48,0.05021167315686779
meta-llama/Meta-Llama-3-70B,hendrycksTest-global_facts,5-shot,acc_norm,0.48,0.05021167315686779
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_biology,5-shot,accuracy,0.9032258064516129,0.016818943416345197
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.9032258064516129,0.016818943416345197
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.7093596059113301,0.03194740072265541
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.7093596059113301,0.03194740072265541
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.86,0.03487350880197772
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.86,0.03487350880197772
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8666666666666667,0.026544435312706467
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8666666666666667,0.026544435312706467
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_geography,5-shot,accuracy,0.9393939393939394,0.016999994927421616
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9393939393939394,0.016999994927421616
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9792746113989638,0.010281417011909027
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9792746113989638,0.010281417011909027
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8256410256410256,0.01923724980340523
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8256410256410256,0.01923724980340523
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.4925925925925926,0.030482192395191492
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.4925925925925926,0.030482192395191492
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8865546218487395,0.02060022575020482
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8865546218487395,0.02060022575020482
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_physics,5-shot,accuracy,0.5695364238410596,0.04042809961395634
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5695364238410596,0.04042809961395634
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9412844036697248,0.010079470534014
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9412844036697248,0.010079470534014
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.7314814814814815,0.030225226160012414
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.7314814814814815,0.030225226160012414
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.9558823529411765,0.014413198705704811
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.9558823529411765,0.014413198705704811
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9409282700421941,0.015346597463888691
meta-llama/Meta-Llama-3-70B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9409282700421941,0.015346597463888691
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_aging,5-shot,accuracy,0.8251121076233184,0.02549528462644497
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_aging,5-shot,acc_norm,0.8251121076233184,0.02549528462644497
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_sexuality,5-shot,accuracy,0.8854961832061069,0.027927473753597453
meta-llama/Meta-Llama-3-70B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8854961832061069,0.027927473753597453
meta-llama/Meta-Llama-3-70B,hendrycksTest-international_law,5-shot,accuracy,0.8925619834710744,0.028268812192540637
meta-llama/Meta-Llama-3-70B,hendrycksTest-international_law,5-shot,acc_norm,0.8925619834710744,0.028268812192540637
meta-llama/Meta-Llama-3-70B,hendrycksTest-jurisprudence,5-shot,accuracy,0.8611111111111112,0.03343270062869621
meta-llama/Meta-Llama-3-70B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8611111111111112,0.03343270062869621
meta-llama/Meta-Llama-3-70B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8773006134969326,0.025777328426978927
meta-llama/Meta-Llama-3-70B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8773006134969326,0.025777328426978927
meta-llama/Meta-Llama-3-70B,hendrycksTest-machine_learning,5-shot,accuracy,0.625,0.04595091388086298
meta-llama/Meta-Llama-3-70B,hendrycksTest-machine_learning,5-shot,acc_norm,0.625,0.04595091388086298
meta-llama/Meta-Llama-3-70B,hendrycksTest-management,5-shot,accuracy,0.912621359223301,0.027960689125970654
meta-llama/Meta-Llama-3-70B,hendrycksTest-management,5-shot,acc_norm,0.912621359223301,0.027960689125970654
meta-llama/Meta-Llama-3-70B,hendrycksTest-marketing,5-shot,accuracy,0.9401709401709402,0.015537514263253888
meta-llama/Meta-Llama-3-70B,hendrycksTest-marketing,5-shot,acc_norm,0.9401709401709402,0.015537514263253888
meta-llama/Meta-Llama-3-70B,hendrycksTest-medical_genetics,5-shot,accuracy,0.91,0.028762349126466115
meta-llama/Meta-Llama-3-70B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.91,0.028762349126466115
meta-llama/Meta-Llama-3-70B,hendrycksTest-miscellaneous,5-shot,accuracy,0.9195402298850575,0.009726831316141849
meta-llama/Meta-Llama-3-70B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9195402298850575,0.009726831316141849
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_disputes,5-shot,accuracy,0.846820809248555,0.01939037010896993
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.846820809248555,0.01939037010896993
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6145251396648045,0.016277927039638197
meta-llama/Meta-Llama-3-70B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6145251396648045,0.016277927039638197
meta-llama/Meta-Llama-3-70B,hendrycksTest-nutrition,5-shot,accuracy,0.869281045751634,0.019301873624215277
meta-llama/Meta-Llama-3-70B,hendrycksTest-nutrition,5-shot,acc_norm,0.869281045751634,0.019301873624215277
meta-llama/Meta-Llama-3-70B,hendrycksTest-philosophy,5-shot,accuracy,0.864951768488746,0.019411520247335105
meta-llama/Meta-Llama-3-70B,hendrycksTest-philosophy,5-shot,acc_norm,0.864951768488746,0.019411520247335105
meta-llama/Meta-Llama-3-70B,hendrycksTest-prehistory,5-shot,accuracy,0.9166666666666666,0.015378494985372755
meta-llama/Meta-Llama-3-70B,hendrycksTest-prehistory,5-shot,acc_norm,0.9166666666666666,0.015378494985372755
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_accounting,5-shot,accuracy,0.6382978723404256,0.028663820147199485
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6382978723404256,0.028663820147199485
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_law,5-shot,accuracy,0.6166883963494133,0.012417603662901188
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_law,5-shot,acc_norm,0.6166883963494133,0.012417603662901188
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_medicine,5-shot,accuracy,0.8676470588235294,0.020585134189220665
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8676470588235294,0.020585134189220665
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_psychology,5-shot,accuracy,0.8562091503267973,0.014194985469419127
meta-llama/Meta-Llama-3-70B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8562091503267973,0.014194985469419127
meta-llama/Meta-Llama-3-70B,hendrycksTest-public_relations,5-shot,accuracy,0.7545454545454545,0.041220665028782855
meta-llama/Meta-Llama-3-70B,hendrycksTest-public_relations,5-shot,acc_norm,0.7545454545454545,0.041220665028782855
meta-llama/Meta-Llama-3-70B,hendrycksTest-security_studies,5-shot,accuracy,0.8489795918367347,0.022923004094736847
meta-llama/Meta-Llama-3-70B,hendrycksTest-security_studies,5-shot,acc_norm,0.8489795918367347,0.022923004094736847
meta-llama/Meta-Llama-3-70B,hendrycksTest-sociology,5-shot,accuracy,0.9353233830845771,0.017391600291491064
meta-llama/Meta-Llama-3-70B,hendrycksTest-sociology,5-shot,acc_norm,0.9353233830845771,0.017391600291491064
meta-llama/Meta-Llama-3-70B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.94,0.023868325657594197
meta-llama/Meta-Llama-3-70B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.94,0.023868325657594197
meta-llama/Meta-Llama-3-70B,hendrycksTest-virology,5-shot,accuracy,0.5843373493975904,0.03836722176598052
meta-llama/Meta-Llama-3-70B,hendrycksTest-virology,5-shot,acc_norm,0.5843373493975904,0.03836722176598052
meta-llama/Meta-Llama-3-70B,hendrycksTest-world_religions,5-shot,accuracy,0.9064327485380117,0.022335993231163274
meta-llama/Meta-Llama-3-70B,hendrycksTest-world_religions,5-shot,acc_norm,0.9064327485380117,0.022335993231163274
meta-llama/Meta-Llama-3-70B,truthfulqa:mc,0-shot,mc1,0.29865361077111385,0.01602157061376854
meta-llama/Meta-Llama-3-70B,truthfulqa:mc,0-shot,mc2,0.45562368201500897,0.013963421323817822
meta-llama/Meta-Llama-3-70B,winogrande,5-shot,accuracy,0.8531965272296764,0.009946627440250704
meta-llama/Meta-Llama-3-70B,gsm8k,5-shot,accuracy,0.7687642153146323,0.01161358750316661
model_name,minerva_math_precalc,5-shot,accuracy,0.13553113553113552,0.014662092847395482
model_name,minerva_math_prealgebra,5-shot,accuracy,0.4890929965556831,0.01694755389652771
model_name,minerva_math_num_theory,5-shot,accuracy,0.17037037037037037,0.016193651111111738
model_name,minerva_math_intermediate_algebra,5-shot,accuracy,0.11738648947951273,0.010717440330431139
model_name,minerva_math_geometry,5-shot,accuracy,0.21294363256784968,0.018724977273263204
model_name,minerva_math_counting_and_prob,5-shot,accuracy,0.1962025316455696,0.01825975937156573
model_name,minerva_math_algebra,5-shot,accuracy,0.4018534119629318,0.014236239984076448
model_name,fld_default,0-shot,accuracy,0.0,
model_name,fld_star,0-shot,accuracy,0.0,
model_name,arithmetic_3da,5-shot,accuracy,0.9945,0.0016541593398342208
model_name,arithmetic_3ds,5-shot,accuracy,0.9995,0.0005000000000000049
model_name,arithmetic_4da,5-shot,accuracy,0.983,0.0028913110935905434
model_name,arithmetic_2ds,5-shot,accuracy,1.0,
model_name,arithmetic_5ds,5-shot,accuracy,0.982,0.0029736208922129114
model_name,arithmetic_5da,5-shot,accuracy,0.9755,0.003457723662536254
model_name,arithmetic_1dc,5-shot,accuracy,0.935,0.005513864466114152
model_name,arithmetic_4ds,5-shot,accuracy,0.995,0.0015775754727385416
model_name,arithmetic_2dm,5-shot,accuracy,0.9525,0.004757435401116703
model_name,arithmetic_2da,5-shot,accuracy,1.0,
model_name,gsm8k_cot,5-shot,accuracy,0.6072782410917361,0.01345174534958657
model_name,gsm8k,5-shot,accuracy,0.5837755875663382,0.01357778833465266
mistralai/Mixtral-8x7B-v0.1,arc:challenge,25-shot,accuracy,0.6373720136518771,0.014049106564955002
mistralai/Mixtral-8x7B-v0.1,arc:challenge,25-shot,acc_norm,0.6638225255972696,0.013804855026205761
mistralai/Mixtral-8x7B-v0.1,hellaswag,10-shot,accuracy,0.6670981876120294,0.004702886273189442
mistralai/Mixtral-8x7B-v0.1,hellaswag,10-shot,acc_norm,0.8653654650468035,0.0034063520713417074
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.34,0.04760952285695236
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.34,0.04760952285695236
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.7185185185185186,0.03885004245800254
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.7185185185185186,0.03885004245800254
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.8289473684210527,0.030643607071677098
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.8289473684210527,0.030643607071677098
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.76,0.04292346959909283
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.76,0.04292346959909283
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7849056603773585,0.02528839450289137
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7849056603773585,0.02528839450289137
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.8680555555555556,0.02830096838204443
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.8680555555555556,0.02830096838204443
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.54,0.05009082659620332
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.54,0.05009082659620332
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.63,0.04852365870939099
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.63,0.04852365870939099
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.46,0.05009082659620332
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.46,0.05009082659620332
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.6994219653179191,0.03496101481191179
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.6994219653179191,0.03496101481191179
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.46078431372549017,0.04959859966384181
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.46078431372549017,0.04959859966384181
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.81,0.039427724440366234
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.81,0.039427724440366234
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6808510638297872,0.030472973363380035
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6808510638297872,0.030472973363380035
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.6491228070175439,0.04489539350270698
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.6491228070175439,0.04489539350270698
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.6896551724137931,0.03855289616378948
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.6896551724137931,0.03855289616378948
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.48148148148148145,0.025733641991838987
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.48148148148148145,0.025733641991838987
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.5634920634920635,0.04435932892851466
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.5634920634920635,0.04435932892851466
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.51,0.05024183937956912
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.51,0.05024183937956912
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.8419354838709677,0.020752831511875274
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.8419354838709677,0.020752831511875274
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6354679802955665,0.0338640574606209
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6354679802955665,0.0338640574606209
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.72,0.04512608598542127
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.72,0.04512608598542127
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8181818181818182,0.030117688929503585
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8181818181818182,0.030117688929503585
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.8636363636363636,0.024450155973189835
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8636363636363636,0.024450155973189835
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9378238341968912,0.017426974154240524
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9378238341968912,0.017426974154240524
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.7051282051282052,0.0231193627582323
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.7051282051282052,0.0231193627582323
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3851851851851852,0.029670906124630886
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3851851851851852,0.029670906124630886
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7857142857142857,0.026653531596715494
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7857142857142857,0.026653531596715494
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.4900662251655629,0.04081677107248436
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.4900662251655629,0.04081677107248436
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8807339449541285,0.013895729292588964
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8807339449541285,0.013895729292588964
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6481481481481481,0.03256850570293647
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6481481481481481,0.03256850570293647
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8480392156862745,0.025195658428931792
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8480392156862745,0.025195658428931792
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.890295358649789,0.02034340073486884
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.890295358649789,0.02034340073486884
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.7802690582959642,0.027790177064383595
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.7802690582959642,0.027790177064383595
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.8091603053435115,0.03446513350752598
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8091603053435115,0.03446513350752598
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.8760330578512396,0.03008309871603521
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.8760330578512396,0.03008309871603521
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.8333333333333334,0.03602814176392645
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8333333333333334,0.03602814176392645
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7730061349693251,0.032910995786157686
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7730061349693251,0.032910995786157686
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.5357142857142857,0.04733667890053756
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.5357142857142857,0.04733667890053756
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-management,5-shot,accuracy,0.883495145631068,0.03176683948640407
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-management,5-shot,acc_norm,0.883495145631068,0.03176683948640407
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.9188034188034188,0.017893784904018533
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.9188034188034188,0.017893784904018533
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.78,0.04163331998932263
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.78,0.04163331998932263
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.8748403575989783,0.011832954239305723
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8748403575989783,0.011832954239305723
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.7976878612716763,0.021628077380196124
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7976878612716763,0.021628077380196124
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4011173184357542,0.01639222189940708
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4011173184357542,0.01639222189940708
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.8235294117647058,0.021828596053108402
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.8235294117647058,0.021828596053108402
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.7845659163987139,0.023350225475471442
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.7845659163987139,0.023350225475471442
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.8395061728395061,0.020423955354778027
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.8395061728395061,0.020423955354778027
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.5177304964539007,0.02980873964223777
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5177304964539007,0.02980873964223777
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.5319426336375489,0.012744149704869645
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.5319426336375489,0.012744149704869645
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.8125,0.023709788253811766
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.8125,0.023709788253811766
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.7843137254901961,0.016639319350313264
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7843137254901961,0.016639319350313264
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.7,0.04389311454644287
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.7,0.04389311454644287
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.7877551020408163,0.026176967197866767
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.7877551020408163,0.026176967197866767
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.8905472636815921,0.022076326101824657
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.8905472636815921,0.022076326101824657
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.92,0.0272659924344291
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.92,0.0272659924344291
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5120481927710844,0.03891364495835817
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5120481927710844,0.03891364495835817
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8771929824561403,0.02517298435015575
mistralai/Mixtral-8x7B-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8771929824561403,0.02517298435015575
mistralai/Mixtral-8x7B-v0.1,truthfulqa:mc,0-shot,mc1,0.3182374541003672,0.01630598864892061
mistralai/Mixtral-8x7B-v0.1,truthfulqa:mc,0-shot,mc2,0.4680543300316138,0.014120170542973978
mistralai/Mixtral-8x7B-v0.1,winogrande,5-shot,accuracy,0.8176795580110497,0.010851565594267224
mistralai/Mixtral-8x7B-v0.1,gsm8k,5-shot,accuracy,0.5852918877937832,0.013570623842304508
Salesforce/codegen-6B-multi,arc:challenge,25-shot,accuracy,0.2380546075085324,0.012445770028026208
Salesforce/codegen-6B-multi,arc:challenge,25-shot,acc_norm,0.2721843003412969,0.013006600406423706
Salesforce/codegen-6B-multi,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215034
Salesforce/codegen-6B-multi,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215034
Salesforce/codegen-6B-multi,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.038850042458002526
Salesforce/codegen-6B-multi,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.038850042458002526
Salesforce/codegen-6B-multi,hendrycksTest-astronomy,5-shot,accuracy,0.20394736842105263,0.0327900040631005
Salesforce/codegen-6B-multi,hendrycksTest-astronomy,5-shot,acc_norm,0.20394736842105263,0.0327900040631005
Salesforce/codegen-6B-multi,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.04408440022768079
Salesforce/codegen-6B-multi,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.04408440022768079
Salesforce/codegen-6B-multi,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.27169811320754716,0.027377706624670713
Salesforce/codegen-6B-multi,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.27169811320754716,0.027377706624670713
Salesforce/codegen-6B-multi,hendrycksTest-college_biology,5-shot,accuracy,0.2916666666666667,0.03800968060554857
Salesforce/codegen-6B-multi,hendrycksTest-college_biology,5-shot,acc_norm,0.2916666666666667,0.03800968060554857
Salesforce/codegen-6B-multi,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036845
Salesforce/codegen-6B-multi,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036845
Salesforce/codegen-6B-multi,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-6B-multi,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.0440844002276808
Salesforce/codegen-6B-multi,hendrycksTest-college_mathematics,5-shot,accuracy,0.23,0.04229525846816506
Salesforce/codegen-6B-multi,hendrycksTest-college_mathematics,5-shot,acc_norm,0.23,0.04229525846816506
Salesforce/codegen-6B-multi,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
Salesforce/codegen-6B-multi,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
Salesforce/codegen-6B-multi,hendrycksTest-college_physics,5-shot,accuracy,0.19607843137254902,0.03950581861179963
Salesforce/codegen-6B-multi,hendrycksTest-college_physics,5-shot,acc_norm,0.19607843137254902,0.03950581861179963
Salesforce/codegen-6B-multi,hendrycksTest-computer_security,5-shot,accuracy,0.29,0.045604802157206845
Salesforce/codegen-6B-multi,hendrycksTest-computer_security,5-shot,acc_norm,0.29,0.045604802157206845
Salesforce/codegen-6B-multi,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3446808510638298,0.03106898596312215
Salesforce/codegen-6B-multi,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3446808510638298,0.03106898596312215
Salesforce/codegen-6B-multi,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.04142439719489361
Salesforce/codegen-6B-multi,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.04142439719489361
Salesforce/codegen-6B-multi,hendrycksTest-electrical_engineering,5-shot,accuracy,0.30344827586206896,0.038312260488503336
Salesforce/codegen-6B-multi,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.30344827586206896,0.038312260488503336
Salesforce/codegen-6B-multi,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24603174603174602,0.022182037202948368
Salesforce/codegen-6B-multi,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24603174603174602,0.022182037202948368
Salesforce/codegen-6B-multi,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.03718489006818116
Salesforce/codegen-6B-multi,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.03718489006818116
Salesforce/codegen-6B-multi,hendrycksTest-global_facts,5-shot,accuracy,0.23,0.04229525846816508
Salesforce/codegen-6B-multi,hendrycksTest-global_facts,5-shot,acc_norm,0.23,0.04229525846816508
Salesforce/codegen-6B-multi,hendrycksTest-high_school_biology,5-shot,accuracy,0.23225806451612904,0.024022256130308235
Salesforce/codegen-6B-multi,hendrycksTest-high_school_biology,5-shot,acc_norm,0.23225806451612904,0.024022256130308235
Salesforce/codegen-6B-multi,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.030516530732694433
Salesforce/codegen-6B-multi,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.030516530732694433
Salesforce/codegen-6B-multi,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.04725815626252604
Salesforce/codegen-6B-multi,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.04725815626252604
Salesforce/codegen-6B-multi,hendrycksTest-high_school_european_history,5-shot,accuracy,0.26666666666666666,0.03453131801885416
Salesforce/codegen-6B-multi,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.26666666666666666,0.03453131801885416
Salesforce/codegen-6B-multi,hendrycksTest-high_school_geography,5-shot,accuracy,0.21717171717171718,0.029376616484945633
Salesforce/codegen-6B-multi,hendrycksTest-high_school_geography,5-shot,acc_norm,0.21717171717171718,0.029376616484945633
Salesforce/codegen-6B-multi,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.20725388601036268,0.029252823291803624
Salesforce/codegen-6B-multi,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.20725388601036268,0.029252823291803624
Salesforce/codegen-6B-multi,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2205128205128205,0.02102067268082791
Salesforce/codegen-6B-multi,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2205128205128205,0.02102067268082791
Salesforce/codegen-6B-multi,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24444444444444444,0.026202766534652148
Salesforce/codegen-6B-multi,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24444444444444444,0.026202766534652148
Salesforce/codegen-6B-multi,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.25210084033613445,0.028205545033277723
Salesforce/codegen-6B-multi,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.25210084033613445,0.028205545033277723
Salesforce/codegen-6B-multi,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.03684881521389023
Salesforce/codegen-6B-multi,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.03684881521389023
Salesforce/codegen-6B-multi,hendrycksTest-high_school_psychology,5-shot,accuracy,0.25504587155963304,0.01868850085653584
Salesforce/codegen-6B-multi,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.25504587155963304,0.01868850085653584
Salesforce/codegen-6B-multi,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2638888888888889,0.03005820270430985
Salesforce/codegen-6B-multi,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2638888888888889,0.03005820270430985
Salesforce/codegen-6B-multi,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2647058823529412,0.030964517926923403
Salesforce/codegen-6B-multi,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2647058823529412,0.030964517926923403
Salesforce/codegen-6B-multi,hendrycksTest-high_school_world_history,5-shot,accuracy,0.28270042194092826,0.02931281415395592
Salesforce/codegen-6B-multi,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.28270042194092826,0.02931281415395592
Salesforce/codegen-6B-multi,hendrycksTest-human_aging,5-shot,accuracy,0.40358744394618834,0.032928028193303135
Salesforce/codegen-6B-multi,hendrycksTest-human_aging,5-shot,acc_norm,0.40358744394618834,0.032928028193303135
Salesforce/codegen-6B-multi,hendrycksTest-human_sexuality,5-shot,accuracy,0.1984732824427481,0.03498149385462472
Salesforce/codegen-6B-multi,hendrycksTest-human_sexuality,5-shot,acc_norm,0.1984732824427481,0.03498149385462472
Salesforce/codegen-6B-multi,hendrycksTest-international_law,5-shot,accuracy,0.34710743801652894,0.04345724570292534
Salesforce/codegen-6B-multi,hendrycksTest-international_law,5-shot,acc_norm,0.34710743801652894,0.04345724570292534
Salesforce/codegen-6B-multi,hendrycksTest-jurisprudence,5-shot,accuracy,0.23148148148148148,0.04077494709252626
Salesforce/codegen-6B-multi,hendrycksTest-jurisprudence,5-shot,acc_norm,0.23148148148148148,0.04077494709252626
Salesforce/codegen-6B-multi,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.035590395316173425
Salesforce/codegen-6B-multi,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.035590395316173425
Salesforce/codegen-6B-multi,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.04287858751340456
Salesforce/codegen-6B-multi,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.04287858751340456
Salesforce/codegen-6B-multi,hendrycksTest-management,5-shot,accuracy,0.17475728155339806,0.037601780060266224
Salesforce/codegen-6B-multi,hendrycksTest-management,5-shot,acc_norm,0.17475728155339806,0.037601780060266224
Salesforce/codegen-6B-multi,hendrycksTest-marketing,5-shot,accuracy,0.2222222222222222,0.027236013946196666
Salesforce/codegen-6B-multi,hendrycksTest-marketing,5-shot,acc_norm,0.2222222222222222,0.027236013946196666
Salesforce/codegen-6B-multi,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.04292346959909282
Salesforce/codegen-6B-multi,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.04292346959909282
Salesforce/codegen-6B-multi,hendrycksTest-miscellaneous,5-shot,accuracy,0.2848020434227331,0.01613917409652258
Salesforce/codegen-6B-multi,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2848020434227331,0.01613917409652258
Salesforce/codegen-6B-multi,hendrycksTest-moral_disputes,5-shot,accuracy,0.29190751445086704,0.024476994076247337
Salesforce/codegen-6B-multi,hendrycksTest-moral_disputes,5-shot,acc_norm,0.29190751445086704,0.024476994076247337
Salesforce/codegen-6B-multi,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.01435591196476786
Salesforce/codegen-6B-multi,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.01435591196476786
Salesforce/codegen-6B-multi,hendrycksTest-nutrition,5-shot,accuracy,0.24836601307189543,0.024739981355113592
Salesforce/codegen-6B-multi,hendrycksTest-nutrition,5-shot,acc_norm,0.24836601307189543,0.024739981355113592
Salesforce/codegen-6B-multi,hendrycksTest-philosophy,5-shot,accuracy,0.31189710610932475,0.02631185807185416
Salesforce/codegen-6B-multi,hendrycksTest-philosophy,5-shot,acc_norm,0.31189710610932475,0.02631185807185416
Salesforce/codegen-6B-multi,hendrycksTest-prehistory,5-shot,accuracy,0.24074074074074073,0.023788583551658526
Salesforce/codegen-6B-multi,hendrycksTest-prehistory,5-shot,acc_norm,0.24074074074074073,0.023788583551658526
Salesforce/codegen-6B-multi,hendrycksTest-professional_accounting,5-shot,accuracy,0.22340425531914893,0.024847921358063962
Salesforce/codegen-6B-multi,hendrycksTest-professional_accounting,5-shot,acc_norm,0.22340425531914893,0.024847921358063962
Salesforce/codegen-6B-multi,hendrycksTest-professional_law,5-shot,accuracy,0.26597131681877445,0.011285033165551277
Salesforce/codegen-6B-multi,hendrycksTest-professional_law,5-shot,acc_norm,0.26597131681877445,0.011285033165551277
Salesforce/codegen-6B-multi,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.023709788253811766
Salesforce/codegen-6B-multi,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.023709788253811766
Salesforce/codegen-6B-multi,hendrycksTest-professional_psychology,5-shot,accuracy,0.25326797385620914,0.01759348689536683
Salesforce/codegen-6B-multi,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25326797385620914,0.01759348689536683
Salesforce/codegen-6B-multi,hendrycksTest-public_relations,5-shot,accuracy,0.2909090909090909,0.04350271442923243
Salesforce/codegen-6B-multi,hendrycksTest-public_relations,5-shot,acc_norm,0.2909090909090909,0.04350271442923243
Salesforce/codegen-6B-multi,hendrycksTest-security_studies,5-shot,accuracy,0.17959183673469387,0.024573293589585637
Salesforce/codegen-6B-multi,hendrycksTest-security_studies,5-shot,acc_norm,0.17959183673469387,0.024573293589585637
Salesforce/codegen-6B-multi,hendrycksTest-sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409224
Salesforce/codegen-6B-multi,hendrycksTest-sociology,5-shot,acc_norm,0.23880597014925373,0.030147775935409224
Salesforce/codegen-6B-multi,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.18,0.03861229196653697
Salesforce/codegen-6B-multi,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.18,0.03861229196653697
Salesforce/codegen-6B-multi,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.03591566797824663
Salesforce/codegen-6B-multi,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.03591566797824663
Salesforce/codegen-6B-multi,hendrycksTest-world_religions,5-shot,accuracy,0.27485380116959063,0.034240429246915824
Salesforce/codegen-6B-multi,hendrycksTest-world_religions,5-shot,acc_norm,0.27485380116959063,0.034240429246915824
Salesforce/codegen-6B-multi,truthfulqa:mc,0-shot,mc1,0.27050183598531213,0.015550778332842897
Salesforce/codegen-6B-multi,truthfulqa:mc,0-shot,mc2,0.4565419940938498,0.015175324004378129
Salesforce/codegen-6B-multi,drop,3-shot,accuracy,0.0010486577181208054,0.0003314581465219097
Salesforce/codegen-6B-multi,drop,3-shot,f1,0.04059878355704704,0.0011641328961688674
openlm-research/open_llama_7b,minerva_math_precalc,5-shot,accuracy,0.027472527472527472,0.007001675776712246
openlm-research/open_llama_7b,minerva_math_prealgebra,5-shot,accuracy,0.05625717566016074,0.0078118908661403115
openlm-research/open_llama_7b,minerva_math_num_theory,5-shot,accuracy,0.016666666666666666,0.0055141728150896
openlm-research/open_llama_7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.017718715393133997,0.0043926922934928985
openlm-research/open_llama_7b,minerva_math_geometry,5-shot,accuracy,0.025052192066805846,0.007148247838013856
openlm-research/open_llama_7b,minerva_math_counting_and_prob,5-shot,accuracy,0.035864978902953586,0.008550148322989818
openlm-research/open_llama_7b,minerva_math_algebra,5-shot,accuracy,0.03369839932603201,0.005239847423284803
openlm-research/open_llama_7b,fld_default,0-shot,accuracy,0.0,
openlm-research/open_llama_7b,fld_star,0-shot,accuracy,0.0,
openlm-research/open_llama_7b,arithmetic_3da,5-shot,accuracy,0.578,0.011046221503516772
openlm-research/open_llama_7b,arithmetic_3ds,5-shot,accuracy,0.387,0.010893798117218193
openlm-research/open_llama_7b,arithmetic_4da,5-shot,accuracy,0.328,0.010500625294037556
openlm-research/open_llama_7b,arithmetic_2ds,5-shot,accuracy,0.471,0.01116431014037372
openlm-research/open_llama_7b,arithmetic_5ds,5-shot,accuracy,0.1505,0.007997302884517568
openlm-research/open_llama_7b,arithmetic_5da,5-shot,accuracy,0.1685,0.00837191253297178
openlm-research/open_llama_7b,arithmetic_1dc,5-shot,accuracy,0.066,0.005553144938623083
openlm-research/open_llama_7b,arithmetic_4ds,5-shot,accuracy,0.2865,0.010112368911511354
openlm-research/open_llama_7b,arithmetic_2dm,5-shot,accuracy,0.167,0.008342079785495497
openlm-research/open_llama_7b,arithmetic_2da,5-shot,accuracy,0.786,0.0091730077965745
openlm-research/open_llama_7b,gsm8k_cot,5-shot,accuracy,0.060652009097801364,0.0065747333814058185
openlm-research/open_llama_7b,gsm8k,5-shot,accuracy,0.01592115238817286,0.0034478192723890037
openlm-research/open_llama_7b,anli_r2,0-shot,brier_score,0.7315652755987271,
openlm-research/open_llama_7b,anli_r3,0-shot,brier_score,0.7126834543344663,
openlm-research/open_llama_7b,anli_r1,0-shot,brier_score,0.7524228409842385,
openlm-research/open_llama_7b,xnli_eu,0-shot,brier_score,1.0796208109080396,
openlm-research/open_llama_7b,xnli_vi,0-shot,brier_score,0.9749295501186998,
openlm-research/open_llama_7b,xnli_ru,0-shot,brier_score,0.9068002030674046,
openlm-research/open_llama_7b,xnli_zh,0-shot,brier_score,0.9572324638810987,
openlm-research/open_llama_7b,xnli_tr,0-shot,brier_score,0.8002787399839949,
openlm-research/open_llama_7b,xnli_fr,0-shot,brier_score,0.7601606558680033,
openlm-research/open_llama_7b,xnli_en,0-shot,brier_score,0.6418456490546002,
openlm-research/open_llama_7b,xnli_ur,0-shot,brier_score,1.3165241257894102,
openlm-research/open_llama_7b,xnli_ar,0-shot,brier_score,1.0829988270046658,
openlm-research/open_llama_7b,xnli_de,0-shot,brier_score,0.8950139994287923,
openlm-research/open_llama_7b,xnli_hi,0-shot,brier_score,0.8444982412224025,
openlm-research/open_llama_7b,xnli_es,0-shot,brier_score,0.8538788249083591,
openlm-research/open_llama_7b,xnli_bg,0-shot,brier_score,0.9184994666520818,
openlm-research/open_llama_7b,xnli_sw,0-shot,brier_score,0.8773492580411928,
openlm-research/open_llama_7b,xnli_el,0-shot,brier_score,0.9607628544532029,
openlm-research/open_llama_7b,xnli_th,0-shot,brier_score,0.9321633089585535,
openlm-research/open_llama_7b,logiqa2,0-shot,brier_score,1.0022188312341742,
openlm-research/open_llama_7b,mathqa,5-shot,brier_score,0.9390200290590519,
openlm-research/open_llama_7b,lambada_standard,0-shot,perplexity,5.162686665204201,0.11470587722922684
openlm-research/open_llama_7b,lambada_standard,0-shot,accuracy,0.6374927226858141,0.00669742799676281
openlm-research/open_llama_7b,lambada_openai,0-shot,perplexity,3.96851611110645,0.08521608041380699
openlm-research/open_llama_7b,lambada_openai,0-shot,accuracy,0.7036677663496992,0.0063618781796918695
openlm-research/open_llama_7b,mmlu_world_religions,0-shot,accuracy,0.3684210526315789,0.03699658017656878
openlm-research/open_llama_7b,mmlu_formal_logic,0-shot,accuracy,0.24603174603174602,0.03852273364924316
openlm-research/open_llama_7b,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.02577311116963045
openlm-research/open_llama_7b,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.014422292204808838
openlm-research/open_llama_7b,mmlu_high_school_world_history,0-shot,accuracy,0.29535864978902954,0.02969633871342288
openlm-research/open_llama_7b,mmlu_moral_disputes,0-shot,accuracy,0.3352601156069364,0.02541600377316555
openlm-research/open_llama_7b,mmlu_professional_law,0-shot,accuracy,0.2627118644067797,0.01124054551499567
openlm-research/open_llama_7b,mmlu_logical_fallacies,0-shot,accuracy,0.3006134969325153,0.03602511318806771
openlm-research/open_llama_7b,mmlu_high_school_us_history,0-shot,accuracy,0.3235294117647059,0.03283472056108566
openlm-research/open_llama_7b,mmlu_philosophy,0-shot,accuracy,0.2829581993569132,0.025583062489984824
openlm-research/open_llama_7b,mmlu_jurisprudence,0-shot,accuracy,0.3425925925925926,0.045879047413018126
openlm-research/open_llama_7b,mmlu_international_law,0-shot,accuracy,0.39669421487603307,0.044658697805310094
openlm-research/open_llama_7b,mmlu_high_school_european_history,0-shot,accuracy,0.296969696969697,0.03567969772268046
openlm-research/open_llama_7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.35233160621761656,0.03447478286414358
openlm-research/open_llama_7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.3025210084033613,0.02983796238829193
openlm-research/open_llama_7b,mmlu_high_school_geography,0-shot,accuracy,0.3484848484848485,0.033948539651564025
openlm-research/open_llama_7b,mmlu_high_school_psychology,0-shot,accuracy,0.3577981651376147,0.020552060784827818
openlm-research/open_llama_7b,mmlu_public_relations,0-shot,accuracy,0.4,0.0469237132203465
openlm-research/open_llama_7b,mmlu_us_foreign_policy,0-shot,accuracy,0.4,0.04923659639173309
openlm-research/open_llama_7b,mmlu_sociology,0-shot,accuracy,0.24875621890547264,0.030567675938916707
openlm-research/open_llama_7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3564102564102564,0.0242831405294673
openlm-research/open_llama_7b,mmlu_security_studies,0-shot,accuracy,0.24489795918367346,0.027529637440174913
openlm-research/open_llama_7b,mmlu_professional_psychology,0-shot,accuracy,0.2696078431372549,0.017952449196987866
openlm-research/open_llama_7b,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
openlm-research/open_llama_7b,mmlu_econometrics,0-shot,accuracy,0.2807017543859649,0.042270544512321984
openlm-research/open_llama_7b,mmlu_miscellaneous,0-shot,accuracy,0.367816091954023,0.01724382889184626
openlm-research/open_llama_7b,mmlu_marketing,0-shot,accuracy,0.358974358974359,0.031426169937919246
openlm-research/open_llama_7b,mmlu_management,0-shot,accuracy,0.2524271844660194,0.04301250399690878
openlm-research/open_llama_7b,mmlu_nutrition,0-shot,accuracy,0.3202614379084967,0.026716118380156847
openlm-research/open_llama_7b,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
openlm-research/open_llama_7b,mmlu_human_aging,0-shot,accuracy,0.273542600896861,0.029918586707798827
openlm-research/open_llama_7b,mmlu_professional_medicine,0-shot,accuracy,0.24632352941176472,0.02617343857052
openlm-research/open_llama_7b,mmlu_college_medicine,0-shot,accuracy,0.3236994219653179,0.0356760379963917
openlm-research/open_llama_7b,mmlu_business_ethics,0-shot,accuracy,0.34,0.04760952285695236
openlm-research/open_llama_7b,mmlu_clinical_knowledge,0-shot,accuracy,0.39622641509433965,0.030102793781791197
openlm-research/open_llama_7b,mmlu_global_facts,0-shot,accuracy,0.33,0.04725815626252605
openlm-research/open_llama_7b,mmlu_virology,0-shot,accuracy,0.35542168674698793,0.03726214354322415
openlm-research/open_llama_7b,mmlu_professional_accounting,0-shot,accuracy,0.26595744680851063,0.026358065698880585
openlm-research/open_llama_7b,mmlu_college_physics,0-shot,accuracy,0.18627450980392157,0.03873958714149354
openlm-research/open_llama_7b,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.035737053147634576
openlm-research/open_llama_7b,mmlu_high_school_biology,0-shot,accuracy,0.3096774193548387,0.026302774983517414
openlm-research/open_llama_7b,mmlu_college_biology,0-shot,accuracy,0.3194444444444444,0.03899073687357335
openlm-research/open_llama_7b,mmlu_anatomy,0-shot,accuracy,0.32592592592592595,0.040491220417025055
openlm-research/open_llama_7b,mmlu_college_chemistry,0-shot,accuracy,0.24,0.04292346959909283
openlm-research/open_llama_7b,mmlu_computer_security,0-shot,accuracy,0.34,0.04760952285695235
openlm-research/open_llama_7b,mmlu_college_computer_science,0-shot,accuracy,0.31,0.04648231987117316
openlm-research/open_llama_7b,mmlu_astronomy,0-shot,accuracy,0.24342105263157895,0.034923496688842384
openlm-research/open_llama_7b,mmlu_college_mathematics,0-shot,accuracy,0.29,0.04560480215720684
openlm-research/open_llama_7b,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.03047297336338006
openlm-research/open_llama_7b,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.04512608598542127
openlm-research/open_llama_7b,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.046056618647183814
openlm-research/open_llama_7b,mmlu_machine_learning,0-shot,accuracy,0.24107142857142858,0.04059867246952685
openlm-research/open_llama_7b,mmlu_high_school_chemistry,0-shot,accuracy,0.2561576354679803,0.0307127300709826
openlm-research/open_llama_7b,mmlu_high_school_statistics,0-shot,accuracy,0.3888888888888889,0.033247089118091176
openlm-research/open_llama_7b,mmlu_elementary_mathematics,0-shot,accuracy,0.2724867724867725,0.022930973071633335
openlm-research/open_llama_7b,mmlu_electrical_engineering,0-shot,accuracy,0.3103448275862069,0.03855289616378948
openlm-research/open_llama_7b,mmlu_high_school_mathematics,0-shot,accuracy,0.24814814814814815,0.026335739404055803
openlm-research/open_llama_7b,arc_challenge,25-shot,accuracy,0.4334470989761092,0.014481376224558902
openlm-research/open_llama_7b,arc_challenge,25-shot,acc_norm,0.4735494880546075,0.014590931358120169
openlm-research/open_llama_7b,hellaswag,10-shot,accuracy,0.5364469229237204,0.004976507121076266
openlm-research/open_llama_7b,hellaswag,10-shot,acc_norm,0.7197769368651663,0.004481902637505662
openlm-research/open_llama_7b,truthfulqa_mc2,0-shot,accuracy,0.3514211609144709,0.01355096005061055
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,bleu_max,27.152576174533763,0.8095916667785428
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,bleu_acc,0.32558139534883723,0.01640398946990785
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,bleu_diff,-7.3095593587963394,0.886002774295994
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge1_max,52.08716441858857,0.8595002597325482
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge1_acc,0.3072215422276622,0.01615020132132304
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge1_diff,-10.058290429072063,0.9747973784961548
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge2_max,36.143564978969415,1.0316941608267736
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge2_acc,0.25458996328029376,0.015250117079156496
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rouge2_diff,-11.837884926202605,1.164433626396958
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rougeL_max,49.3236769159205,0.8804470956979424
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rougeL_acc,0.2974296205630355,0.01600265148736098
openlm-research/open_llama_7b,truthfulqa_gen,0-shot,rougeL_diff,-10.229350835912319,0.9840130390427738
openlm-research/open_llama_7b,truthfulqa_mc1,0-shot,accuracy,0.23133414932680538,0.014761945174862658
openlm-research/open_llama_7b,winogrande,5-shot,accuracy,0.6795580110497238,0.013115085457681712
jisukim8873/falcon-7B-case-c,arc:challenge,25-shot,accuracy,0.454778156996587,0.014551507060836353
jisukim8873/falcon-7B-case-c,arc:challenge,25-shot,acc_norm,0.4854948805460751,0.014605241081370056
jisukim8873/falcon-7B-case-c,hellaswag,10-shot,accuracy,0.5984863572993427,0.004892026457294713
jisukim8873/falcon-7B-case-c,hellaswag,10-shot,acc_norm,0.7866958773152758,0.004088034745195346
jisukim8873/falcon-7B-case-c,hendrycksTest-abstract_algebra,5-shot,accuracy,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-c,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-c,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.03885004245800254
jisukim8873/falcon-7B-case-c,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.03885004245800254
jisukim8873/falcon-7B-case-c,hendrycksTest-astronomy,5-shot,accuracy,0.24342105263157895,0.034923496688842384
jisukim8873/falcon-7B-case-c,hendrycksTest-astronomy,5-shot,acc_norm,0.24342105263157895,0.034923496688842384
jisukim8873/falcon-7B-case-c,hendrycksTest-business_ethics,5-shot,accuracy,0.2,0.04020151261036845
jisukim8873/falcon-7B-case-c,hendrycksTest-business_ethics,5-shot,acc_norm,0.2,0.04020151261036845
jisukim8873/falcon-7B-case-c,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2981132075471698,0.028152837942493854
jisukim8873/falcon-7B-case-c,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2981132075471698,0.028152837942493854
jisukim8873/falcon-7B-case-c,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.03716177437566017
jisukim8873/falcon-7B-case-c,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.03716177437566017
jisukim8873/falcon-7B-case-c,hendrycksTest-college_chemistry,5-shot,accuracy,0.19,0.03942772444036624
jisukim8873/falcon-7B-case-c,hendrycksTest-college_chemistry,5-shot,acc_norm,0.19,0.03942772444036624
jisukim8873/falcon-7B-case-c,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-c,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-c,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.044619604333847394
jisukim8873/falcon-7B-case-c,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.044619604333847394
jisukim8873/falcon-7B-case-c,hendrycksTest-college_medicine,5-shot,accuracy,0.3179190751445087,0.03550683989165582
jisukim8873/falcon-7B-case-c,hendrycksTest-college_medicine,5-shot,acc_norm,0.3179190751445087,0.03550683989165582
jisukim8873/falcon-7B-case-c,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617749
jisukim8873/falcon-7B-case-c,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617749
jisukim8873/falcon-7B-case-c,hendrycksTest-computer_security,5-shot,accuracy,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-c,hendrycksTest-computer_security,5-shot,acc_norm,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-c,hendrycksTest-conceptual_physics,5-shot,accuracy,0.32340425531914896,0.030579442773610334
jisukim8873/falcon-7B-case-c,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.32340425531914896,0.030579442773610334
jisukim8873/falcon-7B-case-c,hendrycksTest-econometrics,5-shot,accuracy,0.2719298245614035,0.04185774424022056
jisukim8873/falcon-7B-case-c,hendrycksTest-econometrics,5-shot,acc_norm,0.2719298245614035,0.04185774424022056
jisukim8873/falcon-7B-case-c,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.03565998174135302
jisukim8873/falcon-7B-case-c,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.03565998174135302
jisukim8873/falcon-7B-case-c,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2698412698412698,0.02286083830923207
jisukim8873/falcon-7B-case-c,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2698412698412698,0.02286083830923207
jisukim8873/falcon-7B-case-c,hendrycksTest-formal_logic,5-shot,accuracy,0.18253968253968253,0.034550710191021496
jisukim8873/falcon-7B-case-c,hendrycksTest-formal_logic,5-shot,acc_norm,0.18253968253968253,0.034550710191021496
jisukim8873/falcon-7B-case-c,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-c,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_biology,5-shot,accuracy,0.31290322580645163,0.02637756702864586
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_biology,5-shot,acc_norm,0.31290322580645163,0.02637756702864586
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.29064039408866993,0.03194740072265541
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.29064039408866993,0.03194740072265541
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_european_history,5-shot,accuracy,0.32727272727272727,0.03663974994391242
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.32727272727272727,0.03663974994391242
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_geography,5-shot,accuracy,0.29292929292929293,0.032424979581788166
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_geography,5-shot,acc_norm,0.29292929292929293,0.032424979581788166
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.31088082901554404,0.03340361906276585
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.31088082901554404,0.03340361906276585
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.26666666666666666,0.022421273612923714
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.26666666666666666,0.022421273612923714
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.026719240783712166
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25925925925925924,0.026719240783712166
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2815126050420168,0.029213549414372177
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2815126050420168,0.029213549414372177
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_physics,5-shot,accuracy,0.304635761589404,0.03757949922943342
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_physics,5-shot,acc_norm,0.304635761589404,0.03757949922943342
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_psychology,5-shot,accuracy,0.26788990825688075,0.018987462257978652
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.26788990825688075,0.018987462257978652
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_statistics,5-shot,accuracy,0.20833333333333334,0.02769691071309394
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.20833333333333334,0.02769691071309394
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_us_history,5-shot,accuracy,0.27450980392156865,0.0313217980308329
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.27450980392156865,0.0313217980308329
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3459915611814346,0.03096481058878671
jisukim8873/falcon-7B-case-c,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3459915611814346,0.03096481058878671
jisukim8873/falcon-7B-case-c,hendrycksTest-human_aging,5-shot,accuracy,0.29596412556053814,0.030636591348699803
jisukim8873/falcon-7B-case-c,hendrycksTest-human_aging,5-shot,acc_norm,0.29596412556053814,0.030636591348699803
jisukim8873/falcon-7B-case-c,hendrycksTest-human_sexuality,5-shot,accuracy,0.1984732824427481,0.03498149385462472
jisukim8873/falcon-7B-case-c,hendrycksTest-human_sexuality,5-shot,acc_norm,0.1984732824427481,0.03498149385462472
jisukim8873/falcon-7B-case-c,hendrycksTest-international_law,5-shot,accuracy,0.32231404958677684,0.042664163633521664
jisukim8873/falcon-7B-case-c,hendrycksTest-international_law,5-shot,acc_norm,0.32231404958677684,0.042664163633521664
jisukim8873/falcon-7B-case-c,hendrycksTest-jurisprudence,5-shot,accuracy,0.32407407407407407,0.04524596007030049
jisukim8873/falcon-7B-case-c,hendrycksTest-jurisprudence,5-shot,acc_norm,0.32407407407407407,0.04524596007030049
jisukim8873/falcon-7B-case-c,hendrycksTest-logical_fallacies,5-shot,accuracy,0.31901840490797545,0.03661997551073836
jisukim8873/falcon-7B-case-c,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.31901840490797545,0.03661997551073836
jisukim8873/falcon-7B-case-c,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.0432704093257873
jisukim8873/falcon-7B-case-c,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.0432704093257873
jisukim8873/falcon-7B-case-c,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.04301250399690878
jisukim8873/falcon-7B-case-c,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.04301250399690878
jisukim8873/falcon-7B-case-c,hendrycksTest-marketing,5-shot,accuracy,0.3034188034188034,0.03011821010694262
jisukim8873/falcon-7B-case-c,hendrycksTest-marketing,5-shot,acc_norm,0.3034188034188034,0.03011821010694262
jisukim8873/falcon-7B-case-c,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.045604802157206824
jisukim8873/falcon-7B-case-c,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.045604802157206824
jisukim8873/falcon-7B-case-c,hendrycksTest-miscellaneous,5-shot,accuracy,0.38058748403575987,0.017362564126075425
jisukim8873/falcon-7B-case-c,hendrycksTest-miscellaneous,5-shot,acc_norm,0.38058748403575987,0.017362564126075425
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_disputes,5-shot,accuracy,0.33815028901734107,0.02546977014940018
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_disputes,5-shot,acc_norm,0.33815028901734107,0.02546977014940018
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2569832402234637,0.01461446582196632
jisukim8873/falcon-7B-case-c,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2569832402234637,0.01461446582196632
jisukim8873/falcon-7B-case-c,hendrycksTest-nutrition,5-shot,accuracy,0.28431372549019607,0.025829163272757468
jisukim8873/falcon-7B-case-c,hendrycksTest-nutrition,5-shot,acc_norm,0.28431372549019607,0.025829163272757468
jisukim8873/falcon-7B-case-c,hendrycksTest-philosophy,5-shot,accuracy,0.3536977491961415,0.027155208103200875
jisukim8873/falcon-7B-case-c,hendrycksTest-philosophy,5-shot,acc_norm,0.3536977491961415,0.027155208103200875
jisukim8873/falcon-7B-case-c,hendrycksTest-prehistory,5-shot,accuracy,0.30864197530864196,0.025702640260603746
jisukim8873/falcon-7B-case-c,hendrycksTest-prehistory,5-shot,acc_norm,0.30864197530864196,0.025702640260603746
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_law,5-shot,accuracy,0.2737940026075619,0.0113886121679794
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_law,5-shot,acc_norm,0.2737940026075619,0.0113886121679794
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_medicine,5-shot,accuracy,0.1948529411764706,0.024060599423487414
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1948529411764706,0.024060599423487414
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_psychology,5-shot,accuracy,0.28594771241830064,0.01828048507295468
jisukim8873/falcon-7B-case-c,hendrycksTest-professional_psychology,5-shot,acc_norm,0.28594771241830064,0.01828048507295468
jisukim8873/falcon-7B-case-c,hendrycksTest-public_relations,5-shot,accuracy,0.23636363636363636,0.040693063197213775
jisukim8873/falcon-7B-case-c,hendrycksTest-public_relations,5-shot,acc_norm,0.23636363636363636,0.040693063197213775
jisukim8873/falcon-7B-case-c,hendrycksTest-security_studies,5-shot,accuracy,0.22448979591836735,0.026711430555538408
jisukim8873/falcon-7B-case-c,hendrycksTest-security_studies,5-shot,acc_norm,0.22448979591836735,0.026711430555538408
jisukim8873/falcon-7B-case-c,hendrycksTest-sociology,5-shot,accuracy,0.29850746268656714,0.03235743789355043
jisukim8873/falcon-7B-case-c,hendrycksTest-sociology,5-shot,acc_norm,0.29850746268656714,0.03235743789355043
jisukim8873/falcon-7B-case-c,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.41,0.04943110704237102
jisukim8873/falcon-7B-case-c,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.41,0.04943110704237102
jisukim8873/falcon-7B-case-c,hendrycksTest-virology,5-shot,accuracy,0.35542168674698793,0.03726214354322415
jisukim8873/falcon-7B-case-c,hendrycksTest-virology,5-shot,acc_norm,0.35542168674698793,0.03726214354322415
jisukim8873/falcon-7B-case-c,hendrycksTest-world_religions,5-shot,accuracy,0.3742690058479532,0.03711601185389481
jisukim8873/falcon-7B-case-c,hendrycksTest-world_religions,5-shot,acc_norm,0.3742690058479532,0.03711601185389481
jisukim8873/falcon-7B-case-c,truthfulqa:mc,0-shot,mc1,0.2668298653610771,0.01548369193923727
jisukim8873/falcon-7B-case-c,truthfulqa:mc,0-shot,mc2,0.3826492372963991,0.014401921891655947
jisukim8873/falcon-7B-case-c,winogrande,5-shot,accuracy,0.7008681925808997,0.012868639066091536
jisukim8873/falcon-7B-case-c,gsm8k,5-shot,accuracy,0.07354056103108415,0.007189835754365258
jisukim8873/falcon-7B-case-c,minerva_math_precalc,5-shot,accuracy,0.01282051282051282,0.004818950982487619
jisukim8873/falcon-7B-case-c,minerva_math_prealgebra,5-shot,accuracy,0.018369690011481057,0.004552660520674617
jisukim8873/falcon-7B-case-c,minerva_math_num_theory,5-shot,accuracy,0.012962962962962963,0.004872192984581488
jisukim8873/falcon-7B-case-c,minerva_math_intermediate_algebra,5-shot,accuracy,0.017718715393133997,0.0043926922934928924
jisukim8873/falcon-7B-case-c,minerva_math_geometry,5-shot,accuracy,0.012526096033402923,0.00508694138967796
jisukim8873/falcon-7B-case-c,minerva_math_counting_and_prob,5-shot,accuracy,0.012658227848101266,0.005140313889578845
jisukim8873/falcon-7B-case-c,minerva_math_algebra,5-shot,accuracy,0.014321819713563605,0.0034500415709370234
jisukim8873/falcon-7B-case-c,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-c,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-c,arithmetic_3da,5-shot,accuracy,0.2725,0.009958486869518226
jisukim8873/falcon-7B-case-c,arithmetic_3ds,5-shot,accuracy,0.468,0.011160209457602885
jisukim8873/falcon-7B-case-c,arithmetic_4da,5-shot,accuracy,0.029,0.0037532044004605267
jisukim8873/falcon-7B-case-c,arithmetic_2ds,5-shot,accuracy,0.4425,0.011108941411747607
jisukim8873/falcon-7B-case-c,arithmetic_5ds,5-shot,accuracy,0.0885,0.006352483925679359
jisukim8873/falcon-7B-case-c,arithmetic_5da,5-shot,accuracy,0.012,0.002435357362429837
jisukim8873/falcon-7B-case-c,arithmetic_1dc,5-shot,accuracy,0.0725,0.0057998874426297645
jisukim8873/falcon-7B-case-c,arithmetic_4ds,5-shot,accuracy,0.122,0.007320163413216715
jisukim8873/falcon-7B-case-c,arithmetic_2dm,5-shot,accuracy,0.1895,0.008765460150261466
jisukim8873/falcon-7B-case-c,arithmetic_2da,5-shot,accuracy,0.768,0.009441004516636096
jisukim8873/falcon-7B-case-c,gsm8k_cot,5-shot,accuracy,0.10765731614859743,0.008537484003023345
jisukim8873/falcon-7B-case-c,anli_r2,0-shot,brier_score,0.8780178026204086,
jisukim8873/falcon-7B-case-c,anli_r3,0-shot,brier_score,0.7901916928644884,
jisukim8873/falcon-7B-case-c,anli_r1,0-shot,brier_score,0.9055919262241393,
jisukim8873/falcon-7B-case-c,xnli_eu,0-shot,brier_score,1.001290310490029,
jisukim8873/falcon-7B-case-c,xnli_vi,0-shot,brier_score,0.9798397634266888,
jisukim8873/falcon-7B-case-c,xnli_ru,0-shot,brier_score,0.859574174744215,
jisukim8873/falcon-7B-case-c,xnli_zh,0-shot,brier_score,0.894488082940237,
jisukim8873/falcon-7B-case-c,xnli_tr,0-shot,brier_score,0.9948912466030565,
jisukim8873/falcon-7B-case-c,xnli_fr,0-shot,brier_score,0.7337998938941023,
jisukim8873/falcon-7B-case-c,xnli_en,0-shot,brier_score,0.6616939089924683,
jisukim8873/falcon-7B-case-c,xnli_ur,0-shot,brier_score,1.3066034233367336,
jisukim8873/falcon-7B-case-c,xnli_ar,0-shot,brier_score,1.2889195153240864,
jisukim8873/falcon-7B-case-c,xnli_de,0-shot,brier_score,0.8343456649143227,
jisukim8873/falcon-7B-case-c,xnli_hi,0-shot,brier_score,1.077691017146163,
jisukim8873/falcon-7B-case-c,xnli_es,0-shot,brier_score,0.811152479987488,
jisukim8873/falcon-7B-case-c,xnli_bg,0-shot,brier_score,0.9276588116275865,
jisukim8873/falcon-7B-case-c,xnli_sw,0-shot,brier_score,1.0923473059839404,
jisukim8873/falcon-7B-case-c,xnli_el,0-shot,brier_score,1.0102156151166493,
jisukim8873/falcon-7B-case-c,xnli_th,0-shot,brier_score,0.9468478602772348,
jisukim8873/falcon-7B-case-c,logiqa2,0-shot,brier_score,1.0599129907264795,
jisukim8873/falcon-7B-case-c,mathqa,5-shot,brier_score,0.9539448440856261,
jisukim8873/falcon-7B-case-c,lambada_standard,0-shot,perplexity,4.013535326746964,0.08617064416280937
jisukim8873/falcon-7B-case-c,lambada_standard,0-shot,accuracy,0.6790219289734135,0.006504166559764691
jisukim8873/falcon-7B-case-c,lambada_openai,0-shot,perplexity,3.2959199182547474,0.06892248167777122
jisukim8873/falcon-7B-case-c,lambada_openai,0-shot,accuracy,0.7376285658839511,0.006128994208430875
tiiuae/falcon-7b,gsm8k,5-shot,accuracy,0.052312357846853674,0.006133057708959229
tiiuae/falcon-7b,drop,3-shot,accuracy,0.0010486577181208054,0.00033145814652193653
tiiuae/falcon-7b,drop,3-shot,f1,0.04824664429530208,0.0012232481165562455
tiiuae/falcon-7b,winogrande,5-shot,accuracy,0.7237569060773481,0.01256681501569816
tiiuae/falcon-7b,arc:challenge,25-shot,accuracy,0.43600682593856654,0.014491225699230914
tiiuae/falcon-7b,arc:challenge,25-shot,acc_norm,0.4786689419795222,0.014598087973127102
tiiuae/falcon-7b,hellaswag,10-shot,accuracy,0.5797649870543716,0.004925877705771197
tiiuae/falcon-7b,hellaswag,10-shot,acc_norm,0.7813184624576778,0.004125072816630354
tiiuae/falcon-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
tiiuae/falcon-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.04351941398892446
tiiuae/falcon-7b,hendrycksTest-anatomy,5-shot,accuracy,0.2,0.03455473702325438
tiiuae/falcon-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.2,0.03455473702325438
tiiuae/falcon-7b,hendrycksTest-astronomy,5-shot,accuracy,0.24342105263157895,0.034923496688842384
tiiuae/falcon-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.24342105263157895,0.034923496688842384
tiiuae/falcon-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.040936018074033256
tiiuae/falcon-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.040936018074033256
tiiuae/falcon-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2981132075471698,0.028152837942493868
tiiuae/falcon-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2981132075471698,0.028152837942493868
tiiuae/falcon-7b,hendrycksTest-college_biology,5-shot,accuracy,0.2361111111111111,0.03551446610810826
tiiuae/falcon-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.2361111111111111,0.03551446610810826
tiiuae/falcon-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.21,0.040936018074033256
tiiuae/falcon-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.21,0.040936018074033256
tiiuae/falcon-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.044619604333847394
tiiuae/falcon-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.044619604333847394
tiiuae/falcon-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.044619604333847394
tiiuae/falcon-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.044619604333847394
tiiuae/falcon-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.24855491329479767,0.03295304696818318
tiiuae/falcon-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.24855491329479767,0.03295304696818318
tiiuae/falcon-7b,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237656
tiiuae/falcon-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237656
tiiuae/falcon-7b,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.04512608598542128
tiiuae/falcon-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.04512608598542128
tiiuae/falcon-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2851063829787234,0.029513196625539355
tiiuae/falcon-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2851063829787234,0.029513196625539355
tiiuae/falcon-7b,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.04142439719489362
tiiuae/falcon-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.04142439719489362
tiiuae/falcon-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2896551724137931,0.03780019230438015
tiiuae/falcon-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2896551724137931,0.03780019230438015
tiiuae/falcon-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24867724867724866,0.022261817692400168
tiiuae/falcon-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24867724867724866,0.022261817692400168
tiiuae/falcon-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.21428571428571427,0.03670066451047181
tiiuae/falcon-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.21428571428571427,0.03670066451047181
tiiuae/falcon-7b,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
tiiuae/falcon-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
tiiuae/falcon-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.25161290322580643,0.02468597928623996
tiiuae/falcon-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.25161290322580643,0.02468597928623996
tiiuae/falcon-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.030516530732694433
tiiuae/falcon-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.030516530732694433
tiiuae/falcon-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.04648231987117316
tiiuae/falcon-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.04648231987117316
tiiuae/falcon-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.24242424242424243,0.03346409881055953
tiiuae/falcon-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.24242424242424243,0.03346409881055953
tiiuae/falcon-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.19696969696969696,0.028335609732463348
tiiuae/falcon-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.19696969696969696,0.028335609732463348
tiiuae/falcon-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.25906735751295334,0.03161877917935411
tiiuae/falcon-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.25906735751295334,0.03161877917935411
tiiuae/falcon-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.24358974358974358,0.021763733684173926
tiiuae/falcon-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.24358974358974358,0.021763733684173926
tiiuae/falcon-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.026842057873833706
tiiuae/falcon-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.026842057873833706
tiiuae/falcon-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.27310924369747897,0.028942004040998167
tiiuae/falcon-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.27310924369747897,0.028942004040998167
tiiuae/falcon-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.31125827814569534,0.03780445850526733
tiiuae/falcon-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.31125827814569534,0.03780445850526733
tiiuae/falcon-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.23486238532110093,0.01817511051034357
tiiuae/falcon-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.23486238532110093,0.01817511051034357
tiiuae/falcon-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1527777777777778,0.02453632602613422
tiiuae/falcon-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1527777777777778,0.02453632602613422
tiiuae/falcon-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.31862745098039214,0.03270287181482079
tiiuae/falcon-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.31862745098039214,0.03270287181482079
tiiuae/falcon-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.26582278481012656,0.028756799629658342
tiiuae/falcon-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.26582278481012656,0.028756799629658342
tiiuae/falcon-7b,hendrycksTest-human_aging,5-shot,accuracy,0.45739910313901344,0.033435777055830646
tiiuae/falcon-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.45739910313901344,0.033435777055830646
tiiuae/falcon-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2900763358778626,0.03980066246467765
tiiuae/falcon-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2900763358778626,0.03980066246467765
tiiuae/falcon-7b,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.038968789850704164
tiiuae/falcon-7b,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.038968789850704164
tiiuae/falcon-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.3148148148148148,0.04489931073591311
tiiuae/falcon-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3148148148148148,0.04489931073591311
tiiuae/falcon-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25766871165644173,0.03436150827846917
tiiuae/falcon-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25766871165644173,0.03436150827846917
tiiuae/falcon-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.38392857142857145,0.04616143075028547
tiiuae/falcon-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.38392857142857145,0.04616143075028547
tiiuae/falcon-7b,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.04301250399690875
tiiuae/falcon-7b,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.04301250399690875
tiiuae/falcon-7b,hendrycksTest-marketing,5-shot,accuracy,0.31196581196581197,0.030351527323344948
tiiuae/falcon-7b,hendrycksTest-marketing,5-shot,acc_norm,0.31196581196581197,0.030351527323344948
tiiuae/falcon-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.04560480215720683
tiiuae/falcon-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.04560480215720683
tiiuae/falcon-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.3065134099616858,0.016486952893041515
tiiuae/falcon-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3065134099616858,0.016486952893041515
tiiuae/falcon-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.28901734104046245,0.024405173935783238
tiiuae/falcon-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.28901734104046245,0.024405173935783238
tiiuae/falcon-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23910614525139665,0.01426555419233115
tiiuae/falcon-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23910614525139665,0.01426555419233115
tiiuae/falcon-7b,hendrycksTest-nutrition,5-shot,accuracy,0.28104575163398693,0.025738854797818716
tiiuae/falcon-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.28104575163398693,0.025738854797818716
tiiuae/falcon-7b,hendrycksTest-philosophy,5-shot,accuracy,0.2958199356913183,0.025922371788818777
tiiuae/falcon-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.2958199356913183,0.025922371788818777
tiiuae/falcon-7b,hendrycksTest-prehistory,5-shot,accuracy,0.32098765432098764,0.025976566010862737
tiiuae/falcon-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.32098765432098764,0.025976566010862737
tiiuae/falcon-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.30141843971631205,0.02737412888263116
tiiuae/falcon-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.30141843971631205,0.02737412888263116
tiiuae/falcon-7b,hendrycksTest-professional_law,5-shot,accuracy,0.24967405475880053,0.011054538377832325
tiiuae/falcon-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.24967405475880053,0.011054538377832325
tiiuae/falcon-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.27941176470588236,0.027257202606114948
tiiuae/falcon-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.27941176470588236,0.027257202606114948
tiiuae/falcon-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2679738562091503,0.017917974069594722
tiiuae/falcon-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2679738562091503,0.017917974069594722
tiiuae/falcon-7b,hendrycksTest-public_relations,5-shot,accuracy,0.35454545454545455,0.045820048415054174
tiiuae/falcon-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.35454545454545455,0.045820048415054174
tiiuae/falcon-7b,hendrycksTest-security_studies,5-shot,accuracy,0.2693877551020408,0.02840125202902294
tiiuae/falcon-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.2693877551020408,0.02840125202902294
tiiuae/falcon-7b,hendrycksTest-sociology,5-shot,accuracy,0.35323383084577115,0.03379790611796777
tiiuae/falcon-7b,hendrycksTest-sociology,5-shot,acc_norm,0.35323383084577115,0.03379790611796777
tiiuae/falcon-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.39,0.04902071300001975
tiiuae/falcon-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.39,0.04902071300001975
tiiuae/falcon-7b,hendrycksTest-virology,5-shot,accuracy,0.3795180722891566,0.03777798822748017
tiiuae/falcon-7b,hendrycksTest-virology,5-shot,acc_norm,0.3795180722891566,0.03777798822748017
tiiuae/falcon-7b,hendrycksTest-world_religions,5-shot,accuracy,0.34502923976608185,0.036459813773888065
tiiuae/falcon-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.34502923976608185,0.036459813773888065
tiiuae/falcon-7b,truthfulqa:mc,0-shot,mc1,0.22399020807833536,0.014594964329474202
tiiuae/falcon-7b,truthfulqa:mc,0-shot,mc2,0.34263825539848,0.01327555829964236
tiiuae/falcon-7b,minerva_math_precalc,5-shot,accuracy,0.02564102564102564,0.006770627800780447
tiiuae/falcon-7b,minerva_math_prealgebra,5-shot,accuracy,0.04822043628013777,0.007263135212103655
tiiuae/falcon-7b,minerva_math_num_theory,5-shot,accuracy,0.014814814814814815,0.005203704987512651
tiiuae/falcon-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.004900093088615795
tiiuae/falcon-7b,minerva_math_geometry,5-shot,accuracy,0.018789144050104383,0.006210416427997393
tiiuae/falcon-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.023206751054852322,0.006922738487143304
tiiuae/falcon-7b,minerva_math_algebra,5-shot,accuracy,0.019376579612468407,0.004002647498105363
tiiuae/falcon-7b,fld_default,0-shot,accuracy,0.0,
tiiuae/falcon-7b,fld_star,0-shot,accuracy,0.0,
tiiuae/falcon-7b,arithmetic_3da,5-shot,accuracy,0.148,0.007942262887231025
tiiuae/falcon-7b,arithmetic_3ds,5-shot,accuracy,0.415,0.011020354990292212
tiiuae/falcon-7b,arithmetic_4da,5-shot,accuracy,0.0035,0.001320888857431578
tiiuae/falcon-7b,arithmetic_2ds,5-shot,accuracy,0.5495,0.01112819811994288
tiiuae/falcon-7b,arithmetic_5ds,5-shot,accuracy,0.082,0.006136515983374219
tiiuae/falcon-7b,arithmetic_5da,5-shot,accuracy,0.0,
tiiuae/falcon-7b,arithmetic_1dc,5-shot,accuracy,0.08,0.006067817499282826
tiiuae/falcon-7b,arithmetic_4ds,5-shot,accuracy,0.11,0.006998177422988276
tiiuae/falcon-7b,arithmetic_2dm,5-shot,accuracy,0.193,0.008826916632018986
tiiuae/falcon-7b,arithmetic_2da,5-shot,accuracy,0.799,0.008963239918406496
tiiuae/falcon-7b,gsm8k_cot,5-shot,accuracy,0.07202426080363912,0.007121147983537127
tiiuae/falcon-7b,anli_r2,0-shot,brier_score,0.7192933891095322,
tiiuae/falcon-7b,anli_r3,0-shot,brier_score,0.7153934424883512,
tiiuae/falcon-7b,anli_r1,0-shot,brier_score,0.7383201344636011,
tiiuae/falcon-7b,xnli_eu,0-shot,brier_score,0.9861818673503582,
tiiuae/falcon-7b,xnli_vi,0-shot,brier_score,0.9775102639166516,
tiiuae/falcon-7b,xnli_ru,0-shot,brier_score,0.8089869795920962,
tiiuae/falcon-7b,xnli_zh,0-shot,brier_score,1.0012841309225609,
tiiuae/falcon-7b,xnli_tr,0-shot,brier_score,0.9627727685034017,
tiiuae/falcon-7b,xnli_fr,0-shot,brier_score,0.7468798318836768,
tiiuae/falcon-7b,xnli_en,0-shot,brier_score,0.6597330345759265,
tiiuae/falcon-7b,xnli_ur,0-shot,brier_score,1.3018299414459367,
tiiuae/falcon-7b,xnli_ar,0-shot,brier_score,1.2469610015956896,
tiiuae/falcon-7b,xnli_de,0-shot,brier_score,0.8410614069823841,
tiiuae/falcon-7b,xnli_hi,0-shot,brier_score,1.132654288498891,
tiiuae/falcon-7b,xnli_es,0-shot,brier_score,0.8190608375779431,
tiiuae/falcon-7b,xnli_bg,0-shot,brier_score,0.8950252179652899,
tiiuae/falcon-7b,xnli_sw,0-shot,brier_score,1.112644389463286,
tiiuae/falcon-7b,xnli_el,0-shot,brier_score,0.8724657539692695,
tiiuae/falcon-7b,xnli_th,0-shot,brier_score,0.9463512763116717,
tiiuae/falcon-7b,logiqa2,0-shot,brier_score,1.039383873940183,
tiiuae/falcon-7b,mathqa,5-shot,brier_score,0.9190095944774515,
tiiuae/falcon-7b,lambada_standard,0-shot,perplexity,3.9540875826593176,0.07735004335637238
tiiuae/falcon-7b,lambada_standard,0-shot,accuracy,0.6881428294197555,0.006454004061618762
tiiuae/falcon-7b,lambada_openai,0-shot,perplexity,3.367914054594161,0.06449443901464368
tiiuae/falcon-7b,lambada_openai,0-shot,accuracy,0.7457791577721715,0.0060662844467191356
facebook/xglm-4.5B,drop,3-shot,accuracy,0.06480704697986577,0.0025211656446620548
facebook/xglm-4.5B,drop,3-shot,f1,0.11480180369127503,0.002765932447728658
facebook/xglm-4.5B,arc:challenge,25-shot,accuracy,0.2977815699658703,0.01336308010724449
facebook/xglm-4.5B,arc:challenge,25-shot,acc_norm,0.3148464163822526,0.01357265770308495
facebook/xglm-4.5B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.2,0.04020151261036845
facebook/xglm-4.5B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.2,0.04020151261036845
facebook/xglm-4.5B,hendrycksTest-anatomy,5-shot,accuracy,0.3037037037037037,0.039725528847851375
facebook/xglm-4.5B,hendrycksTest-anatomy,5-shot,acc_norm,0.3037037037037037,0.039725528847851375
facebook/xglm-4.5B,hendrycksTest-astronomy,5-shot,accuracy,0.21052631578947367,0.03317672787533157
facebook/xglm-4.5B,hendrycksTest-astronomy,5-shot,acc_norm,0.21052631578947367,0.03317672787533157
facebook/xglm-4.5B,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.04229525846816506
facebook/xglm-4.5B,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.04229525846816506
facebook/xglm-4.5B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.22264150943396227,0.0256042334708991
facebook/xglm-4.5B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.22264150943396227,0.0256042334708991
facebook/xglm-4.5B,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
facebook/xglm-4.5B,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.03685651095897532
facebook/xglm-4.5B,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909283
facebook/xglm-4.5B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909283
facebook/xglm-4.5B,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.04512608598542128
facebook/xglm-4.5B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.04512608598542128
facebook/xglm-4.5B,hendrycksTest-college_mathematics,5-shot,accuracy,0.29,0.04560480215720684
facebook/xglm-4.5B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.29,0.04560480215720684
facebook/xglm-4.5B,hendrycksTest-college_medicine,5-shot,accuracy,0.27167630057803466,0.03391750322321659
facebook/xglm-4.5B,hendrycksTest-college_medicine,5-shot,acc_norm,0.27167630057803466,0.03391750322321659
facebook/xglm-4.5B,hendrycksTest-college_physics,5-shot,accuracy,0.19607843137254902,0.039505818611799616
facebook/xglm-4.5B,hendrycksTest-college_physics,5-shot,acc_norm,0.19607843137254902,0.039505818611799616
facebook/xglm-4.5B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.045126085985421276
facebook/xglm-4.5B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.045126085985421276
facebook/xglm-4.5B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2425531914893617,0.028020226271200214
facebook/xglm-4.5B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2425531914893617,0.028020226271200214
facebook/xglm-4.5B,hendrycksTest-econometrics,5-shot,accuracy,0.2807017543859649,0.04227054451232199
facebook/xglm-4.5B,hendrycksTest-econometrics,5-shot,acc_norm,0.2807017543859649,0.04227054451232199
facebook/xglm-4.5B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2,0.0333333333333333
facebook/xglm-4.5B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2,0.0333333333333333
facebook/xglm-4.5B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25132275132275134,0.022340482339643895
facebook/xglm-4.5B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25132275132275134,0.022340482339643895
facebook/xglm-4.5B,hendrycksTest-formal_logic,5-shot,accuracy,0.1746031746031746,0.03395490020856109
facebook/xglm-4.5B,hendrycksTest-formal_logic,5-shot,acc_norm,0.1746031746031746,0.03395490020856109
facebook/xglm-4.5B,hendrycksTest-global_facts,5-shot,accuracy,0.34,0.04760952285695236
facebook/xglm-4.5B,hendrycksTest-global_facts,5-shot,acc_norm,0.34,0.04760952285695236
facebook/xglm-4.5B,hendrycksTest-high_school_biology,5-shot,accuracy,0.25806451612903225,0.02489246917246284
facebook/xglm-4.5B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.25806451612903225,0.02489246917246284
facebook/xglm-4.5B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.26108374384236455,0.030903796952114503
facebook/xglm-4.5B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.26108374384236455,0.030903796952114503
facebook/xglm-4.5B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.04648231987117316
facebook/xglm-4.5B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.04648231987117316
facebook/xglm-4.5B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.20606060606060606,0.03158415324047711
facebook/xglm-4.5B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.20606060606060606,0.03158415324047711
facebook/xglm-4.5B,hendrycksTest-high_school_geography,5-shot,accuracy,0.2727272727272727,0.03173071239071724
facebook/xglm-4.5B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2727272727272727,0.03173071239071724
facebook/xglm-4.5B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.26424870466321243,0.03182155050916649
facebook/xglm-4.5B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.26424870466321243,0.03182155050916649
facebook/xglm-4.5B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2564102564102564,0.02213908110397153
facebook/xglm-4.5B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2564102564102564,0.02213908110397153
facebook/xglm-4.5B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.02646611753895991
facebook/xglm-4.5B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.02646611753895991
facebook/xglm-4.5B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.22268907563025211,0.027025433498882374
facebook/xglm-4.5B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.22268907563025211,0.027025433498882374
facebook/xglm-4.5B,hendrycksTest-high_school_physics,5-shot,accuracy,0.26490066225165565,0.03603038545360384
facebook/xglm-4.5B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.26490066225165565,0.03603038545360384
facebook/xglm-4.5B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.24770642201834864,0.018508143602547822
facebook/xglm-4.5B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.24770642201834864,0.018508143602547822
facebook/xglm-4.5B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.28703703703703703,0.030851992993257013
facebook/xglm-4.5B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.28703703703703703,0.030851992993257013
facebook/xglm-4.5B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.20588235294117646,0.028379449451588674
facebook/xglm-4.5B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.20588235294117646,0.028379449451588674
facebook/xglm-4.5B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.21518987341772153,0.02675082699467617
facebook/xglm-4.5B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.21518987341772153,0.02675082699467617
facebook/xglm-4.5B,hendrycksTest-human_aging,5-shot,accuracy,0.2242152466367713,0.02799153425851953
facebook/xglm-4.5B,hendrycksTest-human_aging,5-shot,acc_norm,0.2242152466367713,0.02799153425851953
facebook/xglm-4.5B,hendrycksTest-human_sexuality,5-shot,accuracy,0.26717557251908397,0.03880848301082396
facebook/xglm-4.5B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.26717557251908397,0.03880848301082396
facebook/xglm-4.5B,hendrycksTest-international_law,5-shot,accuracy,0.33884297520661155,0.043207678075366705
facebook/xglm-4.5B,hendrycksTest-international_law,5-shot,acc_norm,0.33884297520661155,0.043207678075366705
facebook/xglm-4.5B,hendrycksTest-jurisprudence,5-shot,accuracy,0.23148148148148148,0.040774947092526284
facebook/xglm-4.5B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.23148148148148148,0.040774947092526284
facebook/xglm-4.5B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.035590395316173425
facebook/xglm-4.5B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.035590395316173425
facebook/xglm-4.5B,hendrycksTest-machine_learning,5-shot,accuracy,0.24107142857142858,0.04059867246952687
facebook/xglm-4.5B,hendrycksTest-machine_learning,5-shot,acc_norm,0.24107142857142858,0.04059867246952687
facebook/xglm-4.5B,hendrycksTest-management,5-shot,accuracy,0.2912621359223301,0.044986763205729224
facebook/xglm-4.5B,hendrycksTest-management,5-shot,acc_norm,0.2912621359223301,0.044986763205729224
facebook/xglm-4.5B,hendrycksTest-marketing,5-shot,accuracy,0.2222222222222222,0.02723601394619668
facebook/xglm-4.5B,hendrycksTest-marketing,5-shot,acc_norm,0.2222222222222222,0.02723601394619668
facebook/xglm-4.5B,hendrycksTest-medical_genetics,5-shot,accuracy,0.24,0.042923469599092816
facebook/xglm-4.5B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.24,0.042923469599092816
facebook/xglm-4.5B,hendrycksTest-miscellaneous,5-shot,accuracy,0.20434227330779056,0.014419123980931895
facebook/xglm-4.5B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.20434227330779056,0.014419123980931895
facebook/xglm-4.5B,hendrycksTest-moral_disputes,5-shot,accuracy,0.27167630057803466,0.02394851290546836
facebook/xglm-4.5B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.27167630057803466,0.02394851290546836
facebook/xglm-4.5B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24134078212290502,0.014310999547961459
facebook/xglm-4.5B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24134078212290502,0.014310999547961459
facebook/xglm-4.5B,hendrycksTest-nutrition,5-shot,accuracy,0.26143790849673204,0.025160998214292456
facebook/xglm-4.5B,hendrycksTest-nutrition,5-shot,acc_norm,0.26143790849673204,0.025160998214292456
facebook/xglm-4.5B,hendrycksTest-philosophy,5-shot,accuracy,0.3022508038585209,0.02608270069539965
facebook/xglm-4.5B,hendrycksTest-philosophy,5-shot,acc_norm,0.3022508038585209,0.02608270069539965
facebook/xglm-4.5B,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.024569223600460842
facebook/xglm-4.5B,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.024569223600460842
facebook/xglm-4.5B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2765957446808511,0.026684564340461004
facebook/xglm-4.5B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2765957446808511,0.026684564340461004
facebook/xglm-4.5B,hendrycksTest-professional_law,5-shot,accuracy,0.273142112125163,0.01138015056783043
facebook/xglm-4.5B,hendrycksTest-professional_law,5-shot,acc_norm,0.273142112125163,0.01138015056783043
facebook/xglm-4.5B,hendrycksTest-professional_medicine,5-shot,accuracy,0.3272058823529412,0.02850145286039656
facebook/xglm-4.5B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.3272058823529412,0.02850145286039656
facebook/xglm-4.5B,hendrycksTest-professional_psychology,5-shot,accuracy,0.25,0.01751781884501444
facebook/xglm-4.5B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25,0.01751781884501444
facebook/xglm-4.5B,hendrycksTest-public_relations,5-shot,accuracy,0.23636363636363636,0.04069306319721376
facebook/xglm-4.5B,hendrycksTest-public_relations,5-shot,acc_norm,0.23636363636363636,0.04069306319721376
facebook/xglm-4.5B,hendrycksTest-security_studies,5-shot,accuracy,0.23673469387755103,0.027212835884073156
facebook/xglm-4.5B,hendrycksTest-security_studies,5-shot,acc_norm,0.23673469387755103,0.027212835884073156
facebook/xglm-4.5B,hendrycksTest-sociology,5-shot,accuracy,0.31343283582089554,0.032801882053486435
facebook/xglm-4.5B,hendrycksTest-sociology,5-shot,acc_norm,0.31343283582089554,0.032801882053486435
facebook/xglm-4.5B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.04512608598542127
facebook/xglm-4.5B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.04512608598542127
facebook/xglm-4.5B,hendrycksTest-virology,5-shot,accuracy,0.18674698795180722,0.030338749144500597
facebook/xglm-4.5B,hendrycksTest-virology,5-shot,acc_norm,0.18674698795180722,0.030338749144500597
facebook/xglm-4.5B,hendrycksTest-world_religions,5-shot,accuracy,0.2222222222222222,0.031885780176863984
facebook/xglm-4.5B,hendrycksTest-world_religions,5-shot,acc_norm,0.2222222222222222,0.031885780176863984
facebook/xglm-4.5B,truthfulqa:mc,0-shot,mc1,0.20807833537331702,0.014210503473576611
facebook/xglm-4.5B,truthfulqa:mc,0-shot,mc2,0.35839394850406386,0.013785991762306882
jisukim8873/falcon-7B-case-8,arc:challenge,25-shot,accuracy,0.4598976109215017,0.014564318856924848
jisukim8873/falcon-7B-case-8,arc:challenge,25-shot,acc_norm,0.4948805460750853,0.014610624890309157
jisukim8873/falcon-7B-case-8,hellaswag,10-shot,accuracy,0.5965943039235212,0.004895782107786499
jisukim8873/falcon-7B-case-8,hellaswag,10-shot,acc_norm,0.7855008962358097,0.0040963551251175165
jisukim8873/falcon-7B-case-8,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-8,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-8,hendrycksTest-anatomy,5-shot,accuracy,0.3111111111111111,0.039992628766177214
jisukim8873/falcon-7B-case-8,hendrycksTest-anatomy,5-shot,acc_norm,0.3111111111111111,0.039992628766177214
jisukim8873/falcon-7B-case-8,hendrycksTest-astronomy,5-shot,accuracy,0.2894736842105263,0.03690677986137283
jisukim8873/falcon-7B-case-8,hendrycksTest-astronomy,5-shot,acc_norm,0.2894736842105263,0.03690677986137283
jisukim8873/falcon-7B-case-8,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.040936018074033256
jisukim8873/falcon-7B-case-8,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.040936018074033256
jisukim8873/falcon-7B-case-8,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3433962264150943,0.02922452646912479
jisukim8873/falcon-7B-case-8,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3433962264150943,0.02922452646912479
jisukim8873/falcon-7B-case-8,hendrycksTest-college_biology,5-shot,accuracy,0.2847222222222222,0.037738099906869355
jisukim8873/falcon-7B-case-8,hendrycksTest-college_biology,5-shot,acc_norm,0.2847222222222222,0.037738099906869355
jisukim8873/falcon-7B-case-8,hendrycksTest-college_chemistry,5-shot,accuracy,0.11,0.03144660377352203
jisukim8873/falcon-7B-case-8,hendrycksTest-college_chemistry,5-shot,acc_norm,0.11,0.03144660377352203
jisukim8873/falcon-7B-case-8,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-8,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-8,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.04512608598542127
jisukim8873/falcon-7B-case-8,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.04512608598542127
jisukim8873/falcon-7B-case-8,hendrycksTest-college_medicine,5-shot,accuracy,0.2832369942196532,0.034355680560478746
jisukim8873/falcon-7B-case-8,hendrycksTest-college_medicine,5-shot,acc_norm,0.2832369942196532,0.034355680560478746
jisukim8873/falcon-7B-case-8,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617747
jisukim8873/falcon-7B-case-8,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617747
jisukim8873/falcon-7B-case-8,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.04878317312145633
jisukim8873/falcon-7B-case-8,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.04878317312145633
jisukim8873/falcon-7B-case-8,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3276595744680851,0.030683020843231004
jisukim8873/falcon-7B-case-8,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3276595744680851,0.030683020843231004
jisukim8873/falcon-7B-case-8,hendrycksTest-econometrics,5-shot,accuracy,0.21052631578947367,0.0383515395439942
jisukim8873/falcon-7B-case-8,hendrycksTest-econometrics,5-shot,acc_norm,0.21052631578947367,0.0383515395439942
jisukim8873/falcon-7B-case-8,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.036646663372252565
jisukim8873/falcon-7B-case-8,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.036646663372252565
jisukim8873/falcon-7B-case-8,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918417
jisukim8873/falcon-7B-case-8,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.022569897074918417
jisukim8873/falcon-7B-case-8,hendrycksTest-formal_logic,5-shot,accuracy,0.18253968253968253,0.034550710191021496
jisukim8873/falcon-7B-case-8,hendrycksTest-formal_logic,5-shot,acc_norm,0.18253968253968253,0.034550710191021496
jisukim8873/falcon-7B-case-8,hendrycksTest-global_facts,5-shot,accuracy,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-8,hendrycksTest-global_facts,5-shot,acc_norm,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_biology,5-shot,accuracy,0.3161290322580645,0.026450874489042774
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3161290322580645,0.026450874489042774
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.33004926108374383,0.033085304262282574
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.33004926108374383,0.033085304262282574
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_european_history,5-shot,accuracy,0.38181818181818183,0.03793713171165634
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.38181818181818183,0.03793713171165634
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_geography,5-shot,accuracy,0.3383838383838384,0.03371124142626302
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3383838383838384,0.03371124142626302
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.27979274611398963,0.032396370467357036
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.27979274611398963,0.032396370467357036
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2641025641025641,0.02235219373745327
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2641025641025641,0.02235219373745327
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.026719240783712163
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25925925925925924,0.026719240783712163
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.28991596638655465,0.029472485833136084
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.28991596638655465,0.029472485833136084
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_physics,5-shot,accuracy,0.31125827814569534,0.03780445850526732
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_physics,5-shot,acc_norm,0.31125827814569534,0.03780445850526732
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_psychology,5-shot,accuracy,0.29908256880733947,0.019630417285415175
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.29908256880733947,0.019630417285415175
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_statistics,5-shot,accuracy,0.22685185185185186,0.028561650102422263
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.22685185185185186,0.028561650102422263
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_us_history,5-shot,accuracy,0.28431372549019607,0.031660096793998116
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.28431372549019607,0.031660096793998116
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_world_history,5-shot,accuracy,0.29535864978902954,0.029696338713422876
jisukim8873/falcon-7B-case-8,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.29535864978902954,0.029696338713422876
jisukim8873/falcon-7B-case-8,hendrycksTest-human_aging,5-shot,accuracy,0.3901345291479821,0.03273766725459157
jisukim8873/falcon-7B-case-8,hendrycksTest-human_aging,5-shot,acc_norm,0.3901345291479821,0.03273766725459157
jisukim8873/falcon-7B-case-8,hendrycksTest-human_sexuality,5-shot,accuracy,0.3282442748091603,0.04118438565806298
jisukim8873/falcon-7B-case-8,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3282442748091603,0.04118438565806298
jisukim8873/falcon-7B-case-8,hendrycksTest-international_law,5-shot,accuracy,0.371900826446281,0.044120158066245044
jisukim8873/falcon-7B-case-8,hendrycksTest-international_law,5-shot,acc_norm,0.371900826446281,0.044120158066245044
jisukim8873/falcon-7B-case-8,hendrycksTest-jurisprudence,5-shot,accuracy,0.32407407407407407,0.04524596007030048
jisukim8873/falcon-7B-case-8,hendrycksTest-jurisprudence,5-shot,acc_norm,0.32407407407407407,0.04524596007030048
jisukim8873/falcon-7B-case-8,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.035590395316173425
jisukim8873/falcon-7B-case-8,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.035590395316173425
jisukim8873/falcon-7B-case-8,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.04287858751340456
jisukim8873/falcon-7B-case-8,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.04287858751340456
jisukim8873/falcon-7B-case-8,hendrycksTest-management,5-shot,accuracy,0.24271844660194175,0.04245022486384493
jisukim8873/falcon-7B-case-8,hendrycksTest-management,5-shot,acc_norm,0.24271844660194175,0.04245022486384493
jisukim8873/falcon-7B-case-8,hendrycksTest-marketing,5-shot,accuracy,0.3504273504273504,0.03125610824421879
jisukim8873/falcon-7B-case-8,hendrycksTest-marketing,5-shot,acc_norm,0.3504273504273504,0.03125610824421879
jisukim8873/falcon-7B-case-8,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
jisukim8873/falcon-7B-case-8,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.04648231987117316
jisukim8873/falcon-7B-case-8,hendrycksTest-miscellaneous,5-shot,accuracy,0.37292464878671777,0.01729286826945392
jisukim8873/falcon-7B-case-8,hendrycksTest-miscellaneous,5-shot,acc_norm,0.37292464878671777,0.01729286826945392
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_disputes,5-shot,accuracy,0.3670520231213873,0.025950054337654082
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3670520231213873,0.025950054337654082
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24022346368715083,0.014288343803925303
jisukim8873/falcon-7B-case-8,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24022346368715083,0.014288343803925303
jisukim8873/falcon-7B-case-8,hendrycksTest-nutrition,5-shot,accuracy,0.30392156862745096,0.026336613469046633
jisukim8873/falcon-7B-case-8,hendrycksTest-nutrition,5-shot,acc_norm,0.30392156862745096,0.026336613469046633
jisukim8873/falcon-7B-case-8,hendrycksTest-philosophy,5-shot,accuracy,0.3504823151125402,0.027098652621301754
jisukim8873/falcon-7B-case-8,hendrycksTest-philosophy,5-shot,acc_norm,0.3504823151125402,0.027098652621301754
jisukim8873/falcon-7B-case-8,hendrycksTest-prehistory,5-shot,accuracy,0.3148148148148148,0.025842248700902168
jisukim8873/falcon-7B-case-8,hendrycksTest-prehistory,5-shot,acc_norm,0.3148148148148148,0.025842248700902168
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.026789172351140242
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.026789172351140242
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_law,5-shot,accuracy,0.2737940026075619,0.011388612167979395
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_law,5-shot,acc_norm,0.2737940026075619,0.011388612167979395
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_medicine,5-shot,accuracy,0.26838235294117646,0.026917481224377232
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_medicine,5-shot,acc_norm,0.26838235294117646,0.026917481224377232
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_psychology,5-shot,accuracy,0.29901960784313725,0.018521756215423024
jisukim8873/falcon-7B-case-8,hendrycksTest-professional_psychology,5-shot,acc_norm,0.29901960784313725,0.018521756215423024
jisukim8873/falcon-7B-case-8,hendrycksTest-public_relations,5-shot,accuracy,0.33636363636363636,0.04525393596302505
jisukim8873/falcon-7B-case-8,hendrycksTest-public_relations,5-shot,acc_norm,0.33636363636363636,0.04525393596302505
jisukim8873/falcon-7B-case-8,hendrycksTest-security_studies,5-shot,accuracy,0.3183673469387755,0.029822533793982076
jisukim8873/falcon-7B-case-8,hendrycksTest-security_studies,5-shot,acc_norm,0.3183673469387755,0.029822533793982076
jisukim8873/falcon-7B-case-8,hendrycksTest-sociology,5-shot,accuracy,0.3880597014925373,0.0344578996436275
jisukim8873/falcon-7B-case-8,hendrycksTest-sociology,5-shot,acc_norm,0.3880597014925373,0.0344578996436275
jisukim8873/falcon-7B-case-8,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.49,0.05024183937956912
jisukim8873/falcon-7B-case-8,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.49,0.05024183937956912
jisukim8873/falcon-7B-case-8,hendrycksTest-virology,5-shot,accuracy,0.3614457831325301,0.037400593820293204
jisukim8873/falcon-7B-case-8,hendrycksTest-virology,5-shot,acc_norm,0.3614457831325301,0.037400593820293204
jisukim8873/falcon-7B-case-8,hendrycksTest-world_religions,5-shot,accuracy,0.3508771929824561,0.03660298834049162
jisukim8873/falcon-7B-case-8,hendrycksTest-world_religions,5-shot,acc_norm,0.3508771929824561,0.03660298834049162
jisukim8873/falcon-7B-case-8,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.015321821688476196
jisukim8873/falcon-7B-case-8,truthfulqa:mc,0-shot,mc2,0.37575591444529527,0.014304714495441502
jisukim8873/falcon-7B-case-8,winogrande,5-shot,accuracy,0.7048145224940805,0.012819410741754763
jisukim8873/falcon-7B-case-8,gsm8k,5-shot,accuracy,0.07505686125852919,0.007257633145486642
jisukim8873/falcon-7B-case-8,minerva_math_precalc,5-shot,accuracy,0.016483516483516484,0.005454029764766741
jisukim8873/falcon-7B-case-8,minerva_math_prealgebra,5-shot,accuracy,0.03673938002296211,0.006377907088210899
jisukim8873/falcon-7B-case-8,minerva_math_num_theory,5-shot,accuracy,0.007407407407407408,0.0036933821684372575
jisukim8873/falcon-7B-case-8,minerva_math_intermediate_algebra,5-shot,accuracy,0.015503875968992248,0.004113617238360444
jisukim8873/falcon-7B-case-8,minerva_math_geometry,5-shot,accuracy,0.010438413361169102,0.0046486271171846455
jisukim8873/falcon-7B-case-8,minerva_math_counting_and_prob,5-shot,accuracy,0.02109704641350211,0.0066076963656265374
jisukim8873/falcon-7B-case-8,minerva_math_algebra,5-shot,accuracy,0.019376579612468407,0.0040026474981053545
jisukim8873/falcon-7B-case-8,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-8,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-8,arithmetic_3da,5-shot,accuracy,0.2185,0.009242379877114725
jisukim8873/falcon-7B-case-8,arithmetic_3ds,5-shot,accuracy,0.646,0.010695756149043483
jisukim8873/falcon-7B-case-8,arithmetic_4da,5-shot,accuracy,0.0085,0.002053285901060994
jisukim8873/falcon-7B-case-8,arithmetic_2ds,5-shot,accuracy,0.6345,0.010770927603540937
jisukim8873/falcon-7B-case-8,arithmetic_5ds,5-shot,accuracy,0.056,0.005142491867889066
jisukim8873/falcon-7B-case-8,arithmetic_5da,5-shot,accuracy,0.0025,0.0011169148353275223
jisukim8873/falcon-7B-case-8,arithmetic_1dc,5-shot,accuracy,0.053,0.0050107937521926766
jisukim8873/falcon-7B-case-8,arithmetic_4ds,5-shot,accuracy,0.1425,0.007818403847292692
jisukim8873/falcon-7B-case-8,arithmetic_2dm,5-shot,accuracy,0.2425,0.00958607434827748
jisukim8873/falcon-7B-case-8,arithmetic_2da,5-shot,accuracy,0.716,0.010085775202269415
jisukim8873/falcon-7B-case-8,gsm8k_cot,5-shot,accuracy,0.08946171341925702,0.00786158304993971
jisukim8873/falcon-7B-case-8,anli_r2,0-shot,brier_score,0.8665276269305205,
jisukim8873/falcon-7B-case-8,anli_r3,0-shot,brier_score,0.8318243198290138,
jisukim8873/falcon-7B-case-8,anli_r1,0-shot,brier_score,0.8939690888764009,
jisukim8873/falcon-7B-case-8,xnli_eu,0-shot,brier_score,1.0427344013967832,
jisukim8873/falcon-7B-case-8,xnli_vi,0-shot,brier_score,1.0487135581024136,
jisukim8873/falcon-7B-case-8,xnli_ru,0-shot,brier_score,0.8221328820374796,
jisukim8873/falcon-7B-case-8,xnli_zh,0-shot,brier_score,1.0244308666680741,
jisukim8873/falcon-7B-case-8,xnli_tr,0-shot,brier_score,0.9970781849876954,
jisukim8873/falcon-7B-case-8,xnli_fr,0-shot,brier_score,0.7436567906026784,
jisukim8873/falcon-7B-case-8,xnli_en,0-shot,brier_score,0.6425702304442446,
jisukim8873/falcon-7B-case-8,xnli_ur,0-shot,brier_score,1.3117764974375834,
jisukim8873/falcon-7B-case-8,xnli_ar,0-shot,brier_score,1.2508515010911752,
jisukim8873/falcon-7B-case-8,xnli_de,0-shot,brier_score,0.8387305217972838,
jisukim8873/falcon-7B-case-8,xnli_hi,0-shot,brier_score,1.1022494172288069,
jisukim8873/falcon-7B-case-8,xnli_es,0-shot,brier_score,0.8133629952055023,
jisukim8873/falcon-7B-case-8,xnli_bg,0-shot,brier_score,0.9484830535778097,
jisukim8873/falcon-7B-case-8,xnli_sw,0-shot,brier_score,1.0888680824673866,
jisukim8873/falcon-7B-case-8,xnli_el,0-shot,brier_score,0.9888240024756009,
jisukim8873/falcon-7B-case-8,xnli_th,0-shot,brier_score,0.9812608603086016,
jisukim8873/falcon-7B-case-8,logiqa2,0-shot,brier_score,1.059925394028478,
jisukim8873/falcon-7B-case-8,mathqa,5-shot,brier_score,0.9414128848103127,
jisukim8873/falcon-7B-case-8,lambada_standard,0-shot,perplexity,4.213587010663793,0.0909206003026987
jisukim8873/falcon-7B-case-8,lambada_standard,0-shot,accuracy,0.6557345235784979,0.006619464143312433
jisukim8873/falcon-7B-case-8,lambada_openai,0-shot,perplexity,3.391177573570244,0.07192541543832812
jisukim8873/falcon-7B-case-8,lambada_openai,0-shot,accuracy,0.7269551717446148,0.006207016010291535
EleutherAI/pythia-410m,arc:challenge,25-shot,accuracy,0.23122866894197952,0.012320858834772283
EleutherAI/pythia-410m,arc:challenge,25-shot,acc_norm,0.2619453924914676,0.012849054826858115
EleutherAI/pythia-410m,hellaswag,10-shot,accuracy,0.33947420832503483,0.004725630911520322
EleutherAI/pythia-410m,hellaswag,10-shot,acc_norm,0.4084843656642103,0.00490548949400508
EleutherAI/pythia-410m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-410m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.04351941398892446
EleutherAI/pythia-410m,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.03885004245800255
EleutherAI/pythia-410m,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.03885004245800255
EleutherAI/pythia-410m,hendrycksTest-astronomy,5-shot,accuracy,0.21710526315789475,0.03355045304882924
EleutherAI/pythia-410m,hendrycksTest-astronomy,5-shot,acc_norm,0.21710526315789475,0.03355045304882924
EleutherAI/pythia-410m,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.044619604333847394
EleutherAI/pythia-410m,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.044619604333847394
EleutherAI/pythia-410m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2490566037735849,0.02661648298050171
EleutherAI/pythia-410m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2490566037735849,0.02661648298050171
EleutherAI/pythia-410m,hendrycksTest-college_biology,5-shot,accuracy,0.2777777777777778,0.03745554791462457
EleutherAI/pythia-410m,hendrycksTest-college_biology,5-shot,acc_norm,0.2777777777777778,0.03745554791462457
EleutherAI/pythia-410m,hendrycksTest-college_chemistry,5-shot,accuracy,0.42,0.049604496374885836
EleutherAI/pythia-410m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.42,0.049604496374885836
EleutherAI/pythia-410m,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.04725815626252604
EleutherAI/pythia-410m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.04725815626252604
EleutherAI/pythia-410m,hendrycksTest-college_mathematics,5-shot,accuracy,0.34,0.047609522856952365
EleutherAI/pythia-410m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.34,0.047609522856952365
EleutherAI/pythia-410m,hendrycksTest-college_medicine,5-shot,accuracy,0.20809248554913296,0.03095289021774988
EleutherAI/pythia-410m,hendrycksTest-college_medicine,5-shot,acc_norm,0.20809248554913296,0.03095289021774988
EleutherAI/pythia-410m,hendrycksTest-college_physics,5-shot,accuracy,0.24509803921568626,0.04280105837364395
EleutherAI/pythia-410m,hendrycksTest-college_physics,5-shot,acc_norm,0.24509803921568626,0.04280105837364395
EleutherAI/pythia-410m,hendrycksTest-computer_security,5-shot,accuracy,0.2,0.04020151261036843
EleutherAI/pythia-410m,hendrycksTest-computer_security,5-shot,acc_norm,0.2,0.04020151261036843
EleutherAI/pythia-410m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.28936170212765955,0.029644006577009618
EleutherAI/pythia-410m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.28936170212765955,0.029644006577009618
EleutherAI/pythia-410m,hendrycksTest-econometrics,5-shot,accuracy,0.22807017543859648,0.03947152782669415
EleutherAI/pythia-410m,hendrycksTest-econometrics,5-shot,acc_norm,0.22807017543859648,0.03947152782669415
EleutherAI/pythia-410m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.22758620689655173,0.03493950380131183
EleutherAI/pythia-410m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.22758620689655173,0.03493950380131183
EleutherAI/pythia-410m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.23809523809523808,0.021935878081184756
EleutherAI/pythia-410m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.23809523809523808,0.021935878081184756
EleutherAI/pythia-410m,hendrycksTest-formal_logic,5-shot,accuracy,0.29365079365079366,0.04073524322147125
EleutherAI/pythia-410m,hendrycksTest-formal_logic,5-shot,acc_norm,0.29365079365079366,0.04073524322147125
EleutherAI/pythia-410m,hendrycksTest-global_facts,5-shot,accuracy,0.17,0.03775251680686371
EleutherAI/pythia-410m,hendrycksTest-global_facts,5-shot,acc_norm,0.17,0.03775251680686371
EleutherAI/pythia-410m,hendrycksTest-high_school_biology,5-shot,accuracy,0.29354838709677417,0.025906087021319288
EleutherAI/pythia-410m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.29354838709677417,0.025906087021319288
EleutherAI/pythia-410m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.26108374384236455,0.030903796952114475
EleutherAI/pythia-410m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.26108374384236455,0.030903796952114475
EleutherAI/pythia-410m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-410m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-410m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23636363636363636,0.033175059300091805
EleutherAI/pythia-410m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23636363636363636,0.033175059300091805
EleutherAI/pythia-410m,hendrycksTest-high_school_geography,5-shot,accuracy,0.30303030303030304,0.032742879140268674
EleutherAI/pythia-410m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.30303030303030304,0.032742879140268674
EleutherAI/pythia-410m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.23316062176165803,0.030516111371476008
EleutherAI/pythia-410m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.23316062176165803,0.030516111371476008
EleutherAI/pythia-410m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.358974358974359,0.024321738484602364
EleutherAI/pythia-410m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.358974358974359,0.024321738484602364
EleutherAI/pythia-410m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.026842057873833706
EleutherAI/pythia-410m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.026842057873833706
EleutherAI/pythia-410m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23949579831932774,0.027722065493361276
EleutherAI/pythia-410m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23949579831932774,0.027722065493361276
EleutherAI/pythia-410m,hendrycksTest-high_school_physics,5-shot,accuracy,0.2913907284768212,0.03710185726119995
EleutherAI/pythia-410m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2913907284768212,0.03710185726119995
EleutherAI/pythia-410m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.326605504587156,0.020106990889937303
EleutherAI/pythia-410m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.326605504587156,0.020106990889937303
EleutherAI/pythia-410m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2696078431372549,0.031145570659486782
EleutherAI/pythia-410m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2696078431372549,0.031145570659486782
EleutherAI/pythia-410m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.22362869198312235,0.027123298205229972
EleutherAI/pythia-410m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.22362869198312235,0.027123298205229972
EleutherAI/pythia-410m,hendrycksTest-human_aging,5-shot,accuracy,0.25112107623318386,0.029105220833224605
EleutherAI/pythia-410m,hendrycksTest-human_aging,5-shot,acc_norm,0.25112107623318386,0.029105220833224605
EleutherAI/pythia-410m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.037276735755969195
EleutherAI/pythia-410m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.037276735755969195
EleutherAI/pythia-410m,hendrycksTest-international_law,5-shot,accuracy,0.4049586776859504,0.04481137755942469
EleutherAI/pythia-410m,hendrycksTest-international_law,5-shot,acc_norm,0.4049586776859504,0.04481137755942469
EleutherAI/pythia-410m,hendrycksTest-jurisprudence,5-shot,accuracy,0.21296296296296297,0.039578354719809805
EleutherAI/pythia-410m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.21296296296296297,0.039578354719809805
EleutherAI/pythia-410m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2822085889570552,0.03536117886664743
EleutherAI/pythia-410m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2822085889570552,0.03536117886664743
EleutherAI/pythia-410m,hendrycksTest-machine_learning,5-shot,accuracy,0.21428571428571427,0.03894641120044792
EleutherAI/pythia-410m,hendrycksTest-machine_learning,5-shot,acc_norm,0.21428571428571427,0.03894641120044792
EleutherAI/pythia-410m,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.04354631077260594
EleutherAI/pythia-410m,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.04354631077260594
EleutherAI/pythia-410m,hendrycksTest-marketing,5-shot,accuracy,0.20512820512820512,0.026453508054040335
EleutherAI/pythia-410m,hendrycksTest-marketing,5-shot,acc_norm,0.20512820512820512,0.026453508054040335
EleutherAI/pythia-410m,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-410m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-410m,hendrycksTest-miscellaneous,5-shot,accuracy,0.25798212005108556,0.01564583018834895
EleutherAI/pythia-410m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.25798212005108556,0.01564583018834895
EleutherAI/pythia-410m,hendrycksTest-moral_disputes,5-shot,accuracy,0.2543352601156069,0.02344582627654555
EleutherAI/pythia-410m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2543352601156069,0.02344582627654555
EleutherAI/pythia-410m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2446927374301676,0.01437816988409845
EleutherAI/pythia-410m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2446927374301676,0.01437816988409845
EleutherAI/pythia-410m,hendrycksTest-nutrition,5-shot,accuracy,0.2549019607843137,0.024954184324879905
EleutherAI/pythia-410m,hendrycksTest-nutrition,5-shot,acc_norm,0.2549019607843137,0.024954184324879905
EleutherAI/pythia-410m,hendrycksTest-philosophy,5-shot,accuracy,0.24437299035369775,0.024406162094668917
EleutherAI/pythia-410m,hendrycksTest-philosophy,5-shot,acc_norm,0.24437299035369775,0.024406162094668917
EleutherAI/pythia-410m,hendrycksTest-prehistory,5-shot,accuracy,0.22530864197530864,0.023246202647819753
EleutherAI/pythia-410m,hendrycksTest-prehistory,5-shot,acc_norm,0.22530864197530864,0.023246202647819753
EleutherAI/pythia-410m,hendrycksTest-professional_accounting,5-shot,accuracy,0.24468085106382978,0.025645553622266722
EleutherAI/pythia-410m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24468085106382978,0.025645553622266722
EleutherAI/pythia-410m,hendrycksTest-professional_law,5-shot,accuracy,0.24119947848761408,0.010926496102034965
EleutherAI/pythia-410m,hendrycksTest-professional_law,5-shot,acc_norm,0.24119947848761408,0.010926496102034965
EleutherAI/pythia-410m,hendrycksTest-professional_medicine,5-shot,accuracy,0.44485294117647056,0.03018753206032938
EleutherAI/pythia-410m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.44485294117647056,0.03018753206032938
EleutherAI/pythia-410m,hendrycksTest-professional_psychology,5-shot,accuracy,0.2777777777777778,0.018120224251484594
EleutherAI/pythia-410m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2777777777777778,0.018120224251484594
EleutherAI/pythia-410m,hendrycksTest-public_relations,5-shot,accuracy,0.20909090909090908,0.03895091015724137
EleutherAI/pythia-410m,hendrycksTest-public_relations,5-shot,acc_norm,0.20909090909090908,0.03895091015724137
EleutherAI/pythia-410m,hendrycksTest-security_studies,5-shot,accuracy,0.40408163265306124,0.031414708025865885
EleutherAI/pythia-410m,hendrycksTest-security_studies,5-shot,acc_norm,0.40408163265306124,0.031414708025865885
EleutherAI/pythia-410m,hendrycksTest-sociology,5-shot,accuracy,0.23383084577114427,0.029929415408348398
EleutherAI/pythia-410m,hendrycksTest-sociology,5-shot,acc_norm,0.23383084577114427,0.029929415408348398
EleutherAI/pythia-410m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-410m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.25,0.04351941398892446
EleutherAI/pythia-410m,hendrycksTest-virology,5-shot,accuracy,0.2710843373493976,0.03460579907553028
EleutherAI/pythia-410m,hendrycksTest-virology,5-shot,acc_norm,0.2710843373493976,0.03460579907553028
EleutherAI/pythia-410m,hendrycksTest-world_religions,5-shot,accuracy,0.27485380116959063,0.034240429246915824
EleutherAI/pythia-410m,hendrycksTest-world_religions,5-shot,acc_norm,0.27485380116959063,0.034240429246915824
EleutherAI/pythia-410m,truthfulqa:mc,0-shot,mc1,0.23745410036719705,0.01489627744104184
EleutherAI/pythia-410m,truthfulqa:mc,0-shot,mc2,0.4121958367286861,0.014564451157949564
EleutherAI/pythia-410m,winogrande,5-shot,accuracy,0.5311760063141279,0.014025142640639518
EleutherAI/pythia-410m,drop,3-shot,accuracy,0.0018875838926174498,0.00044451099905590827
EleutherAI/pythia-410m,drop,3-shot,f1,0.044600461409396115,0.0012188499729627125
EleutherAI/pythia-410m,gsm8k,5-shot,accuracy,0.006823351023502654,0.0022675371022545013
bigscience/bloom-7b1,minerva_math_precalc,5-shot,accuracy,0.003663003663003663,0.002587757368193454
bigscience/bloom-7b1,minerva_math_prealgebra,5-shot,accuracy,0.001148105625717566,0.0011481056257175725
bigscience/bloom-7b1,minerva_math_num_theory,5-shot,accuracy,0.003703703703703704,0.0026164834572311967
bigscience/bloom-7b1,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417496,0.001565259593407063
bigscience/bloom-7b1,minerva_math_geometry,5-shot,accuracy,0.0,
bigscience/bloom-7b1,minerva_math_counting_and_prob,5-shot,accuracy,0.004219409282700422,0.002980417365102053
bigscience/bloom-7b1,minerva_math_algebra,5-shot,accuracy,0.005897219882055603,0.0022232943288310806
bigscience/bloom-7b1,fld_default,0-shot,accuracy,0.0,
bigscience/bloom-7b1,fld_star,0-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_3da,5-shot,accuracy,0.002,0.0009992493430694982
bigscience/bloom-7b1,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005000000000000116
bigscience/bloom-7b1,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
bigscience/bloom-7b1,arithmetic_2ds,5-shot,accuracy,0.0135,0.0025811249685073444
bigscience/bloom-7b1,arithmetic_5ds,5-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_5da,5-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_1dc,5-shot,accuracy,0.024,0.0034231358327511544
bigscience/bloom-7b1,arithmetic_4ds,5-shot,accuracy,0.0,
bigscience/bloom-7b1,arithmetic_2dm,5-shot,accuracy,0.0315,0.003906597720891821
bigscience/bloom-7b1,arithmetic_2da,5-shot,accuracy,0.0185,0.0030138707185866863
bigscience/bloom-7b1,gsm8k_cot,5-shot,accuracy,0.021986353297952996,0.004039162758110064
bigscience/bloom-7b1,gsm8k,5-shot,accuracy,0.013646702047005308,0.0031957470754807806
bigscience/bloom-7b1,anli_r2,0-shot,brier_score,0.9456261005824846,
bigscience/bloom-7b1,anli_r3,0-shot,brier_score,0.9124531167930634,
bigscience/bloom-7b1,anli_r1,0-shot,brier_score,0.982579018108182,
bigscience/bloom-7b1,xnli_eu,0-shot,brier_score,0.7070982982123462,
bigscience/bloom-7b1,xnli_vi,0-shot,brier_score,0.7579336684566961,
bigscience/bloom-7b1,xnli_ru,0-shot,brier_score,0.7718940275749611,
bigscience/bloom-7b1,xnli_zh,0-shot,brier_score,1.1474241210913452,
bigscience/bloom-7b1,xnli_tr,0-shot,brier_score,0.8440682891485775,
bigscience/bloom-7b1,xnli_fr,0-shot,brier_score,0.7413355596902762,
bigscience/bloom-7b1,xnli_en,0-shot,brier_score,0.6356461345686089,
bigscience/bloom-7b1,xnli_ur,0-shot,brier_score,0.9251624347491019,
bigscience/bloom-7b1,xnli_ar,0-shot,brier_score,1.120036997482241,
bigscience/bloom-7b1,xnli_de,0-shot,brier_score,0.892294512080338,
bigscience/bloom-7b1,xnli_hi,0-shot,brier_score,0.7053726297546287,
bigscience/bloom-7b1,xnli_es,0-shot,brier_score,0.7988409860889734,
bigscience/bloom-7b1,xnli_bg,0-shot,brier_score,0.9255423577067828,
bigscience/bloom-7b1,xnli_sw,0-shot,brier_score,0.9813718635919855,
bigscience/bloom-7b1,xnli_el,0-shot,brier_score,0.9271262855069101,
bigscience/bloom-7b1,xnli_th,0-shot,brier_score,1.032461389997439,
bigscience/bloom-7b1,logiqa2,0-shot,brier_score,1.1331202068301027,
bigscience/bloom-7b1,mathqa,5-shot,brier_score,0.9772026799355674,
bigscience/bloom-7b1,lambada_standard,0-shot,perplexity,7.352795006135428,0.202079419708096
bigscience/bloom-7b1,lambada_standard,0-shot,accuracy,0.581797011449641,0.006872130244051415
bigscience/bloom-7b1,lambada_openai,0-shot,perplexity,6.619167022578283,0.17610608259142468
bigscience/bloom-7b1,lambada_openai,0-shot,accuracy,0.5761692218125364,0.006884673454916903
bigscience/bloom-7b1,mmlu_world_religions,0-shot,accuracy,0.29239766081871343,0.03488647713457922
bigscience/bloom-7b1,mmlu_formal_logic,0-shot,accuracy,0.16666666666666666,0.033333333333333354
bigscience/bloom-7b1,mmlu_prehistory,0-shot,accuracy,0.2623456790123457,0.024477222856135107
bigscience/bloom-7b1,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
bigscience/bloom-7b1,mmlu_high_school_world_history,0-shot,accuracy,0.26582278481012656,0.028756799629658342
bigscience/bloom-7b1,mmlu_moral_disputes,0-shot,accuracy,0.24277456647398843,0.023083658586984204
bigscience/bloom-7b1,mmlu_professional_law,0-shot,accuracy,0.2542372881355932,0.01112112900784068
bigscience/bloom-7b1,mmlu_logical_fallacies,0-shot,accuracy,0.26380368098159507,0.034624199316156234
bigscience/bloom-7b1,mmlu_high_school_us_history,0-shot,accuracy,0.27450980392156865,0.03132179803083292
bigscience/bloom-7b1,mmlu_philosophy,0-shot,accuracy,0.2733118971061093,0.02531176597542612
bigscience/bloom-7b1,mmlu_jurisprudence,0-shot,accuracy,0.28703703703703703,0.043733130409147614
bigscience/bloom-7b1,mmlu_international_law,0-shot,accuracy,0.35537190082644626,0.04369236326573981
bigscience/bloom-7b1,mmlu_high_school_european_history,0-shot,accuracy,0.23636363636363636,0.03317505930009179
bigscience/bloom-7b1,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20725388601036268,0.029252823291803627
bigscience/bloom-7b1,mmlu_high_school_microeconomics,0-shot,accuracy,0.226890756302521,0.027205371538279472
bigscience/bloom-7b1,mmlu_high_school_geography,0-shot,accuracy,0.23232323232323232,0.030088629490217487
bigscience/bloom-7b1,mmlu_high_school_psychology,0-shot,accuracy,0.24403669724770644,0.018415286351416413
bigscience/bloom-7b1,mmlu_public_relations,0-shot,accuracy,0.32727272727272727,0.04494290866252091
bigscience/bloom-7b1,mmlu_us_foreign_policy,0-shot,accuracy,0.21,0.04093601807403326
bigscience/bloom-7b1,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409217
bigscience/bloom-7b1,mmlu_high_school_macroeconomics,0-shot,accuracy,0.23076923076923078,0.021362027725222735
bigscience/bloom-7b1,mmlu_security_studies,0-shot,accuracy,0.3142857142857143,0.02971932942241748
bigscience/bloom-7b1,mmlu_professional_psychology,0-shot,accuracy,0.26143790849673204,0.01777694715752804
bigscience/bloom-7b1,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596918
bigscience/bloom-7b1,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748142
bigscience/bloom-7b1,mmlu_miscellaneous,0-shot,accuracy,0.28991060025542786,0.01622501794477096
bigscience/bloom-7b1,mmlu_marketing,0-shot,accuracy,0.24786324786324787,0.02828632407556438
bigscience/bloom-7b1,mmlu_management,0-shot,accuracy,0.2524271844660194,0.04301250399690878
bigscience/bloom-7b1,mmlu_nutrition,0-shot,accuracy,0.23529411764705882,0.024288619466046112
bigscience/bloom-7b1,mmlu_medical_genetics,0-shot,accuracy,0.26,0.04408440022768079
bigscience/bloom-7b1,mmlu_human_aging,0-shot,accuracy,0.3632286995515695,0.032277904428505
bigscience/bloom-7b1,mmlu_professional_medicine,0-shot,accuracy,0.20588235294117646,0.024562204314142317
bigscience/bloom-7b1,mmlu_college_medicine,0-shot,accuracy,0.20809248554913296,0.030952890217749874
bigscience/bloom-7b1,mmlu_business_ethics,0-shot,accuracy,0.26,0.0440844002276808
bigscience/bloom-7b1,mmlu_clinical_knowledge,0-shot,accuracy,0.27547169811320754,0.027495663683724064
bigscience/bloom-7b1,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621505
bigscience/bloom-7b1,mmlu_virology,0-shot,accuracy,0.3072289156626506,0.035915667978246635
bigscience/bloom-7b1,mmlu_professional_accounting,0-shot,accuracy,0.26595744680851063,0.026358065698880596
bigscience/bloom-7b1,mmlu_college_physics,0-shot,accuracy,0.20588235294117646,0.04023382273617746
bigscience/bloom-7b1,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649006,0.03479185572599661
bigscience/bloom-7b1,mmlu_high_school_biology,0-shot,accuracy,0.25161290322580643,0.02468597928623997
bigscience/bloom-7b1,mmlu_college_biology,0-shot,accuracy,0.19444444444444445,0.03309615177059006
bigscience/bloom-7b1,mmlu_anatomy,0-shot,accuracy,0.24444444444444444,0.037125378336148665
bigscience/bloom-7b1,mmlu_college_chemistry,0-shot,accuracy,0.21,0.040936018074033256
bigscience/bloom-7b1,mmlu_computer_security,0-shot,accuracy,0.23,0.04229525846816506
bigscience/bloom-7b1,mmlu_college_computer_science,0-shot,accuracy,0.33,0.04725815626252604
bigscience/bloom-7b1,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123387
bigscience/bloom-7b1,mmlu_college_mathematics,0-shot,accuracy,0.33,0.04725815626252606
bigscience/bloom-7b1,mmlu_conceptual_physics,0-shot,accuracy,0.32340425531914896,0.03057944277361035
bigscience/bloom-7b1,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768079
bigscience/bloom-7b1,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.04725815626252605
bigscience/bloom-7b1,mmlu_machine_learning,0-shot,accuracy,0.30357142857142855,0.04364226155841044
bigscience/bloom-7b1,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.031270907132976984
bigscience/bloom-7b1,mmlu_high_school_statistics,0-shot,accuracy,0.38425925925925924,0.03317354514310742
bigscience/bloom-7b1,mmlu_elementary_mathematics,0-shot,accuracy,0.2777777777777778,0.02306818884826112
bigscience/bloom-7b1,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.036001056927277716
bigscience/bloom-7b1,mmlu_high_school_mathematics,0-shot,accuracy,0.25555555555555554,0.026593939101844072
bigscience/bloom-7b1,arc_challenge,25-shot,accuracy,0.36945392491467577,0.014104578366491894
bigscience/bloom-7b1,arc_challenge,25-shot,acc_norm,0.3984641638225256,0.014306946052735563
bigscience/bloom-7b1,hellaswag,10-shot,accuracy,0.4622585142401912,0.0049755460189506795
bigscience/bloom-7b1,hellaswag,10-shot,acc_norm,0.6199960167297351,0.004843954338451442
bigscience/bloom-7b1,truthfulqa_mc2,0-shot,accuracy,0.3889181333038942,0.014016257357529404
bigscience/bloom-7b1,truthfulqa_gen,0-shot,bleu_max,6.2009331295626025,0.40016089959474577
bigscience/bloom-7b1,truthfulqa_gen,0-shot,bleu_acc,0.1481028151774786,0.012434552750319277
bigscience/bloom-7b1,truthfulqa_gen,0-shot,bleu_diff,-2.008955984058176,0.36533772424088057
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge1_max,15.973477335995934,0.7756937195072751
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge1_acc,0.1481028151774786,0.012434552750319294
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge1_diff,-3.5047573052504597,0.4764981051843214
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge2_max,9.298241101337537,0.6208670674741479
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge2_acc,0.08935128518971848,0.009985751676755862
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rouge2_diff,-3.8860340582661594,0.5670349584056208
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rougeL_max,15.125543867609249,0.7432106357285632
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rougeL_acc,0.14320685434516525,0.012262381258730764
bigscience/bloom-7b1,truthfulqa_gen,0-shot,rougeL_diff,-3.509115227685031,0.48390180734803606
bigscience/bloom-7b1,truthfulqa_mc1,0-shot,accuracy,0.22399020807833536,0.014594964329474205
bigscience/bloom-7b1,winogrande,5-shot,accuracy,0.654301499605367,0.013366596951934375
llama2_220M_nl_0_code_100,minerva_math_algebra,5-shot,accuracy,0.005897219882055603,0.0022232943288310854
llama2_220M_nl_0_code_100,minerva_math_counting_and_prob,5-shot,accuracy,0.004219409282700422,0.0029804173651020523
llama2_220M_nl_0_code_100,minerva_math_geometry,5-shot,accuracy,0.0041753653444676405,0.002949339217075659
llama2_220M_nl_0_code_100,minerva_math_intermediate_algebra,5-shot,accuracy,0.004429678848283499,0.002211153142378789
llama2_220M_nl_0_code_100,minerva_math_num_theory,5-shot,accuracy,0.003703703703703704,0.0026164834572311862
llama2_220M_nl_0_code_100,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458095,0.0034284443646836584
llama2_220M_nl_0_code_100,minerva_math_precalc,5-shot,accuracy,0.003663003663003663,0.0025877573681934436
llama2_220M_nl_0_code_100,gsm8k,5-shot,accuracy,0.019711902956785442,0.0038289829787357004
llama2_220M_nl_0_code_100,gsm8k_cot,5-shot,accuracy,0.02047005307050796,0.003900413385915721
llama2_220M_nl_0_code_100,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_2ds,5-shot,accuracy,0.0245,0.003457723662536226
llama2_220M_nl_0_code_100,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339296
llama2_220M_nl_0_code_100,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_2da,5-shot,accuracy,0.007,0.001864735536023767
llama2_220M_nl_0_code_100,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_0_code_100,arithmetic_2dm,5-shot,accuracy,0.0265,0.0035923985947876767
llama2_220M_nl_0_code_100,arithmetic_1dc,5-shot,accuracy,0.038,0.004276346989170301
llama2_220M_nl_0_code_100,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646802
llama2_220M_nl_0_code_100,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000152
llama2_220M_nl_0_code_100,xnli_ar,0-shot,brier_score,0.8176540473385855,
llama2_220M_nl_0_code_100,xnli_bg,0-shot,brier_score,0.8604650631115776,
llama2_220M_nl_0_code_100,xnli_de,0-shot,brier_score,0.9403918628464668,
llama2_220M_nl_0_code_100,xnli_el,0-shot,brier_score,1.0431337434314025,
llama2_220M_nl_0_code_100,xnli_en,0-shot,brier_score,0.8164390517281058,
llama2_220M_nl_0_code_100,xnli_es,0-shot,brier_score,1.2047490428504812,
llama2_220M_nl_0_code_100,xnli_fr,0-shot,brier_score,1.020514118729371,
llama2_220M_nl_0_code_100,xnli_hi,0-shot,brier_score,1.1271173074874523,
llama2_220M_nl_0_code_100,xnli_ru,0-shot,brier_score,0.8951801287656805,
llama2_220M_nl_0_code_100,xnli_sw,0-shot,brier_score,0.9851942650553409,
llama2_220M_nl_0_code_100,xnli_th,0-shot,brier_score,0.9235036150582316,
llama2_220M_nl_0_code_100,xnli_tr,0-shot,brier_score,0.9844423823563053,
llama2_220M_nl_0_code_100,xnli_ur,0-shot,brier_score,1.2881916983830637,
llama2_220M_nl_0_code_100,xnli_vi,0-shot,brier_score,0.8189554060315569,
llama2_220M_nl_0_code_100,xnli_zh,0-shot,brier_score,0.9499893581866519,
llama2_220M_nl_0_code_100,anli_r1,0-shot,brier_score,0.8742679757413921,
llama2_220M_nl_0_code_100,anli_r3,0-shot,brier_score,0.8729446932818329,
llama2_220M_nl_0_code_100,anli_r2,0-shot,brier_score,0.8796441516405341,
llama2_220M_nl_0_code_100,mathqa,5-shot,brier_score,1.0060822647582421,
llama2_220M_nl_0_code_100,logiqa2,0-shot,brier_score,1.164538542911119,
llama2_220M_nl_0_code_100,lambada_standard,0-shot,perplexity,591.6803510463476,28.025331080505545
llama2_220M_nl_0_code_100,lambada_standard,0-shot,accuracy,0.14923345623908402,0.0049642126051581015
llama2_220M_nl_0_code_100,lambada_openai,0-shot,perplexity,445.5204646626984,21.81250787402026
llama2_220M_nl_0_code_100,lambada_openai,0-shot,accuracy,0.17543178730836406,0.005298824200667609
google/gemma-2-9b,mmlu_world_religions,0-shot,accuracy,0.8654970760233918,0.026168221344662294
google/gemma-2-9b,mmlu_formal_logic,0-shot,accuracy,0.49206349206349204,0.044715725362943486
google/gemma-2-9b,mmlu_prehistory,0-shot,accuracy,0.808641975308642,0.021887704613396168
google/gemma-2-9b,mmlu_moral_scenarios,0-shot,accuracy,0.35083798882681566,0.015961036675230973
google/gemma-2-9b,mmlu_high_school_world_history,0-shot,accuracy,0.8649789029535865,0.022245776632003694
google/gemma-2-9b,mmlu_moral_disputes,0-shot,accuracy,0.7832369942196532,0.02218347766841285
google/gemma-2-9b,mmlu_professional_law,0-shot,accuracy,0.5449804432855281,0.012718456618701785
google/gemma-2-9b,mmlu_logical_fallacies,0-shot,accuracy,0.8466257668711656,0.0283116014414386
google/gemma-2-9b,mmlu_high_school_us_history,0-shot,accuracy,0.8970588235294118,0.021328337570804365
google/gemma-2-9b,mmlu_philosophy,0-shot,accuracy,0.7717041800643086,0.023839303311398212
google/gemma-2-9b,mmlu_jurisprudence,0-shot,accuracy,0.8240740740740741,0.036809181416738786
google/gemma-2-9b,mmlu_international_law,0-shot,accuracy,0.8429752066115702,0.03321244842547128
google/gemma-2-9b,mmlu_high_school_european_history,0-shot,accuracy,0.793939393939394,0.031584153240477086
google/gemma-2-9b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9533678756476683,0.015216761819262585
google/gemma-2-9b,mmlu_high_school_microeconomics,0-shot,accuracy,0.8361344537815126,0.02404405494044048
google/gemma-2-9b,mmlu_high_school_geography,0-shot,accuracy,0.8888888888888888,0.02239078763821678
google/gemma-2-9b,mmlu_high_school_psychology,0-shot,accuracy,0.908256880733945,0.012376323409137122
google/gemma-2-9b,mmlu_public_relations,0-shot,accuracy,0.7181818181818181,0.043091187099464585
google/gemma-2-9b,mmlu_us_foreign_policy,0-shot,accuracy,0.91,0.028762349126466108
google/gemma-2-9b,mmlu_sociology,0-shot,accuracy,0.8706467661691543,0.023729830881018515
google/gemma-2-9b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.782051282051282,0.02093244577446318
google/gemma-2-9b,mmlu_security_studies,0-shot,accuracy,0.7551020408163265,0.027529637440174927
google/gemma-2-9b,mmlu_professional_psychology,0-shot,accuracy,0.7908496732026143,0.016453399332279323
google/gemma-2-9b,mmlu_human_sexuality,0-shot,accuracy,0.816793893129771,0.033927709264947335
google/gemma-2-9b,mmlu_econometrics,0-shot,accuracy,0.5789473684210527,0.04644602091222318
google/gemma-2-9b,mmlu_miscellaneous,0-shot,accuracy,0.8454661558109834,0.012925773495095966
google/gemma-2-9b,mmlu_marketing,0-shot,accuracy,0.9230769230769231,0.017456987872436183
google/gemma-2-9b,mmlu_management,0-shot,accuracy,0.8640776699029126,0.0339329572976101
google/gemma-2-9b,mmlu_nutrition,0-shot,accuracy,0.761437908496732,0.02440439492808787
google/gemma-2-9b,mmlu_medical_genetics,0-shot,accuracy,0.84,0.03684529491774709
google/gemma-2-9b,mmlu_human_aging,0-shot,accuracy,0.7982062780269058,0.026936111912802263
google/gemma-2-9b,mmlu_professional_medicine,0-shot,accuracy,0.7867647058823529,0.024880971512294275
google/gemma-2-9b,mmlu_college_medicine,0-shot,accuracy,0.7167630057803468,0.03435568056047873
google/gemma-2-9b,mmlu_business_ethics,0-shot,accuracy,0.74,0.0440844002276808
google/gemma-2-9b,mmlu_clinical_knowledge,0-shot,accuracy,0.7660377358490567,0.02605529690115292
google/gemma-2-9b,mmlu_global_facts,0-shot,accuracy,0.5,0.050251890762960605
google/gemma-2-9b,mmlu_virology,0-shot,accuracy,0.536144578313253,0.038823108508905954
google/gemma-2-9b,mmlu_professional_accounting,0-shot,accuracy,0.5177304964539007,0.02980873964223777
google/gemma-2-9b,mmlu_college_physics,0-shot,accuracy,0.5294117647058824,0.04966570903978529
google/gemma-2-9b,mmlu_high_school_physics,0-shot,accuracy,0.4966887417218543,0.04082393379449654
google/gemma-2-9b,mmlu_high_school_biology,0-shot,accuracy,0.8870967741935484,0.018003603325863645
google/gemma-2-9b,mmlu_college_biology,0-shot,accuracy,0.8888888888888888,0.026280550932848094
google/gemma-2-9b,mmlu_anatomy,0-shot,accuracy,0.7407407407407407,0.037857144650666544
google/gemma-2-9b,mmlu_college_chemistry,0-shot,accuracy,0.5,0.050251890762960605
google/gemma-2-9b,mmlu_computer_security,0-shot,accuracy,0.78,0.04163331998932263
google/gemma-2-9b,mmlu_college_computer_science,0-shot,accuracy,0.52,0.050211673156867795
google/gemma-2-9b,mmlu_astronomy,0-shot,accuracy,0.8092105263157895,0.031975658210325
google/gemma-2-9b,mmlu_college_mathematics,0-shot,accuracy,0.44,0.049888765156985884
google/gemma-2-9b,mmlu_conceptual_physics,0-shot,accuracy,0.7531914893617021,0.028185441301234102
google/gemma-2-9b,mmlu_abstract_algebra,0-shot,accuracy,0.41,0.049431107042371025
google/gemma-2-9b,mmlu_high_school_computer_science,0-shot,accuracy,0.74,0.04408440022768078
google/gemma-2-9b,mmlu_machine_learning,0-shot,accuracy,0.5267857142857143,0.047389751192741546
google/gemma-2-9b,mmlu_high_school_chemistry,0-shot,accuracy,0.6748768472906403,0.032957975663112704
google/gemma-2-9b,mmlu_high_school_statistics,0-shot,accuracy,0.6481481481481481,0.032568505702936464
google/gemma-2-9b,mmlu_elementary_mathematics,0-shot,accuracy,0.5634920634920635,0.025542846817400502
google/gemma-2-9b,mmlu_electrical_engineering,0-shot,accuracy,0.7103448275862069,0.03780019230438014
google/gemma-2-9b,mmlu_high_school_mathematics,0-shot,accuracy,0.45555555555555555,0.03036486250482443
google/gemma-2-9b,arc_challenge,25-shot,accuracy,0.643344709897611,0.013998056902620197
google/gemma-2-9b,arc_challenge,25-shot,acc_norm,0.681740614334471,0.013611993916971455
google/gemma-2-9b,hellaswag,10-shot,accuracy,0.635929097789285,0.004801852881329775
google/gemma-2-9b,hellaswag,10-shot,acc_norm,0.8253335988846843,0.0037890554870031357
google/gemma-2-9b,truthfulqa_mc2,0-shot,accuracy,0.4553219595410078,0.014210628836236725
google/gemma-2-9b,truthfulqa_gen,0-shot,bleu_max,33.48367702247776,0.8625505675030039
google/gemma-2-9b,truthfulqa_gen,0-shot,bleu_acc,0.43084455324357407,0.017335272475332366
google/gemma-2-9b,truthfulqa_gen,0-shot,bleu_diff,-0.19870857289763738,1.0675737646404135
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge1_max,59.115576049843746,0.9001849627161471
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge1_acc,0.41982864137086906,0.01727703030177577
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge1_diff,0.3244784775406543,1.280521042715171
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge2_max,44.49834625015418,1.106551965177841
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge2_acc,0.37454100367197063,0.01694353512840533
google/gemma-2-9b,truthfulqa_gen,0-shot,rouge2_diff,-1.3262594872227933,1.471263187794327
google/gemma-2-9b,truthfulqa_gen,0-shot,rougeL_max,56.491514242375466,0.9276044446810214
google/gemma-2-9b,truthfulqa_gen,0-shot,rougeL_acc,0.40514075887392903,0.017185611727753375
google/gemma-2-9b,truthfulqa_gen,0-shot,rougeL_diff,-0.06522153220084319,1.2948177926334004
google/gemma-2-9b,truthfulqa_mc1,0-shot,accuracy,0.3011015911872705,0.016058999026100612
google/gemma-2-9b,winogrande,5-shot,accuracy,0.7876874506708761,0.011493384687249787
google/gemma-2-9b,gsm8k,5-shot,accuracy,0.6747536012130402,0.012903904752543926
llama2_220M_nl_20_code_80,minerva_math_algebra,5-shot,accuracy,0.005054759898904802,0.002059242583556782
llama2_220M_nl_20_code_80,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,minerva_math_geometry,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,minerva_math_intermediate_algebra,5-shot,accuracy,0.005537098560354375,0.0024707690436948588
llama2_220M_nl_20_code_80,minerva_math_num_theory,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,minerva_math_prealgebra,5-shot,accuracy,0.002296211251435132,0.0016227331369346296
llama2_220M_nl_20_code_80,minerva_math_precalc,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,gsm8k,5-shot,accuracy,0.01819560272934041,0.0036816118940738705
llama2_220M_nl_20_code_80,gsm8k_cot,5-shot,accuracy,0.019711902956785442,0.0038289829787357074
llama2_220M_nl_20_code_80,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_2ds,5-shot,accuracy,0.0245,0.003457723662536226
llama2_220M_nl_20_code_80,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339296
llama2_220M_nl_20_code_80,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_2da,5-shot,accuracy,0.007,0.001864735536023767
llama2_220M_nl_20_code_80,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_20_code_80,arithmetic_2dm,5-shot,accuracy,0.02,0.0031312780858980672
llama2_220M_nl_20_code_80,arithmetic_1dc,5-shot,accuracy,0.0055,0.0016541593398342208
llama2_220M_nl_20_code_80,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646802
llama2_220M_nl_20_code_80,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000152
llama2_220M_nl_20_code_80,xnli_ar,0-shot,brier_score,0.8816610669948778,
llama2_220M_nl_20_code_80,xnli_bg,0-shot,brier_score,1.0362565504301993,
llama2_220M_nl_20_code_80,xnli_de,0-shot,brier_score,0.9018997474235173,
llama2_220M_nl_20_code_80,xnli_el,0-shot,brier_score,1.0243195087973478,
llama2_220M_nl_20_code_80,xnli_en,0-shot,brier_score,0.751985940573613,
llama2_220M_nl_20_code_80,xnli_es,0-shot,brier_score,1.0627080486914608,
llama2_220M_nl_20_code_80,xnli_fr,0-shot,brier_score,0.9163039891194814,
llama2_220M_nl_20_code_80,xnli_hi,0-shot,brier_score,0.8668011373154416,
llama2_220M_nl_20_code_80,xnli_ru,0-shot,brier_score,0.9025903954606055,
llama2_220M_nl_20_code_80,xnli_sw,0-shot,brier_score,0.872467551032371,
llama2_220M_nl_20_code_80,xnli_th,0-shot,brier_score,1.2900459205842092,
llama2_220M_nl_20_code_80,xnli_tr,0-shot,brier_score,0.8282263672648711,
llama2_220M_nl_20_code_80,xnli_ur,0-shot,brier_score,1.3235136063845856,
llama2_220M_nl_20_code_80,xnli_vi,0-shot,brier_score,0.9966699117681713,
llama2_220M_nl_20_code_80,xnli_zh,0-shot,brier_score,1.040442178383034,
llama2_220M_nl_20_code_80,anli_r1,0-shot,brier_score,1.0391472408837694,
llama2_220M_nl_20_code_80,anli_r3,0-shot,brier_score,0.9958872583201617,
llama2_220M_nl_20_code_80,anli_r2,0-shot,brier_score,1.0353837496172358,
llama2_220M_nl_20_code_80,mathqa,5-shot,brier_score,1.0223925616716352,
llama2_220M_nl_20_code_80,logiqa2,0-shot,brier_score,1.1990124792426446,
llama2_220M_nl_20_code_80,lambada_standard,0-shot,perplexity,156.01458439376142,6.551228189535927
llama2_220M_nl_20_code_80,lambada_standard,0-shot,accuracy,0.21502037647972055,0.005723755495790099
llama2_220M_nl_20_code_80,lambada_openai,0-shot,perplexity,65.5641350743855,2.61090295400462
llama2_220M_nl_20_code_80,lambada_openai,0-shot,accuracy,0.28721133320395886,0.006303666845525234
AbacusResearch/haLLAwa3,minerva_math_precalc,5-shot,accuracy,0.0695970695970696,0.010900157182653379
AbacusResearch/haLLAwa3,minerva_math_prealgebra,5-shot,accuracy,0.38920780711825487,0.016530191465345578
AbacusResearch/haLLAwa3,minerva_math_num_theory,5-shot,accuracy,0.11296296296296296,0.013634666880074291
AbacusResearch/haLLAwa3,minerva_math_intermediate_algebra,5-shot,accuracy,0.08305647840531562,0.009188714988985457
AbacusResearch/haLLAwa3,minerva_math_geometry,5-shot,accuracy,0.162839248434238,0.016887681356465453
AbacusResearch/haLLAwa3,minerva_math_counting_and_prob,5-shot,accuracy,0.17721518987341772,0.017557514449364223
AbacusResearch/haLLAwa3,minerva_math_algebra,5-shot,accuracy,0.2923336141533277,0.013207217104051101
AbacusResearch/haLLAwa3,fld_default,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa3,fld_star,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa3,arithmetic_3da,5-shot,accuracy,0.983,0.002891311093590562
AbacusResearch/haLLAwa3,arithmetic_3ds,5-shot,accuracy,0.978,0.0032807593162018905
AbacusResearch/haLLAwa3,arithmetic_4da,5-shot,accuracy,0.932,0.0056306173663253235
AbacusResearch/haLLAwa3,arithmetic_2ds,5-shot,accuracy,0.993,0.0018647355360237657
AbacusResearch/haLLAwa3,arithmetic_5ds,5-shot,accuracy,0.883,0.0071889735477559495
AbacusResearch/haLLAwa3,arithmetic_5da,5-shot,accuracy,0.906,0.006527120471603569
AbacusResearch/haLLAwa3,arithmetic_1dc,5-shot,accuracy,0.7045,0.010204996128024123
AbacusResearch/haLLAwa3,arithmetic_4ds,5-shot,accuracy,0.934,0.00555314493862308
AbacusResearch/haLLAwa3,arithmetic_2dm,5-shot,accuracy,0.639,0.010742308811391417
AbacusResearch/haLLAwa3,arithmetic_2da,5-shot,accuracy,0.9965,0.0013208888574315811
AbacusResearch/haLLAwa3,gsm8k_cot,5-shot,accuracy,0.7134192570128886,0.01245484166833771
AbacusResearch/haLLAwa3,gsm8k,5-shot,accuracy,0.6823351023502654,0.012824066621488845
AbacusResearch/haLLAwa3,anli_r2,0-shot,brier_score,0.7822788074399861,
AbacusResearch/haLLAwa3,anli_r3,0-shot,brier_score,0.8264083512806276,
AbacusResearch/haLLAwa3,anli_r1,0-shot,brier_score,0.7075167500716043,
AbacusResearch/haLLAwa3,xnli_eu,0-shot,brier_score,1.1457721381774524,
AbacusResearch/haLLAwa3,xnli_vi,0-shot,brier_score,0.9869158803328254,
AbacusResearch/haLLAwa3,xnli_ru,0-shot,brier_score,0.8716743740754499,
AbacusResearch/haLLAwa3,xnli_zh,0-shot,brier_score,1.090951879740664,
AbacusResearch/haLLAwa3,xnli_tr,0-shot,brier_score,1.0701876481592625,
AbacusResearch/haLLAwa3,xnli_fr,0-shot,brier_score,0.8327048649607862,
AbacusResearch/haLLAwa3,xnli_en,0-shot,brier_score,0.712823188151288,
AbacusResearch/haLLAwa3,xnli_ur,0-shot,brier_score,1.239951455555465,
AbacusResearch/haLLAwa3,xnli_ar,0-shot,brier_score,1.2690967154108113,
AbacusResearch/haLLAwa3,xnli_de,0-shot,brier_score,0.9143433937106298,
AbacusResearch/haLLAwa3,xnli_hi,0-shot,brier_score,0.9583571701761374,
AbacusResearch/haLLAwa3,xnli_es,0-shot,brier_score,0.9546231307238467,
AbacusResearch/haLLAwa3,xnli_bg,0-shot,brier_score,0.9223977711612795,
AbacusResearch/haLLAwa3,xnli_sw,0-shot,brier_score,1.0030011650642259,
AbacusResearch/haLLAwa3,xnli_el,0-shot,brier_score,0.9462798720866608,
AbacusResearch/haLLAwa3,xnli_th,0-shot,brier_score,1.0379297621590724,
AbacusResearch/haLLAwa3,logiqa2,0-shot,brier_score,0.9965660380454943,
AbacusResearch/haLLAwa3,mathqa,5-shot,brier_score,0.9484637194920996,
AbacusResearch/haLLAwa3,lambada_standard,0-shot,perplexity,3.8098181547092502,0.10283604883136323
AbacusResearch/haLLAwa3,lambada_standard,0-shot,accuracy,0.6586454492528624,0.0066060334598652065
AbacusResearch/haLLAwa3,lambada_openai,0-shot,perplexity,3.1571427801436447,0.07480572143016524
AbacusResearch/haLLAwa3,lambada_openai,0-shot,accuracy,0.7143411604890355,0.0062934493900561085
AbacusResearch/haLLAwa3,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.028380919596145866
AbacusResearch/haLLAwa3,mmlu_formal_logic,0-shot,accuracy,0.47619047619047616,0.04467062628403273
AbacusResearch/haLLAwa3,mmlu_prehistory,0-shot,accuracy,0.7222222222222222,0.024922001168886324
AbacusResearch/haLLAwa3,mmlu_moral_scenarios,0-shot,accuracy,0.4692737430167598,0.01669089616194438
AbacusResearch/haLLAwa3,mmlu_high_school_world_history,0-shot,accuracy,0.8143459915611815,0.025310495376944856
AbacusResearch/haLLAwa3,mmlu_moral_disputes,0-shot,accuracy,0.7398843930635838,0.023618678310069367
AbacusResearch/haLLAwa3,mmlu_professional_law,0-shot,accuracy,0.4667535853976532,0.01274197433389723
AbacusResearch/haLLAwa3,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
AbacusResearch/haLLAwa3,mmlu_high_school_us_history,0-shot,accuracy,0.8333333333333334,0.026156867523931038
AbacusResearch/haLLAwa3,mmlu_philosophy,0-shot,accuracy,0.7106109324758842,0.025755865922632935
AbacusResearch/haLLAwa3,mmlu_jurisprudence,0-shot,accuracy,0.8425925925925926,0.03520703990517965
AbacusResearch/haLLAwa3,mmlu_international_law,0-shot,accuracy,0.768595041322314,0.03849856098794088
AbacusResearch/haLLAwa3,mmlu_high_school_european_history,0-shot,accuracy,0.7696969696969697,0.03287666758603489
AbacusResearch/haLLAwa3,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8808290155440415,0.023381935348121455
AbacusResearch/haLLAwa3,mmlu_high_school_microeconomics,0-shot,accuracy,0.6596638655462185,0.030778057422931673
AbacusResearch/haLLAwa3,mmlu_high_school_geography,0-shot,accuracy,0.7929292929292929,0.02886977846026704
AbacusResearch/haLLAwa3,mmlu_high_school_psychology,0-shot,accuracy,0.8385321100917431,0.015776239256163244
AbacusResearch/haLLAwa3,mmlu_public_relations,0-shot,accuracy,0.6272727272727273,0.04631381319425465
AbacusResearch/haLLAwa3,mmlu_us_foreign_policy,0-shot,accuracy,0.85,0.0358870281282637
AbacusResearch/haLLAwa3,mmlu_sociology,0-shot,accuracy,0.8507462686567164,0.025196929874827072
AbacusResearch/haLLAwa3,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6717948717948717,0.023807633198657262
AbacusResearch/haLLAwa3,mmlu_security_studies,0-shot,accuracy,0.726530612244898,0.028535560337128448
AbacusResearch/haLLAwa3,mmlu_professional_psychology,0-shot,accuracy,0.6486928104575164,0.01931267606578655
AbacusResearch/haLLAwa3,mmlu_human_sexuality,0-shot,accuracy,0.7862595419847328,0.0359546161177469
AbacusResearch/haLLAwa3,mmlu_econometrics,0-shot,accuracy,0.45614035087719296,0.046854730419077895
AbacusResearch/haLLAwa3,mmlu_miscellaneous,0-shot,accuracy,0.8263090676883781,0.013547415658662269
AbacusResearch/haLLAwa3,mmlu_marketing,0-shot,accuracy,0.8803418803418803,0.02126271940040698
AbacusResearch/haLLAwa3,mmlu_management,0-shot,accuracy,0.8155339805825242,0.03840423627288276
AbacusResearch/haLLAwa3,mmlu_nutrition,0-shot,accuracy,0.7483660130718954,0.024848018263875195
AbacusResearch/haLLAwa3,mmlu_medical_genetics,0-shot,accuracy,0.68,0.046882617226215034
AbacusResearch/haLLAwa3,mmlu_human_aging,0-shot,accuracy,0.6905829596412556,0.03102441174057221
AbacusResearch/haLLAwa3,mmlu_professional_medicine,0-shot,accuracy,0.6801470588235294,0.02833295951403121
AbacusResearch/haLLAwa3,mmlu_college_medicine,0-shot,accuracy,0.6358381502890174,0.03669072477416905
AbacusResearch/haLLAwa3,mmlu_business_ethics,0-shot,accuracy,0.6,0.049236596391733084
AbacusResearch/haLLAwa3,mmlu_clinical_knowledge,0-shot,accuracy,0.7018867924528301,0.02815283794249386
AbacusResearch/haLLAwa3,mmlu_global_facts,0-shot,accuracy,0.35,0.04793724854411019
AbacusResearch/haLLAwa3,mmlu_virology,0-shot,accuracy,0.536144578313253,0.038823108508905954
AbacusResearch/haLLAwa3,mmlu_professional_accounting,0-shot,accuracy,0.46808510638297873,0.029766675075873866
AbacusResearch/haLLAwa3,mmlu_college_physics,0-shot,accuracy,0.38235294117647056,0.04835503696107224
AbacusResearch/haLLAwa3,mmlu_high_school_physics,0-shot,accuracy,0.31788079470198677,0.038020397601079024
AbacusResearch/haLLAwa3,mmlu_high_school_biology,0-shot,accuracy,0.7612903225806451,0.02425107126220884
AbacusResearch/haLLAwa3,mmlu_college_biology,0-shot,accuracy,0.7361111111111112,0.03685651095897532
AbacusResearch/haLLAwa3,mmlu_anatomy,0-shot,accuracy,0.6148148148148148,0.04203921040156279
AbacusResearch/haLLAwa3,mmlu_college_chemistry,0-shot,accuracy,0.46,0.05009082659620333
AbacusResearch/haLLAwa3,mmlu_computer_security,0-shot,accuracy,0.79,0.04093601807403326
AbacusResearch/haLLAwa3,mmlu_college_computer_science,0-shot,accuracy,0.55,0.05
AbacusResearch/haLLAwa3,mmlu_astronomy,0-shot,accuracy,0.6973684210526315,0.037385206761196665
AbacusResearch/haLLAwa3,mmlu_college_mathematics,0-shot,accuracy,0.29,0.04560480215720684
AbacusResearch/haLLAwa3,mmlu_conceptual_physics,0-shot,accuracy,0.574468085106383,0.03232146916224468
AbacusResearch/haLLAwa3,mmlu_abstract_algebra,0-shot,accuracy,0.34,0.04760952285695235
AbacusResearch/haLLAwa3,mmlu_high_school_computer_science,0-shot,accuracy,0.68,0.04688261722621505
AbacusResearch/haLLAwa3,mmlu_machine_learning,0-shot,accuracy,0.45535714285714285,0.04726835553719099
AbacusResearch/haLLAwa3,mmlu_high_school_chemistry,0-shot,accuracy,0.5024630541871922,0.03517945038691063
AbacusResearch/haLLAwa3,mmlu_high_school_statistics,0-shot,accuracy,0.5046296296296297,0.03409825519163572
AbacusResearch/haLLAwa3,mmlu_elementary_mathematics,0-shot,accuracy,0.41005291005291006,0.02533120243894443
AbacusResearch/haLLAwa3,mmlu_electrical_engineering,0-shot,accuracy,0.5655172413793104,0.041307408795554966
AbacusResearch/haLLAwa3,mmlu_high_school_mathematics,0-shot,accuracy,0.36666666666666664,0.029381620726465073
AbacusResearch/haLLAwa3,arc_challenge,25-shot,accuracy,0.6399317406143344,0.01402751681458519
AbacusResearch/haLLAwa3,arc_challenge,25-shot,acc_norm,0.6800341296928327,0.013631345807016193
AbacusResearch/haLLAwa3,hellaswag,10-shot,accuracy,0.7013543118900617,0.004567287775700555
AbacusResearch/haLLAwa3,hellaswag,10-shot,acc_norm,0.8702449711212906,0.003353469625027663
AbacusResearch/haLLAwa3,truthfulqa_mc2,0-shot,accuracy,0.6405912523524916,0.015307419874400935
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,bleu_max,26.60703488440451,0.8059789583751055
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,bleu_acc,0.5458996328029376,0.01742959309132352
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,bleu_diff,8.310410176146629,0.8246277137737857
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge1_max,53.367218868579776,0.8757933437838162
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge1_acc,0.5667074663402693,0.01734702445010748
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge1_diff,11.738773993934803,1.1820094019339218
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge2_max,40.14406041744083,1.0363145440664192
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge2_acc,0.5226438188494492,0.01748554225848964
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rouge2_diff,11.844606657455826,1.2675724960588215
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rougeL_max,50.376439907152886,0.9112608531974777
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rougeL_acc,0.5471236230110159,0.01742558984831402
AbacusResearch/haLLAwa3,truthfulqa_gen,0-shot,rougeL_diff,11.36007163528697,1.1945981073742722
AbacusResearch/haLLAwa3,truthfulqa_mc1,0-shot,accuracy,0.46511627906976744,0.017460849975873965
AbacusResearch/haLLAwa3,winogrande,5-shot,accuracy,0.7916337805840569,0.011414554399987729
openlm-research/open_llama_7b,drop,3-shot,accuracy,0.0008389261744966443,0.00029649629898012564
openlm-research/open_llama_7b,drop,3-shot,f1,0.054966442953020285,0.00134099148142866
openlm-research/open_llama_7b,arc:challenge,25-shot,accuracy,0.43430034129692835,0.014484703048857362
openlm-research/open_llama_7b,arc:challenge,25-shot,acc_norm,0.47013651877133106,0.014585305840007104
openlm-research/open_llama_7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.04605661864718381
openlm-research/open_llama_7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.04605661864718381
openlm-research/open_llama_7b,hendrycksTest-anatomy,5-shot,accuracy,0.3333333333333333,0.04072314811876837
openlm-research/open_llama_7b,hendrycksTest-anatomy,5-shot,acc_norm,0.3333333333333333,0.04072314811876837
openlm-research/open_llama_7b,hendrycksTest-astronomy,5-shot,accuracy,0.24342105263157895,0.034923496688842384
openlm-research/open_llama_7b,hendrycksTest-astronomy,5-shot,acc_norm,0.24342105263157895,0.034923496688842384
openlm-research/open_llama_7b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.04725815626252605
openlm-research/open_llama_7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.04725815626252605
openlm-research/open_llama_7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3886792452830189,0.030000485448675986
openlm-research/open_llama_7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3886792452830189,0.030000485448675986
openlm-research/open_llama_7b,hendrycksTest-college_biology,5-shot,accuracy,0.3263888888888889,0.03921067198982266
openlm-research/open_llama_7b,hendrycksTest-college_biology,5-shot,acc_norm,0.3263888888888889,0.03921067198982266
openlm-research/open_llama_7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909283
openlm-research/open_llama_7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909283
openlm-research/open_llama_7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
openlm-research/open_llama_7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
openlm-research/open_llama_7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.04512608598542127
openlm-research/open_llama_7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.04512608598542127
openlm-research/open_llama_7b,hendrycksTest-college_medicine,5-shot,accuracy,0.34104046242774566,0.03614665424180826
openlm-research/open_llama_7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.34104046242774566,0.03614665424180826
openlm-research/open_llama_7b,hendrycksTest-college_physics,5-shot,accuracy,0.18627450980392157,0.03873958714149351
openlm-research/open_llama_7b,hendrycksTest-college_physics,5-shot,acc_norm,0.18627450980392157,0.03873958714149351
openlm-research/open_llama_7b,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.04878317312145632
openlm-research/open_llama_7b,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.04878317312145632
openlm-research/open_llama_7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.31063829787234043,0.03025123757921317
openlm-research/open_llama_7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.31063829787234043,0.03025123757921317
openlm-research/open_llama_7b,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.042663394431593935
openlm-research/open_llama_7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.042663394431593935
openlm-research/open_llama_7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.30344827586206896,0.038312260488503336
openlm-research/open_llama_7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.30344827586206896,0.038312260488503336
openlm-research/open_llama_7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2698412698412698,0.022860838309232072
openlm-research/open_llama_7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2698412698412698,0.022860838309232072
openlm-research/open_llama_7b,hendrycksTest-formal_logic,5-shot,accuracy,0.23809523809523808,0.03809523809523812
openlm-research/open_llama_7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.23809523809523808,0.03809523809523812
openlm-research/open_llama_7b,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526045
openlm-research/open_llama_7b,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526045
openlm-research/open_llama_7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.3096774193548387,0.026302774983517418
openlm-research/open_llama_7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3096774193548387,0.026302774983517418
openlm-research/open_llama_7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.030516530732694436
openlm-research/open_llama_7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.030516530732694436
openlm-research/open_llama_7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.045604802157206845
openlm-research/open_llama_7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.045604802157206845
openlm-research/open_llama_7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2909090909090909,0.03546563019624336
openlm-research/open_llama_7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2909090909090909,0.03546563019624336
openlm-research/open_llama_7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.3383838383838384,0.033711241426263014
openlm-research/open_llama_7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3383838383838384,0.033711241426263014
openlm-research/open_llama_7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.35751295336787564,0.034588160421810045
openlm-research/open_llama_7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.35751295336787564,0.034588160421810045
openlm-research/open_llama_7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3487179487179487,0.024162780284017717
openlm-research/open_llama_7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3487179487179487,0.024162780284017717
openlm-research/open_llama_7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24074074074074073,0.026067159222275788
openlm-research/open_llama_7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24074074074074073,0.026067159222275788
openlm-research/open_llama_7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.28991596638655465,0.02947248583313608
openlm-research/open_llama_7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.28991596638655465,0.02947248583313608
openlm-research/open_llama_7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.25165562913907286,0.035433042343899844
openlm-research/open_llama_7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.25165562913907286,0.035433042343899844
openlm-research/open_llama_7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3467889908256881,0.020406097104093027
openlm-research/open_llama_7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3467889908256881,0.020406097104093027
openlm-research/open_llama_7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.375,0.033016908987210894
openlm-research/open_llama_7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.375,0.033016908987210894
openlm-research/open_llama_7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3235294117647059,0.032834720561085676
openlm-research/open_llama_7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3235294117647059,0.032834720561085676
openlm-research/open_llama_7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.32489451476793246,0.030486039389105296
openlm-research/open_llama_7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.32489451476793246,0.030486039389105296
openlm-research/open_llama_7b,hendrycksTest-human_aging,5-shot,accuracy,0.28699551569506726,0.03036037971029195
openlm-research/open_llama_7b,hendrycksTest-human_aging,5-shot,acc_norm,0.28699551569506726,0.03036037971029195
openlm-research/open_llama_7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.26717557251908397,0.03880848301082395
openlm-research/open_llama_7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.26717557251908397,0.03880848301082395
openlm-research/open_llama_7b,hendrycksTest-international_law,5-shot,accuracy,0.36363636363636365,0.043913262867240704
openlm-research/open_llama_7b,hendrycksTest-international_law,5-shot,acc_norm,0.36363636363636365,0.043913262867240704
openlm-research/open_llama_7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.35185185185185186,0.04616631111801713
openlm-research/open_llama_7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.35185185185185186,0.04616631111801713
openlm-research/open_llama_7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.03559039531617342
openlm-research/open_llama_7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.03559039531617342
openlm-research/open_llama_7b,hendrycksTest-machine_learning,5-shot,accuracy,0.25,0.04109974682633932
openlm-research/open_llama_7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.25,0.04109974682633932
openlm-research/open_llama_7b,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.04301250399690877
openlm-research/open_llama_7b,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.04301250399690877
openlm-research/open_llama_7b,hendrycksTest-marketing,5-shot,accuracy,0.3717948717948718,0.03166098891888078
openlm-research/open_llama_7b,hendrycksTest-marketing,5-shot,acc_norm,0.3717948717948718,0.03166098891888078
openlm-research/open_llama_7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.0446196043338474
openlm-research/open_llama_7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.0446196043338474
openlm-research/open_llama_7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.36909323116219667,0.017256283109124613
openlm-research/open_llama_7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.36909323116219667,0.017256283109124613
openlm-research/open_llama_7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.32947976878612717,0.025305258131879713
openlm-research/open_llama_7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.32947976878612717,0.025305258131879713
openlm-research/open_llama_7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24692737430167597,0.014422292204808835
openlm-research/open_llama_7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24692737430167597,0.014422292204808835
openlm-research/open_llama_7b,hendrycksTest-nutrition,5-shot,accuracy,0.32679738562091504,0.02685729466328142
openlm-research/open_llama_7b,hendrycksTest-nutrition,5-shot,acc_norm,0.32679738562091504,0.02685729466328142
openlm-research/open_llama_7b,hendrycksTest-philosophy,5-shot,accuracy,0.2861736334405145,0.02567025924218894
openlm-research/open_llama_7b,hendrycksTest-philosophy,5-shot,acc_norm,0.2861736334405145,0.02567025924218894
openlm-research/open_llama_7b,hendrycksTest-prehistory,5-shot,accuracy,0.3117283950617284,0.025773111169630443
openlm-research/open_llama_7b,hendrycksTest-prehistory,5-shot,acc_norm,0.3117283950617284,0.025773111169630443
openlm-research/open_llama_7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902
openlm-research/open_llama_7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902
openlm-research/open_llama_7b,hendrycksTest-professional_law,5-shot,accuracy,0.2522816166883963,0.011092789056875248
openlm-research/open_llama_7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2522816166883963,0.011092789056875248
openlm-research/open_llama_7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.23897058823529413,0.025905280644893006
openlm-research/open_llama_7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.23897058823529413,0.025905280644893006
openlm-research/open_llama_7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.2696078431372549,0.017952449196987866
openlm-research/open_llama_7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2696078431372549,0.017952449196987866
openlm-research/open_llama_7b,hendrycksTest-public_relations,5-shot,accuracy,0.41818181818181815,0.04724577405731571
openlm-research/open_llama_7b,hendrycksTest-public_relations,5-shot,acc_norm,0.41818181818181815,0.04724577405731571
openlm-research/open_llama_7b,hendrycksTest-security_studies,5-shot,accuracy,0.24489795918367346,0.027529637440174923
openlm-research/open_llama_7b,hendrycksTest-security_studies,5-shot,acc_norm,0.24489795918367346,0.027529637440174923
openlm-research/open_llama_7b,hendrycksTest-sociology,5-shot,accuracy,0.24875621890547264,0.0305676759389167
openlm-research/open_llama_7b,hendrycksTest-sociology,5-shot,acc_norm,0.24875621890547264,0.0305676759389167
openlm-research/open_llama_7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.39,0.04902071300001975
openlm-research/open_llama_7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.39,0.04902071300001975
openlm-research/open_llama_7b,hendrycksTest-virology,5-shot,accuracy,0.3493975903614458,0.0371172519074075
openlm-research/open_llama_7b,hendrycksTest-virology,5-shot,acc_norm,0.3493975903614458,0.0371172519074075
openlm-research/open_llama_7b,hendrycksTest-world_religions,5-shot,accuracy,0.391812865497076,0.037439798259264
openlm-research/open_llama_7b,hendrycksTest-world_religions,5-shot,acc_norm,0.391812865497076,0.037439798259264
openlm-research/open_llama_7b,truthfulqa:mc,0-shot,mc1,0.2350061199510404,0.014843061507731618
openlm-research/open_llama_7b,truthfulqa:mc,0-shot,mc2,0.34847307072652783,0.01355010175265617
llama2_220M_nl_100_code_0,minerva_math_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_geometry,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_num_theory,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_prealgebra,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,minerva_math_precalc,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,gsm8k,5-shot,accuracy,0.013646702047005308,0.003195747075480805
llama2_220M_nl_100_code_0,gsm8k_cot,5-shot,accuracy,0.019711902956785442,0.0038289829787357104
llama2_220M_nl_100_code_0,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_2ds,5-shot,accuracy,0.0245,0.003457723662536226
llama2_220M_nl_100_code_0,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339296
llama2_220M_nl_100_code_0,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_2da,5-shot,accuracy,0.007,0.001864735536023767
llama2_220M_nl_100_code_0,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_2dm,5-shot,accuracy,0.0225,0.0033169829948455193
llama2_220M_nl_100_code_0,arithmetic_1dc,5-shot,accuracy,0.0,
llama2_220M_nl_100_code_0,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646802
llama2_220M_nl_100_code_0,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000152
llama2_220M_nl_100_code_0,xnli_ar,0-shot,brier_score,0.9134361549440045,
llama2_220M_nl_100_code_0,xnli_bg,0-shot,brier_score,1.1059027504585148,
llama2_220M_nl_100_code_0,xnli_de,0-shot,brier_score,0.9182664656781209,
llama2_220M_nl_100_code_0,xnli_el,0-shot,brier_score,1.0146664436828279,
llama2_220M_nl_100_code_0,xnli_en,0-shot,brier_score,0.7165642552011161,
llama2_220M_nl_100_code_0,xnli_es,0-shot,brier_score,1.068453256738231,
llama2_220M_nl_100_code_0,xnli_fr,0-shot,brier_score,0.9966650739305976,
llama2_220M_nl_100_code_0,xnli_hi,0-shot,brier_score,1.085659903781425,
llama2_220M_nl_100_code_0,xnli_ru,0-shot,brier_score,0.8896788290214876,
llama2_220M_nl_100_code_0,xnli_sw,0-shot,brier_score,0.94306595191456,
llama2_220M_nl_100_code_0,xnli_th,0-shot,brier_score,0.925220386341976,
llama2_220M_nl_100_code_0,xnli_tr,0-shot,brier_score,1.1127636740614655,
llama2_220M_nl_100_code_0,xnli_ur,0-shot,brier_score,1.319010883603471,
llama2_220M_nl_100_code_0,xnli_vi,0-shot,brier_score,1.0261665968416116,
llama2_220M_nl_100_code_0,xnli_zh,0-shot,brier_score,1.311822001659871,
llama2_220M_nl_100_code_0,anli_r1,0-shot,brier_score,0.8449496978059758,
llama2_220M_nl_100_code_0,anli_r3,0-shot,brier_score,0.857186090421117,
llama2_220M_nl_100_code_0,anli_r2,0-shot,brier_score,0.8635985195897238,
llama2_220M_nl_100_code_0,mathqa,5-shot,brier_score,1.0389827270143788,
llama2_220M_nl_100_code_0,logiqa2,0-shot,brier_score,1.172976871272462,
llama2_220M_nl_100_code_0,lambada_standard,0-shot,perplexity,111.71321527976326,4.383054748380545
llama2_220M_nl_100_code_0,lambada_standard,0-shot,accuracy,0.21540849990296915,0.005727502539516408
llama2_220M_nl_100_code_0,lambada_openai,0-shot,perplexity,47.444161536621785,1.7285309338117683
llama2_220M_nl_100_code_0,lambada_openai,0-shot,accuracy,0.29807878905491947,0.006372675324693508
facebook/xglm-7.5B,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_prealgebra,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-7.5B,minerva_math_algebra,5-shot,accuracy,0.0,
facebook/xglm-7.5B,fld_default,0-shot,accuracy,0.0,
facebook/xglm-7.5B,fld_star,0-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_3da,5-shot,accuracy,0.0005,0.0005000000000000127
facebook/xglm-7.5B,arithmetic_3ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
facebook/xglm-7.5B,arithmetic_2ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_1dc,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-7.5B,arithmetic_2dm,5-shot,accuracy,0.0045,0.001496995490223334
facebook/xglm-7.5B,arithmetic_2da,5-shot,accuracy,0.0015,0.000865592066052149
facebook/xglm-7.5B,gsm8k_cot,5-shot,accuracy,0.003032600454890068,0.0015145735612245405
facebook/xglm-7.5B,gsm8k,5-shot,accuracy,0.002274450341167551,0.001312157814867432
facebook/xglm-7.5B,anli_r2,0-shot,brier_score,0.7252401406403721,
facebook/xglm-7.5B,anli_r3,0-shot,brier_score,0.7345576777518716,
facebook/xglm-7.5B,anli_r1,0-shot,brier_score,0.7403058522962276,
facebook/xglm-7.5B,xnli_eu,0-shot,brier_score,0.7249978471618856,
facebook/xglm-7.5B,xnli_vi,0-shot,brier_score,0.7449262360122225,
facebook/xglm-7.5B,xnli_ru,0-shot,brier_score,0.776968330898939,
facebook/xglm-7.5B,xnli_zh,0-shot,brier_score,1.0503581196137082,
facebook/xglm-7.5B,xnli_tr,0-shot,brier_score,0.7725432543659072,
facebook/xglm-7.5B,xnli_fr,0-shot,brier_score,0.8140567425559275,
facebook/xglm-7.5B,xnli_en,0-shot,brier_score,0.6517840395340417,
facebook/xglm-7.5B,xnli_ur,0-shot,brier_score,0.8831297423089415,
facebook/xglm-7.5B,xnli_ar,0-shot,brier_score,1.2554211575716623,
facebook/xglm-7.5B,xnli_de,0-shot,brier_score,0.7983353916849525,
facebook/xglm-7.5B,xnli_hi,0-shot,brier_score,0.7335594240977125,
facebook/xglm-7.5B,xnli_es,0-shot,brier_score,0.8237671750458347,
facebook/xglm-7.5B,xnli_bg,0-shot,brier_score,0.8170940878498713,
facebook/xglm-7.5B,xnli_sw,0-shot,brier_score,0.7365080187868047,
facebook/xglm-7.5B,xnli_el,0-shot,brier_score,0.8709162316623921,
facebook/xglm-7.5B,xnli_th,0-shot,brier_score,0.8133822575764703,
facebook/xglm-7.5B,logiqa2,0-shot,brier_score,1.0735932488634556,
facebook/xglm-7.5B,mathqa,5-shot,brier_score,0.988707059624341,
facebook/xglm-7.5B,lambada_standard,0-shot,perplexity,8.331266240940502,0.22169161559199443
facebook/xglm-7.5B,lambada_standard,0-shot,accuracy,0.5460896565107705,0.006936319475444719
facebook/xglm-7.5B,lambada_openai,0-shot,perplexity,7.454260315964878,0.19833941098139538
facebook/xglm-7.5B,lambada_openai,0-shot,accuracy,0.5561808655152338,0.006921864695286302
facebook/xglm-7.5B,mmlu_world_religions,0-shot,accuracy,0.27485380116959063,0.03424042924691583
facebook/xglm-7.5B,mmlu_formal_logic,0-shot,accuracy,0.31746031746031744,0.04163453031302859
facebook/xglm-7.5B,mmlu_prehistory,0-shot,accuracy,0.2716049382716049,0.024748624490537365
facebook/xglm-7.5B,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217892
facebook/xglm-7.5B,mmlu_high_school_world_history,0-shot,accuracy,0.25316455696202533,0.028304657943035296
facebook/xglm-7.5B,mmlu_moral_disputes,0-shot,accuracy,0.23121387283236994,0.02269865716785571
facebook/xglm-7.5B,mmlu_professional_law,0-shot,accuracy,0.2438070404172099,0.010966507972178475
facebook/xglm-7.5B,mmlu_logical_fallacies,0-shot,accuracy,0.25153374233128833,0.03408997886857529
facebook/xglm-7.5B,mmlu_high_school_us_history,0-shot,accuracy,0.23529411764705882,0.029771775228145628
facebook/xglm-7.5B,mmlu_philosophy,0-shot,accuracy,0.2315112540192926,0.023956532766639137
facebook/xglm-7.5B,mmlu_jurisprudence,0-shot,accuracy,0.21296296296296297,0.03957835471980979
facebook/xglm-7.5B,mmlu_international_law,0-shot,accuracy,0.34710743801652894,0.043457245702925335
facebook/xglm-7.5B,mmlu_high_school_european_history,0-shot,accuracy,0.28484848484848485,0.03524390844511783
facebook/xglm-7.5B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.3626943005181347,0.034697137917043715
facebook/xglm-7.5B,mmlu_high_school_microeconomics,0-shot,accuracy,0.3319327731092437,0.030588697013783663
facebook/xglm-7.5B,mmlu_high_school_geography,0-shot,accuracy,0.36363636363636365,0.03427308652999933
facebook/xglm-7.5B,mmlu_high_school_psychology,0-shot,accuracy,0.26238532110091745,0.018861885021534738
facebook/xglm-7.5B,mmlu_public_relations,0-shot,accuracy,0.24545454545454545,0.041220665028782834
facebook/xglm-7.5B,mmlu_us_foreign_policy,0-shot,accuracy,0.22,0.04163331998932269
facebook/xglm-7.5B,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409217
facebook/xglm-7.5B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3564102564102564,0.0242831405294673
facebook/xglm-7.5B,mmlu_security_studies,0-shot,accuracy,0.2857142857142857,0.0289205832206756
facebook/xglm-7.5B,mmlu_professional_psychology,0-shot,accuracy,0.2647058823529412,0.01784808957491322
facebook/xglm-7.5B,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596917
facebook/xglm-7.5B,mmlu_econometrics,0-shot,accuracy,0.21929824561403508,0.038924311065187504
facebook/xglm-7.5B,mmlu_miscellaneous,0-shot,accuracy,0.26053639846743293,0.015696008563807092
facebook/xglm-7.5B,mmlu_marketing,0-shot,accuracy,0.18803418803418803,0.025598193686652244
facebook/xglm-7.5B,mmlu_management,0-shot,accuracy,0.2815533980582524,0.04453254836326466
facebook/xglm-7.5B,mmlu_nutrition,0-shot,accuracy,0.2777777777777778,0.02564686309713791
facebook/xglm-7.5B,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
facebook/xglm-7.5B,mmlu_human_aging,0-shot,accuracy,0.3273542600896861,0.03149384670994131
facebook/xglm-7.5B,mmlu_professional_medicine,0-shot,accuracy,0.4522058823529412,0.03023375855159645
facebook/xglm-7.5B,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/xglm-7.5B,mmlu_business_ethics,0-shot,accuracy,0.25,0.04351941398892446
facebook/xglm-7.5B,mmlu_clinical_knowledge,0-shot,accuracy,0.3018867924528302,0.028254200344438655
facebook/xglm-7.5B,mmlu_global_facts,0-shot,accuracy,0.3,0.046056618647183814
facebook/xglm-7.5B,mmlu_virology,0-shot,accuracy,0.2891566265060241,0.035294868015111155
facebook/xglm-7.5B,mmlu_professional_accounting,0-shot,accuracy,0.25886524822695034,0.026129572527180848
facebook/xglm-7.5B,mmlu_college_physics,0-shot,accuracy,0.19607843137254902,0.039505818611799616
facebook/xglm-7.5B,mmlu_high_school_physics,0-shot,accuracy,0.304635761589404,0.03757949922943343
facebook/xglm-7.5B,mmlu_high_school_biology,0-shot,accuracy,0.33225806451612905,0.026795560848122787
facebook/xglm-7.5B,mmlu_college_biology,0-shot,accuracy,0.2916666666666667,0.03800968060554858
facebook/xglm-7.5B,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.03749850709174021
facebook/xglm-7.5B,mmlu_college_chemistry,0-shot,accuracy,0.27,0.044619604333847394
facebook/xglm-7.5B,mmlu_computer_security,0-shot,accuracy,0.28,0.04512608598542127
facebook/xglm-7.5B,mmlu_college_computer_science,0-shot,accuracy,0.31,0.04648231987117316
facebook/xglm-7.5B,mmlu_astronomy,0-shot,accuracy,0.21710526315789475,0.03355045304882924
facebook/xglm-7.5B,mmlu_college_mathematics,0-shot,accuracy,0.25,0.04351941398892446
facebook/xglm-7.5B,mmlu_conceptual_physics,0-shot,accuracy,0.28085106382978725,0.02937917046412482
facebook/xglm-7.5B,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
facebook/xglm-7.5B,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.04560480215720683
facebook/xglm-7.5B,mmlu_machine_learning,0-shot,accuracy,0.23214285714285715,0.04007341809755806
facebook/xglm-7.5B,mmlu_high_school_chemistry,0-shot,accuracy,0.30049261083743845,0.03225799476233485
facebook/xglm-7.5B,mmlu_high_school_statistics,0-shot,accuracy,0.4212962962962963,0.03367462138896078
facebook/xglm-7.5B,mmlu_elementary_mathematics,0-shot,accuracy,0.2671957671957672,0.02278967314577657
facebook/xglm-7.5B,mmlu_electrical_engineering,0-shot,accuracy,0.21379310344827587,0.034165204477475494
facebook/xglm-7.5B,mmlu_high_school_mathematics,0-shot,accuracy,0.25555555555555554,0.02659393910184407
facebook/xglm-7.5B,arc_challenge,25-shot,accuracy,0.310580204778157,0.013522292098053047
facebook/xglm-7.5B,arc_challenge,25-shot,acc_norm,0.3387372013651877,0.01383056892797433
facebook/xglm-7.5B,hellaswag,10-shot,accuracy,0.44961163114917346,0.004964378762425247
facebook/xglm-7.5B,hellaswag,10-shot,acc_norm,0.6077474606652061,0.004872546302641851
facebook/xglm-7.5B,truthfulqa_mc2,0-shot,accuracy,0.36382995217114145,0.01364982350224989
facebook/xglm-7.5B,truthfulqa_gen,0-shot,bleu_max,6.098360177088277,0.3118236431976185
facebook/xglm-7.5B,truthfulqa_gen,0-shot,bleu_acc,0.2558139534883721,0.01527417621928335
facebook/xglm-7.5B,truthfulqa_gen,0-shot,bleu_diff,-2.2599605981406405,0.2506337182789708
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge1_max,21.779416715306077,0.5244944853770919
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge1_acc,0.3072215422276622,0.016150201321323006
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge1_diff,-3.390869478818047,0.39043003996311954
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge2_max,11.203254328402076,0.4899639367721538
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge2_acc,0.18482252141982863,0.013588091176049533
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rouge2_diff,-4.243657550945619,0.4037168891317911
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rougeL_max,19.716590648355854,0.5021969423588765
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rougeL_acc,0.2864137086903305,0.01582614243950234
facebook/xglm-7.5B,truthfulqa_gen,0-shot,rougeL_diff,-3.591440069116983,0.37059524653156883
facebook/xglm-7.5B,truthfulqa_mc1,0-shot,accuracy,0.20685434516523868,0.014179591496728337
facebook/xglm-7.5B,winogrande,5-shot,accuracy,0.5872138910812944,0.013837060648682094
EleutherAI/pythia-12b-deduped,drop,3-shot,accuracy,0.0008389261744966443,0.0002964962989801232
EleutherAI/pythia-12b-deduped,drop,3-shot,f1,0.04548238255033574,0.0011460514648967963
EleutherAI/pythia-12b-deduped,gsm8k,5-shot,accuracy,0.014404852160727824,0.003282055917136946
EleutherAI/pythia-12b-deduped,winogrande,5-shot,accuracy,0.664561957379637,0.013269575904851425
EleutherAI/pythia-12b-deduped,arc:challenge,25-shot,accuracy,0.378839590443686,0.01417591549000032
EleutherAI/pythia-12b-deduped,arc:challenge,25-shot,acc_norm,0.4138225255972696,0.014392730009221009
EleutherAI/pythia-12b-deduped,hellaswag,10-shot,accuracy,0.5183230432184823,0.0049864298081467705
EleutherAI/pythia-12b-deduped,hellaswag,10-shot,acc_norm,0.7026488747261501,0.004561582009834582
EleutherAI/pythia-12b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.29,0.04560480215720684
EleutherAI/pythia-12b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.29,0.04560480215720684
EleutherAI/pythia-12b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.22962962962962963,0.03633384414073464
EleutherAI/pythia-12b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.22962962962962963,0.03633384414073464
EleutherAI/pythia-12b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.03583496176361063
EleutherAI/pythia-12b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.03583496176361063
EleutherAI/pythia-12b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.2,0.04020151261036846
EleutherAI/pythia-12b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.2,0.04020151261036846
EleutherAI/pythia-12b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.26037735849056604,0.027008766090708104
EleutherAI/pythia-12b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.26037735849056604,0.027008766090708104
EleutherAI/pythia-12b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
EleutherAI/pythia-12b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.03685651095897532
EleutherAI/pythia-12b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.19,0.03942772444036625
EleutherAI/pythia-12b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.19,0.03942772444036625
EleutherAI/pythia-12b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.22,0.0416333199893227
EleutherAI/pythia-12b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.22,0.0416333199893227
EleutherAI/pythia-12b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/pythia-12b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.04512608598542128
EleutherAI/pythia-12b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.2254335260115607,0.03186209851641143
EleutherAI/pythia-12b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.2254335260115607,0.03186209851641143
EleutherAI/pythia-12b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.18627450980392157,0.03873958714149351
EleutherAI/pythia-12b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.18627450980392157,0.03873958714149351
EleutherAI/pythia-12b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-12b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-12b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.028957342788342347
EleutherAI/pythia-12b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.028957342788342347
EleutherAI/pythia-12b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.041424397194893596
EleutherAI/pythia-12b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.041424397194893596
EleutherAI/pythia-12b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.22758620689655173,0.03493950380131183
EleutherAI/pythia-12b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.22758620689655173,0.03493950380131183
EleutherAI/pythia-12b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.26455026455026454,0.022717467897708614
EleutherAI/pythia-12b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.26455026455026454,0.022717467897708614
EleutherAI/pythia-12b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.19047619047619047,0.03512207412302054
EleutherAI/pythia-12b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.19047619047619047,0.03512207412302054
EleutherAI/pythia-12b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.2,0.04020151261036846
EleutherAI/pythia-12b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.2,0.04020151261036846
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.23870967741935484,0.024251071262208834
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.23870967741935484,0.024251071262208834
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2660098522167488,0.031089826002937523
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2660098522167488,0.031089826002937523
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.045126085985421276
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23636363636363636,0.03317505930009181
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23636363636363636,0.03317505930009181
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.029620227874790486
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.029620227874790486
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.20725388601036268,0.02925282329180363
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.20725388601036268,0.02925282329180363
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.258974358974359,0.022211106810061665
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.258974358974359,0.022211106810061665
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2740740740740741,0.02719593480408562
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2740740740740741,0.02719593480408562
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2815126050420168,0.02921354941437217
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2815126050420168,0.02921354941437217
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.2251655629139073,0.03410435282008937
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2251655629139073,0.03410435282008937
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2036697247706422,0.01726674208763079
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2036697247706422,0.01726674208763079
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.16666666666666666,0.025416428388767478
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.16666666666666666,0.025416428388767478
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.24509803921568626,0.03019028245350195
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.24509803921568626,0.03019028245350195
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598035
EleutherAI/pythia-12b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2742616033755274,0.029041333510598035
EleutherAI/pythia-12b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.36771300448430494,0.03236198350928275
EleutherAI/pythia-12b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.36771300448430494,0.03236198350928275
EleutherAI/pythia-12b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/pythia-12b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
EleutherAI/pythia-12b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.4628099173553719,0.04551711196104218
EleutherAI/pythia-12b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.4628099173553719,0.04551711196104218
EleutherAI/pythia-12b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.044143436668549335
EleutherAI/pythia-12b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.044143436668549335
EleutherAI/pythia-12b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2331288343558282,0.03322015795776741
EleutherAI/pythia-12b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2331288343558282,0.03322015795776741
EleutherAI/pythia-12b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.25,0.04109974682633932
EleutherAI/pythia-12b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.25,0.04109974682633932
EleutherAI/pythia-12b-deduped,hendrycksTest-management,5-shot,accuracy,0.22330097087378642,0.04123553189891431
EleutherAI/pythia-12b-deduped,hendrycksTest-management,5-shot,acc_norm,0.22330097087378642,0.04123553189891431
EleutherAI/pythia-12b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.26495726495726496,0.028911208802749472
EleutherAI/pythia-12b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.26495726495726496,0.028911208802749472
EleutherAI/pythia-12b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.21,0.04093601807403326
EleutherAI/pythia-12b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.21,0.04093601807403326
EleutherAI/pythia-12b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.29246487867177523,0.016267000684598642
EleutherAI/pythia-12b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.29246487867177523,0.016267000684598642
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.3092485549132948,0.024883140570071755
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3092485549132948,0.024883140570071755
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.01435591196476786
EleutherAI/pythia-12b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.01435591196476786
EleutherAI/pythia-12b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.2973856209150327,0.02617390850671858
EleutherAI/pythia-12b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.2973856209150327,0.02617390850671858
EleutherAI/pythia-12b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.27009646302250806,0.025218040373410616
EleutherAI/pythia-12b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.27009646302250806,0.025218040373410616
EleutherAI/pythia-12b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.25925925925925924,0.024383665531035457
EleutherAI/pythia-12b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.25925925925925924,0.024383665531035457
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.24822695035460993,0.025770015644290382
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24822695035460993,0.025770015644290382
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.26140808344198174,0.011222528169771314
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.26140808344198174,0.011222528169771314
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.19852941176470587,0.024231013370541104
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.19852941176470587,0.024231013370541104
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.29248366013071897,0.01840341571010979
EleutherAI/pythia-12b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.29248366013071897,0.01840341571010979
EleutherAI/pythia-12b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.04172343038705383
EleutherAI/pythia-12b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.04172343038705383
EleutherAI/pythia-12b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.2530612244897959,0.02783302387139968
EleutherAI/pythia-12b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.2530612244897959,0.02783302387139968
EleutherAI/pythia-12b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.26865671641791045,0.03134328358208954
EleutherAI/pythia-12b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.26865671641791045,0.03134328358208954
EleutherAI/pythia-12b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.040936018074033256
EleutherAI/pythia-12b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.040936018074033256
EleutherAI/pythia-12b-deduped,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.0362933532994786
EleutherAI/pythia-12b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.0362933532994786
EleutherAI/pythia-12b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.3508771929824561,0.03660298834049162
EleutherAI/pythia-12b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.3508771929824561,0.03660298834049162
EleutherAI/pythia-12b-deduped,truthfulqa:mc,0-shot,mc1,0.20930232558139536,0.014241219434785828
EleutherAI/pythia-12b-deduped,truthfulqa:mc,0-shot,mc2,0.329988729051366,0.013122098603854236
facebook/opt-350m,arc:challenge,25-shot,accuracy,0.20563139931740615,0.011810745260742585
facebook/opt-350m,arc:challenge,25-shot,acc_norm,0.2354948805460751,0.012399451855004745
facebook/opt-350m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.04163331998932268
facebook/opt-350m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.04163331998932268
facebook/opt-350m,hendrycksTest-anatomy,5-shot,accuracy,0.2740740740740741,0.03853254836552003
facebook/opt-350m,hendrycksTest-anatomy,5-shot,acc_norm,0.2740740740740741,0.03853254836552003
facebook/opt-350m,hendrycksTest-astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
facebook/opt-350m,hendrycksTest-astronomy,5-shot,acc_norm,0.17763157894736842,0.031103182383123398
facebook/opt-350m,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.040936018074033256
facebook/opt-350m,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.040936018074033256
facebook/opt-350m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.02725726032249485
facebook/opt-350m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.02725726032249485
facebook/opt-350m,hendrycksTest-college_biology,5-shot,accuracy,0.22916666666666666,0.035146974678623884
facebook/opt-350m,hendrycksTest-college_biology,5-shot,acc_norm,0.22916666666666666,0.035146974678623884
facebook/opt-350m,hendrycksTest-college_chemistry,5-shot,accuracy,0.35,0.0479372485441102
facebook/opt-350m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.35,0.0479372485441102
facebook/opt-350m,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.046056618647183814
facebook/opt-350m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.046056618647183814
facebook/opt-350m,hendrycksTest-college_mathematics,5-shot,accuracy,0.27,0.044619604333847394
facebook/opt-350m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.27,0.044619604333847394
facebook/opt-350m,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/opt-350m,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
facebook/opt-350m,hendrycksTest-college_physics,5-shot,accuracy,0.22549019607843138,0.04158307533083286
facebook/opt-350m,hendrycksTest-college_physics,5-shot,acc_norm,0.22549019607843138,0.04158307533083286
facebook/opt-350m,hendrycksTest-computer_security,5-shot,accuracy,0.18,0.038612291966536934
facebook/opt-350m,hendrycksTest-computer_security,5-shot,acc_norm,0.18,0.038612291966536934
facebook/opt-350m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2723404255319149,0.0291012906983867
facebook/opt-350m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2723404255319149,0.0291012906983867
facebook/opt-350m,hendrycksTest-econometrics,5-shot,accuracy,0.23684210526315788,0.039994238792813344
facebook/opt-350m,hendrycksTest-econometrics,5-shot,acc_norm,0.23684210526315788,0.039994238792813344
facebook/opt-350m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.038061426873099935
facebook/opt-350m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.038061426873099935
facebook/opt-350m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2566137566137566,0.022494510767503154
facebook/opt-350m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2566137566137566,0.022494510767503154
facebook/opt-350m,hendrycksTest-formal_logic,5-shot,accuracy,0.23809523809523808,0.038095238095238126
facebook/opt-350m,hendrycksTest-formal_logic,5-shot,acc_norm,0.23809523809523808,0.038095238095238126
facebook/opt-350m,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.03942772444036624
facebook/opt-350m,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.03942772444036624
facebook/opt-350m,hendrycksTest-high_school_biology,5-shot,accuracy,0.3,0.026069362295335137
facebook/opt-350m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3,0.026069362295335137
facebook/opt-350m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.29064039408866993,0.031947400722655395
facebook/opt-350m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.29064039408866993,0.031947400722655395
facebook/opt-350m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.039427724440366234
facebook/opt-350m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.039427724440366234
facebook/opt-350m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.26666666666666666,0.03453131801885415
facebook/opt-350m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.26666666666666666,0.03453131801885415
facebook/opt-350m,hendrycksTest-high_school_geography,5-shot,accuracy,0.35858585858585856,0.03416903640391521
facebook/opt-350m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.35858585858585856,0.03416903640391521
facebook/opt-350m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3471502590673575,0.03435696168361355
facebook/opt-350m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3471502590673575,0.03435696168361355
facebook/opt-350m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2743589743589744,0.022622765767493214
facebook/opt-350m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2743589743589744,0.022622765767493214
facebook/opt-350m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2777777777777778,0.027309140588230175
facebook/opt-350m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2777777777777778,0.027309140588230175
facebook/opt-350m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.27310924369747897,0.028942004040998167
facebook/opt-350m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.27310924369747897,0.028942004040998167
facebook/opt-350m,hendrycksTest-high_school_physics,5-shot,accuracy,0.33112582781456956,0.038425817186598696
facebook/opt-350m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.33112582781456956,0.038425817186598696
facebook/opt-350m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.326605504587156,0.0201069908899373
facebook/opt-350m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.326605504587156,0.0201069908899373
facebook/opt-350m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-350m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
facebook/opt-350m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25980392156862747,0.030778554678693257
facebook/opt-350m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25980392156862747,0.030778554678693257
facebook/opt-350m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.20675105485232068,0.026361651668389104
facebook/opt-350m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.20675105485232068,0.026361651668389104
facebook/opt-350m,hendrycksTest-human_aging,5-shot,accuracy,0.13452914798206278,0.02290118376157559
facebook/opt-350m,hendrycksTest-human_aging,5-shot,acc_norm,0.13452914798206278,0.02290118376157559
facebook/opt-350m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
facebook/opt-350m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
facebook/opt-350m,hendrycksTest-international_law,5-shot,accuracy,0.371900826446281,0.044120158066245044
facebook/opt-350m,hendrycksTest-international_law,5-shot,acc_norm,0.371900826446281,0.044120158066245044
facebook/opt-350m,hendrycksTest-jurisprudence,5-shot,accuracy,0.2222222222222222,0.040191074725573483
facebook/opt-350m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2222222222222222,0.040191074725573483
facebook/opt-350m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.24539877300613497,0.03380939813943354
facebook/opt-350m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.24539877300613497,0.03380939813943354
facebook/opt-350m,hendrycksTest-machine_learning,5-shot,accuracy,0.17857142857142858,0.03635209121577806
facebook/opt-350m,hendrycksTest-machine_learning,5-shot,acc_norm,0.17857142857142858,0.03635209121577806
facebook/opt-350m,hendrycksTest-management,5-shot,accuracy,0.22330097087378642,0.04123553189891431
facebook/opt-350m,hendrycksTest-management,5-shot,acc_norm,0.22330097087378642,0.04123553189891431
facebook/opt-350m,hendrycksTest-marketing,5-shot,accuracy,0.21367521367521367,0.02685345037700915
facebook/opt-350m,hendrycksTest-marketing,5-shot,acc_norm,0.21367521367521367,0.02685345037700915
facebook/opt-350m,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.045604802157206845
facebook/opt-350m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.045604802157206845
facebook/opt-350m,hendrycksTest-miscellaneous,5-shot,accuracy,0.20561941251596424,0.014452500456785825
facebook/opt-350m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.20561941251596424,0.014452500456785825
facebook/opt-350m,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.023703099525258182
facebook/opt-350m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.023703099525258182
facebook/opt-350m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24692737430167597,0.014422292204808835
facebook/opt-350m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24692737430167597,0.014422292204808835
facebook/opt-350m,hendrycksTest-nutrition,5-shot,accuracy,0.26143790849673204,0.025160998214292456
facebook/opt-350m,hendrycksTest-nutrition,5-shot,acc_norm,0.26143790849673204,0.025160998214292456
facebook/opt-350m,hendrycksTest-philosophy,5-shot,accuracy,0.19614147909967847,0.022552447780478026
facebook/opt-350m,hendrycksTest-philosophy,5-shot,acc_norm,0.19614147909967847,0.022552447780478026
facebook/opt-350m,hendrycksTest-prehistory,5-shot,accuracy,0.2716049382716049,0.02474862449053737
facebook/opt-350m,hendrycksTest-prehistory,5-shot,acc_norm,0.2716049382716049,0.02474862449053737
facebook/opt-350m,hendrycksTest-professional_accounting,5-shot,accuracy,0.24822695035460993,0.02577001564429039
facebook/opt-350m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24822695035460993,0.02577001564429039
facebook/opt-350m,hendrycksTest-professional_law,5-shot,accuracy,0.24119947848761408,0.010926496102034956
facebook/opt-350m,hendrycksTest-professional_law,5-shot,acc_norm,0.24119947848761408,0.010926496102034956
facebook/opt-350m,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121593
facebook/opt-350m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.030211479609121593
facebook/opt-350m,hendrycksTest-professional_psychology,5-shot,accuracy,0.24183006535947713,0.017322789207784326
facebook/opt-350m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.24183006535947713,0.017322789207784326
facebook/opt-350m,hendrycksTest-public_relations,5-shot,accuracy,0.19090909090909092,0.03764425585984924
facebook/opt-350m,hendrycksTest-public_relations,5-shot,acc_norm,0.19090909090909092,0.03764425585984924
facebook/opt-350m,hendrycksTest-security_studies,5-shot,accuracy,0.39591836734693875,0.03130802899065686
facebook/opt-350m,hendrycksTest-security_studies,5-shot,acc_norm,0.39591836734693875,0.03130802899065686
facebook/opt-350m,hendrycksTest-sociology,5-shot,accuracy,0.21890547263681592,0.029239174636647
facebook/opt-350m,hendrycksTest-sociology,5-shot,acc_norm,0.21890547263681592,0.029239174636647
facebook/opt-350m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.04408440022768078
facebook/opt-350m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.04408440022768078
facebook/opt-350m,hendrycksTest-virology,5-shot,accuracy,0.1927710843373494,0.030709824050565274
facebook/opt-350m,hendrycksTest-virology,5-shot,acc_norm,0.1927710843373494,0.030709824050565274
facebook/opt-350m,hendrycksTest-world_religions,5-shot,accuracy,0.1871345029239766,0.029913127232368015
facebook/opt-350m,hendrycksTest-world_religions,5-shot,acc_norm,0.1871345029239766,0.029913127232368015
facebook/opt-350m,truthfulqa:mc,0-shot,mc1,0.23255813953488372,0.014789157531080514
facebook/opt-350m,truthfulqa:mc,0-shot,mc2,0.40828532777844695,0.014670017081097357
facebook/opt-350m,drop,3-shot,accuracy,0.0006291946308724832,0.0002568002749723937
facebook/opt-350m,drop,3-shot,f1,0.04159815436241622,0.0011509154641292957
aisingapore/sea-lion-7b,minerva_math_precalc,5-shot,accuracy,0.018315018315018316,0.00574369673165366
aisingapore/sea-lion-7b,minerva_math_prealgebra,5-shot,accuracy,0.02640642939150402,0.005436057762573988
aisingapore/sea-lion-7b,minerva_math_num_theory,5-shot,accuracy,0.018518518518518517,0.0058069728079122715
aisingapore/sea-lion-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.02547065337763012,0.0052458302725593275
aisingapore/sea-lion-7b,minerva_math_geometry,5-shot,accuracy,0.016701461377870562,0.005861462425818019
aisingapore/sea-lion-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.008438818565400843,0.004206007207713057
aisingapore/sea-lion-7b,minerva_math_algebra,5-shot,accuracy,0.020219039595619208,0.004086979080518444
aisingapore/sea-lion-7b,fld_default,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b,fld_star,0-shot,accuracy,0.0,
aisingapore/sea-lion-7b,arithmetic_3da,5-shot,accuracy,0.0055,0.0016541593398342208
aisingapore/sea-lion-7b,arithmetic_3ds,5-shot,accuracy,0.017,0.002891311093590548
aisingapore/sea-lion-7b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
aisingapore/sea-lion-7b,arithmetic_2ds,5-shot,accuracy,0.1115,0.0070397907871727824
aisingapore/sea-lion-7b,arithmetic_5ds,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b,arithmetic_5da,5-shot,accuracy,0.0,
aisingapore/sea-lion-7b,arithmetic_1dc,5-shot,accuracy,0.068,0.005630617366325325
aisingapore/sea-lion-7b,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005000000000000043
aisingapore/sea-lion-7b,arithmetic_2dm,5-shot,accuracy,0.0935,0.006511534000335054
aisingapore/sea-lion-7b,arithmetic_2da,5-shot,accuracy,0.087,0.006303599581496369
aisingapore/sea-lion-7b,gsm8k_cot,5-shot,accuracy,0.03335860500379075,0.0049462826491737735
aisingapore/sea-lion-7b,gsm8k,5-shot,accuracy,0.02577710386656558,0.004365042953621813
aisingapore/sea-lion-7b,anli_r2,0-shot,brier_score,0.7371776965553825,
aisingapore/sea-lion-7b,anli_r3,0-shot,brier_score,0.7142007390796328,
aisingapore/sea-lion-7b,anli_r1,0-shot,brier_score,0.7412739329209331,
aisingapore/sea-lion-7b,xnli_eu,0-shot,brier_score,1.113847354553935,
aisingapore/sea-lion-7b,xnli_vi,0-shot,brier_score,0.7514185268043483,
aisingapore/sea-lion-7b,xnli_ru,0-shot,brier_score,0.8063321258911998,
aisingapore/sea-lion-7b,xnli_zh,0-shot,brier_score,1.0744744185180415,
aisingapore/sea-lion-7b,xnli_tr,0-shot,brier_score,0.8904322785337762,
aisingapore/sea-lion-7b,xnli_fr,0-shot,brier_score,0.7973751206359644,
aisingapore/sea-lion-7b,xnli_en,0-shot,brier_score,0.6864404290885256,
aisingapore/sea-lion-7b,xnli_ur,0-shot,brier_score,1.3105023198255918,
aisingapore/sea-lion-7b,xnli_ar,0-shot,brier_score,0.8566459473172638,
aisingapore/sea-lion-7b,xnli_de,0-shot,brier_score,0.8048918665563971,
aisingapore/sea-lion-7b,xnli_hi,0-shot,brier_score,1.0018933981610276,
aisingapore/sea-lion-7b,xnli_es,0-shot,brier_score,0.9309039010456585,
aisingapore/sea-lion-7b,xnli_bg,0-shot,brier_score,0.7926756354946493,
aisingapore/sea-lion-7b,xnli_sw,0-shot,brier_score,0.9224405259782638,
aisingapore/sea-lion-7b,xnli_el,0-shot,brier_score,1.1955451021051349,
aisingapore/sea-lion-7b,xnli_th,0-shot,brier_score,0.7636640796291084,
aisingapore/sea-lion-7b,logiqa2,0-shot,brier_score,1.1108267492611301,
aisingapore/sea-lion-7b,mathqa,5-shot,brier_score,0.9547448253077948,
aisingapore/sea-lion-7b,lambada_standard,0-shot,perplexity,6.716223440457293,0.171380584266027
aisingapore/sea-lion-7b,lambada_standard,0-shot,accuracy,0.5767514069474092,0.006883418357084482
aisingapore/sea-lion-7b,lambada_openai,0-shot,perplexity,5.278455783743519,0.1252746999189692
aisingapore/sea-lion-7b,lambada_openai,0-shot,accuracy,0.638074907820687,0.00669510284767741
aisingapore/sea-lion-7b,mmlu_world_religions,0-shot,accuracy,0.30994152046783624,0.03546976959393164
aisingapore/sea-lion-7b,mmlu_formal_logic,0-shot,accuracy,0.2619047619047619,0.039325376803928724
aisingapore/sea-lion-7b,mmlu_prehistory,0-shot,accuracy,0.25,0.02409347123262133
aisingapore/sea-lion-7b,mmlu_moral_scenarios,0-shot,accuracy,0.27262569832402234,0.014893391735249603
aisingapore/sea-lion-7b,mmlu_high_school_world_history,0-shot,accuracy,0.28270042194092826,0.029312814153955917
aisingapore/sea-lion-7b,mmlu_moral_disputes,0-shot,accuracy,0.2976878612716763,0.024617055388677003
aisingapore/sea-lion-7b,mmlu_professional_law,0-shot,accuracy,0.288135593220339,0.011567140661324568
aisingapore/sea-lion-7b,mmlu_logical_fallacies,0-shot,accuracy,0.25153374233128833,0.034089978868575295
aisingapore/sea-lion-7b,mmlu_high_school_us_history,0-shot,accuracy,0.25980392156862747,0.03077855467869326
aisingapore/sea-lion-7b,mmlu_philosophy,0-shot,accuracy,0.3054662379421222,0.026160584450140485
aisingapore/sea-lion-7b,mmlu_jurisprudence,0-shot,accuracy,0.28703703703703703,0.043733130409147614
aisingapore/sea-lion-7b,mmlu_international_law,0-shot,accuracy,0.21487603305785125,0.037494924487096966
aisingapore/sea-lion-7b,mmlu_high_school_european_history,0-shot,accuracy,0.26666666666666666,0.03453131801885415
aisingapore/sea-lion-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.29015544041450775,0.03275264467791515
aisingapore/sea-lion-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.23109243697478993,0.027381406927868973
aisingapore/sea-lion-7b,mmlu_high_school_geography,0-shot,accuracy,0.35353535353535354,0.03406086723547153
aisingapore/sea-lion-7b,mmlu_high_school_psychology,0-shot,accuracy,0.27339449541284405,0.019109299846098278
aisingapore/sea-lion-7b,mmlu_public_relations,0-shot,accuracy,0.2818181818181818,0.043091187099464606
aisingapore/sea-lion-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.31,0.04648231987117316
aisingapore/sea-lion-7b,mmlu_sociology,0-shot,accuracy,0.2885572139303483,0.03203841040213322
aisingapore/sea-lion-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2923076923076923,0.023060438380857747
aisingapore/sea-lion-7b,mmlu_security_studies,0-shot,accuracy,0.44081632653061226,0.03178419114175363
aisingapore/sea-lion-7b,mmlu_professional_psychology,0-shot,accuracy,0.2630718954248366,0.017812676542320657
aisingapore/sea-lion-7b,mmlu_human_sexuality,0-shot,accuracy,0.2824427480916031,0.03948406125768361
aisingapore/sea-lion-7b,mmlu_econometrics,0-shot,accuracy,0.2807017543859649,0.042270544512321984
aisingapore/sea-lion-7b,mmlu_miscellaneous,0-shot,accuracy,0.26053639846743293,0.01569600856380709
aisingapore/sea-lion-7b,mmlu_marketing,0-shot,accuracy,0.2606837606837607,0.028760348956523414
aisingapore/sea-lion-7b,mmlu_management,0-shot,accuracy,0.23300970873786409,0.04185832598928315
aisingapore/sea-lion-7b,mmlu_nutrition,0-shot,accuracy,0.28104575163398693,0.02573885479781871
aisingapore/sea-lion-7b,mmlu_medical_genetics,0-shot,accuracy,0.23,0.04229525846816506
aisingapore/sea-lion-7b,mmlu_human_aging,0-shot,accuracy,0.20179372197309417,0.026936111912802256
aisingapore/sea-lion-7b,mmlu_professional_medicine,0-shot,accuracy,0.19852941176470587,0.024231013370541073
aisingapore/sea-lion-7b,mmlu_college_medicine,0-shot,accuracy,0.3583815028901734,0.03656343653353159
aisingapore/sea-lion-7b,mmlu_business_ethics,0-shot,accuracy,0.31,0.04648231987117316
aisingapore/sea-lion-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.2943396226415094,0.028049186315695248
aisingapore/sea-lion-7b,mmlu_global_facts,0-shot,accuracy,0.29,0.045604802157206845
aisingapore/sea-lion-7b,mmlu_virology,0-shot,accuracy,0.25903614457831325,0.034106466140718564
aisingapore/sea-lion-7b,mmlu_professional_accounting,0-shot,accuracy,0.2695035460992908,0.026469036818590634
aisingapore/sea-lion-7b,mmlu_college_physics,0-shot,accuracy,0.1568627450980392,0.036186648199362466
aisingapore/sea-lion-7b,mmlu_high_school_physics,0-shot,accuracy,0.23178807947019867,0.034454062719870546
aisingapore/sea-lion-7b,mmlu_high_school_biology,0-shot,accuracy,0.23225806451612904,0.02402225613030824
aisingapore/sea-lion-7b,mmlu_college_biology,0-shot,accuracy,0.3263888888888889,0.03921067198982266
aisingapore/sea-lion-7b,mmlu_anatomy,0-shot,accuracy,0.22962962962962963,0.03633384414073465
aisingapore/sea-lion-7b,mmlu_college_chemistry,0-shot,accuracy,0.21,0.040936018074033256
aisingapore/sea-lion-7b,mmlu_computer_security,0-shot,accuracy,0.31,0.046482319871173156
aisingapore/sea-lion-7b,mmlu_college_computer_science,0-shot,accuracy,0.31,0.04648231987117316
aisingapore/sea-lion-7b,mmlu_astronomy,0-shot,accuracy,0.26973684210526316,0.03611780560284898
aisingapore/sea-lion-7b,mmlu_college_mathematics,0-shot,accuracy,0.26,0.0440844002276808
aisingapore/sea-lion-7b,mmlu_conceptual_physics,0-shot,accuracy,0.23829787234042554,0.027851252973889795
aisingapore/sea-lion-7b,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
aisingapore/sea-lion-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.28,0.04512608598542127
aisingapore/sea-lion-7b,mmlu_machine_learning,0-shot,accuracy,0.25892857142857145,0.04157751539865629
aisingapore/sea-lion-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.22660098522167488,0.02945486383529298
aisingapore/sea-lion-7b,mmlu_high_school_statistics,0-shot,accuracy,0.25462962962962965,0.029711275860005357
aisingapore/sea-lion-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.25132275132275134,0.022340482339643895
aisingapore/sea-lion-7b,mmlu_electrical_engineering,0-shot,accuracy,0.31724137931034485,0.03878352372138622
aisingapore/sea-lion-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.22592592592592592,0.02549753263960955
aisingapore/sea-lion-7b,arc_challenge,25-shot,accuracy,0.35921501706484643,0.014020224155839154
aisingapore/sea-lion-7b,arc_challenge,25-shot,acc_norm,0.39590443686006827,0.014291228393536587
aisingapore/sea-lion-7b,hellaswag,10-shot,accuracy,0.5088627763393746,0.0049889974671344945
aisingapore/sea-lion-7b,hellaswag,10-shot,acc_norm,0.6869149571798446,0.004628008661955106
aisingapore/sea-lion-7b,truthfulqa_mc2,0-shot,accuracy,0.35126194842988767,0.013458543099827976
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,bleu_max,22.656533340238912,0.7120387543349416
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,bleu_acc,0.29865361077111385,0.016021570613768542
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,bleu_diff,-9.533528953913965,0.7845698517561239
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge1_max,47.07013430195858,0.8588618247484433
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge1_acc,0.2521419828641371,0.015201522246299969
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge1_diff,-11.523044856861965,0.8632903667062516
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge2_max,29.865710554472003,0.9675510003865858
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge2_acc,0.193390452876377,0.013826240752599066
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rouge2_diff,-14.306988948573508,1.009761759343484
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rougeL_max,44.404211913477454,0.8669444299614717
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rougeL_acc,0.24724602203182375,0.015102404797359652
aisingapore/sea-lion-7b,truthfulqa_gen,0-shot,rougeL_diff,-11.962363986877257,0.869977470816951
aisingapore/sea-lion-7b,truthfulqa_mc1,0-shot,accuracy,0.204406364749082,0.01411717433743262
aisingapore/sea-lion-7b,winogrande,5-shot,accuracy,0.6156274664561957,0.013671567600836194
jisukim8873/falcon-7B-case-6,arc:challenge,25-shot,accuracy,0.4274744027303754,0.014456862944650654
jisukim8873/falcon-7B-case-6,arc:challenge,25-shot,acc_norm,0.46501706484641636,0.014575583922019665
jisukim8873/falcon-7B-case-6,hellaswag,10-shot,accuracy,0.5976897032463653,0.0048936170149753
jisukim8873/falcon-7B-case-6,hellaswag,10-shot,acc_norm,0.7849034056960765,0.004100495978108428
jisukim8873/falcon-7B-case-6,hendrycksTest-abstract_algebra,5-shot,accuracy,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-6,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-6,hendrycksTest-anatomy,5-shot,accuracy,0.2962962962962963,0.03944624162501116
jisukim8873/falcon-7B-case-6,hendrycksTest-anatomy,5-shot,acc_norm,0.2962962962962963,0.03944624162501116
jisukim8873/falcon-7B-case-6,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.037385206761196686
jisukim8873/falcon-7B-case-6,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.037385206761196686
jisukim8873/falcon-7B-case-6,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.042295258468165065
jisukim8873/falcon-7B-case-6,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.042295258468165065
jisukim8873/falcon-7B-case-6,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.028254200344438662
jisukim8873/falcon-7B-case-6,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.028254200344438662
jisukim8873/falcon-7B-case-6,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
jisukim8873/falcon-7B-case-6,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.03685651095897532
jisukim8873/falcon-7B-case-6,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.038612291966536955
jisukim8873/falcon-7B-case-6,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.038612291966536955
jisukim8873/falcon-7B-case-6,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-6,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
jisukim8873/falcon-7B-case-6,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-6,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-6,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.03368762932259431
jisukim8873/falcon-7B-case-6,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.03368762932259431
jisukim8873/falcon-7B-case-6,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.040925639582376536
jisukim8873/falcon-7B-case-6,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.040925639582376536
jisukim8873/falcon-7B-case-6,hendrycksTest-computer_security,5-shot,accuracy,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-6,hendrycksTest-computer_security,5-shot,acc_norm,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-6,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.03036358219723817
jisukim8873/falcon-7B-case-6,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.03036358219723817
jisukim8873/falcon-7B-case-6,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.04266339443159394
jisukim8873/falcon-7B-case-6,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.04266339443159394
jisukim8873/falcon-7B-case-6,hendrycksTest-electrical_engineering,5-shot,accuracy,0.27586206896551724,0.037245636197746325
jisukim8873/falcon-7B-case-6,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.27586206896551724,0.037245636197746325
jisukim8873/falcon-7B-case-6,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.02256989707491841
jisukim8873/falcon-7B-case-6,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.02256989707491841
jisukim8873/falcon-7B-case-6,hendrycksTest-formal_logic,5-shot,accuracy,0.1349206349206349,0.030557101589417515
jisukim8873/falcon-7B-case-6,hendrycksTest-formal_logic,5-shot,acc_norm,0.1349206349206349,0.030557101589417515
jisukim8873/falcon-7B-case-6,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-6,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.04725815626252605
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_biology,5-shot,accuracy,0.33225806451612905,0.02679556084812279
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_biology,5-shot,acc_norm,0.33225806451612905,0.02679556084812279
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3497536945812808,0.03355400904969566
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3497536945812808,0.03355400904969566
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.31,0.04648231987117316
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.31,0.04648231987117316
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3151515151515151,0.0362773057502241
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3151515151515151,0.0362773057502241
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_geography,5-shot,accuracy,0.30303030303030304,0.03274287914026869
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_geography,5-shot,acc_norm,0.30303030303030304,0.03274287914026869
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.25906735751295334,0.03161877917935411
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.25906735751295334,0.03161877917935411
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.24615384615384617,0.021840866990423095
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.24615384615384617,0.021840866990423095
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24444444444444444,0.026202766534652155
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24444444444444444,0.026202766534652155
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.24369747899159663,0.027886828078380572
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.24369747899159663,0.027886828078380572
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_physics,5-shot,accuracy,0.2781456953642384,0.03658603262763743
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2781456953642384,0.03658603262763743
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_psychology,5-shot,accuracy,0.28990825688073396,0.019453066609201597
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.28990825688073396,0.019453066609201597
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_statistics,5-shot,accuracy,0.19444444444444445,0.026991454502036744
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.19444444444444445,0.026991454502036744
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_us_history,5-shot,accuracy,0.27450980392156865,0.03132179803083289
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.27450980392156865,0.03132179803083289
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_world_history,5-shot,accuracy,0.31645569620253167,0.03027497488021897
jisukim8873/falcon-7B-case-6,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.31645569620253167,0.03027497488021897
jisukim8873/falcon-7B-case-6,hendrycksTest-human_aging,5-shot,accuracy,0.37668161434977576,0.03252113489929188
jisukim8873/falcon-7B-case-6,hendrycksTest-human_aging,5-shot,acc_norm,0.37668161434977576,0.03252113489929188
jisukim8873/falcon-7B-case-6,hendrycksTest-human_sexuality,5-shot,accuracy,0.26717557251908397,0.03880848301082396
jisukim8873/falcon-7B-case-6,hendrycksTest-human_sexuality,5-shot,acc_norm,0.26717557251908397,0.03880848301082396
jisukim8873/falcon-7B-case-6,hendrycksTest-international_law,5-shot,accuracy,0.4132231404958678,0.04495087843548408
jisukim8873/falcon-7B-case-6,hendrycksTest-international_law,5-shot,acc_norm,0.4132231404958678,0.04495087843548408
jisukim8873/falcon-7B-case-6,hendrycksTest-jurisprudence,5-shot,accuracy,0.3148148148148148,0.04489931073591312
jisukim8873/falcon-7B-case-6,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3148148148148148,0.04489931073591312
jisukim8873/falcon-7B-case-6,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.035590395316173425
jisukim8873/falcon-7B-case-6,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.035590395316173425
jisukim8873/falcon-7B-case-6,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.042878587513404565
jisukim8873/falcon-7B-case-6,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.042878587513404565
jisukim8873/falcon-7B-case-6,hendrycksTest-management,5-shot,accuracy,0.32038834951456313,0.04620284082280039
jisukim8873/falcon-7B-case-6,hendrycksTest-management,5-shot,acc_norm,0.32038834951456313,0.04620284082280039
jisukim8873/falcon-7B-case-6,hendrycksTest-marketing,5-shot,accuracy,0.3076923076923077,0.03023638994217307
jisukim8873/falcon-7B-case-6,hendrycksTest-marketing,5-shot,acc_norm,0.3076923076923077,0.03023638994217307
jisukim8873/falcon-7B-case-6,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
jisukim8873/falcon-7B-case-6,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.04648231987117316
jisukim8873/falcon-7B-case-6,hendrycksTest-miscellaneous,5-shot,accuracy,0.3537675606641124,0.017098184708161903
jisukim8873/falcon-7B-case-6,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3537675606641124,0.017098184708161903
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_disputes,5-shot,accuracy,0.3236994219653179,0.025190181327608422
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3236994219653179,0.025190181327608422
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24692737430167597,0.014422292204808835
jisukim8873/falcon-7B-case-6,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24692737430167597,0.014422292204808835
jisukim8873/falcon-7B-case-6,hendrycksTest-nutrition,5-shot,accuracy,0.3202614379084967,0.026716118380156844
jisukim8873/falcon-7B-case-6,hendrycksTest-nutrition,5-shot,acc_norm,0.3202614379084967,0.026716118380156844
jisukim8873/falcon-7B-case-6,hendrycksTest-philosophy,5-shot,accuracy,0.3183279742765273,0.026457225067811025
jisukim8873/falcon-7B-case-6,hendrycksTest-philosophy,5-shot,acc_norm,0.3183279742765273,0.026457225067811025
jisukim8873/falcon-7B-case-6,hendrycksTest-prehistory,5-shot,accuracy,0.2777777777777778,0.024922001168886335
jisukim8873/falcon-7B-case-6,hendrycksTest-prehistory,5-shot,acc_norm,0.2777777777777778,0.024922001168886335
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_accounting,5-shot,accuracy,0.24113475177304963,0.02551873104953776
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24113475177304963,0.02551873104953776
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_law,5-shot,accuracy,0.2627118644067797,0.01124054551499567
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_law,5-shot,acc_norm,0.2627118644067797,0.01124054551499567
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_medicine,5-shot,accuracy,0.21323529411764705,0.024880971512294292
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_medicine,5-shot,acc_norm,0.21323529411764705,0.024880971512294292
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_psychology,5-shot,accuracy,0.2630718954248366,0.017812676542320657
jisukim8873/falcon-7B-case-6,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2630718954248366,0.017812676542320657
jisukim8873/falcon-7B-case-6,hendrycksTest-public_relations,5-shot,accuracy,0.24545454545454545,0.04122066502878284
jisukim8873/falcon-7B-case-6,hendrycksTest-public_relations,5-shot,acc_norm,0.24545454545454545,0.04122066502878284
jisukim8873/falcon-7B-case-6,hendrycksTest-security_studies,5-shot,accuracy,0.24489795918367346,0.02752963744017493
jisukim8873/falcon-7B-case-6,hendrycksTest-security_studies,5-shot,acc_norm,0.24489795918367346,0.02752963744017493
jisukim8873/falcon-7B-case-6,hendrycksTest-sociology,5-shot,accuracy,0.3034825870646766,0.032510068164586174
jisukim8873/falcon-7B-case-6,hendrycksTest-sociology,5-shot,acc_norm,0.3034825870646766,0.032510068164586174
jisukim8873/falcon-7B-case-6,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.42,0.049604496374885836
jisukim8873/falcon-7B-case-6,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.42,0.049604496374885836
jisukim8873/falcon-7B-case-6,hendrycksTest-virology,5-shot,accuracy,0.3253012048192771,0.03647168523683227
jisukim8873/falcon-7B-case-6,hendrycksTest-virology,5-shot,acc_norm,0.3253012048192771,0.03647168523683227
jisukim8873/falcon-7B-case-6,hendrycksTest-world_religions,5-shot,accuracy,0.3391812865497076,0.03631053496488905
jisukim8873/falcon-7B-case-6,hendrycksTest-world_religions,5-shot,acc_norm,0.3391812865497076,0.03631053496488905
jisukim8873/falcon-7B-case-6,truthfulqa:mc,0-shot,mc1,0.25091799265605874,0.015176985027707687
jisukim8873/falcon-7B-case-6,truthfulqa:mc,0-shot,mc2,0.364571668218642,0.014117416041879967
jisukim8873/falcon-7B-case-6,winogrande,5-shot,accuracy,0.7008681925808997,0.012868639066091541
jisukim8873/falcon-7B-case-6,gsm8k,5-shot,accuracy,0.07657316148597422,0.00732456488145157
jisukim8873/falcon-7B-case-6,minerva_math_precalc,5-shot,accuracy,0.0018315018315018315,0.0018315018315018339
jisukim8873/falcon-7B-case-6,minerva_math_prealgebra,5-shot,accuracy,0.027554535017221583,0.005549700480393218
jisukim8873/falcon-7B-case-6,minerva_math_num_theory,5-shot,accuracy,0.011111111111111112,0.0045150037076946556
jisukim8873/falcon-7B-case-6,minerva_math_intermediate_algebra,5-shot,accuracy,0.007751937984496124,0.0029201960269643963
jisukim8873/falcon-7B-case-6,minerva_math_geometry,5-shot,accuracy,0.0041753653444676405,0.0029493392170756557
jisukim8873/falcon-7B-case-6,minerva_math_counting_and_prob,5-shot,accuracy,0.016877637130801686,0.0059228268948526815
jisukim8873/falcon-7B-case-6,minerva_math_algebra,5-shot,accuracy,0.010951979780960405,0.0030221266536702993
jisukim8873/falcon-7B-case-6,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-6,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-6,arithmetic_3da,5-shot,accuracy,0.103,0.006798426972811525
jisukim8873/falcon-7B-case-6,arithmetic_3ds,5-shot,accuracy,0.069,0.005668824197652668
jisukim8873/falcon-7B-case-6,arithmetic_4da,5-shot,accuracy,0.002,0.000999249343069489
jisukim8873/falcon-7B-case-6,arithmetic_2ds,5-shot,accuracy,0.2985,0.010234805842091585
jisukim8873/falcon-7B-case-6,arithmetic_5ds,5-shot,accuracy,0.0015,0.000865592066052146
jisukim8873/falcon-7B-case-6,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-6,arithmetic_1dc,5-shot,accuracy,0.081,0.006102304405675851
jisukim8873/falcon-7B-case-6,arithmetic_4ds,5-shot,accuracy,0.0005,0.0005000000000000099
jisukim8873/falcon-7B-case-6,arithmetic_2dm,5-shot,accuracy,0.148,0.007942262887231029
jisukim8873/falcon-7B-case-6,arithmetic_2da,5-shot,accuracy,0.468,0.011160209457602892
jisukim8873/falcon-7B-case-6,gsm8k_cot,5-shot,accuracy,0.10083396512509477,0.00829403119212661
jisukim8873/falcon-7B-case-6,anli_r2,0-shot,brier_score,1.0215867022310285,
jisukim8873/falcon-7B-case-6,anli_r3,0-shot,brier_score,0.9227540524470021,
jisukim8873/falcon-7B-case-6,anli_r1,0-shot,brier_score,1.0587549732497896,
jisukim8873/falcon-7B-case-6,xnli_eu,0-shot,brier_score,1.0846362802483993,
jisukim8873/falcon-7B-case-6,xnli_vi,0-shot,brier_score,0.9955744880152678,
jisukim8873/falcon-7B-case-6,xnli_ru,0-shot,brier_score,0.8139283536030013,
jisukim8873/falcon-7B-case-6,xnli_zh,0-shot,brier_score,1.0232273505528497,
jisukim8873/falcon-7B-case-6,xnli_tr,0-shot,brier_score,1.0068194344208217,
jisukim8873/falcon-7B-case-6,xnli_fr,0-shot,brier_score,0.7378926188868539,
jisukim8873/falcon-7B-case-6,xnli_en,0-shot,brier_score,0.6473635963303874,
jisukim8873/falcon-7B-case-6,xnli_ur,0-shot,brier_score,1.3048334974182783,
jisukim8873/falcon-7B-case-6,xnli_ar,0-shot,brier_score,1.2965712362618014,
jisukim8873/falcon-7B-case-6,xnli_de,0-shot,brier_score,0.8442829185961966,
jisukim8873/falcon-7B-case-6,xnli_hi,0-shot,brier_score,1.068890027784789,
jisukim8873/falcon-7B-case-6,xnli_es,0-shot,brier_score,0.8113170196363131,
jisukim8873/falcon-7B-case-6,xnli_bg,0-shot,brier_score,0.8936463916487021,
jisukim8873/falcon-7B-case-6,xnli_sw,0-shot,brier_score,1.0700519786794918,
jisukim8873/falcon-7B-case-6,xnli_el,0-shot,brier_score,0.9588694161285166,
jisukim8873/falcon-7B-case-6,xnli_th,0-shot,brier_score,0.9835081112012901,
jisukim8873/falcon-7B-case-6,logiqa2,0-shot,brier_score,1.0489832657150588,
jisukim8873/falcon-7B-case-6,mathqa,5-shot,brier_score,0.9350069504644906,
jisukim8873/falcon-7B-case-6,lambada_standard,0-shot,perplexity,4.177882741614251,0.09059481447386622
jisukim8873/falcon-7B-case-6,lambada_standard,0-shot,accuracy,0.6652435474480884,0.006574562930156437
jisukim8873/falcon-7B-case-6,lambada_openai,0-shot,perplexity,3.327303471293643,0.06972922854362806
jisukim8873/falcon-7B-case-6,lambada_openai,0-shot,accuracy,0.7349117019212109,0.006149289402158157
jb723/llama2-ko-7B-model,arc:challenge,25-shot,accuracy,0.5349829351535836,0.014575583922019674
jb723/llama2-ko-7B-model,arc:challenge,25-shot,acc_norm,0.5631399317406144,0.014494421584256532
jb723/llama2-ko-7B-model,hellaswag,10-shot,accuracy,0.6069508066122287,0.004874293964843518
jb723/llama2-ko-7B-model,hellaswag,10-shot,acc_norm,0.7893845847440749,0.004069123905324906
jb723/llama2-ko-7B-model,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.04648231987117316
jb723/llama2-ko-7B-model,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.04648231987117316
jb723/llama2-ko-7B-model,hendrycksTest-anatomy,5-shot,accuracy,0.45925925925925926,0.04304979692464242
jb723/llama2-ko-7B-model,hendrycksTest-anatomy,5-shot,acc_norm,0.45925925925925926,0.04304979692464242
jb723/llama2-ko-7B-model,hendrycksTest-astronomy,5-shot,accuracy,0.4605263157894737,0.04056242252249034
jb723/llama2-ko-7B-model,hendrycksTest-astronomy,5-shot,acc_norm,0.4605263157894737,0.04056242252249034
jb723/llama2-ko-7B-model,hendrycksTest-business_ethics,5-shot,accuracy,0.45,0.05
jb723/llama2-ko-7B-model,hendrycksTest-business_ethics,5-shot,acc_norm,0.45,0.05
jb723/llama2-ko-7B-model,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.45660377358490567,0.030656748696739438
jb723/llama2-ko-7B-model,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.45660377358490567,0.030656748696739438
jb723/llama2-ko-7B-model,hendrycksTest-college_biology,5-shot,accuracy,0.4375,0.04148415739394154
jb723/llama2-ko-7B-model,hendrycksTest-college_biology,5-shot,acc_norm,0.4375,0.04148415739394154
jb723/llama2-ko-7B-model,hendrycksTest-college_chemistry,5-shot,accuracy,0.36,0.04824181513244218
jb723/llama2-ko-7B-model,hendrycksTest-college_chemistry,5-shot,acc_norm,0.36,0.04824181513244218
jb723/llama2-ko-7B-model,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.04725815626252604
jb723/llama2-ko-7B-model,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.04725815626252604
jb723/llama2-ko-7B-model,hendrycksTest-college_mathematics,5-shot,accuracy,0.37,0.04852365870939098
jb723/llama2-ko-7B-model,hendrycksTest-college_mathematics,5-shot,acc_norm,0.37,0.04852365870939098
jb723/llama2-ko-7B-model,hendrycksTest-college_medicine,5-shot,accuracy,0.4046242774566474,0.03742461193887248
jb723/llama2-ko-7B-model,hendrycksTest-college_medicine,5-shot,acc_norm,0.4046242774566474,0.03742461193887248
jb723/llama2-ko-7B-model,hendrycksTest-college_physics,5-shot,accuracy,0.14705882352941177,0.035240689515674495
jb723/llama2-ko-7B-model,hendrycksTest-college_physics,5-shot,acc_norm,0.14705882352941177,0.035240689515674495
jb723/llama2-ko-7B-model,hendrycksTest-computer_security,5-shot,accuracy,0.52,0.05021167315686779
jb723/llama2-ko-7B-model,hendrycksTest-computer_security,5-shot,acc_norm,0.52,0.05021167315686779
jb723/llama2-ko-7B-model,hendrycksTest-conceptual_physics,5-shot,accuracy,0.4127659574468085,0.03218471141400351
jb723/llama2-ko-7B-model,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.4127659574468085,0.03218471141400351
jb723/llama2-ko-7B-model,hendrycksTest-econometrics,5-shot,accuracy,0.32456140350877194,0.04404556157374767
jb723/llama2-ko-7B-model,hendrycksTest-econometrics,5-shot,acc_norm,0.32456140350877194,0.04404556157374767
jb723/llama2-ko-7B-model,hendrycksTest-electrical_engineering,5-shot,accuracy,0.4482758620689655,0.04144311810878151
jb723/llama2-ko-7B-model,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.4482758620689655,0.04144311810878151
jb723/llama2-ko-7B-model,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.31746031746031744,0.023973861998992072
jb723/llama2-ko-7B-model,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.31746031746031744,0.023973861998992072
jb723/llama2-ko-7B-model,hendrycksTest-formal_logic,5-shot,accuracy,0.23809523809523808,0.03809523809523812
jb723/llama2-ko-7B-model,hendrycksTest-formal_logic,5-shot,acc_norm,0.23809523809523808,0.03809523809523812
jb723/llama2-ko-7B-model,hendrycksTest-global_facts,5-shot,accuracy,0.39,0.04902071300001974
jb723/llama2-ko-7B-model,hendrycksTest-global_facts,5-shot,acc_norm,0.39,0.04902071300001974
jb723/llama2-ko-7B-model,hendrycksTest-high_school_biology,5-shot,accuracy,0.4935483870967742,0.02844163823354051
jb723/llama2-ko-7B-model,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4935483870967742,0.02844163823354051
jb723/llama2-ko-7B-model,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.33004926108374383,0.033085304262282574
jb723/llama2-ko-7B-model,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.33004926108374383,0.033085304262282574
jb723/llama2-ko-7B-model,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.41,0.04943110704237102
jb723/llama2-ko-7B-model,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.41,0.04943110704237102
jb723/llama2-ko-7B-model,hendrycksTest-high_school_european_history,5-shot,accuracy,0.5878787878787879,0.03843566993588717
jb723/llama2-ko-7B-model,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.5878787878787879,0.03843566993588717
jb723/llama2-ko-7B-model,hendrycksTest-high_school_geography,5-shot,accuracy,0.5757575757575758,0.035212249088415845
jb723/llama2-ko-7B-model,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5757575757575758,0.035212249088415845
jb723/llama2-ko-7B-model,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.689119170984456,0.033403619062765864
jb723/llama2-ko-7B-model,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.689119170984456,0.033403619062765864
jb723/llama2-ko-7B-model,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4461538461538462,0.02520357177302833
jb723/llama2-ko-7B-model,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4461538461538462,0.02520357177302833
jb723/llama2-ko-7B-model,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.026719240783712173
jb723/llama2-ko-7B-model,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25925925925925924,0.026719240783712173
jb723/llama2-ko-7B-model,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.40756302521008403,0.03191863374478466
jb723/llama2-ko-7B-model,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.40756302521008403,0.03191863374478466
jb723/llama2-ko-7B-model,hendrycksTest-high_school_physics,5-shot,accuracy,0.31788079470198677,0.03802039760107903
jb723/llama2-ko-7B-model,hendrycksTest-high_school_physics,5-shot,acc_norm,0.31788079470198677,0.03802039760107903
jb723/llama2-ko-7B-model,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6146788990825688,0.02086585085279412
jb723/llama2-ko-7B-model,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6146788990825688,0.02086585085279412
jb723/llama2-ko-7B-model,hendrycksTest-high_school_statistics,5-shot,accuracy,0.28703703703703703,0.030851992993257017
jb723/llama2-ko-7B-model,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.28703703703703703,0.030851992993257017
jb723/llama2-ko-7B-model,hendrycksTest-high_school_us_history,5-shot,accuracy,0.5833333333333334,0.03460228327239171
jb723/llama2-ko-7B-model,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.5833333333333334,0.03460228327239171
jb723/llama2-ko-7B-model,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6666666666666666,0.030685820596610795
jb723/llama2-ko-7B-model,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6666666666666666,0.030685820596610795
jb723/llama2-ko-7B-model,hendrycksTest-human_aging,5-shot,accuracy,0.5246636771300448,0.03351695167652628
jb723/llama2-ko-7B-model,hendrycksTest-human_aging,5-shot,acc_norm,0.5246636771300448,0.03351695167652628
jb723/llama2-ko-7B-model,hendrycksTest-human_sexuality,5-shot,accuracy,0.5343511450381679,0.04374928560599738
jb723/llama2-ko-7B-model,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5343511450381679,0.04374928560599738
jb723/llama2-ko-7B-model,hendrycksTest-international_law,5-shot,accuracy,0.6446280991735537,0.0436923632657398
jb723/llama2-ko-7B-model,hendrycksTest-international_law,5-shot,acc_norm,0.6446280991735537,0.0436923632657398
jb723/llama2-ko-7B-model,hendrycksTest-jurisprudence,5-shot,accuracy,0.5,0.04833682445228318
jb723/llama2-ko-7B-model,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5,0.04833682445228318
jb723/llama2-ko-7B-model,hendrycksTest-logical_fallacies,5-shot,accuracy,0.44785276073619634,0.03906947479456602
jb723/llama2-ko-7B-model,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.44785276073619634,0.03906947479456602
jb723/llama2-ko-7B-model,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.043270409325787296
jb723/llama2-ko-7B-model,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.043270409325787296
jb723/llama2-ko-7B-model,hendrycksTest-management,5-shot,accuracy,0.5631067961165048,0.04911147107365777
jb723/llama2-ko-7B-model,hendrycksTest-management,5-shot,acc_norm,0.5631067961165048,0.04911147107365777
jb723/llama2-ko-7B-model,hendrycksTest-marketing,5-shot,accuracy,0.6752136752136753,0.030679022765498828
jb723/llama2-ko-7B-model,hendrycksTest-marketing,5-shot,acc_norm,0.6752136752136753,0.030679022765498828
jb723/llama2-ko-7B-model,hendrycksTest-medical_genetics,5-shot,accuracy,0.53,0.05016135580465919
jb723/llama2-ko-7B-model,hendrycksTest-medical_genetics,5-shot,acc_norm,0.53,0.05016135580465919
jb723/llama2-ko-7B-model,hendrycksTest-miscellaneous,5-shot,accuracy,0.6577266922094508,0.016967031766413624
jb723/llama2-ko-7B-model,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6577266922094508,0.016967031766413624
jb723/llama2-ko-7B-model,hendrycksTest-moral_disputes,5-shot,accuracy,0.5202312138728323,0.026897049996382868
jb723/llama2-ko-7B-model,hendrycksTest-moral_disputes,5-shot,acc_norm,0.5202312138728323,0.026897049996382868
jb723/llama2-ko-7B-model,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23798882681564246,0.014242630070574915
jb723/llama2-ko-7B-model,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23798882681564246,0.014242630070574915
jb723/llama2-ko-7B-model,hendrycksTest-nutrition,5-shot,accuracy,0.477124183006536,0.028599936776089782
jb723/llama2-ko-7B-model,hendrycksTest-nutrition,5-shot,acc_norm,0.477124183006536,0.028599936776089782
jb723/llama2-ko-7B-model,hendrycksTest-philosophy,5-shot,accuracy,0.5562700964630225,0.028217683556652315
jb723/llama2-ko-7B-model,hendrycksTest-philosophy,5-shot,acc_norm,0.5562700964630225,0.028217683556652315
jb723/llama2-ko-7B-model,hendrycksTest-prehistory,5-shot,accuracy,0.5216049382716049,0.027794760105008736
jb723/llama2-ko-7B-model,hendrycksTest-prehistory,5-shot,acc_norm,0.5216049382716049,0.027794760105008736
jb723/llama2-ko-7B-model,hendrycksTest-professional_accounting,5-shot,accuracy,0.3475177304964539,0.02840662780959095
jb723/llama2-ko-7B-model,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3475177304964539,0.02840662780959095
jb723/llama2-ko-7B-model,hendrycksTest-professional_law,5-shot,accuracy,0.35528031290743156,0.012223623364044037
jb723/llama2-ko-7B-model,hendrycksTest-professional_law,5-shot,acc_norm,0.35528031290743156,0.012223623364044037
jb723/llama2-ko-7B-model,hendrycksTest-professional_medicine,5-shot,accuracy,0.4411764705882353,0.03016191193076711
jb723/llama2-ko-7B-model,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4411764705882353,0.03016191193076711
jb723/llama2-ko-7B-model,hendrycksTest-professional_psychology,5-shot,accuracy,0.4526143790849673,0.020136790918492523
jb723/llama2-ko-7B-model,hendrycksTest-professional_psychology,5-shot,acc_norm,0.4526143790849673,0.020136790918492523
jb723/llama2-ko-7B-model,hendrycksTest-public_relations,5-shot,accuracy,0.5272727272727272,0.04782001791380061
jb723/llama2-ko-7B-model,hendrycksTest-public_relations,5-shot,acc_norm,0.5272727272727272,0.04782001791380061
jb723/llama2-ko-7B-model,hendrycksTest-security_studies,5-shot,accuracy,0.4326530612244898,0.031717528240626645
jb723/llama2-ko-7B-model,hendrycksTest-security_studies,5-shot,acc_norm,0.4326530612244898,0.031717528240626645
jb723/llama2-ko-7B-model,hendrycksTest-sociology,5-shot,accuracy,0.572139303482587,0.03498541988407795
jb723/llama2-ko-7B-model,hendrycksTest-sociology,5-shot,acc_norm,0.572139303482587,0.03498541988407795
jb723/llama2-ko-7B-model,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.63,0.04852365870939099
jb723/llama2-ko-7B-model,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.63,0.04852365870939099
jb723/llama2-ko-7B-model,hendrycksTest-virology,5-shot,accuracy,0.42168674698795183,0.03844453181770917
jb723/llama2-ko-7B-model,hendrycksTest-virology,5-shot,acc_norm,0.42168674698795183,0.03844453181770917
jb723/llama2-ko-7B-model,hendrycksTest-world_religions,5-shot,accuracy,0.6842105263157895,0.03565079670708311
jb723/llama2-ko-7B-model,hendrycksTest-world_religions,5-shot,acc_norm,0.6842105263157895,0.03565079670708311
jb723/llama2-ko-7B-model,truthfulqa:mc,0-shot,mc1,0.28151774785801714,0.015744027248256045
jb723/llama2-ko-7B-model,truthfulqa:mc,0-shot,mc2,0.4097811489004275,0.015552291335837638
jb723/llama2-ko-7B-model,drop,3-shot,accuracy,0.23427013422818793,0.004337464243138509
jb723/llama2-ko-7B-model,drop,3-shot,f1,0.3152516778523505,0.004353725712557671
jb723/llama2-ko-7B-model,gsm8k,5-shot,accuracy,0.0803639120545868,0.007488258573239077
jb723/llama2-ko-7B-model,winogrande,5-shot,accuracy,0.7000789265982637,0.012878347526636072
jb723/llama2-ko-7B-model,mmlu_world_religions,0-shot,accuracy,0.672514619883041,0.035993357714560276
jb723/llama2-ko-7B-model,mmlu_formal_logic,0-shot,accuracy,0.24603174603174602,0.038522733649243156
jb723/llama2-ko-7B-model,mmlu_prehistory,0-shot,accuracy,0.5154320987654321,0.027807490044276184
jb723/llama2-ko-7B-model,mmlu_moral_scenarios,0-shot,accuracy,0.23798882681564246,0.014242630070574885
jb723/llama2-ko-7B-model,mmlu_high_school_world_history,0-shot,accuracy,0.6751054852320675,0.030486039389105307
jb723/llama2-ko-7B-model,mmlu_moral_disputes,0-shot,accuracy,0.5260115606936416,0.02688264343402289
jb723/llama2-ko-7B-model,mmlu_professional_law,0-shot,accuracy,0.3546284224250326,0.012218576439090174
jb723/llama2-ko-7B-model,mmlu_logical_fallacies,0-shot,accuracy,0.4785276073619632,0.0392474687675113
jb723/llama2-ko-7B-model,mmlu_high_school_us_history,0-shot,accuracy,0.5637254901960784,0.03480693138457038
jb723/llama2-ko-7B-model,mmlu_philosophy,0-shot,accuracy,0.5594855305466238,0.02819640057419743
jb723/llama2-ko-7B-model,mmlu_jurisprudence,0-shot,accuracy,0.49074074074074076,0.04832853553437055
jb723/llama2-ko-7B-model,mmlu_international_law,0-shot,accuracy,0.6033057851239669,0.04465869780531009
jb723/llama2-ko-7B-model,mmlu_high_school_european_history,0-shot,accuracy,0.5757575757575758,0.038592681420702636
jb723/llama2-ko-7B-model,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6787564766839378,0.033699508685490674
jb723/llama2-ko-7B-model,mmlu_high_school_microeconomics,0-shot,accuracy,0.41596638655462187,0.03201650100739615
jb723/llama2-ko-7B-model,mmlu_high_school_geography,0-shot,accuracy,0.5909090909090909,0.03502975799413007
jb723/llama2-ko-7B-model,mmlu_high_school_psychology,0-shot,accuracy,0.6201834862385321,0.020808825617866244
jb723/llama2-ko-7B-model,mmlu_public_relations,0-shot,accuracy,0.5545454545454546,0.047605488214603246
jb723/llama2-ko-7B-model,mmlu_us_foreign_policy,0-shot,accuracy,0.64,0.04824181513244218
jb723/llama2-ko-7B-model,mmlu_sociology,0-shot,accuracy,0.5870646766169154,0.03481520803367348
jb723/llama2-ko-7B-model,mmlu_high_school_macroeconomics,0-shot,accuracy,0.44358974358974357,0.025189149894764194
jb723/llama2-ko-7B-model,mmlu_security_studies,0-shot,accuracy,0.4489795918367347,0.03184213866687579
jb723/llama2-ko-7B-model,mmlu_professional_psychology,0-shot,accuracy,0.4477124183006536,0.020116925347422425
jb723/llama2-ko-7B-model,mmlu_human_sexuality,0-shot,accuracy,0.549618320610687,0.04363643698524779
jb723/llama2-ko-7B-model,mmlu_econometrics,0-shot,accuracy,0.32456140350877194,0.04404556157374767
jb723/llama2-ko-7B-model,mmlu_miscellaneous,0-shot,accuracy,0.6360153256704981,0.017205684809032232
jb723/llama2-ko-7B-model,mmlu_marketing,0-shot,accuracy,0.6623931623931624,0.030980296992618558
jb723/llama2-ko-7B-model,mmlu_management,0-shot,accuracy,0.5825242718446602,0.048828405482122375
jb723/llama2-ko-7B-model,mmlu_nutrition,0-shot,accuracy,0.4803921568627451,0.028607893699576063
jb723/llama2-ko-7B-model,mmlu_medical_genetics,0-shot,accuracy,0.56,0.049888765156985884
jb723/llama2-ko-7B-model,mmlu_human_aging,0-shot,accuracy,0.5336322869955157,0.03348180017060306
jb723/llama2-ko-7B-model,mmlu_professional_medicine,0-shot,accuracy,0.4338235294117647,0.030105636570016636
jb723/llama2-ko-7B-model,mmlu_college_medicine,0-shot,accuracy,0.43352601156069365,0.03778621079092056
jb723/llama2-ko-7B-model,mmlu_business_ethics,0-shot,accuracy,0.48,0.050211673156867795
jb723/llama2-ko-7B-model,mmlu_clinical_knowledge,0-shot,accuracy,0.4679245283018868,0.030709486992556552
jb723/llama2-ko-7B-model,mmlu_global_facts,0-shot,accuracy,0.39,0.04902071300001974
jb723/llama2-ko-7B-model,mmlu_virology,0-shot,accuracy,0.4397590361445783,0.03864139923699122
jb723/llama2-ko-7B-model,mmlu_professional_accounting,0-shot,accuracy,0.3617021276595745,0.028663820147199492
jb723/llama2-ko-7B-model,mmlu_college_physics,0-shot,accuracy,0.14705882352941177,0.035240689515674495
jb723/llama2-ko-7B-model,mmlu_high_school_physics,0-shot,accuracy,0.31788079470198677,0.038020397601079024
jb723/llama2-ko-7B-model,mmlu_high_school_biology,0-shot,accuracy,0.5,0.028444006199428714
jb723/llama2-ko-7B-model,mmlu_college_biology,0-shot,accuracy,0.4652777777777778,0.04171115858181618
jb723/llama2-ko-7B-model,mmlu_anatomy,0-shot,accuracy,0.4444444444444444,0.04292596718256981
jb723/llama2-ko-7B-model,mmlu_college_chemistry,0-shot,accuracy,0.37,0.048523658709391
jb723/llama2-ko-7B-model,mmlu_computer_security,0-shot,accuracy,0.51,0.05024183937956911
jb723/llama2-ko-7B-model,mmlu_college_computer_science,0-shot,accuracy,0.35,0.047937248544110196
jb723/llama2-ko-7B-model,mmlu_astronomy,0-shot,accuracy,0.4407894736842105,0.04040311062490437
jb723/llama2-ko-7B-model,mmlu_college_mathematics,0-shot,accuracy,0.33,0.04725815626252604
jb723/llama2-ko-7B-model,mmlu_conceptual_physics,0-shot,accuracy,0.4,0.03202563076101735
jb723/llama2-ko-7B-model,mmlu_abstract_algebra,0-shot,accuracy,0.3,0.046056618647183814
jb723/llama2-ko-7B-model,mmlu_high_school_computer_science,0-shot,accuracy,0.41,0.04943110704237102
jb723/llama2-ko-7B-model,mmlu_machine_learning,0-shot,accuracy,0.32142857142857145,0.04432804055291518
jb723/llama2-ko-7B-model,mmlu_high_school_chemistry,0-shot,accuracy,0.32019704433497537,0.032826493853041504
jb723/llama2-ko-7B-model,mmlu_high_school_statistics,0-shot,accuracy,0.2824074074074074,0.03070137211151092
jb723/llama2-ko-7B-model,mmlu_elementary_mathematics,0-shot,accuracy,0.31746031746031744,0.023973861998992072
jb723/llama2-ko-7B-model,mmlu_electrical_engineering,0-shot,accuracy,0.47586206896551725,0.0416180850350153
jb723/llama2-ko-7B-model,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.026466117538959912
jb723/llama2-ko-7B-model,arc_challenge,25-shot,accuracy,0.5136518771331058,0.014605943429860942
jb723/llama2-ko-7B-model,arc_challenge,25-shot,acc_norm,0.53839590443686,0.014568245550296365
jb723/llama2-ko-7B-model,truthfulqa_mc2,0-shot,accuracy,0.4095609976995192,0.015544249293655412
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,bleu_max,20.89104935353322,0.690580917763846
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,bleu_acc,0.34394124847001223,0.016629087514276816
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,bleu_diff,-4.982714805919928,0.6525563274154834
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge1_max,46.0179025495015,0.8157872400574628
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge1_acc,0.34149326805385555,0.016600688619950833
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge1_diff,-7.817434391713708,0.7537330659697064
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge2_max,30.284883181541517,0.885344085472441
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge2_acc,0.2913096695226438,0.015905987048184824
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rouge2_diff,-8.624090028516706,0.8953473056334376
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rougeL_max,42.39510902130912,0.8131112049425046
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rougeL_acc,0.31334149326805383,0.016238065069059573
jb723/llama2-ko-7B-model,truthfulqa_gen,0-shot,rougeL_diff,-8.169098689998256,0.7490350953113237
jb723/llama2-ko-7B-model,truthfulqa_mc1,0-shot,accuracy,0.2876376988984088,0.015846315101394795
cerebras/Cerebras-GPT-1.3B,arc:challenge,25-shot,accuracy,0.23720136518771331,0.01243039982926084
cerebras/Cerebras-GPT-1.3B,arc:challenge,25-shot,acc_norm,0.2627986348122867,0.012862523175351333
cerebras/Cerebras-GPT-1.3B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
cerebras/Cerebras-GPT-1.3B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.04351941398892446
cerebras/Cerebras-GPT-1.3B,hendrycksTest-anatomy,5-shot,accuracy,0.2074074074074074,0.03502553170678316
cerebras/Cerebras-GPT-1.3B,hendrycksTest-anatomy,5-shot,acc_norm,0.2074074074074074,0.03502553170678316
cerebras/Cerebras-GPT-1.3B,hendrycksTest-astronomy,5-shot,accuracy,0.21052631578947367,0.033176727875331574
cerebras/Cerebras-GPT-1.3B,hendrycksTest-astronomy,5-shot,acc_norm,0.21052631578947367,0.033176727875331574
cerebras/Cerebras-GPT-1.3B,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.041633319989322695
cerebras/Cerebras-GPT-1.3B,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.041633319989322695
cerebras/Cerebras-GPT-1.3B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2679245283018868,0.02725726032249485
cerebras/Cerebras-GPT-1.3B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2679245283018868,0.02725726032249485
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_biology,5-shot,accuracy,0.24305555555555555,0.03586879280080343
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_biology,5-shot,acc_norm,0.24305555555555555,0.03586879280080343
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909284
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909284
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_computer_science,5-shot,accuracy,0.34,0.04760952285695235
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.34,0.04760952285695235
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.03368762932259431
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.03368762932259431
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_physics,5-shot,accuracy,0.22549019607843138,0.041583075330832865
cerebras/Cerebras-GPT-1.3B,hendrycksTest-college_physics,5-shot,acc_norm,0.22549019607843138,0.041583075330832865
cerebras/Cerebras-GPT-1.3B,hendrycksTest-computer_security,5-shot,accuracy,0.21,0.04093601807403326
cerebras/Cerebras-GPT-1.3B,hendrycksTest-computer_security,5-shot,acc_norm,0.21,0.04093601807403326
cerebras/Cerebras-GPT-1.3B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.029241883869628806
cerebras/Cerebras-GPT-1.3B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.029241883869628806
cerebras/Cerebras-GPT-1.3B,hendrycksTest-econometrics,5-shot,accuracy,0.24561403508771928,0.040493392977481404
cerebras/Cerebras-GPT-1.3B,hendrycksTest-econometrics,5-shot,acc_norm,0.24561403508771928,0.040493392977481404
cerebras/Cerebras-GPT-1.3B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.23448275862068965,0.035306258743465914
cerebras/Cerebras-GPT-1.3B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.23448275862068965,0.035306258743465914
cerebras/Cerebras-GPT-1.3B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24074074074074073,0.022019080012217893
cerebras/Cerebras-GPT-1.3B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24074074074074073,0.022019080012217893
cerebras/Cerebras-GPT-1.3B,hendrycksTest-formal_logic,5-shot,accuracy,0.2619047619047619,0.039325376803928704
cerebras/Cerebras-GPT-1.3B,hendrycksTest-formal_logic,5-shot,acc_norm,0.2619047619047619,0.039325376803928704
cerebras/Cerebras-GPT-1.3B,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
cerebras/Cerebras-GPT-1.3B,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_biology,5-shot,accuracy,0.2032258064516129,0.022891687984554963
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2032258064516129,0.022891687984554963
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.22660098522167488,0.029454863835292965
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.22660098522167488,0.029454863835292965
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.045604802157206845
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.045604802157206845
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2606060606060606,0.03427743175816524
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2606060606060606,0.03427743175816524
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_geography,5-shot,accuracy,0.31313131313131315,0.033042050878136525
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.31313131313131315,0.033042050878136525
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3471502590673575,0.034356961683613546
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3471502590673575,0.034356961683613546
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3230769230769231,0.023710888501970562
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3230769230769231,0.023710888501970562
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25925925925925924,0.026719240783712166
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25925925925925924,0.026719240783712166
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.22268907563025211,0.027025433498882364
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.22268907563025211,0.027025433498882364
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_physics,5-shot,accuracy,0.23841059602649006,0.03479185572599659
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23841059602649006,0.03479185572599659
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3431192660550459,0.02035477773608604
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3431192660550459,0.02035477773608604
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.22058823529411764,0.02910225438967409
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.22058823529411764,0.02910225438967409
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2742616033755274,0.029041333510598014
cerebras/Cerebras-GPT-1.3B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2742616033755274,0.029041333510598014
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_aging,5-shot,accuracy,0.23318385650224216,0.028380391147094716
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_aging,5-shot,acc_norm,0.23318385650224216,0.028380391147094716
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.037276735755969174
cerebras/Cerebras-GPT-1.3B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.037276735755969174
cerebras/Cerebras-GPT-1.3B,hendrycksTest-international_law,5-shot,accuracy,0.2809917355371901,0.04103203830514512
cerebras/Cerebras-GPT-1.3B,hendrycksTest-international_law,5-shot,acc_norm,0.2809917355371901,0.04103203830514512
cerebras/Cerebras-GPT-1.3B,hendrycksTest-jurisprudence,5-shot,accuracy,0.24074074074074073,0.04133119440243839
cerebras/Cerebras-GPT-1.3B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.24074074074074073,0.04133119440243839
cerebras/Cerebras-GPT-1.3B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3006134969325153,0.03602511318806771
cerebras/Cerebras-GPT-1.3B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3006134969325153,0.03602511318806771
cerebras/Cerebras-GPT-1.3B,hendrycksTest-machine_learning,5-shot,accuracy,0.30357142857142855,0.04364226155841044
cerebras/Cerebras-GPT-1.3B,hendrycksTest-machine_learning,5-shot,acc_norm,0.30357142857142855,0.04364226155841044
cerebras/Cerebras-GPT-1.3B,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.039166677628225836
cerebras/Cerebras-GPT-1.3B,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.039166677628225836
cerebras/Cerebras-GPT-1.3B,hendrycksTest-marketing,5-shot,accuracy,0.2948717948717949,0.029872577708891155
cerebras/Cerebras-GPT-1.3B,hendrycksTest-marketing,5-shot,acc_norm,0.2948717948717949,0.029872577708891155
cerebras/Cerebras-GPT-1.3B,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.04512608598542129
cerebras/Cerebras-GPT-1.3B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.04512608598542129
cerebras/Cerebras-GPT-1.3B,hendrycksTest-miscellaneous,5-shot,accuracy,0.24648786717752236,0.015411308769686929
cerebras/Cerebras-GPT-1.3B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.24648786717752236,0.015411308769686929
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.023703099525258172
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.023703099525258172
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23687150837988827,0.014219570788103982
cerebras/Cerebras-GPT-1.3B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23687150837988827,0.014219570788103982
cerebras/Cerebras-GPT-1.3B,hendrycksTest-nutrition,5-shot,accuracy,0.29411764705882354,0.026090162504279042
cerebras/Cerebras-GPT-1.3B,hendrycksTest-nutrition,5-shot,acc_norm,0.29411764705882354,0.026090162504279042
cerebras/Cerebras-GPT-1.3B,hendrycksTest-philosophy,5-shot,accuracy,0.26366559485530544,0.02502553850053234
cerebras/Cerebras-GPT-1.3B,hendrycksTest-philosophy,5-shot,acc_norm,0.26366559485530544,0.02502553850053234
cerebras/Cerebras-GPT-1.3B,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.02456922360046085
cerebras/Cerebras-GPT-1.3B,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.02456922360046085
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.02678917235114023
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.02678917235114023
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_law,5-shot,accuracy,0.2379400260756193,0.010875700787694242
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_law,5-shot,acc_norm,0.2379400260756193,0.010875700787694242
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_medicine,5-shot,accuracy,0.41911764705882354,0.02997280717046462
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.41911764705882354,0.02997280717046462
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2777777777777778,0.018120224251484587
cerebras/Cerebras-GPT-1.3B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2777777777777778,0.018120224251484587
cerebras/Cerebras-GPT-1.3B,hendrycksTest-public_relations,5-shot,accuracy,0.16363636363636364,0.03543433054298678
cerebras/Cerebras-GPT-1.3B,hendrycksTest-public_relations,5-shot,acc_norm,0.16363636363636364,0.03543433054298678
cerebras/Cerebras-GPT-1.3B,hendrycksTest-security_studies,5-shot,accuracy,0.23673469387755103,0.027212835884073142
cerebras/Cerebras-GPT-1.3B,hendrycksTest-security_studies,5-shot,acc_norm,0.23673469387755103,0.027212835884073142
cerebras/Cerebras-GPT-1.3B,hendrycksTest-sociology,5-shot,accuracy,0.22885572139303484,0.029705284056772422
cerebras/Cerebras-GPT-1.3B,hendrycksTest-sociology,5-shot,acc_norm,0.22885572139303484,0.029705284056772422
cerebras/Cerebras-GPT-1.3B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.19,0.03942772444036625
cerebras/Cerebras-GPT-1.3B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.19,0.03942772444036625
cerebras/Cerebras-GPT-1.3B,hendrycksTest-virology,5-shot,accuracy,0.3313253012048193,0.036643147772880864
cerebras/Cerebras-GPT-1.3B,hendrycksTest-virology,5-shot,acc_norm,0.3313253012048193,0.036643147772880864
cerebras/Cerebras-GPT-1.3B,hendrycksTest-world_religions,5-shot,accuracy,0.30409356725146197,0.03528211258245232
cerebras/Cerebras-GPT-1.3B,hendrycksTest-world_religions,5-shot,acc_norm,0.30409356725146197,0.03528211258245232
cerebras/Cerebras-GPT-1.3B,truthfulqa:mc,0-shot,mc1,0.24479804161566707,0.01505186948671501
cerebras/Cerebras-GPT-1.3B,truthfulqa:mc,0-shot,mc2,0.4269871364718513,0.014897248723095273
cerebras/Cerebras-GPT-1.3B,drop,3-shot,accuracy,0.00041946308724832214,0.00020969854707829098
cerebras/Cerebras-GPT-1.3B,drop,3-shot,f1,0.03696203859060411,0.0010536462556224307
Monero/Manticore-13b-Chat-Pyg-Guanaco,drop,3-shot,accuracy,0.1636954697986577,0.00378913611358371
Monero/Manticore-13b-Chat-Pyg-Guanaco,drop,3-shot,f1,0.25622378355704734,0.003909791858313052
Monero/Manticore-13b-Chat-Pyg-Guanaco,gsm8k,5-shot,accuracy,0.22896133434420016,0.01157341289241821
Monero/Manticore-13b-Chat-Pyg-Guanaco,winogrande,5-shot,accuracy,0.739542225730071,0.012334833671998289
Monero/Manticore-13b-Chat-Pyg-Guanaco,arc:challenge,25-shot,accuracy,0.5290102389078498,0.014586776355294321
Monero/Manticore-13b-Chat-Pyg-Guanaco,arc:challenge,25-shot,acc_norm,0.568259385665529,0.014474591427196199
Monero/Manticore-13b-Chat-Pyg-Guanaco,hellaswag,10-shot,accuracy,0.6149173471420036,0.004856203374715455
Monero/Manticore-13b-Chat-Pyg-Guanaco,hellaswag,10-shot,acc_norm,0.823043218482374,0.0038085217687699284
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-abstract_algebra,5-shot,accuracy,0.4,0.049236596391733084
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.4,0.049236596391733084
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-anatomy,5-shot,accuracy,0.4740740740740741,0.04313531696750574
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-anatomy,5-shot,acc_norm,0.4740740740740741,0.04313531696750574
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-astronomy,5-shot,accuracy,0.47368421052631576,0.04063302731486671
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-astronomy,5-shot,acc_norm,0.47368421052631576,0.04063302731486671
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-business_ethics,5-shot,accuracy,0.45,0.05
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-business_ethics,5-shot,acc_norm,0.45,0.05
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.5056603773584906,0.030770900763851316
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.5056603773584906,0.030770900763851316
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_biology,5-shot,accuracy,0.5069444444444444,0.04180806750294938
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_biology,5-shot,acc_norm,0.5069444444444444,0.04180806750294938
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.04648231987117316
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.04648231987117316
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_computer_science,5-shot,accuracy,0.43,0.049756985195624284
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_computer_science,5-shot,acc_norm,0.43,0.049756985195624284
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_mathematics,5-shot,accuracy,0.32,0.04688261722621505
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_mathematics,5-shot,acc_norm,0.32,0.04688261722621505
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_medicine,5-shot,accuracy,0.37572254335260113,0.036928207672648664
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_medicine,5-shot,acc_norm,0.37572254335260113,0.036928207672648664
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_physics,5-shot,accuracy,0.23529411764705882,0.04220773659171452
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-college_physics,5-shot,acc_norm,0.23529411764705882,0.04220773659171452
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-computer_security,5-shot,accuracy,0.6,0.049236596391733084
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-computer_security,5-shot,acc_norm,0.6,0.049236596391733084
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-conceptual_physics,5-shot,accuracy,0.4127659574468085,0.03218471141400351
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.4127659574468085,0.03218471141400351
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-econometrics,5-shot,accuracy,0.3508771929824561,0.044895393502707
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-econometrics,5-shot,acc_norm,0.3508771929824561,0.044895393502707
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3793103448275862,0.040434618619167466
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3793103448275862,0.040434618619167466
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.291005291005291,0.02339382650048487
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.291005291005291,0.02339382650048487
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-formal_logic,5-shot,accuracy,0.30158730158730157,0.04104947269903394
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-formal_logic,5-shot,acc_norm,0.30158730158730157,0.04104947269903394
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.048241815132442176
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.048241815132442176
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_biology,5-shot,accuracy,0.5129032258064516,0.028434533152681855
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_biology,5-shot,acc_norm,0.5129032258064516,0.028434533152681855
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3448275862068966,0.03344283744280458
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3448275862068966,0.03344283744280458
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.49,0.05024183937956912
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.49,0.05024183937956912
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_european_history,5-shot,accuracy,0.5515151515151515,0.03883565977956929
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.5515151515151515,0.03883565977956929
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_geography,5-shot,accuracy,0.5808080808080808,0.03515520728670417
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5808080808080808,0.03515520728670417
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6839378238341969,0.03355397369686172
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6839378238341969,0.03355397369686172
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.4128205128205128,0.024962683564331803
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.4128205128205128,0.024962683564331803
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2962962962962963,0.027840811495871937
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2962962962962963,0.027840811495871937
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.48739495798319327,0.032468167657521745
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.48739495798319327,0.032468167657521745
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_physics,5-shot,accuracy,0.2582781456953642,0.035737053147634576
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2582781456953642,0.035737053147634576
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6293577981651376,0.02070745816435298
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6293577981651376,0.02070745816435298
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_statistics,5-shot,accuracy,0.30092592592592593,0.03128039084329881
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.30092592592592593,0.03128039084329881
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_us_history,5-shot,accuracy,0.6323529411764706,0.03384132045674119
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.6323529411764706,0.03384132045674119
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6371308016877637,0.031299208255302136
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6371308016877637,0.031299208255302136
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_aging,5-shot,accuracy,0.5919282511210763,0.03298574607842822
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_aging,5-shot,acc_norm,0.5919282511210763,0.03298574607842822
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_sexuality,5-shot,accuracy,0.5267175572519084,0.04379024936553894
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5267175572519084,0.04379024936553894
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-international_law,5-shot,accuracy,0.5702479338842975,0.04519082021319773
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-international_law,5-shot,acc_norm,0.5702479338842975,0.04519082021319773
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-jurisprudence,5-shot,accuracy,0.6018518518518519,0.04732332615978814
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-jurisprudence,5-shot,acc_norm,0.6018518518518519,0.04732332615978814
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-logical_fallacies,5-shot,accuracy,0.5214723926380368,0.03924746876751128
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.5214723926380368,0.03924746876751128
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-machine_learning,5-shot,accuracy,0.35714285714285715,0.04547960999764376
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-machine_learning,5-shot,acc_norm,0.35714285714285715,0.04547960999764376
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-management,5-shot,accuracy,0.6407766990291263,0.04750458399041694
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-management,5-shot,acc_norm,0.6407766990291263,0.04750458399041694
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-marketing,5-shot,accuracy,0.7222222222222222,0.029343114798094462
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-marketing,5-shot,acc_norm,0.7222222222222222,0.029343114798094462
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-medical_genetics,5-shot,accuracy,0.47,0.050161355804659205
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-medical_genetics,5-shot,acc_norm,0.47,0.050161355804659205
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-miscellaneous,5-shot,accuracy,0.6551724137931034,0.01699712334611344
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6551724137931034,0.01699712334611344
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_disputes,5-shot,accuracy,0.5317919075144508,0.02686462436675665
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_disputes,5-shot,acc_norm,0.5317919075144508,0.02686462436675665
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_scenarios,5-shot,accuracy,0.26256983240223464,0.014716824273017763
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.26256983240223464,0.014716824273017763
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-nutrition,5-shot,accuracy,0.47058823529411764,0.028580341065138293
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-nutrition,5-shot,acc_norm,0.47058823529411764,0.028580341065138293
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-philosophy,5-shot,accuracy,0.5080385852090032,0.02839442137098453
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-philosophy,5-shot,acc_norm,0.5080385852090032,0.02839442137098453
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-prehistory,5-shot,accuracy,0.5648148148148148,0.027586006221607715
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-prehistory,5-shot,acc_norm,0.5648148148148148,0.027586006221607715
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_accounting,5-shot,accuracy,0.4078014184397163,0.029316011776343562
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4078014184397163,0.029316011776343562
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_law,5-shot,accuracy,0.3878748370273794,0.01244499830967562
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_law,5-shot,acc_norm,0.3878748370273794,0.01244499830967562
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_medicine,5-shot,accuracy,0.48161764705882354,0.03035230339535196
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_medicine,5-shot,acc_norm,0.48161764705882354,0.03035230339535196
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_psychology,5-shot,accuracy,0.46405228758169936,0.02017548876548404
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-professional_psychology,5-shot,acc_norm,0.46405228758169936,0.02017548876548404
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-public_relations,5-shot,accuracy,0.5636363636363636,0.04750185058907296
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-public_relations,5-shot,acc_norm,0.5636363636363636,0.04750185058907296
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-security_studies,5-shot,accuracy,0.5224489795918368,0.03197694118713672
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-security_studies,5-shot,acc_norm,0.5224489795918368,0.03197694118713672
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-sociology,5-shot,accuracy,0.5671641791044776,0.03503490923673282
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-sociology,5-shot,acc_norm,0.5671641791044776,0.03503490923673282
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.76,0.042923469599092816
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.76,0.042923469599092816
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-virology,5-shot,accuracy,0.42168674698795183,0.03844453181770917
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-virology,5-shot,acc_norm,0.42168674698795183,0.03844453181770917
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-world_religions,5-shot,accuracy,0.6842105263157895,0.03565079670708312
Monero/Manticore-13b-Chat-Pyg-Guanaco,hendrycksTest-world_religions,5-shot,acc_norm,0.6842105263157895,0.03565079670708312
Monero/Manticore-13b-Chat-Pyg-Guanaco,truthfulqa:mc,0-shot,mc1,0.36474908200734396,0.016850961061720113
Monero/Manticore-13b-Chat-Pyg-Guanaco,truthfulqa:mc,0-shot,mc2,0.5228532383120019,0.01587766921395582
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_precalc,5-shot,accuracy,0.018315018315018316,0.005743696731653661
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_prealgebra,5-shot,accuracy,0.06199770378874857,0.00817579751206219
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_num_theory,5-shot,accuracy,0.024074074074074074,0.006602202509815387
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_intermediate_algebra,5-shot,accuracy,0.021040974529346623,0.004778723623319642
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_geometry,5-shot,accuracy,0.010438413361169102,0.004648627117184662
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_counting_and_prob,5-shot,accuracy,0.04219409282700422,0.009243448209077151
Monero/Manticore-13b-Chat-Pyg-Guanaco,minerva_math_algebra,5-shot,accuracy,0.04296545914069082,0.005888181524949664
Monero/Manticore-13b-Chat-Pyg-Guanaco,fld_default,0-shot,accuracy,0.0,
Monero/Manticore-13b-Chat-Pyg-Guanaco,fld_star,0-shot,accuracy,0.0,
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_3da,5-shot,accuracy,0.594,0.010983729838291715
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_3ds,5-shot,accuracy,0.2335,0.009462221822643413
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_4da,5-shot,accuracy,0.5165,0.011177045144808303
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_2ds,5-shot,accuracy,0.21,0.009109966835717524
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_5ds,5-shot,accuracy,0.0525,0.004988418302285765
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_5da,5-shot,accuracy,0.2795,0.010036944013122736
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_1dc,5-shot,accuracy,0.087,0.0063035995814963615
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_4ds,5-shot,accuracy,0.192,0.00880947236795147
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_2dm,5-shot,accuracy,0.0375,0.004249223805764557
Monero/Manticore-13b-Chat-Pyg-Guanaco,arithmetic_2da,5-shot,accuracy,0.652,0.010653860914062438
Monero/Manticore-13b-Chat-Pyg-Guanaco,gsm8k_cot,5-shot,accuracy,0.244882486732373,0.011844819027863673
Monero/Manticore-13b-Chat-Pyg-Guanaco,anli_r2,0-shot,brier_score,0.8852013154077675,
Monero/Manticore-13b-Chat-Pyg-Guanaco,anli_r3,0-shot,brier_score,0.7816093138893003,
Monero/Manticore-13b-Chat-Pyg-Guanaco,anli_r1,0-shot,brier_score,0.8665860062055024,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_eu,0-shot,brier_score,1.295309492627305,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_vi,0-shot,brier_score,1.0284578721600939,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_ru,0-shot,brier_score,0.9424584802883822,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_zh,0-shot,brier_score,1.0732202227433019,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_tr,0-shot,brier_score,1.1494729899079483,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_fr,0-shot,brier_score,0.8836516075827534,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_en,0-shot,brier_score,0.8049487003810644,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_ur,0-shot,brier_score,1.2526672854095398,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_ar,0-shot,brier_score,1.249788260932463,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_de,0-shot,brier_score,0.9308427131328507,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_hi,0-shot,brier_score,1.014295071171663,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_es,0-shot,brier_score,0.932183728871887,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_bg,0-shot,brier_score,0.9824751588952393,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_sw,0-shot,brier_score,1.0143704119021475,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_el,0-shot,brier_score,1.021583604470639,
Monero/Manticore-13b-Chat-Pyg-Guanaco,xnli_th,0-shot,brier_score,1.1706481918320806,
Monero/Manticore-13b-Chat-Pyg-Guanaco,logiqa2,0-shot,brier_score,1.0546915532913335,
Monero/Manticore-13b-Chat-Pyg-Guanaco,mathqa,5-shot,brier_score,0.998681916345562,
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_standard,0-shot,perplexity,4.071811300991231,0.11908741129946793
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_standard,0-shot,accuracy,0.6567048321366195,0.006615017904433671
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_openai,0-shot,perplexity,3.103419189766064,0.08087798156130009
Monero/Manticore-13b-Chat-Pyg-Guanaco,lambada_openai,0-shot,accuracy,0.7155055307587813,0.0062857265569445
mosaicml/mpt-30b,arc:challenge,25-shot,accuracy,0.5290102389078498,0.014586776355294317
mosaicml/mpt-30b,arc:challenge,25-shot,acc_norm,0.5597269624573379,0.014506769524804237
mosaicml/mpt-30b,hellaswag,10-shot,accuracy,0.6195976897032464,0.004844935327599206
mosaicml/mpt-30b,hellaswag,10-shot,acc_norm,0.8242381995618403,0.0037983950550215346
mosaicml/mpt-30b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.04648231987117316
mosaicml/mpt-30b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.04648231987117316
mosaicml/mpt-30b,hendrycksTest-anatomy,5-shot,accuracy,0.4888888888888889,0.04318275491977976
mosaicml/mpt-30b,hendrycksTest-anatomy,5-shot,acc_norm,0.4888888888888889,0.04318275491977976
mosaicml/mpt-30b,hendrycksTest-astronomy,5-shot,accuracy,0.40789473684210525,0.03999309712777471
mosaicml/mpt-30b,hendrycksTest-astronomy,5-shot,acc_norm,0.40789473684210525,0.03999309712777471
mosaicml/mpt-30b,hendrycksTest-business_ethics,5-shot,accuracy,0.53,0.05016135580465919
mosaicml/mpt-30b,hendrycksTest-business_ethics,5-shot,acc_norm,0.53,0.05016135580465919
mosaicml/mpt-30b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4867924528301887,0.030762134874500476
mosaicml/mpt-30b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4867924528301887,0.030762134874500476
mosaicml/mpt-30b,hendrycksTest-college_biology,5-shot,accuracy,0.5138888888888888,0.04179596617581
mosaicml/mpt-30b,hendrycksTest-college_biology,5-shot,acc_norm,0.5138888888888888,0.04179596617581
mosaicml/mpt-30b,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.04648231987117316
mosaicml/mpt-30b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.04648231987117316
mosaicml/mpt-30b,hendrycksTest-college_computer_science,5-shot,accuracy,0.42,0.049604496374885836
mosaicml/mpt-30b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.42,0.049604496374885836
mosaicml/mpt-30b,hendrycksTest-college_mathematics,5-shot,accuracy,0.35,0.0479372485441102
mosaicml/mpt-30b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.35,0.0479372485441102
mosaicml/mpt-30b,hendrycksTest-college_medicine,5-shot,accuracy,0.4508670520231214,0.037940126746970275
mosaicml/mpt-30b,hendrycksTest-college_medicine,5-shot,acc_norm,0.4508670520231214,0.037940126746970275
mosaicml/mpt-30b,hendrycksTest-college_physics,5-shot,accuracy,0.30392156862745096,0.045766654032077636
mosaicml/mpt-30b,hendrycksTest-college_physics,5-shot,acc_norm,0.30392156862745096,0.045766654032077636
mosaicml/mpt-30b,hendrycksTest-computer_security,5-shot,accuracy,0.6,0.049236596391733084
mosaicml/mpt-30b,hendrycksTest-computer_security,5-shot,acc_norm,0.6,0.049236596391733084
mosaicml/mpt-30b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.41702127659574467,0.03223276266711712
mosaicml/mpt-30b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.41702127659574467,0.03223276266711712
mosaicml/mpt-30b,hendrycksTest-econometrics,5-shot,accuracy,0.2982456140350877,0.04303684033537315
mosaicml/mpt-30b,hendrycksTest-econometrics,5-shot,acc_norm,0.2982456140350877,0.04303684033537315
mosaicml/mpt-30b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5172413793103449,0.04164188720169375
mosaicml/mpt-30b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5172413793103449,0.04164188720169375
mosaicml/mpt-30b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3306878306878307,0.02422996529842509
mosaicml/mpt-30b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3306878306878307,0.02422996529842509
mosaicml/mpt-30b,hendrycksTest-formal_logic,5-shot,accuracy,0.2857142857142857,0.0404061017820884
mosaicml/mpt-30b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2857142857142857,0.0404061017820884
mosaicml/mpt-30b,hendrycksTest-global_facts,5-shot,accuracy,0.38,0.048783173121456316
mosaicml/mpt-30b,hendrycksTest-global_facts,5-shot,acc_norm,0.38,0.048783173121456316
mosaicml/mpt-30b,hendrycksTest-high_school_biology,5-shot,accuracy,0.5419354838709678,0.028343787250540632
mosaicml/mpt-30b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.5419354838709678,0.028343787250540632
mosaicml/mpt-30b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.35467980295566504,0.0336612448905145
mosaicml/mpt-30b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.35467980295566504,0.0336612448905145
mosaicml/mpt-30b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.53,0.05016135580465919
mosaicml/mpt-30b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.53,0.05016135580465919
mosaicml/mpt-30b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.6,0.03825460278380026
mosaicml/mpt-30b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.6,0.03825460278380026
mosaicml/mpt-30b,hendrycksTest-high_school_geography,5-shot,accuracy,0.5959595959595959,0.03496130972056128
mosaicml/mpt-30b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5959595959595959,0.03496130972056128
mosaicml/mpt-30b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6476683937823834,0.03447478286414357
mosaicml/mpt-30b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6476683937823834,0.03447478286414357
mosaicml/mpt-30b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.46923076923076923,0.02530295889085015
mosaicml/mpt-30b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.46923076923076923,0.02530295889085015
mosaicml/mpt-30b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2814814814814815,0.027420019350945284
mosaicml/mpt-30b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2814814814814815,0.027420019350945284
mosaicml/mpt-30b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.4411764705882353,0.0322529423239964
mosaicml/mpt-30b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.4411764705882353,0.0322529423239964
mosaicml/mpt-30b,hendrycksTest-high_school_physics,5-shot,accuracy,0.2980132450331126,0.037345356767871984
mosaicml/mpt-30b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2980132450331126,0.037345356767871984
mosaicml/mpt-30b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6770642201834862,0.020048115923415315
mosaicml/mpt-30b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6770642201834862,0.020048115923415315
mosaicml/mpt-30b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.36574074074074076,0.03284738857647208
mosaicml/mpt-30b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.36574074074074076,0.03284738857647208
mosaicml/mpt-30b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.6666666666666666,0.03308611113236436
mosaicml/mpt-30b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.6666666666666666,0.03308611113236436
mosaicml/mpt-30b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6708860759493671,0.03058732629470236
mosaicml/mpt-30b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6708860759493671,0.03058732629470236
mosaicml/mpt-30b,hendrycksTest-human_aging,5-shot,accuracy,0.5381165919282511,0.033460150119732274
mosaicml/mpt-30b,hendrycksTest-human_aging,5-shot,acc_norm,0.5381165919282511,0.033460150119732274
mosaicml/mpt-30b,hendrycksTest-human_sexuality,5-shot,accuracy,0.5343511450381679,0.043749285605997376
mosaicml/mpt-30b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5343511450381679,0.043749285605997376
mosaicml/mpt-30b,hendrycksTest-international_law,5-shot,accuracy,0.4297520661157025,0.04519082021319773
mosaicml/mpt-30b,hendrycksTest-international_law,5-shot,acc_norm,0.4297520661157025,0.04519082021319773
mosaicml/mpt-30b,hendrycksTest-jurisprudence,5-shot,accuracy,0.4722222222222222,0.048262172941398944
mosaicml/mpt-30b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.4722222222222222,0.048262172941398944
mosaicml/mpt-30b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.4785276073619632,0.0392474687675113
mosaicml/mpt-30b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.4785276073619632,0.0392474687675113
mosaicml/mpt-30b,hendrycksTest-machine_learning,5-shot,accuracy,0.45535714285714285,0.047268355537191
mosaicml/mpt-30b,hendrycksTest-machine_learning,5-shot,acc_norm,0.45535714285714285,0.047268355537191
mosaicml/mpt-30b,hendrycksTest-management,5-shot,accuracy,0.5631067961165048,0.04911147107365777
mosaicml/mpt-30b,hendrycksTest-management,5-shot,acc_norm,0.5631067961165048,0.04911147107365777
mosaicml/mpt-30b,hendrycksTest-marketing,5-shot,accuracy,0.7136752136752137,0.029614323690456655
mosaicml/mpt-30b,hendrycksTest-marketing,5-shot,acc_norm,0.7136752136752137,0.029614323690456655
mosaicml/mpt-30b,hendrycksTest-medical_genetics,5-shot,accuracy,0.5,0.050251890762960605
mosaicml/mpt-30b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.5,0.050251890762960605
mosaicml/mpt-30b,hendrycksTest-miscellaneous,5-shot,accuracy,0.6871008939974457,0.016580935940304038
mosaicml/mpt-30b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6871008939974457,0.016580935940304038
mosaicml/mpt-30b,hendrycksTest-moral_disputes,5-shot,accuracy,0.5115606936416185,0.026911898686377913
mosaicml/mpt-30b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.5115606936416185,0.026911898686377913
mosaicml/mpt-30b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.26927374301675977,0.014835616582882606
mosaicml/mpt-30b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.26927374301675977,0.014835616582882606
mosaicml/mpt-30b,hendrycksTest-nutrition,5-shot,accuracy,0.5098039215686274,0.02862441255016795
mosaicml/mpt-30b,hendrycksTest-nutrition,5-shot,acc_norm,0.5098039215686274,0.02862441255016795
mosaicml/mpt-30b,hendrycksTest-philosophy,5-shot,accuracy,0.5466237942122186,0.028274359854894245
mosaicml/mpt-30b,hendrycksTest-philosophy,5-shot,acc_norm,0.5466237942122186,0.028274359854894245
mosaicml/mpt-30b,hendrycksTest-prehistory,5-shot,accuracy,0.5679012345679012,0.02756301097160668
mosaicml/mpt-30b,hendrycksTest-prehistory,5-shot,acc_norm,0.5679012345679012,0.02756301097160668
mosaicml/mpt-30b,hendrycksTest-professional_accounting,5-shot,accuracy,0.3617021276595745,0.028663820147199492
mosaicml/mpt-30b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3617021276595745,0.028663820147199492
mosaicml/mpt-30b,hendrycksTest-professional_law,5-shot,accuracy,0.37809647979139505,0.012384878406798095
mosaicml/mpt-30b,hendrycksTest-professional_law,5-shot,acc_norm,0.37809647979139505,0.012384878406798095
mosaicml/mpt-30b,hendrycksTest-professional_medicine,5-shot,accuracy,0.38235294117647056,0.029520095697687765
mosaicml/mpt-30b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.38235294117647056,0.029520095697687765
mosaicml/mpt-30b,hendrycksTest-professional_psychology,5-shot,accuracy,0.4526143790849673,0.020136790918492534
mosaicml/mpt-30b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.4526143790849673,0.020136790918492534
mosaicml/mpt-30b,hendrycksTest-public_relations,5-shot,accuracy,0.5727272727272728,0.04738198703545483
mosaicml/mpt-30b,hendrycksTest-public_relations,5-shot,acc_norm,0.5727272727272728,0.04738198703545483
mosaicml/mpt-30b,hendrycksTest-security_studies,5-shot,accuracy,0.5510204081632653,0.03184213866687579
mosaicml/mpt-30b,hendrycksTest-security_studies,5-shot,acc_norm,0.5510204081632653,0.03184213866687579
mosaicml/mpt-30b,hendrycksTest-sociology,5-shot,accuracy,0.5472636815920398,0.035197027175769155
mosaicml/mpt-30b,hendrycksTest-sociology,5-shot,acc_norm,0.5472636815920398,0.035197027175769155
mosaicml/mpt-30b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.67,0.04725815626252607
mosaicml/mpt-30b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.67,0.04725815626252607
mosaicml/mpt-30b,hendrycksTest-virology,5-shot,accuracy,0.4457831325301205,0.03869543323472101
mosaicml/mpt-30b,hendrycksTest-virology,5-shot,acc_norm,0.4457831325301205,0.03869543323472101
mosaicml/mpt-30b,hendrycksTest-world_religions,5-shot,accuracy,0.6783625730994152,0.03582529442573122
mosaicml/mpt-30b,hendrycksTest-world_religions,5-shot,acc_norm,0.6783625730994152,0.03582529442573122
mosaicml/mpt-30b,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.015321821688476196
mosaicml/mpt-30b,truthfulqa:mc,0-shot,mc2,0.3841558252351552,0.013607507438444062
mosaicml/mpt-30b,winogrande,5-shot,accuracy,0.7490134175217048,0.01218577622051616
mosaicml/mpt-30b,gsm8k,5-shot,accuracy,0.16906747536012132,0.01032417144549735
EleutherAI/gpt-neo-125M,mmlu_world_religions,0-shot,accuracy,0.2046783625730994,0.030944459778533204
EleutherAI/gpt-neo-125M,mmlu_formal_logic,0-shot,accuracy,0.24603174603174602,0.03852273364924316
EleutherAI/gpt-neo-125M,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.022899162918445796
EleutherAI/gpt-neo-125M,mmlu_moral_scenarios,0-shot,accuracy,0.2245810055865922,0.01395680366654464
EleutherAI/gpt-neo-125M,mmlu_high_school_world_history,0-shot,accuracy,0.22362869198312235,0.027123298205229972
EleutherAI/gpt-neo-125M,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
EleutherAI/gpt-neo-125M,mmlu_professional_law,0-shot,accuracy,0.2470664928292047,0.011015752255279327
EleutherAI/gpt-neo-125M,mmlu_logical_fallacies,0-shot,accuracy,0.24539877300613497,0.03380939813943354
EleutherAI/gpt-neo-125M,mmlu_high_school_us_history,0-shot,accuracy,0.29901960784313725,0.03213325717373615
EleutherAI/gpt-neo-125M,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/gpt-neo-125M,mmlu_jurisprudence,0-shot,accuracy,0.23148148148148148,0.04077494709252627
EleutherAI/gpt-neo-125M,mmlu_international_law,0-shot,accuracy,0.2231404958677686,0.03800754475228733
EleutherAI/gpt-neo-125M,mmlu_high_school_european_history,0-shot,accuracy,0.24848484848484848,0.03374402644139404
EleutherAI/gpt-neo-125M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.35751295336787564,0.034588160421810066
EleutherAI/gpt-neo-125M,mmlu_high_school_microeconomics,0-shot,accuracy,0.23109243697478993,0.027381406927868952
EleutherAI/gpt-neo-125M,mmlu_high_school_geography,0-shot,accuracy,0.36363636363636365,0.03427308652999935
EleutherAI/gpt-neo-125M,mmlu_high_school_psychology,0-shot,accuracy,0.27889908256880735,0.019227468876463514
EleutherAI/gpt-neo-125M,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.041723430387053825
EleutherAI/gpt-neo-125M,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.044619604333847394
EleutherAI/gpt-neo-125M,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/gpt-neo-125M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2512820512820513,0.021992016662370533
EleutherAI/gpt-neo-125M,mmlu_security_studies,0-shot,accuracy,0.4,0.031362502409358936
EleutherAI/gpt-neo-125M,mmlu_professional_psychology,0-shot,accuracy,0.27124183006535946,0.017986615304030295
EleutherAI/gpt-neo-125M,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/gpt-neo-125M,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.041424397194893624
EleutherAI/gpt-neo-125M,mmlu_miscellaneous,0-shot,accuracy,0.24010217113665389,0.015274685213734191
EleutherAI/gpt-neo-125M,mmlu_marketing,0-shot,accuracy,0.2692307692307692,0.029058588303748845
EleutherAI/gpt-neo-125M,mmlu_management,0-shot,accuracy,0.2524271844660194,0.04301250399690878
EleutherAI/gpt-neo-125M,mmlu_nutrition,0-shot,accuracy,0.27124183006535946,0.02545775669666787
EleutherAI/gpt-neo-125M,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neo-125M,mmlu_human_aging,0-shot,accuracy,0.27802690582959644,0.030069584874494043
EleutherAI/gpt-neo-125M,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.030211479609121593
EleutherAI/gpt-neo-125M,mmlu_college_medicine,0-shot,accuracy,0.1907514450867052,0.029957851329869334
EleutherAI/gpt-neo-125M,mmlu_business_ethics,0-shot,accuracy,0.23,0.042295258468165044
EleutherAI/gpt-neo-125M,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.02688064788905199
EleutherAI/gpt-neo-125M,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
EleutherAI/gpt-neo-125M,mmlu_virology,0-shot,accuracy,0.19879518072289157,0.03106939026078942
EleutherAI/gpt-neo-125M,mmlu_professional_accounting,0-shot,accuracy,0.24113475177304963,0.025518731049537766
EleutherAI/gpt-neo-125M,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.04158307533083286
EleutherAI/gpt-neo-125M,mmlu_high_school_physics,0-shot,accuracy,0.31788079470198677,0.038020397601079024
EleutherAI/gpt-neo-125M,mmlu_high_school_biology,0-shot,accuracy,0.25161290322580643,0.02468597928623996
EleutherAI/gpt-neo-125M,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.03716177437566018
EleutherAI/gpt-neo-125M,mmlu_anatomy,0-shot,accuracy,0.2740740740740741,0.03853254836552003
EleutherAI/gpt-neo-125M,mmlu_college_chemistry,0-shot,accuracy,0.24,0.042923469599092816
EleutherAI/gpt-neo-125M,mmlu_computer_security,0-shot,accuracy,0.19,0.03942772444036624
EleutherAI/gpt-neo-125M,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526045
EleutherAI/gpt-neo-125M,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/gpt-neo-125M,mmlu_college_mathematics,0-shot,accuracy,0.27,0.044619604333847394
EleutherAI/gpt-neo-125M,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.029513196625539355
EleutherAI/gpt-neo-125M,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/gpt-neo-125M,mmlu_high_school_computer_science,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/gpt-neo-125M,mmlu_machine_learning,0-shot,accuracy,0.26785714285714285,0.04203277291467763
EleutherAI/gpt-neo-125M,mmlu_high_school_chemistry,0-shot,accuracy,0.27586206896551724,0.03144712581678242
EleutherAI/gpt-neo-125M,mmlu_high_school_statistics,0-shot,accuracy,0.4583333333333333,0.03398110890294636
EleutherAI/gpt-neo-125M,mmlu_elementary_mathematics,0-shot,accuracy,0.24074074074074073,0.022019080012217897
EleutherAI/gpt-neo-125M,mmlu_electrical_engineering,0-shot,accuracy,0.23448275862068965,0.035306258743465914
EleutherAI/gpt-neo-125M,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.026466117538959912
EleutherAI/gpt-neo-125M,arc_challenge,25-shot,accuracy,0.20136518771331058,0.011718927477444267
EleutherAI/gpt-neo-125M,arc_challenge,25-shot,acc_norm,0.23890784982935154,0.01246107137631662
EleutherAI/gpt-neo-125M,hellaswag,10-shot,accuracy,0.2849034056960765,0.004504459553909797
EleutherAI/gpt-neo-125M,hellaswag,10-shot,acc_norm,0.3020314678350926,0.004582004744713358
EleutherAI/gpt-neo-125M,truthfulqa_mc2,0-shot,accuracy,0.45579370727747115,0.015399303339136132
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,bleu_max,13.82766815700744,0.5154784183208384
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,bleu_acc,0.39167686658506734,0.017087795881769643
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,bleu_diff,-0.3071316802783103,0.48061175573860104
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge1_max,34.23389894145755,0.7612633125881414
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge1_acc,0.35862913096695226,0.016789289499502022
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge1_diff,-1.3810176678838904,0.7542591368961628
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge2_max,16.07909085959659,0.7862657983849908
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge2_acc,0.1909424724602203,0.013759285842685716
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rouge2_diff,-2.05928101776518,0.7203850974001921
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rougeL_max,31.690463611915124,0.7444436261611048
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rougeL_acc,0.3659730722154223,0.016862941684088383
EleutherAI/gpt-neo-125M,truthfulqa_gen,0-shot,rougeL_diff,-0.9361203963395062,0.7449460193846943
EleutherAI/gpt-neo-125M,truthfulqa_mc1,0-shot,accuracy,0.2582619339045288,0.015321821688476187
EleutherAI/gpt-neo-125M,winogrande,5-shot,accuracy,0.510655090765588,0.014049294536290403
google/gemma-2-2b,mmlu_world_religions,0-shot,accuracy,0.695906432748538,0.03528211258245231
google/gemma-2-2b,mmlu_formal_logic,0-shot,accuracy,0.3253968253968254,0.041905964388711366
google/gemma-2-2b,mmlu_prehistory,0-shot,accuracy,0.5679012345679012,0.027563010971606676
google/gemma-2-2b,mmlu_moral_scenarios,0-shot,accuracy,0.2759776536312849,0.014950103002475358
google/gemma-2-2b,mmlu_high_school_world_history,0-shot,accuracy,0.6666666666666666,0.0306858205966108
google/gemma-2-2b,mmlu_moral_disputes,0-shot,accuracy,0.6040462427745664,0.02632981334194624
google/gemma-2-2b,mmlu_professional_law,0-shot,accuracy,0.4067796610169492,0.01254632559656952
google/gemma-2-2b,mmlu_logical_fallacies,0-shot,accuracy,0.6073619631901841,0.03836740907831029
google/gemma-2-2b,mmlu_high_school_us_history,0-shot,accuracy,0.6715686274509803,0.03296245110172227
google/gemma-2-2b,mmlu_philosophy,0-shot,accuracy,0.5852090032154341,0.02798268045975956
google/gemma-2-2b,mmlu_jurisprudence,0-shot,accuracy,0.6018518518518519,0.04732332615978815
google/gemma-2-2b,mmlu_international_law,0-shot,accuracy,0.6859504132231405,0.042369647530410184
google/gemma-2-2b,mmlu_high_school_european_history,0-shot,accuracy,0.6666666666666666,0.036810508691615486
google/gemma-2-2b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.772020725388601,0.030276909945178263
google/gemma-2-2b,mmlu_high_school_microeconomics,0-shot,accuracy,0.6134453781512605,0.031631458075523776
google/gemma-2-2b,mmlu_high_school_geography,0-shot,accuracy,0.7474747474747475,0.030954055470365907
google/gemma-2-2b,mmlu_high_school_psychology,0-shot,accuracy,0.7394495412844037,0.018819182034850068
google/gemma-2-2b,mmlu_public_relations,0-shot,accuracy,0.6,0.0469237132203465
google/gemma-2-2b,mmlu_us_foreign_policy,0-shot,accuracy,0.72,0.04512608598542128
google/gemma-2-2b,mmlu_sociology,0-shot,accuracy,0.7512437810945274,0.030567675938916718
google/gemma-2-2b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.5307692307692308,0.025302958890850154
google/gemma-2-2b,mmlu_security_studies,0-shot,accuracy,0.5918367346938775,0.03146465712827424
google/gemma-2-2b,mmlu_professional_psychology,0-shot,accuracy,0.5441176470588235,0.020148939420415752
google/gemma-2-2b,mmlu_human_sexuality,0-shot,accuracy,0.5954198473282443,0.043046937953806645
google/gemma-2-2b,mmlu_econometrics,0-shot,accuracy,0.32456140350877194,0.04404556157374767
google/gemma-2-2b,mmlu_miscellaneous,0-shot,accuracy,0.6704980842911877,0.01680832226174047
google/gemma-2-2b,mmlu_marketing,0-shot,accuracy,0.7948717948717948,0.026453508054040346
google/gemma-2-2b,mmlu_management,0-shot,accuracy,0.6893203883495146,0.0458212416016155
google/gemma-2-2b,mmlu_nutrition,0-shot,accuracy,0.6045751633986928,0.02799672318063145
google/gemma-2-2b,mmlu_medical_genetics,0-shot,accuracy,0.7,0.046056618647183814
google/gemma-2-2b,mmlu_human_aging,0-shot,accuracy,0.5874439461883408,0.03304062175449297
google/gemma-2-2b,mmlu_professional_medicine,0-shot,accuracy,0.41544117647058826,0.029935342707877746
google/gemma-2-2b,mmlu_college_medicine,0-shot,accuracy,0.6127167630057804,0.03714325906302065
google/gemma-2-2b,mmlu_business_ethics,0-shot,accuracy,0.54,0.05009082659620332
google/gemma-2-2b,mmlu_clinical_knowledge,0-shot,accuracy,0.5811320754716981,0.0303650508291152
google/gemma-2-2b,mmlu_global_facts,0-shot,accuracy,0.3,0.046056618647183814
google/gemma-2-2b,mmlu_virology,0-shot,accuracy,0.46987951807228917,0.03885425420866766
google/gemma-2-2b,mmlu_professional_accounting,0-shot,accuracy,0.3971631205673759,0.0291898056735871
google/gemma-2-2b,mmlu_college_physics,0-shot,accuracy,0.38235294117647056,0.04835503696107223
google/gemma-2-2b,mmlu_high_school_physics,0-shot,accuracy,0.40397350993377484,0.04006485685365342
google/gemma-2-2b,mmlu_high_school_biology,0-shot,accuracy,0.6709677419354839,0.026729499068349954
google/gemma-2-2b,mmlu_college_biology,0-shot,accuracy,0.6180555555555556,0.040629907841466674
google/gemma-2-2b,mmlu_anatomy,0-shot,accuracy,0.5185185185185185,0.043163785995113245
google/gemma-2-2b,mmlu_college_chemistry,0-shot,accuracy,0.42,0.04960449637488583
google/gemma-2-2b,mmlu_computer_security,0-shot,accuracy,0.64,0.048241815132442176
google/gemma-2-2b,mmlu_college_computer_science,0-shot,accuracy,0.45,0.05
google/gemma-2-2b,mmlu_astronomy,0-shot,accuracy,0.5657894736842105,0.0403356566784832
google/gemma-2-2b,mmlu_college_mathematics,0-shot,accuracy,0.32,0.04688261722621504
google/gemma-2-2b,mmlu_conceptual_physics,0-shot,accuracy,0.4723404255319149,0.03263597118409769
google/gemma-2-2b,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768078
google/gemma-2-2b,mmlu_high_school_computer_science,0-shot,accuracy,0.48,0.050211673156867795
google/gemma-2-2b,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.04493949068613539
google/gemma-2-2b,mmlu_high_school_chemistry,0-shot,accuracy,0.4729064039408867,0.03512819077876106
google/gemma-2-2b,mmlu_high_school_statistics,0-shot,accuracy,0.46296296296296297,0.03400603625538271
google/gemma-2-2b,mmlu_elementary_mathematics,0-shot,accuracy,0.3439153439153439,0.024464426625596426
google/gemma-2-2b,mmlu_electrical_engineering,0-shot,accuracy,0.6068965517241379,0.0407032901370707
google/gemma-2-2b,mmlu_high_school_mathematics,0-shot,accuracy,0.3,0.02794045713622842
google/gemma-2-2b,arc_challenge,25-shot,accuracy,0.5008532423208191,0.014611369529813286
google/gemma-2-2b,arc_challenge,25-shot,acc_norm,0.5435153583617748,0.014555949760496444
google/gemma-2-2b,hellaswag,10-shot,accuracy,0.5613423620792671,0.004952087083128916
google/gemma-2-2b,hellaswag,10-shot,acc_norm,0.744672376020713,0.00435154060398855
google/gemma-2-2b,truthfulqa_mc2,0-shot,accuracy,0.3622657549504685,0.013773618785299651
google/gemma-2-2b,truthfulqa_gen,0-shot,bleu_max,28.775970992464718,0.8088400452045031
google/gemma-2-2b,truthfulqa_gen,0-shot,bleu_acc,0.2962056303549572,0.015983595101811396
google/gemma-2-2b,truthfulqa_gen,0-shot,bleu_diff,-8.481338149090616,0.8892475779963439
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge1_max,54.25443225505836,0.8465255595845338
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge1_acc,0.2668298653610771,0.015483691939237262
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge1_diff,-10.29473637169676,0.9369283983381803
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge2_max,38.93076935593885,0.9966155175968517
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge2_acc,0.24969400244798043,0.015152286907148125
google/gemma-2-2b,truthfulqa_gen,0-shot,rouge2_diff,-12.10709092268605,1.1397993058329141
google/gemma-2-2b,truthfulqa_gen,0-shot,rougeL_max,51.455864843719006,0.8655454011373582
google/gemma-2-2b,truthfulqa_gen,0-shot,rougeL_acc,0.27050183598531213,0.015550778332842883
google/gemma-2-2b,truthfulqa_gen,0-shot,rougeL_diff,-10.280143405614036,0.9525463151211888
google/gemma-2-2b,truthfulqa_mc1,0-shot,accuracy,0.24112607099143207,0.014974827279752334
google/gemma-2-2b,winogrande,5-shot,accuracy,0.7158642462509865,0.012675392786772722
google/gemma-2-2b,gsm8k,5-shot,accuracy,0.2623199393479909,0.012116912419925699
mosaicml/mpt-30b-instruct,arc:challenge,25-shot,accuracy,0.5426621160409556,0.014558106543924067
mosaicml/mpt-30b-instruct,arc:challenge,25-shot,acc_norm,0.5844709897610921,0.014401366641216384
mosaicml/mpt-30b-instruct,hellaswag,10-shot,accuracy,0.652459669388568,0.004752158936871872
mosaicml/mpt-30b-instruct,hellaswag,10-shot,acc_norm,0.8430591515634336,0.0036300159898964026
mosaicml/mpt-30b-instruct,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.04725815626252606
mosaicml/mpt-30b-instruct,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.04725815626252606
mosaicml/mpt-30b-instruct,hendrycksTest-anatomy,5-shot,accuracy,0.4222222222222222,0.04266763404099582
mosaicml/mpt-30b-instruct,hendrycksTest-anatomy,5-shot,acc_norm,0.4222222222222222,0.04266763404099582
mosaicml/mpt-30b-instruct,hendrycksTest-astronomy,5-shot,accuracy,0.5,0.04068942293855797
mosaicml/mpt-30b-instruct,hendrycksTest-astronomy,5-shot,acc_norm,0.5,0.04068942293855797
mosaicml/mpt-30b-instruct,hendrycksTest-business_ethics,5-shot,accuracy,0.5,0.050251890762960605
mosaicml/mpt-30b-instruct,hendrycksTest-business_ethics,5-shot,acc_norm,0.5,0.050251890762960605
mosaicml/mpt-30b-instruct,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.5169811320754717,0.030755120364119905
mosaicml/mpt-30b-instruct,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.5169811320754717,0.030755120364119905
mosaicml/mpt-30b-instruct,hendrycksTest-college_biology,5-shot,accuracy,0.5347222222222222,0.04171115858181618
mosaicml/mpt-30b-instruct,hendrycksTest-college_biology,5-shot,acc_norm,0.5347222222222222,0.04171115858181618
mosaicml/mpt-30b-instruct,hendrycksTest-college_chemistry,5-shot,accuracy,0.32,0.04688261722621503
mosaicml/mpt-30b-instruct,hendrycksTest-college_chemistry,5-shot,acc_norm,0.32,0.04688261722621503
mosaicml/mpt-30b-instruct,hendrycksTest-college_computer_science,5-shot,accuracy,0.47,0.050161355804659205
mosaicml/mpt-30b-instruct,hendrycksTest-college_computer_science,5-shot,acc_norm,0.47,0.050161355804659205
mosaicml/mpt-30b-instruct,hendrycksTest-college_mathematics,5-shot,accuracy,0.33,0.047258156262526045
mosaicml/mpt-30b-instruct,hendrycksTest-college_mathematics,5-shot,acc_norm,0.33,0.047258156262526045
mosaicml/mpt-30b-instruct,hendrycksTest-college_medicine,5-shot,accuracy,0.4508670520231214,0.03794012674697028
mosaicml/mpt-30b-instruct,hendrycksTest-college_medicine,5-shot,acc_norm,0.4508670520231214,0.03794012674697028
mosaicml/mpt-30b-instruct,hendrycksTest-college_physics,5-shot,accuracy,0.35294117647058826,0.047551296160629475
mosaicml/mpt-30b-instruct,hendrycksTest-college_physics,5-shot,acc_norm,0.35294117647058826,0.047551296160629475
mosaicml/mpt-30b-instruct,hendrycksTest-computer_security,5-shot,accuracy,0.64,0.04824181513244218
mosaicml/mpt-30b-instruct,hendrycksTest-computer_security,5-shot,acc_norm,0.64,0.04824181513244218
mosaicml/mpt-30b-instruct,hendrycksTest-conceptual_physics,5-shot,accuracy,0.46382978723404256,0.032600385118357715
mosaicml/mpt-30b-instruct,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.46382978723404256,0.032600385118357715
mosaicml/mpt-30b-instruct,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.04266339443159394
mosaicml/mpt-30b-instruct,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.04266339443159394
mosaicml/mpt-30b-instruct,hendrycksTest-electrical_engineering,5-shot,accuracy,0.46206896551724136,0.041546596717075474
mosaicml/mpt-30b-instruct,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.46206896551724136,0.041546596717075474
mosaicml/mpt-30b-instruct,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.3201058201058201,0.0240268463928735
mosaicml/mpt-30b-instruct,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.3201058201058201,0.0240268463928735
mosaicml/mpt-30b-instruct,hendrycksTest-formal_logic,5-shot,accuracy,0.3492063492063492,0.04263906892795132
mosaicml/mpt-30b-instruct,hendrycksTest-formal_logic,5-shot,acc_norm,0.3492063492063492,0.04263906892795132
mosaicml/mpt-30b-instruct,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
mosaicml/mpt-30b-instruct,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_biology,5-shot,accuracy,0.532258064516129,0.028384747788813332
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_biology,5-shot,acc_norm,0.532258064516129,0.028384747788813332
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3793103448275862,0.03413963805906235
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3793103448275862,0.03413963805906235
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.51,0.05024183937956912
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.51,0.05024183937956912
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_european_history,5-shot,accuracy,0.6484848484848484,0.037282069986826503
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.6484848484848484,0.037282069986826503
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_geography,5-shot,accuracy,0.6464646464646465,0.03406086723547153
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_geography,5-shot,acc_norm,0.6464646464646465,0.03406086723547153
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6735751295336787,0.033840286211432945
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6735751295336787,0.033840286211432945
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.45897435897435895,0.025265525491284295
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.45897435897435895,0.025265525491284295
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.3296296296296296,0.028661201116524575
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.3296296296296296,0.028661201116524575
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.44537815126050423,0.0322841062671639
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.44537815126050423,0.0322841062671639
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_physics,5-shot,accuracy,0.32450331125827814,0.038227469376587525
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_physics,5-shot,acc_norm,0.32450331125827814,0.038227469376587525
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_psychology,5-shot,accuracy,0.6587155963302752,0.020328612816592446
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.6587155963302752,0.020328612816592446
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2916666666666667,0.03099866630456053
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2916666666666667,0.03099866630456053
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_us_history,5-shot,accuracy,0.7205882352941176,0.031493281045079556
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.7205882352941176,0.031493281045079556
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_world_history,5-shot,accuracy,0.6877637130801688,0.030165137867847015
mosaicml/mpt-30b-instruct,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.6877637130801688,0.030165137867847015
mosaicml/mpt-30b-instruct,hendrycksTest-human_aging,5-shot,accuracy,0.5919282511210763,0.03298574607842822
mosaicml/mpt-30b-instruct,hendrycksTest-human_aging,5-shot,acc_norm,0.5919282511210763,0.03298574607842822
mosaicml/mpt-30b-instruct,hendrycksTest-human_sexuality,5-shot,accuracy,0.5877862595419847,0.04317171194870254
mosaicml/mpt-30b-instruct,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5877862595419847,0.04317171194870254
mosaicml/mpt-30b-instruct,hendrycksTest-international_law,5-shot,accuracy,0.4462809917355372,0.0453793517794788
mosaicml/mpt-30b-instruct,hendrycksTest-international_law,5-shot,acc_norm,0.4462809917355372,0.0453793517794788
mosaicml/mpt-30b-instruct,hendrycksTest-jurisprudence,5-shot,accuracy,0.5740740740740741,0.047803436269367894
mosaicml/mpt-30b-instruct,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5740740740740741,0.047803436269367894
mosaicml/mpt-30b-instruct,hendrycksTest-logical_fallacies,5-shot,accuracy,0.49079754601226994,0.039277056007874414
mosaicml/mpt-30b-instruct,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.49079754601226994,0.039277056007874414
mosaicml/mpt-30b-instruct,hendrycksTest-machine_learning,5-shot,accuracy,0.375,0.04595091388086298
mosaicml/mpt-30b-instruct,hendrycksTest-machine_learning,5-shot,acc_norm,0.375,0.04595091388086298
mosaicml/mpt-30b-instruct,hendrycksTest-management,5-shot,accuracy,0.6116504854368932,0.0482572933735639
mosaicml/mpt-30b-instruct,hendrycksTest-management,5-shot,acc_norm,0.6116504854368932,0.0482572933735639
mosaicml/mpt-30b-instruct,hendrycksTest-marketing,5-shot,accuracy,0.7051282051282052,0.02987257770889119
mosaicml/mpt-30b-instruct,hendrycksTest-marketing,5-shot,acc_norm,0.7051282051282052,0.02987257770889119
mosaicml/mpt-30b-instruct,hendrycksTest-medical_genetics,5-shot,accuracy,0.46,0.05009082659620333
mosaicml/mpt-30b-instruct,hendrycksTest-medical_genetics,5-shot,acc_norm,0.46,0.05009082659620333
mosaicml/mpt-30b-instruct,hendrycksTest-miscellaneous,5-shot,accuracy,0.6922094508301405,0.01650604504515564
mosaicml/mpt-30b-instruct,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6922094508301405,0.01650604504515564
mosaicml/mpt-30b-instruct,hendrycksTest-moral_disputes,5-shot,accuracy,0.49710982658959535,0.026918645383239015
mosaicml/mpt-30b-instruct,hendrycksTest-moral_disputes,5-shot,acc_norm,0.49710982658959535,0.026918645383239015
mosaicml/mpt-30b-instruct,hendrycksTest-moral_scenarios,5-shot,accuracy,0.3027932960893855,0.015366860386397112
mosaicml/mpt-30b-instruct,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.3027932960893855,0.015366860386397112
mosaicml/mpt-30b-instruct,hendrycksTest-nutrition,5-shot,accuracy,0.5163398692810458,0.028614624752805434
mosaicml/mpt-30b-instruct,hendrycksTest-nutrition,5-shot,acc_norm,0.5163398692810458,0.028614624752805434
mosaicml/mpt-30b-instruct,hendrycksTest-philosophy,5-shot,accuracy,0.5659163987138264,0.028150232244535594
mosaicml/mpt-30b-instruct,hendrycksTest-philosophy,5-shot,acc_norm,0.5659163987138264,0.028150232244535594
mosaicml/mpt-30b-instruct,hendrycksTest-prehistory,5-shot,accuracy,0.5771604938271605,0.027487472980871588
mosaicml/mpt-30b-instruct,hendrycksTest-prehistory,5-shot,acc_norm,0.5771604938271605,0.027487472980871588
mosaicml/mpt-30b-instruct,hendrycksTest-professional_accounting,5-shot,accuracy,0.3900709219858156,0.02909767559946393
mosaicml/mpt-30b-instruct,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3900709219858156,0.02909767559946393
mosaicml/mpt-30b-instruct,hendrycksTest-professional_law,5-shot,accuracy,0.37027379400260757,0.012332930781256728
mosaicml/mpt-30b-instruct,hendrycksTest-professional_law,5-shot,acc_norm,0.37027379400260757,0.012332930781256728
mosaicml/mpt-30b-instruct,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121596
mosaicml/mpt-30b-instruct,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.030211479609121596
mosaicml/mpt-30b-instruct,hendrycksTest-professional_psychology,5-shot,accuracy,0.49019607843137253,0.020223946005074305
mosaicml/mpt-30b-instruct,hendrycksTest-professional_psychology,5-shot,acc_norm,0.49019607843137253,0.020223946005074305
mosaicml/mpt-30b-instruct,hendrycksTest-public_relations,5-shot,accuracy,0.5181818181818182,0.04785964010794916
mosaicml/mpt-30b-instruct,hendrycksTest-public_relations,5-shot,acc_norm,0.5181818181818182,0.04785964010794916
mosaicml/mpt-30b-instruct,hendrycksTest-security_studies,5-shot,accuracy,0.563265306122449,0.031751952375833226
mosaicml/mpt-30b-instruct,hendrycksTest-security_studies,5-shot,acc_norm,0.563265306122449,0.031751952375833226
mosaicml/mpt-30b-instruct,hendrycksTest-sociology,5-shot,accuracy,0.5522388059701493,0.03516184772952167
mosaicml/mpt-30b-instruct,hendrycksTest-sociology,5-shot,acc_norm,0.5522388059701493,0.03516184772952167
mosaicml/mpt-30b-instruct,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.69,0.04648231987117316
mosaicml/mpt-30b-instruct,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.69,0.04648231987117316
mosaicml/mpt-30b-instruct,hendrycksTest-virology,5-shot,accuracy,0.463855421686747,0.03882310850890593
mosaicml/mpt-30b-instruct,hendrycksTest-virology,5-shot,acc_norm,0.463855421686747,0.03882310850890593
mosaicml/mpt-30b-instruct,hendrycksTest-world_religions,5-shot,accuracy,0.6666666666666666,0.03615507630310935
mosaicml/mpt-30b-instruct,hendrycksTest-world_religions,5-shot,acc_norm,0.6666666666666666,0.03615507630310935
mosaicml/mpt-30b-instruct,truthfulqa:mc,0-shot,mc1,0.2582619339045288,0.0153218216884762
mosaicml/mpt-30b-instruct,truthfulqa:mc,0-shot,mc2,0.3804643219400445,0.015216520266283156
mosaicml/mpt-30b-instruct,drop,3-shot,accuracy,0.3308515100671141,0.004818562129043009
mosaicml/mpt-30b-instruct,drop,3-shot,f1,0.38283766778523554,0.00472140525052066
mosaicml/mpt-30b-instruct,gsm8k,5-shot,accuracy,0.15314632297194844,0.009919728152791466
mosaicml/mpt-30b-instruct,winogrande,5-shot,accuracy,0.7513812154696132,0.012147314713403112
facebook/xglm-564M,arc:challenge,25-shot,accuracy,0.19795221843003413,0.011643990971573395
facebook/xglm-564M,arc:challenge,25-shot,acc_norm,0.24573378839590443,0.012581033453730107
facebook/xglm-564M,hellaswag,10-shot,accuracy,0.30481975702051384,0.004593902601979336
facebook/xglm-564M,hellaswag,10-shot,acc_norm,0.34933280223063135,0.004757849023411965
facebook/xglm-564M,hendrycksTest-abstract_algebra,5-shot,accuracy,0.2,0.04020151261036846
facebook/xglm-564M,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.2,0.04020151261036846
facebook/xglm-564M,hendrycksTest-anatomy,5-shot,accuracy,0.3111111111111111,0.03999262876617722
facebook/xglm-564M,hendrycksTest-anatomy,5-shot,acc_norm,0.3111111111111111,0.03999262876617722
facebook/xglm-564M,hendrycksTest-astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
facebook/xglm-564M,hendrycksTest-astronomy,5-shot,acc_norm,0.17763157894736842,0.031103182383123398
facebook/xglm-564M,hendrycksTest-business_ethics,5-shot,accuracy,0.27,0.044619604333847394
facebook/xglm-564M,hendrycksTest-business_ethics,5-shot,acc_norm,0.27,0.044619604333847394
facebook/xglm-564M,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2037735849056604,0.0247907845017754
facebook/xglm-564M,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2037735849056604,0.0247907845017754
facebook/xglm-564M,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.03476590104304134
facebook/xglm-564M,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.03476590104304134
facebook/xglm-564M,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036845
facebook/xglm-564M,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036845
facebook/xglm-564M,hendrycksTest-college_computer_science,5-shot,accuracy,0.15,0.03588702812826372
facebook/xglm-564M,hendrycksTest-college_computer_science,5-shot,acc_norm,0.15,0.03588702812826372
facebook/xglm-564M,hendrycksTest-college_mathematics,5-shot,accuracy,0.23,0.04229525846816506
facebook/xglm-564M,hendrycksTest-college_mathematics,5-shot,acc_norm,0.23,0.04229525846816506
facebook/xglm-564M,hendrycksTest-college_medicine,5-shot,accuracy,0.2138728323699422,0.031265112061730424
facebook/xglm-564M,hendrycksTest-college_medicine,5-shot,acc_norm,0.2138728323699422,0.031265112061730424
facebook/xglm-564M,hendrycksTest-college_physics,5-shot,accuracy,0.22549019607843138,0.041583075330832865
facebook/xglm-564M,hendrycksTest-college_physics,5-shot,acc_norm,0.22549019607843138,0.041583075330832865
facebook/xglm-564M,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.04229525846816506
facebook/xglm-564M,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.04229525846816506
facebook/xglm-564M,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3148936170212766,0.030363582197238167
facebook/xglm-564M,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3148936170212766,0.030363582197238167
facebook/xglm-564M,hendrycksTest-econometrics,5-shot,accuracy,0.2719298245614035,0.04185774424022056
facebook/xglm-564M,hendrycksTest-econometrics,5-shot,acc_norm,0.2719298245614035,0.04185774424022056
facebook/xglm-564M,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.036646663372252565
facebook/xglm-564M,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.036646663372252565
facebook/xglm-564M,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2619047619047619,0.022644212615525218
facebook/xglm-564M,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2619047619047619,0.022644212615525218
facebook/xglm-564M,hendrycksTest-formal_logic,5-shot,accuracy,0.14285714285714285,0.03129843185743808
facebook/xglm-564M,hendrycksTest-formal_logic,5-shot,acc_norm,0.14285714285714285,0.03129843185743808
facebook/xglm-564M,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.038612291966536934
facebook/xglm-564M,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.038612291966536934
facebook/xglm-564M,hendrycksTest-high_school_biology,5-shot,accuracy,0.2709677419354839,0.025284416114900156
facebook/xglm-564M,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2709677419354839,0.025284416114900156
facebook/xglm-564M,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2955665024630542,0.032104944337514575
facebook/xglm-564M,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2955665024630542,0.032104944337514575
facebook/xglm-564M,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.04760952285695235
facebook/xglm-564M,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.04760952285695235
facebook/xglm-564M,hendrycksTest-high_school_european_history,5-shot,accuracy,0.26666666666666666,0.03453131801885415
facebook/xglm-564M,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.26666666666666666,0.03453131801885415
facebook/xglm-564M,hendrycksTest-high_school_geography,5-shot,accuracy,0.19696969696969696,0.028335609732463355
facebook/xglm-564M,hendrycksTest-high_school_geography,5-shot,acc_norm,0.19696969696969696,0.028335609732463355
facebook/xglm-564M,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2694300518134715,0.03201867122877793
facebook/xglm-564M,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2694300518134715,0.03201867122877793
facebook/xglm-564M,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2128205128205128,0.020752423722128002
facebook/xglm-564M,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2128205128205128,0.020752423722128002
facebook/xglm-564M,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24814814814814815,0.0263357394040558
facebook/xglm-564M,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24814814814814815,0.0263357394040558
facebook/xglm-564M,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23949579831932774,0.02772206549336127
facebook/xglm-564M,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23949579831932774,0.02772206549336127
facebook/xglm-564M,hendrycksTest-high_school_physics,5-shot,accuracy,0.1986754966887417,0.03257847384436775
facebook/xglm-564M,hendrycksTest-high_school_physics,5-shot,acc_norm,0.1986754966887417,0.03257847384436775
facebook/xglm-564M,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1871559633027523,0.01672268452620016
facebook/xglm-564M,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1871559633027523,0.01672268452620016
facebook/xglm-564M,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4537037037037037,0.033953227263757976
facebook/xglm-564M,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4537037037037037,0.033953227263757976
facebook/xglm-564M,hendrycksTest-high_school_us_history,5-shot,accuracy,0.28921568627450983,0.03182231867647554
facebook/xglm-564M,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.28921568627450983,0.03182231867647554
facebook/xglm-564M,hendrycksTest-high_school_world_history,5-shot,accuracy,0.24050632911392406,0.02782078198114968
facebook/xglm-564M,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.24050632911392406,0.02782078198114968
facebook/xglm-564M,hendrycksTest-human_aging,5-shot,accuracy,0.3273542600896861,0.03149384670994131
facebook/xglm-564M,hendrycksTest-human_aging,5-shot,acc_norm,0.3273542600896861,0.03149384670994131
facebook/xglm-564M,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
facebook/xglm-564M,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
facebook/xglm-564M,hendrycksTest-international_law,5-shot,accuracy,0.3305785123966942,0.04294340845212095
facebook/xglm-564M,hendrycksTest-international_law,5-shot,acc_norm,0.3305785123966942,0.04294340845212095
facebook/xglm-564M,hendrycksTest-jurisprudence,5-shot,accuracy,0.25925925925925924,0.042365112580946336
facebook/xglm-564M,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25925925925925924,0.042365112580946336
facebook/xglm-564M,hendrycksTest-logical_fallacies,5-shot,accuracy,0.27607361963190186,0.0351238528370505
facebook/xglm-564M,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.27607361963190186,0.0351238528370505
facebook/xglm-564M,hendrycksTest-machine_learning,5-shot,accuracy,0.33035714285714285,0.04464285714285713
facebook/xglm-564M,hendrycksTest-machine_learning,5-shot,acc_norm,0.33035714285714285,0.04464285714285713
facebook/xglm-564M,hendrycksTest-management,5-shot,accuracy,0.18446601941747573,0.03840423627288276
facebook/xglm-564M,hendrycksTest-management,5-shot,acc_norm,0.18446601941747573,0.03840423627288276
facebook/xglm-564M,hendrycksTest-marketing,5-shot,accuracy,0.2905982905982906,0.029745048572674036
facebook/xglm-564M,hendrycksTest-marketing,5-shot,acc_norm,0.2905982905982906,0.029745048572674036
facebook/xglm-564M,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
facebook/xglm-564M,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.046056618647183814
facebook/xglm-564M,hendrycksTest-miscellaneous,5-shot,accuracy,0.23754789272030652,0.015218733046150193
facebook/xglm-564M,hendrycksTest-miscellaneous,5-shot,acc_norm,0.23754789272030652,0.015218733046150193
facebook/xglm-564M,hendrycksTest-moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
facebook/xglm-564M,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24855491329479767,0.023267528432100174
facebook/xglm-564M,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
facebook/xglm-564M,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
facebook/xglm-564M,hendrycksTest-nutrition,5-shot,accuracy,0.21895424836601307,0.02367908986180772
facebook/xglm-564M,hendrycksTest-nutrition,5-shot,acc_norm,0.21895424836601307,0.02367908986180772
facebook/xglm-564M,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.02212243977248077
facebook/xglm-564M,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.02212243977248077
facebook/xglm-564M,hendrycksTest-prehistory,5-shot,accuracy,0.21604938271604937,0.022899162918445806
facebook/xglm-564M,hendrycksTest-prehistory,5-shot,acc_norm,0.21604938271604937,0.022899162918445806
facebook/xglm-564M,hendrycksTest-professional_accounting,5-shot,accuracy,0.2765957446808511,0.026684564340460997
facebook/xglm-564M,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2765957446808511,0.026684564340460997
facebook/xglm-564M,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.010996156635142692
facebook/xglm-564M,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.010996156635142692
facebook/xglm-564M,hendrycksTest-professional_medicine,5-shot,accuracy,0.44485294117647056,0.030187532060329376
facebook/xglm-564M,hendrycksTest-professional_medicine,5-shot,acc_norm,0.44485294117647056,0.030187532060329376
facebook/xglm-564M,hendrycksTest-professional_psychology,5-shot,accuracy,0.25,0.01751781884501444
facebook/xglm-564M,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25,0.01751781884501444
facebook/xglm-564M,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
facebook/xglm-564M,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
facebook/xglm-564M,hendrycksTest-security_studies,5-shot,accuracy,0.24081632653061225,0.027372942201788167
facebook/xglm-564M,hendrycksTest-security_studies,5-shot,acc_norm,0.24081632653061225,0.027372942201788167
facebook/xglm-564M,hendrycksTest-sociology,5-shot,accuracy,0.24378109452736318,0.030360490154014666
facebook/xglm-564M,hendrycksTest-sociology,5-shot,acc_norm,0.24378109452736318,0.030360490154014666
facebook/xglm-564M,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.23,0.042295258468165044
facebook/xglm-564M,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.23,0.042295258468165044
facebook/xglm-564M,hendrycksTest-virology,5-shot,accuracy,0.28313253012048195,0.03507295431370518
facebook/xglm-564M,hendrycksTest-virology,5-shot,acc_norm,0.28313253012048195,0.03507295431370518
facebook/xglm-564M,hendrycksTest-world_religions,5-shot,accuracy,0.23391812865497075,0.032467217651178264
facebook/xglm-564M,hendrycksTest-world_religions,5-shot,acc_norm,0.23391812865497075,0.032467217651178264
facebook/xglm-564M,truthfulqa:mc,0-shot,mc1,0.23255813953488372,0.014789157531080508
facebook/xglm-564M,truthfulqa:mc,0-shot,mc2,0.40432058770396273,0.01487339214667429
facebook/xglm-564M,drop,3-shot,accuracy,0.013422818791946308,0.0011784931108563684
facebook/xglm-564M,drop,3-shot,f1,0.060359689597315525,0.0017160396766447692
facebook/xglm-564M,gsm8k,5-shot,accuracy,0.016679302501895376,0.0035275958887224573
facebook/xglm-564M,winogrande,5-shot,accuracy,0.5130228887134964,0.014047718393997667
EleutherAI/gpt-neo-2.7B,minerva_math_precalc,5-shot,accuracy,0.029304029304029304,0.007224487305459686
EleutherAI/gpt-neo-2.7B,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458095,0.0034284443646836614
EleutherAI/gpt-neo-2.7B,minerva_math_num_theory,5-shot,accuracy,0.025925925925925925,0.0068449258444466055
EleutherAI/gpt-neo-2.7B,minerva_math_intermediate_algebra,5-shot,accuracy,0.019933554817275746,0.004653898088316119
EleutherAI/gpt-neo-2.7B,minerva_math_geometry,5-shot,accuracy,0.016701461377870562,0.005861462425818025
EleutherAI/gpt-neo-2.7B,minerva_math_counting_and_prob,5-shot,accuracy,0.012658227848101266,0.005140313889578842
EleutherAI/gpt-neo-2.7B,minerva_math_algebra,5-shot,accuracy,0.016006739679865205,0.0036442247924417257
EleutherAI/gpt-neo-2.7B,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
EleutherAI/gpt-neo-2.7B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
EleutherAI/gpt-neo-2.7B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
EleutherAI/gpt-neo-2.7B,arithmetic_2ds,5-shot,accuracy,0.012,0.0024353573624298214
EleutherAI/gpt-neo-2.7B,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339602
EleutherAI/gpt-neo-2.7B,arithmetic_4ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-2.7B,arithmetic_2dm,5-shot,accuracy,0.02,0.0031312780858980494
EleutherAI/gpt-neo-2.7B,arithmetic_2da,5-shot,accuracy,0.0065,0.0017973564602277766
EleutherAI/gpt-neo-2.7B,gsm8k_cot,5-shot,accuracy,0.01819560272934041,0.0036816118940738727
EleutherAI/gpt-neo-2.7B,anli_r2,0-shot,brier_score,0.8345390393015258,
EleutherAI/gpt-neo-2.7B,anli_r3,0-shot,brier_score,0.7689959499686069,
EleutherAI/gpt-neo-2.7B,anli_r1,0-shot,brier_score,0.8666257668901235,
EleutherAI/gpt-neo-2.7B,xnli_eu,0-shot,brier_score,1.1095833790701086,
EleutherAI/gpt-neo-2.7B,xnli_vi,0-shot,brier_score,0.7461864238715018,
EleutherAI/gpt-neo-2.7B,xnli_ru,0-shot,brier_score,0.7999015977427434,
EleutherAI/gpt-neo-2.7B,xnli_zh,0-shot,brier_score,0.9806971205870976,
EleutherAI/gpt-neo-2.7B,xnli_tr,0-shot,brier_score,0.8960590433909379,
EleutherAI/gpt-neo-2.7B,xnli_fr,0-shot,brier_score,0.8146332923707491,
EleutherAI/gpt-neo-2.7B,xnli_en,0-shot,brier_score,0.6628158350379858,
EleutherAI/gpt-neo-2.7B,xnli_ur,0-shot,brier_score,1.0717135016627397,
EleutherAI/gpt-neo-2.7B,xnli_ar,0-shot,brier_score,1.273910719834552,
EleutherAI/gpt-neo-2.7B,xnli_de,0-shot,brier_score,0.8583968193026806,
EleutherAI/gpt-neo-2.7B,xnli_hi,0-shot,brier_score,0.8196953013215778,
EleutherAI/gpt-neo-2.7B,xnli_es,0-shot,brier_score,0.8843937740416182,
EleutherAI/gpt-neo-2.7B,xnli_bg,0-shot,brier_score,0.8937202436821879,
EleutherAI/gpt-neo-2.7B,xnli_sw,0-shot,brier_score,0.7992106097885577,
EleutherAI/gpt-neo-2.7B,xnli_el,0-shot,brier_score,0.9270116356402289,
EleutherAI/gpt-neo-2.7B,xnli_th,0-shot,brier_score,0.7847320508164434,
EleutherAI/gpt-neo-2.7B,logiqa2,0-shot,brier_score,1.1416601553118555,
EleutherAI/gpt-neo-2.7B,mathqa,5-shot,brier_score,0.9872909262605601,
EleutherAI/gpt-neo-2.7B,lambada_standard,0-shot,perplexity,9.472999881770361,0.26574628102110154
EleutherAI/gpt-neo-2.7B,lambada_standard,0-shot,accuracy,0.5175625849019988,0.00696167909747916
EleutherAI/gpt-neo-2.7B,lambada_openai,0-shot,perplexity,5.625735040282958,0.13851851129115517
EleutherAI/gpt-neo-2.7B,lambada_openai,0-shot,accuracy,0.622355909179119,0.006754183076526671
EleutherAI/gpt-neo-2.7B,mmlu_world_religions,0-shot,accuracy,0.27485380116959063,0.03424042924691584
EleutherAI/gpt-neo-2.7B,mmlu_formal_logic,0-shot,accuracy,0.18253968253968253,0.03455071019102149
EleutherAI/gpt-neo-2.7B,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.02577311116963045
EleutherAI/gpt-neo-2.7B,mmlu_moral_scenarios,0-shot,accuracy,0.27039106145251396,0.014854993938010085
EleutherAI/gpt-neo-2.7B,mmlu_high_school_world_history,0-shot,accuracy,0.22362869198312235,0.02712329820522997
EleutherAI/gpt-neo-2.7B,mmlu_moral_disputes,0-shot,accuracy,0.24566473988439305,0.023176298203992005
EleutherAI/gpt-neo-2.7B,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
EleutherAI/gpt-neo-2.7B,mmlu_logical_fallacies,0-shot,accuracy,0.25766871165644173,0.03436150827846917
EleutherAI/gpt-neo-2.7B,mmlu_high_school_us_history,0-shot,accuracy,0.19607843137254902,0.027865942286639318
EleutherAI/gpt-neo-2.7B,mmlu_philosophy,0-shot,accuracy,0.3247588424437299,0.026596782287697046
EleutherAI/gpt-neo-2.7B,mmlu_jurisprudence,0-shot,accuracy,0.2962962962962963,0.04414343666854933
EleutherAI/gpt-neo-2.7B,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794088
EleutherAI/gpt-neo-2.7B,mmlu_high_school_european_history,0-shot,accuracy,0.23030303030303031,0.0328766675860349
EleutherAI/gpt-neo-2.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.26424870466321243,0.03182155050916646
EleutherAI/gpt-neo-2.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.027553614467863804
EleutherAI/gpt-neo-2.7B,mmlu_high_school_geography,0-shot,accuracy,0.3282828282828283,0.03345678422756775
EleutherAI/gpt-neo-2.7B,mmlu_high_school_psychology,0-shot,accuracy,0.30642201834862387,0.01976551722045852
EleutherAI/gpt-neo-2.7B,mmlu_public_relations,0-shot,accuracy,0.18181818181818182,0.036942843353378
EleutherAI/gpt-neo-2.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-neo-2.7B,mmlu_sociology,0-shot,accuracy,0.22885572139303484,0.02970528405677243
EleutherAI/gpt-neo-2.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3487179487179487,0.024162780284017717
EleutherAI/gpt-neo-2.7B,mmlu_security_studies,0-shot,accuracy,0.2816326530612245,0.02879518557429127
EleutherAI/gpt-neo-2.7B,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594726
EleutherAI/gpt-neo-2.7B,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.03915345408847836
EleutherAI/gpt-neo-2.7B,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.04142439719489361
EleutherAI/gpt-neo-2.7B,mmlu_miscellaneous,0-shot,accuracy,0.2388250319284802,0.015246803197398682
EleutherAI/gpt-neo-2.7B,mmlu_marketing,0-shot,accuracy,0.2606837606837607,0.028760348956523414
EleutherAI/gpt-neo-2.7B,mmlu_management,0-shot,accuracy,0.27184466019417475,0.044052680241409216
EleutherAI/gpt-neo-2.7B,mmlu_nutrition,0-shot,accuracy,0.3104575163398693,0.026493033225145894
EleutherAI/gpt-neo-2.7B,mmlu_medical_genetics,0-shot,accuracy,0.29,0.04560480215720684
EleutherAI/gpt-neo-2.7B,mmlu_human_aging,0-shot,accuracy,0.18385650224215247,0.02599837909235651
EleutherAI/gpt-neo-2.7B,mmlu_professional_medicine,0-shot,accuracy,0.43014705882352944,0.030074971917302875
EleutherAI/gpt-neo-2.7B,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.032424147574830996
EleutherAI/gpt-neo-2.7B,mmlu_business_ethics,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-neo-2.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.2641509433962264,0.027134291628741713
EleutherAI/gpt-neo-2.7B,mmlu_global_facts,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/gpt-neo-2.7B,mmlu_virology,0-shot,accuracy,0.3253012048192771,0.03647168523683229
EleutherAI/gpt-neo-2.7B,mmlu_professional_accounting,0-shot,accuracy,0.2553191489361702,0.026011992930902013
EleutherAI/gpt-neo-2.7B,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237657
EleutherAI/gpt-neo-2.7B,mmlu_high_school_physics,0-shot,accuracy,0.23841059602649006,0.0347918557259966
EleutherAI/gpt-neo-2.7B,mmlu_high_school_biology,0-shot,accuracy,0.24838709677419354,0.024580028921481003
EleutherAI/gpt-neo-2.7B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
EleutherAI/gpt-neo-2.7B,mmlu_anatomy,0-shot,accuracy,0.2,0.03455473702325436
EleutherAI/gpt-neo-2.7B,mmlu_college_chemistry,0-shot,accuracy,0.22,0.04163331998932269
EleutherAI/gpt-neo-2.7B,mmlu_computer_security,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/gpt-neo-2.7B,mmlu_college_computer_science,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/gpt-neo-2.7B,mmlu_astronomy,0-shot,accuracy,0.19078947368421054,0.031975658210325004
EleutherAI/gpt-neo-2.7B,mmlu_college_mathematics,0-shot,accuracy,0.28,0.04512608598542127
EleutherAI/gpt-neo-2.7B,mmlu_conceptual_physics,0-shot,accuracy,0.2765957446808511,0.029241883869628817
EleutherAI/gpt-neo-2.7B,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.042923469599092816
EleutherAI/gpt-neo-2.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.35,0.0479372485441102
EleutherAI/gpt-neo-2.7B,mmlu_machine_learning,0-shot,accuracy,0.16964285714285715,0.0356236785009539
EleutherAI/gpt-neo-2.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.2413793103448276,0.030108330718011625
EleutherAI/gpt-neo-2.7B,mmlu_high_school_statistics,0-shot,accuracy,0.4074074074074074,0.03350991604696043
EleutherAI/gpt-neo-2.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.29365079365079366,0.02345603738398203
EleutherAI/gpt-neo-2.7B,mmlu_electrical_engineering,0-shot,accuracy,0.25517241379310346,0.03632984052707842
EleutherAI/gpt-neo-2.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.24814814814814815,0.0263357394040558
EleutherAI/gpt-neo-2.7B,arc_challenge,25-shot,accuracy,0.3199658703071672,0.013631345807016196
EleutherAI/gpt-neo-2.7B,arc_challenge,25-shot,acc_norm,0.34897610921501704,0.013928933461382504
EleutherAI/gpt-neo-2.7B,truthfulqa_mc2,0-shot,accuracy,0.39862631918419716,0.014042065906814954
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,bleu_max,24.047333287791275,0.7526012282674005
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,bleu_acc,0.35862913096695226,0.016789289499502022
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,bleu_diff,-2.089423502619666,0.8400708361193482
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge1_max,49.67982165948041,0.8665227117461761
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge1_acc,0.34149326805385555,0.01660068861995083
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge1_diff,-1.963312198645994,1.0594843696396452
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge2_max,32.68414904349889,1.029790643453569
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge2_acc,0.26193390452876375,0.015392118805015016
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rouge2_diff,-3.8823758442208036,1.1699264928139463
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rougeL_max,46.885237240783155,0.885979367776478
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rougeL_acc,0.3427172582619339,0.016614949385347046
EleutherAI/gpt-neo-2.7B,truthfulqa_gen,0-shot,rougeL_diff,-2.1065494987322304,1.0736368489730006
EleutherAI/gpt-neo-2.7B,truthfulqa_mc1,0-shot,accuracy,0.2386780905752754,0.014922629695456416
jisukim8873/falcon-7B-case-2,arc:challenge,25-shot,accuracy,0.4334470989761092,0.014481376224558896
jisukim8873/falcon-7B-case-2,arc:challenge,25-shot,acc_norm,0.4718430034129693,0.0145882041051022
jisukim8873/falcon-7B-case-2,hellaswag,10-shot,accuracy,0.5975901214897431,0.00489381489020832
jisukim8873/falcon-7B-case-2,hellaswag,10-shot,acc_norm,0.7847042421828321,0.004101873407354699
jisukim8873/falcon-7B-case-2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.04461960433384739
jisukim8873/falcon-7B-case-2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.04461960433384739
jisukim8873/falcon-7B-case-2,hendrycksTest-anatomy,5-shot,accuracy,0.3037037037037037,0.03972552884785136
jisukim8873/falcon-7B-case-2,hendrycksTest-anatomy,5-shot,acc_norm,0.3037037037037037,0.03972552884785136
jisukim8873/falcon-7B-case-2,hendrycksTest-astronomy,5-shot,accuracy,0.20394736842105263,0.0327900040631005
jisukim8873/falcon-7B-case-2,hendrycksTest-astronomy,5-shot,acc_norm,0.20394736842105263,0.0327900040631005
jisukim8873/falcon-7B-case-2,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.0440844002276808
jisukim8873/falcon-7B-case-2,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.0440844002276808
jisukim8873/falcon-7B-case-2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.32452830188679244,0.028815615713432115
jisukim8873/falcon-7B-case-2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.32452830188679244,0.028815615713432115
jisukim8873/falcon-7B-case-2,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.03716177437566016
jisukim8873/falcon-7B-case-2,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.03716177437566016
jisukim8873/falcon-7B-case-2,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.03775251680686371
jisukim8873/falcon-7B-case-2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.03775251680686371
jisukim8873/falcon-7B-case-2,hendrycksTest-college_computer_science,5-shot,accuracy,0.27,0.044619604333847394
jisukim8873/falcon-7B-case-2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.27,0.044619604333847394
jisukim8873/falcon-7B-case-2,hendrycksTest-college_mathematics,5-shot,accuracy,0.24,0.04292346959909283
jisukim8873/falcon-7B-case-2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.24,0.04292346959909283
jisukim8873/falcon-7B-case-2,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.033687629322594316
jisukim8873/falcon-7B-case-2,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.033687629322594316
jisukim8873/falcon-7B-case-2,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.043364327079931785
jisukim8873/falcon-7B-case-2,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.043364327079931785
jisukim8873/falcon-7B-case-2,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.04878317312145633
jisukim8873/falcon-7B-case-2,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.04878317312145633
jisukim8873/falcon-7B-case-2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3191489361702128,0.03047297336338005
jisukim8873/falcon-7B-case-2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3191489361702128,0.03047297336338005
jisukim8873/falcon-7B-case-2,hendrycksTest-econometrics,5-shot,accuracy,0.30701754385964913,0.04339138322579861
jisukim8873/falcon-7B-case-2,hendrycksTest-econometrics,5-shot,acc_norm,0.30701754385964913,0.04339138322579861
jisukim8873/falcon-7B-case-2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3103448275862069,0.038552896163789485
jisukim8873/falcon-7B-case-2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3103448275862069,0.038552896163789485
jisukim8873/falcon-7B-case-2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.26455026455026454,0.022717467897708628
jisukim8873/falcon-7B-case-2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.26455026455026454,0.022717467897708628
jisukim8873/falcon-7B-case-2,hendrycksTest-formal_logic,5-shot,accuracy,0.1746031746031746,0.03395490020856109
jisukim8873/falcon-7B-case-2,hendrycksTest-formal_logic,5-shot,acc_norm,0.1746031746031746,0.03395490020856109
jisukim8873/falcon-7B-case-2,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.039427724440366234
jisukim8873/falcon-7B-case-2,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.039427724440366234
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_biology,5-shot,accuracy,0.3258064516129032,0.026662010578567104
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3258064516129032,0.026662010578567104
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3103448275862069,0.03255086769970103
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3103448275862069,0.03255086769970103
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.04512608598542128
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.04512608598542128
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3393939393939394,0.03697442205031596
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3393939393939394,0.03697442205031596
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_geography,5-shot,accuracy,0.30303030303030304,0.03274287914026868
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.30303030303030304,0.03274287914026868
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.24352331606217617,0.03097543638684543
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.24352331606217617,0.03097543638684543
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2564102564102564,0.022139081103971545
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2564102564102564,0.022139081103971545
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.29259259259259257,0.02773896963217609
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.29259259259259257,0.02773896963217609
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2773109243697479,0.02907937453948001
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2773109243697479,0.02907937453948001
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_physics,5-shot,accuracy,0.24503311258278146,0.035118075718047245
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.24503311258278146,0.035118075718047245
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.28440366972477066,0.019342036587702584
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.28440366972477066,0.019342036587702584
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.19907407407407407,0.02723229846269021
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.19907407407407407,0.02723229846269021
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.28921568627450983,0.03182231867647553
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.28921568627450983,0.03182231867647553
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3080168776371308,0.030052389335605702
jisukim8873/falcon-7B-case-2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3080168776371308,0.030052389335605702
jisukim8873/falcon-7B-case-2,hendrycksTest-human_aging,5-shot,accuracy,0.3991031390134529,0.032867453125679603
jisukim8873/falcon-7B-case-2,hendrycksTest-human_aging,5-shot,acc_norm,0.3991031390134529,0.032867453125679603
jisukim8873/falcon-7B-case-2,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.037276735755969195
jisukim8873/falcon-7B-case-2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.037276735755969195
jisukim8873/falcon-7B-case-2,hendrycksTest-international_law,5-shot,accuracy,0.34710743801652894,0.04345724570292535
jisukim8873/falcon-7B-case-2,hendrycksTest-international_law,5-shot,acc_norm,0.34710743801652894,0.04345724570292535
jisukim8873/falcon-7B-case-2,hendrycksTest-jurisprudence,5-shot,accuracy,0.28703703703703703,0.04373313040914761
jisukim8873/falcon-7B-case-2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.28703703703703703,0.04373313040914761
jisukim8873/falcon-7B-case-2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.294478527607362,0.03581165790474082
jisukim8873/falcon-7B-case-2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.294478527607362,0.03581165790474082
jisukim8873/falcon-7B-case-2,hendrycksTest-machine_learning,5-shot,accuracy,0.35714285714285715,0.04547960999764376
jisukim8873/falcon-7B-case-2,hendrycksTest-machine_learning,5-shot,acc_norm,0.35714285714285715,0.04547960999764376
jisukim8873/falcon-7B-case-2,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.043012503996908764
jisukim8873/falcon-7B-case-2,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.043012503996908764
jisukim8873/falcon-7B-case-2,hendrycksTest-marketing,5-shot,accuracy,0.36752136752136755,0.03158539157745637
jisukim8873/falcon-7B-case-2,hendrycksTest-marketing,5-shot,acc_norm,0.36752136752136755,0.03158539157745637
jisukim8873/falcon-7B-case-2,hendrycksTest-medical_genetics,5-shot,accuracy,0.33,0.04725815626252604
jisukim8873/falcon-7B-case-2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.33,0.04725815626252604
jisukim8873/falcon-7B-case-2,hendrycksTest-miscellaneous,5-shot,accuracy,0.36015325670498083,0.017166362471369295
jisukim8873/falcon-7B-case-2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.36015325670498083,0.017166362471369295
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_disputes,5-shot,accuracy,0.32947976878612717,0.025305258131879702
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.32947976878612717,0.025305258131879702
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2871508379888268,0.015131608849963759
jisukim8873/falcon-7B-case-2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2871508379888268,0.015131608849963759
jisukim8873/falcon-7B-case-2,hendrycksTest-nutrition,5-shot,accuracy,0.32679738562091504,0.02685729466328142
jisukim8873/falcon-7B-case-2,hendrycksTest-nutrition,5-shot,acc_norm,0.32679738562091504,0.02685729466328142
jisukim8873/falcon-7B-case-2,hendrycksTest-philosophy,5-shot,accuracy,0.31189710610932475,0.02631185807185416
jisukim8873/falcon-7B-case-2,hendrycksTest-philosophy,5-shot,acc_norm,0.31189710610932475,0.02631185807185416
jisukim8873/falcon-7B-case-2,hendrycksTest-prehistory,5-shot,accuracy,0.24691358024691357,0.02399350170904211
jisukim8873/falcon-7B-case-2,hendrycksTest-prehistory,5-shot,acc_norm,0.24691358024691357,0.02399350170904211
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.026789172351140245
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.026789172351140245
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_law,5-shot,accuracy,0.25749674054758803,0.011167706014904156
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_law,5-shot,acc_norm,0.25749674054758803,0.011167706014904156
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_medicine,5-shot,accuracy,0.20588235294117646,0.024562204314142314
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.20588235294117646,0.024562204314142314
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_psychology,5-shot,accuracy,0.2647058823529412,0.01784808957491322
jisukim8873/falcon-7B-case-2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2647058823529412,0.01784808957491322
jisukim8873/falcon-7B-case-2,hendrycksTest-public_relations,5-shot,accuracy,0.32727272727272727,0.04494290866252089
jisukim8873/falcon-7B-case-2,hendrycksTest-public_relations,5-shot,acc_norm,0.32727272727272727,0.04494290866252089
jisukim8873/falcon-7B-case-2,hendrycksTest-security_studies,5-shot,accuracy,0.2163265306122449,0.026358916334904038
jisukim8873/falcon-7B-case-2,hendrycksTest-security_studies,5-shot,acc_norm,0.2163265306122449,0.026358916334904038
jisukim8873/falcon-7B-case-2,hendrycksTest-sociology,5-shot,accuracy,0.2835820895522388,0.031871875379197986
jisukim8873/falcon-7B-case-2,hendrycksTest-sociology,5-shot,acc_norm,0.2835820895522388,0.031871875379197986
jisukim8873/falcon-7B-case-2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.41,0.04943110704237102
jisukim8873/falcon-7B-case-2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.41,0.04943110704237102
jisukim8873/falcon-7B-case-2,hendrycksTest-virology,5-shot,accuracy,0.3795180722891566,0.03777798822748017
jisukim8873/falcon-7B-case-2,hendrycksTest-virology,5-shot,acc_norm,0.3795180722891566,0.03777798822748017
jisukim8873/falcon-7B-case-2,hendrycksTest-world_religions,5-shot,accuracy,0.3391812865497076,0.036310534964889056
jisukim8873/falcon-7B-case-2,hendrycksTest-world_religions,5-shot,acc_norm,0.3391812865497076,0.036310534964889056
jisukim8873/falcon-7B-case-2,truthfulqa:mc,0-shot,mc1,0.26193390452876375,0.01539211880501503
jisukim8873/falcon-7B-case-2,truthfulqa:mc,0-shot,mc2,0.3862844409155128,0.014439073256995538
jisukim8873/falcon-7B-case-2,winogrande,5-shot,accuracy,0.7040252565114443,0.012829348226339014
jisukim8873/falcon-7B-case-2,gsm8k,5-shot,accuracy,0.06974981046247157,0.007016389571013849
jisukim8873/falcon-7B-case-2,minerva_math_precalc,5-shot,accuracy,0.009157509157509158,0.004080306065049012
jisukim8873/falcon-7B-case-2,minerva_math_prealgebra,5-shot,accuracy,0.02525832376578645,0.005319703220303023
jisukim8873/falcon-7B-case-2,minerva_math_num_theory,5-shot,accuracy,0.009259259259259259,0.00412547301549024
jisukim8873/falcon-7B-case-2,minerva_math_intermediate_algebra,5-shot,accuracy,0.012181616832779624,0.0036524791938863832
jisukim8873/falcon-7B-case-2,minerva_math_geometry,5-shot,accuracy,0.022964509394572025,0.006851249878769243
jisukim8873/falcon-7B-case-2,minerva_math_counting_and_prob,5-shot,accuracy,0.008438818565400843,0.0042060072077130545
jisukim8873/falcon-7B-case-2,minerva_math_algebra,5-shot,accuracy,0.016006739679865205,0.0036442247924417305
jisukim8873/falcon-7B-case-2,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-2,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-2,arithmetic_3da,5-shot,accuracy,0.033,0.003995432609977368
jisukim8873/falcon-7B-case-2,arithmetic_3ds,5-shot,accuracy,0.0555,0.005120838456077828
jisukim8873/falcon-7B-case-2,arithmetic_4da,5-shot,accuracy,0.002,0.000999249343069495
jisukim8873/falcon-7B-case-2,arithmetic_2ds,5-shot,accuracy,0.3135,0.010376064107029178
jisukim8873/falcon-7B-case-2,arithmetic_5ds,5-shot,accuracy,0.0025,0.0011169148353275362
jisukim8873/falcon-7B-case-2,arithmetic_5da,5-shot,accuracy,0.0005,0.0005000000000000087
jisukim8873/falcon-7B-case-2,arithmetic_1dc,5-shot,accuracy,0.092,0.006464433033702537
jisukim8873/falcon-7B-case-2,arithmetic_4ds,5-shot,accuracy,0.004,0.0014117352790977043
jisukim8873/falcon-7B-case-2,arithmetic_2dm,5-shot,accuracy,0.1035,0.006813008406113383
jisukim8873/falcon-7B-case-2,arithmetic_2da,5-shot,accuracy,0.147,0.007920029256998863
jisukim8873/falcon-7B-case-2,gsm8k_cot,5-shot,accuracy,0.10462471569370735,0.008430668082029294
jisukim8873/falcon-7B-case-2,anli_r2,0-shot,brier_score,0.9517210089818693,
jisukim8873/falcon-7B-case-2,anli_r3,0-shot,brier_score,0.9091491237138922,
jisukim8873/falcon-7B-case-2,anli_r1,0-shot,brier_score,0.9740803307547983,
jisukim8873/falcon-7B-case-2,xnli_eu,0-shot,brier_score,1.039626455615514,
jisukim8873/falcon-7B-case-2,xnli_vi,0-shot,brier_score,0.9944426502362462,
jisukim8873/falcon-7B-case-2,xnli_ru,0-shot,brier_score,0.8204429465406277,
jisukim8873/falcon-7B-case-2,xnli_zh,0-shot,brier_score,1.0121312596962848,
jisukim8873/falcon-7B-case-2,xnli_tr,0-shot,brier_score,0.9511392449037739,
jisukim8873/falcon-7B-case-2,xnli_fr,0-shot,brier_score,0.7485518412292533,
jisukim8873/falcon-7B-case-2,xnli_en,0-shot,brier_score,0.6574942705526986,
jisukim8873/falcon-7B-case-2,xnli_ur,0-shot,brier_score,1.3224866657294507,
jisukim8873/falcon-7B-case-2,xnli_ar,0-shot,brier_score,1.2550232606263443,
jisukim8873/falcon-7B-case-2,xnli_de,0-shot,brier_score,0.8394366588628737,
jisukim8873/falcon-7B-case-2,xnli_hi,0-shot,brier_score,1.1395961000527088,
jisukim8873/falcon-7B-case-2,xnli_es,0-shot,brier_score,0.8186718184732203,
jisukim8873/falcon-7B-case-2,xnli_bg,0-shot,brier_score,0.9250478368056083,
jisukim8873/falcon-7B-case-2,xnli_sw,0-shot,brier_score,1.106399353671628,
jisukim8873/falcon-7B-case-2,xnli_el,0-shot,brier_score,0.9901456477406901,
jisukim8873/falcon-7B-case-2,xnli_th,0-shot,brier_score,0.9661701648978412,
jisukim8873/falcon-7B-case-2,logiqa2,0-shot,brier_score,1.0772771409734918,
jisukim8873/falcon-7B-case-2,mathqa,5-shot,brier_score,0.9470815194657676,
jisukim8873/falcon-7B-case-2,lambada_standard,0-shot,perplexity,4.151489270057361,0.0898434559924617
jisukim8873/falcon-7B-case-2,lambada_standard,0-shot,accuracy,0.6600038812342325,0.006599671169668108
jisukim8873/falcon-7B-case-2,lambada_openai,0-shot,perplexity,3.3084712739580997,0.06869330646899366
jisukim8873/falcon-7B-case-2,lambada_openai,0-shot,accuracy,0.7345235784979623,0.006152164239586487
llama2_220M_nl_40_code_60,minerva_math_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_geometry,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_num_theory,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_prealgebra,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,minerva_math_precalc,5-shot,accuracy,0.005494505494505495,0.0031664282264934705
llama2_220M_nl_40_code_60,gsm8k,5-shot,accuracy,0.017437452615617893,0.0036054868679982494
llama2_220M_nl_40_code_60,gsm8k_cot,5-shot,accuracy,0.018953752843062926,0.003756078341031472
llama2_220M_nl_40_code_60,fld_default,0-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_2ds,5-shot,accuracy,0.0245,0.003457723662536226
llama2_220M_nl_40_code_60,arithmetic_5da,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339296
llama2_220M_nl_40_code_60,arithmetic_4ds,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_2da,5-shot,accuracy,0.007,0.001864735536023767
llama2_220M_nl_40_code_60,arithmetic_5ds,5-shot,accuracy,0.0,
llama2_220M_nl_40_code_60,arithmetic_2dm,5-shot,accuracy,0.0225,0.0033169829948455193
llama2_220M_nl_40_code_60,arithmetic_1dc,5-shot,accuracy,0.03,0.003815400193861725
llama2_220M_nl_40_code_60,arithmetic_3ds,5-shot,accuracy,0.003,0.0012232122154646802
llama2_220M_nl_40_code_60,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000152
llama2_220M_nl_40_code_60,xnli_ar,0-shot,brier_score,0.8526083493571404,
llama2_220M_nl_40_code_60,xnli_bg,0-shot,brier_score,1.109603855891853,
llama2_220M_nl_40_code_60,xnli_de,0-shot,brier_score,0.9675590091388491,
llama2_220M_nl_40_code_60,xnli_el,0-shot,brier_score,1.2587457676986242,
llama2_220M_nl_40_code_60,xnli_en,0-shot,brier_score,0.8104211950949408,
llama2_220M_nl_40_code_60,xnli_es,0-shot,brier_score,1.1051924190889053,
llama2_220M_nl_40_code_60,xnli_fr,0-shot,brier_score,1.0247892872235147,
llama2_220M_nl_40_code_60,xnli_hi,0-shot,brier_score,0.8712001219730566,
llama2_220M_nl_40_code_60,xnli_ru,0-shot,brier_score,0.9112448987489539,
llama2_220M_nl_40_code_60,xnli_sw,0-shot,brier_score,0.8868665086169029,
llama2_220M_nl_40_code_60,xnli_th,0-shot,brier_score,1.0068035659090697,
llama2_220M_nl_40_code_60,xnli_tr,0-shot,brier_score,1.0693224084960327,
llama2_220M_nl_40_code_60,xnli_ur,0-shot,brier_score,1.3194230685980182,
llama2_220M_nl_40_code_60,xnli_vi,0-shot,brier_score,1.1831272459663027,
llama2_220M_nl_40_code_60,xnli_zh,0-shot,brier_score,1.087982745013511,
llama2_220M_nl_40_code_60,anli_r1,0-shot,brier_score,1.0380056753402154,
llama2_220M_nl_40_code_60,anli_r3,0-shot,brier_score,0.9868700416556002,
llama2_220M_nl_40_code_60,anli_r2,0-shot,brier_score,1.0434998246462197,
llama2_220M_nl_40_code_60,mathqa,5-shot,brier_score,1.0390905386349782,
llama2_220M_nl_40_code_60,logiqa2,0-shot,brier_score,1.1791336005228032,
llama2_220M_nl_40_code_60,lambada_standard,0-shot,perplexity,131.9624436400779,5.489409719204202
llama2_220M_nl_40_code_60,lambada_standard,0-shot,accuracy,0.22685814088880263,0.005834704296179015
llama2_220M_nl_40_code_60,lambada_openai,0-shot,perplexity,55.532837759867704,2.1332535891947404
llama2_220M_nl_40_code_60,lambada_openai,0-shot,accuracy,0.2965262953619251,0.006363083362328484
EleutherAI/pythia-1b-deduped,drop,3-shot,accuracy,0.0016778523489932886,0.0004191330178826894
EleutherAI/pythia-1b-deduped,drop,3-shot,f1,0.047140310402684724,0.001227508776398318
EleutherAI/pythia-1b-deduped,gsm8k,5-shot,accuracy,0.011372251705837756,0.002920666198788757
EleutherAI/pythia-1b-deduped,winogrande,5-shot,accuracy,0.5359116022099447,0.014016193433958312
EleutherAI/pythia-1b-deduped,arc:challenge,25-shot,accuracy,0.2645051194539249,0.012889272949313363
EleutherAI/pythia-1b-deduped,arc:challenge,25-shot,acc_norm,0.2909556313993174,0.013273077865907593
EleutherAI/pythia-1b-deduped,hellaswag,10-shot,accuracy,0.38727345150368453,0.004861314613286843
EleutherAI/pythia-1b-deduped,hellaswag,10-shot,acc_norm,0.4965146385182235,0.004989660180792167
EleutherAI/pythia-1b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.04229525846816505
EleutherAI/pythia-1b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.04229525846816505
EleutherAI/pythia-1b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.23703703703703705,0.03673731683969506
EleutherAI/pythia-1b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.23703703703703705,0.03673731683969506
EleutherAI/pythia-1b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.14473684210526316,0.028631951845930387
EleutherAI/pythia-1b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.14473684210526316,0.028631951845930387
EleutherAI/pythia-1b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.18,0.038612291966536955
EleutherAI/pythia-1b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.18,0.038612291966536955
EleutherAI/pythia-1b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.028254200344438655
EleutherAI/pythia-1b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.028254200344438655
EleutherAI/pythia-1b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.24305555555555555,0.03586879280080341
EleutherAI/pythia-1b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.24305555555555555,0.03586879280080341
EleutherAI/pythia-1b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-1b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.040201512610368445
EleutherAI/pythia-1b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
EleutherAI/pythia-1b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
EleutherAI/pythia-1b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.2,0.040201512610368445
EleutherAI/pythia-1b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.2,0.040201512610368445
EleutherAI/pythia-1b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.26011560693641617,0.03345036916788992
EleutherAI/pythia-1b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.26011560693641617,0.03345036916788992
EleutherAI/pythia-1b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.18627450980392157,0.03873958714149352
EleutherAI/pythia-1b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.18627450980392157,0.03873958714149352
EleutherAI/pythia-1b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.045126085985421276
EleutherAI/pythia-1b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.045126085985421276
EleutherAI/pythia-1b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.029241883869628817
EleutherAI/pythia-1b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.029241883869628817
EleutherAI/pythia-1b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.18421052631578946,0.03646758875075566
EleutherAI/pythia-1b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.18421052631578946,0.03646758875075566
EleutherAI/pythia-1b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.19310344827586207,0.03289445522127398
EleutherAI/pythia-1b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.19310344827586207,0.03289445522127398
EleutherAI/pythia-1b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24338624338624337,0.022101128787415433
EleutherAI/pythia-1b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24338624338624337,0.022101128787415433
EleutherAI/pythia-1b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.24603174603174602,0.03852273364924316
EleutherAI/pythia-1b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.24603174603174602,0.03852273364924316
EleutherAI/pythia-1b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-1b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.25,0.04351941398892446
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.267741935483871,0.02518900666021238
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.267741935483871,0.02518900666021238
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.23645320197044334,0.029896114291733552
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.23645320197044334,0.029896114291733552
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.04512608598542129
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.04512608598542129
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23636363636363636,0.033175059300091805
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23636363636363636,0.033175059300091805
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.21717171717171718,0.02937661648494563
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.21717171717171718,0.02937661648494563
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.21243523316062177,0.029519282616817254
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.21243523316062177,0.029519282616817254
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.20512820512820512,0.020473233173551975
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.20512820512820512,0.020473233173551975
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.02646611753895991
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.02646611753895991
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.18907563025210083,0.025435119438105343
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.18907563025210083,0.025435119438105343
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.23178807947019867,0.03445406271987053
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23178807947019867,0.03445406271987053
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.29357798165137616,0.019525151122639667
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.29357798165137616,0.019525151122639667
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.25,0.029531221160930918
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.25,0.029531221160930918
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.03039153369274154
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.03039153369274154
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.028900721906293426
EleutherAI/pythia-1b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.028900721906293426
EleutherAI/pythia-1b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.336322869955157,0.031708824268455
EleutherAI/pythia-1b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.336322869955157,0.031708824268455
EleutherAI/pythia-1b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.037276735755969174
EleutherAI/pythia-1b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.037276735755969174
EleutherAI/pythia-1b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.03984979653302872
EleutherAI/pythia-1b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.03984979653302872
EleutherAI/pythia-1b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.28703703703703703,0.043733130409147614
EleutherAI/pythia-1b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.28703703703703703,0.043733130409147614
EleutherAI/pythia-1b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.22699386503067484,0.03291099578615769
EleutherAI/pythia-1b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.22699386503067484,0.03291099578615769
EleutherAI/pythia-1b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.30357142857142855,0.04364226155841044
EleutherAI/pythia-1b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.30357142857142855,0.04364226155841044
EleutherAI/pythia-1b-deduped,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.03916667762822585
EleutherAI/pythia-1b-deduped,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.03916667762822585
EleutherAI/pythia-1b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2777777777777778,0.029343114798094472
EleutherAI/pythia-1b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2777777777777778,0.029343114798094472
EleutherAI/pythia-1b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.23,0.04229525846816505
EleutherAI/pythia-1b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.23,0.04229525846816505
EleutherAI/pythia-1b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.27586206896551724,0.015982814774695625
EleutherAI/pythia-1b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.27586206896551724,0.015982814774695625
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.023703099525258155
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.023703099525258155
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24134078212290502,0.014310999547961441
EleutherAI/pythia-1b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24134078212290502,0.014310999547961441
EleutherAI/pythia-1b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.23202614379084968,0.024170840879341
EleutherAI/pythia-1b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.23202614379084968,0.024170840879341
EleutherAI/pythia-1b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.26688102893890675,0.025122637608816646
EleutherAI/pythia-1b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.26688102893890675,0.025122637608816646
EleutherAI/pythia-1b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.2623456790123457,0.024477222856135107
EleutherAI/pythia-1b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.2623456790123457,0.024477222856135107
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.24113475177304963,0.02551873104953778
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24113475177304963,0.02551873104953778
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.23859191655801826,0.0108859297420022
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.23859191655801826,0.0108859297420022
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.2610294117647059,0.02667925227010312
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2610294117647059,0.02667925227010312
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.28431372549019607,0.01824902441120766
EleutherAI/pythia-1b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.28431372549019607,0.01824902441120766
EleutherAI/pythia-1b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2727272727272727,0.04265792110940588
EleutherAI/pythia-1b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2727272727272727,0.04265792110940588
EleutherAI/pythia-1b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.1469387755102041,0.022665400417217638
EleutherAI/pythia-1b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.1469387755102041,0.022665400417217638
EleutherAI/pythia-1b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409224
EleutherAI/pythia-1b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.23880597014925373,0.030147775935409224
EleutherAI/pythia-1b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.17,0.03775251680686371
EleutherAI/pythia-1b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.17,0.03775251680686371
EleutherAI/pythia-1b-deduped,hendrycksTest-virology,5-shot,accuracy,0.27710843373493976,0.034843315926805875
EleutherAI/pythia-1b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.27710843373493976,0.034843315926805875
EleutherAI/pythia-1b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.24561403508771928,0.03301405946987251
EleutherAI/pythia-1b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.24561403508771928,0.03301405946987251
EleutherAI/pythia-1b-deduped,truthfulqa:mc,0-shot,mc1,0.22643818849449204,0.014651337324602576
EleutherAI/pythia-1b-deduped,truthfulqa:mc,0-shot,mc2,0.38939378209614806,0.014314917746383592
mosaicml/mpt-7b,minerva_math_precalc,5-shot,accuracy,0.029304029304029304,0.007224487305459692
mosaicml/mpt-7b,minerva_math_prealgebra,5-shot,accuracy,0.06659012629161883,0.008452428160416104
mosaicml/mpt-7b,minerva_math_num_theory,5-shot,accuracy,0.040740740740740744,0.008515067163720174
mosaicml/mpt-7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.02547065337763012,0.005245830272559335
mosaicml/mpt-7b,minerva_math_geometry,5-shot,accuracy,0.033402922755741124,0.008218660203335972
mosaicml/mpt-7b,minerva_math_counting_and_prob,5-shot,accuracy,0.046413502109704644,0.009673232932861578
mosaicml/mpt-7b,minerva_math_algebra,5-shot,accuracy,0.03622577927548441,0.005425680006601679
mosaicml/mpt-7b,fld_default,0-shot,accuracy,0.0,
mosaicml/mpt-7b,fld_star,0-shot,accuracy,0.0,
mosaicml/mpt-7b,arithmetic_3da,5-shot,accuracy,0.0405,0.004409035585862131
mosaicml/mpt-7b,arithmetic_3ds,5-shot,accuracy,0.017,0.0028913110935905586
mosaicml/mpt-7b,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521504
mosaicml/mpt-7b,arithmetic_2ds,5-shot,accuracy,0.217,0.00921943593716572
mosaicml/mpt-7b,arithmetic_5ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b,arithmetic_5da,5-shot,accuracy,0.001,0.0007069298939339509
mosaicml/mpt-7b,arithmetic_1dc,5-shot,accuracy,0.017,0.0028913110935905443
mosaicml/mpt-7b,arithmetic_4ds,5-shot,accuracy,0.0005,0.000500000000000013
mosaicml/mpt-7b,arithmetic_2dm,5-shot,accuracy,0.0485,0.004804728682127105
mosaicml/mpt-7b,arithmetic_2da,5-shot,accuracy,0.199,0.008929690346526223
mosaicml/mpt-7b,gsm8k_cot,5-shot,accuracy,0.08567096285064443,0.007709218855882752
mosaicml/mpt-7b,gsm8k,5-shot,accuracy,0.0401819560272934,0.005409439736970527
mosaicml/mpt-7b,anli_r2,0-shot,brier_score,0.7871039264444815,
mosaicml/mpt-7b,anli_r3,0-shot,brier_score,0.7735706458763796,
mosaicml/mpt-7b,anli_r1,0-shot,brier_score,0.8064532070514647,
mosaicml/mpt-7b,xnli_eu,0-shot,brier_score,0.9836769306459834,
mosaicml/mpt-7b,xnli_vi,0-shot,brier_score,0.9706863969461264,
mosaicml/mpt-7b,xnli_ru,0-shot,brier_score,0.7263265977856224,
mosaicml/mpt-7b,xnli_zh,0-shot,brier_score,0.9844979242445991,
mosaicml/mpt-7b,xnli_tr,0-shot,brier_score,0.9073384888674126,
mosaicml/mpt-7b,xnli_fr,0-shot,brier_score,0.7455615780108278,
mosaicml/mpt-7b,xnli_en,0-shot,brier_score,0.6521561020686549,
mosaicml/mpt-7b,xnli_ur,0-shot,brier_score,1.3114352706806065,
mosaicml/mpt-7b,xnli_ar,0-shot,brier_score,1.1879643155024167,
mosaicml/mpt-7b,xnli_de,0-shot,brier_score,0.8140312095660267,
mosaicml/mpt-7b,xnli_hi,0-shot,brier_score,0.9094996182021011,
mosaicml/mpt-7b,xnli_es,0-shot,brier_score,0.9037442585372435,
mosaicml/mpt-7b,xnli_bg,0-shot,brier_score,0.825572748604095,
mosaicml/mpt-7b,xnli_sw,0-shot,brier_score,1.1722319061632498,
mosaicml/mpt-7b,xnli_el,0-shot,brier_score,1.0056819587703962,
mosaicml/mpt-7b,xnli_th,0-shot,brier_score,0.8582353033497166,
mosaicml/mpt-7b,logiqa2,0-shot,brier_score,0.9764895261294206,
mosaicml/mpt-7b,mathqa,5-shot,brier_score,0.9492149034082438,
mosaicml/mpt-7b,lambada_standard,0-shot,perplexity,4.924753971769574,0.10796399230777208
mosaicml/mpt-7b,lambada_standard,0-shot,accuracy,0.6194449835047545,0.006764289222028883
mosaicml/mpt-7b,lambada_openai,0-shot,perplexity,3.8685392219566808,0.08095253429483709
mosaicml/mpt-7b,lambada_openai,0-shot,accuracy,0.685231903745391,0.006470326766225591
mosaicml/mpt-7b,mmlu_world_religions,0-shot,accuracy,0.3333333333333333,0.03615507630310937
mosaicml/mpt-7b,mmlu_formal_logic,0-shot,accuracy,0.19047619047619047,0.03512207412302053
mosaicml/mpt-7b,mmlu_prehistory,0-shot,accuracy,0.3117283950617284,0.02577311116963046
mosaicml/mpt-7b,mmlu_moral_scenarios,0-shot,accuracy,0.25139664804469275,0.014508979453553991
mosaicml/mpt-7b,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
mosaicml/mpt-7b,mmlu_moral_disputes,0-shot,accuracy,0.2861271676300578,0.024332146779134128
mosaicml/mpt-7b,mmlu_professional_law,0-shot,accuracy,0.2633637548891786,0.011249506403605284
mosaicml/mpt-7b,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.03322015795776741
mosaicml/mpt-7b,mmlu_high_school_us_history,0-shot,accuracy,0.2696078431372549,0.03114557065948678
mosaicml/mpt-7b,mmlu_philosophy,0-shot,accuracy,0.3086816720257235,0.026236965881153266
mosaicml/mpt-7b,mmlu_jurisprudence,0-shot,accuracy,0.3425925925925926,0.045879047413018105
mosaicml/mpt-7b,mmlu_international_law,0-shot,accuracy,0.23140495867768596,0.03849856098794089
mosaicml/mpt-7b,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.03346409881055953
mosaicml/mpt-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.31088082901554404,0.03340361906276588
mosaicml/mpt-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2689075630252101,0.02880139219363127
mosaicml/mpt-7b,mmlu_high_school_geography,0-shot,accuracy,0.22727272727272727,0.02985751567338641
mosaicml/mpt-7b,mmlu_high_school_psychology,0-shot,accuracy,0.26238532110091745,0.018861885021534745
mosaicml/mpt-7b,mmlu_public_relations,0-shot,accuracy,0.3090909090909091,0.044262946482000985
mosaicml/mpt-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.46,0.05009082659620333
mosaicml/mpt-7b,mmlu_sociology,0-shot,accuracy,0.20398009950248755,0.02849317624532607
mosaicml/mpt-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2743589743589744,0.0226227657674932
mosaicml/mpt-7b,mmlu_security_studies,0-shot,accuracy,0.3224489795918367,0.029923100563683906
mosaicml/mpt-7b,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594726
mosaicml/mpt-7b,mmlu_human_sexuality,0-shot,accuracy,0.3053435114503817,0.040393149787245605
mosaicml/mpt-7b,mmlu_econometrics,0-shot,accuracy,0.2894736842105263,0.04266339443159394
mosaicml/mpt-7b,mmlu_miscellaneous,0-shot,accuracy,0.29757343550446996,0.01634911191290942
mosaicml/mpt-7b,mmlu_marketing,0-shot,accuracy,0.3333333333333333,0.030882736974138653
mosaicml/mpt-7b,mmlu_management,0-shot,accuracy,0.2621359223300971,0.04354631077260595
mosaicml/mpt-7b,mmlu_nutrition,0-shot,accuracy,0.27124183006535946,0.025457756696667878
mosaicml/mpt-7b,mmlu_medical_genetics,0-shot,accuracy,0.35,0.0479372485441102
mosaicml/mpt-7b,mmlu_human_aging,0-shot,accuracy,0.3542600896860987,0.032100621541349864
mosaicml/mpt-7b,mmlu_professional_medicine,0-shot,accuracy,0.20588235294117646,0.024562204314142314
mosaicml/mpt-7b,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
mosaicml/mpt-7b,mmlu_business_ethics,0-shot,accuracy,0.28,0.04512608598542127
mosaicml/mpt-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.02688064788905199
mosaicml/mpt-7b,mmlu_global_facts,0-shot,accuracy,0.22,0.04163331998932269
mosaicml/mpt-7b,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.036108050180310235
mosaicml/mpt-7b,mmlu_professional_accounting,0-shot,accuracy,0.25177304964539005,0.0258921511567094
mosaicml/mpt-7b,mmlu_college_physics,0-shot,accuracy,0.20588235294117646,0.04023382273617749
mosaicml/mpt-7b,mmlu_high_school_physics,0-shot,accuracy,0.25165562913907286,0.03543304234389985
mosaicml/mpt-7b,mmlu_high_school_biology,0-shot,accuracy,0.25806451612903225,0.02489246917246283
mosaicml/mpt-7b,mmlu_college_biology,0-shot,accuracy,0.2986111111111111,0.03827052357950756
mosaicml/mpt-7b,mmlu_anatomy,0-shot,accuracy,0.25925925925925924,0.03785714465066654
mosaicml/mpt-7b,mmlu_college_chemistry,0-shot,accuracy,0.27,0.0446196043338474
mosaicml/mpt-7b,mmlu_computer_security,0-shot,accuracy,0.27,0.044619604333847394
mosaicml/mpt-7b,mmlu_college_computer_science,0-shot,accuracy,0.35,0.0479372485441102
mosaicml/mpt-7b,mmlu_astronomy,0-shot,accuracy,0.26973684210526316,0.036117805602848975
mosaicml/mpt-7b,mmlu_college_mathematics,0-shot,accuracy,0.35,0.047937248544110196
mosaicml/mpt-7b,mmlu_conceptual_physics,0-shot,accuracy,0.32340425531914896,0.03057944277361034
mosaicml/mpt-7b,mmlu_abstract_algebra,0-shot,accuracy,0.19,0.039427724440366234
mosaicml/mpt-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.042295258468165044
mosaicml/mpt-7b,mmlu_machine_learning,0-shot,accuracy,0.35714285714285715,0.04547960999764376
mosaicml/mpt-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.16748768472906403,0.026273086047535425
mosaicml/mpt-7b,mmlu_high_school_statistics,0-shot,accuracy,0.27314814814814814,0.03038805130167812
mosaicml/mpt-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.24867724867724866,0.022261817692400158
mosaicml/mpt-7b,mmlu_electrical_engineering,0-shot,accuracy,0.3448275862068966,0.03960933549451208
mosaicml/mpt-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.29259259259259257,0.027738969632176088
mosaicml/mpt-7b,arc_challenge,25-shot,accuracy,0.4351535836177474,0.014487986197186047
mosaicml/mpt-7b,arc_challenge,25-shot,acc_norm,0.47013651877133106,0.014585305840007105
mosaicml/mpt-7b,hellaswag,10-shot,accuracy,0.5730930093606851,0.004936176784631949
mosaicml/mpt-7b,hellaswag,10-shot,acc_norm,0.7753435570603465,0.0041650291643616005
mosaicml/mpt-7b,truthfulqa_mc2,0-shot,accuracy,0.33431576358453946,0.013089145381228533
mosaicml/mpt-7b,truthfulqa_gen,0-shot,bleu_max,24.602330933425307,0.7657159724776746
mosaicml/mpt-7b,truthfulqa_gen,0-shot,bleu_acc,0.3072215422276622,0.016150201321323044
mosaicml/mpt-7b,truthfulqa_gen,0-shot,bleu_diff,-9.827181119106774,0.8506396518002634
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge1_max,48.4307380348122,0.9034301197408506
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge1_acc,0.2729498164014688,0.01559475363200655
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge1_diff,-12.051963649063818,0.9400681010531302
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge2_max,32.229261537796205,1.018069487091937
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge2_acc,0.20930232558139536,0.01424121943478583
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rouge2_diff,-14.473968970783242,1.0917907802458022
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rougeL_max,45.768993552153226,0.9070545874177821
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rougeL_acc,0.26805385556915545,0.015506204722834555
mosaicml/mpt-7b,truthfulqa_gen,0-shot,rougeL_diff,-12.622302003402247,0.9501788414558511
mosaicml/mpt-7b,truthfulqa_mc1,0-shot,accuracy,0.204406364749082,0.014117174337432616
mosaicml/mpt-7b,winogrande,5-shot,accuracy,0.7213891081294396,0.012599896649493876
EleutherAI/pythia-160m,arc:challenge,25-shot,accuracy,0.19027303754266212,0.011470424179225697
EleutherAI/pythia-160m,arc:challenge,25-shot,acc_norm,0.22781569965870307,0.012256708602326905
EleutherAI/pythia-160m,hellaswag,10-shot,accuracy,0.2874925313682533,0.0045166819538790745
EleutherAI/pythia-160m,hellaswag,10-shot,acc_norm,0.30342561242780325,0.004587978625582475
EleutherAI/pythia-160m,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.04163331998932268
EleutherAI/pythia-160m,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.04163331998932268
EleutherAI/pythia-160m,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.03749850709174021
EleutherAI/pythia-160m,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.03749850709174021
EleutherAI/pythia-160m,hendrycksTest-astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-160m,hendrycksTest-astronomy,5-shot,acc_norm,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-160m,hendrycksTest-business_ethics,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-160m,hendrycksTest-business_ethics,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-160m,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2037735849056604,0.0247907845017754
EleutherAI/pythia-160m,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2037735849056604,0.0247907845017754
EleutherAI/pythia-160m,hendrycksTest-college_biology,5-shot,accuracy,0.24305555555555555,0.03586879280080342
EleutherAI/pythia-160m,hendrycksTest-college_biology,5-shot,acc_norm,0.24305555555555555,0.03586879280080342
EleutherAI/pythia-160m,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036845
EleutherAI/pythia-160m,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036845
EleutherAI/pythia-160m,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-160m,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/pythia-160m,hendrycksTest-college_mathematics,5-shot,accuracy,0.26,0.04408440022768079
EleutherAI/pythia-160m,hendrycksTest-college_mathematics,5-shot,acc_norm,0.26,0.04408440022768079
EleutherAI/pythia-160m,hendrycksTest-college_medicine,5-shot,accuracy,0.23699421965317918,0.03242414757483098
EleutherAI/pythia-160m,hendrycksTest-college_medicine,5-shot,acc_norm,0.23699421965317918,0.03242414757483098
EleutherAI/pythia-160m,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237654
EleutherAI/pythia-160m,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237654
EleutherAI/pythia-160m,hendrycksTest-computer_security,5-shot,accuracy,0.18,0.038612291966536975
EleutherAI/pythia-160m,hendrycksTest-computer_security,5-shot,acc_norm,0.18,0.038612291966536975
EleutherAI/pythia-160m,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2680851063829787,0.028957342788342343
EleutherAI/pythia-160m,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2680851063829787,0.028957342788342343
EleutherAI/pythia-160m,hendrycksTest-econometrics,5-shot,accuracy,0.24561403508771928,0.04049339297748141
EleutherAI/pythia-160m,hendrycksTest-econometrics,5-shot,acc_norm,0.24561403508771928,0.04049339297748141
EleutherAI/pythia-160m,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.036001056927277716
EleutherAI/pythia-160m,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.036001056927277716
EleutherAI/pythia-160m,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25132275132275134,0.022340482339643898
EleutherAI/pythia-160m,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25132275132275134,0.022340482339643898
EleutherAI/pythia-160m,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.03567016675276863
EleutherAI/pythia-160m,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.03567016675276863
EleutherAI/pythia-160m,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.038612291966536934
EleutherAI/pythia-160m,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.038612291966536934
EleutherAI/pythia-160m,hendrycksTest-high_school_biology,5-shot,accuracy,0.31290322580645163,0.02637756702864586
EleutherAI/pythia-160m,hendrycksTest-high_school_biology,5-shot,acc_norm,0.31290322580645163,0.02637756702864586
EleutherAI/pythia-160m,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.03127090713297698
EleutherAI/pythia-160m,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.03127090713297698
EleutherAI/pythia-160m,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.2,0.040201512610368466
EleutherAI/pythia-160m,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.2,0.040201512610368466
EleutherAI/pythia-160m,hendrycksTest-high_school_european_history,5-shot,accuracy,0.24848484848484848,0.033744026441394036
EleutherAI/pythia-160m,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.24848484848484848,0.033744026441394036
EleutherAI/pythia-160m,hendrycksTest-high_school_geography,5-shot,accuracy,0.23232323232323232,0.030088629490217483
EleutherAI/pythia-160m,hendrycksTest-high_school_geography,5-shot,acc_norm,0.23232323232323232,0.030088629490217483
EleutherAI/pythia-160m,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.24352331606217617,0.03097543638684543
EleutherAI/pythia-160m,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.24352331606217617,0.03097543638684543
EleutherAI/pythia-160m,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.24615384615384617,0.021840866990423077
EleutherAI/pythia-160m,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.24615384615384617,0.021840866990423077
EleutherAI/pythia-160m,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.02646611753895991
EleutherAI/pythia-160m,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.02646611753895991
EleutherAI/pythia-160m,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.27310924369747897,0.02894200404099817
EleutherAI/pythia-160m,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.27310924369747897,0.02894200404099817
EleutherAI/pythia-160m,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.03631329803969654
EleutherAI/pythia-160m,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.03631329803969654
EleutherAI/pythia-160m,hendrycksTest-high_school_psychology,5-shot,accuracy,0.22385321100917432,0.017871217767790215
EleutherAI/pythia-160m,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.22385321100917432,0.017871217767790215
EleutherAI/pythia-160m,hendrycksTest-high_school_statistics,5-shot,accuracy,0.47685185185185186,0.03406315360711507
EleutherAI/pythia-160m,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.47685185185185186,0.03406315360711507
EleutherAI/pythia-160m,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25980392156862747,0.03077855467869327
EleutherAI/pythia-160m,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25980392156862747,0.03077855467869327
EleutherAI/pythia-160m,hendrycksTest-high_school_world_history,5-shot,accuracy,0.29535864978902954,0.029696338713422876
EleutherAI/pythia-160m,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.29535864978902954,0.029696338713422876
EleutherAI/pythia-160m,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.03102441174057222
EleutherAI/pythia-160m,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.03102441174057222
EleutherAI/pythia-160m,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.037276735755969174
EleutherAI/pythia-160m,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.037276735755969174
EleutherAI/pythia-160m,hendrycksTest-international_law,5-shot,accuracy,0.2231404958677686,0.03800754475228733
EleutherAI/pythia-160m,hendrycksTest-international_law,5-shot,acc_norm,0.2231404958677686,0.03800754475228733
EleutherAI/pythia-160m,hendrycksTest-jurisprudence,5-shot,accuracy,0.2777777777777778,0.043300437496507437
EleutherAI/pythia-160m,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2777777777777778,0.043300437496507437
EleutherAI/pythia-160m,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2085889570552147,0.03192193448934724
EleutherAI/pythia-160m,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2085889570552147,0.03192193448934724
EleutherAI/pythia-160m,hendrycksTest-machine_learning,5-shot,accuracy,0.1875,0.0370468111477387
EleutherAI/pythia-160m,hendrycksTest-machine_learning,5-shot,acc_norm,0.1875,0.0370468111477387
EleutherAI/pythia-160m,hendrycksTest-management,5-shot,accuracy,0.1941747572815534,0.03916667762822585
EleutherAI/pythia-160m,hendrycksTest-management,5-shot,acc_norm,0.1941747572815534,0.03916667762822585
EleutherAI/pythia-160m,hendrycksTest-marketing,5-shot,accuracy,0.18376068376068377,0.025372139671722933
EleutherAI/pythia-160m,hendrycksTest-marketing,5-shot,acc_norm,0.18376068376068377,0.025372139671722933
EleutherAI/pythia-160m,hendrycksTest-medical_genetics,5-shot,accuracy,0.32,0.04688261722621504
EleutherAI/pythia-160m,hendrycksTest-medical_genetics,5-shot,acc_norm,0.32,0.04688261722621504
EleutherAI/pythia-160m,hendrycksTest-miscellaneous,5-shot,accuracy,0.25287356321839083,0.01554337731371968
EleutherAI/pythia-160m,hendrycksTest-miscellaneous,5-shot,acc_norm,0.25287356321839083,0.01554337731371968
EleutherAI/pythia-160m,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.023703099525258172
EleutherAI/pythia-160m,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.023703099525258172
EleutherAI/pythia-160m,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2770949720670391,0.01496877243581215
EleutherAI/pythia-160m,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2770949720670391,0.01496877243581215
EleutherAI/pythia-160m,hendrycksTest-nutrition,5-shot,accuracy,0.23529411764705882,0.024288619466046112
EleutherAI/pythia-160m,hendrycksTest-nutrition,5-shot,acc_norm,0.23529411764705882,0.024288619466046112
EleutherAI/pythia-160m,hendrycksTest-philosophy,5-shot,accuracy,0.18971061093247588,0.02226819625878323
EleutherAI/pythia-160m,hendrycksTest-philosophy,5-shot,acc_norm,0.18971061093247588,0.02226819625878323
EleutherAI/pythia-160m,hendrycksTest-prehistory,5-shot,accuracy,0.25925925925925924,0.024383665531035447
EleutherAI/pythia-160m,hendrycksTest-prehistory,5-shot,acc_norm,0.25925925925925924,0.024383665531035447
EleutherAI/pythia-160m,hendrycksTest-professional_accounting,5-shot,accuracy,0.2801418439716312,0.026789172351140242
EleutherAI/pythia-160m,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2801418439716312,0.026789172351140242
EleutherAI/pythia-160m,hendrycksTest-professional_law,5-shot,accuracy,0.23728813559322035,0.010865436690780272
EleutherAI/pythia-160m,hendrycksTest-professional_law,5-shot,acc_norm,0.23728813559322035,0.010865436690780272
EleutherAI/pythia-160m,hendrycksTest-professional_medicine,5-shot,accuracy,0.4485294117647059,0.030211479609121593
EleutherAI/pythia-160m,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4485294117647059,0.030211479609121593
EleutherAI/pythia-160m,hendrycksTest-professional_psychology,5-shot,accuracy,0.25163398692810457,0.01755581809132226
EleutherAI/pythia-160m,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25163398692810457,0.01755581809132226
EleutherAI/pythia-160m,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.04172343038705383
EleutherAI/pythia-160m,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.04172343038705383
EleutherAI/pythia-160m,hendrycksTest-security_studies,5-shot,accuracy,0.24489795918367346,0.027529637440174937
EleutherAI/pythia-160m,hendrycksTest-security_studies,5-shot,acc_norm,0.24489795918367346,0.027529637440174937
EleutherAI/pythia-160m,hendrycksTest-sociology,5-shot,accuracy,0.24378109452736318,0.03036049015401465
EleutherAI/pythia-160m,hendrycksTest-sociology,5-shot,acc_norm,0.24378109452736318,0.03036049015401465
EleutherAI/pythia-160m,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.04408440022768079
EleutherAI/pythia-160m,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.04408440022768079
EleutherAI/pythia-160m,hendrycksTest-virology,5-shot,accuracy,0.18072289156626506,0.02995573785581014
EleutherAI/pythia-160m,hendrycksTest-virology,5-shot,acc_norm,0.18072289156626506,0.02995573785581014
EleutherAI/pythia-160m,hendrycksTest-world_religions,5-shot,accuracy,0.2046783625730994,0.03094445977853322
EleutherAI/pythia-160m,hendrycksTest-world_religions,5-shot,acc_norm,0.2046783625730994,0.03094445977853322
EleutherAI/pythia-160m,truthfulqa:mc,0-shot,mc1,0.25091799265605874,0.015176985027707687
EleutherAI/pythia-160m,truthfulqa:mc,0-shot,mc2,0.44263082100781215,0.014953643384240857
EleutherAI/pythia-160m,drop,3-shot,accuracy,0.0012583892617449664,0.0003630560893119131
EleutherAI/pythia-160m,drop,3-shot,f1,0.03449874161073832,0.0010696643616809897
EleutherAI/pythia-160m,gsm8k,5-shot,accuracy,0.002274450341167551,0.0013121578148674164
EleutherAI/pythia-160m,winogrande,5-shot,accuracy,0.5153906866614049,0.01404582678978366
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_precalc,5-shot,accuracy,0.016483516483516484,0.005454029764766739
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_prealgebra,5-shot,accuracy,0.12973593570608496,0.011391896832385503
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_num_theory,5-shot,accuracy,0.027777777777777776,0.0070784332144114845
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_intermediate_algebra,5-shot,accuracy,0.028792912513842746,0.005567951601475892
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_geometry,5-shot,accuracy,0.037578288100208766,0.00869835750903299
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_counting_and_prob,5-shot,accuracy,0.05063291139240506,0.010080984934213222
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,minerva_math_algebra,5-shot,accuracy,0.06571187868576242,0.007194821287587887
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,fld_default,0-shot,accuracy,0.0,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,fld_star,0-shot,accuracy,0.0,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_3da,5-shot,accuracy,0.141,0.007783944687460703
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_3ds,5-shot,accuracy,0.3925,0.010921607746018008
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_4da,5-shot,accuracy,0.1515,0.008019103940840797
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_2ds,5-shot,accuracy,0.3235,0.010463202870400423
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_5ds,5-shot,accuracy,0.105,0.006856457212201517
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_5da,5-shot,accuracy,0.053,0.00501079375219268
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_1dc,5-shot,accuracy,0.0,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_4ds,5-shot,accuracy,0.2895,0.010143782487887824
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_2dm,5-shot,accuracy,0.35,0.010668031845271564
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,arithmetic_2da,5-shot,accuracy,0.2235,0.009317579280146631
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,gsm8k_cot,5-shot,accuracy,0.3419257012888552,0.01306608962518281
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,gsm8k,5-shot,accuracy,0.27748294162244125,0.012333447581047539
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,anli_r2,0-shot,brier_score,0.8329852734143958,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,anli_r3,0-shot,brier_score,0.7793826620488095,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,anli_r1,0-shot,brier_score,0.8351897730779554,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_eu,0-shot,brier_score,1.089858260901451,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_vi,0-shot,brier_score,1.0476274173469688,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_ru,0-shot,brier_score,0.9100989816072319,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_zh,0-shot,brier_score,1.147378120240311,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_tr,0-shot,brier_score,1.0121680067471919,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_fr,0-shot,brier_score,0.880532625822361,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_en,0-shot,brier_score,0.9798329124068353,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_ur,0-shot,brier_score,1.3026970239585185,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_ar,0-shot,brier_score,1.1205455462723755,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_de,0-shot,brier_score,0.9866185162056197,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_hi,0-shot,brier_score,1.049863204263663,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_es,0-shot,brier_score,0.944072774960891,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_bg,0-shot,brier_score,1.116276803181401,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_sw,0-shot,brier_score,1.0313388576056377,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_el,0-shot,brier_score,0.9209651581499029,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,xnli_th,0-shot,brier_score,1.1454058334629815,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,logiqa2,0-shot,brier_score,1.0434407612473975,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,mathqa,5-shot,brier_score,0.9620259901451523,
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_standard,0-shot,perplexity,4.372930344427915,0.1373038297230332
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_standard,0-shot,accuracy,0.6532117213273821,0.006630881434338537
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_openai,0-shot,perplexity,3.642807427442857,0.12011465988578897
Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,lambada_openai,0-shot,accuracy,0.6984281001358432,0.006393937119331441
HuggingFaceTB/SmolLM-1.7B,mmlu_world_religions,0-shot,accuracy,0.34502923976608185,0.03645981377388807
HuggingFaceTB/SmolLM-1.7B,mmlu_formal_logic,0-shot,accuracy,0.2222222222222222,0.037184890068181146
HuggingFaceTB/SmolLM-1.7B,mmlu_prehistory,0-shot,accuracy,0.3271604938271605,0.026105673861409814
HuggingFaceTB/SmolLM-1.7B,mmlu_moral_scenarios,0-shot,accuracy,0.2446927374301676,0.014378169884098424
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_world_history,0-shot,accuracy,0.26582278481012656,0.028756799629658332
HuggingFaceTB/SmolLM-1.7B,mmlu_moral_disputes,0-shot,accuracy,0.3236994219653179,0.02519018132760842
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_law,0-shot,accuracy,0.2835723598435463,0.011511900775968318
HuggingFaceTB/SmolLM-1.7B,mmlu_logical_fallacies,0-shot,accuracy,0.26380368098159507,0.03462419931615624
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_us_history,0-shot,accuracy,0.23039215686274508,0.029554292605695063
HuggingFaceTB/SmolLM-1.7B,mmlu_philosophy,0-shot,accuracy,0.36977491961414793,0.027417996705630995
HuggingFaceTB/SmolLM-1.7B,mmlu_jurisprudence,0-shot,accuracy,0.3055555555555556,0.044531975073749834
HuggingFaceTB/SmolLM-1.7B,mmlu_international_law,0-shot,accuracy,0.36363636363636365,0.043913262867240704
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.033464098810559534
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.27461139896373055,0.032210245080411544
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.2605042016806723,0.02851025151234193
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_geography,0-shot,accuracy,0.3484848484848485,0.033948539651564025
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_psychology,0-shot,accuracy,0.30458715596330277,0.019732299420354045
HuggingFaceTB/SmolLM-1.7B,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.04172343038705383
HuggingFaceTB/SmolLM-1.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.39,0.04902071300001974
HuggingFaceTB/SmolLM-1.7B,mmlu_sociology,0-shot,accuracy,0.2885572139303483,0.0320384104021332
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.28974358974358977,0.023000628243687954
HuggingFaceTB/SmolLM-1.7B,mmlu_security_studies,0-shot,accuracy,0.363265306122449,0.030789051139030806
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_psychology,0-shot,accuracy,0.26633986928104575,0.01788318813466721
HuggingFaceTB/SmolLM-1.7B,mmlu_human_sexuality,0-shot,accuracy,0.29770992366412213,0.04010358942462203
HuggingFaceTB/SmolLM-1.7B,mmlu_econometrics,0-shot,accuracy,0.20175438596491227,0.03775205013583639
HuggingFaceTB/SmolLM-1.7B,mmlu_miscellaneous,0-shot,accuracy,0.33205619412515963,0.016841174655295717
HuggingFaceTB/SmolLM-1.7B,mmlu_marketing,0-shot,accuracy,0.2863247863247863,0.029614323690456648
HuggingFaceTB/SmolLM-1.7B,mmlu_management,0-shot,accuracy,0.30097087378640774,0.04541609446503948
HuggingFaceTB/SmolLM-1.7B,mmlu_nutrition,0-shot,accuracy,0.3333333333333333,0.02699254433929723
HuggingFaceTB/SmolLM-1.7B,mmlu_medical_genetics,0-shot,accuracy,0.32,0.04688261722621504
HuggingFaceTB/SmolLM-1.7B,mmlu_human_aging,0-shot,accuracy,0.33183856502242154,0.031602951437766785
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_medicine,0-shot,accuracy,0.3492647058823529,0.02895975519682487
HuggingFaceTB/SmolLM-1.7B,mmlu_college_medicine,0-shot,accuracy,0.2658959537572254,0.0336876293225943
HuggingFaceTB/SmolLM-1.7B,mmlu_business_ethics,0-shot,accuracy,0.32,0.046882617226215034
HuggingFaceTB/SmolLM-1.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.35094339622641507,0.029373646253234686
HuggingFaceTB/SmolLM-1.7B,mmlu_global_facts,0-shot,accuracy,0.35,0.0479372485441102
HuggingFaceTB/SmolLM-1.7B,mmlu_virology,0-shot,accuracy,0.3795180722891566,0.03777798822748018
HuggingFaceTB/SmolLM-1.7B,mmlu_professional_accounting,0-shot,accuracy,0.24113475177304963,0.02551873104953776
HuggingFaceTB/SmolLM-1.7B,mmlu_college_physics,0-shot,accuracy,0.2647058823529412,0.04389869956808779
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_physics,0-shot,accuracy,0.271523178807947,0.03631329803969653
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_biology,0-shot,accuracy,0.33225806451612905,0.026795560848122797
HuggingFaceTB/SmolLM-1.7B,mmlu_college_biology,0-shot,accuracy,0.3541666666666667,0.039994111357535424
HuggingFaceTB/SmolLM-1.7B,mmlu_anatomy,0-shot,accuracy,0.4148148148148148,0.04256193767901407
HuggingFaceTB/SmolLM-1.7B,mmlu_college_chemistry,0-shot,accuracy,0.32,0.046882617226215034
HuggingFaceTB/SmolLM-1.7B,mmlu_computer_security,0-shot,accuracy,0.31,0.04648231987117316
HuggingFaceTB/SmolLM-1.7B,mmlu_college_computer_science,0-shot,accuracy,0.34,0.04760952285695235
HuggingFaceTB/SmolLM-1.7B,mmlu_astronomy,0-shot,accuracy,0.3618421052631579,0.03910525752849725
HuggingFaceTB/SmolLM-1.7B,mmlu_college_mathematics,0-shot,accuracy,0.27,0.044619604333847394
HuggingFaceTB/SmolLM-1.7B,mmlu_conceptual_physics,0-shot,accuracy,0.28085106382978725,0.02937917046412482
HuggingFaceTB/SmolLM-1.7B,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909283
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.33,0.047258156262526045
HuggingFaceTB/SmolLM-1.7B,mmlu_machine_learning,0-shot,accuracy,0.32142857142857145,0.04432804055291518
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.2413793103448276,0.030108330718011625
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_statistics,0-shot,accuracy,0.3194444444444444,0.0317987634217685
HuggingFaceTB/SmolLM-1.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.24867724867724866,0.022261817692400182
HuggingFaceTB/SmolLM-1.7B,mmlu_electrical_engineering,0-shot,accuracy,0.2206896551724138,0.0345593020192481
HuggingFaceTB/SmolLM-1.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.24814814814814815,0.0263357394040558
HuggingFaceTB/SmolLM-1.7B,arc_challenge,25-shot,accuracy,0.4616040955631399,0.014568245550296358
HuggingFaceTB/SmolLM-1.7B,arc_challenge,25-shot,acc_norm,0.48976109215017066,0.014608326906285012
HuggingFaceTB/SmolLM-1.7B,hellaswag,10-shot,accuracy,0.4985062736506672,0.004989759144812283
HuggingFaceTB/SmolLM-1.7B,hellaswag,10-shot,acc_norm,0.673770165305716,0.004678743563766636
HuggingFaceTB/SmolLM-1.7B,truthfulqa_mc2,0-shot,accuracy,0.3850766766083665,0.0141318041269147
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,bleu_max,25.637781985326768,0.7862985780149876
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,bleu_acc,0.3243574051407589,0.016387976779647935
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,bleu_diff,-7.032993037032112,0.8260513065264542
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge1_max,50.02333005649277,0.888601487283363
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge1_acc,0.2974296205630355,0.016002651487361005
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge1_diff,-9.43923743956951,0.9261892834631157
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge2_max,33.70014727089389,1.0124553331073005
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge2_acc,0.25458996328029376,0.015250117079156487
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rouge2_diff,-11.30035982468476,1.0978598755923092
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rougeL_max,47.21056335206833,0.8970416486465508
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rougeL_acc,0.2839657282741738,0.015785370858396718
HuggingFaceTB/SmolLM-1.7B,truthfulqa_gen,0-shot,rougeL_diff,-9.830444388716177,0.9254935688315172
HuggingFaceTB/SmolLM-1.7B,truthfulqa_mc1,0-shot,accuracy,0.24479804161566707,0.015051869486715006
HuggingFaceTB/SmolLM-1.7B,winogrande,5-shot,accuracy,0.611681136543015,0.013697456658457228
HuggingFaceTB/SmolLM-1.7B,gsm8k,5-shot,accuracy,0.052312357846853674,0.006133057708959227
mistralai/Mixtral-8x7B-v0.1,minerva_math_precalc,5-shot,accuracy,0.13553113553113552,0.014662092847395482
mistralai/Mixtral-8x7B-v0.1,minerva_math_prealgebra,5-shot,accuracy,0.4890929965556831,0.01694755389652771
mistralai/Mixtral-8x7B-v0.1,minerva_math_num_theory,5-shot,accuracy,0.17037037037037037,0.016193651111111738
mistralai/Mixtral-8x7B-v0.1,minerva_math_intermediate_algebra,5-shot,accuracy,0.11738648947951273,0.010717440330431139
mistralai/Mixtral-8x7B-v0.1,minerva_math_geometry,5-shot,accuracy,0.21294363256784968,0.018724977273263204
mistralai/Mixtral-8x7B-v0.1,minerva_math_counting_and_prob,5-shot,accuracy,0.1962025316455696,0.01825975937156573
mistralai/Mixtral-8x7B-v0.1,minerva_math_algebra,5-shot,accuracy,0.4018534119629318,0.014236239984076448
mistralai/Mixtral-8x7B-v0.1,fld_default,0-shot,accuracy,0.0,
mistralai/Mixtral-8x7B-v0.1,fld_star,0-shot,accuracy,0.0,
mistralai/Mixtral-8x7B-v0.1,arithmetic_3da,5-shot,accuracy,0.9945,0.0016541593398342208
mistralai/Mixtral-8x7B-v0.1,arithmetic_3ds,5-shot,accuracy,0.9995,0.0005000000000000049
mistralai/Mixtral-8x7B-v0.1,arithmetic_4da,5-shot,accuracy,0.983,0.0028913110935905434
mistralai/Mixtral-8x7B-v0.1,arithmetic_2ds,5-shot,accuracy,1.0,
mistralai/Mixtral-8x7B-v0.1,arithmetic_5ds,5-shot,accuracy,0.982,0.0029736208922129114
mistralai/Mixtral-8x7B-v0.1,arithmetic_5da,5-shot,accuracy,0.9755,0.003457723662536254
mistralai/Mixtral-8x7B-v0.1,arithmetic_1dc,5-shot,accuracy,0.935,0.005513864466114152
mistralai/Mixtral-8x7B-v0.1,arithmetic_4ds,5-shot,accuracy,0.995,0.0015775754727385416
mistralai/Mixtral-8x7B-v0.1,arithmetic_2dm,5-shot,accuracy,0.9525,0.004757435401116703
mistralai/Mixtral-8x7B-v0.1,arithmetic_2da,5-shot,accuracy,1.0,
mistralai/Mixtral-8x7B-v0.1,gsm8k_cot,5-shot,accuracy,0.6072782410917361,0.01345174534958657
mistralai/Mixtral-8x7B-v0.1,anli_r2,0-shot,brier_score,0.7204381849274958,
mistralai/Mixtral-8x7B-v0.1,anli_r3,0-shot,brier_score,0.7096885290045314,
mistralai/Mixtral-8x7B-v0.1,anli_r1,0-shot,brier_score,0.7298870357934575,
mistralai/Mixtral-8x7B-v0.1,xnli_eu,0-shot,brier_score,0.89697426333893,
mistralai/Mixtral-8x7B-v0.1,xnli_vi,0-shot,brier_score,0.852492855484468,
mistralai/Mixtral-8x7B-v0.1,xnli_ru,0-shot,brier_score,0.8140281538289218,
mistralai/Mixtral-8x7B-v0.1,xnli_zh,0-shot,brier_score,0.9735386004468622,
mistralai/Mixtral-8x7B-v0.1,xnli_tr,0-shot,brier_score,0.85461427730411,
mistralai/Mixtral-8x7B-v0.1,xnli_fr,0-shot,brier_score,0.7191029684114558,
mistralai/Mixtral-8x7B-v0.1,xnli_en,0-shot,brier_score,0.66142672854583,
mistralai/Mixtral-8x7B-v0.1,xnli_ur,0-shot,brier_score,0.9375055471719587,
mistralai/Mixtral-8x7B-v0.1,xnli_ar,0-shot,brier_score,1.170226749895146,
mistralai/Mixtral-8x7B-v0.1,xnli_de,0-shot,brier_score,0.7939831042563315,
mistralai/Mixtral-8x7B-v0.1,xnli_hi,0-shot,brier_score,0.8257619603057463,
mistralai/Mixtral-8x7B-v0.1,xnli_es,0-shot,brier_score,0.7897584112368303,
mistralai/Mixtral-8x7B-v0.1,xnli_bg,0-shot,brier_score,0.8047571756917079,
mistralai/Mixtral-8x7B-v0.1,xnli_sw,0-shot,brier_score,0.7954490376583093,
mistralai/Mixtral-8x7B-v0.1,xnli_el,0-shot,brier_score,0.8321932402650458,
mistralai/Mixtral-8x7B-v0.1,xnli_th,0-shot,brier_score,0.79821941139044,
mistralai/Mixtral-8x7B-v0.1,logiqa2,0-shot,brier_score,0.8376936123486322,
mistralai/Mixtral-8x7B-v0.1,mathqa,5-shot,brier_score,0.7425151563861571,
mistralai/Mixtral-8x7B-v0.1,lambada_standard,0-shot,perplexity,3.317198599562467,0.05963528944321586
mistralai/Mixtral-8x7B-v0.1,lambada_standard,0-shot,accuracy,0.7316126528235979,0.006173531884910567
mistralai/Mixtral-8x7B-v0.1,lambada_openai,0-shot,perplexity,2.788140791153981,0.04714050381983658
mistralai/Mixtral-8x7B-v0.1,lambada_openai,0-shot,accuracy,0.7822627595575393,0.005749826735287363
mistralai/Mixtral-8x7B-v0.1,mmlu_world_religions,0-shot,accuracy,0.8713450292397661,0.02567934272327693
mistralai/Mixtral-8x7B-v0.1,mmlu_formal_logic,0-shot,accuracy,0.5714285714285714,0.0442626668137991
mistralai/Mixtral-8x7B-v0.1,mmlu_prehistory,0-shot,accuracy,0.8425925925925926,0.020263764996385717
mistralai/Mixtral-8x7B-v0.1,mmlu_moral_scenarios,0-shot,accuracy,0.4100558659217877,0.016449708209026078
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_world_history,0-shot,accuracy,0.8860759493670886,0.020681745135884565
mistralai/Mixtral-8x7B-v0.1,mmlu_moral_disputes,0-shot,accuracy,0.8005780346820809,0.02151190065425256
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_law,0-shot,accuracy,0.5423728813559322,0.012724296550980188
mistralai/Mixtral-8x7B-v0.1,mmlu_logical_fallacies,0-shot,accuracy,0.7852760736196319,0.032262193772867744
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_us_history,0-shot,accuracy,0.8627450980392157,0.024152225962801588
mistralai/Mixtral-8x7B-v0.1,mmlu_philosophy,0-shot,accuracy,0.7877813504823151,0.023222756797435136
mistralai/Mixtral-8x7B-v0.1,mmlu_jurisprudence,0-shot,accuracy,0.8333333333333334,0.03602814176392643
mistralai/Mixtral-8x7B-v0.1,mmlu_international_law,0-shot,accuracy,0.859504132231405,0.031722334260021585
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_european_history,0-shot,accuracy,0.806060606060606,0.030874145136562097
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9430051813471503,0.01673108529360757
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_microeconomics,0-shot,accuracy,0.7815126050420168,0.026841514322958945
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_geography,0-shot,accuracy,0.8636363636363636,0.024450155973189835
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_psychology,0-shot,accuracy,0.8844036697247707,0.01370874953417264
mistralai/Mixtral-8x7B-v0.1,mmlu_public_relations,0-shot,accuracy,0.6909090909090909,0.044262946482000985
mistralai/Mixtral-8x7B-v0.1,mmlu_us_foreign_policy,0-shot,accuracy,0.93,0.025643239997624294
mistralai/Mixtral-8x7B-v0.1,mmlu_sociology,0-shot,accuracy,0.8905472636815921,0.02207632610182463
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7128205128205128,0.022939925418530613
mistralai/Mixtral-8x7B-v0.1,mmlu_security_studies,0-shot,accuracy,0.7877551020408163,0.026176967197866767
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_psychology,0-shot,accuracy,0.7794117647058824,0.01677467236546851
mistralai/Mixtral-8x7B-v0.1,mmlu_human_sexuality,0-shot,accuracy,0.816793893129771,0.033927709264947335
mistralai/Mixtral-8x7B-v0.1,mmlu_econometrics,0-shot,accuracy,0.6140350877192983,0.04579639422070434
mistralai/Mixtral-8x7B-v0.1,mmlu_miscellaneous,0-shot,accuracy,0.879948914431673,0.011622736692041261
mistralai/Mixtral-8x7B-v0.1,mmlu_marketing,0-shot,accuracy,0.9145299145299145,0.018315891685625838
mistralai/Mixtral-8x7B-v0.1,mmlu_management,0-shot,accuracy,0.8737864077669902,0.03288180278808628
mistralai/Mixtral-8x7B-v0.1,mmlu_nutrition,0-shot,accuracy,0.8300653594771242,0.021505383121231368
mistralai/Mixtral-8x7B-v0.1,mmlu_medical_genetics,0-shot,accuracy,0.75,0.04351941398892446
mistralai/Mixtral-8x7B-v0.1,mmlu_human_aging,0-shot,accuracy,0.7847533632286996,0.027584066602208274
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_medicine,0-shot,accuracy,0.8014705882352942,0.024231013370541114
mistralai/Mixtral-8x7B-v0.1,mmlu_college_medicine,0-shot,accuracy,0.7167630057803468,0.034355680560478746
mistralai/Mixtral-8x7B-v0.1,mmlu_business_ethics,0-shot,accuracy,0.75,0.04351941398892446
mistralai/Mixtral-8x7B-v0.1,mmlu_clinical_knowledge,0-shot,accuracy,0.7849056603773585,0.025288394502891363
mistralai/Mixtral-8x7B-v0.1,mmlu_global_facts,0-shot,accuracy,0.47,0.050161355804659205
mistralai/Mixtral-8x7B-v0.1,mmlu_virology,0-shot,accuracy,0.5120481927710844,0.03891364495835817
mistralai/Mixtral-8x7B-v0.1,mmlu_professional_accounting,0-shot,accuracy,0.524822695035461,0.02979071924382972
mistralai/Mixtral-8x7B-v0.1,mmlu_college_physics,0-shot,accuracy,0.47058823529411764,0.049665709039785295
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_physics,0-shot,accuracy,0.48344370860927155,0.040802441856289715
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_biology,0-shot,accuracy,0.8451612903225807,0.020579287326583227
mistralai/Mixtral-8x7B-v0.1,mmlu_college_biology,0-shot,accuracy,0.8541666666666666,0.029514245964291776
mistralai/Mixtral-8x7B-v0.1,mmlu_anatomy,0-shot,accuracy,0.6962962962962963,0.039725528847851375
mistralai/Mixtral-8x7B-v0.1,mmlu_college_chemistry,0-shot,accuracy,0.53,0.050161355804659205
mistralai/Mixtral-8x7B-v0.1,mmlu_computer_security,0-shot,accuracy,0.82,0.038612291966536934
mistralai/Mixtral-8x7B-v0.1,mmlu_college_computer_science,0-shot,accuracy,0.6,0.049236596391733084
mistralai/Mixtral-8x7B-v0.1,mmlu_astronomy,0-shot,accuracy,0.8157894736842105,0.0315469804508223
mistralai/Mixtral-8x7B-v0.1,mmlu_college_mathematics,0-shot,accuracy,0.5,0.050251890762960605
mistralai/Mixtral-8x7B-v0.1,mmlu_conceptual_physics,0-shot,accuracy,0.6680851063829787,0.03078373675774565
mistralai/Mixtral-8x7B-v0.1,mmlu_abstract_algebra,0-shot,accuracy,0.34,0.04760952285695236
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_computer_science,0-shot,accuracy,0.71,0.045604802157206845
mistralai/Mixtral-8x7B-v0.1,mmlu_machine_learning,0-shot,accuracy,0.5714285714285714,0.04697113923010212
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_chemistry,0-shot,accuracy,0.6305418719211823,0.033959703819985726
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_statistics,0-shot,accuracy,0.6527777777777778,0.032468872436376486
mistralai/Mixtral-8x7B-v0.1,mmlu_elementary_mathematics,0-shot,accuracy,0.47883597883597884,0.025728230952130723
mistralai/Mixtral-8x7B-v0.1,mmlu_electrical_engineering,0-shot,accuracy,0.6827586206896552,0.038783523721386215
mistralai/Mixtral-8x7B-v0.1,mmlu_high_school_mathematics,0-shot,accuracy,0.35185185185185186,0.029116617606083004
mistralai/Mixtral-8x7B-v0.1,arc_challenge,25-shot,accuracy,0.6348122866894198,0.014070265519268804
mistralai/Mixtral-8x7B-v0.1,arc_challenge,25-shot,acc_norm,0.6655290102389079,0.013787460322441372
mistralai/Mixtral-8x7B-v0.1,truthfulqa_mc2,0-shot,accuracy,0.48611514866693484,0.01457621729243261
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,bleu_max,27.87574335360594,0.8116531950644869
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,bleu_acc,0.4430844553243574,0.017389730346877123
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,bleu_diff,-2.5011189775154095,0.8046394649945218
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge1_max,53.64568184726008,0.863385829484222
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge1_acc,0.4430844553243574,0.01738973034687712
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge1_diff,-3.586087198522764,0.9509186358018955
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge2_max,38.03594067433759,1.016784051264887
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge2_acc,0.37576499388004897,0.016954584060214287
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rouge2_diff,-4.7518222494468025,1.101614085994352
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rougeL_max,50.9726724729725,0.8717408425200354
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rougeL_acc,0.43084455324357407,0.01733527247533237
mistralai/Mixtral-8x7B-v0.1,truthfulqa_gen,0-shot,rougeL_diff,-3.686864596715992,0.9550700976719013
mistralai/Mixtral-8x7B-v0.1,truthfulqa_mc1,0-shot,accuracy,0.34516523867809057,0.01664310331927494
EleutherAI/pythia-12b,arc:challenge,25-shot,accuracy,0.37372013651877134,0.014137708601759096
EleutherAI/pythia-12b,arc:challenge,25-shot,acc_norm,0.39590443686006827,0.014291228393536588
EleutherAI/pythia-12b,hellaswag,10-shot,accuracy,0.502688707428799,0.004989709267191016
EleutherAI/pythia-12b,hellaswag,10-shot,acc_norm,0.688209520015933,0.00462278057520917
EleutherAI/pythia-12b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.04229525846816505
EleutherAI/pythia-12b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.04229525846816505
EleutherAI/pythia-12b,hendrycksTest-anatomy,5-shot,accuracy,0.3333333333333333,0.04072314811876837
EleutherAI/pythia-12b,hendrycksTest-anatomy,5-shot,acc_norm,0.3333333333333333,0.04072314811876837
EleutherAI/pythia-12b,hendrycksTest-astronomy,5-shot,accuracy,0.23684210526315788,0.034597776068105345
EleutherAI/pythia-12b,hendrycksTest-astronomy,5-shot,acc_norm,0.23684210526315788,0.034597776068105345
EleutherAI/pythia-12b,hendrycksTest-business_ethics,5-shot,accuracy,0.21,0.04093601807403326
EleutherAI/pythia-12b,hendrycksTest-business_ethics,5-shot,acc_norm,0.21,0.04093601807403326
EleutherAI/pythia-12b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.26037735849056604,0.027008766090708094
EleutherAI/pythia-12b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.26037735849056604,0.027008766090708094
EleutherAI/pythia-12b,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
EleutherAI/pythia-12b,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.03685651095897532
EleutherAI/pythia-12b,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909282
EleutherAI/pythia-12b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909282
EleutherAI/pythia-12b,hendrycksTest-college_computer_science,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-12b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-12b,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-12b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/pythia-12b,hendrycksTest-college_medicine,5-shot,accuracy,0.23121387283236994,0.0321473730202947
EleutherAI/pythia-12b,hendrycksTest-college_medicine,5-shot,acc_norm,0.23121387283236994,0.0321473730202947
EleutherAI/pythia-12b,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237656
EleutherAI/pythia-12b,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237656
EleutherAI/pythia-12b,hendrycksTest-computer_security,5-shot,accuracy,0.38,0.048783173121456316
EleutherAI/pythia-12b,hendrycksTest-computer_security,5-shot,acc_norm,0.38,0.048783173121456316
EleutherAI/pythia-12b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2297872340425532,0.027501752944412424
EleutherAI/pythia-12b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2297872340425532,0.027501752944412424
EleutherAI/pythia-12b,hendrycksTest-econometrics,5-shot,accuracy,0.20175438596491227,0.037752050135836386
EleutherAI/pythia-12b,hendrycksTest-econometrics,5-shot,acc_norm,0.20175438596491227,0.037752050135836386
EleutherAI/pythia-12b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.27586206896551724,0.037245636197746325
EleutherAI/pythia-12b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.27586206896551724,0.037245636197746325
EleutherAI/pythia-12b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2698412698412698,0.022860838309232072
EleutherAI/pythia-12b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2698412698412698,0.022860838309232072
EleutherAI/pythia-12b,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.03567016675276863
EleutherAI/pythia-12b,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.03567016675276863
EleutherAI/pythia-12b,hendrycksTest-global_facts,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/pythia-12b,hendrycksTest-global_facts,5-shot,acc_norm,0.28,0.04512608598542128
EleutherAI/pythia-12b,hendrycksTest-high_school_biology,5-shot,accuracy,0.3161290322580645,0.02645087448904277
EleutherAI/pythia-12b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.3161290322580645,0.02645087448904277
EleutherAI/pythia-12b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.03127090713297698
EleutherAI/pythia-12b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.03127090713297698
EleutherAI/pythia-12b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-12b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/pythia-12b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.18181818181818182,0.03011768892950359
EleutherAI/pythia-12b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.18181818181818182,0.03011768892950359
EleutherAI/pythia-12b,hendrycksTest-high_school_geography,5-shot,accuracy,0.3282828282828283,0.03345678422756776
EleutherAI/pythia-12b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3282828282828283,0.03345678422756776
EleutherAI/pythia-12b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.2849740932642487,0.03257714077709661
EleutherAI/pythia-12b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.2849740932642487,0.03257714077709661
EleutherAI/pythia-12b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2641025641025641,0.022352193737453285
EleutherAI/pythia-12b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2641025641025641,0.022352193737453285
EleutherAI/pythia-12b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.27037037037037037,0.02708037281514566
EleutherAI/pythia-12b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.27037037037037037,0.02708037281514566
EleutherAI/pythia-12b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23529411764705882,0.027553614467863797
EleutherAI/pythia-12b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23529411764705882,0.027553614467863797
EleutherAI/pythia-12b,hendrycksTest-high_school_physics,5-shot,accuracy,0.26490066225165565,0.03603038545360385
EleutherAI/pythia-12b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.26490066225165565,0.03603038545360385
EleutherAI/pythia-12b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.26788990825688075,0.01898746225797865
EleutherAI/pythia-12b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.26788990825688075,0.01898746225797865
EleutherAI/pythia-12b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.36574074074074076,0.03284738857647207
EleutherAI/pythia-12b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.36574074074074076,0.03284738857647207
EleutherAI/pythia-12b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.27450980392156865,0.03132179803083291
EleutherAI/pythia-12b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.27450980392156865,0.03132179803083291
EleutherAI/pythia-12b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25316455696202533,0.0283046579430353
EleutherAI/pythia-12b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25316455696202533,0.0283046579430353
EleutherAI/pythia-12b,hendrycksTest-human_aging,5-shot,accuracy,0.28699551569506726,0.030360379710291964
EleutherAI/pythia-12b,hendrycksTest-human_aging,5-shot,acc_norm,0.28699551569506726,0.030360379710291964
EleutherAI/pythia-12b,hendrycksTest-human_sexuality,5-shot,accuracy,0.24427480916030533,0.03768335959728742
EleutherAI/pythia-12b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.24427480916030533,0.03768335959728742
EleutherAI/pythia-12b,hendrycksTest-international_law,5-shot,accuracy,0.2975206611570248,0.04173349148083497
EleutherAI/pythia-12b,hendrycksTest-international_law,5-shot,acc_norm,0.2975206611570248,0.04173349148083497
EleutherAI/pythia-12b,hendrycksTest-jurisprudence,5-shot,accuracy,0.26851851851851855,0.04284467968052191
EleutherAI/pythia-12b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.26851851851851855,0.04284467968052191
EleutherAI/pythia-12b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.294478527607362,0.03581165790474082
EleutherAI/pythia-12b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.294478527607362,0.03581165790474082
EleutherAI/pythia-12b,hendrycksTest-machine_learning,5-shot,accuracy,0.33035714285714285,0.04464285714285714
EleutherAI/pythia-12b,hendrycksTest-machine_learning,5-shot,acc_norm,0.33035714285714285,0.04464285714285714
EleutherAI/pythia-12b,hendrycksTest-management,5-shot,accuracy,0.21359223300970873,0.04058042015646034
EleutherAI/pythia-12b,hendrycksTest-management,5-shot,acc_norm,0.21359223300970873,0.04058042015646034
EleutherAI/pythia-12b,hendrycksTest-marketing,5-shot,accuracy,0.26495726495726496,0.028911208802749475
EleutherAI/pythia-12b,hendrycksTest-marketing,5-shot,acc_norm,0.26495726495726496,0.028911208802749475
EleutherAI/pythia-12b,hendrycksTest-medical_genetics,5-shot,accuracy,0.19,0.03942772444036623
EleutherAI/pythia-12b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.19,0.03942772444036623
EleutherAI/pythia-12b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2541507024265645,0.015569254692045755
EleutherAI/pythia-12b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2541507024265645,0.015569254692045755
EleutherAI/pythia-12b,hendrycksTest-moral_disputes,5-shot,accuracy,0.28901734104046245,0.02440517393578323
EleutherAI/pythia-12b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.28901734104046245,0.02440517393578323
EleutherAI/pythia-12b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
EleutherAI/pythia-12b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
EleutherAI/pythia-12b,hendrycksTest-nutrition,5-shot,accuracy,0.2679738562091503,0.025360603796242557
EleutherAI/pythia-12b,hendrycksTest-nutrition,5-shot,acc_norm,0.2679738562091503,0.025360603796242557
EleutherAI/pythia-12b,hendrycksTest-philosophy,5-shot,accuracy,0.3022508038585209,0.02608270069539965
EleutherAI/pythia-12b,hendrycksTest-philosophy,5-shot,acc_norm,0.3022508038585209,0.02608270069539965
EleutherAI/pythia-12b,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.024569223600460845
EleutherAI/pythia-12b,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.024569223600460845
EleutherAI/pythia-12b,hendrycksTest-professional_accounting,5-shot,accuracy,0.26595744680851063,0.02635806569888059
EleutherAI/pythia-12b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.26595744680851063,0.02635806569888059
EleutherAI/pythia-12b,hendrycksTest-professional_law,5-shot,accuracy,0.25554106910039115,0.01113985783359853
EleutherAI/pythia-12b,hendrycksTest-professional_law,5-shot,acc_norm,0.25554106910039115,0.01113985783359853
EleutherAI/pythia-12b,hendrycksTest-professional_medicine,5-shot,accuracy,0.20220588235294118,0.02439819298665492
EleutherAI/pythia-12b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.20220588235294118,0.02439819298665492
EleutherAI/pythia-12b,hendrycksTest-professional_psychology,5-shot,accuracy,0.25980392156862747,0.01774089950917779
EleutherAI/pythia-12b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25980392156862747,0.01774089950917779
EleutherAI/pythia-12b,hendrycksTest-public_relations,5-shot,accuracy,0.23636363636363636,0.040693063197213754
EleutherAI/pythia-12b,hendrycksTest-public_relations,5-shot,acc_norm,0.23636363636363636,0.040693063197213754
EleutherAI/pythia-12b,hendrycksTest-security_studies,5-shot,accuracy,0.2571428571428571,0.027979823538744546
EleutherAI/pythia-12b,hendrycksTest-security_studies,5-shot,acc_norm,0.2571428571428571,0.027979823538744546
EleutherAI/pythia-12b,hendrycksTest-sociology,5-shot,accuracy,0.3034825870646766,0.032510068164586174
EleutherAI/pythia-12b,hendrycksTest-sociology,5-shot,acc_norm,0.3034825870646766,0.032510068164586174
EleutherAI/pythia-12b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-12b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/pythia-12b,hendrycksTest-virology,5-shot,accuracy,0.3192771084337349,0.036293353299478595
EleutherAI/pythia-12b,hendrycksTest-virology,5-shot,acc_norm,0.3192771084337349,0.036293353299478595
EleutherAI/pythia-12b,hendrycksTest-world_religions,5-shot,accuracy,0.30409356725146197,0.03528211258245232
EleutherAI/pythia-12b,hendrycksTest-world_religions,5-shot,acc_norm,0.30409356725146197,0.03528211258245232
EleutherAI/pythia-12b,truthfulqa:mc,0-shot,mc1,0.20563035495716034,0.014148482219460969
EleutherAI/pythia-12b,truthfulqa:mc,0-shot,mc2,0.3185371350132154,0.013103356003141161
EleutherAI/pythia-12b,drop,3-shot,accuracy,0.0006291946308724832,0.0002568002749723885
EleutherAI/pythia-12b,drop,3-shot,f1,0.04447986577181216,0.0010992181181045415
EleutherAI/pythia-12b,gsm8k,5-shot,accuracy,0.017437452615617893,0.003605486867998272
EleutherAI/pythia-12b,winogrande,5-shot,accuracy,0.6416732438831886,0.013476581172567535
EleutherAI/gpt-j-6b,arc:challenge,25-shot,accuracy,0.36860068259385664,0.014097810678042184
EleutherAI/gpt-j-6b,arc:challenge,25-shot,acc_norm,0.4138225255972696,0.014392730009221007
EleutherAI/gpt-j-6b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-j-6b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/gpt-j-6b,hendrycksTest-anatomy,5-shot,accuracy,0.26666666666666666,0.03820169914517904
EleutherAI/gpt-j-6b,hendrycksTest-anatomy,5-shot,acc_norm,0.26666666666666666,0.03820169914517904
EleutherAI/gpt-j-6b,hendrycksTest-astronomy,5-shot,accuracy,0.26973684210526316,0.03611780560284898
EleutherAI/gpt-j-6b,hendrycksTest-astronomy,5-shot,acc_norm,0.26973684210526316,0.03611780560284898
EleutherAI/gpt-j-6b,hendrycksTest-business_ethics,5-shot,accuracy,0.28,0.04512608598542126
EleutherAI/gpt-j-6b,hendrycksTest-business_ethics,5-shot,acc_norm,0.28,0.04512608598542126
EleutherAI/gpt-j-6b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.26037735849056604,0.027008766090708104
EleutherAI/gpt-j-6b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.26037735849056604,0.027008766090708104
EleutherAI/gpt-j-6b,hendrycksTest-college_biology,5-shot,accuracy,0.24305555555555555,0.03586879280080339
EleutherAI/gpt-j-6b,hendrycksTest-college_biology,5-shot,acc_norm,0.24305555555555555,0.03586879280080339
EleutherAI/gpt-j-6b,hendrycksTest-college_chemistry,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/gpt-j-6b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/gpt-j-6b,hendrycksTest-college_computer_science,5-shot,accuracy,0.23,0.04229525846816508
EleutherAI/gpt-j-6b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.23,0.04229525846816508
EleutherAI/gpt-j-6b,hendrycksTest-college_mathematics,5-shot,accuracy,0.32,0.04688261722621505
EleutherAI/gpt-j-6b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.32,0.04688261722621505
EleutherAI/gpt-j-6b,hendrycksTest-college_medicine,5-shot,accuracy,0.2832369942196532,0.03435568056047875
EleutherAI/gpt-j-6b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2832369942196532,0.03435568056047875
EleutherAI/gpt-j-6b,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237655
EleutherAI/gpt-j-6b,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237655
EleutherAI/gpt-j-6b,hendrycksTest-computer_security,5-shot,accuracy,0.41,0.04943110704237101
EleutherAI/gpt-j-6b,hendrycksTest-computer_security,5-shot,acc_norm,0.41,0.04943110704237101
EleutherAI/gpt-j-6b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3404255319148936,0.03097669299853443
EleutherAI/gpt-j-6b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3404255319148936,0.03097669299853443
EleutherAI/gpt-j-6b,hendrycksTest-econometrics,5-shot,accuracy,0.2807017543859649,0.042270544512322
EleutherAI/gpt-j-6b,hendrycksTest-econometrics,5-shot,acc_norm,0.2807017543859649,0.042270544512322
EleutherAI/gpt-j-6b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2896551724137931,0.037800192304380135
EleutherAI/gpt-j-6b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2896551724137931,0.037800192304380135
EleutherAI/gpt-j-6b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.23015873015873015,0.021679219663693145
EleutherAI/gpt-j-6b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.23015873015873015,0.021679219663693145
EleutherAI/gpt-j-6b,hendrycksTest-formal_logic,5-shot,accuracy,0.18253968253968253,0.03455071019102149
EleutherAI/gpt-j-6b,hendrycksTest-formal_logic,5-shot,acc_norm,0.18253968253968253,0.03455071019102149
EleutherAI/gpt-j-6b,hendrycksTest-global_facts,5-shot,accuracy,0.21,0.040936018074033256
EleutherAI/gpt-j-6b,hendrycksTest-global_facts,5-shot,acc_norm,0.21,0.040936018074033256
EleutherAI/gpt-j-6b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2032258064516129,0.022891687984554966
EleutherAI/gpt-j-6b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2032258064516129,0.022891687984554966
EleutherAI/gpt-j-6b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.24630541871921183,0.030315099285617732
EleutherAI/gpt-j-6b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.24630541871921183,0.030315099285617732
EleutherAI/gpt-j-6b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.15,0.03588702812826369
EleutherAI/gpt-j-6b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.15,0.03588702812826369
EleutherAI/gpt-j-6b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.28484848484848485,0.035243908445117836
EleutherAI/gpt-j-6b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.28484848484848485,0.035243908445117836
EleutherAI/gpt-j-6b,hendrycksTest-high_school_geography,5-shot,accuracy,0.23232323232323232,0.030088629490217483
EleutherAI/gpt-j-6b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.23232323232323232,0.030088629490217483
EleutherAI/gpt-j-6b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.22279792746113988,0.03003114797764154
EleutherAI/gpt-j-6b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.22279792746113988,0.03003114797764154
EleutherAI/gpt-j-6b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2230769230769231,0.021107730127243984
EleutherAI/gpt-j-6b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2230769230769231,0.021107730127243984
EleutherAI/gpt-j-6b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2740740740740741,0.027195934804085622
EleutherAI/gpt-j-6b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2740740740740741,0.027195934804085622
EleutherAI/gpt-j-6b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.25630252100840334,0.02835962087053395
EleutherAI/gpt-j-6b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.25630252100840334,0.02835962087053395
EleutherAI/gpt-j-6b,hendrycksTest-high_school_physics,5-shot,accuracy,0.25165562913907286,0.035433042343899844
EleutherAI/gpt-j-6b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.25165562913907286,0.035433042343899844
EleutherAI/gpt-j-6b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.22568807339449543,0.017923087667803057
EleutherAI/gpt-j-6b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.22568807339449543,0.017923087667803057
EleutherAI/gpt-j-6b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.16203703703703703,0.025130453652268455
EleutherAI/gpt-j-6b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.16203703703703703,0.025130453652268455
EleutherAI/gpt-j-6b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.28921568627450983,0.03182231867647555
EleutherAI/gpt-j-6b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.28921568627450983,0.03182231867647555
EleutherAI/gpt-j-6b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2869198312236287,0.02944377302259469
EleutherAI/gpt-j-6b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2869198312236287,0.02944377302259469
EleutherAI/gpt-j-6b,hendrycksTest-human_aging,5-shot,accuracy,0.336322869955157,0.031708824268455005
EleutherAI/gpt-j-6b,hendrycksTest-human_aging,5-shot,acc_norm,0.336322869955157,0.031708824268455005
EleutherAI/gpt-j-6b,hendrycksTest-human_sexuality,5-shot,accuracy,0.21374045801526717,0.0359546161177469
EleutherAI/gpt-j-6b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.21374045801526717,0.0359546161177469
EleutherAI/gpt-j-6b,hendrycksTest-international_law,5-shot,accuracy,0.24793388429752067,0.03941897526516302
EleutherAI/gpt-j-6b,hendrycksTest-international_law,5-shot,acc_norm,0.24793388429752067,0.03941897526516302
EleutherAI/gpt-j-6b,hendrycksTest-jurisprudence,5-shot,accuracy,0.28703703703703703,0.043733130409147614
EleutherAI/gpt-j-6b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.28703703703703703,0.043733130409147614
EleutherAI/gpt-j-6b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25153374233128833,0.03408997886857529
EleutherAI/gpt-j-6b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25153374233128833,0.03408997886857529
EleutherAI/gpt-j-6b,hendrycksTest-machine_learning,5-shot,accuracy,0.38392857142857145,0.04616143075028547
EleutherAI/gpt-j-6b,hendrycksTest-machine_learning,5-shot,acc_norm,0.38392857142857145,0.04616143075028547
EleutherAI/gpt-j-6b,hendrycksTest-management,5-shot,accuracy,0.21359223300970873,0.040580420156460344
EleutherAI/gpt-j-6b,hendrycksTest-management,5-shot,acc_norm,0.21359223300970873,0.040580420156460344
EleutherAI/gpt-j-6b,hendrycksTest-marketing,5-shot,accuracy,0.26495726495726496,0.028911208802749482
EleutherAI/gpt-j-6b,hendrycksTest-marketing,5-shot,acc_norm,0.26495726495726496,0.028911208802749482
EleutherAI/gpt-j-6b,hendrycksTest-medical_genetics,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-j-6b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/gpt-j-6b,hendrycksTest-miscellaneous,5-shot,accuracy,0.31417624521072796,0.016599291735884904
EleutherAI/gpt-j-6b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.31417624521072796,0.016599291735884904
EleutherAI/gpt-j-6b,hendrycksTest-moral_disputes,5-shot,accuracy,0.27167630057803466,0.023948512905468358
EleutherAI/gpt-j-6b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.27167630057803466,0.023948512905468358
EleutherAI/gpt-j-6b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2435754189944134,0.014355911964767864
EleutherAI/gpt-j-6b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2435754189944134,0.014355911964767864
EleutherAI/gpt-j-6b,hendrycksTest-nutrition,5-shot,accuracy,0.2647058823529412,0.025261691219729494
EleutherAI/gpt-j-6b,hendrycksTest-nutrition,5-shot,acc_norm,0.2647058823529412,0.025261691219729494
EleutherAI/gpt-j-6b,hendrycksTest-philosophy,5-shot,accuracy,0.2604501607717042,0.024926723224845543
EleutherAI/gpt-j-6b,hendrycksTest-philosophy,5-shot,acc_norm,0.2604501607717042,0.024926723224845543
EleutherAI/gpt-j-6b,hendrycksTest-prehistory,5-shot,accuracy,0.3117283950617284,0.02577311116963045
EleutherAI/gpt-j-6b,hendrycksTest-prehistory,5-shot,acc_norm,0.3117283950617284,0.02577311116963045
EleutherAI/gpt-j-6b,hendrycksTest-professional_accounting,5-shot,accuracy,0.28368794326241137,0.02689170942834396
EleutherAI/gpt-j-6b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.28368794326241137,0.02689170942834396
EleutherAI/gpt-j-6b,hendrycksTest-professional_law,5-shot,accuracy,0.2894393741851369,0.011582659702210252
EleutherAI/gpt-j-6b,hendrycksTest-professional_law,5-shot,acc_norm,0.2894393741851369,0.011582659702210252
EleutherAI/gpt-j-6b,hendrycksTest-professional_medicine,5-shot,accuracy,0.2426470588235294,0.02604066247420127
EleutherAI/gpt-j-6b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2426470588235294,0.02604066247420127
EleutherAI/gpt-j-6b,hendrycksTest-professional_psychology,5-shot,accuracy,0.27941176470588236,0.018152871051538816
EleutherAI/gpt-j-6b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.27941176470588236,0.018152871051538816
EleutherAI/gpt-j-6b,hendrycksTest-public_relations,5-shot,accuracy,0.34545454545454546,0.04554619617541054
EleutherAI/gpt-j-6b,hendrycksTest-public_relations,5-shot,acc_norm,0.34545454545454546,0.04554619617541054
EleutherAI/gpt-j-6b,hendrycksTest-security_studies,5-shot,accuracy,0.35918367346938773,0.030713560455108493
EleutherAI/gpt-j-6b,hendrycksTest-security_studies,5-shot,acc_norm,0.35918367346938773,0.030713560455108493
EleutherAI/gpt-j-6b,hendrycksTest-sociology,5-shot,accuracy,0.27860696517412936,0.031700561834973086
EleutherAI/gpt-j-6b,hendrycksTest-sociology,5-shot,acc_norm,0.27860696517412936,0.031700561834973086
EleutherAI/gpt-j-6b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.04560480215720684
EleutherAI/gpt-j-6b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.04560480215720684
EleutherAI/gpt-j-6b,hendrycksTest-virology,5-shot,accuracy,0.3373493975903614,0.03680783690727581
EleutherAI/gpt-j-6b,hendrycksTest-virology,5-shot,acc_norm,0.3373493975903614,0.03680783690727581
EleutherAI/gpt-j-6b,hendrycksTest-world_religions,5-shot,accuracy,0.34502923976608185,0.03645981377388807
EleutherAI/gpt-j-6b,hendrycksTest-world_religions,5-shot,acc_norm,0.34502923976608185,0.03645981377388807
EleutherAI/gpt-j-6b,truthfulqa:mc,0-shot,mc1,0.20195838433292534,0.014053957441512348
EleutherAI/gpt-j-6b,truthfulqa:mc,0-shot,mc2,0.35962472949507807,0.013462019520008167
EleutherAI/gpt-j-6b,drop,3-shot,accuracy,0.0009437919463087249,0.00031446531194132096
EleutherAI/gpt-j-6b,drop,3-shot,f1,0.0461545721476511,0.0011697500055632092
gpt2,anli,0-shot,accuracy,0.345,0.0147
gpt2,xnli,0-shot,accuracy,0.3506,0.0249
gpt2,mathqa,5-shot,accuracy,0.2111,0.0075
gpt2,logiqa2,0-shot,accuracy,0.2392,0.0108
gpt2,mmlu_world_religions,0-shot,accuracy,0.21052631578947367,0.031267817146631786
gpt2,mmlu_formal_logic,0-shot,accuracy,0.14285714285714285,0.031298431857438073
gpt2,mmlu_prehistory,0-shot,accuracy,0.22530864197530864,0.023246202647819746
gpt2,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
gpt2,mmlu_high_school_world_history,0-shot,accuracy,0.2489451476793249,0.028146970599422644
gpt2,mmlu_moral_disputes,0-shot,accuracy,0.24277456647398843,0.023083658586984204
gpt2,mmlu_professional_law,0-shot,accuracy,0.24641460234680573,0.011005971399927244
gpt2,mmlu_logical_fallacies,0-shot,accuracy,0.25766871165644173,0.03436150827846917
gpt2,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
gpt2,mmlu_philosophy,0-shot,accuracy,0.2540192926045016,0.02472386150477169
gpt2,mmlu_jurisprudence,0-shot,accuracy,0.21296296296296297,0.039578354719809784
gpt2,mmlu_international_law,0-shot,accuracy,0.32231404958677684,0.042664163633521685
gpt2,mmlu_high_school_european_history,0-shot,accuracy,0.21212121212121213,0.03192271569548299
gpt2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.36787564766839376,0.034801756684660366
gpt2,mmlu_high_school_microeconomics,0-shot,accuracy,0.28991596638655465,0.029472485833136098
gpt2,mmlu_high_school_geography,0-shot,accuracy,0.35353535353535354,0.03406086723547153
gpt2,mmlu_high_school_psychology,0-shot,accuracy,0.3486238532110092,0.020431254090714328
gpt2,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
gpt2,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.044619604333847394
gpt2,mmlu_sociology,0-shot,accuracy,0.22885572139303484,0.029705284056772453
gpt2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2717948717948718,0.022556551010132368
gpt2,mmlu_security_studies,0-shot,accuracy,0.4,0.031362502409358936
gpt2,mmlu_professional_psychology,0-shot,accuracy,0.2630718954248366,0.01781267654232065
gpt2,mmlu_human_sexuality,0-shot,accuracy,0.26717557251908397,0.03880848301082397
gpt2,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.0409698513984367
gpt2,mmlu_miscellaneous,0-shot,accuracy,0.21839080459770116,0.01477435831993449
gpt2,mmlu_marketing,0-shot,accuracy,0.1794871794871795,0.025140935950335445
gpt2,mmlu_management,0-shot,accuracy,0.3300970873786408,0.046561471100123486
gpt2,mmlu_nutrition,0-shot,accuracy,0.21895424836601307,0.02367908986180772
gpt2,mmlu_medical_genetics,0-shot,accuracy,0.27,0.0446196043338474
gpt2,mmlu_human_aging,0-shot,accuracy,0.26905829596412556,0.029763779406874972
gpt2,mmlu_professional_medicine,0-shot,accuracy,0.44485294117647056,0.03018753206032938
gpt2,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
gpt2,mmlu_business_ethics,0-shot,accuracy,0.18,0.038612291966536955
gpt2,mmlu_clinical_knowledge,0-shot,accuracy,0.2339622641509434,0.02605529690115292
gpt2,mmlu_global_facts,0-shot,accuracy,0.15,0.035887028128263714
gpt2,mmlu_virology,0-shot,accuracy,0.1927710843373494,0.03070982405056527
gpt2,mmlu_professional_accounting,0-shot,accuracy,0.26595744680851063,0.026358065698880592
gpt2,mmlu_college_physics,0-shot,accuracy,0.2549019607843137,0.043364327079931785
gpt2,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
gpt2,mmlu_high_school_biology,0-shot,accuracy,0.3032258064516129,0.02614868593067175
gpt2,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.03476590104304134
gpt2,mmlu_anatomy,0-shot,accuracy,0.23703703703703705,0.03673731683969506
gpt2,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368445
gpt2,mmlu_computer_security,0-shot,accuracy,0.16,0.03684529491774709
gpt2,mmlu_college_computer_science,0-shot,accuracy,0.28,0.04512608598542127
gpt2,mmlu_astronomy,0-shot,accuracy,0.16447368421052633,0.03016753346863271
gpt2,mmlu_college_mathematics,0-shot,accuracy,0.28,0.04512608598542127
gpt2,mmlu_conceptual_physics,0-shot,accuracy,0.2680851063829787,0.02895734278834235
gpt2,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.040936018074033256
gpt2,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.04408440022768078
gpt2,mmlu_machine_learning,0-shot,accuracy,0.23214285714285715,0.04007341809755806
gpt2,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.031270907132976984
gpt2,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
gpt2,mmlu_elementary_mathematics,0-shot,accuracy,0.25396825396825395,0.022418042891113946
gpt2,mmlu_electrical_engineering,0-shot,accuracy,0.2413793103448276,0.03565998174135302
gpt2,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.026842057873833713
gpt2,arc_challenge,25-shot,accuracy,0.19197952218430034,0.01150959890659812
gpt2,arc_challenge,25-shot,acc_norm,0.21843003412969283,0.01207429160570097
gpt2,hellaswag,10-shot,accuracy,0.291575383389763,0.004535589759202636
gpt2,hellaswag,10-shot,acc_norm,0.31517625970922125,0.004636365534819766
gpt2,truthfulqa_mc2,0-shot,accuracy,0.4069358340982317,0.01492194956899506
gpt2,truthfulqa_gen,0-shot,bleu_max,0.7189915567640092,0.18475025971013068
gpt2,truthfulqa_gen,0-shot,bleu_acc,0.009791921664626682,0.003447085998464435
gpt2,truthfulqa_gen,0-shot,bleu_diff,-0.16841589341392382,0.11447969225022704
gpt2,truthfulqa_gen,0-shot,rouge1_max,1.4549180041272496,0.31578733165082745
gpt2,truthfulqa_gen,0-shot,rouge1_acc,0.008567931456548347,0.003226445945631001
gpt2,truthfulqa_gen,0-shot,rouge1_diff,-0.3752635907645988,0.182185700437024
gpt2,truthfulqa_gen,0-shot,rouge2_max,0.8607031116173555,0.23682006006327766
gpt2,truthfulqa_gen,0-shot,rouge2_acc,0.0036719706242350062,0.002117413579031932
gpt2,truthfulqa_gen,0-shot,rouge2_diff,-0.3291615551523294,0.14899511750185152
gpt2,truthfulqa_gen,0-shot,rougeL_max,1.3855912860137218,0.30611710377860496
gpt2,truthfulqa_gen,0-shot,rougeL_acc,0.009791921664626682,0.0034470859984644473
gpt2,truthfulqa_gen,0-shot,rougeL_diff,-0.34950587425767154,0.16209746133838165
gpt2,truthfulqa_mc1,0-shot,accuracy,0.22766217870257038,0.014679255032111068
gpt2,winogrande,5-shot,accuracy,0.5114443567482242,0.014048804199859322
gpt2,gsm8k,5-shot,accuracy,0.006823351023502654,0.002267537102254489
openai-community/gpt2,arc:challenge,25-shot,accuracy,0.197098976109215,0.011625047669880633
openai-community/gpt2,arc:challenge,25-shot,acc_norm,0.22013651877133106,0.01210812488346097
openai-community/gpt2,hellaswag,10-shot,accuracy,0.29267078271260705,0.004540586983229993
openai-community/gpt2,hellaswag,10-shot,acc_norm,0.3152758414658435,0.0046367607625228515
openai-community/gpt2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.21,0.040936018074033256
openai-community/gpt2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.21,0.040936018074033256
openai-community/gpt2,hendrycksTest-anatomy,5-shot,accuracy,0.22962962962962963,0.03633384414073462
openai-community/gpt2,hendrycksTest-anatomy,5-shot,acc_norm,0.22962962962962963,0.03633384414073462
openai-community/gpt2,hendrycksTest-astronomy,5-shot,accuracy,0.16447368421052633,0.0301675334686327
openai-community/gpt2,hendrycksTest-astronomy,5-shot,acc_norm,0.16447368421052633,0.0301675334686327
openai-community/gpt2,hendrycksTest-business_ethics,5-shot,accuracy,0.17,0.0377525168068637
openai-community/gpt2,hendrycksTest-business_ethics,5-shot,acc_norm,0.17,0.0377525168068637
openai-community/gpt2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.24150943396226415,0.026341480371118345
openai-community/gpt2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.24150943396226415,0.026341480371118345
openai-community/gpt2,hendrycksTest-college_biology,5-shot,accuracy,0.2222222222222222,0.03476590104304134
openai-community/gpt2,hendrycksTest-college_biology,5-shot,acc_norm,0.2222222222222222,0.03476590104304134
openai-community/gpt2,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036846
openai-community/gpt2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036846
openai-community/gpt2,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.04512608598542128
openai-community/gpt2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.04512608598542128
openai-community/gpt2,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
openai-community/gpt2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.046056618647183814
openai-community/gpt2,hendrycksTest-college_medicine,5-shot,accuracy,0.24277456647398843,0.0326926380614177
openai-community/gpt2,hendrycksTest-college_medicine,5-shot,acc_norm,0.24277456647398843,0.0326926380614177
openai-community/gpt2,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.043364327079931785
openai-community/gpt2,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.043364327079931785
openai-community/gpt2,hendrycksTest-computer_security,5-shot,accuracy,0.16,0.03684529491774709
openai-community/gpt2,hendrycksTest-computer_security,5-shot,acc_norm,0.16,0.03684529491774709
openai-community/gpt2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2723404255319149,0.029101290698386698
openai-community/gpt2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2723404255319149,0.029101290698386698
openai-community/gpt2,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.041424397194893624
openai-community/gpt2,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.041424397194893624
openai-community/gpt2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.03565998174135302
openai-community/gpt2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.03565998174135302
openai-community/gpt2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25396825396825395,0.022418042891113942
openai-community/gpt2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25396825396825395,0.022418042891113942
openai-community/gpt2,hendrycksTest-formal_logic,5-shot,accuracy,0.14285714285714285,0.0312984318574381
openai-community/gpt2,hendrycksTest-formal_logic,5-shot,acc_norm,0.14285714285714285,0.0312984318574381
openai-community/gpt2,hendrycksTest-global_facts,5-shot,accuracy,0.15,0.035887028128263686
openai-community/gpt2,hendrycksTest-global_facts,5-shot,acc_norm,0.15,0.035887028128263686
openai-community/gpt2,hendrycksTest-high_school_biology,5-shot,accuracy,0.2967741935483871,0.025988500792411894
openai-community/gpt2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2967741935483871,0.025988500792411894
openai-community/gpt2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.03127090713297698
openai-community/gpt2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.03127090713297698
openai-community/gpt2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.26,0.04408440022768079
openai-community/gpt2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.26,0.04408440022768079
openai-community/gpt2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.21818181818181817,0.03225078108306289
openai-community/gpt2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.21818181818181817,0.03225078108306289
openai-community/gpt2,hendrycksTest-high_school_geography,5-shot,accuracy,0.35353535353535354,0.03406086723547153
openai-community/gpt2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.35353535353535354,0.03406086723547153
openai-community/gpt2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.36787564766839376,0.03480175668466036
openai-community/gpt2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.36787564766839376,0.03480175668466036
openai-community/gpt2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2717948717948718,0.022556551010132358
openai-community/gpt2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2717948717948718,0.022556551010132358
openai-community/gpt2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.026842057873833706
openai-community/gpt2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.026842057873833706
openai-community/gpt2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.28991596638655465,0.029472485833136098
openai-community/gpt2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.28991596638655465,0.029472485833136098
openai-community/gpt2,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.03631329803969654
openai-community/gpt2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.03631329803969654
openai-community/gpt2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3486238532110092,0.020431254090714328
openai-community/gpt2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3486238532110092,0.020431254090714328
openai-community/gpt2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
openai-community/gpt2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
openai-community/gpt2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.03039153369274154
openai-community/gpt2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.03039153369274154
openai-community/gpt2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.24472573839662448,0.027985699387036416
openai-community/gpt2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.24472573839662448,0.027985699387036416
openai-community/gpt2,hendrycksTest-human_aging,5-shot,accuracy,0.2914798206278027,0.030500283176545923
openai-community/gpt2,hendrycksTest-human_aging,5-shot,acc_norm,0.2914798206278027,0.030500283176545923
openai-community/gpt2,hendrycksTest-human_sexuality,5-shot,accuracy,0.26717557251908397,0.038808483010823944
openai-community/gpt2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.26717557251908397,0.038808483010823944
openai-community/gpt2,hendrycksTest-international_law,5-shot,accuracy,0.32231404958677684,0.04266416363352168
openai-community/gpt2,hendrycksTest-international_law,5-shot,acc_norm,0.32231404958677684,0.04266416363352168
openai-community/gpt2,hendrycksTest-jurisprudence,5-shot,accuracy,0.21296296296296297,0.03957835471980981
openai-community/gpt2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.21296296296296297,0.03957835471980981
openai-community/gpt2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26380368098159507,0.03462419931615623
openai-community/gpt2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26380368098159507,0.03462419931615623
openai-community/gpt2,hendrycksTest-machine_learning,5-shot,accuracy,0.25892857142857145,0.041577515398656284
openai-community/gpt2,hendrycksTest-machine_learning,5-shot,acc_norm,0.25892857142857145,0.041577515398656284
openai-community/gpt2,hendrycksTest-management,5-shot,accuracy,0.34951456310679613,0.04721188506097173
openai-community/gpt2,hendrycksTest-management,5-shot,acc_norm,0.34951456310679613,0.04721188506097173
openai-community/gpt2,hendrycksTest-marketing,5-shot,accuracy,0.1794871794871795,0.025140935950335418
openai-community/gpt2,hendrycksTest-marketing,5-shot,acc_norm,0.1794871794871795,0.025140935950335418
openai-community/gpt2,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.044619604333847394
openai-community/gpt2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.044619604333847394
openai-community/gpt2,hendrycksTest-miscellaneous,5-shot,accuracy,0.21583652618135377,0.014711684386139958
openai-community/gpt2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.21583652618135377,0.014711684386139958
openai-community/gpt2,hendrycksTest-moral_disputes,5-shot,accuracy,0.24277456647398843,0.0230836585869842
openai-community/gpt2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24277456647398843,0.0230836585869842
openai-community/gpt2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
openai-community/gpt2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
openai-community/gpt2,hendrycksTest-nutrition,5-shot,accuracy,0.21895424836601307,0.02367908986180772
openai-community/gpt2,hendrycksTest-nutrition,5-shot,acc_norm,0.21895424836601307,0.02367908986180772
openai-community/gpt2,hendrycksTest-philosophy,5-shot,accuracy,0.24758842443729903,0.024513879973621967
openai-community/gpt2,hendrycksTest-philosophy,5-shot,acc_norm,0.24758842443729903,0.024513879973621967
openai-community/gpt2,hendrycksTest-prehistory,5-shot,accuracy,0.22530864197530864,0.023246202647819746
openai-community/gpt2,hendrycksTest-prehistory,5-shot,acc_norm,0.22530864197530864,0.023246202647819746
openai-community/gpt2,hendrycksTest-professional_accounting,5-shot,accuracy,0.26595744680851063,0.026358065698880592
openai-community/gpt2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.26595744680851063,0.026358065698880592
openai-community/gpt2,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.010996156635142692
openai-community/gpt2,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.010996156635142692
openai-community/gpt2,hendrycksTest-professional_medicine,5-shot,accuracy,0.44485294117647056,0.030187532060329376
openai-community/gpt2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.44485294117647056,0.030187532060329376
openai-community/gpt2,hendrycksTest-professional_psychology,5-shot,accuracy,0.26143790849673204,0.017776947157528034
openai-community/gpt2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.26143790849673204,0.017776947157528034
openai-community/gpt2,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
openai-community/gpt2,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
openai-community/gpt2,hendrycksTest-security_studies,5-shot,accuracy,0.4,0.031362502409358936
openai-community/gpt2,hendrycksTest-security_studies,5-shot,acc_norm,0.4,0.031362502409358936
openai-community/gpt2,hendrycksTest-sociology,5-shot,accuracy,0.22885572139303484,0.029705284056772426
openai-community/gpt2,hendrycksTest-sociology,5-shot,acc_norm,0.22885572139303484,0.029705284056772426
openai-community/gpt2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.27,0.04461960433384739
openai-community/gpt2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.27,0.04461960433384739
openai-community/gpt2,hendrycksTest-virology,5-shot,accuracy,0.1927710843373494,0.030709824050565274
openai-community/gpt2,hendrycksTest-virology,5-shot,acc_norm,0.1927710843373494,0.030709824050565274
openai-community/gpt2,hendrycksTest-world_religions,5-shot,accuracy,0.21052631578947367,0.0312678171466318
openai-community/gpt2,hendrycksTest-world_religions,5-shot,acc_norm,0.21052631578947367,0.0312678171466318
openai-community/gpt2,truthfulqa:mc,0-shot,mc1,0.22766217870257038,0.01467925503211107
openai-community/gpt2,truthfulqa:mc,0-shot,mc2,0.4069116400376613,0.014934250122346554
openai-community/gpt2,winogrande,5-shot,accuracy,0.5043409629044988,0.014051956064076887
openai-community/gpt2,gsm8k,5-shot,accuracy,0.016679302501895376,0.0035275958887224265
openai-community/gpt2,minerva_math_precalc,5-shot,accuracy,0.007326007326007326,0.0036529080893830334
openai-community/gpt2,minerva_math_prealgebra,5-shot,accuracy,0.01722158438576349,0.004410671674161436
openai-community/gpt2,minerva_math_num_theory,5-shot,accuracy,0.003703703703703704,0.0026164834572311862
openai-community/gpt2,minerva_math_intermediate_algebra,5-shot,accuracy,0.005537098560354375,0.0024707690436948444
openai-community/gpt2,minerva_math_geometry,5-shot,accuracy,0.0041753653444676405,0.0029493392170756513
openai-community/gpt2,minerva_math_counting_and_prob,5-shot,accuracy,0.012658227848101266,0.005140313889578848
openai-community/gpt2,minerva_math_algebra,5-shot,accuracy,0.016849199663016005,0.003737294849759706
openai-community/gpt2,fld_default,0-shot,accuracy,0.0,
openai-community/gpt2,fld_star,0-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
openai-community/gpt2,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
openai-community/gpt2,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
openai-community/gpt2,arithmetic_2ds,5-shot,accuracy,0.0075,0.0019296986470519848
openai-community/gpt2,arithmetic_5ds,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_5da,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_1dc,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_4ds,5-shot,accuracy,0.0,
openai-community/gpt2,arithmetic_2dm,5-shot,accuracy,0.0125,0.0024849471787626726
openai-community/gpt2,arithmetic_2da,5-shot,accuracy,0.0035,0.0013208888574315748
openai-community/gpt2,gsm8k_cot,5-shot,accuracy,0.02122820318423048,0.003970449129848636
openai-community/gpt2,anli_r2,0-shot,brier_score,0.8430078263586929,
openai-community/gpt2,anli_r3,0-shot,brier_score,0.8013382623470885,
openai-community/gpt2,anli_r1,0-shot,brier_score,0.8498025055736901,
openai-community/gpt2,xnli_eu,0-shot,brier_score,0.9535989536207443,
openai-community/gpt2,xnli_vi,0-shot,brier_score,1.0211886040569071,
openai-community/gpt2,xnli_ru,0-shot,brier_score,0.7731589865372022,
openai-community/gpt2,xnli_zh,0-shot,brier_score,0.9057661094866232,
openai-community/gpt2,xnli_tr,0-shot,brier_score,1.2240303777002093,
openai-community/gpt2,xnli_fr,0-shot,brier_score,1.0840384510822554,
openai-community/gpt2,xnli_en,0-shot,brier_score,0.7392875401126219,
openai-community/gpt2,xnli_ur,0-shot,brier_score,1.230167174519266,
openai-community/gpt2,xnli_ar,0-shot,brier_score,0.8911067783116383,
openai-community/gpt2,xnli_de,0-shot,brier_score,0.9368314062777842,
openai-community/gpt2,xnli_hi,0-shot,brier_score,1.019823037820243,
openai-community/gpt2,xnli_es,0-shot,brier_score,1.190947162499289,
openai-community/gpt2,xnli_bg,0-shot,brier_score,0.8735272166946944,
openai-community/gpt2,xnli_sw,0-shot,brier_score,1.0233152940648171,
openai-community/gpt2,xnli_el,0-shot,brier_score,1.26573102048963,
openai-community/gpt2,xnli_th,0-shot,brier_score,1.0943639845839195,
openai-community/gpt2,logiqa2,0-shot,brier_score,1.1036405901271378,
openai-community/gpt2,mathqa,5-shot,brier_score,1.03887777935678,
openai-community/gpt2,lambada_standard,0-shot,perplexity,93.73017204246435,3.812055170694076
openai-community/gpt2,lambada_standard,0-shot,accuracy,0.25965457015330873,0.0061083970427305056
openai-community/gpt2,lambada_openai,0-shot,perplexity,40.05542696796472,1.4840536659360848
openai-community/gpt2,lambada_openai,0-shot,accuracy,0.32563555210556955,0.006528678957835457
playdev7/theseed-v0.3,arc:challenge,25-shot,accuracy,0.22610921501706485,0.012224202097063286
playdev7/theseed-v0.3,arc:challenge,25-shot,acc_norm,0.2593856655290102,0.012808273573927102
playdev7/theseed-v0.3,hellaswag,10-shot,accuracy,0.2561242780322645,0.004355992090030989
playdev7/theseed-v0.3,hellaswag,10-shot,acc_norm,0.2605058753236407,0.004380136468543945
playdev7/theseed-v0.3,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.04512608598542129
playdev7/theseed-v0.3,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.04512608598542129
playdev7/theseed-v0.3,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.037498507091740206
playdev7/theseed-v0.3,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.037498507091740206
playdev7/theseed-v0.3,hendrycksTest-astronomy,5-shot,accuracy,0.26973684210526316,0.03611780560284898
playdev7/theseed-v0.3,hendrycksTest-astronomy,5-shot,acc_norm,0.26973684210526316,0.03611780560284898
playdev7/theseed-v0.3,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.04408440022768079
playdev7/theseed-v0.3,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.04408440022768079
playdev7/theseed-v0.3,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.16981132075471697,0.023108393799841316
playdev7/theseed-v0.3,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.16981132075471697,0.023108393799841316
playdev7/theseed-v0.3,hendrycksTest-college_biology,5-shot,accuracy,0.19444444444444445,0.03309615177059006
playdev7/theseed-v0.3,hendrycksTest-college_biology,5-shot,acc_norm,0.19444444444444445,0.03309615177059006
playdev7/theseed-v0.3,hendrycksTest-college_chemistry,5-shot,accuracy,0.16,0.03684529491774709
playdev7/theseed-v0.3,hendrycksTest-college_chemistry,5-shot,acc_norm,0.16,0.03684529491774709
playdev7/theseed-v0.3,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
playdev7/theseed-v0.3,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
playdev7/theseed-v0.3,hendrycksTest-college_mathematics,5-shot,accuracy,0.19,0.03942772444036622
playdev7/theseed-v0.3,hendrycksTest-college_mathematics,5-shot,acc_norm,0.19,0.03942772444036622
playdev7/theseed-v0.3,hendrycksTest-college_medicine,5-shot,accuracy,0.2138728323699422,0.03126511206173043
playdev7/theseed-v0.3,hendrycksTest-college_medicine,5-shot,acc_norm,0.2138728323699422,0.03126511206173043
playdev7/theseed-v0.3,hendrycksTest-college_physics,5-shot,accuracy,0.27450980392156865,0.044405219061793254
playdev7/theseed-v0.3,hendrycksTest-college_physics,5-shot,acc_norm,0.27450980392156865,0.044405219061793254
playdev7/theseed-v0.3,hendrycksTest-computer_security,5-shot,accuracy,0.25,0.04351941398892446
playdev7/theseed-v0.3,hendrycksTest-computer_security,5-shot,acc_norm,0.25,0.04351941398892446
playdev7/theseed-v0.3,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2723404255319149,0.029101290698386708
playdev7/theseed-v0.3,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2723404255319149,0.029101290698386708
playdev7/theseed-v0.3,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.041424397194893624
playdev7/theseed-v0.3,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.041424397194893624
playdev7/theseed-v0.3,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.03806142687309993
playdev7/theseed-v0.3,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.03806142687309993
playdev7/theseed-v0.3,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918428
playdev7/theseed-v0.3,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.022569897074918428
playdev7/theseed-v0.3,hendrycksTest-formal_logic,5-shot,accuracy,0.23809523809523808,0.03809523809523809
playdev7/theseed-v0.3,hendrycksTest-formal_logic,5-shot,acc_norm,0.23809523809523808,0.03809523809523809
playdev7/theseed-v0.3,hendrycksTest-global_facts,5-shot,accuracy,0.26,0.0440844002276808
playdev7/theseed-v0.3,hendrycksTest-global_facts,5-shot,acc_norm,0.26,0.0440844002276808
playdev7/theseed-v0.3,hendrycksTest-high_school_biology,5-shot,accuracy,0.2129032258064516,0.023287665127268535
playdev7/theseed-v0.3,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2129032258064516,0.023287665127268535
playdev7/theseed-v0.3,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.031270907132976984
playdev7/theseed-v0.3,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.031270907132976984
playdev7/theseed-v0.3,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.19,0.03942772444036624
playdev7/theseed-v0.3,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.19,0.03942772444036624
playdev7/theseed-v0.3,hendrycksTest-high_school_european_history,5-shot,accuracy,0.3151515151515151,0.0362773057502241
playdev7/theseed-v0.3,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.3151515151515151,0.0362773057502241
playdev7/theseed-v0.3,hendrycksTest-high_school_geography,5-shot,accuracy,0.21717171717171718,0.029376616484945637
playdev7/theseed-v0.3,hendrycksTest-high_school_geography,5-shot,acc_norm,0.21717171717171718,0.029376616484945637
playdev7/theseed-v0.3,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.25906735751295334,0.031618779179354115
playdev7/theseed-v0.3,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.25906735751295334,0.031618779179354115
playdev7/theseed-v0.3,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2128205128205128,0.020752423722128006
playdev7/theseed-v0.3,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2128205128205128,0.020752423722128006
playdev7/theseed-v0.3,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.23333333333333334,0.025787874220959316
playdev7/theseed-v0.3,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.23333333333333334,0.025787874220959316
playdev7/theseed-v0.3,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2689075630252101,0.028801392193631276
playdev7/theseed-v0.3,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2689075630252101,0.028801392193631276
playdev7/theseed-v0.3,hendrycksTest-high_school_physics,5-shot,accuracy,0.2582781456953642,0.035737053147634576
playdev7/theseed-v0.3,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2582781456953642,0.035737053147634576
playdev7/theseed-v0.3,hendrycksTest-high_school_psychology,5-shot,accuracy,0.21834862385321102,0.017712600528722738
playdev7/theseed-v0.3,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.21834862385321102,0.017712600528722738
playdev7/theseed-v0.3,hendrycksTest-high_school_statistics,5-shot,accuracy,0.25925925925925924,0.02988691054762696
playdev7/theseed-v0.3,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.25925925925925924,0.02988691054762696
playdev7/theseed-v0.3,hendrycksTest-high_school_us_history,5-shot,accuracy,0.27941176470588236,0.03149328104507955
playdev7/theseed-v0.3,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.27941176470588236,0.03149328104507955
playdev7/theseed-v0.3,hendrycksTest-high_school_world_history,5-shot,accuracy,0.20675105485232068,0.0263616516683891
playdev7/theseed-v0.3,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.20675105485232068,0.0263616516683891
playdev7/theseed-v0.3,hendrycksTest-human_aging,5-shot,accuracy,0.2825112107623318,0.030216831011508766
playdev7/theseed-v0.3,hendrycksTest-human_aging,5-shot,acc_norm,0.2825112107623318,0.030216831011508766
playdev7/theseed-v0.3,hendrycksTest-human_sexuality,5-shot,accuracy,0.26717557251908397,0.038808483010823944
playdev7/theseed-v0.3,hendrycksTest-human_sexuality,5-shot,acc_norm,0.26717557251908397,0.038808483010823944
playdev7/theseed-v0.3,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.03984979653302872
playdev7/theseed-v0.3,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.03984979653302872
playdev7/theseed-v0.3,hendrycksTest-jurisprudence,5-shot,accuracy,0.3333333333333333,0.04557239513497752
playdev7/theseed-v0.3,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3333333333333333,0.04557239513497752
playdev7/theseed-v0.3,hendrycksTest-logical_fallacies,5-shot,accuracy,0.22085889570552147,0.032591773927421776
playdev7/theseed-v0.3,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.22085889570552147,0.032591773927421776
playdev7/theseed-v0.3,hendrycksTest-machine_learning,5-shot,accuracy,0.24107142857142858,0.040598672469526864
playdev7/theseed-v0.3,hendrycksTest-machine_learning,5-shot,acc_norm,0.24107142857142858,0.040598672469526864
playdev7/theseed-v0.3,hendrycksTest-management,5-shot,accuracy,0.17475728155339806,0.0376017800602662
playdev7/theseed-v0.3,hendrycksTest-management,5-shot,acc_norm,0.17475728155339806,0.0376017800602662
playdev7/theseed-v0.3,hendrycksTest-marketing,5-shot,accuracy,0.23504273504273504,0.02777883590493543
playdev7/theseed-v0.3,hendrycksTest-marketing,5-shot,acc_norm,0.23504273504273504,0.02777883590493543
playdev7/theseed-v0.3,hendrycksTest-medical_genetics,5-shot,accuracy,0.19,0.03942772444036622
playdev7/theseed-v0.3,hendrycksTest-medical_genetics,5-shot,acc_norm,0.19,0.03942772444036622
playdev7/theseed-v0.3,hendrycksTest-miscellaneous,5-shot,accuracy,0.2822477650063857,0.01609530296987856
playdev7/theseed-v0.3,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2822477650063857,0.01609530296987856
playdev7/theseed-v0.3,hendrycksTest-moral_disputes,5-shot,accuracy,0.26011560693641617,0.023618678310069363
playdev7/theseed-v0.3,hendrycksTest-moral_disputes,5-shot,acc_norm,0.26011560693641617,0.023618678310069363
playdev7/theseed-v0.3,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2335195530726257,0.014149575348976266
playdev7/theseed-v0.3,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2335195530726257,0.014149575348976266
playdev7/theseed-v0.3,hendrycksTest-nutrition,5-shot,accuracy,0.2679738562091503,0.02536060379624256
playdev7/theseed-v0.3,hendrycksTest-nutrition,5-shot,acc_norm,0.2679738562091503,0.02536060379624256
playdev7/theseed-v0.3,hendrycksTest-philosophy,5-shot,accuracy,0.22508038585209003,0.02372008851617903
playdev7/theseed-v0.3,hendrycksTest-philosophy,5-shot,acc_norm,0.22508038585209003,0.02372008851617903
playdev7/theseed-v0.3,hendrycksTest-prehistory,5-shot,accuracy,0.25,0.02409347123262133
playdev7/theseed-v0.3,hendrycksTest-prehistory,5-shot,acc_norm,0.25,0.02409347123262133
playdev7/theseed-v0.3,hendrycksTest-professional_accounting,5-shot,accuracy,0.24468085106382978,0.025645553622266733
playdev7/theseed-v0.3,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24468085106382978,0.025645553622266733
playdev7/theseed-v0.3,hendrycksTest-professional_law,5-shot,accuracy,0.23989569752281617,0.010906282617981659
playdev7/theseed-v0.3,hendrycksTest-professional_law,5-shot,acc_norm,0.23989569752281617,0.010906282617981659
playdev7/theseed-v0.3,hendrycksTest-professional_medicine,5-shot,accuracy,0.2426470588235294,0.026040662474201275
playdev7/theseed-v0.3,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2426470588235294,0.026040662474201275
playdev7/theseed-v0.3,hendrycksTest-professional_psychology,5-shot,accuracy,0.2434640522875817,0.017362473762146627
playdev7/theseed-v0.3,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2434640522875817,0.017362473762146627
playdev7/theseed-v0.3,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.043091187099464585
playdev7/theseed-v0.3,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.043091187099464585
playdev7/theseed-v0.3,hendrycksTest-security_studies,5-shot,accuracy,0.2571428571428571,0.027979823538744543
playdev7/theseed-v0.3,hendrycksTest-security_studies,5-shot,acc_norm,0.2571428571428571,0.027979823538744543
playdev7/theseed-v0.3,hendrycksTest-sociology,5-shot,accuracy,0.24378109452736318,0.030360490154014676
playdev7/theseed-v0.3,hendrycksTest-sociology,5-shot,acc_norm,0.24378109452736318,0.030360490154014676
playdev7/theseed-v0.3,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.18,0.038612291966536955
playdev7/theseed-v0.3,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.18,0.038612291966536955
playdev7/theseed-v0.3,hendrycksTest-virology,5-shot,accuracy,0.26506024096385544,0.03436024037944967
playdev7/theseed-v0.3,hendrycksTest-virology,5-shot,acc_norm,0.26506024096385544,0.03436024037944967
playdev7/theseed-v0.3,hendrycksTest-world_religions,5-shot,accuracy,0.25146198830409355,0.033275044238468436
playdev7/theseed-v0.3,hendrycksTest-world_religions,5-shot,acc_norm,0.25146198830409355,0.033275044238468436
playdev7/theseed-v0.3,truthfulqa:mc,0-shot,mc1,0.2215422276621787,0.01453786760130114
playdev7/theseed-v0.3,truthfulqa:mc,0-shot,mc2,0.46326315174171945,0.017055263889811677
playdev7/theseed-v0.3,winogrande,5-shot,accuracy,0.5256511444356748,0.014033980956108553
playdev7/theseed-v0.3,gsm8k,5-shot,accuracy,0.0,
Dampish/StellarX-4B-V0,arc:challenge,25-shot,accuracy,0.32337883959044367,0.013669421630012125
Dampish/StellarX-4B-V0,arc:challenge,25-shot,acc_norm,0.36945392491467577,0.0141045783664919
Dampish/StellarX-4B-V0,hellaswag,10-shot,accuracy,0.4584744074885481,0.004972543127767877
Dampish/StellarX-4B-V0,hellaswag,10-shot,acc_norm,0.6190001991635132,0.004846400325585233
Dampish/StellarX-4B-V0,hendrycksTest-abstract_algebra,5-shot,accuracy,0.34,0.047609522856952365
Dampish/StellarX-4B-V0,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.34,0.047609522856952365
Dampish/StellarX-4B-V0,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.03749850709174022
Dampish/StellarX-4B-V0,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.03749850709174022
Dampish/StellarX-4B-V0,hendrycksTest-astronomy,5-shot,accuracy,0.34210526315789475,0.03860731599316092
Dampish/StellarX-4B-V0,hendrycksTest-astronomy,5-shot,acc_norm,0.34210526315789475,0.03860731599316092
Dampish/StellarX-4B-V0,hendrycksTest-business_ethics,5-shot,accuracy,0.23,0.04229525846816506
Dampish/StellarX-4B-V0,hendrycksTest-business_ethics,5-shot,acc_norm,0.23,0.04229525846816506
Dampish/StellarX-4B-V0,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2830188679245283,0.0277242364927009
Dampish/StellarX-4B-V0,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2830188679245283,0.0277242364927009
Dampish/StellarX-4B-V0,hendrycksTest-college_biology,5-shot,accuracy,0.24305555555555555,0.03586879280080341
Dampish/StellarX-4B-V0,hendrycksTest-college_biology,5-shot,acc_norm,0.24305555555555555,0.03586879280080341
Dampish/StellarX-4B-V0,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.0440844002276808
Dampish/StellarX-4B-V0,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.0440844002276808
Dampish/StellarX-4B-V0,hendrycksTest-college_computer_science,5-shot,accuracy,0.33,0.04725815626252606
Dampish/StellarX-4B-V0,hendrycksTest-college_computer_science,5-shot,acc_norm,0.33,0.04725815626252606
Dampish/StellarX-4B-V0,hendrycksTest-college_mathematics,5-shot,accuracy,0.36,0.04824181513244218
Dampish/StellarX-4B-V0,hendrycksTest-college_mathematics,5-shot,acc_norm,0.36,0.04824181513244218
Dampish/StellarX-4B-V0,hendrycksTest-college_medicine,5-shot,accuracy,0.26011560693641617,0.03345036916788991
Dampish/StellarX-4B-V0,hendrycksTest-college_medicine,5-shot,acc_norm,0.26011560693641617,0.03345036916788991
Dampish/StellarX-4B-V0,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617747
Dampish/StellarX-4B-V0,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617747
Dampish/StellarX-4B-V0,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.045126085985421276
Dampish/StellarX-4B-V0,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.045126085985421276
Dampish/StellarX-4B-V0,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2851063829787234,0.02951319662553935
Dampish/StellarX-4B-V0,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2851063829787234,0.02951319662553935
Dampish/StellarX-4B-V0,hendrycksTest-econometrics,5-shot,accuracy,0.2719298245614035,0.04185774424022056
Dampish/StellarX-4B-V0,hendrycksTest-econometrics,5-shot,acc_norm,0.2719298245614035,0.04185774424022056
Dampish/StellarX-4B-V0,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.03806142687309993
Dampish/StellarX-4B-V0,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.03806142687309993
Dampish/StellarX-4B-V0,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.26455026455026454,0.022717467897708617
Dampish/StellarX-4B-V0,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.26455026455026454,0.022717467897708617
Dampish/StellarX-4B-V0,hendrycksTest-formal_logic,5-shot,accuracy,0.15079365079365079,0.03200686497287394
Dampish/StellarX-4B-V0,hendrycksTest-formal_logic,5-shot,acc_norm,0.15079365079365079,0.03200686497287394
Dampish/StellarX-4B-V0,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526045
Dampish/StellarX-4B-V0,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526045
Dampish/StellarX-4B-V0,hendrycksTest-high_school_biology,5-shot,accuracy,0.22903225806451613,0.02390491431178265
Dampish/StellarX-4B-V0,hendrycksTest-high_school_biology,5-shot,acc_norm,0.22903225806451613,0.02390491431178265
Dampish/StellarX-4B-V0,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2512315270935961,0.030516530732694433
Dampish/StellarX-4B-V0,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2512315270935961,0.030516530732694433
Dampish/StellarX-4B-V0,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.04292346959909282
Dampish/StellarX-4B-V0,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.04292346959909282
Dampish/StellarX-4B-V0,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23030303030303031,0.03287666758603489
Dampish/StellarX-4B-V0,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23030303030303031,0.03287666758603489
Dampish/StellarX-4B-V0,hendrycksTest-high_school_geography,5-shot,accuracy,0.35353535353535354,0.03406086723547153
Dampish/StellarX-4B-V0,hendrycksTest-high_school_geography,5-shot,acc_norm,0.35353535353535354,0.03406086723547153
Dampish/StellarX-4B-V0,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.26424870466321243,0.031821550509166484
Dampish/StellarX-4B-V0,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.26424870466321243,0.031821550509166484
Dampish/StellarX-4B-V0,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2512820512820513,0.02199201666237056
Dampish/StellarX-4B-V0,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2512820512820513,0.02199201666237056
Dampish/StellarX-4B-V0,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2222222222222222,0.025348097468097838
Dampish/StellarX-4B-V0,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2222222222222222,0.025348097468097838
Dampish/StellarX-4B-V0,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.19327731092436976,0.0256494702658892
Dampish/StellarX-4B-V0,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.19327731092436976,0.0256494702658892
Dampish/StellarX-4B-V0,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.03631329803969653
Dampish/StellarX-4B-V0,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.03631329803969653
Dampish/StellarX-4B-V0,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3284403669724771,0.020135902797298395
Dampish/StellarX-4B-V0,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3284403669724771,0.020135902797298395
Dampish/StellarX-4B-V0,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3287037037037037,0.032036140846700596
Dampish/StellarX-4B-V0,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3287037037037037,0.032036140846700596
Dampish/StellarX-4B-V0,hendrycksTest-high_school_us_history,5-shot,accuracy,0.24019607843137256,0.02998373305591362
Dampish/StellarX-4B-V0,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.24019607843137256,0.02998373305591362
Dampish/StellarX-4B-V0,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.028900721906293426
Dampish/StellarX-4B-V0,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.028900721906293426
Dampish/StellarX-4B-V0,hendrycksTest-human_aging,5-shot,accuracy,0.11659192825112108,0.02153963981624447
Dampish/StellarX-4B-V0,hendrycksTest-human_aging,5-shot,acc_norm,0.11659192825112108,0.02153963981624447
Dampish/StellarX-4B-V0,hendrycksTest-human_sexuality,5-shot,accuracy,0.24427480916030533,0.03768335959728744
Dampish/StellarX-4B-V0,hendrycksTest-human_sexuality,5-shot,acc_norm,0.24427480916030533,0.03768335959728744
Dampish/StellarX-4B-V0,hendrycksTest-international_law,5-shot,accuracy,0.4049586776859504,0.044811377559424694
Dampish/StellarX-4B-V0,hendrycksTest-international_law,5-shot,acc_norm,0.4049586776859504,0.044811377559424694
Dampish/StellarX-4B-V0,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.04186091791394607
Dampish/StellarX-4B-V0,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.04186091791394607
Dampish/StellarX-4B-V0,hendrycksTest-logical_fallacies,5-shot,accuracy,0.24539877300613497,0.03380939813943354
Dampish/StellarX-4B-V0,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.24539877300613497,0.03380939813943354
Dampish/StellarX-4B-V0,hendrycksTest-machine_learning,5-shot,accuracy,0.2767857142857143,0.04246624336697625
Dampish/StellarX-4B-V0,hendrycksTest-machine_learning,5-shot,acc_norm,0.2767857142857143,0.04246624336697625
Dampish/StellarX-4B-V0,hendrycksTest-management,5-shot,accuracy,0.30097087378640774,0.045416094465039476
Dampish/StellarX-4B-V0,hendrycksTest-management,5-shot,acc_norm,0.30097087378640774,0.045416094465039476
Dampish/StellarX-4B-V0,hendrycksTest-marketing,5-shot,accuracy,0.29914529914529914,0.029996951858349483
Dampish/StellarX-4B-V0,hendrycksTest-marketing,5-shot,acc_norm,0.29914529914529914,0.029996951858349483
Dampish/StellarX-4B-V0,hendrycksTest-medical_genetics,5-shot,accuracy,0.27,0.044619604333847394
Dampish/StellarX-4B-V0,hendrycksTest-medical_genetics,5-shot,acc_norm,0.27,0.044619604333847394
Dampish/StellarX-4B-V0,hendrycksTest-miscellaneous,5-shot,accuracy,0.28735632183908044,0.0161824107306827
Dampish/StellarX-4B-V0,hendrycksTest-miscellaneous,5-shot,acc_norm,0.28735632183908044,0.0161824107306827
Dampish/StellarX-4B-V0,hendrycksTest-moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
Dampish/StellarX-4B-V0,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24855491329479767,0.023267528432100174
Dampish/StellarX-4B-V0,hendrycksTest-moral_scenarios,5-shot,accuracy,0.27262569832402234,0.014893391735249588
Dampish/StellarX-4B-V0,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.27262569832402234,0.014893391735249588
Dampish/StellarX-4B-V0,hendrycksTest-nutrition,5-shot,accuracy,0.2549019607843137,0.02495418432487991
Dampish/StellarX-4B-V0,hendrycksTest-nutrition,5-shot,acc_norm,0.2549019607843137,0.02495418432487991
Dampish/StellarX-4B-V0,hendrycksTest-philosophy,5-shot,accuracy,0.28938906752411575,0.02575586592263294
Dampish/StellarX-4B-V0,hendrycksTest-philosophy,5-shot,acc_norm,0.28938906752411575,0.02575586592263294
Dampish/StellarX-4B-V0,hendrycksTest-prehistory,5-shot,accuracy,0.2654320987654321,0.024569223600460852
Dampish/StellarX-4B-V0,hendrycksTest-prehistory,5-shot,acc_norm,0.2654320987654321,0.024569223600460852
Dampish/StellarX-4B-V0,hendrycksTest-professional_accounting,5-shot,accuracy,0.2695035460992908,0.026469036818590627
Dampish/StellarX-4B-V0,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2695035460992908,0.026469036818590627
Dampish/StellarX-4B-V0,hendrycksTest-professional_law,5-shot,accuracy,0.2633637548891786,0.011249506403605291
Dampish/StellarX-4B-V0,hendrycksTest-professional_law,5-shot,acc_norm,0.2633637548891786,0.011249506403605291
Dampish/StellarX-4B-V0,hendrycksTest-professional_medicine,5-shot,accuracy,0.2426470588235294,0.02604066247420126
Dampish/StellarX-4B-V0,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2426470588235294,0.02604066247420126
Dampish/StellarX-4B-V0,hendrycksTest-professional_psychology,5-shot,accuracy,0.24019607843137256,0.017282760695167414
Dampish/StellarX-4B-V0,hendrycksTest-professional_psychology,5-shot,acc_norm,0.24019607843137256,0.017282760695167414
Dampish/StellarX-4B-V0,hendrycksTest-public_relations,5-shot,accuracy,0.33636363636363636,0.04525393596302506
Dampish/StellarX-4B-V0,hendrycksTest-public_relations,5-shot,acc_norm,0.33636363636363636,0.04525393596302506
Dampish/StellarX-4B-V0,hendrycksTest-security_studies,5-shot,accuracy,0.23673469387755103,0.02721283588407314
Dampish/StellarX-4B-V0,hendrycksTest-security_studies,5-shot,acc_norm,0.23673469387755103,0.02721283588407314
Dampish/StellarX-4B-V0,hendrycksTest-sociology,5-shot,accuracy,0.24378109452736318,0.03036049015401468
Dampish/StellarX-4B-V0,hendrycksTest-sociology,5-shot,acc_norm,0.24378109452736318,0.03036049015401468
Dampish/StellarX-4B-V0,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.26,0.04408440022768078
Dampish/StellarX-4B-V0,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.26,0.04408440022768078
Dampish/StellarX-4B-V0,hendrycksTest-virology,5-shot,accuracy,0.23493975903614459,0.03300533186128922
Dampish/StellarX-4B-V0,hendrycksTest-virology,5-shot,acc_norm,0.23493975903614459,0.03300533186128922
Dampish/StellarX-4B-V0,hendrycksTest-world_religions,5-shot,accuracy,0.29239766081871343,0.034886477134579215
Dampish/StellarX-4B-V0,hendrycksTest-world_religions,5-shot,acc_norm,0.29239766081871343,0.034886477134579215
Dampish/StellarX-4B-V0,truthfulqa:mc,0-shot,mc1,0.20685434516523868,0.014179591496728343
Dampish/StellarX-4B-V0,truthfulqa:mc,0-shot,mc2,0.34296822571733665,0.013628027163865984
Dampish/StellarX-4B-V0,drop,3-shot,accuracy,0.0269505033557047,0.0016584048452624597
Dampish/StellarX-4B-V0,drop,3-shot,f1,0.1094840604026844,0.002280673263690395
Dampish/StellarX-4B-V0,gsm8k,5-shot,accuracy,0.0,
Dampish/StellarX-4B-V0,winogrande,5-shot,accuracy,0.6385161799526441,0.013502479670791294
Qwen/Qwen1.5-14B,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.028380919596145866
Qwen/Qwen1.5-14B,mmlu_formal_logic,0-shot,accuracy,0.5476190476190477,0.044518079590553275
Qwen/Qwen1.5-14B,mmlu_prehistory,0-shot,accuracy,0.7129629629629629,0.02517104191530968
Qwen/Qwen1.5-14B,mmlu_moral_scenarios,0-shot,accuracy,0.4424581005586592,0.016611393687268577
Qwen/Qwen1.5-14B,mmlu_high_school_world_history,0-shot,accuracy,0.8312236286919831,0.024381406832586223
Qwen/Qwen1.5-14B,mmlu_moral_disputes,0-shot,accuracy,0.7456647398843931,0.023445826276545546
Qwen/Qwen1.5-14B,mmlu_professional_law,0-shot,accuracy,0.485006518904824,0.012764493202193257
Qwen/Qwen1.5-14B,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
Qwen/Qwen1.5-14B,mmlu_high_school_us_history,0-shot,accuracy,0.8137254901960784,0.027325470966716312
Qwen/Qwen1.5-14B,mmlu_philosophy,0-shot,accuracy,0.7202572347266881,0.0254942593506949
Qwen/Qwen1.5-14B,mmlu_jurisprudence,0-shot,accuracy,0.75,0.04186091791394607
Qwen/Qwen1.5-14B,mmlu_international_law,0-shot,accuracy,0.8429752066115702,0.0332124484254713
Qwen/Qwen1.5-14B,mmlu_high_school_european_history,0-shot,accuracy,0.8424242424242424,0.02845038880528436
Qwen/Qwen1.5-14B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8963730569948186,0.02199531196364424
Qwen/Qwen1.5-14B,mmlu_high_school_microeconomics,0-shot,accuracy,0.7605042016806722,0.02772206549336127
Qwen/Qwen1.5-14B,mmlu_high_school_geography,0-shot,accuracy,0.8686868686868687,0.024063156416822523
Qwen/Qwen1.5-14B,mmlu_high_school_psychology,0-shot,accuracy,0.8642201834862385,0.014686907556340001
Qwen/Qwen1.5-14B,mmlu_public_relations,0-shot,accuracy,0.6454545454545455,0.04582004841505417
Qwen/Qwen1.5-14B,mmlu_us_foreign_policy,0-shot,accuracy,0.88,0.03265986323710906
Qwen/Qwen1.5-14B,mmlu_sociology,0-shot,accuracy,0.8407960199004975,0.02587064676616913
Qwen/Qwen1.5-14B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7333333333333333,0.02242127361292371
Qwen/Qwen1.5-14B,mmlu_security_studies,0-shot,accuracy,0.8040816326530612,0.025409301953225678
Qwen/Qwen1.5-14B,mmlu_professional_psychology,0-shot,accuracy,0.704248366013072,0.018463154132632813
Qwen/Qwen1.5-14B,mmlu_human_sexuality,0-shot,accuracy,0.7633587786259542,0.03727673575596915
Qwen/Qwen1.5-14B,mmlu_econometrics,0-shot,accuracy,0.5526315789473685,0.046774730044911984
Qwen/Qwen1.5-14B,mmlu_miscellaneous,0-shot,accuracy,0.8378033205619413,0.013182222616720876
Qwen/Qwen1.5-14B,mmlu_marketing,0-shot,accuracy,0.8675213675213675,0.022209309073165612
Qwen/Qwen1.5-14B,mmlu_management,0-shot,accuracy,0.8058252427184466,0.039166677628225836
Qwen/Qwen1.5-14B,mmlu_nutrition,0-shot,accuracy,0.7483660130718954,0.024848018263875202
Qwen/Qwen1.5-14B,mmlu_medical_genetics,0-shot,accuracy,0.79,0.04093601807403326
Qwen/Qwen1.5-14B,mmlu_human_aging,0-shot,accuracy,0.7309417040358744,0.029763779406874975
Qwen/Qwen1.5-14B,mmlu_professional_medicine,0-shot,accuracy,0.7205882352941176,0.027257202606114944
Qwen/Qwen1.5-14B,mmlu_college_medicine,0-shot,accuracy,0.6936416184971098,0.035149425512674394
Qwen/Qwen1.5-14B,mmlu_business_ethics,0-shot,accuracy,0.76,0.04292346959909282
Qwen/Qwen1.5-14B,mmlu_clinical_knowledge,0-shot,accuracy,0.7471698113207547,0.026749899771241214
Qwen/Qwen1.5-14B,mmlu_global_facts,0-shot,accuracy,0.5,0.050251890762960605
Qwen/Qwen1.5-14B,mmlu_virology,0-shot,accuracy,0.4578313253012048,0.0387862677100236
Qwen/Qwen1.5-14B,mmlu_professional_accounting,0-shot,accuracy,0.5,0.029827499313594685
Qwen/Qwen1.5-14B,mmlu_college_physics,0-shot,accuracy,0.5,0.04975185951049946
Qwen/Qwen1.5-14B,mmlu_high_school_physics,0-shot,accuracy,0.48344370860927155,0.04080244185628972
Qwen/Qwen1.5-14B,mmlu_high_school_biology,0-shot,accuracy,0.8419354838709677,0.02075283151187525
Qwen/Qwen1.5-14B,mmlu_college_biology,0-shot,accuracy,0.7638888888888888,0.03551446610810826
Qwen/Qwen1.5-14B,mmlu_anatomy,0-shot,accuracy,0.6370370370370371,0.041539484047424
Qwen/Qwen1.5-14B,mmlu_college_chemistry,0-shot,accuracy,0.5,0.050251890762960605
Qwen/Qwen1.5-14B,mmlu_computer_security,0-shot,accuracy,0.8,0.04020151261036845
Qwen/Qwen1.5-14B,mmlu_college_computer_science,0-shot,accuracy,0.59,0.04943110704237102
Qwen/Qwen1.5-14B,mmlu_astronomy,0-shot,accuracy,0.7302631578947368,0.03611780560284898
Qwen/Qwen1.5-14B,mmlu_college_mathematics,0-shot,accuracy,0.47,0.05016135580465919
Qwen/Qwen1.5-14B,mmlu_conceptual_physics,0-shot,accuracy,0.7021276595744681,0.029896145682095462
Qwen/Qwen1.5-14B,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.048241815132442176
Qwen/Qwen1.5-14B,mmlu_high_school_computer_science,0-shot,accuracy,0.76,0.042923469599092816
Qwen/Qwen1.5-14B,mmlu_machine_learning,0-shot,accuracy,0.5446428571428571,0.04726835553719098
Qwen/Qwen1.5-14B,mmlu_high_school_chemistry,0-shot,accuracy,0.5911330049261084,0.03459058815883232
Qwen/Qwen1.5-14B,mmlu_high_school_statistics,0-shot,accuracy,0.6296296296296297,0.03293377139415191
Qwen/Qwen1.5-14B,mmlu_elementary_mathematics,0-shot,accuracy,0.58994708994709,0.025331202438944433
Qwen/Qwen1.5-14B,mmlu_electrical_engineering,0-shot,accuracy,0.7103448275862069,0.03780019230438014
Qwen/Qwen1.5-14B,mmlu_high_school_mathematics,0-shot,accuracy,0.4444444444444444,0.03029677128606732
Qwen/Qwen1.5-14B,arc_challenge,25-shot,accuracy,0.5307167235494881,0.014583792546304037
Qwen/Qwen1.5-14B,arc_challenge,25-shot,acc_norm,0.5691126279863481,0.014471133392642466
Qwen/Qwen1.5-14B,hellaswag,10-shot,accuracy,0.6134236207926708,0.004859699562451458
Qwen/Qwen1.5-14B,hellaswag,10-shot,acc_norm,0.8116908982274448,0.0039015979142465163
Qwen/Qwen1.5-14B,truthfulqa_mc2,0-shot,accuracy,0.5193716235903669,0.014930202170726823
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,bleu_max,8.957020382370416,0.587628866982265
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,bleu_acc,0.1701346389228886,0.013153917423346956
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,bleu_diff,-0.9550174536574029,0.45358343912334625
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge1_max,19.147077982839196,0.933106339842253
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge1_acc,0.1701346389228886,0.013153917423346956
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge1_diff,-0.931315508168068,0.5903759003425647
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge2_max,13.61098601109907,0.8079627333695129
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge2_acc,0.15055079559363524,0.012518870733256157
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rouge2_diff,-1.4228955265303969,0.6795788734397342
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rougeL_max,18.241818721109897,0.9100007129718393
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rougeL_acc,0.17380660954712362,0.01326566185096658
Qwen/Qwen1.5-14B,truthfulqa_gen,0-shot,rougeL_diff,-0.95605323900642,0.6021888793558169
Qwen/Qwen1.5-14B,truthfulqa_mc1,0-shot,accuracy,0.3574051407588739,0.016776599676729398
Qwen/Qwen1.5-14B,winogrande,5-shot,accuracy,0.7292817679558011,0.012487904760626304
Qwen/Qwen1.5-14B,gsm8k,5-shot,accuracy,0.6944655041698257,0.012688134076726882
microsoft/phi-1_5,mmlu_world_religions,0-shot,accuracy,0.4502923976608187,0.03815827365913236
microsoft/phi-1_5,mmlu_formal_logic,0-shot,accuracy,0.2698412698412698,0.03970158273235173
microsoft/phi-1_5,mmlu_prehistory,0-shot,accuracy,0.4228395061728395,0.027487472980871605
microsoft/phi-1_5,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.014422292204808843
microsoft/phi-1_5,mmlu_high_school_world_history,0-shot,accuracy,0.510548523206751,0.032539983791662855
microsoft/phi-1_5,mmlu_moral_disputes,0-shot,accuracy,0.5491329479768786,0.026788811931562753
microsoft/phi-1_5,mmlu_professional_law,0-shot,accuracy,0.34028683181225555,0.012101217610223773
microsoft/phi-1_5,mmlu_logical_fallacies,0-shot,accuracy,0.5276073619631901,0.0392237829061099
microsoft/phi-1_5,mmlu_high_school_us_history,0-shot,accuracy,0.46078431372549017,0.03498501649369527
microsoft/phi-1_5,mmlu_philosophy,0-shot,accuracy,0.4790996784565916,0.028373270961069414
microsoft/phi-1_5,mmlu_jurisprudence,0-shot,accuracy,0.5185185185185185,0.04830366024635331
microsoft/phi-1_5,mmlu_international_law,0-shot,accuracy,0.6033057851239669,0.04465869780531009
microsoft/phi-1_5,mmlu_high_school_european_history,0-shot,accuracy,0.48484848484848486,0.039025510073744475
microsoft/phi-1_5,mmlu_high_school_government_and_politics,0-shot,accuracy,0.538860103626943,0.035975244117345775
microsoft/phi-1_5,mmlu_high_school_microeconomics,0-shot,accuracy,0.453781512605042,0.03233943468182088
microsoft/phi-1_5,mmlu_high_school_geography,0-shot,accuracy,0.5202020202020202,0.03559443565563919
microsoft/phi-1_5,mmlu_high_school_psychology,0-shot,accuracy,0.5724770642201835,0.021210910204300434
microsoft/phi-1_5,mmlu_public_relations,0-shot,accuracy,0.5181818181818182,0.04785964010794916
microsoft/phi-1_5,mmlu_us_foreign_policy,0-shot,accuracy,0.65,0.047937248544110196
microsoft/phi-1_5,mmlu_sociology,0-shot,accuracy,0.6417910447761194,0.03390393042268814
microsoft/phi-1_5,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4282051282051282,0.02508830145469483
microsoft/phi-1_5,mmlu_security_studies,0-shot,accuracy,0.5061224489795918,0.03200682020163908
microsoft/phi-1_5,mmlu_professional_psychology,0-shot,accuracy,0.4019607843137255,0.01983517648437538
microsoft/phi-1_5,mmlu_human_sexuality,0-shot,accuracy,0.4732824427480916,0.04379024936553894
microsoft/phi-1_5,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813365
microsoft/phi-1_5,mmlu_miscellaneous,0-shot,accuracy,0.508301404853129,0.017877498991072
microsoft/phi-1_5,mmlu_marketing,0-shot,accuracy,0.7008547008547008,0.029996951858349483
microsoft/phi-1_5,mmlu_management,0-shot,accuracy,0.5825242718446602,0.04882840548212238
microsoft/phi-1_5,mmlu_nutrition,0-shot,accuracy,0.5032679738562091,0.028629305194003533
microsoft/phi-1_5,mmlu_medical_genetics,0-shot,accuracy,0.5,0.050251890762960605
microsoft/phi-1_5,mmlu_human_aging,0-shot,accuracy,0.484304932735426,0.0335412657542081
microsoft/phi-1_5,mmlu_professional_medicine,0-shot,accuracy,0.33455882352941174,0.02866199620233531
microsoft/phi-1_5,mmlu_college_medicine,0-shot,accuracy,0.4161849710982659,0.037585177754049466
microsoft/phi-1_5,mmlu_business_ethics,0-shot,accuracy,0.51,0.05024183937956912
microsoft/phi-1_5,mmlu_clinical_knowledge,0-shot,accuracy,0.49433962264150944,0.03077090076385131
microsoft/phi-1_5,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621504
microsoft/phi-1_5,mmlu_virology,0-shot,accuracy,0.41566265060240964,0.03836722176598052
microsoft/phi-1_5,mmlu_professional_accounting,0-shot,accuracy,0.3049645390070922,0.02746470844202214
microsoft/phi-1_5,mmlu_college_physics,0-shot,accuracy,0.29411764705882354,0.04533838195929775
microsoft/phi-1_5,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
microsoft/phi-1_5,mmlu_high_school_biology,0-shot,accuracy,0.4483870967741935,0.028292056830112728
microsoft/phi-1_5,mmlu_college_biology,0-shot,accuracy,0.375,0.04048439222695598
microsoft/phi-1_5,mmlu_anatomy,0-shot,accuracy,0.4666666666666667,0.043097329010363554
microsoft/phi-1_5,mmlu_college_chemistry,0-shot,accuracy,0.27,0.044619604333847394
microsoft/phi-1_5,mmlu_computer_security,0-shot,accuracy,0.53,0.05016135580465919
microsoft/phi-1_5,mmlu_college_computer_science,0-shot,accuracy,0.46,0.05009082659620333
microsoft/phi-1_5,mmlu_astronomy,0-shot,accuracy,0.4144736842105263,0.04008973785779206
microsoft/phi-1_5,mmlu_college_mathematics,0-shot,accuracy,0.41,0.04943110704237101
microsoft/phi-1_5,mmlu_conceptual_physics,0-shot,accuracy,0.37446808510638296,0.031639106653672915
microsoft/phi-1_5,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.048241815132442176
microsoft/phi-1_5,mmlu_high_school_computer_science,0-shot,accuracy,0.46,0.05009082659620333
microsoft/phi-1_5,mmlu_machine_learning,0-shot,accuracy,0.39285714285714285,0.04635550135609976
microsoft/phi-1_5,mmlu_high_school_chemistry,0-shot,accuracy,0.3448275862068966,0.03344283744280458
microsoft/phi-1_5,mmlu_high_school_statistics,0-shot,accuracy,0.28703703703703703,0.030851992993257013
microsoft/phi-1_5,mmlu_elementary_mathematics,0-shot,accuracy,0.30423280423280424,0.023695415009463084
microsoft/phi-1_5,mmlu_electrical_engineering,0-shot,accuracy,0.4689655172413793,0.04158632762097828
microsoft/phi-1_5,mmlu_high_school_mathematics,0-shot,accuracy,0.23333333333333334,0.02578787422095931
microsoft/phi-1_5,arc_challenge,25-shot,accuracy,0.5008532423208191,0.014611369529813262
microsoft/phi-1_5,arc_challenge,25-shot,acc_norm,0.5315699658703071,0.014582236460866977
microsoft/phi-1_5,hellaswag,10-shot,accuracy,0.48615813582951606,0.004987868988630005
microsoft/phi-1_5,hellaswag,10-shot,acc_norm,0.6392152957578172,0.0047924672558997605
microsoft/phi-1_5,truthfulqa_mc2,0-shot,accuracy,0.4085779509507484,0.014840269110411395
microsoft/phi-1_5,truthfulqa_gen,0-shot,bleu_max,27.567668355661375,0.7836996753793046
microsoft/phi-1_5,truthfulqa_gen,0-shot,bleu_acc,0.31946144430844553,0.016322644182960505
microsoft/phi-1_5,truthfulqa_gen,0-shot,bleu_diff,-4.901645580186722,0.877157693134709
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge1_max,52.930362387533215,0.8338583348805911
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge1_acc,0.32802937576499386,0.016435632932815
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge1_diff,-5.2936235823035105,1.0149100288222572
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge2_max,38.05023260311765,0.9792456124605255
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge2_acc,0.28151774785801714,0.015744027248256045
microsoft/phi-1_5,truthfulqa_gen,0-shot,rouge2_diff,-6.717662093135759,1.1866608093259197
microsoft/phi-1_5,truthfulqa_gen,0-shot,rougeL_max,50.36955243236768,0.8580936496769774
microsoft/phi-1_5,truthfulqa_gen,0-shot,rougeL_acc,0.3157894736842105,0.016272287957916885
microsoft/phi-1_5,truthfulqa_gen,0-shot,rougeL_diff,-5.513065735562071,1.0278544546573318
microsoft/phi-1_5,truthfulqa_mc1,0-shot,accuracy,0.27050183598531213,0.0155507783328429
microsoft/phi-1_5,winogrande,5-shot,accuracy,0.7103393843725335,0.012748550807638261
microsoft/phi-1_5,gsm8k,5-shot,accuracy,0.3206974981046247,0.012856468433722292
cerebras/Cerebras-GPT-2.7B,drop,3-shot,accuracy,0.0010486577181208054,0.00033145814652192537
cerebras/Cerebras-GPT-2.7B,drop,3-shot,f1,0.045849412751678,0.0011802883893565243
cerebras/Cerebras-GPT-2.7B,arc:challenge,25-shot,accuracy,0.2696245733788396,0.012968040686869148
cerebras/Cerebras-GPT-2.7B,arc:challenge,25-shot,acc_norm,0.2909556313993174,0.013273077865907592
cerebras/Cerebras-GPT-2.7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.23,0.04229525846816505
cerebras/Cerebras-GPT-2.7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.23,0.04229525846816505
cerebras/Cerebras-GPT-2.7B,hendrycksTest-anatomy,5-shot,accuracy,0.26666666666666666,0.03820169914517904
cerebras/Cerebras-GPT-2.7B,hendrycksTest-anatomy,5-shot,acc_norm,0.26666666666666666,0.03820169914517904
cerebras/Cerebras-GPT-2.7B,hendrycksTest-astronomy,5-shot,accuracy,0.19078947368421054,0.03197565821032499
cerebras/Cerebras-GPT-2.7B,hendrycksTest-astronomy,5-shot,acc_norm,0.19078947368421054,0.03197565821032499
cerebras/Cerebras-GPT-2.7B,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.04163331998932269
cerebras/Cerebras-GPT-2.7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.04163331998932269
cerebras/Cerebras-GPT-2.7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.21509433962264152,0.02528839450289137
cerebras/Cerebras-GPT-2.7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.21509433962264152,0.02528839450289137
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.037161774375660164
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.037161774375660164
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036845
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036845
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.37,0.048523658709391
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.37,0.048523658709391
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.25,0.04351941398892446
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.25,0.04351941398892446
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_medicine,5-shot,accuracy,0.23121387283236994,0.032147373020294696
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.23121387283236994,0.032147373020294696
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_physics,5-shot,accuracy,0.17647058823529413,0.0379328118530781
cerebras/Cerebras-GPT-2.7B,hendrycksTest-college_physics,5-shot,acc_norm,0.17647058823529413,0.0379328118530781
cerebras/Cerebras-GPT-2.7B,hendrycksTest-computer_security,5-shot,accuracy,0.26,0.044084400227680794
cerebras/Cerebras-GPT-2.7B,hendrycksTest-computer_security,5-shot,acc_norm,0.26,0.044084400227680794
cerebras/Cerebras-GPT-2.7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2553191489361702,0.02850485647051419
cerebras/Cerebras-GPT-2.7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2553191489361702,0.02850485647051419
cerebras/Cerebras-GPT-2.7B,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
cerebras/Cerebras-GPT-2.7B,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
cerebras/Cerebras-GPT-2.7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.03565998174135302
cerebras/Cerebras-GPT-2.7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.03565998174135302
cerebras/Cerebras-GPT-2.7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.20105820105820105,0.020641810782370165
cerebras/Cerebras-GPT-2.7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.20105820105820105,0.020641810782370165
cerebras/Cerebras-GPT-2.7B,hendrycksTest-formal_logic,5-shot,accuracy,0.2222222222222222,0.037184890068181146
cerebras/Cerebras-GPT-2.7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.2222222222222222,0.037184890068181146
cerebras/Cerebras-GPT-2.7B,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.038612291966536934
cerebras/Cerebras-GPT-2.7B,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.038612291966536934
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.24516129032258063,0.02447224384089553
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.24516129032258063,0.02447224384089553
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.031270907132976984
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.031270907132976984
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.39,0.04902071300001975
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.39,0.04902071300001975
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.23030303030303031,0.03287666758603488
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.23030303030303031,0.03287666758603488
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.02962022787479048
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.02962022787479048
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.21761658031088082,0.029778663037752947
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.21761658031088082,0.029778663037752947
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.23846153846153847,0.021606294494647727
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.23846153846153847,0.021606294494647727
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.27037037037037037,0.027080372815145668
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.27037037037037037,0.027080372815145668
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.23109243697478993,0.027381406927868952
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.23109243697478993,0.027381406927868952
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2781456953642384,0.03658603262763743
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2781456953642384,0.03658603262763743
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.23302752293577983,0.0181256691808615
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.23302752293577983,0.0181256691808615
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.28703703703703703,0.030851992993257013
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.28703703703703703,0.030851992993257013
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2696078431372549,0.03114557065948678
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2696078431372549,0.03114557065948678
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25738396624472576,0.028458820991460302
cerebras/Cerebras-GPT-2.7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25738396624472576,0.028458820991460302
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_aging,5-shot,accuracy,0.242152466367713,0.028751392398694755
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_aging,5-shot,acc_norm,0.242152466367713,0.028751392398694755
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.037276735755969174
cerebras/Cerebras-GPT-2.7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.037276735755969174
cerebras/Cerebras-GPT-2.7B,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.03896878985070417
cerebras/Cerebras-GPT-2.7B,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.03896878985070417
cerebras/Cerebras-GPT-2.7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.18518518518518517,0.03755265865037183
cerebras/Cerebras-GPT-2.7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.18518518518518517,0.03755265865037183
cerebras/Cerebras-GPT-2.7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.31901840490797545,0.03661997551073836
cerebras/Cerebras-GPT-2.7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.31901840490797545,0.03661997551073836
cerebras/Cerebras-GPT-2.7B,hendrycksTest-machine_learning,5-shot,accuracy,0.3392857142857143,0.04493949068613539
cerebras/Cerebras-GPT-2.7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.3392857142857143,0.04493949068613539
cerebras/Cerebras-GPT-2.7B,hendrycksTest-management,5-shot,accuracy,0.21359223300970873,0.04058042015646034
cerebras/Cerebras-GPT-2.7B,hendrycksTest-management,5-shot,acc_norm,0.21359223300970873,0.04058042015646034
cerebras/Cerebras-GPT-2.7B,hendrycksTest-marketing,5-shot,accuracy,0.2564102564102564,0.02860595370200424
cerebras/Cerebras-GPT-2.7B,hendrycksTest-marketing,5-shot,acc_norm,0.2564102564102564,0.02860595370200424
cerebras/Cerebras-GPT-2.7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.22,0.04163331998932269
cerebras/Cerebras-GPT-2.7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.22,0.04163331998932269
cerebras/Cerebras-GPT-2.7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2669220945083014,0.015818450894777566
cerebras/Cerebras-GPT-2.7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2669220945083014,0.015818450894777566
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2658959537572254,0.023786203255508287
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2658959537572254,0.023786203255508287
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.27150837988826815,0.01487425216809527
cerebras/Cerebras-GPT-2.7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.27150837988826815,0.01487425216809527
cerebras/Cerebras-GPT-2.7B,hendrycksTest-nutrition,5-shot,accuracy,0.27124183006535946,0.02545775669666787
cerebras/Cerebras-GPT-2.7B,hendrycksTest-nutrition,5-shot,acc_norm,0.27124183006535946,0.02545775669666787
cerebras/Cerebras-GPT-2.7B,hendrycksTest-philosophy,5-shot,accuracy,0.26688102893890675,0.025122637608816632
cerebras/Cerebras-GPT-2.7B,hendrycksTest-philosophy,5-shot,acc_norm,0.26688102893890675,0.025122637608816632
cerebras/Cerebras-GPT-2.7B,hendrycksTest-prehistory,5-shot,accuracy,0.23148148148148148,0.023468429832451173
cerebras/Cerebras-GPT-2.7B,hendrycksTest-prehistory,5-shot,acc_norm,0.23148148148148148,0.023468429832451173
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.26595744680851063,0.02635806569888059
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.26595744680851063,0.02635806569888059
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_law,5-shot,accuracy,0.26401564537157757,0.011258435537723818
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_law,5-shot,acc_norm,0.26401564537157757,0.011258435537723818
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.2536764705882353,0.026431329870789534
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.2536764705882353,0.026431329870789534
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2630718954248366,0.017812676542320657
cerebras/Cerebras-GPT-2.7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2630718954248366,0.017812676542320657
cerebras/Cerebras-GPT-2.7B,hendrycksTest-public_relations,5-shot,accuracy,0.17272727272727273,0.03620691833929219
cerebras/Cerebras-GPT-2.7B,hendrycksTest-public_relations,5-shot,acc_norm,0.17272727272727273,0.03620691833929219
cerebras/Cerebras-GPT-2.7B,hendrycksTest-security_studies,5-shot,accuracy,0.3183673469387755,0.029822533793982052
cerebras/Cerebras-GPT-2.7B,hendrycksTest-security_studies,5-shot,acc_norm,0.3183673469387755,0.029822533793982052
cerebras/Cerebras-GPT-2.7B,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.03076944496729601
cerebras/Cerebras-GPT-2.7B,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.03076944496729601
cerebras/Cerebras-GPT-2.7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.04560480215720684
cerebras/Cerebras-GPT-2.7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.04560480215720684
cerebras/Cerebras-GPT-2.7B,hendrycksTest-virology,5-shot,accuracy,0.28313253012048195,0.03507295431370519
cerebras/Cerebras-GPT-2.7B,hendrycksTest-virology,5-shot,acc_norm,0.28313253012048195,0.03507295431370519
cerebras/Cerebras-GPT-2.7B,hendrycksTest-world_religions,5-shot,accuracy,0.26900584795321636,0.0340105262010409
cerebras/Cerebras-GPT-2.7B,hendrycksTest-world_religions,5-shot,acc_norm,0.26900584795321636,0.0340105262010409
cerebras/Cerebras-GPT-2.7B,truthfulqa:mc,0-shot,mc1,0.2460220318237454,0.015077219200662592
cerebras/Cerebras-GPT-2.7B,truthfulqa:mc,0-shot,mc2,0.4136763359861922,0.014439422755488887
cerebras/Cerebras-GPT-6.7B,drop,3-shot,accuracy,0.0008389261744966443,0.00029649629898012217
cerebras/Cerebras-GPT-6.7B,drop,3-shot,f1,0.047345847315436396,0.0011636776448840373
cerebras/Cerebras-GPT-6.7B,arc:challenge,25-shot,accuracy,0.30887372013651876,0.013501770929344003
cerebras/Cerebras-GPT-6.7B,arc:challenge,25-shot,acc_norm,0.3506825938566553,0.013944635930726087
cerebras/Cerebras-GPT-6.7B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.28,0.045126085985421276
cerebras/Cerebras-GPT-6.7B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.28,0.045126085985421276
cerebras/Cerebras-GPT-6.7B,hendrycksTest-anatomy,5-shot,accuracy,0.23703703703703705,0.03673731683969506
cerebras/Cerebras-GPT-6.7B,hendrycksTest-anatomy,5-shot,acc_norm,0.23703703703703705,0.03673731683969506
cerebras/Cerebras-GPT-6.7B,hendrycksTest-astronomy,5-shot,accuracy,0.24342105263157895,0.034923496688842384
cerebras/Cerebras-GPT-6.7B,hendrycksTest-astronomy,5-shot,acc_norm,0.24342105263157895,0.034923496688842384
cerebras/Cerebras-GPT-6.7B,hendrycksTest-business_ethics,5-shot,accuracy,0.13,0.03379976689896309
cerebras/Cerebras-GPT-6.7B,hendrycksTest-business_ethics,5-shot,acc_norm,0.13,0.03379976689896309
cerebras/Cerebras-GPT-6.7B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.22264150943396227,0.025604233470899098
cerebras/Cerebras-GPT-6.7B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.22264150943396227,0.025604233470899098
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_biology,5-shot,accuracy,0.2152777777777778,0.03437079344106135
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_biology,5-shot,acc_norm,0.2152777777777778,0.03437079344106135
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.046482319871173156
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.046482319871173156
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_computer_science,5-shot,accuracy,0.4,0.049236596391733084
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.4,0.049236596391733084
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.046056618647183814
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_medicine,5-shot,accuracy,0.24277456647398843,0.0326926380614177
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_medicine,5-shot,acc_norm,0.24277456647398843,0.0326926380614177
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_physics,5-shot,accuracy,0.24509803921568626,0.04280105837364396
cerebras/Cerebras-GPT-6.7B,hendrycksTest-college_physics,5-shot,acc_norm,0.24509803921568626,0.04280105837364396
cerebras/Cerebras-GPT-6.7B,hendrycksTest-computer_security,5-shot,accuracy,0.22,0.041633319989322695
cerebras/Cerebras-GPT-6.7B,hendrycksTest-computer_security,5-shot,acc_norm,0.22,0.041633319989322695
cerebras/Cerebras-GPT-6.7B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3021276595744681,0.030017554471880557
cerebras/Cerebras-GPT-6.7B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3021276595744681,0.030017554471880557
cerebras/Cerebras-GPT-6.7B,hendrycksTest-econometrics,5-shot,accuracy,0.24561403508771928,0.040493392977481404
cerebras/Cerebras-GPT-6.7B,hendrycksTest-econometrics,5-shot,acc_norm,0.24561403508771928,0.040493392977481404
cerebras/Cerebras-GPT-6.7B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.25517241379310346,0.03632984052707842
cerebras/Cerebras-GPT-6.7B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.25517241379310346,0.03632984052707842
cerebras/Cerebras-GPT-6.7B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24338624338624337,0.022101128787415415
cerebras/Cerebras-GPT-6.7B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24338624338624337,0.022101128787415415
cerebras/Cerebras-GPT-6.7B,hendrycksTest-formal_logic,5-shot,accuracy,0.23809523809523808,0.038095238095238106
cerebras/Cerebras-GPT-6.7B,hendrycksTest-formal_logic,5-shot,acc_norm,0.23809523809523808,0.038095238095238106
cerebras/Cerebras-GPT-6.7B,hendrycksTest-global_facts,5-shot,accuracy,0.19,0.03942772444036625
cerebras/Cerebras-GPT-6.7B,hendrycksTest-global_facts,5-shot,acc_norm,0.19,0.03942772444036625
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_biology,5-shot,accuracy,0.21935483870967742,0.023540799358723292
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.21935483870967742,0.023540799358723292
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.20689655172413793,0.028501378167893946
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.20689655172413793,0.028501378167893946
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.37,0.04852365870939099
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.37,0.04852365870939099
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.28484848484848485,0.03524390844511784
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.28484848484848485,0.03524390844511784
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_geography,5-shot,accuracy,0.18686868686868688,0.027772533334218974
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.18686868686868688,0.027772533334218974
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.21761658031088082,0.029778663037752943
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.21761658031088082,0.029778663037752943
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.24871794871794872,0.021916957709213793
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.24871794871794872,0.021916957709213793
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.02684205787383371
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.02684205787383371
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.24789915966386555,0.028047967224176896
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.24789915966386555,0.028047967224176896
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.03631329803969653
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.03631329803969653
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.30458715596330277,0.01973229942035404
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.30458715596330277,0.01973229942035404
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3425925925925926,0.03236585252602156
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3425925925925926,0.03236585252602156
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.24019607843137256,0.02998373305591361
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.24019607843137256,0.02998373305591361
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.22362869198312235,0.02712329820522997
cerebras/Cerebras-GPT-6.7B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.22362869198312235,0.02712329820522997
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_aging,5-shot,accuracy,0.29596412556053814,0.030636591348699796
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_aging,5-shot,acc_norm,0.29596412556053814,0.030636591348699796
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
cerebras/Cerebras-GPT-6.7B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
cerebras/Cerebras-GPT-6.7B,hendrycksTest-international_law,5-shot,accuracy,0.2644628099173554,0.04026187527591203
cerebras/Cerebras-GPT-6.7B,hendrycksTest-international_law,5-shot,acc_norm,0.2644628099173554,0.04026187527591203
cerebras/Cerebras-GPT-6.7B,hendrycksTest-jurisprudence,5-shot,accuracy,0.2222222222222222,0.0401910747255735
cerebras/Cerebras-GPT-6.7B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2222222222222222,0.0401910747255735
cerebras/Cerebras-GPT-6.7B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2883435582822086,0.035590395316173425
cerebras/Cerebras-GPT-6.7B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2883435582822086,0.035590395316173425
cerebras/Cerebras-GPT-6.7B,hendrycksTest-machine_learning,5-shot,accuracy,0.23214285714285715,0.04007341809755806
cerebras/Cerebras-GPT-6.7B,hendrycksTest-machine_learning,5-shot,acc_norm,0.23214285714285715,0.04007341809755806
cerebras/Cerebras-GPT-6.7B,hendrycksTest-management,5-shot,accuracy,0.27184466019417475,0.044052680241409216
cerebras/Cerebras-GPT-6.7B,hendrycksTest-management,5-shot,acc_norm,0.27184466019417475,0.044052680241409216
cerebras/Cerebras-GPT-6.7B,hendrycksTest-marketing,5-shot,accuracy,0.23931623931623933,0.027951826808924333
cerebras/Cerebras-GPT-6.7B,hendrycksTest-marketing,5-shot,acc_norm,0.23931623931623933,0.027951826808924333
cerebras/Cerebras-GPT-6.7B,hendrycksTest-medical_genetics,5-shot,accuracy,0.26,0.0440844002276808
cerebras/Cerebras-GPT-6.7B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.26,0.0440844002276808
cerebras/Cerebras-GPT-6.7B,hendrycksTest-miscellaneous,5-shot,accuracy,0.26053639846743293,0.015696008563807082
cerebras/Cerebras-GPT-6.7B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.26053639846743293,0.015696008563807082
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_disputes,5-shot,accuracy,0.2832369942196532,0.02425790170532337
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2832369942196532,0.02425790170532337
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.27262569832402234,0.014893391735249588
cerebras/Cerebras-GPT-6.7B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.27262569832402234,0.014893391735249588
cerebras/Cerebras-GPT-6.7B,hendrycksTest-nutrition,5-shot,accuracy,0.23529411764705882,0.02428861946604611
cerebras/Cerebras-GPT-6.7B,hendrycksTest-nutrition,5-shot,acc_norm,0.23529411764705882,0.02428861946604611
cerebras/Cerebras-GPT-6.7B,hendrycksTest-philosophy,5-shot,accuracy,0.2604501607717042,0.024926723224845543
cerebras/Cerebras-GPT-6.7B,hendrycksTest-philosophy,5-shot,acc_norm,0.2604501607717042,0.024926723224845543
cerebras/Cerebras-GPT-6.7B,hendrycksTest-prehistory,5-shot,accuracy,0.24074074074074073,0.023788583551658523
cerebras/Cerebras-GPT-6.7B,hendrycksTest-prehistory,5-shot,acc_norm,0.24074074074074073,0.023788583551658523
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_accounting,5-shot,accuracy,0.24822695035460993,0.025770015644290385
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24822695035460993,0.025770015644290385
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_law,5-shot,accuracy,0.26988265971316816,0.011337381084250402
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_law,5-shot,acc_norm,0.26988265971316816,0.011337381084250402
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_medicine,5-shot,accuracy,0.4117647058823529,0.029896163033125474
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4117647058823529,0.029896163033125474
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_psychology,5-shot,accuracy,0.24836601307189543,0.017479487001364764
cerebras/Cerebras-GPT-6.7B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.24836601307189543,0.017479487001364764
cerebras/Cerebras-GPT-6.7B,hendrycksTest-public_relations,5-shot,accuracy,0.24545454545454545,0.041220665028782834
cerebras/Cerebras-GPT-6.7B,hendrycksTest-public_relations,5-shot,acc_norm,0.24545454545454545,0.041220665028782834
cerebras/Cerebras-GPT-6.7B,hendrycksTest-security_studies,5-shot,accuracy,0.2,0.025607375986579153
cerebras/Cerebras-GPT-6.7B,hendrycksTest-security_studies,5-shot,acc_norm,0.2,0.025607375986579153
cerebras/Cerebras-GPT-6.7B,hendrycksTest-sociology,5-shot,accuracy,0.2537313432835821,0.030769444967296014
cerebras/Cerebras-GPT-6.7B,hendrycksTest-sociology,5-shot,acc_norm,0.2537313432835821,0.030769444967296014
cerebras/Cerebras-GPT-6.7B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.045126085985421276
cerebras/Cerebras-GPT-6.7B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.045126085985421276
cerebras/Cerebras-GPT-6.7B,hendrycksTest-virology,5-shot,accuracy,0.26506024096385544,0.03436024037944967
cerebras/Cerebras-GPT-6.7B,hendrycksTest-virology,5-shot,acc_norm,0.26506024096385544,0.03436024037944967
cerebras/Cerebras-GPT-6.7B,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.03582529442573122
cerebras/Cerebras-GPT-6.7B,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.03582529442573122
cerebras/Cerebras-GPT-6.7B,truthfulqa:mc,0-shot,mc1,0.24357405140758873,0.015026354824910782
cerebras/Cerebras-GPT-6.7B,truthfulqa:mc,0-shot,mc2,0.3802394598585255,0.013925842027078916
NinedayWang/PolyCoder-2.7B,mmlu_world_religions,0-shot,accuracy,0.3157894736842105,0.03565079670708312
NinedayWang/PolyCoder-2.7B,mmlu_formal_logic,0-shot,accuracy,0.18253968253968253,0.03455071019102147
NinedayWang/PolyCoder-2.7B,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.023016705640262196
NinedayWang/PolyCoder-2.7B,mmlu_moral_scenarios,0-shot,accuracy,0.24804469273743016,0.014444157808261436
NinedayWang/PolyCoder-2.7B,mmlu_high_school_world_history,0-shot,accuracy,0.2742616033755274,0.02904133351059802
NinedayWang/PolyCoder-2.7B,mmlu_moral_disputes,0-shot,accuracy,0.23699421965317918,0.02289408248992599
NinedayWang/PolyCoder-2.7B,mmlu_professional_law,0-shot,accuracy,0.24771838331160365,0.011025499291443737
NinedayWang/PolyCoder-2.7B,mmlu_logical_fallacies,0-shot,accuracy,0.27607361963190186,0.0351238528370505
NinedayWang/PolyCoder-2.7B,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.03096451792692341
NinedayWang/PolyCoder-2.7B,mmlu_philosophy,0-shot,accuracy,0.26366559485530544,0.02502553850053234
NinedayWang/PolyCoder-2.7B,mmlu_jurisprudence,0-shot,accuracy,0.21296296296296297,0.03957835471980978
NinedayWang/PolyCoder-2.7B,mmlu_international_law,0-shot,accuracy,0.3140495867768595,0.04236964753041019
NinedayWang/PolyCoder-2.7B,mmlu_high_school_european_history,0-shot,accuracy,0.2606060606060606,0.03427743175816524
NinedayWang/PolyCoder-2.7B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.31088082901554404,0.03340361906276585
NinedayWang/PolyCoder-2.7B,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.027553614467863804
NinedayWang/PolyCoder-2.7B,mmlu_high_school_geography,0-shot,accuracy,0.2727272727272727,0.03173071239071724
NinedayWang/PolyCoder-2.7B,mmlu_high_school_psychology,0-shot,accuracy,0.20733944954128442,0.017381415563608674
NinedayWang/PolyCoder-2.7B,mmlu_public_relations,0-shot,accuracy,0.22727272727272727,0.04013964554072773
NinedayWang/PolyCoder-2.7B,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.042923469599092816
NinedayWang/PolyCoder-2.7B,mmlu_sociology,0-shot,accuracy,0.22388059701492538,0.029475250236017193
NinedayWang/PolyCoder-2.7B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.34615384615384615,0.024121125416941183
NinedayWang/PolyCoder-2.7B,mmlu_security_studies,0-shot,accuracy,0.1836734693877551,0.024789071332007643
NinedayWang/PolyCoder-2.7B,mmlu_professional_psychology,0-shot,accuracy,0.24509803921568626,0.017401816711427657
NinedayWang/PolyCoder-2.7B,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
NinedayWang/PolyCoder-2.7B,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.03999423879281335
NinedayWang/PolyCoder-2.7B,mmlu_miscellaneous,0-shot,accuracy,0.25287356321839083,0.015543377313719681
NinedayWang/PolyCoder-2.7B,mmlu_marketing,0-shot,accuracy,0.1794871794871795,0.02514093595033545
NinedayWang/PolyCoder-2.7B,mmlu_management,0-shot,accuracy,0.13592233009708737,0.033932957297610145
NinedayWang/PolyCoder-2.7B,mmlu_nutrition,0-shot,accuracy,0.23529411764705882,0.024288619466046105
NinedayWang/PolyCoder-2.7B,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
NinedayWang/PolyCoder-2.7B,mmlu_human_aging,0-shot,accuracy,0.31390134529147984,0.031146796482972465
NinedayWang/PolyCoder-2.7B,mmlu_professional_medicine,0-shot,accuracy,0.4411764705882353,0.0301619119307671
NinedayWang/PolyCoder-2.7B,mmlu_college_medicine,0-shot,accuracy,0.23121387283236994,0.032147373020294696
NinedayWang/PolyCoder-2.7B,mmlu_business_ethics,0-shot,accuracy,0.22,0.041633319989322695
NinedayWang/PolyCoder-2.7B,mmlu_clinical_knowledge,0-shot,accuracy,0.2641509433962264,0.027134291628741702
NinedayWang/PolyCoder-2.7B,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
NinedayWang/PolyCoder-2.7B,mmlu_virology,0-shot,accuracy,0.1746987951807229,0.029560326211256822
NinedayWang/PolyCoder-2.7B,mmlu_professional_accounting,0-shot,accuracy,0.26595744680851063,0.026358065698880592
NinedayWang/PolyCoder-2.7B,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
NinedayWang/PolyCoder-2.7B,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
NinedayWang/PolyCoder-2.7B,mmlu_high_school_biology,0-shot,accuracy,0.3032258064516129,0.02614868593067175
NinedayWang/PolyCoder-2.7B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
NinedayWang/PolyCoder-2.7B,mmlu_anatomy,0-shot,accuracy,0.3333333333333333,0.04072314811876837
NinedayWang/PolyCoder-2.7B,mmlu_college_chemistry,0-shot,accuracy,0.19,0.039427724440366234
NinedayWang/PolyCoder-2.7B,mmlu_computer_security,0-shot,accuracy,0.28,0.04512608598542127
NinedayWang/PolyCoder-2.7B,mmlu_college_computer_science,0-shot,accuracy,0.34,0.04760952285695236
NinedayWang/PolyCoder-2.7B,mmlu_astronomy,0-shot,accuracy,0.19078947368421054,0.031975658210325
NinedayWang/PolyCoder-2.7B,mmlu_college_mathematics,0-shot,accuracy,0.27,0.044619604333847394
NinedayWang/PolyCoder-2.7B,mmlu_conceptual_physics,0-shot,accuracy,0.2170212765957447,0.02694748312149622
NinedayWang/PolyCoder-2.7B,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909283
NinedayWang/PolyCoder-2.7B,mmlu_high_school_computer_science,0-shot,accuracy,0.3,0.04605661864718381
NinedayWang/PolyCoder-2.7B,mmlu_machine_learning,0-shot,accuracy,0.25892857142857145,0.04157751539865629
NinedayWang/PolyCoder-2.7B,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.031270907132976984
NinedayWang/PolyCoder-2.7B,mmlu_high_school_statistics,0-shot,accuracy,0.38425925925925924,0.03317354514310742
NinedayWang/PolyCoder-2.7B,mmlu_elementary_mathematics,0-shot,accuracy,0.2222222222222222,0.021411684393694196
NinedayWang/PolyCoder-2.7B,mmlu_electrical_engineering,0-shot,accuracy,0.22758620689655173,0.03493950380131184
NinedayWang/PolyCoder-2.7B,mmlu_high_school_mathematics,0-shot,accuracy,0.25555555555555554,0.02659393910184407
NinedayWang/PolyCoder-2.7B,arc_challenge,25-shot,accuracy,0.181740614334471,0.011269198948880236
NinedayWang/PolyCoder-2.7B,arc_challenge,25-shot,acc_norm,0.21928327645051193,0.012091245787615713
NinedayWang/PolyCoder-2.7B,hellaswag,10-shot,accuracy,0.2773351921927903,0.004467684132772418
NinedayWang/PolyCoder-2.7B,hellaswag,10-shot,acc_norm,0.2975502887870942,0.004562462665505278
NinedayWang/PolyCoder-2.7B,truthfulqa_mc2,0-shot,accuracy,0.4900198282743296,0.015924776108438145
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,bleu_max,19.109929609778494,0.6347291486261849
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,bleu_acc,0.2913096695226438,0.015905987048184824
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,bleu_diff,-2.73808807451127,0.4927649447592839
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge1_max,43.64910481680449,0.7663193889003078
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge1_acc,0.2974296205630355,0.01600265148736098
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge1_diff,-5.224813168517498,0.5828636449526232
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge2_max,27.07899530604482,0.8474168667870206
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge2_acc,0.24724602203182375,0.015102404797359652
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rouge2_diff,-4.815471980544821,0.6322835005066595
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rougeL_max,40.60861307441461,0.765726782060013
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rougeL_acc,0.2802937576499388,0.015723139524608777
NinedayWang/PolyCoder-2.7B,truthfulqa_gen,0-shot,rougeL_diff,-4.909635032740114,0.5671337300796637
NinedayWang/PolyCoder-2.7B,truthfulqa_mc1,0-shot,accuracy,0.2876376988984088,0.015846315101394805
NinedayWang/PolyCoder-2.7B,winogrande,5-shot,accuracy,0.5209155485398579,0.014040185494212942
NinedayWang/PolyCoder-2.7B,gsm8k,5-shot,accuracy,0.02122820318423048,0.003970449129848635
EleutherAI/gpt-neo-1.3B,minerva_math_precalc,5-shot,accuracy,0.01282051282051282,0.004818950982487613
EleutherAI/gpt-neo-1.3B,minerva_math_prealgebra,5-shot,accuracy,0.01951779563719862,0.004690029935284562
EleutherAI/gpt-neo-1.3B,minerva_math_num_theory,5-shot,accuracy,0.003703703703703704,0.002616483457231187
EleutherAI/gpt-neo-1.3B,minerva_math_intermediate_algebra,5-shot,accuracy,0.016611295681063124,0.004255602872194619
EleutherAI/gpt-neo-1.3B,minerva_math_geometry,5-shot,accuracy,0.020876826722338204,0.0065393857958139425
EleutherAI/gpt-neo-1.3B,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.006275362513989611
EleutherAI/gpt-neo-1.3B,minerva_math_algebra,5-shot,accuracy,0.017691659646166806,0.003827946497642315
EleutherAI/gpt-neo-1.3B,fld_default,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,fld_star,0-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
EleutherAI/gpt-neo-1.3B,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
EleutherAI/gpt-neo-1.3B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
EleutherAI/gpt-neo-1.3B,arithmetic_2ds,5-shot,accuracy,0.0125,0.00248494717876267
EleutherAI/gpt-neo-1.3B,arithmetic_5ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_5da,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_1dc,5-shot,accuracy,0.0055,0.0016541593398342208
EleutherAI/gpt-neo-1.3B,arithmetic_4ds,5-shot,accuracy,0.0,
EleutherAI/gpt-neo-1.3B,arithmetic_2dm,5-shot,accuracy,0.022,0.0032807593162018913
EleutherAI/gpt-neo-1.3B,arithmetic_2da,5-shot,accuracy,0.0075,0.0019296986470519835
EleutherAI/gpt-neo-1.3B,gsm8k_cot,5-shot,accuracy,0.014404852160727824,0.0032820559171369834
EleutherAI/gpt-neo-1.3B,anli_r2,0-shot,brier_score,0.8796036508016054,
EleutherAI/gpt-neo-1.3B,anli_r3,0-shot,brier_score,0.8324430010683331,
EleutherAI/gpt-neo-1.3B,anli_r1,0-shot,brier_score,0.917864024592765,
EleutherAI/gpt-neo-1.3B,xnli_eu,0-shot,brier_score,1.2200574234266282,
EleutherAI/gpt-neo-1.3B,xnli_vi,0-shot,brier_score,0.7903674559535868,
EleutherAI/gpt-neo-1.3B,xnli_ru,0-shot,brier_score,0.7566305493567674,
EleutherAI/gpt-neo-1.3B,xnli_zh,0-shot,brier_score,0.9856404379148058,
EleutherAI/gpt-neo-1.3B,xnli_tr,0-shot,brier_score,0.9169373031276504,
EleutherAI/gpt-neo-1.3B,xnli_fr,0-shot,brier_score,0.856853328987711,
EleutherAI/gpt-neo-1.3B,xnli_en,0-shot,brier_score,0.6629460290225301,
EleutherAI/gpt-neo-1.3B,xnli_ur,0-shot,brier_score,0.9441652578494679,
EleutherAI/gpt-neo-1.3B,xnli_ar,0-shot,brier_score,1.2124809306829618,
EleutherAI/gpt-neo-1.3B,xnli_de,0-shot,brier_score,0.8893252434461513,
EleutherAI/gpt-neo-1.3B,xnli_hi,0-shot,brier_score,0.8044741628382988,
EleutherAI/gpt-neo-1.3B,xnli_es,0-shot,brier_score,0.9012945391331052,
EleutherAI/gpt-neo-1.3B,xnli_bg,0-shot,brier_score,0.8686133483895749,
EleutherAI/gpt-neo-1.3B,xnli_sw,0-shot,brier_score,0.9058789893603033,
EleutherAI/gpt-neo-1.3B,xnli_el,0-shot,brier_score,1.0604737322480473,
EleutherAI/gpt-neo-1.3B,xnli_th,0-shot,brier_score,0.7762030553378716,
EleutherAI/gpt-neo-1.3B,logiqa2,0-shot,brier_score,1.225701255044906,
EleutherAI/gpt-neo-1.3B,mathqa,5-shot,brier_score,0.988187749870061,
EleutherAI/gpt-neo-1.3B,lambada_standard,0-shot,perplexity,14.582486374632817,0.46752693319723554
EleutherAI/gpt-neo-1.3B,lambada_standard,0-shot,accuracy,0.45294003493110807,0.0069350547518701846
EleutherAI/gpt-neo-1.3B,lambada_openai,0-shot,perplexity,7.497819400340795,0.19958274093516393
EleutherAI/gpt-neo-1.3B,lambada_openai,0-shot,accuracy,0.5720939258684261,0.006893185516930775
EleutherAI/gpt-neo-1.3B,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
EleutherAI/gpt-neo-1.3B,mmlu_formal_logic,0-shot,accuracy,0.25396825396825395,0.038932596106046706
EleutherAI/gpt-neo-1.3B,mmlu_prehistory,0-shot,accuracy,0.23148148148148148,0.023468429832451166
EleutherAI/gpt-neo-1.3B,mmlu_moral_scenarios,0-shot,accuracy,0.23910614525139665,0.014265554192331154
EleutherAI/gpt-neo-1.3B,mmlu_high_school_world_history,0-shot,accuracy,0.28270042194092826,0.02931281415395592
EleutherAI/gpt-neo-1.3B,mmlu_moral_disputes,0-shot,accuracy,0.2514450867052023,0.023357365785874037
EleutherAI/gpt-neo-1.3B,mmlu_professional_law,0-shot,accuracy,0.2516297262059974,0.011083276280441907
EleutherAI/gpt-neo-1.3B,mmlu_logical_fallacies,0-shot,accuracy,0.2392638036809816,0.033519538795212696
EleutherAI/gpt-neo-1.3B,mmlu_high_school_us_history,0-shot,accuracy,0.23039215686274508,0.029554292605695066
EleutherAI/gpt-neo-1.3B,mmlu_philosophy,0-shot,accuracy,0.1864951768488746,0.02212243977248077
EleutherAI/gpt-neo-1.3B,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
EleutherAI/gpt-neo-1.3B,mmlu_international_law,0-shot,accuracy,0.30578512396694213,0.04205953933884122
EleutherAI/gpt-neo-1.3B,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
EleutherAI/gpt-neo-1.3B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20725388601036268,0.029252823291803644
EleutherAI/gpt-neo-1.3B,mmlu_high_school_microeconomics,0-shot,accuracy,0.23109243697478993,0.027381406927868963
EleutherAI/gpt-neo-1.3B,mmlu_high_school_geography,0-shot,accuracy,0.17676767676767677,0.027178752639044915
EleutherAI/gpt-neo-1.3B,mmlu_high_school_psychology,0-shot,accuracy,0.1944954128440367,0.016970289090458026
EleutherAI/gpt-neo-1.3B,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/gpt-neo-1.3B,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.045604802157206845
EleutherAI/gpt-neo-1.3B,mmlu_sociology,0-shot,accuracy,0.24378109452736318,0.030360490154014652
EleutherAI/gpt-neo-1.3B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3128205128205128,0.023507579020645365
EleutherAI/gpt-neo-1.3B,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904028
EleutherAI/gpt-neo-1.3B,mmlu_professional_psychology,0-shot,accuracy,0.24836601307189543,0.017479487001364764
EleutherAI/gpt-neo-1.3B,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
EleutherAI/gpt-neo-1.3B,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.03999423879281334
EleutherAI/gpt-neo-1.3B,mmlu_miscellaneous,0-shot,accuracy,0.22349936143039592,0.014897235229450708
EleutherAI/gpt-neo-1.3B,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.029872577708891172
EleutherAI/gpt-neo-1.3B,mmlu_management,0-shot,accuracy,0.17475728155339806,0.03760178006026621
EleutherAI/gpt-neo-1.3B,mmlu_nutrition,0-shot,accuracy,0.22875816993464052,0.024051029739912258
EleutherAI/gpt-neo-1.3B,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/gpt-neo-1.3B,mmlu_human_aging,0-shot,accuracy,0.3094170403587444,0.031024411740572206
EleutherAI/gpt-neo-1.3B,mmlu_professional_medicine,0-shot,accuracy,0.16544117647058823,0.02257177102549476
EleutherAI/gpt-neo-1.3B,mmlu_college_medicine,0-shot,accuracy,0.2023121387283237,0.03063114553919882
EleutherAI/gpt-neo-1.3B,mmlu_business_ethics,0-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neo-1.3B,mmlu_clinical_knowledge,0-shot,accuracy,0.2679245283018868,0.02725726032249485
EleutherAI/gpt-neo-1.3B,mmlu_global_facts,0-shot,accuracy,0.19,0.039427724440366234
EleutherAI/gpt-neo-1.3B,mmlu_virology,0-shot,accuracy,0.27710843373493976,0.034843315926805875
EleutherAI/gpt-neo-1.3B,mmlu_professional_accounting,0-shot,accuracy,0.2375886524822695,0.025389512552729903
EleutherAI/gpt-neo-1.3B,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.040925639582376556
EleutherAI/gpt-neo-1.3B,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.035737053147634576
EleutherAI/gpt-neo-1.3B,mmlu_high_school_biology,0-shot,accuracy,0.2,0.022755204959542936
EleutherAI/gpt-neo-1.3B,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
EleutherAI/gpt-neo-1.3B,mmlu_anatomy,0-shot,accuracy,0.14814814814814814,0.03068864761035268
EleutherAI/gpt-neo-1.3B,mmlu_college_chemistry,0-shot,accuracy,0.21,0.040936018074033256
EleutherAI/gpt-neo-1.3B,mmlu_computer_security,0-shot,accuracy,0.28,0.04512608598542128
EleutherAI/gpt-neo-1.3B,mmlu_college_computer_science,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/gpt-neo-1.3B,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/gpt-neo-1.3B,mmlu_college_mathematics,0-shot,accuracy,0.29,0.04560480215720683
EleutherAI/gpt-neo-1.3B,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.029513196625539355
EleutherAI/gpt-neo-1.3B,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.045126085985421276
EleutherAI/gpt-neo-1.3B,mmlu_high_school_computer_science,0-shot,accuracy,0.27,0.0446196043338474
EleutherAI/gpt-neo-1.3B,mmlu_machine_learning,0-shot,accuracy,0.30357142857142855,0.04364226155841044
EleutherAI/gpt-neo-1.3B,mmlu_high_school_chemistry,0-shot,accuracy,0.19704433497536947,0.02798672466673622
EleutherAI/gpt-neo-1.3B,mmlu_high_school_statistics,0-shot,accuracy,0.4027777777777778,0.033448873829978666
EleutherAI/gpt-neo-1.3B,mmlu_elementary_mathematics,0-shot,accuracy,0.25396825396825395,0.02241804289111394
EleutherAI/gpt-neo-1.3B,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.037528339580033376
EleutherAI/gpt-neo-1.3B,mmlu_high_school_mathematics,0-shot,accuracy,0.2518518518518518,0.026466117538959905
EleutherAI/gpt-neo-1.3B,arc_challenge,25-shot,accuracy,0.27559726962457337,0.01305716965576184
EleutherAI/gpt-neo-1.3B,arc_challenge,25-shot,acc_norm,0.30887372013651876,0.013501770929344004
EleutherAI/gpt-neo-1.3B,truthfulqa_mc2,0-shot,accuracy,0.3961377966394332,0.014266511238016675
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,bleu_max,22.087978514504556,0.710438739697618
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,bleu_acc,0.32313341493268055,0.016371836286454607
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,bleu_diff,-3.357699260688641,0.7725285332901728
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge1_max,46.248363981939576,0.870416787403499
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge1_acc,0.29253365973072215,0.015925597445286165
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge1_diff,-4.926670357978892,0.9736757701753777
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge2_max,29.894549790277882,0.991318388653234
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge2_acc,0.2423500611995104,0.015000674373570342
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rouge2_diff,-5.599665381566807,1.0842752003655973
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rougeL_max,43.635205520666176,0.8764442787347267
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rougeL_acc,0.2839657282741738,0.015785370858396736
EleutherAI/gpt-neo-1.3B,truthfulqa_gen,0-shot,rougeL_diff,-4.913623343185734,0.9748055918882317
EleutherAI/gpt-neo-1.3B,truthfulqa_mc1,0-shot,accuracy,0.23133414932680538,0.014761945174862665
EleutherAI/gpt-neox-20b,arc:challenge,25-shot,accuracy,0.42662116040955633,0.014453185592920293
EleutherAI/gpt-neox-20b,arc:challenge,25-shot,acc_norm,0.45733788395904434,0.014558106543924063
EleutherAI/gpt-neox-20b,hellaswag,10-shot,accuracy,0.5416251742680741,0.004972460206842309
EleutherAI/gpt-neox-20b,hellaswag,10-shot,acc_norm,0.73451503684525,0.004406886100685863
EleutherAI/gpt-neox-20b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/gpt-neox-20b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.04163331998932269
EleutherAI/gpt-neox-20b,hendrycksTest-anatomy,5-shot,accuracy,0.23703703703703705,0.03673731683969506
EleutherAI/gpt-neox-20b,hendrycksTest-anatomy,5-shot,acc_norm,0.23703703703703705,0.03673731683969506
EleutherAI/gpt-neox-20b,hendrycksTest-astronomy,5-shot,accuracy,0.23026315789473684,0.03426059424403165
EleutherAI/gpt-neox-20b,hendrycksTest-astronomy,5-shot,acc_norm,0.23026315789473684,0.03426059424403165
EleutherAI/gpt-neox-20b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.04725815626252604
EleutherAI/gpt-neox-20b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.04725815626252604
EleutherAI/gpt-neox-20b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.22264150943396227,0.025604233470899095
EleutherAI/gpt-neox-20b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.22264150943396227,0.025604233470899095
EleutherAI/gpt-neox-20b,hendrycksTest-college_biology,5-shot,accuracy,0.25,0.03621034121889507
EleutherAI/gpt-neox-20b,hendrycksTest-college_biology,5-shot,acc_norm,0.25,0.03621034121889507
EleutherAI/gpt-neox-20b,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.041633319989322695
EleutherAI/gpt-neox-20b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.041633319989322695
EleutherAI/gpt-neox-20b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
EleutherAI/gpt-neox-20b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
EleutherAI/gpt-neox-20b,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/gpt-neox-20b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.04163331998932269
EleutherAI/gpt-neox-20b,hendrycksTest-college_medicine,5-shot,accuracy,0.21965317919075145,0.031568093627031744
EleutherAI/gpt-neox-20b,hendrycksTest-college_medicine,5-shot,acc_norm,0.21965317919075145,0.031568093627031744
EleutherAI/gpt-neox-20b,hendrycksTest-college_physics,5-shot,accuracy,0.22549019607843138,0.041583075330832865
EleutherAI/gpt-neox-20b,hendrycksTest-college_physics,5-shot,acc_norm,0.22549019607843138,0.041583075330832865
EleutherAI/gpt-neox-20b,hendrycksTest-computer_security,5-shot,accuracy,0.22,0.04163331998932269
EleutherAI/gpt-neox-20b,hendrycksTest-computer_security,5-shot,acc_norm,0.22,0.04163331998932269
EleutherAI/gpt-neox-20b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3617021276595745,0.0314108219759624
EleutherAI/gpt-neox-20b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3617021276595745,0.0314108219759624
EleutherAI/gpt-neox-20b,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neox-20b,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
EleutherAI/gpt-neox-20b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.20689655172413793,0.03375672449560554
EleutherAI/gpt-neox-20b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.20689655172413793,0.03375672449560554
EleutherAI/gpt-neox-20b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918417
EleutherAI/gpt-neox-20b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.022569897074918417
EleutherAI/gpt-neox-20b,hendrycksTest-formal_logic,5-shot,accuracy,0.23015873015873015,0.03764950879790607
EleutherAI/gpt-neox-20b,hendrycksTest-formal_logic,5-shot,acc_norm,0.23015873015873015,0.03764950879790607
EleutherAI/gpt-neox-20b,hendrycksTest-global_facts,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/gpt-neox-20b,hendrycksTest-global_facts,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_biology,5-shot,accuracy,0.2064516129032258,0.023025899617188712
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.2064516129032258,0.023025899617188712
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.1477832512315271,0.024969621333521277
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.1477832512315271,0.024969621333521277
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.27,0.04461960433384741
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.27,0.04461960433384741
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.21212121212121213,0.031922715695483
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.21212121212121213,0.031922715695483
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_geography,5-shot,accuracy,0.18686868686868688,0.02777253333421898
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.18686868686868688,0.02777253333421898
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.22797927461139897,0.030276909945178256
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.22797927461139897,0.030276909945178256
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2,0.020280805062535722
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2,0.020280805062535722
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.22962962962962963,0.025644108639267634
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.22962962962962963,0.025644108639267634
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.2184873949579832,0.026841514322958924
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.2184873949579832,0.026841514322958924
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_physics,5-shot,accuracy,0.24503311258278146,0.035118075718047245
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.24503311258278146,0.035118075718047245
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.21467889908256882,0.01760430414925649
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.21467889908256882,0.01760430414925649
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.17592592592592593,0.025967420958258533
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.17592592592592593,0.025967420958258533
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.29411764705882354,0.03198001660115071
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.29411764705882354,0.03198001660115071
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.26582278481012656,0.02875679962965834
EleutherAI/gpt-neox-20b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.26582278481012656,0.02875679962965834
EleutherAI/gpt-neox-20b,hendrycksTest-human_aging,5-shot,accuracy,0.34977578475336324,0.03200736719484503
EleutherAI/gpt-neox-20b,hendrycksTest-human_aging,5-shot,acc_norm,0.34977578475336324,0.03200736719484503
EleutherAI/gpt-neox-20b,hendrycksTest-human_sexuality,5-shot,accuracy,0.25190839694656486,0.03807387116306086
EleutherAI/gpt-neox-20b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.25190839694656486,0.03807387116306086
EleutherAI/gpt-neox-20b,hendrycksTest-international_law,5-shot,accuracy,0.2727272727272727,0.04065578140908705
EleutherAI/gpt-neox-20b,hendrycksTest-international_law,5-shot,acc_norm,0.2727272727272727,0.04065578140908705
EleutherAI/gpt-neox-20b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2777777777777778,0.04330043749650744
EleutherAI/gpt-neox-20b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2777777777777778,0.04330043749650744
EleutherAI/gpt-neox-20b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.2392638036809816,0.033519538795212696
EleutherAI/gpt-neox-20b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.2392638036809816,0.033519538795212696
EleutherAI/gpt-neox-20b,hendrycksTest-machine_learning,5-shot,accuracy,0.25,0.04109974682633932
EleutherAI/gpt-neox-20b,hendrycksTest-machine_learning,5-shot,acc_norm,0.25,0.04109974682633932
EleutherAI/gpt-neox-20b,hendrycksTest-management,5-shot,accuracy,0.24271844660194175,0.04245022486384495
EleutherAI/gpt-neox-20b,hendrycksTest-management,5-shot,acc_norm,0.24271844660194175,0.04245022486384495
EleutherAI/gpt-neox-20b,hendrycksTest-marketing,5-shot,accuracy,0.2863247863247863,0.02961432369045665
EleutherAI/gpt-neox-20b,hendrycksTest-marketing,5-shot,acc_norm,0.2863247863247863,0.02961432369045665
EleutherAI/gpt-neox-20b,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
EleutherAI/gpt-neox-20b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.04648231987117316
EleutherAI/gpt-neox-20b,hendrycksTest-miscellaneous,5-shot,accuracy,0.2656449553001277,0.01579430248788871
EleutherAI/gpt-neox-20b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2656449553001277,0.01579430248788871
EleutherAI/gpt-neox-20b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.023703099525258172
EleutherAI/gpt-neox-20b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.023703099525258172
EleutherAI/gpt-neox-20b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2446927374301676,0.014378169884098443
EleutherAI/gpt-neox-20b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2446927374301676,0.014378169884098443
EleutherAI/gpt-neox-20b,hendrycksTest-nutrition,5-shot,accuracy,0.27450980392156865,0.025553169991826507
EleutherAI/gpt-neox-20b,hendrycksTest-nutrition,5-shot,acc_norm,0.27450980392156865,0.025553169991826507
EleutherAI/gpt-neox-20b,hendrycksTest-philosophy,5-shot,accuracy,0.2765273311897106,0.025403832978179604
EleutherAI/gpt-neox-20b,hendrycksTest-philosophy,5-shot,acc_norm,0.2765273311897106,0.025403832978179604
EleutherAI/gpt-neox-20b,hendrycksTest-prehistory,5-shot,accuracy,0.25925925925925924,0.02438366553103545
EleutherAI/gpt-neox-20b,hendrycksTest-prehistory,5-shot,acc_norm,0.25925925925925924,0.02438366553103545
EleutherAI/gpt-neox-20b,hendrycksTest-professional_accounting,5-shot,accuracy,0.25886524822695034,0.026129572527180848
EleutherAI/gpt-neox-20b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25886524822695034,0.026129572527180848
EleutherAI/gpt-neox-20b,hendrycksTest-professional_law,5-shot,accuracy,0.23859191655801826,0.010885929742002207
EleutherAI/gpt-neox-20b,hendrycksTest-professional_law,5-shot,acc_norm,0.23859191655801826,0.010885929742002207
EleutherAI/gpt-neox-20b,hendrycksTest-professional_medicine,5-shot,accuracy,0.19117647058823528,0.02388688192244034
EleutherAI/gpt-neox-20b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.19117647058823528,0.02388688192244034
EleutherAI/gpt-neox-20b,hendrycksTest-professional_psychology,5-shot,accuracy,0.25980392156862747,0.017740899509177795
EleutherAI/gpt-neox-20b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25980392156862747,0.017740899509177795
EleutherAI/gpt-neox-20b,hendrycksTest-public_relations,5-shot,accuracy,0.2545454545454545,0.04172343038705383
EleutherAI/gpt-neox-20b,hendrycksTest-public_relations,5-shot,acc_norm,0.2545454545454545,0.04172343038705383
EleutherAI/gpt-neox-20b,hendrycksTest-security_studies,5-shot,accuracy,0.20408163265306123,0.025801283475090496
EleutherAI/gpt-neox-20b,hendrycksTest-security_studies,5-shot,acc_norm,0.20408163265306123,0.025801283475090496
EleutherAI/gpt-neox-20b,hendrycksTest-sociology,5-shot,accuracy,0.2885572139303483,0.032038410402133226
EleutherAI/gpt-neox-20b,hendrycksTest-sociology,5-shot,acc_norm,0.2885572139303483,0.032038410402133226
EleutherAI/gpt-neox-20b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.22,0.041633319989322695
EleutherAI/gpt-neox-20b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.22,0.041633319989322695
EleutherAI/gpt-neox-20b,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.03591566797824662
EleutherAI/gpt-neox-20b,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.03591566797824662
EleutherAI/gpt-neox-20b,hendrycksTest-world_religions,5-shot,accuracy,0.3333333333333333,0.036155076303109344
EleutherAI/gpt-neox-20b,hendrycksTest-world_religions,5-shot,acc_norm,0.3333333333333333,0.036155076303109344
EleutherAI/gpt-neox-20b,truthfulqa:mc,0-shot,mc1,0.193390452876377,0.013826240752599066
EleutherAI/gpt-neox-20b,truthfulqa:mc,0-shot,mc2,0.3161314596733849,0.013022756719177409
EleutherAI/gpt-neox-20b,drop,3-shot,accuracy,0.001363255033557047,0.00037786091964606844
EleutherAI/gpt-neox-20b,drop,3-shot,f1,0.050428901006711505,0.0012240402281522937
EleutherAI/gpt-neox-20b,gsm8k,5-shot,accuracy,0.05458680818802123,0.00625744403791253
EleutherAI/gpt-neox-20b,winogrande,5-shot,accuracy,0.6890292028413575,0.013009534736286067
allenai/OLMo-1.7-7B-hf,minerva_math_precalc,5-shot,accuracy,0.023809523809523808,0.006530469219761487
allenai/OLMo-1.7-7B-hf,minerva_math_prealgebra,5-shot,accuracy,0.10677382319173363,0.01047016417181583
allenai/OLMo-1.7-7B-hf,minerva_math_num_theory,5-shot,accuracy,0.022222222222222223,0.006349206349206315
allenai/OLMo-1.7-7B-hf,minerva_math_intermediate_algebra,5-shot,accuracy,0.029900332225913623,0.005670781528776242
allenai/OLMo-1.7-7B-hf,minerva_math_geometry,5-shot,accuracy,0.04175365344467641,0.009148963161034336
allenai/OLMo-1.7-7B-hf,minerva_math_counting_and_prob,5-shot,accuracy,0.05063291139240506,0.010080984934213225
allenai/OLMo-1.7-7B-hf,minerva_math_algebra,5-shot,accuracy,0.08508845829823083,0.008101818991032753
allenai/OLMo-1.7-7B-hf,fld_default,0-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,fld_star,0-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_3da,5-shot,accuracy,0.005,0.0015775754727385088
allenai/OLMo-1.7-7B-hf,arithmetic_3ds,5-shot,accuracy,0.012,0.002435357362429852
allenai/OLMo-1.7-7B-hf,arithmetic_4da,5-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_2ds,5-shot,accuracy,0.054,0.005055173329243417
allenai/OLMo-1.7-7B-hf,arithmetic_5ds,5-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_5da,5-shot,accuracy,0.0,
allenai/OLMo-1.7-7B-hf,arithmetic_1dc,5-shot,accuracy,0.0275,0.0036576719757437657
allenai/OLMo-1.7-7B-hf,arithmetic_4ds,5-shot,accuracy,0.0015,0.0008655920660521547
allenai/OLMo-1.7-7B-hf,arithmetic_2dm,5-shot,accuracy,0.0525,0.004988418302285764
allenai/OLMo-1.7-7B-hf,arithmetic_2da,5-shot,accuracy,0.0665,0.0055726476832024295
allenai/OLMo-1.7-7B-hf,gsm8k_cot,5-shot,accuracy,0.26156178923426837,0.012105605733382442
allenai/OLMo-1.7-7B-hf,gsm8k,5-shot,accuracy,0.266868840030326,0.012183780551887954
allenai/OLMo-1.7-7B-hf,anli_r2,0-shot,brier_score,0.8391596666130678,
allenai/OLMo-1.7-7B-hf,anli_r3,0-shot,brier_score,0.7378078013226562,
allenai/OLMo-1.7-7B-hf,anli_r1,0-shot,brier_score,0.8642801317460914,
allenai/OLMo-1.7-7B-hf,xnli_eu,0-shot,brier_score,0.9129364330761797,
allenai/OLMo-1.7-7B-hf,xnli_vi,0-shot,brier_score,0.967681110248311,
allenai/OLMo-1.7-7B-hf,xnli_ru,0-shot,brier_score,0.7775890640925086,
allenai/OLMo-1.7-7B-hf,xnli_zh,0-shot,brier_score,0.9694985754970613,
allenai/OLMo-1.7-7B-hf,xnli_tr,0-shot,brier_score,0.8886549764770375,
allenai/OLMo-1.7-7B-hf,xnli_fr,0-shot,brier_score,0.819053260383407,
allenai/OLMo-1.7-7B-hf,xnli_en,0-shot,brier_score,0.634008528496921,
allenai/OLMo-1.7-7B-hf,xnli_ur,0-shot,brier_score,1.289747157627863,
allenai/OLMo-1.7-7B-hf,xnli_ar,0-shot,brier_score,1.1791061584321425,
allenai/OLMo-1.7-7B-hf,xnli_de,0-shot,brier_score,0.8058792999721985,
allenai/OLMo-1.7-7B-hf,xnli_hi,0-shot,brier_score,1.0788350632986858,
allenai/OLMo-1.7-7B-hf,xnli_es,0-shot,brier_score,0.870575516923212,
allenai/OLMo-1.7-7B-hf,xnli_bg,0-shot,brier_score,0.8898152315502774,
allenai/OLMo-1.7-7B-hf,xnli_sw,0-shot,brier_score,0.9207528263844112,
allenai/OLMo-1.7-7B-hf,xnli_el,0-shot,brier_score,1.1774979653735478,
allenai/OLMo-1.7-7B-hf,xnli_th,0-shot,brier_score,0.9694268391093578,
allenai/OLMo-1.7-7B-hf,logiqa2,0-shot,brier_score,1.0623218884422152,
allenai/OLMo-1.7-7B-hf,mathqa,5-shot,brier_score,0.8861806154897796,
allenai/OLMo-1.7-7B-hf,lambada_standard,0-shot,perplexity,4.885471880639927,0.10747308964809098
allenai/OLMo-1.7-7B-hf,lambada_standard,0-shot,accuracy,0.6382689695323113,0.006694325434645211
allenai/OLMo-1.7-7B-hf,lambada_openai,0-shot,perplexity,3.8600315637206424,0.07946409721254376
allenai/OLMo-1.7-7B-hf,lambada_openai,0-shot,accuracy,0.7075490005821852,0.0063374841865443225
allenai/OLMo-1.7-7B-hf,mmlu_world_religions,0-shot,accuracy,0.7076023391812866,0.034886477134579215
allenai/OLMo-1.7-7B-hf,mmlu_formal_logic,0-shot,accuracy,0.25396825396825395,0.03893259610604674
allenai/OLMo-1.7-7B-hf,mmlu_prehistory,0-shot,accuracy,0.5740740740740741,0.027513747284379424
allenai/OLMo-1.7-7B-hf,mmlu_moral_scenarios,0-shot,accuracy,0.24022346368715083,0.014288343803925319
allenai/OLMo-1.7-7B-hf,mmlu_high_school_world_history,0-shot,accuracy,0.6962025316455697,0.029936696387138615
allenai/OLMo-1.7-7B-hf,mmlu_moral_disputes,0-shot,accuracy,0.5751445086705202,0.026613350840261736
allenai/OLMo-1.7-7B-hf,mmlu_professional_law,0-shot,accuracy,0.38005215123859193,0.012397328205137812
allenai/OLMo-1.7-7B-hf,mmlu_logical_fallacies,0-shot,accuracy,0.588957055214724,0.038656978537853624
allenai/OLMo-1.7-7B-hf,mmlu_high_school_us_history,0-shot,accuracy,0.6225490196078431,0.03402272044340704
allenai/OLMo-1.7-7B-hf,mmlu_philosophy,0-shot,accuracy,0.594855305466238,0.027882383791325963
allenai/OLMo-1.7-7B-hf,mmlu_jurisprudence,0-shot,accuracy,0.5833333333333334,0.04766075165356461
allenai/OLMo-1.7-7B-hf,mmlu_international_law,0-shot,accuracy,0.6776859504132231,0.042664163633521664
allenai/OLMo-1.7-7B-hf,mmlu_high_school_european_history,0-shot,accuracy,0.6787878787878788,0.03646204963253812
allenai/OLMo-1.7-7B-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.7357512953367875,0.03182155050916644
allenai/OLMo-1.7-7B-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.5588235294117647,0.0322529423239964
allenai/OLMo-1.7-7B-hf,mmlu_high_school_geography,0-shot,accuracy,0.7272727272727273,0.03173071239071724
allenai/OLMo-1.7-7B-hf,mmlu_high_school_psychology,0-shot,accuracy,0.7394495412844037,0.018819182034850068
allenai/OLMo-1.7-7B-hf,mmlu_public_relations,0-shot,accuracy,0.6,0.0469237132203465
allenai/OLMo-1.7-7B-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.79,0.040936018074033256
allenai/OLMo-1.7-7B-hf,mmlu_sociology,0-shot,accuracy,0.746268656716418,0.030769444967296035
allenai/OLMo-1.7-7B-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.5282051282051282,0.0253106392549339
allenai/OLMo-1.7-7B-hf,mmlu_security_studies,0-shot,accuracy,0.5836734693877551,0.031557828165561644
allenai/OLMo-1.7-7B-hf,mmlu_professional_psychology,0-shot,accuracy,0.5081699346405228,0.02022513434305727
allenai/OLMo-1.7-7B-hf,mmlu_human_sexuality,0-shot,accuracy,0.6030534351145038,0.04291135671009224
allenai/OLMo-1.7-7B-hf,mmlu_econometrics,0-shot,accuracy,0.35964912280701755,0.04514496132873633
allenai/OLMo-1.7-7B-hf,mmlu_miscellaneous,0-shot,accuracy,0.7113665389527458,0.016203792703197797
allenai/OLMo-1.7-7B-hf,mmlu_marketing,0-shot,accuracy,0.8034188034188035,0.02603538609895129
allenai/OLMo-1.7-7B-hf,mmlu_management,0-shot,accuracy,0.6893203883495146,0.045821241601615506
allenai/OLMo-1.7-7B-hf,mmlu_nutrition,0-shot,accuracy,0.6111111111111112,0.02791405551046801
allenai/OLMo-1.7-7B-hf,mmlu_medical_genetics,0-shot,accuracy,0.57,0.049756985195624284
allenai/OLMo-1.7-7B-hf,mmlu_human_aging,0-shot,accuracy,0.6278026905829597,0.03244305283008731
allenai/OLMo-1.7-7B-hf,mmlu_professional_medicine,0-shot,accuracy,0.4852941176470588,0.03035969707904611
allenai/OLMo-1.7-7B-hf,mmlu_college_medicine,0-shot,accuracy,0.5086705202312138,0.03811890988940412
allenai/OLMo-1.7-7B-hf,mmlu_business_ethics,0-shot,accuracy,0.59,0.04943110704237102
allenai/OLMo-1.7-7B-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.5773584905660377,0.03040233144576954
allenai/OLMo-1.7-7B-hf,mmlu_global_facts,0-shot,accuracy,0.35,0.0479372485441102
allenai/OLMo-1.7-7B-hf,mmlu_virology,0-shot,accuracy,0.4457831325301205,0.03869543323472101
allenai/OLMo-1.7-7B-hf,mmlu_professional_accounting,0-shot,accuracy,0.4326241134751773,0.029555454236778838
allenai/OLMo-1.7-7B-hf,mmlu_college_physics,0-shot,accuracy,0.30392156862745096,0.04576665403207764
allenai/OLMo-1.7-7B-hf,mmlu_high_school_physics,0-shot,accuracy,0.3509933774834437,0.03896981964257375
allenai/OLMo-1.7-7B-hf,mmlu_high_school_biology,0-shot,accuracy,0.6548387096774193,0.027045746573534334
allenai/OLMo-1.7-7B-hf,mmlu_college_biology,0-shot,accuracy,0.5347222222222222,0.041711158581816184
allenai/OLMo-1.7-7B-hf,mmlu_anatomy,0-shot,accuracy,0.4888888888888889,0.043182754919779756
allenai/OLMo-1.7-7B-hf,mmlu_college_chemistry,0-shot,accuracy,0.43,0.049756985195624284
allenai/OLMo-1.7-7B-hf,mmlu_computer_security,0-shot,accuracy,0.62,0.04878317312145634
allenai/OLMo-1.7-7B-hf,mmlu_college_computer_science,0-shot,accuracy,0.48,0.050211673156867795
allenai/OLMo-1.7-7B-hf,mmlu_astronomy,0-shot,accuracy,0.5131578947368421,0.04067533136309172
allenai/OLMo-1.7-7B-hf,mmlu_college_mathematics,0-shot,accuracy,0.34,0.04760952285695235
allenai/OLMo-1.7-7B-hf,mmlu_conceptual_physics,0-shot,accuracy,0.425531914893617,0.03232146916224469
allenai/OLMo-1.7-7B-hf,mmlu_abstract_algebra,0-shot,accuracy,0.36,0.04824181513244218
allenai/OLMo-1.7-7B-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.57,0.04975698519562428
allenai/OLMo-1.7-7B-hf,mmlu_machine_learning,0-shot,accuracy,0.3482142857142857,0.04521829902833585
allenai/OLMo-1.7-7B-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.42857142857142855,0.034819048444388045
allenai/OLMo-1.7-7B-hf,mmlu_high_school_statistics,0-shot,accuracy,0.4444444444444444,0.03388857118502325
allenai/OLMo-1.7-7B-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.30158730158730157,0.023636975996101803
allenai/OLMo-1.7-7B-hf,mmlu_electrical_engineering,0-shot,accuracy,0.496551724137931,0.0416656757710158
allenai/OLMo-1.7-7B-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.32592592592592595,0.02857834836547308
allenai/OLMo-1.7-7B-hf,arc_challenge,25-shot,accuracy,0.4598976109215017,0.01456431885692485
allenai/OLMo-1.7-7B-hf,arc_challenge,25-shot,acc_norm,0.4974402730375427,0.014611199329843784
allenai/OLMo-1.7-7B-hf,hellaswag,10-shot,accuracy,0.5886277633937462,0.004910767540867411
allenai/OLMo-1.7-7B-hf,hellaswag,10-shot,acc_norm,0.788886675960964,0.00407264587499226
allenai/OLMo-1.7-7B-hf,truthfulqa_mc2,0-shot,accuracy,0.3590781045200021,0.013472691004650292
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,bleu_max,25.135570028092875,0.7937442609696079
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,bleu_acc,0.31211750305997554,0.01622075676952092
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,bleu_diff,-8.039782636719476,0.8039064432980269
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge1_max,50.26135196252015,0.8621379304595767
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge1_acc,0.29008567931456547,0.01588623687420952
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge1_diff,-11.903242447741343,0.9071414792248184
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge2_max,33.597620557273906,1.0023202672623095
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge2_acc,0.24724602203182375,0.015102404797359649
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rouge2_diff,-12.949554590188171,1.025590144258018
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rougeL_max,47.52109438620076,0.8690433934810369
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rougeL_acc,0.28151774785801714,0.01574402724825605
allenai/OLMo-1.7-7B-hf,truthfulqa_gen,0-shot,rougeL_diff,-11.977711482592492,0.9105847605364588
allenai/OLMo-1.7-7B-hf,truthfulqa_mc1,0-shot,accuracy,0.2386780905752754,0.014922629695456416
allenai/OLMo-1.7-7B-hf,winogrande,5-shot,accuracy,0.7253354380426204,0.012544516005117192
microsoft/phi-2,mmlu_world_religions,0-shot,accuracy,0.6900584795321637,0.03546976959393163
microsoft/phi-2,mmlu_formal_logic,0-shot,accuracy,0.35714285714285715,0.04285714285714281
microsoft/phi-2,mmlu_prehistory,0-shot,accuracy,0.6234567901234568,0.026959344518747784
microsoft/phi-2,mmlu_moral_scenarios,0-shot,accuracy,0.3139664804469274,0.015521923933523647
microsoft/phi-2,mmlu_high_school_world_history,0-shot,accuracy,0.7383966244725738,0.028609516716994934
microsoft/phi-2,mmlu_moral_disputes,0-shot,accuracy,0.6676300578034682,0.025361168749688214
microsoft/phi-2,mmlu_professional_law,0-shot,accuracy,0.4198174706649283,0.01260496081608737
microsoft/phi-2,mmlu_logical_fallacies,0-shot,accuracy,0.7484662576687117,0.034089978868575295
microsoft/phi-2,mmlu_high_school_us_history,0-shot,accuracy,0.6617647058823529,0.03320574612945431
microsoft/phi-2,mmlu_philosophy,0-shot,accuracy,0.6270096463022508,0.027466610213140112
microsoft/phi-2,mmlu_jurisprudence,0-shot,accuracy,0.7129629629629629,0.043733130409147614
microsoft/phi-2,mmlu_international_law,0-shot,accuracy,0.743801652892562,0.03984979653302872
microsoft/phi-2,mmlu_high_school_european_history,0-shot,accuracy,0.6424242424242425,0.037425970438065864
microsoft/phi-2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8082901554404145,0.02840895362624526
microsoft/phi-2,mmlu_high_school_microeconomics,0-shot,accuracy,0.6050420168067226,0.03175367846096626
microsoft/phi-2,mmlu_high_school_geography,0-shot,accuracy,0.7373737373737373,0.03135305009533085
microsoft/phi-2,mmlu_high_school_psychology,0-shot,accuracy,0.8,0.01714985851425093
microsoft/phi-2,mmlu_public_relations,0-shot,accuracy,0.6727272727272727,0.0449429086625209
microsoft/phi-2,mmlu_us_foreign_policy,0-shot,accuracy,0.76,0.04292346959909283
microsoft/phi-2,mmlu_sociology,0-shot,accuracy,0.8109452736318408,0.027686913588013028
microsoft/phi-2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.5692307692307692,0.02510682066053975
microsoft/phi-2,mmlu_security_studies,0-shot,accuracy,0.7306122448979592,0.02840125202902294
microsoft/phi-2,mmlu_professional_psychology,0-shot,accuracy,0.5604575163398693,0.02007942040808792
microsoft/phi-2,mmlu_human_sexuality,0-shot,accuracy,0.7099236641221374,0.03980066246467765
microsoft/phi-2,mmlu_econometrics,0-shot,accuracy,0.38596491228070173,0.045796394220704334
microsoft/phi-2,mmlu_miscellaneous,0-shot,accuracy,0.6896551724137931,0.016543785026048318
microsoft/phi-2,mmlu_marketing,0-shot,accuracy,0.8205128205128205,0.025140935950335442
microsoft/phi-2,mmlu_management,0-shot,accuracy,0.7378640776699029,0.04354631077260595
microsoft/phi-2,mmlu_nutrition,0-shot,accuracy,0.6176470588235294,0.027826109307283683
microsoft/phi-2,mmlu_medical_genetics,0-shot,accuracy,0.63,0.04852365870939099
microsoft/phi-2,mmlu_human_aging,0-shot,accuracy,0.6502242152466368,0.03200736719484503
microsoft/phi-2,mmlu_professional_medicine,0-shot,accuracy,0.47794117647058826,0.03034326422421352
microsoft/phi-2,mmlu_college_medicine,0-shot,accuracy,0.5953757225433526,0.03742461193887248
microsoft/phi-2,mmlu_business_ethics,0-shot,accuracy,0.6,0.049236596391733084
microsoft/phi-2,mmlu_clinical_knowledge,0-shot,accuracy,0.6113207547169811,0.030000485448675986
microsoft/phi-2,mmlu_global_facts,0-shot,accuracy,0.34,0.04760952285695235
microsoft/phi-2,mmlu_virology,0-shot,accuracy,0.4759036144578313,0.03887971849597264
microsoft/phi-2,mmlu_professional_accounting,0-shot,accuracy,0.4432624113475177,0.029634838473766006
microsoft/phi-2,mmlu_college_physics,0-shot,accuracy,0.3627450980392157,0.04784060704105654
microsoft/phi-2,mmlu_high_school_physics,0-shot,accuracy,0.37748344370860926,0.0395802723112157
microsoft/phi-2,mmlu_high_school_biology,0-shot,accuracy,0.7,0.02606936229533514
microsoft/phi-2,mmlu_college_biology,0-shot,accuracy,0.6666666666666666,0.039420826399272135
microsoft/phi-2,mmlu_anatomy,0-shot,accuracy,0.4444444444444444,0.04292596718256981
microsoft/phi-2,mmlu_college_chemistry,0-shot,accuracy,0.41,0.049431107042371025
microsoft/phi-2,mmlu_computer_security,0-shot,accuracy,0.74,0.044084400227680794
microsoft/phi-2,mmlu_college_computer_science,0-shot,accuracy,0.43,0.04975698519562428
microsoft/phi-2,mmlu_astronomy,0-shot,accuracy,0.5855263157894737,0.04008973785779206
microsoft/phi-2,mmlu_college_mathematics,0-shot,accuracy,0.38,0.04878317312145632
microsoft/phi-2,mmlu_conceptual_physics,0-shot,accuracy,0.5063829787234042,0.032683358999363366
microsoft/phi-2,mmlu_abstract_algebra,0-shot,accuracy,0.31,0.04648231987117316
microsoft/phi-2,mmlu_high_school_computer_science,0-shot,accuracy,0.64,0.04824181513244218
microsoft/phi-2,mmlu_machine_learning,0-shot,accuracy,0.49107142857142855,0.04745033255489123
microsoft/phi-2,mmlu_high_school_chemistry,0-shot,accuracy,0.4827586206896552,0.035158955511657
microsoft/phi-2,mmlu_high_school_statistics,0-shot,accuracy,0.49537037037037035,0.03409825519163572
microsoft/phi-2,mmlu_elementary_mathematics,0-shot,accuracy,0.455026455026455,0.02564692836104939
microsoft/phi-2,mmlu_electrical_engineering,0-shot,accuracy,0.5448275862068965,0.04149886942192117
microsoft/phi-2,mmlu_high_school_mathematics,0-shot,accuracy,0.3333333333333333,0.028742040903948485
microsoft/phi-2,arc_challenge,25-shot,accuracy,0.575938566552901,0.014441889627464398
microsoft/phi-2,arc_challenge,25-shot,acc_norm,0.6083617747440273,0.014264122124938215
microsoft/phi-2,hellaswag,10-shot,accuracy,0.5655247958573989,0.004946748608271356
microsoft/phi-2,hellaswag,10-shot,acc_norm,0.7542322246564429,0.004296615862786664
microsoft/phi-2,truthfulqa_mc2,0-shot,accuracy,0.44503297968968214,0.015126633399430035
microsoft/phi-2,truthfulqa_gen,0-shot,bleu_max,30.636426608129604,0.8253318984766498
microsoft/phi-2,truthfulqa_gen,0-shot,bleu_acc,0.3843329253365973,0.01702870730124523
microsoft/phi-2,truthfulqa_gen,0-shot,bleu_diff,-1.6737273900429643,0.9377681674822301
microsoft/phi-2,truthfulqa_gen,0-shot,rouge1_max,56.49815681439475,0.8385465502514413
microsoft/phi-2,truthfulqa_gen,0-shot,rouge1_acc,0.3929008567931457,0.017097248285233065
microsoft/phi-2,truthfulqa_gen,0-shot,rouge1_diff,-1.2755753639684835,1.1278344789587786
microsoft/phi-2,truthfulqa_gen,0-shot,rouge2_max,42.30652845433004,1.0170277948205164
microsoft/phi-2,truthfulqa_gen,0-shot,rouge2_acc,0.35495716034271724,0.01675086238137591
microsoft/phi-2,truthfulqa_gen,0-shot,rouge2_diff,-2.1576932295313727,1.2989464511092814
microsoft/phi-2,truthfulqa_gen,0-shot,rougeL_max,53.81199070019868,0.8662590848816515
microsoft/phi-2,truthfulqa_gen,0-shot,rougeL_acc,0.3769889840881273,0.016965517578930358
microsoft/phi-2,truthfulqa_gen,0-shot,rougeL_diff,-1.3616416414143386,1.1477177478773908
microsoft/phi-2,truthfulqa_mc1,0-shot,accuracy,0.30966952264381886,0.016185744355144933
microsoft/phi-2,winogrande,5-shot,accuracy,0.734017363851618,0.012418323153051051
microsoft/phi-2,gsm8k,5-shot,accuracy,0.576194086429113,0.013611632008810359
EleutherAI/pythia-160m-deduped,arc:challenge,25-shot,accuracy,0.20563139931740615,0.011810745260742569
EleutherAI/pythia-160m-deduped,arc:challenge,25-shot,acc_norm,0.24061433447098976,0.012491468532390566
EleutherAI/pythia-160m-deduped,hellaswag,10-shot,accuracy,0.28649671380203146,0.004512002459757946
EleutherAI/pythia-160m-deduped,hellaswag,10-shot,acc_norm,0.31388169687313283,0.004631205099684944
EleutherAI/pythia-160m-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-160m-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.25,0.04351941398892446
EleutherAI/pythia-160m-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.2222222222222222,0.035914440841969694
EleutherAI/pythia-160m-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.2222222222222222,0.035914440841969694
EleutherAI/pythia-160m-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-160m-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.17763157894736842,0.031103182383123398
EleutherAI/pythia-160m-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.17,0.0377525168068637
EleutherAI/pythia-160m-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.17,0.0377525168068637
EleutherAI/pythia-160m-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.20754716981132076,0.02495991802891127
EleutherAI/pythia-160m-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.20754716981132076,0.02495991802891127
EleutherAI/pythia-160m-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.22916666666666666,0.03514697467862388
EleutherAI/pythia-160m-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.22916666666666666,0.03514697467862388
EleutherAI/pythia-160m-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036845
EleutherAI/pythia-160m-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036845
EleutherAI/pythia-160m-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/pythia-160m-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.04512608598542128
EleutherAI/pythia-160m-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-160m-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.25,0.04351941398892446
EleutherAI/pythia-160m-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.20809248554913296,0.03095289021774988
EleutherAI/pythia-160m-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.20809248554913296,0.03095289021774988
EleutherAI/pythia-160m-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.2549019607843137,0.04336432707993178
EleutherAI/pythia-160m-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.2549019607843137,0.04336432707993178
EleutherAI/pythia-160m-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.04512608598542128
EleutherAI/pythia-160m-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.04512608598542128
EleutherAI/pythia-160m-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.23404255319148937,0.02767845257821239
EleutherAI/pythia-160m-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.23404255319148937,0.02767845257821239
EleutherAI/pythia-160m-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.21929824561403508,0.03892431106518754
EleutherAI/pythia-160m-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.21929824561403508,0.03892431106518754
EleutherAI/pythia-160m-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2827586206896552,0.037528339580033376
EleutherAI/pythia-160m-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2827586206896552,0.037528339580033376
EleutherAI/pythia-160m-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25396825396825395,0.022418042891113942
EleutherAI/pythia-160m-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25396825396825395,0.022418042891113942
EleutherAI/pythia-160m-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.20634920634920634,0.03619604524124251
EleutherAI/pythia-160m-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.20634920634920634,0.03619604524124251
EleutherAI/pythia-160m-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.16,0.03684529491774708
EleutherAI/pythia-160m-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.16,0.03684529491774708
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.25483870967741934,0.024790118459332208
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.25483870967741934,0.024790118459332208
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3103448275862069,0.03255086769970103
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3103448275862069,0.03255086769970103
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.24,0.04292346959909281
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.24,0.04292346959909281
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.21212121212121213,0.03192271569548299
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.21212121212121213,0.03192271569548299
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.2474747474747475,0.030746300742124505
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2474747474747475,0.030746300742124505
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.25906735751295334,0.031618779179354094
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.25906735751295334,0.031618779179354094
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.2153846153846154,0.020843034557462878
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.2153846153846154,0.020843034557462878
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25555555555555554,0.02659393910184407
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25555555555555554,0.02659393910184407
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.226890756302521,0.027205371538279483
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.226890756302521,0.027205371538279483
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.32450331125827814,0.03822746937658754
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.32450331125827814,0.03822746937658754
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.24220183486238533,0.018368176306598615
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.24220183486238533,0.018368176306598615
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.20098039215686275,0.02812597226565439
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.20098039215686275,0.02812597226565439
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.27848101265822783,0.029178682304842548
EleutherAI/pythia-160m-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.27848101265822783,0.029178682304842548
EleutherAI/pythia-160m-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.36771300448430494,0.03236198350928276
EleutherAI/pythia-160m-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.36771300448430494,0.03236198350928276
EleutherAI/pythia-160m-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.26717557251908397,0.038808483010823944
EleutherAI/pythia-160m-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.26717557251908397,0.038808483010823944
EleutherAI/pythia-160m-deduped,hendrycksTest-international_law,5-shot,accuracy,0.30578512396694213,0.04205953933884123
EleutherAI/pythia-160m-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.30578512396694213,0.04205953933884123
EleutherAI/pythia-160m-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.21296296296296297,0.0395783547198098
EleutherAI/pythia-160m-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.21296296296296297,0.0395783547198098
EleutherAI/pythia-160m-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.294478527607362,0.03581165790474082
EleutherAI/pythia-160m-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.294478527607362,0.03581165790474082
EleutherAI/pythia-160m-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.17857142857142858,0.03635209121577806
EleutherAI/pythia-160m-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.17857142857142858,0.03635209121577806
EleutherAI/pythia-160m-deduped,hendrycksTest-management,5-shot,accuracy,0.1553398058252427,0.03586594738573973
EleutherAI/pythia-160m-deduped,hendrycksTest-management,5-shot,acc_norm,0.1553398058252427,0.03586594738573973
EleutherAI/pythia-160m-deduped,hendrycksTest-marketing,5-shot,accuracy,0.1752136752136752,0.02490443909891823
EleutherAI/pythia-160m-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.1752136752136752,0.02490443909891823
EleutherAI/pythia-160m-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.32,0.04688261722621504
EleutherAI/pythia-160m-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.32,0.04688261722621504
EleutherAI/pythia-160m-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.2413793103448276,0.015302380123542094
EleutherAI/pythia-160m-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2413793103448276,0.015302380123542094
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.24566473988439305,0.023176298203992016
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24566473988439305,0.023176298203992016
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.01433352205921789
EleutherAI/pythia-160m-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.01433352205921789
EleutherAI/pythia-160m-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.24836601307189543,0.02473998135511359
EleutherAI/pythia-160m-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.24836601307189543,0.02473998135511359
EleutherAI/pythia-160m-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.2057877813504823,0.022961339906764234
EleutherAI/pythia-160m-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.2057877813504823,0.022961339906764234
EleutherAI/pythia-160m-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.24074074074074073,0.02378858355165854
EleutherAI/pythia-160m-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.24074074074074073,0.02378858355165854
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.23404255319148937,0.025257861359432407
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.23404255319148937,0.025257861359432407
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.23728813559322035,0.010865436690780278
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.23728813559322035,0.010865436690780278
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.4375,0.030134614954403924
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4375,0.030134614954403924
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.25326797385620914,0.017593486895366835
EleutherAI/pythia-160m-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25326797385620914,0.017593486895366835
EleutherAI/pythia-160m-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.2,0.03831305140884603
EleutherAI/pythia-160m-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.2,0.03831305140884603
EleutherAI/pythia-160m-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.30612244897959184,0.029504896454595985
EleutherAI/pythia-160m-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.30612244897959184,0.029504896454595985
EleutherAI/pythia-160m-deduped,hendrycksTest-sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409217
EleutherAI/pythia-160m-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.23880597014925373,0.030147775935409217
EleutherAI/pythia-160m-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.29,0.045604802157206845
EleutherAI/pythia-160m-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.29,0.045604802157206845
EleutherAI/pythia-160m-deduped,hendrycksTest-virology,5-shot,accuracy,0.2469879518072289,0.03357351982064536
EleutherAI/pythia-160m-deduped,hendrycksTest-virology,5-shot,acc_norm,0.2469879518072289,0.03357351982064536
EleutherAI/pythia-160m-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.19883040935672514,0.03061111655743253
EleutherAI/pythia-160m-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.19883040935672514,0.03061111655743253
EleutherAI/pythia-160m-deduped,truthfulqa:mc,0-shot,mc1,0.24112607099143207,0.01497482727975233
EleutherAI/pythia-160m-deduped,truthfulqa:mc,0-shot,mc2,0.44340446376402504,0.01520717725288623
EleutherAI/pythia-160m-deduped,drop,3-shot,accuracy,0.003145973154362416,0.0005734993648436387
EleutherAI/pythia-160m-deduped,drop,3-shot,f1,0.033831795302013495,0.0011064778180343976
EleutherAI/pythia-160m-deduped,gsm8k,5-shot,accuracy,0.002274450341167551,0.0013121578148674233
EleutherAI/pythia-160m-deduped,winogrande,5-shot,accuracy,0.5138121546961326,0.014047122916440422
cerebras/btlm-3b-8k-base,minerva_math_precalc,5-shot,accuracy,0.014652014652014652,0.005146894158982162
cerebras/btlm-3b-8k-base,minerva_math_prealgebra,5-shot,accuracy,0.03444316877152698,0.006182738010487318
cerebras/btlm-3b-8k-base,minerva_math_num_theory,5-shot,accuracy,0.014814814814814815,0.005203704987512652
cerebras/btlm-3b-8k-base,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.004900093088615796
cerebras/btlm-3b-8k-base,minerva_math_geometry,5-shot,accuracy,0.027139874739039668,0.00743216209077084
cerebras/btlm-3b-8k-base,minerva_math_counting_and_prob,5-shot,accuracy,0.027426160337552744,0.0075095381303842195
cerebras/btlm-3b-8k-base,minerva_math_algebra,5-shot,accuracy,0.019376579612468407,0.004002647498105362
cerebras/btlm-3b-8k-base,fld_default,0-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,fld_star,0-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,arithmetic_3da,5-shot,accuracy,0.0215,0.003244092641792814
cerebras/btlm-3b-8k-base,arithmetic_3ds,5-shot,accuracy,0.043,0.004537156917767878
cerebras/btlm-3b-8k-base,arithmetic_4da,5-shot,accuracy,0.0015,0.0008655920660521425
cerebras/btlm-3b-8k-base,arithmetic_2ds,5-shot,accuracy,0.45,0.011127079848413738
cerebras/btlm-3b-8k-base,arithmetic_5ds,5-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,arithmetic_5da,5-shot,accuracy,0.0,
cerebras/btlm-3b-8k-base,arithmetic_1dc,5-shot,accuracy,0.0005,0.0005000000000000037
cerebras/btlm-3b-8k-base,arithmetic_4ds,5-shot,accuracy,0.001,0.0007069298939339592
cerebras/btlm-3b-8k-base,arithmetic_2dm,5-shot,accuracy,0.0335,0.004024546370306097
cerebras/btlm-3b-8k-base,arithmetic_2da,5-shot,accuracy,0.207,0.009061818707033228
cerebras/btlm-3b-8k-base,gsm8k_cot,5-shot,accuracy,0.04169825625473844,0.005506205058175758
cerebras/btlm-3b-8k-base,gsm8k,5-shot,accuracy,0.052312357846853674,0.0061330577089592315
cerebras/btlm-3b-8k-base,anli_r2,0-shot,brier_score,0.7462669818323581,
cerebras/btlm-3b-8k-base,anli_r3,0-shot,brier_score,0.7470389244949329,
cerebras/btlm-3b-8k-base,anli_r1,0-shot,brier_score,0.7739522354279014,
cerebras/btlm-3b-8k-base,xnli_eu,0-shot,brier_score,1.26760834525426,
cerebras/btlm-3b-8k-base,xnli_vi,0-shot,brier_score,1.0776254851454548,
cerebras/btlm-3b-8k-base,xnli_ru,0-shot,brier_score,0.8572693761958452,
cerebras/btlm-3b-8k-base,xnli_zh,0-shot,brier_score,1.02506106359857,
cerebras/btlm-3b-8k-base,xnli_tr,0-shot,brier_score,0.9690088326153724,
cerebras/btlm-3b-8k-base,xnli_fr,0-shot,brier_score,0.7572273331665541,
cerebras/btlm-3b-8k-base,xnli_en,0-shot,brier_score,0.6442514069003313,
cerebras/btlm-3b-8k-base,xnli_ur,0-shot,brier_score,1.306062980613888,
cerebras/btlm-3b-8k-base,xnli_ar,0-shot,brier_score,0.9938749206769895,
cerebras/btlm-3b-8k-base,xnli_de,0-shot,brier_score,0.9222235658348613,
cerebras/btlm-3b-8k-base,xnli_hi,0-shot,brier_score,0.894747482032871,
cerebras/btlm-3b-8k-base,xnli_es,0-shot,brier_score,0.925763882691409,
cerebras/btlm-3b-8k-base,xnli_bg,0-shot,brier_score,0.8952905602595471,
cerebras/btlm-3b-8k-base,xnli_sw,0-shot,brier_score,0.9874132082553371,
cerebras/btlm-3b-8k-base,xnli_el,0-shot,brier_score,1.0575280874979667,
cerebras/btlm-3b-8k-base,xnli_th,0-shot,brier_score,1.2107325943420113,
cerebras/btlm-3b-8k-base,logiqa2,0-shot,brier_score,1.0390121306040254,
cerebras/btlm-3b-8k-base,mathqa,5-shot,brier_score,0.9502679389813659,
cerebras/btlm-3b-8k-base,lambada_standard,0-shot,perplexity,5.9215811991829925,0.14341353528409098
cerebras/btlm-3b-8k-base,lambada_standard,0-shot,accuracy,0.6079953425189211,0.006801548708056979
cerebras/btlm-3b-8k-base,lambada_openai,0-shot,perplexity,4.721463260829105,0.11007675882818492
cerebras/btlm-3b-8k-base,lambada_openai,0-shot,accuracy,0.6615563749272269,0.0065923259327411495
cerebras/btlm-3b-8k-base,mmlu_world_religions,0-shot,accuracy,0.30409356725146197,0.03528211258245231
cerebras/btlm-3b-8k-base,mmlu_formal_logic,0-shot,accuracy,0.21428571428571427,0.03670066451047181
cerebras/btlm-3b-8k-base,mmlu_prehistory,0-shot,accuracy,0.2808641975308642,0.025006469755799208
cerebras/btlm-3b-8k-base,mmlu_moral_scenarios,0-shot,accuracy,0.24581005586592178,0.014400296429225619
cerebras/btlm-3b-8k-base,mmlu_high_school_world_history,0-shot,accuracy,0.33755274261603374,0.030781549102026223
cerebras/btlm-3b-8k-base,mmlu_moral_disputes,0-shot,accuracy,0.2861271676300578,0.024332146779134128
cerebras/btlm-3b-8k-base,mmlu_professional_law,0-shot,accuracy,0.2529335071707953,0.011102268713839987
cerebras/btlm-3b-8k-base,mmlu_logical_fallacies,0-shot,accuracy,0.2147239263803681,0.03226219377286774
cerebras/btlm-3b-8k-base,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.030587591351604243
cerebras/btlm-3b-8k-base,mmlu_philosophy,0-shot,accuracy,0.2765273311897106,0.025403832978179604
cerebras/btlm-3b-8k-base,mmlu_jurisprudence,0-shot,accuracy,0.26851851851851855,0.04284467968052191
cerebras/btlm-3b-8k-base,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.04026187527591205
cerebras/btlm-3b-8k-base,mmlu_high_school_european_history,0-shot,accuracy,0.296969696969697,0.03567969772268047
cerebras/btlm-3b-8k-base,mmlu_high_school_government_and_politics,0-shot,accuracy,0.27461139896373055,0.03221024508041156
cerebras/btlm-3b-8k-base,mmlu_high_school_microeconomics,0-shot,accuracy,0.25630252100840334,0.02835962087053395
cerebras/btlm-3b-8k-base,mmlu_high_school_geography,0-shot,accuracy,0.22727272727272727,0.029857515673386414
cerebras/btlm-3b-8k-base,mmlu_high_school_psychology,0-shot,accuracy,0.26238532110091745,0.018861885021534745
cerebras/btlm-3b-8k-base,mmlu_public_relations,0-shot,accuracy,0.2818181818181818,0.043091187099464585
cerebras/btlm-3b-8k-base,mmlu_us_foreign_policy,0-shot,accuracy,0.29,0.04560480215720684
cerebras/btlm-3b-8k-base,mmlu_sociology,0-shot,accuracy,0.3034825870646766,0.03251006816458619
cerebras/btlm-3b-8k-base,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24102564102564103,0.02168554666533319
cerebras/btlm-3b-8k-base,mmlu_security_studies,0-shot,accuracy,0.3510204081632653,0.03055531675557364
cerebras/btlm-3b-8k-base,mmlu_professional_psychology,0-shot,accuracy,0.27124183006535946,0.017986615304030316
cerebras/btlm-3b-8k-base,mmlu_human_sexuality,0-shot,accuracy,0.25190839694656486,0.038073871163060866
cerebras/btlm-3b-8k-base,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.040969851398436716
cerebras/btlm-3b-8k-base,mmlu_miscellaneous,0-shot,accuracy,0.31928480204342274,0.01667126174953872
cerebras/btlm-3b-8k-base,mmlu_marketing,0-shot,accuracy,0.28205128205128205,0.02948036054954119
cerebras/btlm-3b-8k-base,mmlu_management,0-shot,accuracy,0.20388349514563106,0.0398913985953177
cerebras/btlm-3b-8k-base,mmlu_nutrition,0-shot,accuracy,0.25163398692810457,0.0248480182638752
cerebras/btlm-3b-8k-base,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
cerebras/btlm-3b-8k-base,mmlu_human_aging,0-shot,accuracy,0.3901345291479821,0.03273766725459157
cerebras/btlm-3b-8k-base,mmlu_professional_medicine,0-shot,accuracy,0.27205882352941174,0.027033041151681456
cerebras/btlm-3b-8k-base,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
cerebras/btlm-3b-8k-base,mmlu_business_ethics,0-shot,accuracy,0.31,0.04648231987117316
cerebras/btlm-3b-8k-base,mmlu_clinical_knowledge,0-shot,accuracy,0.2339622641509434,0.02605529690115292
cerebras/btlm-3b-8k-base,mmlu_global_facts,0-shot,accuracy,0.32,0.04688261722621505
cerebras/btlm-3b-8k-base,mmlu_virology,0-shot,accuracy,0.3132530120481928,0.036108050180310235
cerebras/btlm-3b-8k-base,mmlu_professional_accounting,0-shot,accuracy,0.24468085106382978,0.02564555362226673
cerebras/btlm-3b-8k-base,mmlu_college_physics,0-shot,accuracy,0.2549019607843137,0.04336432707993177
cerebras/btlm-3b-8k-base,mmlu_high_school_physics,0-shot,accuracy,0.23178807947019867,0.034454062719870546
cerebras/btlm-3b-8k-base,mmlu_high_school_biology,0-shot,accuracy,0.23870967741935484,0.024251071262208837
cerebras/btlm-3b-8k-base,mmlu_college_biology,0-shot,accuracy,0.3055555555555556,0.03852084696008534
cerebras/btlm-3b-8k-base,mmlu_anatomy,0-shot,accuracy,0.2074074074074074,0.03502553170678318
cerebras/btlm-3b-8k-base,mmlu_college_chemistry,0-shot,accuracy,0.27,0.04461960433384741
cerebras/btlm-3b-8k-base,mmlu_computer_security,0-shot,accuracy,0.26,0.04408440022768076
cerebras/btlm-3b-8k-base,mmlu_college_computer_science,0-shot,accuracy,0.3,0.046056618647183814
cerebras/btlm-3b-8k-base,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.035834961763610645
cerebras/btlm-3b-8k-base,mmlu_college_mathematics,0-shot,accuracy,0.3,0.046056618647183814
cerebras/btlm-3b-8k-base,mmlu_conceptual_physics,0-shot,accuracy,0.2978723404255319,0.029896145682095462
cerebras/btlm-3b-8k-base,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768079
cerebras/btlm-3b-8k-base,mmlu_high_school_computer_science,0-shot,accuracy,0.23,0.042295258468165065
cerebras/btlm-3b-8k-base,mmlu_machine_learning,0-shot,accuracy,0.25892857142857145,0.04157751539865629
cerebras/btlm-3b-8k-base,mmlu_high_school_chemistry,0-shot,accuracy,0.22660098522167488,0.029454863835292968
cerebras/btlm-3b-8k-base,mmlu_high_school_statistics,0-shot,accuracy,0.25462962962962965,0.029711275860005357
cerebras/btlm-3b-8k-base,mmlu_elementary_mathematics,0-shot,accuracy,0.2619047619047619,0.022644212615525214
cerebras/btlm-3b-8k-base,mmlu_electrical_engineering,0-shot,accuracy,0.21379310344827587,0.034165204477475494
cerebras/btlm-3b-8k-base,mmlu_high_school_mathematics,0-shot,accuracy,0.25555555555555554,0.026593939101844065
cerebras/btlm-3b-8k-base,arc_challenge,25-shot,accuracy,0.38310580204778155,0.01420647266167288
cerebras/btlm-3b-8k-base,arc_challenge,25-shot,acc_norm,0.4112627986348123,0.014379441068522084
cerebras/btlm-3b-8k-base,hellaswag,10-shot,accuracy,0.5226050587532364,0.004984679359375625
cerebras/btlm-3b-8k-base,hellaswag,10-shot,acc_norm,0.7099183429595698,0.004528723951878226
cerebras/btlm-3b-8k-base,truthfulqa_mc2,0-shot,accuracy,0.35964570198337426,0.013582290589589971
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,bleu_max,24.382575479594944,0.7680990447091552
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,bleu_acc,0.3243574051407589,0.01638797677964794
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,bleu_diff,-7.56470475629629,0.8511362909045714
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge1_max,48.69810706635801,0.8872210250095384
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge1_acc,0.2778457772337821,0.015680929364024657
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge1_diff,-9.685772374361353,1.0120488242587284
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge2_max,32.46140782430542,1.0148035582417252
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge2_acc,0.23378212974296206,0.014816195991931593
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rouge2_diff,-11.236829286123939,1.121260025022326
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rougeL_max,46.12917282015782,0.9023627784282651
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rougeL_acc,0.26560587515299877,0.015461027627253586
cerebras/btlm-3b-8k-base,truthfulqa_gen,0-shot,rougeL_diff,-9.989538296930288,1.0154544227366404
cerebras/btlm-3b-8k-base,truthfulqa_mc1,0-shot,accuracy,0.22276621787025705,0.01456650696139675
cerebras/btlm-3b-8k-base,winogrande,5-shot,accuracy,0.6614048934490924,0.013300169865842426
CohereForAI/c4ai-command-r-v01,arc:challenge,25-shot,accuracy,0.64419795221843,0.013990571137918763
CohereForAI/c4ai-command-r-v01,arc:challenge,25-shot,acc_norm,0.6552901023890785,0.01388881628678211
CohereForAI/c4ai-command-r-v01,hellaswag,10-shot,accuracy,0.6742680740888269,0.004676898861978905
CohereForAI/c4ai-command-r-v01,hellaswag,10-shot,acc_norm,0.8700458076080462,0.0033556582385714864
CohereForAI/c4ai-command-r-v01,hendrycksTest-abstract_algebra,5-shot,accuracy,0.39,0.04902071300001974
CohereForAI/c4ai-command-r-v01,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.39,0.04902071300001974
CohereForAI/c4ai-command-r-v01,hendrycksTest-anatomy,5-shot,accuracy,0.6296296296296297,0.041716541613545426
CohereForAI/c4ai-command-r-v01,hendrycksTest-anatomy,5-shot,acc_norm,0.6296296296296297,0.041716541613545426
CohereForAI/c4ai-command-r-v01,hendrycksTest-astronomy,5-shot,accuracy,0.7631578947368421,0.03459777606810536
CohereForAI/c4ai-command-r-v01,hendrycksTest-astronomy,5-shot,acc_norm,0.7631578947368421,0.03459777606810536
CohereForAI/c4ai-command-r-v01,hendrycksTest-business_ethics,5-shot,accuracy,0.75,0.04351941398892446
CohereForAI/c4ai-command-r-v01,hendrycksTest-business_ethics,5-shot,acc_norm,0.75,0.04351941398892446
CohereForAI/c4ai-command-r-v01,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7509433962264151,0.026616482980501704
CohereForAI/c4ai-command-r-v01,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7509433962264151,0.026616482980501704
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_biology,5-shot,accuracy,0.7777777777777778,0.03476590104304134
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_biology,5-shot,acc_norm,0.7777777777777778,0.03476590104304134
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_chemistry,5-shot,accuracy,0.48,0.050211673156867795
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_chemistry,5-shot,acc_norm,0.48,0.050211673156867795
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_computer_science,5-shot,accuracy,0.63,0.04852365870939099
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_computer_science,5-shot,acc_norm,0.63,0.04852365870939099
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_medicine,5-shot,accuracy,0.6589595375722543,0.03614665424180826
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_medicine,5-shot,acc_norm,0.6589595375722543,0.03614665424180826
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_physics,5-shot,accuracy,0.4019607843137255,0.04878608714466996
CohereForAI/c4ai-command-r-v01,hendrycksTest-college_physics,5-shot,acc_norm,0.4019607843137255,0.04878608714466996
CohereForAI/c4ai-command-r-v01,hendrycksTest-computer_security,5-shot,accuracy,0.78,0.04163331998932263
CohereForAI/c4ai-command-r-v01,hendrycksTest-computer_security,5-shot,acc_norm,0.78,0.04163331998932263
CohereForAI/c4ai-command-r-v01,hendrycksTest-conceptual_physics,5-shot,accuracy,0.6042553191489362,0.03196758697835363
CohereForAI/c4ai-command-r-v01,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.6042553191489362,0.03196758697835363
CohereForAI/c4ai-command-r-v01,hendrycksTest-econometrics,5-shot,accuracy,0.5614035087719298,0.04668000738510455
CohereForAI/c4ai-command-r-v01,hendrycksTest-econometrics,5-shot,acc_norm,0.5614035087719298,0.04668000738510455
CohereForAI/c4ai-command-r-v01,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5793103448275863,0.0411391498118926
CohereForAI/c4ai-command-r-v01,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5793103448275863,0.0411391498118926
CohereForAI/c4ai-command-r-v01,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4656084656084656,0.025690321762493855
CohereForAI/c4ai-command-r-v01,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4656084656084656,0.025690321762493855
CohereForAI/c4ai-command-r-v01,hendrycksTest-formal_logic,5-shot,accuracy,0.4603174603174603,0.04458029125470973
CohereForAI/c4ai-command-r-v01,hendrycksTest-formal_logic,5-shot,acc_norm,0.4603174603174603,0.04458029125470973
CohereForAI/c4ai-command-r-v01,hendrycksTest-global_facts,5-shot,accuracy,0.46,0.05009082659620332
CohereForAI/c4ai-command-r-v01,hendrycksTest-global_facts,5-shot,acc_norm,0.46,0.05009082659620332
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_biology,5-shot,accuracy,0.7741935483870968,0.023785577884181012
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7741935483870968,0.023785577884181012
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.5270935960591133,0.03512819077876106
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.5270935960591133,0.03512819077876106
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.73,0.044619604333847394
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.73,0.044619604333847394
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8121212121212121,0.03050193405942914
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8121212121212121,0.03050193405942914
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_geography,5-shot,accuracy,0.8181818181818182,0.027479603010538787
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8181818181818182,0.027479603010538787
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9119170984455959,0.02045374660160103
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9119170984455959,0.02045374660160103
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6615384615384615,0.023991500500313033
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6615384615384615,0.023991500500313033
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.31851851851851853,0.02840653309060846
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.31851851851851853,0.02840653309060846
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.7226890756302521,0.029079374539480007
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.7226890756302521,0.029079374539480007
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_physics,5-shot,accuracy,0.3841059602649007,0.03971301814719197
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3841059602649007,0.03971301814719197
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8440366972477065,0.015555802713590179
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8440366972477065,0.015555802713590179
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5694444444444444,0.03376922151252336
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5694444444444444,0.03376922151252336
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8382352941176471,0.025845017986926917
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8382352941176471,0.025845017986926917
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_world_history,5-shot,accuracy,0.869198312236287,0.021948766059470767
CohereForAI/c4ai-command-r-v01,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.869198312236287,0.021948766059470767
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_aging,5-shot,accuracy,0.7757847533632287,0.02799153425851952
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_aging,5-shot,acc_norm,0.7757847533632287,0.02799153425851952
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_sexuality,5-shot,accuracy,0.8244274809160306,0.03336820338476075
CohereForAI/c4ai-command-r-v01,hendrycksTest-human_sexuality,5-shot,acc_norm,0.8244274809160306,0.03336820338476075
CohereForAI/c4ai-command-r-v01,hendrycksTest-international_law,5-shot,accuracy,0.859504132231405,0.03172233426002158
CohereForAI/c4ai-command-r-v01,hendrycksTest-international_law,5-shot,acc_norm,0.859504132231405,0.03172233426002158
CohereForAI/c4ai-command-r-v01,hendrycksTest-jurisprudence,5-shot,accuracy,0.8518518518518519,0.03434300243631
CohereForAI/c4ai-command-r-v01,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8518518518518519,0.03434300243631
CohereForAI/c4ai-command-r-v01,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8404907975460123,0.02876748172598386
CohereForAI/c4ai-command-r-v01,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8404907975460123,0.02876748172598386
CohereForAI/c4ai-command-r-v01,hendrycksTest-machine_learning,5-shot,accuracy,0.4732142857142857,0.04738975119274155
CohereForAI/c4ai-command-r-v01,hendrycksTest-machine_learning,5-shot,acc_norm,0.4732142857142857,0.04738975119274155
CohereForAI/c4ai-command-r-v01,hendrycksTest-management,5-shot,accuracy,0.7961165048543689,0.039891398595317706
CohereForAI/c4ai-command-r-v01,hendrycksTest-management,5-shot,acc_norm,0.7961165048543689,0.039891398595317706
CohereForAI/c4ai-command-r-v01,hendrycksTest-marketing,5-shot,accuracy,0.905982905982906,0.019119892798924985
CohereForAI/c4ai-command-r-v01,hendrycksTest-marketing,5-shot,acc_norm,0.905982905982906,0.019119892798924985
CohereForAI/c4ai-command-r-v01,hendrycksTest-medical_genetics,5-shot,accuracy,0.79,0.040936018074033256
CohereForAI/c4ai-command-r-v01,hendrycksTest-medical_genetics,5-shot,acc_norm,0.79,0.040936018074033256
CohereForAI/c4ai-command-r-v01,hendrycksTest-miscellaneous,5-shot,accuracy,0.859514687100894,0.012426211353093434
CohereForAI/c4ai-command-r-v01,hendrycksTest-miscellaneous,5-shot,acc_norm,0.859514687100894,0.012426211353093434
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_disputes,5-shot,accuracy,0.7196531791907514,0.02418242749657761
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7196531791907514,0.02418242749657761
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_scenarios,5-shot,accuracy,0.5720670391061452,0.016547887997416112
CohereForAI/c4ai-command-r-v01,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.5720670391061452,0.016547887997416112
CohereForAI/c4ai-command-r-v01,hendrycksTest-nutrition,5-shot,accuracy,0.7222222222222222,0.025646863097137904
CohereForAI/c4ai-command-r-v01,hendrycksTest-nutrition,5-shot,acc_norm,0.7222222222222222,0.025646863097137904
CohereForAI/c4ai-command-r-v01,hendrycksTest-philosophy,5-shot,accuracy,0.7491961414790996,0.024619771956697168
CohereForAI/c4ai-command-r-v01,hendrycksTest-philosophy,5-shot,acc_norm,0.7491961414790996,0.024619771956697168
CohereForAI/c4ai-command-r-v01,hendrycksTest-prehistory,5-shot,accuracy,0.7808641975308642,0.02301670564026219
CohereForAI/c4ai-command-r-v01,hendrycksTest-prehistory,5-shot,acc_norm,0.7808641975308642,0.02301670564026219
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_accounting,5-shot,accuracy,0.5390070921985816,0.02973659252642444
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_accounting,5-shot,acc_norm,0.5390070921985816,0.02973659252642444
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_law,5-shot,accuracy,0.5508474576271186,0.012704030518851477
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_law,5-shot,acc_norm,0.5508474576271186,0.012704030518851477
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_medicine,5-shot,accuracy,0.6838235294117647,0.02824568739146293
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_medicine,5-shot,acc_norm,0.6838235294117647,0.02824568739146293
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_psychology,5-shot,accuracy,0.7238562091503268,0.018087276935663133
CohereForAI/c4ai-command-r-v01,hendrycksTest-professional_psychology,5-shot,acc_norm,0.7238562091503268,0.018087276935663133
CohereForAI/c4ai-command-r-v01,hendrycksTest-public_relations,5-shot,accuracy,0.7363636363636363,0.04220224692971987
CohereForAI/c4ai-command-r-v01,hendrycksTest-public_relations,5-shot,acc_norm,0.7363636363636363,0.04220224692971987
CohereForAI/c4ai-command-r-v01,hendrycksTest-security_studies,5-shot,accuracy,0.763265306122449,0.02721283588407316
CohereForAI/c4ai-command-r-v01,hendrycksTest-security_studies,5-shot,acc_norm,0.763265306122449,0.02721283588407316
CohereForAI/c4ai-command-r-v01,hendrycksTest-sociology,5-shot,accuracy,0.8805970149253731,0.02292879327721974
CohereForAI/c4ai-command-r-v01,hendrycksTest-sociology,5-shot,acc_norm,0.8805970149253731,0.02292879327721974
CohereForAI/c4ai-command-r-v01,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.03487350880197769
CohereForAI/c4ai-command-r-v01,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.03487350880197769
CohereForAI/c4ai-command-r-v01,hendrycksTest-virology,5-shot,accuracy,0.5301204819277109,0.03885425420866767
CohereForAI/c4ai-command-r-v01,hendrycksTest-virology,5-shot,acc_norm,0.5301204819277109,0.03885425420866767
CohereForAI/c4ai-command-r-v01,hendrycksTest-world_religions,5-shot,accuracy,0.847953216374269,0.027539122889061452
CohereForAI/c4ai-command-r-v01,hendrycksTest-world_religions,5-shot,acc_norm,0.847953216374269,0.027539122889061452
CohereForAI/c4ai-command-r-v01,truthfulqa:mc,0-shot,mc1,0.3635250917992656,0.01683886288396583
CohereForAI/c4ai-command-r-v01,truthfulqa:mc,0-shot,mc2,0.5231991183271746,0.015493018963817363
CohereForAI/c4ai-command-r-v01,winogrande,5-shot,accuracy,0.8153117600631413,0.010905978112156883
CohereForAI/c4ai-command-r-v01,gsm8k,5-shot,accuracy,0.5663381349507203,0.013650728047064692
facebook/xglm-564M,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_prealgebra,5-shot,accuracy,0.001148105625717566,0.0011481056257175673
facebook/xglm-564M,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-564M,minerva_math_algebra,5-shot,accuracy,0.0008424599831508003,0.0008424599831507863
facebook/xglm-564M,fld_default,0-shot,accuracy,0.0,
facebook/xglm-564M,fld_star,0-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
facebook/xglm-564M,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005000000000000116
facebook/xglm-564M,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
facebook/xglm-564M,arithmetic_2ds,5-shot,accuracy,0.008,0.001992482118488464
facebook/xglm-564M,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339569
facebook/xglm-564M,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-564M,arithmetic_2dm,5-shot,accuracy,0.0185,0.0030138707185866456
facebook/xglm-564M,arithmetic_2da,5-shot,accuracy,0.0065,0.0017973564602277766
facebook/xglm-564M,gsm8k_cot,5-shot,accuracy,0.02122820318423048,0.003970449129848635
facebook/xglm-564M,anli_r2,0-shot,brier_score,0.8949833517502548,
facebook/xglm-564M,anli_r3,0-shot,brier_score,0.8360668742052519,
facebook/xglm-564M,anli_r1,0-shot,brier_score,0.8884687642270159,
facebook/xglm-564M,xnli_eu,0-shot,brier_score,0.7371713039069413,
facebook/xglm-564M,xnli_vi,0-shot,brier_score,0.8654369273137161,
facebook/xglm-564M,xnli_ru,0-shot,brier_score,0.7606713225425753,
facebook/xglm-564M,xnli_zh,0-shot,brier_score,1.0486949265688723,
facebook/xglm-564M,xnli_tr,0-shot,brier_score,0.8214109452628465,
facebook/xglm-564M,xnli_fr,0-shot,brier_score,0.7815638841837665,
facebook/xglm-564M,xnli_en,0-shot,brier_score,0.7112428085311449,
facebook/xglm-564M,xnli_ur,0-shot,brier_score,1.062454393810576,
facebook/xglm-564M,xnli_ar,0-shot,brier_score,1.296895724941598,
facebook/xglm-564M,xnli_de,0-shot,brier_score,0.8351648045281843,
facebook/xglm-564M,xnli_hi,0-shot,brier_score,0.8394046981053793,
facebook/xglm-564M,xnli_es,0-shot,brier_score,0.8566611033916195,
facebook/xglm-564M,xnli_bg,0-shot,brier_score,0.7676392908318753,
facebook/xglm-564M,xnli_sw,0-shot,brier_score,0.8344826636346988,
facebook/xglm-564M,xnli_el,0-shot,brier_score,0.8773823206071238,
facebook/xglm-564M,xnli_th,0-shot,brier_score,0.8581311641212404,
facebook/xglm-564M,logiqa2,0-shot,brier_score,1.1939318152210763,
facebook/xglm-564M,mathqa,5-shot,brier_score,1.0334681136329835,
facebook/xglm-564M,lambada_standard,0-shot,perplexity,42.939212075145306,1.641006871749824
facebook/xglm-564M,lambada_standard,0-shot,accuracy,0.3291286629148069,0.0065465809755531025
facebook/xglm-564M,lambada_openai,0-shot,perplexity,28.56367062584693,1.0264515146295097
facebook/xglm-564M,lambada_openai,0-shot,accuracy,0.3584319813700757,0.006680928173680378
facebook/xglm-564M,mmlu_world_religions,0-shot,accuracy,0.22807017543859648,0.032180937956023566
facebook/xglm-564M,mmlu_formal_logic,0-shot,accuracy,0.14285714285714285,0.03129843185743809
facebook/xglm-564M,mmlu_prehistory,0-shot,accuracy,0.21604938271604937,0.02289916291844581
facebook/xglm-564M,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217892
facebook/xglm-564M,mmlu_high_school_world_history,0-shot,accuracy,0.2489451476793249,0.028146970599422644
facebook/xglm-564M,mmlu_moral_disputes,0-shot,accuracy,0.24855491329479767,0.023267528432100174
facebook/xglm-564M,mmlu_professional_law,0-shot,accuracy,0.2457627118644068,0.01099615663514269
facebook/xglm-564M,mmlu_logical_fallacies,0-shot,accuracy,0.26380368098159507,0.03462419931615624
facebook/xglm-564M,mmlu_high_school_us_history,0-shot,accuracy,0.27941176470588236,0.031493281045079556
facebook/xglm-564M,mmlu_philosophy,0-shot,accuracy,0.1832797427652733,0.021974198848265812
facebook/xglm-564M,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094631
facebook/xglm-564M,mmlu_international_law,0-shot,accuracy,0.32231404958677684,0.042664163633521664
facebook/xglm-564M,mmlu_high_school_european_history,0-shot,accuracy,0.26666666666666666,0.03453131801885415
facebook/xglm-564M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2694300518134715,0.03201867122877794
facebook/xglm-564M,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.02755361446786379
facebook/xglm-564M,mmlu_high_school_geography,0-shot,accuracy,0.18686868686868688,0.02777253333421898
facebook/xglm-564M,mmlu_high_school_psychology,0-shot,accuracy,0.1926605504587156,0.016909276884936087
facebook/xglm-564M,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
facebook/xglm-564M,mmlu_us_foreign_policy,0-shot,accuracy,0.23,0.04229525846816507
facebook/xglm-564M,mmlu_sociology,0-shot,accuracy,0.22885572139303484,0.029705284056772436
facebook/xglm-564M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2076923076923077,0.0205675395672468
facebook/xglm-564M,mmlu_security_studies,0-shot,accuracy,0.2612244897959184,0.028123429335142787
facebook/xglm-564M,mmlu_professional_psychology,0-shot,accuracy,0.25,0.01751781884501444
facebook/xglm-564M,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
facebook/xglm-564M,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.04185774424022056
facebook/xglm-564M,mmlu_miscellaneous,0-shot,accuracy,0.23754789272030652,0.015218733046150191
facebook/xglm-564M,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.02987257770889115
facebook/xglm-564M,mmlu_management,0-shot,accuracy,0.18446601941747573,0.03840423627288276
facebook/xglm-564M,mmlu_nutrition,0-shot,accuracy,0.21895424836601307,0.02367908986180772
facebook/xglm-564M,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
facebook/xglm-564M,mmlu_human_aging,0-shot,accuracy,0.33183856502242154,0.03160295143776679
facebook/xglm-564M,mmlu_professional_medicine,0-shot,accuracy,0.44485294117647056,0.030187532060329383
facebook/xglm-564M,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.03126511206173042
facebook/xglm-564M,mmlu_business_ethics,0-shot,accuracy,0.28,0.045126085985421296
facebook/xglm-564M,mmlu_clinical_knowledge,0-shot,accuracy,0.2037735849056604,0.024790784501775406
facebook/xglm-564M,mmlu_global_facts,0-shot,accuracy,0.18,0.03861229196653694
facebook/xglm-564M,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370519
facebook/xglm-564M,mmlu_professional_accounting,0-shot,accuracy,0.2801418439716312,0.026789172351140245
facebook/xglm-564M,mmlu_college_physics,0-shot,accuracy,0.23529411764705882,0.04220773659171453
facebook/xglm-564M,mmlu_high_school_physics,0-shot,accuracy,0.2052980132450331,0.03297986648473835
facebook/xglm-564M,mmlu_high_school_biology,0-shot,accuracy,0.2709677419354839,0.02528441611490016
facebook/xglm-564M,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.03476590104304134
facebook/xglm-564M,mmlu_anatomy,0-shot,accuracy,0.2962962962962963,0.03944624162501116
facebook/xglm-564M,mmlu_college_chemistry,0-shot,accuracy,0.2,0.04020151261036845
facebook/xglm-564M,mmlu_computer_security,0-shot,accuracy,0.25,0.04351941398892446
facebook/xglm-564M,mmlu_college_computer_science,0-shot,accuracy,0.16,0.036845294917747094
facebook/xglm-564M,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123387
facebook/xglm-564M,mmlu_college_mathematics,0-shot,accuracy,0.24,0.04292346959909282
facebook/xglm-564M,mmlu_conceptual_physics,0-shot,accuracy,0.30638297872340425,0.03013590647851756
facebook/xglm-564M,mmlu_abstract_algebra,0-shot,accuracy,0.2,0.040201512610368445
facebook/xglm-564M,mmlu_high_school_computer_science,0-shot,accuracy,0.31,0.046482319871173156
facebook/xglm-564M,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.04493949068613539
facebook/xglm-564M,mmlu_high_school_chemistry,0-shot,accuracy,0.2955665024630542,0.032104944337514575
facebook/xglm-564M,mmlu_high_school_statistics,0-shot,accuracy,0.4583333333333333,0.03398110890294636
facebook/xglm-564M,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.022494510767503154
facebook/xglm-564M,mmlu_electrical_engineering,0-shot,accuracy,0.25517241379310346,0.03632984052707842
facebook/xglm-564M,mmlu_high_school_mathematics,0-shot,accuracy,0.23703703703703705,0.02592887613276614
facebook/xglm-564M,arc_challenge,25-shot,accuracy,0.20648464163822525,0.011828865619002316
facebook/xglm-564M,arc_challenge,25-shot,acc_norm,0.25426621160409557,0.012724999945157732
facebook/xglm-564M,truthfulqa_mc2,0-shot,accuracy,0.4037316135738844,0.014865718962222852
facebook/xglm-564M,truthfulqa_gen,0-shot,bleu_max,1.3466045659305554,0.06990039606064374
facebook/xglm-564M,truthfulqa_gen,0-shot,bleu_acc,0.32802937576499386,0.016435632932815022
facebook/xglm-564M,truthfulqa_gen,0-shot,bleu_diff,-0.24132198144345063,0.04226398695222224
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge1_max,6.215723953891085,0.1887917253922373
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge1_acc,0.3561811505507956,0.016763790728446325
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge1_diff,-0.5530378301879756,0.10492155060893699
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge2_max,3.0775068077414733,0.1443089347191126
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge2_acc,0.23990208078335373,0.01494881267906214
facebook/xglm-564M,truthfulqa_gen,0-shot,rouge2_diff,-0.6499181850303296,0.0934320138734598
facebook/xglm-564M,truthfulqa_gen,0-shot,rougeL_max,5.696483921597016,0.1816196852303411
facebook/xglm-564M,truthfulqa_gen,0-shot,rougeL_acc,0.3463892288861689,0.016656997109125136
facebook/xglm-564M,truthfulqa_gen,0-shot,rougeL_diff,-0.5754493671860949,0.10036316110705906
facebook/xglm-564M,truthfulqa_mc1,0-shot,accuracy,0.22888616891064872,0.014706994909055027
Salesforce/codegen-350M-multi,minerva_math_precalc,5-shot,accuracy,0.01098901098901099,0.004465618427331418
Salesforce/codegen-350M-multi,minerva_math_prealgebra,5-shot,accuracy,0.01951779563719862,0.004690029935284569
Salesforce/codegen-350M-multi,minerva_math_num_theory,5-shot,accuracy,0.005555555555555556,0.00320154512732093
Salesforce/codegen-350M-multi,minerva_math_intermediate_algebra,5-shot,accuracy,0.012181616832779624,0.0036524791938863663
Salesforce/codegen-350M-multi,minerva_math_geometry,5-shot,accuracy,0.0020876826722338203,0.0020876826722338333
Salesforce/codegen-350M-multi,minerva_math_counting_and_prob,5-shot,accuracy,0.008438818565400843,0.004206007207713054
Salesforce/codegen-350M-multi,minerva_math_algebra,5-shot,accuracy,0.014321819713563605,0.003450041570937017
Salesforce/codegen-350M-multi,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521454
Salesforce/codegen-350M-multi,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339515
Salesforce/codegen-350M-multi,arithmetic_4da,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_2ds,5-shot,accuracy,0.0015,0.0008655920660521503
Salesforce/codegen-350M-multi,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_1dc,5-shot,accuracy,0.029,0.0037532044004605115
Salesforce/codegen-350M-multi,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-350M-multi,arithmetic_2dm,5-shot,accuracy,0.016,0.002806410156941532
Salesforce/codegen-350M-multi,arithmetic_2da,5-shot,accuracy,0.004,0.0014117352790977004
Salesforce/codegen-350M-multi,gsm8k_cot,5-shot,accuracy,0.022744503411675512,0.004106620637749713
Salesforce/codegen-350M-multi,gsm8k,5-shot,accuracy,0.02047005307050796,0.003900413385915717
Salesforce/codegen-350M-multi,anli_r2,0-shot,brier_score,0.985719346196084,
Salesforce/codegen-350M-multi,anli_r3,0-shot,brier_score,0.9732009467249916,
Salesforce/codegen-350M-multi,anli_r1,0-shot,brier_score,0.9985649407847307,
Salesforce/codegen-350M-multi,xnli_eu,0-shot,brier_score,1.2461840807929438,
Salesforce/codegen-350M-multi,xnli_vi,0-shot,brier_score,1.1678074729947867,
Salesforce/codegen-350M-multi,xnli_ru,0-shot,brier_score,0.9554534962904667,
Salesforce/codegen-350M-multi,xnli_zh,0-shot,brier_score,1.0339034216345016,
Salesforce/codegen-350M-multi,xnli_tr,0-shot,brier_score,1.255509308223099,
Salesforce/codegen-350M-multi,xnli_fr,0-shot,brier_score,1.2357496951451048,
Salesforce/codegen-350M-multi,xnli_en,0-shot,brier_score,0.8204070568367904,
Salesforce/codegen-350M-multi,xnli_ur,0-shot,brier_score,1.2449382078214195,
Salesforce/codegen-350M-multi,xnli_ar,0-shot,brier_score,1.026144943391084,
Salesforce/codegen-350M-multi,xnli_de,0-shot,brier_score,0.9722273139773239,
Salesforce/codegen-350M-multi,xnli_hi,0-shot,brier_score,1.0013653907261761,
Salesforce/codegen-350M-multi,xnli_es,0-shot,brier_score,1.1864389653417817,
Salesforce/codegen-350M-multi,xnli_bg,0-shot,brier_score,0.9590675242887678,
Salesforce/codegen-350M-multi,xnli_sw,0-shot,brier_score,1.087228705558193,
Salesforce/codegen-350M-multi,xnli_el,0-shot,brier_score,1.107009491211521,
Salesforce/codegen-350M-multi,xnli_th,0-shot,brier_score,1.0205217980811283,
Salesforce/codegen-350M-multi,logiqa2,0-shot,brier_score,1.1839026222831999,
Salesforce/codegen-350M-multi,mathqa,5-shot,brier_score,1.0109975167317484,
Salesforce/codegen-350M-multi,lambada_standard,0-shot,perplexity,929.8432444456479,44.08843957528335
Salesforce/codegen-350M-multi,lambada_standard,0-shot,accuracy,0.12012419949543955,0.004529372226920103
Salesforce/codegen-350M-multi,lambada_openai,0-shot,perplexity,1118.2514080440103,58.16346545779122
Salesforce/codegen-350M-multi,lambada_openai,0-shot,accuracy,0.12652823597904134,0.004631591355662349
Salesforce/codegen-350M-multi,mmlu_world_religions,0-shot,accuracy,0.25146198830409355,0.033275044238468436
Salesforce/codegen-350M-multi,mmlu_formal_logic,0-shot,accuracy,0.18253968253968253,0.03455071019102147
Salesforce/codegen-350M-multi,mmlu_prehistory,0-shot,accuracy,0.22839506172839505,0.023358211840626267
Salesforce/codegen-350M-multi,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217889
Salesforce/codegen-350M-multi,mmlu_high_school_world_history,0-shot,accuracy,0.2911392405063291,0.029571601065753374
Salesforce/codegen-350M-multi,mmlu_moral_disputes,0-shot,accuracy,0.23410404624277456,0.022797110278071134
Salesforce/codegen-350M-multi,mmlu_professional_law,0-shot,accuracy,0.2470664928292047,0.011015752255279329
Salesforce/codegen-350M-multi,mmlu_logical_fallacies,0-shot,accuracy,0.2883435582822086,0.035590395316173425
Salesforce/codegen-350M-multi,mmlu_high_school_us_history,0-shot,accuracy,0.27450980392156865,0.03132179803083289
Salesforce/codegen-350M-multi,mmlu_philosophy,0-shot,accuracy,0.2572347266881029,0.024826171289250888
Salesforce/codegen-350M-multi,mmlu_jurisprudence,0-shot,accuracy,0.23148148148148148,0.04077494709252626
Salesforce/codegen-350M-multi,mmlu_international_law,0-shot,accuracy,0.2809917355371901,0.04103203830514512
Salesforce/codegen-350M-multi,mmlu_high_school_european_history,0-shot,accuracy,0.2909090909090909,0.03546563019624336
Salesforce/codegen-350M-multi,mmlu_high_school_government_and_politics,0-shot,accuracy,0.36787564766839376,0.034801756684660366
Salesforce/codegen-350M-multi,mmlu_high_school_microeconomics,0-shot,accuracy,0.21428571428571427,0.026653531596715473
Salesforce/codegen-350M-multi,mmlu_high_school_geography,0-shot,accuracy,0.3484848484848485,0.033948539651564025
Salesforce/codegen-350M-multi,mmlu_high_school_psychology,0-shot,accuracy,0.30642201834862387,0.019765517220458523
Salesforce/codegen-350M-multi,mmlu_public_relations,0-shot,accuracy,0.14545454545454545,0.0337689831983308
Salesforce/codegen-350M-multi,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
Salesforce/codegen-350M-multi,mmlu_sociology,0-shot,accuracy,0.23383084577114427,0.029929415408348384
Salesforce/codegen-350M-multi,mmlu_high_school_macroeconomics,0-shot,accuracy,0.32564102564102565,0.02375966576741229
Salesforce/codegen-350M-multi,mmlu_security_studies,0-shot,accuracy,0.30612244897959184,0.02950489645459596
Salesforce/codegen-350M-multi,mmlu_professional_psychology,0-shot,accuracy,0.2549019607843137,0.017630827375148383
Salesforce/codegen-350M-multi,mmlu_human_sexuality,0-shot,accuracy,0.24427480916030533,0.03768335959728743
Salesforce/codegen-350M-multi,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813344
Salesforce/codegen-350M-multi,mmlu_miscellaneous,0-shot,accuracy,0.24521072796934865,0.01538435228454394
Salesforce/codegen-350M-multi,mmlu_marketing,0-shot,accuracy,0.19658119658119658,0.02603538609895129
Salesforce/codegen-350M-multi,mmlu_management,0-shot,accuracy,0.2621359223300971,0.04354631077260595
Salesforce/codegen-350M-multi,mmlu_nutrition,0-shot,accuracy,0.26143790849673204,0.025160998214292456
Salesforce/codegen-350M-multi,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
Salesforce/codegen-350M-multi,mmlu_human_aging,0-shot,accuracy,0.2825112107623318,0.030216831011508755
Salesforce/codegen-350M-multi,mmlu_professional_medicine,0-shot,accuracy,0.4522058823529412,0.030233758551596445
Salesforce/codegen-350M-multi,mmlu_college_medicine,0-shot,accuracy,0.27167630057803466,0.03391750322321659
Salesforce/codegen-350M-multi,mmlu_business_ethics,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-350M-multi,mmlu_clinical_knowledge,0-shot,accuracy,0.2830188679245283,0.027724236492700907
Salesforce/codegen-350M-multi,mmlu_global_facts,0-shot,accuracy,0.16,0.036845294917747094
Salesforce/codegen-350M-multi,mmlu_virology,0-shot,accuracy,0.1686746987951807,0.029152009627856544
Salesforce/codegen-350M-multi,mmlu_professional_accounting,0-shot,accuracy,0.22695035460992907,0.024987106365642976
Salesforce/codegen-350M-multi,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.04158307533083286
Salesforce/codegen-350M-multi,mmlu_high_school_physics,0-shot,accuracy,0.2781456953642384,0.03658603262763743
Salesforce/codegen-350M-multi,mmlu_high_school_biology,0-shot,accuracy,0.27419354838709675,0.0253781399708852
Salesforce/codegen-350M-multi,mmlu_college_biology,0-shot,accuracy,0.20833333333333334,0.033961162058453336
Salesforce/codegen-350M-multi,mmlu_anatomy,0-shot,accuracy,0.2518518518518518,0.03749850709174023
Salesforce/codegen-350M-multi,mmlu_college_chemistry,0-shot,accuracy,0.29,0.045604802157206845
Salesforce/codegen-350M-multi,mmlu_computer_security,0-shot,accuracy,0.23,0.04229525846816506
Salesforce/codegen-350M-multi,mmlu_college_computer_science,0-shot,accuracy,0.29,0.045604802157206845
Salesforce/codegen-350M-multi,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123394
Salesforce/codegen-350M-multi,mmlu_college_mathematics,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-350M-multi,mmlu_conceptual_physics,0-shot,accuracy,0.2425531914893617,0.028020226271200217
Salesforce/codegen-350M-multi,mmlu_abstract_algebra,0-shot,accuracy,0.23,0.042295258468165065
Salesforce/codegen-350M-multi,mmlu_high_school_computer_science,0-shot,accuracy,0.19,0.03942772444036623
Salesforce/codegen-350M-multi,mmlu_machine_learning,0-shot,accuracy,0.33035714285714285,0.04464285714285712
Salesforce/codegen-350M-multi,mmlu_high_school_chemistry,0-shot,accuracy,0.21674876847290642,0.028990331252516235
Salesforce/codegen-350M-multi,mmlu_high_school_statistics,0-shot,accuracy,0.46296296296296297,0.034006036255382704
Salesforce/codegen-350M-multi,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.022494510767503154
Salesforce/codegen-350M-multi,mmlu_electrical_engineering,0-shot,accuracy,0.27586206896551724,0.037245636197746304
Salesforce/codegen-350M-multi,mmlu_high_school_mathematics,0-shot,accuracy,0.2814814814814815,0.02742001935094527
Salesforce/codegen-350M-multi,arc_challenge,25-shot,accuracy,0.17320819112627986,0.011058694183280328
Salesforce/codegen-350M-multi,arc_challenge,25-shot,acc_norm,0.21928327645051193,0.012091245787615716
Salesforce/codegen-350M-multi,hellaswag,10-shot,accuracy,0.27454690300736906,0.004453735900947804
Salesforce/codegen-350M-multi,hellaswag,10-shot,acc_norm,0.29356701852220674,0.0045446519760401006
Salesforce/codegen-350M-multi,truthfulqa_mc2,0-shot,accuracy,0.46878684418952904,0.015919757808354366
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,bleu_max,14.843134516314572,0.5728559782867888
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,bleu_acc,0.2864137086903305,0.01582614243950239
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,bleu_diff,-2.2717050031248,0.43331168281195936
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge1_max,35.03124298423832,0.7866389432856384
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge1_acc,0.28886168910648713,0.015866346401384315
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge1_diff,-4.893306750479266,0.5725461641851453
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge2_max,19.758755174632952,0.79627641363227
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge2_acc,0.20685434516523868,0.014179591496728337
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rouge2_diff,-4.2748721462575014,0.5919097348223242
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rougeL_max,32.14833428429696,0.7758150933271327
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rougeL_acc,0.2668298653610771,0.015483691939237277
Salesforce/codegen-350M-multi,truthfulqa_gen,0-shot,rougeL_diff,-4.921493483348142,0.5510430423849421
Salesforce/codegen-350M-multi,truthfulqa_mc1,0-shot,accuracy,0.27539779681762544,0.01563813566777552
Salesforce/codegen-350M-multi,winogrande,5-shot,accuracy,0.516179952644041,0.0140451261309786
mistral-community/Mixtral-8x22B-v0.1,arc:challenge,25-shot,accuracy,0.6646757679180887,0.013796182947785562
mistral-community/Mixtral-8x22B-v0.1,arc:challenge,25-shot,acc_norm,0.7047781569965871,0.013329750293382316
mistral-community/Mixtral-8x22B-v0.1,hellaswag,10-shot,accuracy,0.7050388368850826,0.004550933142528781
mistral-community/Mixtral-8x22B-v0.1,hellaswag,10-shot,acc_norm,0.8872734515036845,0.003156118964752945
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.5,0.050251890762960605
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.5,0.050251890762960605
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-anatomy,5-shot,accuracy,0.762962962962963,0.03673731683969506
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-anatomy,5-shot,acc_norm,0.762962962962963,0.03673731683969506
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-astronomy,5-shot,accuracy,0.881578947368421,0.026293995855474924
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-astronomy,5-shot,acc_norm,0.881578947368421,0.026293995855474924
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-business_ethics,5-shot,accuracy,0.73,0.044619604333847394
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-business_ethics,5-shot,acc_norm,0.73,0.044619604333847394
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.8264150943396227,0.02331058302600625
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.8264150943396227,0.02331058302600625
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_biology,5-shot,accuracy,0.8819444444444444,0.026983346503309368
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_biology,5-shot,acc_norm,0.8819444444444444,0.026983346503309368
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_chemistry,5-shot,accuracy,0.6,0.049236596391733084
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.6,0.049236596391733084
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_computer_science,5-shot,accuracy,0.72,0.045126085985421255
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.72,0.045126085985421255
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_mathematics,5-shot,accuracy,0.48,0.050211673156867795
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.48,0.050211673156867795
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_medicine,5-shot,accuracy,0.7861271676300579,0.03126511206173044
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_medicine,5-shot,acc_norm,0.7861271676300579,0.03126511206173044
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_physics,5-shot,accuracy,0.5392156862745098,0.04959859966384181
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-college_physics,5-shot,acc_norm,0.5392156862745098,0.04959859966384181
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-computer_security,5-shot,accuracy,0.82,0.03861229196653694
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-computer_security,5-shot,acc_norm,0.82,0.03861229196653694
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.8085106382978723,0.02572214999263779
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.8085106382978723,0.02572214999263779
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-econometrics,5-shot,accuracy,0.6842105263157895,0.04372748290278007
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-econometrics,5-shot,acc_norm,0.6842105263157895,0.04372748290278007
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.7724137931034483,0.034939503801311826
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.7724137931034483,0.034939503801311826
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.6190476190476191,0.025010749116137595
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.6190476190476191,0.025010749116137595
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-formal_logic,5-shot,accuracy,0.6031746031746031,0.043758884927270585
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-formal_logic,5-shot,acc_norm,0.6031746031746031,0.043758884927270585
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-global_facts,5-shot,accuracy,0.55,0.05
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-global_facts,5-shot,acc_norm,0.55,0.05
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_biology,5-shot,accuracy,0.9129032258064517,0.01604110074169669
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.9129032258064517,0.01604110074169669
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.6699507389162561,0.03308530426228258
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.6699507389162561,0.03308530426228258
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.87,0.03379976689896309
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.87,0.03379976689896309
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.8545454545454545,0.027530196355066584
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.8545454545454545,0.027530196355066584
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_geography,5-shot,accuracy,0.9141414141414141,0.01996022556317289
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.9141414141414141,0.01996022556317289
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9689119170984456,0.012525310625527041
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9689119170984456,0.012525310625527041
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.8051282051282052,0.020083167595181393
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.8051282051282052,0.020083167595181393
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.45185185185185184,0.030343862998512633
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.45185185185185184,0.030343862998512633
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.8739495798319328,0.021559623121213914
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.8739495798319328,0.021559623121213914
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_physics,5-shot,accuracy,0.5562913907284768,0.04056527902281733
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.5562913907284768,0.04056527902281733
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.9247706422018349,0.011308662537571767
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.9247706422018349,0.011308662537571767
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.6898148148148148,0.03154696285656628
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.6898148148148148,0.03154696285656628
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8970588235294118,0.02132833757080438
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8970588235294118,0.02132833757080438
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.9029535864978903,0.019269323025640273
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.9029535864978903,0.019269323025640273
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_aging,5-shot,accuracy,0.7982062780269058,0.026936111912802263
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_aging,5-shot,acc_norm,0.7982062780269058,0.026936111912802263
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_sexuality,5-shot,accuracy,0.9007633587786259,0.02622223517147737
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.9007633587786259,0.02622223517147737
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-international_law,5-shot,accuracy,0.9173553719008265,0.025135382356604227
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-international_law,5-shot,acc_norm,0.9173553719008265,0.025135382356604227
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-jurisprudence,5-shot,accuracy,0.8425925925925926,0.03520703990517963
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8425925925925926,0.03520703990517963
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.8773006134969326,0.025777328426978927
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.8773006134969326,0.025777328426978927
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-machine_learning,5-shot,accuracy,0.6428571428571429,0.04547960999764376
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-machine_learning,5-shot,acc_norm,0.6428571428571429,0.04547960999764376
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-management,5-shot,accuracy,0.8737864077669902,0.03288180278808628
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-management,5-shot,acc_norm,0.8737864077669902,0.03288180278808628
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-marketing,5-shot,accuracy,0.9230769230769231,0.017456987872436186
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-marketing,5-shot,acc_norm,0.9230769230769231,0.017456987872436186
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-medical_genetics,5-shot,accuracy,0.84,0.036845294917747094
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.84,0.036845294917747094
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-miscellaneous,5-shot,accuracy,0.9029374201787995,0.01058647471201829
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.9029374201787995,0.01058647471201829
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_disputes,5-shot,accuracy,0.8294797687861272,0.020247961569303728
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.8294797687861272,0.020247961569303728
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.6547486033519553,0.015901432608930358
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.6547486033519553,0.015901432608930358
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-nutrition,5-shot,accuracy,0.8660130718954249,0.019504890618464815
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-nutrition,5-shot,acc_norm,0.8660130718954249,0.019504890618464815
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-philosophy,5-shot,accuracy,0.842443729903537,0.02069223727358399
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-philosophy,5-shot,acc_norm,0.842443729903537,0.02069223727358399
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-prehistory,5-shot,accuracy,0.8703703703703703,0.01868972572106206
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-prehistory,5-shot,acc_norm,0.8703703703703703,0.01868972572106206
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_accounting,5-shot,accuracy,0.6205673758865248,0.0289473388516141
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.6205673758865248,0.0289473388516141
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_law,5-shot,accuracy,0.6114732724902217,0.012448817838292365
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_law,5-shot,acc_norm,0.6114732724902217,0.012448817838292365
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_medicine,5-shot,accuracy,0.875,0.020089743302935947
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.875,0.020089743302935947
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_psychology,5-shot,accuracy,0.8349673202614379,0.015017550799247322
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.8349673202614379,0.015017550799247322
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-public_relations,5-shot,accuracy,0.7727272727272727,0.04013964554072775
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-public_relations,5-shot,acc_norm,0.7727272727272727,0.04013964554072775
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-security_studies,5-shot,accuracy,0.8653061224489796,0.021855658840811615
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-security_studies,5-shot,acc_norm,0.8653061224489796,0.021855658840811615
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-sociology,5-shot,accuracy,0.9203980099502488,0.019139685633503815
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-sociology,5-shot,acc_norm,0.9203980099502488,0.019139685633503815
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.94,0.023868325657594145
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.94,0.023868325657594145
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-virology,5-shot,accuracy,0.5783132530120482,0.03844453181770917
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-virology,5-shot,acc_norm,0.5783132530120482,0.03844453181770917
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-world_religions,5-shot,accuracy,0.8947368421052632,0.02353755765789256
mistral-community/Mixtral-8x22B-v0.1,hendrycksTest-world_religions,5-shot,acc_norm,0.8947368421052632,0.02353755765789256
mistral-community/Mixtral-8x22B-v0.1,truthfulqa:mc,0-shot,mc1,0.33414932680538556,0.016512530677150535
mistral-community/Mixtral-8x22B-v0.1,truthfulqa:mc,0-shot,mc2,0.5108062819806165,0.014560943713053241
mistral-community/Mixtral-8x22B-v0.1,winogrande,5-shot,accuracy,0.8453038674033149,0.010163172650433538
mistral-community/Mixtral-8x22B-v0.1,gsm8k,5-shot,accuracy,0.7414708112206216,0.012059911372516132
meta-llama/Llama-2-7b-chat-hf,drop,3-shot,accuracy,0.06763842281879194,0.0025717489509556085
meta-llama/Llama-2-7b-chat-hf,drop,3-shot,f1,0.13085570469798627,0.0028825856446422905
meta-llama/Llama-2-7b-chat-hf,gsm8k,5-shot,accuracy,0.22441243366186506,0.01149161775663055
meta-llama/Llama-2-7b-chat-hf,winogrande,5-shot,accuracy,0.7237569060773481,0.012566815015698167
meta-llama/Llama-2-7b-chat-hf,mmlu_world_religions,0-shot,accuracy,0.7251461988304093,0.034240429246915824
meta-llama/Llama-2-7b-chat-hf,mmlu_formal_logic,0-shot,accuracy,0.25396825396825395,0.038932596106046734
meta-llama/Llama-2-7b-chat-hf,mmlu_prehistory,0-shot,accuracy,0.5679012345679012,0.02756301097160668
meta-llama/Llama-2-7b-chat-hf,mmlu_moral_scenarios,0-shot,accuracy,0.23128491620111732,0.014102223623152586
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_world_history,0-shot,accuracy,0.6708860759493671,0.03058732629470236
meta-llama/Llama-2-7b-chat-hf,mmlu_moral_disputes,0-shot,accuracy,0.523121387283237,0.026890297881303125
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_law,0-shot,accuracy,0.3500651890482399,0.012182552313215175
meta-llama/Llama-2-7b-chat-hf,mmlu_logical_fallacies,0-shot,accuracy,0.5398773006134969,0.03915857291436971
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_us_history,0-shot,accuracy,0.6764705882352942,0.032834720561085606
meta-llama/Llama-2-7b-chat-hf,mmlu_philosophy,0-shot,accuracy,0.5594855305466238,0.028196400574197426
meta-llama/Llama-2-7b-chat-hf,mmlu_jurisprudence,0-shot,accuracy,0.5925925925925926,0.04750077341199986
meta-llama/Llama-2-7b-chat-hf,mmlu_international_law,0-shot,accuracy,0.6363636363636364,0.04391326286724071
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_european_history,0-shot,accuracy,0.5818181818181818,0.03851716319398395
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_government_and_politics,0-shot,accuracy,0.6994818652849741,0.03308818594415749
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_microeconomics,0-shot,accuracy,0.42436974789915966,0.032104790510157764
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_geography,0-shot,accuracy,0.5909090909090909,0.035029757994130065
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_psychology,0-shot,accuracy,0.6752293577981652,0.02007772910931033
meta-llama/Llama-2-7b-chat-hf,mmlu_public_relations,0-shot,accuracy,0.5272727272727272,0.04782001791380061
meta-llama/Llama-2-7b-chat-hf,mmlu_us_foreign_policy,0-shot,accuracy,0.71,0.045604802157206845
meta-llama/Llama-2-7b-chat-hf,mmlu_sociology,0-shot,accuracy,0.6467661691542289,0.03379790611796777
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_macroeconomics,0-shot,accuracy,0.4153846153846154,0.02498535492310234
meta-llama/Llama-2-7b-chat-hf,mmlu_security_studies,0-shot,accuracy,0.5265306122448979,0.03196412734523272
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_psychology,0-shot,accuracy,0.47875816993464054,0.02020957238860025
meta-llama/Llama-2-7b-chat-hf,mmlu_human_sexuality,0-shot,accuracy,0.5725190839694656,0.04338920305792401
meta-llama/Llama-2-7b-chat-hf,mmlu_econometrics,0-shot,accuracy,0.3684210526315789,0.04537815354939391
meta-llama/Llama-2-7b-chat-hf,mmlu_miscellaneous,0-shot,accuracy,0.6781609195402298,0.016706381415057914
meta-llama/Llama-2-7b-chat-hf,mmlu_marketing,0-shot,accuracy,0.7094017094017094,0.029745048572674064
meta-llama/Llama-2-7b-chat-hf,mmlu_management,0-shot,accuracy,0.6699029126213593,0.04656147110012351
meta-llama/Llama-2-7b-chat-hf,mmlu_nutrition,0-shot,accuracy,0.5196078431372549,0.028607893699576066
meta-llama/Llama-2-7b-chat-hf,mmlu_medical_genetics,0-shot,accuracy,0.48,0.050211673156867795
meta-llama/Llama-2-7b-chat-hf,mmlu_human_aging,0-shot,accuracy,0.5739910313901345,0.0331883328621728
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_medicine,0-shot,accuracy,0.45588235294117646,0.03025437257397669
meta-llama/Llama-2-7b-chat-hf,mmlu_college_medicine,0-shot,accuracy,0.4046242774566474,0.037424611938872476
meta-llama/Llama-2-7b-chat-hf,mmlu_business_ethics,0-shot,accuracy,0.52,0.050211673156867795
meta-llama/Llama-2-7b-chat-hf,mmlu_clinical_knowledge,0-shot,accuracy,0.5433962264150943,0.03065674869673943
meta-llama/Llama-2-7b-chat-hf,mmlu_global_facts,0-shot,accuracy,0.36,0.04824181513244218
meta-llama/Llama-2-7b-chat-hf,mmlu_virology,0-shot,accuracy,0.42771084337349397,0.038515976837185335
meta-llama/Llama-2-7b-chat-hf,mmlu_professional_accounting,0-shot,accuracy,0.3617021276595745,0.02866382014719947
meta-llama/Llama-2-7b-chat-hf,mmlu_college_physics,0-shot,accuracy,0.22549019607843138,0.04158307533083286
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.03684881521389024
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_biology,0-shot,accuracy,0.5161290322580645,0.028429203176724555
meta-llama/Llama-2-7b-chat-hf,mmlu_college_biology,0-shot,accuracy,0.5138888888888888,0.04179596617581
meta-llama/Llama-2-7b-chat-hf,mmlu_anatomy,0-shot,accuracy,0.4222222222222222,0.04266763404099582
meta-llama/Llama-2-7b-chat-hf,mmlu_college_chemistry,0-shot,accuracy,0.28,0.04512608598542128
meta-llama/Llama-2-7b-chat-hf,mmlu_computer_security,0-shot,accuracy,0.58,0.04960449637488584
meta-llama/Llama-2-7b-chat-hf,mmlu_college_computer_science,0-shot,accuracy,0.38,0.048783173121456344
meta-llama/Llama-2-7b-chat-hf,mmlu_astronomy,0-shot,accuracy,0.48026315789473684,0.040657710025626036
meta-llama/Llama-2-7b-chat-hf,mmlu_college_mathematics,0-shot,accuracy,0.35,0.047937248544110196
meta-llama/Llama-2-7b-chat-hf,mmlu_conceptual_physics,0-shot,accuracy,0.41702127659574467,0.03223276266711712
meta-llama/Llama-2-7b-chat-hf,mmlu_abstract_algebra,0-shot,accuracy,0.28,0.04512608598542128
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_computer_science,0-shot,accuracy,0.41,0.049431107042371025
meta-llama/Llama-2-7b-chat-hf,mmlu_machine_learning,0-shot,accuracy,0.30357142857142855,0.04364226155841044
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_chemistry,0-shot,accuracy,0.35960591133004927,0.033764582465095665
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_statistics,0-shot,accuracy,0.33796296296296297,0.03225941352631295
meta-llama/Llama-2-7b-chat-hf,mmlu_elementary_mathematics,0-shot,accuracy,0.29894179894179895,0.02357760479165581
meta-llama/Llama-2-7b-chat-hf,mmlu_electrical_engineering,0-shot,accuracy,0.503448275862069,0.041665675771015785
meta-llama/Llama-2-7b-chat-hf,mmlu_high_school_mathematics,0-shot,accuracy,0.25555555555555554,0.026593939101844072
meta-llama/Llama-2-7b-chat-hf,arc_challenge,25-shot,accuracy,0.49658703071672355,0.014611050403244077
meta-llama/Llama-2-7b-chat-hf,arc_challenge,25-shot,acc_norm,0.5315699658703071,0.014582236460866984
meta-llama/Llama-2-7b-chat-hf,hellaswag,10-shot,accuracy,0.5947022505476997,0.004899462111832326
meta-llama/Llama-2-7b-chat-hf,hellaswag,10-shot,acc_norm,0.7894841665006971,0.004068418417275699
meta-llama/Llama-2-7b-chat-hf,truthfulqa_mc2,0-shot,accuracy,0.4531582138681175,0.01563904135331698
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,bleu_max,20.472111648556485,0.6992083497906967
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,bleu_acc,0.44920440636474906,0.017412941986115305
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,bleu_diff,-1.7469763977464083,0.6106131740110677
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge1_max,45.30661702215245,0.7994928909675085
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge1_acc,0.4455324357405141,0.01739933528014035
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge1_diff,-1.823653233932142,0.7513119253259004
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge2_max,30.171722487396277,0.8894034989138645
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge2_acc,0.3818849449204406,0.017008101939163495
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rouge2_diff,-3.252024210254133,0.8763263995994484
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rougeL_max,42.04839858066761,0.8019864423352677
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rougeL_acc,0.4467564259485924,0.017403977522557144
meta-llama/Llama-2-7b-chat-hf,truthfulqa_gen,0-shot,rougeL_diff,-2.098732465700866,0.7482280753043922
meta-llama/Llama-2-7b-chat-hf,truthfulqa_mc1,0-shot,accuracy,0.30354957160342716,0.016095884155386844
bigscience/bloom-1b7,minerva_math_precalc,5-shot,accuracy,0.007326007326007326,0.0036529080893830334
bigscience/bloom-1b7,minerva_math_prealgebra,5-shot,accuracy,0.004592422502870264,0.0022922488477038036
bigscience/bloom-1b7,minerva_math_num_theory,5-shot,accuracy,0.001851851851851852,0.0018518518518518487
bigscience/bloom-1b7,minerva_math_intermediate_algebra,5-shot,accuracy,0.0022148394241417496,0.0015652595934070718
bigscience/bloom-1b7,minerva_math_geometry,5-shot,accuracy,0.0020876826722338203,0.0020876826722338276
bigscience/bloom-1b7,minerva_math_counting_and_prob,5-shot,accuracy,0.004219409282700422,0.002980417365102052
bigscience/bloom-1b7,minerva_math_algebra,5-shot,accuracy,0.0016849199663016006,0.001190915943716835
bigscience/bloom-1b7,fld_default,0-shot,accuracy,0.0,
bigscience/bloom-1b7,fld_star,0-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
bigscience/bloom-1b7,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005000000000000116
bigscience/bloom-1b7,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
bigscience/bloom-1b7,arithmetic_2ds,5-shot,accuracy,0.0125,0.00248494717876267
bigscience/bloom-1b7,arithmetic_5ds,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_5da,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_1dc,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_4ds,5-shot,accuracy,0.0,
bigscience/bloom-1b7,arithmetic_2dm,5-shot,accuracy,0.0215,0.0032440926417928273
bigscience/bloom-1b7,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237557
bigscience/bloom-1b7,gsm8k_cot,5-shot,accuracy,0.019711902956785442,0.0038289829787357126
bigscience/bloom-1b7,anli_r2,0-shot,brier_score,1.0330875069706043,
bigscience/bloom-1b7,anli_r3,0-shot,brier_score,0.9725282585009661,
bigscience/bloom-1b7,anli_r1,0-shot,brier_score,1.060858355860431,
bigscience/bloom-1b7,xnli_eu,0-shot,brier_score,0.7072047602264854,
bigscience/bloom-1b7,xnli_vi,0-shot,brier_score,0.7456517590436983,
bigscience/bloom-1b7,xnli_ru,0-shot,brier_score,0.8780666443369725,
bigscience/bloom-1b7,xnli_zh,0-shot,brier_score,1.0595421588854064,
bigscience/bloom-1b7,xnli_tr,0-shot,brier_score,1.0231158229219457,
bigscience/bloom-1b7,xnli_fr,0-shot,brier_score,0.7775316951866434,
bigscience/bloom-1b7,xnli_en,0-shot,brier_score,0.6711576322549884,
bigscience/bloom-1b7,xnli_ur,0-shot,brier_score,0.923647671686833,
bigscience/bloom-1b7,xnli_ar,0-shot,brier_score,1.1627421056647778,
bigscience/bloom-1b7,xnli_de,0-shot,brier_score,0.8945053213391374,
bigscience/bloom-1b7,xnli_hi,0-shot,brier_score,0.7454041920812018,
bigscience/bloom-1b7,xnli_es,0-shot,brier_score,0.8212945296045504,
bigscience/bloom-1b7,xnli_bg,0-shot,brier_score,0.9352486767969873,
bigscience/bloom-1b7,xnli_sw,0-shot,brier_score,1.033987465428587,
bigscience/bloom-1b7,xnli_el,0-shot,brier_score,0.9576070804353392,
bigscience/bloom-1b7,xnli_th,0-shot,brier_score,1.311056215973815,
bigscience/bloom-1b7,logiqa2,0-shot,brier_score,1.1991580009166556,
bigscience/bloom-1b7,mathqa,5-shot,brier_score,0.9845015446410308,
bigscience/bloom-1b7,lambada_standard,0-shot,perplexity,16.696442854077176,0.5544057536741888
bigscience/bloom-1b7,lambada_standard,0-shot,accuracy,0.4449835047545119,0.006923679791679091
bigscience/bloom-1b7,lambada_openai,0-shot,perplexity,12.585182126996132,0.40089107426896314
bigscience/bloom-1b7,lambada_openai,0-shot,accuracy,0.4622549970890743,0.0069461006470815665
bigscience/bloom-1b7,mmlu_world_religions,0-shot,accuracy,0.26900584795321636,0.034010526201040905
bigscience/bloom-1b7,mmlu_formal_logic,0-shot,accuracy,0.35714285714285715,0.04285714285714281
bigscience/bloom-1b7,mmlu_prehistory,0-shot,accuracy,0.22530864197530864,0.023246202647819746
bigscience/bloom-1b7,mmlu_moral_scenarios,0-shot,accuracy,0.27262569832402234,0.014893391735249603
bigscience/bloom-1b7,mmlu_high_school_world_history,0-shot,accuracy,0.24472573839662448,0.02798569938703643
bigscience/bloom-1b7,mmlu_moral_disputes,0-shot,accuracy,0.27167630057803466,0.023948512905468365
bigscience/bloom-1b7,mmlu_professional_law,0-shot,accuracy,0.2646675358539765,0.011267332992845533
bigscience/bloom-1b7,mmlu_logical_fallacies,0-shot,accuracy,0.26993865030674846,0.034878251684978906
bigscience/bloom-1b7,mmlu_high_school_us_history,0-shot,accuracy,0.24509803921568626,0.03019028245350194
bigscience/bloom-1b7,mmlu_philosophy,0-shot,accuracy,0.2797427652733119,0.025494259350694902
bigscience/bloom-1b7,mmlu_jurisprudence,0-shot,accuracy,0.24074074074074073,0.04133119440243838
bigscience/bloom-1b7,mmlu_international_law,0-shot,accuracy,0.2231404958677686,0.03800754475228733
bigscience/bloom-1b7,mmlu_high_school_european_history,0-shot,accuracy,0.2787878787878788,0.03501438706296781
bigscience/bloom-1b7,mmlu_high_school_government_and_politics,0-shot,accuracy,0.37823834196891193,0.03499807276193338
bigscience/bloom-1b7,mmlu_high_school_microeconomics,0-shot,accuracy,0.29411764705882354,0.02959732973097809
bigscience/bloom-1b7,mmlu_high_school_geography,0-shot,accuracy,0.36363636363636365,0.03427308652999936
bigscience/bloom-1b7,mmlu_high_school_psychology,0-shot,accuracy,0.3486238532110092,0.020431254090714328
bigscience/bloom-1b7,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.04265792110940589
bigscience/bloom-1b7,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
bigscience/bloom-1b7,mmlu_sociology,0-shot,accuracy,0.26865671641791045,0.03134328358208954
bigscience/bloom-1b7,mmlu_high_school_macroeconomics,0-shot,accuracy,0.358974358974359,0.024321738484602364
bigscience/bloom-1b7,mmlu_security_studies,0-shot,accuracy,0.4,0.031362502409358936
bigscience/bloom-1b7,mmlu_professional_psychology,0-shot,accuracy,0.2908496732026144,0.018373116915903966
bigscience/bloom-1b7,mmlu_human_sexuality,0-shot,accuracy,0.22900763358778625,0.036853466317118506
bigscience/bloom-1b7,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813344
bigscience/bloom-1b7,mmlu_miscellaneous,0-shot,accuracy,0.20434227330779056,0.0144191239809319
bigscience/bloom-1b7,mmlu_marketing,0-shot,accuracy,0.23931623931623933,0.027951826808924333
bigscience/bloom-1b7,mmlu_management,0-shot,accuracy,0.3883495145631068,0.0482572933735639
bigscience/bloom-1b7,mmlu_nutrition,0-shot,accuracy,0.2777777777777778,0.02564686309713792
bigscience/bloom-1b7,mmlu_medical_genetics,0-shot,accuracy,0.25,0.04351941398892446
bigscience/bloom-1b7,mmlu_human_aging,0-shot,accuracy,0.13452914798206278,0.02290118376157557
bigscience/bloom-1b7,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.030211479609121593
bigscience/bloom-1b7,mmlu_college_medicine,0-shot,accuracy,0.24277456647398843,0.0326926380614177
bigscience/bloom-1b7,mmlu_business_ethics,0-shot,accuracy,0.21,0.040936018074033256
bigscience/bloom-1b7,mmlu_clinical_knowledge,0-shot,accuracy,0.2981132075471698,0.028152837942493875
bigscience/bloom-1b7,mmlu_global_facts,0-shot,accuracy,0.31,0.04648231987117316
bigscience/bloom-1b7,mmlu_virology,0-shot,accuracy,0.19879518072289157,0.031069390260789413
bigscience/bloom-1b7,mmlu_professional_accounting,0-shot,accuracy,0.2730496453900709,0.026577860943307854
bigscience/bloom-1b7,mmlu_college_physics,0-shot,accuracy,0.2647058823529412,0.043898699568087764
bigscience/bloom-1b7,mmlu_high_school_physics,0-shot,accuracy,0.33112582781456956,0.038425817186598696
bigscience/bloom-1b7,mmlu_high_school_biology,0-shot,accuracy,0.2967741935483871,0.0259885007924119
bigscience/bloom-1b7,mmlu_college_biology,0-shot,accuracy,0.2777777777777778,0.03745554791462457
bigscience/bloom-1b7,mmlu_anatomy,0-shot,accuracy,0.24444444444444444,0.03712537833614865
bigscience/bloom-1b7,mmlu_college_chemistry,0-shot,accuracy,0.22,0.04163331998932269
bigscience/bloom-1b7,mmlu_computer_security,0-shot,accuracy,0.21,0.040936018074033256
bigscience/bloom-1b7,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526045
bigscience/bloom-1b7,mmlu_astronomy,0-shot,accuracy,0.2631578947368421,0.03583496176361062
bigscience/bloom-1b7,mmlu_college_mathematics,0-shot,accuracy,0.31,0.04648231987117316
bigscience/bloom-1b7,mmlu_conceptual_physics,0-shot,accuracy,0.225531914893617,0.027321078417387533
bigscience/bloom-1b7,mmlu_abstract_algebra,0-shot,accuracy,0.18,0.03861229196653696
bigscience/bloom-1b7,mmlu_high_school_computer_science,0-shot,accuracy,0.22,0.04163331998932269
bigscience/bloom-1b7,mmlu_machine_learning,0-shot,accuracy,0.17857142857142858,0.036352091215778065
bigscience/bloom-1b7,mmlu_high_school_chemistry,0-shot,accuracy,0.28078817733990147,0.0316185633535861
bigscience/bloom-1b7,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
bigscience/bloom-1b7,mmlu_elementary_mathematics,0-shot,accuracy,0.2328042328042328,0.021765961672154523
bigscience/bloom-1b7,mmlu_electrical_engineering,0-shot,accuracy,0.2482758620689655,0.03600105692727772
bigscience/bloom-1b7,mmlu_high_school_mathematics,0-shot,accuracy,0.25555555555555554,0.026593939101844058
bigscience/bloom-1b7,arc_challenge,25-shot,accuracy,0.2627986348122867,0.012862523175351331
bigscience/bloom-1b7,arc_challenge,25-shot,acc_norm,0.2977815699658703,0.013363080107244487
bigscience/bloom-1b7,truthfulqa_mc2,0-shot,accuracy,0.4132695309257005,0.014434938787204109
bigscience/bloom-1b7,truthfulqa_gen,0-shot,bleu_max,6.25057775012128,0.3528698043271998
bigscience/bloom-1b7,truthfulqa_gen,0-shot,bleu_acc,0.24724602203182375,0.015102404797359654
bigscience/bloom-1b7,truthfulqa_gen,0-shot,bleu_diff,-0.4783944349233914,0.29103233082588886
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge1_max,18.178185223654555,0.7404994679390557
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge1_acc,0.2521419828641371,0.01520152224629999
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge1_diff,-1.0202739853119616,0.4615520291209216
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge2_max,10.60641799597891,0.5882439588833563
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge2_acc,0.18727050183598531,0.013657229868067033
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rouge2_diff,-1.1219916084328412,0.4934334962891775
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rougeL_max,17.075940690862858,0.7066741788370577
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rougeL_acc,0.25091799265605874,0.015176985027707682
bigscience/bloom-1b7,truthfulqa_gen,0-shot,rougeL_diff,-0.81444520512165,0.45367739492458453
bigscience/bloom-1b7,truthfulqa_mc1,0-shot,accuracy,0.24479804161566707,0.015051869486714997
mosaicml/mpt-7b-instruct,drop,3-shot,accuracy,0.2429739932885906,0.004392127579519805
mosaicml/mpt-7b-instruct,drop,3-shot,f1,0.2939712667785233,0.004382684089142145
mosaicml/mpt-7b-instruct,arc:challenge,25-shot,accuracy,0.44368600682593856,0.01451842182567045
mosaicml/mpt-7b-instruct,arc:challenge,25-shot,acc_norm,0.5034129692832765,0.014611050403244081
mosaicml/mpt-7b-instruct,hendrycksTest-abstract_algebra,5-shot,accuracy,0.31,0.046482319871173156
mosaicml/mpt-7b-instruct,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.31,0.046482319871173156
mosaicml/mpt-7b-instruct,hendrycksTest-anatomy,5-shot,accuracy,0.2814814814814815,0.03885004245800254
mosaicml/mpt-7b-instruct,hendrycksTest-anatomy,5-shot,acc_norm,0.2814814814814815,0.03885004245800254
mosaicml/mpt-7b-instruct,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.037385206761196686
mosaicml/mpt-7b-instruct,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.037385206761196686
mosaicml/mpt-7b-instruct,hendrycksTest-business_ethics,5-shot,accuracy,0.28,0.045126085985421255
mosaicml/mpt-7b-instruct,hendrycksTest-business_ethics,5-shot,acc_norm,0.28,0.045126085985421255
mosaicml/mpt-7b-instruct,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.35094339622641507,0.029373646253234686
mosaicml/mpt-7b-instruct,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.35094339622641507,0.029373646253234686
mosaicml/mpt-7b-instruct,hendrycksTest-college_biology,5-shot,accuracy,0.3194444444444444,0.038990736873573344
mosaicml/mpt-7b-instruct,hendrycksTest-college_biology,5-shot,acc_norm,0.3194444444444444,0.038990736873573344
mosaicml/mpt-7b-instruct,hendrycksTest-college_chemistry,5-shot,accuracy,0.35,0.047937248544110196
mosaicml/mpt-7b-instruct,hendrycksTest-college_chemistry,5-shot,acc_norm,0.35,0.047937248544110196
mosaicml/mpt-7b-instruct,hendrycksTest-college_computer_science,5-shot,accuracy,0.38,0.048783173121456316
mosaicml/mpt-7b-instruct,hendrycksTest-college_computer_science,5-shot,acc_norm,0.38,0.048783173121456316
mosaicml/mpt-7b-instruct,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
mosaicml/mpt-7b-instruct,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
mosaicml/mpt-7b-instruct,hendrycksTest-college_medicine,5-shot,accuracy,0.3063583815028902,0.035149425512674366
mosaicml/mpt-7b-instruct,hendrycksTest-college_medicine,5-shot,acc_norm,0.3063583815028902,0.035149425512674366
mosaicml/mpt-7b-instruct,hendrycksTest-college_physics,5-shot,accuracy,0.23529411764705882,0.042207736591714506
mosaicml/mpt-7b-instruct,hendrycksTest-college_physics,5-shot,acc_norm,0.23529411764705882,0.042207736591714506
mosaicml/mpt-7b-instruct,hendrycksTest-computer_security,5-shot,accuracy,0.37,0.048523658709391
mosaicml/mpt-7b-instruct,hendrycksTest-computer_security,5-shot,acc_norm,0.37,0.048523658709391
mosaicml/mpt-7b-instruct,hendrycksTest-conceptual_physics,5-shot,accuracy,0.34893617021276596,0.03115852213135778
mosaicml/mpt-7b-instruct,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.34893617021276596,0.03115852213135778
mosaicml/mpt-7b-instruct,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.040969851398436716
mosaicml/mpt-7b-instruct,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.040969851398436716
mosaicml/mpt-7b-instruct,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3586206896551724,0.03996629574876719
mosaicml/mpt-7b-instruct,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3586206896551724,0.03996629574876719
mosaicml/mpt-7b-instruct,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.26455026455026454,0.022717467897708607
mosaicml/mpt-7b-instruct,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.26455026455026454,0.022717467897708607
mosaicml/mpt-7b-instruct,hendrycksTest-formal_logic,5-shot,accuracy,0.1984126984126984,0.03567016675276865
mosaicml/mpt-7b-instruct,hendrycksTest-formal_logic,5-shot,acc_norm,0.1984126984126984,0.03567016675276865
mosaicml/mpt-7b-instruct,hendrycksTest-global_facts,5-shot,accuracy,0.36,0.048241815132442176
mosaicml/mpt-7b-instruct,hendrycksTest-global_facts,5-shot,acc_norm,0.36,0.048241815132442176
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_biology,5-shot,accuracy,0.38064516129032255,0.027621717832907046
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_biology,5-shot,acc_norm,0.38064516129032255,0.027621717832907046
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2955665024630542,0.032104944337514575
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2955665024630542,0.032104944337514575
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.047258156262526045
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.047258156262526045
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2909090909090909,0.03546563019624336
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2909090909090909,0.03546563019624336
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_geography,5-shot,accuracy,0.4444444444444444,0.035402943770953675
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_geography,5-shot,acc_norm,0.4444444444444444,0.035402943770953675
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.37823834196891193,0.03499807276193339
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.37823834196891193,0.03499807276193339
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.36923076923076925,0.02446861524147892
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.36923076923076925,0.02446861524147892
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.25555555555555554,0.026593939101844065
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.25555555555555554,0.026593939101844065
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.36134453781512604,0.031204691225150013
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.36134453781512604,0.031204691225150013
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_physics,5-shot,accuracy,0.3576158940397351,0.03913453431177258
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3576158940397351,0.03913453431177258
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3357798165137615,0.02024808139675293
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3357798165137615,0.02024808139675293
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_statistics,5-shot,accuracy,0.39814814814814814,0.033384734032074016
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.39814814814814814,0.033384734032074016
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25980392156862747,0.030778554678693264
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25980392156862747,0.030778554678693264
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25738396624472576,0.0284588209914603
mosaicml/mpt-7b-instruct,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25738396624472576,0.0284588209914603
mosaicml/mpt-7b-instruct,hendrycksTest-human_aging,5-shot,accuracy,0.35874439461883406,0.032190792004199956
mosaicml/mpt-7b-instruct,hendrycksTest-human_aging,5-shot,acc_norm,0.35874439461883406,0.032190792004199956
mosaicml/mpt-7b-instruct,hendrycksTest-human_sexuality,5-shot,accuracy,0.3893129770992366,0.04276486542814591
mosaicml/mpt-7b-instruct,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3893129770992366,0.04276486542814591
mosaicml/mpt-7b-instruct,hendrycksTest-international_law,5-shot,accuracy,0.3140495867768595,0.042369647530410184
mosaicml/mpt-7b-instruct,hendrycksTest-international_law,5-shot,acc_norm,0.3140495867768595,0.042369647530410184
mosaicml/mpt-7b-instruct,hendrycksTest-jurisprudence,5-shot,accuracy,0.37037037037037035,0.04668408033024931
mosaicml/mpt-7b-instruct,hendrycksTest-jurisprudence,5-shot,acc_norm,0.37037037037037035,0.04668408033024931
mosaicml/mpt-7b-instruct,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3558282208588957,0.03761521380046734
mosaicml/mpt-7b-instruct,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3558282208588957,0.03761521380046734
mosaicml/mpt-7b-instruct,hendrycksTest-machine_learning,5-shot,accuracy,0.2857142857142857,0.04287858751340456
mosaicml/mpt-7b-instruct,hendrycksTest-machine_learning,5-shot,acc_norm,0.2857142857142857,0.04287858751340456
mosaicml/mpt-7b-instruct,hendrycksTest-management,5-shot,accuracy,0.36893203883495146,0.0477761518115674
mosaicml/mpt-7b-instruct,hendrycksTest-management,5-shot,acc_norm,0.36893203883495146,0.0477761518115674
mosaicml/mpt-7b-instruct,hendrycksTest-marketing,5-shot,accuracy,0.3247863247863248,0.03067902276549883
mosaicml/mpt-7b-instruct,hendrycksTest-marketing,5-shot,acc_norm,0.3247863247863248,0.03067902276549883
mosaicml/mpt-7b-instruct,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.04512608598542127
mosaicml/mpt-7b-instruct,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.04512608598542127
mosaicml/mpt-7b-instruct,hendrycksTest-miscellaneous,5-shot,accuracy,0.3499361430395913,0.017055679797150423
mosaicml/mpt-7b-instruct,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3499361430395913,0.017055679797150423
mosaicml/mpt-7b-instruct,hendrycksTest-moral_disputes,5-shot,accuracy,0.31213872832369943,0.02494679222527231
mosaicml/mpt-7b-instruct,hendrycksTest-moral_disputes,5-shot,acc_norm,0.31213872832369943,0.02494679222527231
mosaicml/mpt-7b-instruct,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
mosaicml/mpt-7b-instruct,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
mosaicml/mpt-7b-instruct,hendrycksTest-nutrition,5-shot,accuracy,0.3137254901960784,0.026568921015457152
mosaicml/mpt-7b-instruct,hendrycksTest-nutrition,5-shot,acc_norm,0.3137254901960784,0.026568921015457152
mosaicml/mpt-7b-instruct,hendrycksTest-philosophy,5-shot,accuracy,0.3279742765273312,0.02666441088693762
mosaicml/mpt-7b-instruct,hendrycksTest-philosophy,5-shot,acc_norm,0.3279742765273312,0.02666441088693762
mosaicml/mpt-7b-instruct,hendrycksTest-prehistory,5-shot,accuracy,0.3148148148148148,0.02584224870090217
mosaicml/mpt-7b-instruct,hendrycksTest-prehistory,5-shot,acc_norm,0.3148148148148148,0.02584224870090217
mosaicml/mpt-7b-instruct,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.026011992930902013
mosaicml/mpt-7b-instruct,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.026011992930902013
mosaicml/mpt-7b-instruct,hendrycksTest-professional_law,5-shot,accuracy,0.27835723598435463,0.011446990197380985
mosaicml/mpt-7b-instruct,hendrycksTest-professional_law,5-shot,acc_norm,0.27835723598435463,0.011446990197380985
mosaicml/mpt-7b-instruct,hendrycksTest-professional_medicine,5-shot,accuracy,0.25735294117647056,0.026556519470041524
mosaicml/mpt-7b-instruct,hendrycksTest-professional_medicine,5-shot,acc_norm,0.25735294117647056,0.026556519470041524
mosaicml/mpt-7b-instruct,hendrycksTest-professional_psychology,5-shot,accuracy,0.3137254901960784,0.018771683893528172
mosaicml/mpt-7b-instruct,hendrycksTest-professional_psychology,5-shot,acc_norm,0.3137254901960784,0.018771683893528172
mosaicml/mpt-7b-instruct,hendrycksTest-public_relations,5-shot,accuracy,0.33636363636363636,0.04525393596302505
mosaicml/mpt-7b-instruct,hendrycksTest-public_relations,5-shot,acc_norm,0.33636363636363636,0.04525393596302505
mosaicml/mpt-7b-instruct,hendrycksTest-security_studies,5-shot,accuracy,0.4163265306122449,0.03155782816556165
mosaicml/mpt-7b-instruct,hendrycksTest-security_studies,5-shot,acc_norm,0.4163265306122449,0.03155782816556165
mosaicml/mpt-7b-instruct,hendrycksTest-sociology,5-shot,accuracy,0.3383084577114428,0.033455630703391935
mosaicml/mpt-7b-instruct,hendrycksTest-sociology,5-shot,acc_norm,0.3383084577114428,0.033455630703391935
mosaicml/mpt-7b-instruct,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.42,0.049604496374885836
mosaicml/mpt-7b-instruct,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.42,0.049604496374885836
mosaicml/mpt-7b-instruct,hendrycksTest-virology,5-shot,accuracy,0.3614457831325301,0.037400593820293204
mosaicml/mpt-7b-instruct,hendrycksTest-virology,5-shot,acc_norm,0.3614457831325301,0.037400593820293204
mosaicml/mpt-7b-instruct,hendrycksTest-world_religions,5-shot,accuracy,0.2573099415204678,0.03352799844161865
mosaicml/mpt-7b-instruct,hendrycksTest-world_religions,5-shot,acc_norm,0.2573099415204678,0.03352799844161865
mosaicml/mpt-7b-instruct,truthfulqa:mc,0-shot,mc1,0.22888616891064872,0.014706994909055027
mosaicml/mpt-7b-instruct,truthfulqa:mc,0-shot,mc2,0.3508407855782673,0.013771122171386638
gpt2-medium,arc:challenge,25-shot,accuracy,0.22098976109215018,0.012124929206818258
gpt2-medium,arc:challenge,25-shot,acc_norm,0.27047781569965873,0.012980954547659554
gpt2-medium,hellaswag,10-shot,accuracy,0.33061143198566023,0.004694718918225759
gpt2-medium,hellaswag,10-shot,acc_norm,0.4017128062139016,0.004892425356375716
gpt2-medium,hendrycksTest-abstract_algebra,5-shot,accuracy,0.26,0.04408440022768081
gpt2-medium,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.26,0.04408440022768081
gpt2-medium,hendrycksTest-anatomy,5-shot,accuracy,0.24444444444444444,0.03712537833614867
gpt2-medium,hendrycksTest-anatomy,5-shot,acc_norm,0.24444444444444444,0.03712537833614867
gpt2-medium,hendrycksTest-astronomy,5-shot,accuracy,0.3026315789473684,0.03738520676119668
gpt2-medium,hendrycksTest-astronomy,5-shot,acc_norm,0.3026315789473684,0.03738520676119668
gpt2-medium,hendrycksTest-business_ethics,5-shot,accuracy,0.15,0.035887028128263686
gpt2-medium,hendrycksTest-business_ethics,5-shot,acc_norm,0.15,0.035887028128263686
gpt2-medium,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.02825420034443867
gpt2-medium,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.02825420034443867
gpt2-medium,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.03716177437566016
gpt2-medium,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.03716177437566016
gpt2-medium,hendrycksTest-college_chemistry,5-shot,accuracy,0.31,0.046482319871173156
gpt2-medium,hendrycksTest-college_chemistry,5-shot,acc_norm,0.31,0.046482319871173156
gpt2-medium,hendrycksTest-college_computer_science,5-shot,accuracy,0.36,0.04824181513244218
gpt2-medium,hendrycksTest-college_computer_science,5-shot,acc_norm,0.36,0.04824181513244218
gpt2-medium,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.04163331998932269
gpt2-medium,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.04163331998932269
gpt2-medium,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.033687629322594316
gpt2-medium,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.033687629322594316
gpt2-medium,hendrycksTest-college_physics,5-shot,accuracy,0.1568627450980392,0.036186648199362466
gpt2-medium,hendrycksTest-college_physics,5-shot,acc_norm,0.1568627450980392,0.036186648199362466
gpt2-medium,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.042295258468165044
gpt2-medium,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.042295258468165044
gpt2-medium,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2936170212765957,0.029771642712491227
gpt2-medium,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2936170212765957,0.029771642712491227
gpt2-medium,hendrycksTest-econometrics,5-shot,accuracy,0.30701754385964913,0.0433913832257986
gpt2-medium,hendrycksTest-econometrics,5-shot,acc_norm,0.30701754385964913,0.0433913832257986
gpt2-medium,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.03600105692727771
gpt2-medium,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.03600105692727771
gpt2-medium,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918424
gpt2-medium,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.022569897074918424
gpt2-medium,hendrycksTest-formal_logic,5-shot,accuracy,0.15873015873015872,0.03268454013011742
gpt2-medium,hendrycksTest-formal_logic,5-shot,acc_norm,0.15873015873015872,0.03268454013011742
gpt2-medium,hendrycksTest-global_facts,5-shot,accuracy,0.31,0.04648231987117316
gpt2-medium,hendrycksTest-global_facts,5-shot,acc_norm,0.31,0.04648231987117316
gpt2-medium,hendrycksTest-high_school_biology,5-shot,accuracy,0.24193548387096775,0.0243625996930311
gpt2-medium,hendrycksTest-high_school_biology,5-shot,acc_norm,0.24193548387096775,0.0243625996930311
gpt2-medium,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3054187192118227,0.03240661565868408
gpt2-medium,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3054187192118227,0.03240661565868408
gpt2-medium,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.04512608598542127
gpt2-medium,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.04512608598542127
gpt2-medium,hendrycksTest-high_school_european_history,5-shot,accuracy,0.26666666666666666,0.03453131801885415
gpt2-medium,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.26666666666666666,0.03453131801885415
gpt2-medium,hendrycksTest-high_school_geography,5-shot,accuracy,0.35858585858585856,0.03416903640391521
gpt2-medium,hendrycksTest-high_school_geography,5-shot,acc_norm,0.35858585858585856,0.03416903640391521
gpt2-medium,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3160621761658031,0.033553973696861736
gpt2-medium,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3160621761658031,0.033553973696861736
gpt2-medium,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3076923076923077,0.023400928918310502
gpt2-medium,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3076923076923077,0.023400928918310502
gpt2-medium,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26296296296296295,0.026842057873833706
gpt2-medium,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26296296296296295,0.026842057873833706
gpt2-medium,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3025210084033613,0.02983796238829192
gpt2-medium,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3025210084033613,0.02983796238829192
gpt2-medium,hendrycksTest-high_school_physics,5-shot,accuracy,0.3443708609271523,0.038796870240733264
gpt2-medium,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3443708609271523,0.038796870240733264
gpt2-medium,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3357798165137615,0.020248081396752937
gpt2-medium,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3357798165137615,0.020248081396752937
gpt2-medium,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4074074074074074,0.033509916046960436
gpt2-medium,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4074074074074074,0.033509916046960436
gpt2-medium,hendrycksTest-high_school_us_history,5-shot,accuracy,0.22549019607843138,0.02933116229425173
gpt2-medium,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.22549019607843138,0.02933116229425173
gpt2-medium,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2109704641350211,0.02655837250266192
gpt2-medium,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2109704641350211,0.02655837250266192
gpt2-medium,hendrycksTest-human_aging,5-shot,accuracy,0.2062780269058296,0.02715715047956382
gpt2-medium,hendrycksTest-human_aging,5-shot,acc_norm,0.2062780269058296,0.02715715047956382
gpt2-medium,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
gpt2-medium,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
gpt2-medium,hendrycksTest-international_law,5-shot,accuracy,0.1652892561983471,0.03390780612972777
gpt2-medium,hendrycksTest-international_law,5-shot,acc_norm,0.1652892561983471,0.03390780612972777
gpt2-medium,hendrycksTest-jurisprudence,5-shot,accuracy,0.2037037037037037,0.03893542518824847
gpt2-medium,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2037037037037037,0.03893542518824847
gpt2-medium,hendrycksTest-logical_fallacies,5-shot,accuracy,0.26380368098159507,0.03462419931615624
gpt2-medium,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.26380368098159507,0.03462419931615624
gpt2-medium,hendrycksTest-machine_learning,5-shot,accuracy,0.19642857142857142,0.03770970049347019
gpt2-medium,hendrycksTest-machine_learning,5-shot,acc_norm,0.19642857142857142,0.03770970049347019
gpt2-medium,hendrycksTest-management,5-shot,accuracy,0.3592233009708738,0.04750458399041692
gpt2-medium,hendrycksTest-management,5-shot,acc_norm,0.3592233009708738,0.04750458399041692
gpt2-medium,hendrycksTest-marketing,5-shot,accuracy,0.20512820512820512,0.026453508054040353
gpt2-medium,hendrycksTest-marketing,5-shot,acc_norm,0.20512820512820512,0.026453508054040353
gpt2-medium,hendrycksTest-medical_genetics,5-shot,accuracy,0.28,0.045126085985421296
gpt2-medium,hendrycksTest-medical_genetics,5-shot,acc_norm,0.28,0.045126085985421296
gpt2-medium,hendrycksTest-miscellaneous,5-shot,accuracy,0.24010217113665389,0.015274685213734191
gpt2-medium,hendrycksTest-miscellaneous,5-shot,acc_norm,0.24010217113665389,0.015274685213734191
gpt2-medium,hendrycksTest-moral_disputes,5-shot,accuracy,0.24277456647398843,0.0230836585869842
gpt2-medium,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24277456647398843,0.0230836585869842
gpt2-medium,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
gpt2-medium,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
gpt2-medium,hendrycksTest-nutrition,5-shot,accuracy,0.2679738562091503,0.025360603796242557
gpt2-medium,hendrycksTest-nutrition,5-shot,acc_norm,0.2679738562091503,0.025360603796242557
gpt2-medium,hendrycksTest-philosophy,5-shot,accuracy,0.24115755627009647,0.02429659403476343
gpt2-medium,hendrycksTest-philosophy,5-shot,acc_norm,0.24115755627009647,0.02429659403476343
gpt2-medium,hendrycksTest-prehistory,5-shot,accuracy,0.2191358024691358,0.023016705640262196
gpt2-medium,hendrycksTest-prehistory,5-shot,acc_norm,0.2191358024691358,0.023016705640262196
gpt2-medium,hendrycksTest-professional_accounting,5-shot,accuracy,0.25886524822695034,0.02612957252718085
gpt2-medium,hendrycksTest-professional_accounting,5-shot,acc_norm,0.25886524822695034,0.02612957252718085
gpt2-medium,hendrycksTest-professional_law,5-shot,accuracy,0.24511082138200782,0.010986307870045524
gpt2-medium,hendrycksTest-professional_law,5-shot,acc_norm,0.24511082138200782,0.010986307870045524
gpt2-medium,hendrycksTest-professional_medicine,5-shot,accuracy,0.4522058823529412,0.030233758551596455
gpt2-medium,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4522058823529412,0.030233758551596455
gpt2-medium,hendrycksTest-professional_psychology,5-shot,accuracy,0.23529411764705882,0.01716058723504635
gpt2-medium,hendrycksTest-professional_psychology,5-shot,acc_norm,0.23529411764705882,0.01716058723504635
gpt2-medium,hendrycksTest-public_relations,5-shot,accuracy,0.2636363636363636,0.04220224692971987
gpt2-medium,hendrycksTest-public_relations,5-shot,acc_norm,0.2636363636363636,0.04220224692971987
gpt2-medium,hendrycksTest-security_studies,5-shot,accuracy,0.3346938775510204,0.030209235226242307
gpt2-medium,hendrycksTest-security_studies,5-shot,acc_norm,0.3346938775510204,0.030209235226242307
gpt2-medium,hendrycksTest-sociology,5-shot,accuracy,0.22885572139303484,0.029705284056772432
gpt2-medium,hendrycksTest-sociology,5-shot,acc_norm,0.22885572139303484,0.029705284056772432
gpt2-medium,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.045126085985421276
gpt2-medium,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.045126085985421276
gpt2-medium,hendrycksTest-virology,5-shot,accuracy,0.20481927710843373,0.03141784291663926
gpt2-medium,hendrycksTest-virology,5-shot,acc_norm,0.20481927710843373,0.03141784291663926
gpt2-medium,hendrycksTest-world_religions,5-shot,accuracy,0.25146198830409355,0.033275044238468436
gpt2-medium,hendrycksTest-world_religions,5-shot,acc_norm,0.25146198830409355,0.033275044238468436
gpt2-medium,truthfulqa:mc,0-shot,mc1,0.2252141982864137,0.014623240768023505
gpt2-medium,truthfulqa:mc,0-shot,mc2,0.4075602335796246,0.014596763158762415
EleutherAI/pythia-410M,mmlu_world_religions,0-shot,accuracy,0.27485380116959063,0.03424042924691582
EleutherAI/pythia-410M,mmlu_formal_logic,0-shot,accuracy,0.29365079365079366,0.04073524322147127
EleutherAI/pythia-410M,mmlu_prehistory,0-shot,accuracy,0.2191358024691358,0.023016705640262196
EleutherAI/pythia-410M,mmlu_moral_scenarios,0-shot,accuracy,0.25251396648044694,0.014530330201468634
EleutherAI/pythia-410M,mmlu_high_school_world_history,0-shot,accuracy,0.2109704641350211,0.026558372502661923
EleutherAI/pythia-410M,mmlu_moral_disputes,0-shot,accuracy,0.2630057803468208,0.023703099525258165
EleutherAI/pythia-410M,mmlu_professional_law,0-shot,accuracy,0.24185136897001303,0.01093655081382704
EleutherAI/pythia-410M,mmlu_logical_fallacies,0-shot,accuracy,0.2883435582822086,0.035590395316173425
EleutherAI/pythia-410M,mmlu_high_school_us_history,0-shot,accuracy,0.2647058823529412,0.030964517926923424
EleutherAI/pythia-410M,mmlu_philosophy,0-shot,accuracy,0.24115755627009647,0.02429659403476343
EleutherAI/pythia-410M,mmlu_jurisprudence,0-shot,accuracy,0.2037037037037037,0.03893542518824847
EleutherAI/pythia-410M,mmlu_international_law,0-shot,accuracy,0.4049586776859504,0.04481137755942469
EleutherAI/pythia-410M,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.033464098810559534
EleutherAI/pythia-410M,mmlu_high_school_government_and_politics,0-shot,accuracy,0.22797927461139897,0.030276909945178263
EleutherAI/pythia-410M,mmlu_high_school_microeconomics,0-shot,accuracy,0.23949579831932774,0.02772206549336128
EleutherAI/pythia-410M,mmlu_high_school_geography,0-shot,accuracy,0.30303030303030304,0.03274287914026867
EleutherAI/pythia-410M,mmlu_high_school_psychology,0-shot,accuracy,0.3211009174311927,0.020018149772733744
EleutherAI/pythia-410M,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-410M,mmlu_us_foreign_policy,0-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-410M,mmlu_sociology,0-shot,accuracy,0.23880597014925373,0.030147775935409217
EleutherAI/pythia-410M,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3487179487179487,0.02416278028401772
EleutherAI/pythia-410M,mmlu_security_studies,0-shot,accuracy,0.40408163265306124,0.03141470802586589
EleutherAI/pythia-410M,mmlu_professional_psychology,0-shot,accuracy,0.27450980392156865,0.018054027458815194
EleutherAI/pythia-410M,mmlu_human_sexuality,0-shot,accuracy,0.25190839694656486,0.03807387116306086
EleutherAI/pythia-410M,mmlu_econometrics,0-shot,accuracy,0.22807017543859648,0.03947152782669415
EleutherAI/pythia-410M,mmlu_miscellaneous,0-shot,accuracy,0.25925925925925924,0.01567100600933956
EleutherAI/pythia-410M,mmlu_marketing,0-shot,accuracy,0.19230769230769232,0.025819233256483713
EleutherAI/pythia-410M,mmlu_management,0-shot,accuracy,0.2621359223300971,0.04354631077260595
EleutherAI/pythia-410M,mmlu_nutrition,0-shot,accuracy,0.26143790849673204,0.025160998214292456
EleutherAI/pythia-410M,mmlu_medical_genetics,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-410M,mmlu_human_aging,0-shot,accuracy,0.22869955156950672,0.028188240046929193
EleutherAI/pythia-410M,mmlu_professional_medicine,0-shot,accuracy,0.44485294117647056,0.03018753206032938
EleutherAI/pythia-410M,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.031265112061730424
EleutherAI/pythia-410M,mmlu_business_ethics,0-shot,accuracy,0.24,0.04292346959909283
EleutherAI/pythia-410M,mmlu_clinical_knowledge,0-shot,accuracy,0.25660377358490566,0.02688064788905197
EleutherAI/pythia-410M,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
EleutherAI/pythia-410M,mmlu_virology,0-shot,accuracy,0.26506024096385544,0.03436024037944967
EleutherAI/pythia-410M,mmlu_professional_accounting,0-shot,accuracy,0.2375886524822695,0.025389512552729903
EleutherAI/pythia-410M,mmlu_college_physics,0-shot,accuracy,0.24509803921568626,0.04280105837364396
EleutherAI/pythia-410M,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.03710185726119996
EleutherAI/pythia-410M,mmlu_high_school_biology,0-shot,accuracy,0.2903225806451613,0.0258221061194159
EleutherAI/pythia-410M,mmlu_college_biology,0-shot,accuracy,0.2777777777777778,0.03745554791462457
EleutherAI/pythia-410M,mmlu_anatomy,0-shot,accuracy,0.28888888888888886,0.0391545063041425
EleutherAI/pythia-410M,mmlu_college_chemistry,0-shot,accuracy,0.42,0.04960449637488583
EleutherAI/pythia-410M,mmlu_computer_security,0-shot,accuracy,0.2,0.04020151261036846
EleutherAI/pythia-410M,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526045
EleutherAI/pythia-410M,mmlu_astronomy,0-shot,accuracy,0.21710526315789475,0.03355045304882923
EleutherAI/pythia-410M,mmlu_college_mathematics,0-shot,accuracy,0.32,0.046882617226215034
EleutherAI/pythia-410M,mmlu_conceptual_physics,0-shot,accuracy,0.2851063829787234,0.02951319662553935
EleutherAI/pythia-410M,mmlu_abstract_algebra,0-shot,accuracy,0.25,0.04351941398892446
EleutherAI/pythia-410M,mmlu_high_school_computer_science,0-shot,accuracy,0.31,0.04648231987117316
EleutherAI/pythia-410M,mmlu_machine_learning,0-shot,accuracy,0.21428571428571427,0.03894641120044793
EleutherAI/pythia-410M,mmlu_high_school_chemistry,0-shot,accuracy,0.2660098522167488,0.031089826002937523
EleutherAI/pythia-410M,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
EleutherAI/pythia-410M,mmlu_elementary_mathematics,0-shot,accuracy,0.25132275132275134,0.022340482339643895
EleutherAI/pythia-410M,mmlu_electrical_engineering,0-shot,accuracy,0.2206896551724138,0.03455930201924812
EleutherAI/pythia-410M,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.026842057873833706
EleutherAI/pythia-410M,arc_challenge,25-shot,accuracy,0.22696245733788395,0.012240491536132866
EleutherAI/pythia-410M,arc_challenge,25-shot,acc_norm,0.27047781569965873,0.012980954547659556
EleutherAI/pythia-410M,hellaswag,10-shot,accuracy,0.33917546305516827,0.004724619193427591
EleutherAI/pythia-410M,hellaswag,10-shot,acc_norm,0.41107349133638715,0.004910229643262737
EleutherAI/pythia-410M,truthfulqa_mc2,0-shot,accuracy,0.41237903356936856,0.014564373669182272
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,bleu_max,16.26090055559814,0.5405929938913795
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,bleu_acc,0.38555691554467564,0.017038839010591663
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,bleu_diff,-3.0508334325257387,0.5347158599372487
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge1_max,36.94365940903368,0.8170526245474131
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge1_acc,0.29498164014687883,0.015964400965589647
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge1_diff,-7.001303766637389,0.7647828158374674
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge2_max,19.221965303272068,0.8332853887183771
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge2_acc,0.18115055079559364,0.013482697187817895
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rouge2_diff,-7.513009385966499,0.7649345167230245
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rougeL_max,34.22730634914542,0.7952582275069422
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rougeL_acc,0.29253365973072215,0.015925597445286165
EleutherAI/pythia-410M,truthfulqa_gen,0-shot,rougeL_diff,-6.85285056456493,0.7573700129593819
EleutherAI/pythia-410M,truthfulqa_mc1,0-shot,accuracy,0.23623011015911874,0.014869755015871093
EleutherAI/pythia-410M,winogrande,5-shot,accuracy,0.5232833464877664,0.014037241309573642
EleutherAI/pythia-410M,gsm8k,5-shot,accuracy,0.01819560272934041,0.003681611894073871
Salesforce/codegen-2B-mono,minerva_math_precalc,5-shot,accuracy,0.014652014652014652,0.0051468941589821625
Salesforce/codegen-2B-mono,minerva_math_prealgebra,5-shot,accuracy,0.03214695752009185,0.005980190540374792
Salesforce/codegen-2B-mono,minerva_math_num_theory,5-shot,accuracy,0.03148148148148148,0.007521200438716891
Salesforce/codegen-2B-mono,minerva_math_intermediate_algebra,5-shot,accuracy,0.016611295681063124,0.004255602872194612
Salesforce/codegen-2B-mono,minerva_math_geometry,5-shot,accuracy,0.008350730688935281,0.004162242110295852
Salesforce/codegen-2B-mono,minerva_math_counting_and_prob,5-shot,accuracy,0.010548523206751054,0.004697453735376151
Salesforce/codegen-2B-mono,minerva_math_algebra,5-shot,accuracy,0.015164279696714406,0.003548546043132554
Salesforce/codegen-2B-mono,fld_default,0-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,fld_star,0-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_3da,5-shot,accuracy,0.0085,0.002053285901060974
Salesforce/codegen-2B-mono,arithmetic_3ds,5-shot,accuracy,0.0075,0.0019296986470519835
Salesforce/codegen-2B-mono,arithmetic_4da,5-shot,accuracy,0.001,0.0007069298939339426
Salesforce/codegen-2B-mono,arithmetic_2ds,5-shot,accuracy,0.108,0.006942052725816968
Salesforce/codegen-2B-mono,arithmetic_5ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_5da,5-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_1dc,5-shot,accuracy,0.0775,0.005980364318224231
Salesforce/codegen-2B-mono,arithmetic_4ds,5-shot,accuracy,0.0,
Salesforce/codegen-2B-mono,arithmetic_2dm,5-shot,accuracy,0.054,0.005055173329243415
Salesforce/codegen-2B-mono,arithmetic_2da,5-shot,accuracy,0.104,0.006827540380973844
Salesforce/codegen-2B-mono,gsm8k_cot,5-shot,accuracy,0.024260803639120546,0.004238007900001392
Salesforce/codegen-2B-mono,gsm8k,5-shot,accuracy,0.02577710386656558,0.004365042953621819
Salesforce/codegen-2B-mono,anli_r2,0-shot,brier_score,0.7685841435173599,
Salesforce/codegen-2B-mono,anli_r3,0-shot,brier_score,0.8015102484190165,
Salesforce/codegen-2B-mono,anli_r1,0-shot,brier_score,0.8031169120598974,
Salesforce/codegen-2B-mono,xnli_eu,0-shot,brier_score,1.1017671299956897,
Salesforce/codegen-2B-mono,xnli_vi,0-shot,brier_score,1.131535014847705,
Salesforce/codegen-2B-mono,xnli_ru,0-shot,brier_score,0.8173338590693098,
Salesforce/codegen-2B-mono,xnli_zh,0-shot,brier_score,1.0696657250411659,
Salesforce/codegen-2B-mono,xnli_tr,0-shot,brier_score,0.903173033726133,
Salesforce/codegen-2B-mono,xnli_fr,0-shot,brier_score,1.073901212241402,
Salesforce/codegen-2B-mono,xnli_en,0-shot,brier_score,0.7006196663679488,
Salesforce/codegen-2B-mono,xnli_ur,0-shot,brier_score,1.2038380352221547,
Salesforce/codegen-2B-mono,xnli_ar,0-shot,brier_score,0.9165665499376836,
Salesforce/codegen-2B-mono,xnli_de,0-shot,brier_score,1.0937779753771633,
Salesforce/codegen-2B-mono,xnli_hi,0-shot,brier_score,1.0356865346007569,
Salesforce/codegen-2B-mono,xnli_es,0-shot,brier_score,0.952199600899833,
Salesforce/codegen-2B-mono,xnli_bg,0-shot,brier_score,0.9337331322076657,
Salesforce/codegen-2B-mono,xnli_sw,0-shot,brier_score,0.8983199952247677,
Salesforce/codegen-2B-mono,xnli_el,0-shot,brier_score,0.831761035039731,
Salesforce/codegen-2B-mono,xnli_th,0-shot,brier_score,1.1558354492975338,
Salesforce/codegen-2B-mono,logiqa2,0-shot,brier_score,1.1424943532533816,
Salesforce/codegen-2B-mono,mathqa,5-shot,brier_score,0.983335118929985,
Salesforce/codegen-2B-mono,lambada_standard,0-shot,perplexity,127.0654873504989,5.294900704959425
Salesforce/codegen-2B-mono,lambada_standard,0-shot,accuracy,0.20881040170774307,0.005662763469361886
Salesforce/codegen-2B-mono,lambada_openai,0-shot,perplexity,120.31307761952347,5.2874389234283665
Salesforce/codegen-2B-mono,lambada_openai,0-shot,accuracy,0.2109450805356103,0.005683951840704778
Salesforce/codegen-2B-mono,mmlu_world_religions,0-shot,accuracy,0.2631578947368421,0.033773102522091945
Salesforce/codegen-2B-mono,mmlu_formal_logic,0-shot,accuracy,0.2857142857142857,0.04040610178208841
Salesforce/codegen-2B-mono,mmlu_prehistory,0-shot,accuracy,0.23148148148148148,0.02346842983245115
Salesforce/codegen-2B-mono,mmlu_moral_scenarios,0-shot,accuracy,0.2446927374301676,0.014378169884098436
Salesforce/codegen-2B-mono,mmlu_high_school_world_history,0-shot,accuracy,0.26582278481012656,0.028756799629658335
Salesforce/codegen-2B-mono,mmlu_moral_disputes,0-shot,accuracy,0.22254335260115607,0.02239421566194282
Salesforce/codegen-2B-mono,mmlu_professional_law,0-shot,accuracy,0.2438070404172099,0.010966507972178475
Salesforce/codegen-2B-mono,mmlu_logical_fallacies,0-shot,accuracy,0.26993865030674846,0.034878251684978906
Salesforce/codegen-2B-mono,mmlu_high_school_us_history,0-shot,accuracy,0.20588235294117646,0.02837944945158868
Salesforce/codegen-2B-mono,mmlu_philosophy,0-shot,accuracy,0.2861736334405145,0.025670259242188936
Salesforce/codegen-2B-mono,mmlu_jurisprudence,0-shot,accuracy,0.2222222222222222,0.040191074725573483
Salesforce/codegen-2B-mono,mmlu_international_law,0-shot,accuracy,0.3305785123966942,0.04294340845212095
Salesforce/codegen-2B-mono,mmlu_high_school_european_history,0-shot,accuracy,0.23030303030303031,0.0328766675860349
Salesforce/codegen-2B-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.31088082901554404,0.03340361906276585
Salesforce/codegen-2B-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.2689075630252101,0.028801392193631276
Salesforce/codegen-2B-mono,mmlu_high_school_geography,0-shot,accuracy,0.2828282828282828,0.032087795587867514
Salesforce/codegen-2B-mono,mmlu_high_school_psychology,0-shot,accuracy,0.24770642201834864,0.018508143602547805
Salesforce/codegen-2B-mono,mmlu_public_relations,0-shot,accuracy,0.2545454545454545,0.041723430387053825
Salesforce/codegen-2B-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-2B-mono,mmlu_sociology,0-shot,accuracy,0.26865671641791045,0.031343283582089536
Salesforce/codegen-2B-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3384615384615385,0.023991500500313036
Salesforce/codegen-2B-mono,mmlu_security_studies,0-shot,accuracy,0.3551020408163265,0.030635655150387634
Salesforce/codegen-2B-mono,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594726
Salesforce/codegen-2B-mono,mmlu_human_sexuality,0-shot,accuracy,0.24427480916030533,0.03768335959728744
Salesforce/codegen-2B-mono,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.03999423879281336
Salesforce/codegen-2B-mono,mmlu_miscellaneous,0-shot,accuracy,0.2247765006385696,0.01492744710193716
Salesforce/codegen-2B-mono,mmlu_marketing,0-shot,accuracy,0.3034188034188034,0.030118210106942645
Salesforce/codegen-2B-mono,mmlu_management,0-shot,accuracy,0.33980582524271846,0.046897659372781335
Salesforce/codegen-2B-mono,mmlu_nutrition,0-shot,accuracy,0.2777777777777778,0.02564686309713792
Salesforce/codegen-2B-mono,mmlu_medical_genetics,0-shot,accuracy,0.28,0.04512608598542127
Salesforce/codegen-2B-mono,mmlu_human_aging,0-shot,accuracy,0.17488789237668162,0.02549528462644497
Salesforce/codegen-2B-mono,mmlu_professional_medicine,0-shot,accuracy,0.4375,0.030134614954403924
Salesforce/codegen-2B-mono,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
Salesforce/codegen-2B-mono,mmlu_business_ethics,0-shot,accuracy,0.28,0.045126085985421276
Salesforce/codegen-2B-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.2792452830188679,0.027611163402399715
Salesforce/codegen-2B-mono,mmlu_global_facts,0-shot,accuracy,0.17,0.0377525168068637
Salesforce/codegen-2B-mono,mmlu_virology,0-shot,accuracy,0.27710843373493976,0.034843315926805875
Salesforce/codegen-2B-mono,mmlu_professional_accounting,0-shot,accuracy,0.2695035460992908,0.026469036818590627
Salesforce/codegen-2B-mono,mmlu_college_physics,0-shot,accuracy,0.17647058823529413,0.0379328118530781
Salesforce/codegen-2B-mono,mmlu_high_school_physics,0-shot,accuracy,0.3443708609271523,0.038796870240733264
Salesforce/codegen-2B-mono,mmlu_high_school_biology,0-shot,accuracy,0.1870967741935484,0.022185710092252255
Salesforce/codegen-2B-mono,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.03716177437566017
Salesforce/codegen-2B-mono,mmlu_anatomy,0-shot,accuracy,0.24444444444444444,0.037125378336148665
Salesforce/codegen-2B-mono,mmlu_college_chemistry,0-shot,accuracy,0.29,0.04560480215720685
Salesforce/codegen-2B-mono,mmlu_computer_security,0-shot,accuracy,0.21,0.040936018074033256
Salesforce/codegen-2B-mono,mmlu_college_computer_science,0-shot,accuracy,0.39,0.04902071300001974
Salesforce/codegen-2B-mono,mmlu_astronomy,0-shot,accuracy,0.2565789473684211,0.035541803680256896
Salesforce/codegen-2B-mono,mmlu_college_mathematics,0-shot,accuracy,0.31,0.04648231987117316
Salesforce/codegen-2B-mono,mmlu_conceptual_physics,0-shot,accuracy,0.2936170212765957,0.029771642712491227
Salesforce/codegen-2B-mono,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.04560480215720683
Salesforce/codegen-2B-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-2B-mono,mmlu_machine_learning,0-shot,accuracy,0.19642857142857142,0.03770970049347018
Salesforce/codegen-2B-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.031270907132976984
Salesforce/codegen-2B-mono,mmlu_high_school_statistics,0-shot,accuracy,0.36574074074074076,0.03284738857647207
Salesforce/codegen-2B-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.25132275132275134,0.022340482339643898
Salesforce/codegen-2B-mono,mmlu_electrical_engineering,0-shot,accuracy,0.25517241379310346,0.03632984052707842
Salesforce/codegen-2B-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.25925925925925924,0.026719240783712173
Salesforce/codegen-2B-mono,arc_challenge,25-shot,accuracy,0.19197952218430034,0.011509598906598107
Salesforce/codegen-2B-mono,arc_challenge,25-shot,acc_norm,0.23293515358361774,0.012352507042617387
Salesforce/codegen-2B-mono,hellaswag,10-shot,accuracy,0.3044214299940251,0.0045922151182952635
Salesforce/codegen-2B-mono,hellaswag,10-shot,acc_norm,0.3430591515634336,0.004737608340163424
Salesforce/codegen-2B-mono,truthfulqa_mc2,0-shot,accuracy,0.4489096881754911,0.01542412879319777
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,bleu_max,16.04577191970097,0.5552162891689132
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,bleu_acc,0.37209302325581395,0.016921090118814045
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,bleu_diff,-2.0090839666050755,0.52202727913542
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge1_max,34.93723491168912,0.8615146874033828
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge1_acc,0.2962056303549572,0.015983595101811385
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge1_diff,-5.533004957393886,0.7883741299337192
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge2_max,18.30765799283543,0.8421305079189312
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge2_acc,0.16523867809057527,0.01300145435649922
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rouge2_diff,-5.205693990711149,0.7125780222110801
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rougeL_max,32.32955226819275,0.8369096403695482
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rougeL_acc,0.3047735618115055,0.01611412415688243
Salesforce/codegen-2B-mono,truthfulqa_gen,0-shot,rougeL_diff,-5.442950391491437,0.7788716099947031
Salesforce/codegen-2B-mono,truthfulqa_mc1,0-shot,accuracy,0.2582619339045288,0.01532182168847618
Salesforce/codegen-2B-mono,winogrande,5-shot,accuracy,0.5122336227308603,0.01404827882040562
facebook/xglm-7.5B,drop,3-shot,accuracy,0.13905201342281878,0.0035433720039612262
facebook/xglm-7.5B,drop,3-shot,f1,0.18580851510067117,0.0037071149655913006
facebook/xglm-7.5B,arc:challenge,25-shot,accuracy,0.30716723549488056,0.013481034054980945
facebook/xglm-7.5B,arc:challenge,25-shot,acc_norm,0.3412969283276451,0.013855831287497723
facebook/xglm-7.5B,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.044619604333847415
facebook/xglm-7.5B,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.044619604333847415
facebook/xglm-7.5B,hendrycksTest-anatomy,5-shot,accuracy,0.2518518518518518,0.03749850709174022
facebook/xglm-7.5B,hendrycksTest-anatomy,5-shot,acc_norm,0.2518518518518518,0.03749850709174022
facebook/xglm-7.5B,hendrycksTest-astronomy,5-shot,accuracy,0.2236842105263158,0.03391160934343602
facebook/xglm-7.5B,hendrycksTest-astronomy,5-shot,acc_norm,0.2236842105263158,0.03391160934343602
facebook/xglm-7.5B,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.04408440022768078
facebook/xglm-7.5B,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.04408440022768078
facebook/xglm-7.5B,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.3018867924528302,0.028254200344438662
facebook/xglm-7.5B,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.3018867924528302,0.028254200344438662
facebook/xglm-7.5B,hendrycksTest-college_biology,5-shot,accuracy,0.2986111111111111,0.03827052357950756
facebook/xglm-7.5B,hendrycksTest-college_biology,5-shot,acc_norm,0.2986111111111111,0.03827052357950756
facebook/xglm-7.5B,hendrycksTest-college_chemistry,5-shot,accuracy,0.27,0.04461960433384741
facebook/xglm-7.5B,hendrycksTest-college_chemistry,5-shot,acc_norm,0.27,0.04461960433384741
facebook/xglm-7.5B,hendrycksTest-college_computer_science,5-shot,accuracy,0.28,0.04512608598542128
facebook/xglm-7.5B,hendrycksTest-college_computer_science,5-shot,acc_norm,0.28,0.04512608598542128
facebook/xglm-7.5B,hendrycksTest-college_mathematics,5-shot,accuracy,0.25,0.04351941398892446
facebook/xglm-7.5B,hendrycksTest-college_mathematics,5-shot,acc_norm,0.25,0.04351941398892446
facebook/xglm-7.5B,hendrycksTest-college_medicine,5-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/xglm-7.5B,hendrycksTest-college_medicine,5-shot,acc_norm,0.2543352601156069,0.0332055644308557
facebook/xglm-7.5B,hendrycksTest-college_physics,5-shot,accuracy,0.19607843137254902,0.03950581861179961
facebook/xglm-7.5B,hendrycksTest-college_physics,5-shot,acc_norm,0.19607843137254902,0.03950581861179961
facebook/xglm-7.5B,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.04512608598542128
facebook/xglm-7.5B,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.04512608598542128
facebook/xglm-7.5B,hendrycksTest-conceptual_physics,5-shot,accuracy,0.28936170212765955,0.029644006577009618
facebook/xglm-7.5B,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.28936170212765955,0.029644006577009618
facebook/xglm-7.5B,hendrycksTest-econometrics,5-shot,accuracy,0.2543859649122807,0.040969851398436695
facebook/xglm-7.5B,hendrycksTest-econometrics,5-shot,acc_norm,0.2543859649122807,0.040969851398436695
facebook/xglm-7.5B,hendrycksTest-electrical_engineering,5-shot,accuracy,0.20689655172413793,0.03375672449560553
facebook/xglm-7.5B,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.20689655172413793,0.03375672449560553
facebook/xglm-7.5B,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2671957671957672,0.02278967314577656
facebook/xglm-7.5B,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2671957671957672,0.02278967314577656
facebook/xglm-7.5B,hendrycksTest-formal_logic,5-shot,accuracy,0.31746031746031744,0.04163453031302859
facebook/xglm-7.5B,hendrycksTest-formal_logic,5-shot,acc_norm,0.31746031746031744,0.04163453031302859
facebook/xglm-7.5B,hendrycksTest-global_facts,5-shot,accuracy,0.29,0.04560480215720684
facebook/xglm-7.5B,hendrycksTest-global_facts,5-shot,acc_norm,0.29,0.04560480215720684
facebook/xglm-7.5B,hendrycksTest-high_school_biology,5-shot,accuracy,0.33225806451612905,0.026795560848122797
facebook/xglm-7.5B,hendrycksTest-high_school_biology,5-shot,acc_norm,0.33225806451612905,0.026795560848122797
facebook/xglm-7.5B,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.3103448275862069,0.032550867699701024
facebook/xglm-7.5B,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.3103448275862069,0.032550867699701024
facebook/xglm-7.5B,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.28,0.045126085985421296
facebook/xglm-7.5B,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.28,0.045126085985421296
facebook/xglm-7.5B,hendrycksTest-high_school_european_history,5-shot,accuracy,0.2787878787878788,0.0350143870629678
facebook/xglm-7.5B,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.2787878787878788,0.0350143870629678
facebook/xglm-7.5B,hendrycksTest-high_school_geography,5-shot,accuracy,0.35858585858585856,0.03416903640391521
facebook/xglm-7.5B,hendrycksTest-high_school_geography,5-shot,acc_norm,0.35858585858585856,0.03416903640391521
facebook/xglm-7.5B,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.3626943005181347,0.03469713791704371
facebook/xglm-7.5B,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.3626943005181347,0.03469713791704371
facebook/xglm-7.5B,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3564102564102564,0.024283140529467295
facebook/xglm-7.5B,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3564102564102564,0.024283140529467295
facebook/xglm-7.5B,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24814814814814815,0.0263357394040558
facebook/xglm-7.5B,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24814814814814815,0.0263357394040558
facebook/xglm-7.5B,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3319327731092437,0.030588697013783663
facebook/xglm-7.5B,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3319327731092437,0.030588697013783663
facebook/xglm-7.5B,hendrycksTest-high_school_physics,5-shot,accuracy,0.2980132450331126,0.037345356767871984
facebook/xglm-7.5B,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2980132450331126,0.037345356767871984
facebook/xglm-7.5B,hendrycksTest-high_school_psychology,5-shot,accuracy,0.27706422018348625,0.019188482590169538
facebook/xglm-7.5B,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.27706422018348625,0.019188482590169538
facebook/xglm-7.5B,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4212962962962963,0.03367462138896078
facebook/xglm-7.5B,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4212962962962963,0.03367462138896078
facebook/xglm-7.5B,hendrycksTest-high_school_us_history,5-shot,accuracy,0.23529411764705882,0.02977177522814563
facebook/xglm-7.5B,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.23529411764705882,0.02977177522814563
facebook/xglm-7.5B,hendrycksTest-high_school_world_history,5-shot,accuracy,0.25738396624472576,0.028458820991460295
facebook/xglm-7.5B,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.25738396624472576,0.028458820991460295
facebook/xglm-7.5B,hendrycksTest-human_aging,5-shot,accuracy,0.3452914798206278,0.03191100192835794
facebook/xglm-7.5B,hendrycksTest-human_aging,5-shot,acc_norm,0.3452914798206278,0.03191100192835794
facebook/xglm-7.5B,hendrycksTest-human_sexuality,5-shot,accuracy,0.2366412213740458,0.0372767357559692
facebook/xglm-7.5B,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2366412213740458,0.0372767357559692
facebook/xglm-7.5B,hendrycksTest-international_law,5-shot,accuracy,0.36363636363636365,0.043913262867240704
facebook/xglm-7.5B,hendrycksTest-international_law,5-shot,acc_norm,0.36363636363636365,0.043913262867240704
facebook/xglm-7.5B,hendrycksTest-jurisprudence,5-shot,accuracy,0.21296296296296297,0.0395783547198098
facebook/xglm-7.5B,hendrycksTest-jurisprudence,5-shot,acc_norm,0.21296296296296297,0.0395783547198098
facebook/xglm-7.5B,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25766871165644173,0.03436150827846917
facebook/xglm-7.5B,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25766871165644173,0.03436150827846917
facebook/xglm-7.5B,hendrycksTest-machine_learning,5-shot,accuracy,0.22321428571428573,0.039523019677025116
facebook/xglm-7.5B,hendrycksTest-machine_learning,5-shot,acc_norm,0.22321428571428573,0.039523019677025116
facebook/xglm-7.5B,hendrycksTest-management,5-shot,accuracy,0.2621359223300971,0.043546310772605956
facebook/xglm-7.5B,hendrycksTest-management,5-shot,acc_norm,0.2621359223300971,0.043546310772605956
facebook/xglm-7.5B,hendrycksTest-marketing,5-shot,accuracy,0.1794871794871795,0.025140935950335428
facebook/xglm-7.5B,hendrycksTest-marketing,5-shot,acc_norm,0.1794871794871795,0.025140935950335428
facebook/xglm-7.5B,hendrycksTest-medical_genetics,5-shot,accuracy,0.31,0.04648231987117316
facebook/xglm-7.5B,hendrycksTest-medical_genetics,5-shot,acc_norm,0.31,0.04648231987117316
facebook/xglm-7.5B,hendrycksTest-miscellaneous,5-shot,accuracy,0.2554278416347382,0.015594955384455777
facebook/xglm-7.5B,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2554278416347382,0.015594955384455777
facebook/xglm-7.5B,hendrycksTest-moral_disputes,5-shot,accuracy,0.23121387283236994,0.022698657167855716
facebook/xglm-7.5B,hendrycksTest-moral_disputes,5-shot,acc_norm,0.23121387283236994,0.022698657167855716
facebook/xglm-7.5B,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
facebook/xglm-7.5B,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
facebook/xglm-7.5B,hendrycksTest-nutrition,5-shot,accuracy,0.2777777777777778,0.025646863097137897
facebook/xglm-7.5B,hendrycksTest-nutrition,5-shot,acc_norm,0.2777777777777778,0.025646863097137897
facebook/xglm-7.5B,hendrycksTest-philosophy,5-shot,accuracy,0.24437299035369775,0.024406162094668872
facebook/xglm-7.5B,hendrycksTest-philosophy,5-shot,acc_norm,0.24437299035369775,0.024406162094668872
facebook/xglm-7.5B,hendrycksTest-prehistory,5-shot,accuracy,0.27469135802469136,0.02483605786829468
facebook/xglm-7.5B,hendrycksTest-prehistory,5-shot,acc_norm,0.27469135802469136,0.02483605786829468
facebook/xglm-7.5B,hendrycksTest-professional_accounting,5-shot,accuracy,0.2624113475177305,0.026244920349843014
facebook/xglm-7.5B,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2624113475177305,0.026244920349843014
facebook/xglm-7.5B,hendrycksTest-professional_law,5-shot,accuracy,0.24445893089960888,0.010976425013113906
facebook/xglm-7.5B,hendrycksTest-professional_law,5-shot,acc_norm,0.24445893089960888,0.010976425013113906
facebook/xglm-7.5B,hendrycksTest-professional_medicine,5-shot,accuracy,0.4522058823529412,0.030233758551596452
facebook/xglm-7.5B,hendrycksTest-professional_medicine,5-shot,acc_norm,0.4522058823529412,0.030233758551596452
facebook/xglm-7.5B,hendrycksTest-professional_psychology,5-shot,accuracy,0.2696078431372549,0.017952449196987862
facebook/xglm-7.5B,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2696078431372549,0.017952449196987862
facebook/xglm-7.5B,hendrycksTest-public_relations,5-shot,accuracy,0.24545454545454545,0.041220665028782834
facebook/xglm-7.5B,hendrycksTest-public_relations,5-shot,acc_norm,0.24545454545454545,0.041220665028782834
facebook/xglm-7.5B,hendrycksTest-security_studies,5-shot,accuracy,0.2897959183673469,0.02904308868330435
facebook/xglm-7.5B,hendrycksTest-security_studies,5-shot,acc_norm,0.2897959183673469,0.02904308868330435
facebook/xglm-7.5B,hendrycksTest-sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409224
facebook/xglm-7.5B,hendrycksTest-sociology,5-shot,acc_norm,0.23880597014925373,0.030147775935409224
facebook/xglm-7.5B,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.24,0.042923469599092816
facebook/xglm-7.5B,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.24,0.042923469599092816
facebook/xglm-7.5B,hendrycksTest-virology,5-shot,accuracy,0.2891566265060241,0.03529486801511115
facebook/xglm-7.5B,hendrycksTest-virology,5-shot,acc_norm,0.2891566265060241,0.03529486801511115
facebook/xglm-7.5B,hendrycksTest-world_religions,5-shot,accuracy,0.2807017543859649,0.034462962170884265
facebook/xglm-7.5B,hendrycksTest-world_religions,5-shot,acc_norm,0.2807017543859649,0.034462962170884265
facebook/xglm-7.5B,truthfulqa:mc,0-shot,mc1,0.20930232558139536,0.014241219434785828
facebook/xglm-7.5B,truthfulqa:mc,0-shot,mc2,0.36661523882354585,0.013666875267077829
AbacusResearch/RasGulla1-7b,arc:challenge,25-shot,accuracy,0.659556313993174,0.013847460518892976
AbacusResearch/RasGulla1-7b,arc:challenge,25-shot,acc_norm,0.697098976109215,0.013428241573185349
AbacusResearch/RasGulla1-7b,hellaswag,10-shot,accuracy,0.6983668591913962,0.0045802887281959775
AbacusResearch/RasGulla1-7b,hellaswag,10-shot,acc_norm,0.8740290778729337,0.0033113844981586408
AbacusResearch/RasGulla1-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.37,0.04852365870939099
AbacusResearch/RasGulla1-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.37,0.04852365870939099
AbacusResearch/RasGulla1-7b,hendrycksTest-anatomy,5-shot,accuracy,0.6518518518518519,0.041153246103369526
AbacusResearch/RasGulla1-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.6518518518518519,0.041153246103369526
AbacusResearch/RasGulla1-7b,hendrycksTest-astronomy,5-shot,accuracy,0.6907894736842105,0.037610708698674805
AbacusResearch/RasGulla1-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.6907894736842105,0.037610708698674805
AbacusResearch/RasGulla1-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.61,0.04902071300001975
AbacusResearch/RasGulla1-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.61,0.04902071300001975
AbacusResearch/RasGulla1-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.7132075471698113,0.02783491252754406
AbacusResearch/RasGulla1-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.7132075471698113,0.02783491252754406
AbacusResearch/RasGulla1-7b,hendrycksTest-college_biology,5-shot,accuracy,0.7708333333333334,0.03514697467862388
AbacusResearch/RasGulla1-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.7708333333333334,0.03514697467862388
AbacusResearch/RasGulla1-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.47,0.050161355804659205
AbacusResearch/RasGulla1-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.47,0.050161355804659205
AbacusResearch/RasGulla1-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.57,0.04975698519562428
AbacusResearch/RasGulla1-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.57,0.04975698519562428
AbacusResearch/RasGulla1-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.31,0.04648231987117316
AbacusResearch/RasGulla1-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.31,0.04648231987117316
AbacusResearch/RasGulla1-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.6878612716763006,0.03533133389323657
AbacusResearch/RasGulla1-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.6878612716763006,0.03533133389323657
AbacusResearch/RasGulla1-7b,hendrycksTest-college_physics,5-shot,accuracy,0.4117647058823529,0.048971049527263666
AbacusResearch/RasGulla1-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.4117647058823529,0.048971049527263666
AbacusResearch/RasGulla1-7b,hendrycksTest-computer_security,5-shot,accuracy,0.77,0.04229525846816508
AbacusResearch/RasGulla1-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.77,0.04229525846816508
AbacusResearch/RasGulla1-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.5829787234042553,0.03223276266711712
AbacusResearch/RasGulla1-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.5829787234042553,0.03223276266711712
AbacusResearch/RasGulla1-7b,hendrycksTest-econometrics,5-shot,accuracy,0.47368421052631576,0.046970851366478626
AbacusResearch/RasGulla1-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.47368421052631576,0.046970851366478626
AbacusResearch/RasGulla1-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.5517241379310345,0.04144311810878152
AbacusResearch/RasGulla1-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.5517241379310345,0.04144311810878152
AbacusResearch/RasGulla1-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.4021164021164021,0.025253032554997692
AbacusResearch/RasGulla1-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.4021164021164021,0.025253032554997692
AbacusResearch/RasGulla1-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.49206349206349204,0.044715725362943486
AbacusResearch/RasGulla1-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.49206349206349204,0.044715725362943486
AbacusResearch/RasGulla1-7b,hendrycksTest-global_facts,5-shot,accuracy,0.32,0.04688261722621505
AbacusResearch/RasGulla1-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.32,0.04688261722621505
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.7774193548387097,0.023664216671642518
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.7774193548387097,0.023664216671642518
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.4827586206896552,0.035158955511656986
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.4827586206896552,0.035158955511656986
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.69,0.04648231987117316
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.69,0.04648231987117316
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.7818181818181819,0.03225078108306289
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.7818181818181819,0.03225078108306289
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.8080808080808081,0.028057791672989017
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.8080808080808081,0.028057791672989017
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.9015544041450777,0.021500249576033456
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.9015544041450777,0.021500249576033456
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.6666666666666666,0.023901157979402534
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.6666666666666666,0.023901157979402534
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.34814814814814815,0.029045600290616255
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.34814814814814815,0.029045600290616255
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.6680672268907563,0.03058869701378364
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.6680672268907563,0.03058869701378364
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.3443708609271523,0.038796870240733264
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.3443708609271523,0.038796870240733264
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.8477064220183487,0.015405084393157074
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.8477064220183487,0.015405084393157074
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.5046296296296297,0.03409825519163572
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.5046296296296297,0.03409825519163572
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.8529411764705882,0.024857478080250458
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.8529411764705882,0.024857478080250458
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.8143459915611815,0.025310495376944856
AbacusResearch/RasGulla1-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.8143459915611815,0.025310495376944856
AbacusResearch/RasGulla1-7b,hendrycksTest-human_aging,5-shot,accuracy,0.695067264573991,0.030898610882477515
AbacusResearch/RasGulla1-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.695067264573991,0.030898610882477515
AbacusResearch/RasGulla1-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.7938931297709924,0.03547771004159464
AbacusResearch/RasGulla1-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.7938931297709924,0.03547771004159464
AbacusResearch/RasGulla1-7b,hendrycksTest-international_law,5-shot,accuracy,0.7933884297520661,0.03695980128098824
AbacusResearch/RasGulla1-7b,hendrycksTest-international_law,5-shot,acc_norm,0.7933884297520661,0.03695980128098824
AbacusResearch/RasGulla1-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.8055555555555556,0.038260763248848646
AbacusResearch/RasGulla1-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.8055555555555556,0.038260763248848646
AbacusResearch/RasGulla1-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.7668711656441718,0.0332201579577674
AbacusResearch/RasGulla1-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.7668711656441718,0.0332201579577674
AbacusResearch/RasGulla1-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.42857142857142855,0.04697113923010212
AbacusResearch/RasGulla1-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.42857142857142855,0.04697113923010212
AbacusResearch/RasGulla1-7b,hendrycksTest-management,5-shot,accuracy,0.7864077669902912,0.040580420156460344
AbacusResearch/RasGulla1-7b,hendrycksTest-management,5-shot,acc_norm,0.7864077669902912,0.040580420156460344
AbacusResearch/RasGulla1-7b,hendrycksTest-marketing,5-shot,accuracy,0.8803418803418803,0.021262719400406957
AbacusResearch/RasGulla1-7b,hendrycksTest-marketing,5-shot,acc_norm,0.8803418803418803,0.021262719400406957
AbacusResearch/RasGulla1-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.71,0.045604802157206845
AbacusResearch/RasGulla1-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.71,0.045604802157206845
AbacusResearch/RasGulla1-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.8237547892720306,0.013625556907993462
AbacusResearch/RasGulla1-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.8237547892720306,0.013625556907993462
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.7485549132947977,0.02335736578587403
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.7485549132947977,0.02335736578587403
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.4581005586592179,0.016663683295020527
AbacusResearch/RasGulla1-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.4581005586592179,0.016663683295020527
AbacusResearch/RasGulla1-7b,hendrycksTest-nutrition,5-shot,accuracy,0.7320261437908496,0.025360603796242557
AbacusResearch/RasGulla1-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.7320261437908496,0.025360603796242557
AbacusResearch/RasGulla1-7b,hendrycksTest-philosophy,5-shot,accuracy,0.7138263665594855,0.025670259242188933
AbacusResearch/RasGulla1-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.7138263665594855,0.025670259242188933
AbacusResearch/RasGulla1-7b,hendrycksTest-prehistory,5-shot,accuracy,0.7314814814814815,0.024659685185967284
AbacusResearch/RasGulla1-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.7314814814814815,0.024659685185967284
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.4645390070921986,0.02975238965742705
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.4645390070921986,0.02975238965742705
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_law,5-shot,accuracy,0.46870925684485004,0.012745204626083133
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.46870925684485004,0.012745204626083133
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.6838235294117647,0.028245687391462923
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.6838235294117647,0.028245687391462923
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.6797385620915033,0.018875682938069443
AbacusResearch/RasGulla1-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.6797385620915033,0.018875682938069443
AbacusResearch/RasGulla1-7b,hendrycksTest-public_relations,5-shot,accuracy,0.6727272727272727,0.0449429086625209
AbacusResearch/RasGulla1-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.6727272727272727,0.0449429086625209
AbacusResearch/RasGulla1-7b,hendrycksTest-security_studies,5-shot,accuracy,0.7346938775510204,0.028263889943784593
AbacusResearch/RasGulla1-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.7346938775510204,0.028263889943784593
AbacusResearch/RasGulla1-7b,hendrycksTest-sociology,5-shot,accuracy,0.8557213930348259,0.024845753212306053
AbacusResearch/RasGulla1-7b,hendrycksTest-sociology,5-shot,acc_norm,0.8557213930348259,0.024845753212306053
AbacusResearch/RasGulla1-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.86,0.0348735088019777
AbacusResearch/RasGulla1-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.86,0.0348735088019777
AbacusResearch/RasGulla1-7b,hendrycksTest-virology,5-shot,accuracy,0.5481927710843374,0.03874371556587953
AbacusResearch/RasGulla1-7b,hendrycksTest-virology,5-shot,acc_norm,0.5481927710843374,0.03874371556587953
AbacusResearch/RasGulla1-7b,hendrycksTest-world_religions,5-shot,accuracy,0.8421052631578947,0.027966785859160893
AbacusResearch/RasGulla1-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.8421052631578947,0.027966785859160893
AbacusResearch/RasGulla1-7b,truthfulqa:mc,0-shot,mc1,0.4589963280293758,0.017444544447661192
AbacusResearch/RasGulla1-7b,truthfulqa:mc,0-shot,mc2,0.633141301315501,0.015119391252537636
AbacusResearch/RasGulla1-7b,winogrande,5-shot,accuracy,0.8089976322020521,0.011047808761510427
AbacusResearch/RasGulla1-7b,gsm8k,5-shot,accuracy,0.7172100075815011,0.012405020417873619
bigscience/bloom-7b1,drop,3-shot,accuracy,0.0009437919463087249,0.00031446531194131934
bigscience/bloom-7b1,drop,3-shot,f1,0.04796455536912761,0.0011701293326885863
bigscience/bloom-7b1,arc:challenge,25-shot,accuracy,0.3643344709897611,0.014063260279882412
bigscience/bloom-7b1,arc:challenge,25-shot,acc_norm,0.4112627986348123,0.01437944106852208
bigscience/bloom-7b1,hendrycksTest-abstract_algebra,5-shot,accuracy,0.26,0.04408440022768081
bigscience/bloom-7b1,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.26,0.04408440022768081
bigscience/bloom-7b1,hendrycksTest-anatomy,5-shot,accuracy,0.24444444444444444,0.03712537833614866
bigscience/bloom-7b1,hendrycksTest-anatomy,5-shot,acc_norm,0.24444444444444444,0.03712537833614866
bigscience/bloom-7b1,hendrycksTest-astronomy,5-shot,accuracy,0.17763157894736842,0.03110318238312338
bigscience/bloom-7b1,hendrycksTest-astronomy,5-shot,acc_norm,0.17763157894736842,0.03110318238312338
bigscience/bloom-7b1,hendrycksTest-business_ethics,5-shot,accuracy,0.26,0.04408440022768079
bigscience/bloom-7b1,hendrycksTest-business_ethics,5-shot,acc_norm,0.26,0.04408440022768079
bigscience/bloom-7b1,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.27547169811320754,0.027495663683724057
bigscience/bloom-7b1,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.27547169811320754,0.027495663683724057
bigscience/bloom-7b1,hendrycksTest-college_biology,5-shot,accuracy,0.19444444444444445,0.03309615177059004
bigscience/bloom-7b1,hendrycksTest-college_biology,5-shot,acc_norm,0.19444444444444445,0.03309615177059004
bigscience/bloom-7b1,hendrycksTest-college_chemistry,5-shot,accuracy,0.21,0.04093601807403326
bigscience/bloom-7b1,hendrycksTest-college_chemistry,5-shot,acc_norm,0.21,0.04093601807403326
bigscience/bloom-7b1,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
bigscience/bloom-7b1,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
bigscience/bloom-7b1,hendrycksTest-college_mathematics,5-shot,accuracy,0.32,0.04688261722621504
bigscience/bloom-7b1,hendrycksTest-college_mathematics,5-shot,acc_norm,0.32,0.04688261722621504
bigscience/bloom-7b1,hendrycksTest-college_medicine,5-shot,accuracy,0.20809248554913296,0.0309528902177499
bigscience/bloom-7b1,hendrycksTest-college_medicine,5-shot,acc_norm,0.20809248554913296,0.0309528902177499
bigscience/bloom-7b1,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617749
bigscience/bloom-7b1,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617749
bigscience/bloom-7b1,hendrycksTest-computer_security,5-shot,accuracy,0.23,0.04229525846816505
bigscience/bloom-7b1,hendrycksTest-computer_security,5-shot,acc_norm,0.23,0.04229525846816505
bigscience/bloom-7b1,hendrycksTest-conceptual_physics,5-shot,accuracy,0.32340425531914896,0.030579442773610334
bigscience/bloom-7b1,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.32340425531914896,0.030579442773610334
bigscience/bloom-7b1,hendrycksTest-econometrics,5-shot,accuracy,0.23684210526315788,0.039994238792813365
bigscience/bloom-7b1,hendrycksTest-econometrics,5-shot,acc_norm,0.23684210526315788,0.039994238792813365
bigscience/bloom-7b1,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2482758620689655,0.03600105692727771
bigscience/bloom-7b1,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2482758620689655,0.03600105692727771
bigscience/bloom-7b1,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2751322751322751,0.023000086859068646
bigscience/bloom-7b1,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2751322751322751,0.023000086859068646
bigscience/bloom-7b1,hendrycksTest-formal_logic,5-shot,accuracy,0.16666666666666666,0.033333333333333375
bigscience/bloom-7b1,hendrycksTest-formal_logic,5-shot,acc_norm,0.16666666666666666,0.033333333333333375
bigscience/bloom-7b1,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.04725815626252604
bigscience/bloom-7b1,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.04725815626252604
bigscience/bloom-7b1,hendrycksTest-high_school_biology,5-shot,accuracy,0.25483870967741934,0.024790118459332208
bigscience/bloom-7b1,hendrycksTest-high_school_biology,5-shot,acc_norm,0.25483870967741934,0.024790118459332208
bigscience/bloom-7b1,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.270935960591133,0.031270907132976984
bigscience/bloom-7b1,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.270935960591133,0.031270907132976984
bigscience/bloom-7b1,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.35,0.0479372485441102
bigscience/bloom-7b1,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
bigscience/bloom-7b1,hendrycksTest-high_school_european_history,5-shot,accuracy,0.26666666666666666,0.03453131801885415
bigscience/bloom-7b1,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.26666666666666666,0.03453131801885415
bigscience/bloom-7b1,hendrycksTest-high_school_geography,5-shot,accuracy,0.22727272727272727,0.0298575156733864
bigscience/bloom-7b1,hendrycksTest-high_school_geography,5-shot,acc_norm,0.22727272727272727,0.0298575156733864
bigscience/bloom-7b1,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.20725388601036268,0.02925282329180362
bigscience/bloom-7b1,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.20725388601036268,0.02925282329180362
bigscience/bloom-7b1,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.23076923076923078,0.02136202772522271
bigscience/bloom-7b1,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.23076923076923078,0.02136202772522271
bigscience/bloom-7b1,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.26666666666666666,0.026962424325073852
bigscience/bloom-7b1,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.26666666666666666,0.026962424325073852
bigscience/bloom-7b1,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.226890756302521,0.02720537153827948
bigscience/bloom-7b1,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.226890756302521,0.02720537153827948
bigscience/bloom-7b1,hendrycksTest-high_school_physics,5-shot,accuracy,0.23841059602649006,0.034791855725996586
bigscience/bloom-7b1,hendrycksTest-high_school_physics,5-shot,acc_norm,0.23841059602649006,0.034791855725996586
bigscience/bloom-7b1,hendrycksTest-high_school_psychology,5-shot,accuracy,0.24770642201834864,0.018508143602547808
bigscience/bloom-7b1,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.24770642201834864,0.018508143602547808
bigscience/bloom-7b1,hendrycksTest-high_school_statistics,5-shot,accuracy,0.38425925925925924,0.03317354514310742
bigscience/bloom-7b1,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.38425925925925924,0.03317354514310742
bigscience/bloom-7b1,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2647058823529412,0.030964517926923403
bigscience/bloom-7b1,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2647058823529412,0.030964517926923403
bigscience/bloom-7b1,hendrycksTest-high_school_world_history,5-shot,accuracy,0.26582278481012656,0.028756799629658335
bigscience/bloom-7b1,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.26582278481012656,0.028756799629658335
bigscience/bloom-7b1,hendrycksTest-human_aging,5-shot,accuracy,0.3632286995515695,0.03227790442850499
bigscience/bloom-7b1,hendrycksTest-human_aging,5-shot,acc_norm,0.3632286995515695,0.03227790442850499
bigscience/bloom-7b1,hendrycksTest-human_sexuality,5-shot,accuracy,0.22900763358778625,0.036853466317118506
bigscience/bloom-7b1,hendrycksTest-human_sexuality,5-shot,acc_norm,0.22900763358778625,0.036853466317118506
bigscience/bloom-7b1,hendrycksTest-international_law,5-shot,accuracy,0.36363636363636365,0.043913262867240704
bigscience/bloom-7b1,hendrycksTest-international_law,5-shot,acc_norm,0.36363636363636365,0.043913262867240704
bigscience/bloom-7b1,hendrycksTest-jurisprudence,5-shot,accuracy,0.28703703703703703,0.043733130409147614
bigscience/bloom-7b1,hendrycksTest-jurisprudence,5-shot,acc_norm,0.28703703703703703,0.043733130409147614
bigscience/bloom-7b1,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25766871165644173,0.03436150827846917
bigscience/bloom-7b1,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25766871165644173,0.03436150827846917
bigscience/bloom-7b1,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.0432704093257873
bigscience/bloom-7b1,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.0432704093257873
bigscience/bloom-7b1,hendrycksTest-management,5-shot,accuracy,0.2524271844660194,0.04301250399690877
bigscience/bloom-7b1,hendrycksTest-management,5-shot,acc_norm,0.2524271844660194,0.04301250399690877
bigscience/bloom-7b1,hendrycksTest-marketing,5-shot,accuracy,0.25213675213675213,0.02844796547623101
bigscience/bloom-7b1,hendrycksTest-marketing,5-shot,acc_norm,0.25213675213675213,0.02844796547623101
bigscience/bloom-7b1,hendrycksTest-medical_genetics,5-shot,accuracy,0.26,0.044084400227680794
bigscience/bloom-7b1,hendrycksTest-medical_genetics,5-shot,acc_norm,0.26,0.044084400227680794
bigscience/bloom-7b1,hendrycksTest-miscellaneous,5-shot,accuracy,0.2886334610472541,0.016203792703197804
bigscience/bloom-7b1,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2886334610472541,0.016203792703197804
bigscience/bloom-7b1,hendrycksTest-moral_disputes,5-shot,accuracy,0.24566473988439305,0.02317629820399201
bigscience/bloom-7b1,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24566473988439305,0.02317629820399201
bigscience/bloom-7b1,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2424581005586592,0.014333522059217889
bigscience/bloom-7b1,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2424581005586592,0.014333522059217889
bigscience/bloom-7b1,hendrycksTest-nutrition,5-shot,accuracy,0.23202614379084968,0.02417084087934101
bigscience/bloom-7b1,hendrycksTest-nutrition,5-shot,acc_norm,0.23202614379084968,0.02417084087934101
bigscience/bloom-7b1,hendrycksTest-philosophy,5-shot,accuracy,0.2733118971061093,0.02531176597542612
bigscience/bloom-7b1,hendrycksTest-philosophy,5-shot,acc_norm,0.2733118971061093,0.02531176597542612
bigscience/bloom-7b1,hendrycksTest-prehistory,5-shot,accuracy,0.2623456790123457,0.02447722285613511
bigscience/bloom-7b1,hendrycksTest-prehistory,5-shot,acc_norm,0.2623456790123457,0.02447722285613511
bigscience/bloom-7b1,hendrycksTest-professional_accounting,5-shot,accuracy,0.26595744680851063,0.02635806569888059
bigscience/bloom-7b1,hendrycksTest-professional_accounting,5-shot,acc_norm,0.26595744680851063,0.02635806569888059
bigscience/bloom-7b1,hendrycksTest-professional_law,5-shot,accuracy,0.2529335071707953,0.011102268713839987
bigscience/bloom-7b1,hendrycksTest-professional_law,5-shot,acc_norm,0.2529335071707953,0.011102268713839987
bigscience/bloom-7b1,hendrycksTest-professional_medicine,5-shot,accuracy,0.20955882352941177,0.024723110407677055
bigscience/bloom-7b1,hendrycksTest-professional_medicine,5-shot,acc_norm,0.20955882352941177,0.024723110407677055
bigscience/bloom-7b1,hendrycksTest-professional_psychology,5-shot,accuracy,0.26143790849673204,0.017776947157528034
bigscience/bloom-7b1,hendrycksTest-professional_psychology,5-shot,acc_norm,0.26143790849673204,0.017776947157528034
bigscience/bloom-7b1,hendrycksTest-public_relations,5-shot,accuracy,0.32727272727272727,0.0449429086625209
bigscience/bloom-7b1,hendrycksTest-public_relations,5-shot,acc_norm,0.32727272727272727,0.0449429086625209
bigscience/bloom-7b1,hendrycksTest-security_studies,5-shot,accuracy,0.3020408163265306,0.02939360931987981
bigscience/bloom-7b1,hendrycksTest-security_studies,5-shot,acc_norm,0.3020408163265306,0.02939360931987981
bigscience/bloom-7b1,hendrycksTest-sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409224
bigscience/bloom-7b1,hendrycksTest-sociology,5-shot,acc_norm,0.23880597014925373,0.030147775935409224
bigscience/bloom-7b1,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.21,0.040936018074033256
bigscience/bloom-7b1,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.21,0.040936018074033256
bigscience/bloom-7b1,hendrycksTest-virology,5-shot,accuracy,0.3072289156626506,0.035915667978246635
bigscience/bloom-7b1,hendrycksTest-virology,5-shot,acc_norm,0.3072289156626506,0.035915667978246635
bigscience/bloom-7b1,hendrycksTest-world_religions,5-shot,accuracy,0.29239766081871343,0.03488647713457922
bigscience/bloom-7b1,hendrycksTest-world_religions,5-shot,acc_norm,0.29239766081871343,0.03488647713457922
bigscience/bloom-7b1,truthfulqa:mc,0-shot,mc1,0.22399020807833536,0.014594964329474202
bigscience/bloom-7b1,truthfulqa:mc,0-shot,mc2,0.38897842190357873,0.014015753482036425
mosaicml/mpt-7b,drop,3-shot,accuracy,0.0006291946308724832,0.0002568002749724036
mosaicml/mpt-7b,drop,3-shot,f1,0.055483431208053824,0.0012896726370180557
mosaicml/mpt-7b,arc:challenge,25-shot,accuracy,0.42918088737201365,0.014464085894870653
mosaicml/mpt-7b,arc:challenge,25-shot,acc_norm,0.47696245733788395,0.014595873205358269
mosaicml/mpt-7b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.19,0.03942772444036624
mosaicml/mpt-7b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.19,0.03942772444036624
mosaicml/mpt-7b,hendrycksTest-anatomy,5-shot,accuracy,0.2222222222222222,0.035914440841969694
mosaicml/mpt-7b,hendrycksTest-anatomy,5-shot,acc_norm,0.2222222222222222,0.035914440841969694
mosaicml/mpt-7b,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.03583496176361062
mosaicml/mpt-7b,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.03583496176361062
mosaicml/mpt-7b,hendrycksTest-business_ethics,5-shot,accuracy,0.33,0.04725815626252604
mosaicml/mpt-7b,hendrycksTest-business_ethics,5-shot,acc_norm,0.33,0.04725815626252604
mosaicml/mpt-7b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.28679245283018867,0.027834912527544067
mosaicml/mpt-7b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.28679245283018867,0.027834912527544067
mosaicml/mpt-7b,hendrycksTest-college_biology,5-shot,accuracy,0.2847222222222222,0.03773809990686935
mosaicml/mpt-7b,hendrycksTest-college_biology,5-shot,acc_norm,0.2847222222222222,0.03773809990686935
mosaicml/mpt-7b,hendrycksTest-college_chemistry,5-shot,accuracy,0.25,0.04351941398892446
mosaicml/mpt-7b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.25,0.04351941398892446
mosaicml/mpt-7b,hendrycksTest-college_computer_science,5-shot,accuracy,0.32,0.046882617226215034
mosaicml/mpt-7b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
mosaicml/mpt-7b,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
mosaicml/mpt-7b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.046056618647183814
mosaicml/mpt-7b,hendrycksTest-college_medicine,5-shot,accuracy,0.2658959537572254,0.033687629322594295
mosaicml/mpt-7b,hendrycksTest-college_medicine,5-shot,acc_norm,0.2658959537572254,0.033687629322594295
mosaicml/mpt-7b,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617747
mosaicml/mpt-7b,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617747
mosaicml/mpt-7b,hendrycksTest-computer_security,5-shot,accuracy,0.29,0.045604802157206845
mosaicml/mpt-7b,hendrycksTest-computer_security,5-shot,acc_norm,0.29,0.045604802157206845
mosaicml/mpt-7b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3404255319148936,0.03097669299853442
mosaicml/mpt-7b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3404255319148936,0.03097669299853442
mosaicml/mpt-7b,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.04266339443159394
mosaicml/mpt-7b,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.04266339443159394
mosaicml/mpt-7b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2620689655172414,0.036646663372252565
mosaicml/mpt-7b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2620689655172414,0.036646663372252565
mosaicml/mpt-7b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.23809523809523808,0.021935878081184763
mosaicml/mpt-7b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.23809523809523808,0.021935878081184763
mosaicml/mpt-7b,hendrycksTest-formal_logic,5-shot,accuracy,0.23015873015873015,0.03764950879790605
mosaicml/mpt-7b,hendrycksTest-formal_logic,5-shot,acc_norm,0.23015873015873015,0.03764950879790605
mosaicml/mpt-7b,hendrycksTest-global_facts,5-shot,accuracy,0.23,0.04229525846816505
mosaicml/mpt-7b,hendrycksTest-global_facts,5-shot,acc_norm,0.23,0.04229525846816505
mosaicml/mpt-7b,hendrycksTest-high_school_biology,5-shot,accuracy,0.25161290322580643,0.024685979286239952
mosaicml/mpt-7b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.25161290322580643,0.024685979286239952
mosaicml/mpt-7b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.20689655172413793,0.02850137816789395
mosaicml/mpt-7b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.20689655172413793,0.02850137816789395
mosaicml/mpt-7b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.04725815626252606
mosaicml/mpt-7b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.04725815626252606
mosaicml/mpt-7b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.24242424242424243,0.033464098810559534
mosaicml/mpt-7b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.24242424242424243,0.033464098810559534
mosaicml/mpt-7b,hendrycksTest-high_school_geography,5-shot,accuracy,0.2222222222222222,0.02962022787479047
mosaicml/mpt-7b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.2222222222222222,0.02962022787479047
mosaicml/mpt-7b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.35751295336787564,0.03458816042181006
mosaicml/mpt-7b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.35751295336787564,0.03458816042181006
mosaicml/mpt-7b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.32051282051282054,0.02366129639396427
mosaicml/mpt-7b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.32051282051282054,0.02366129639396427
mosaicml/mpt-7b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2851851851851852,0.027528599210340496
mosaicml/mpt-7b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2851851851851852,0.027528599210340496
mosaicml/mpt-7b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.28991596638655465,0.029472485833136098
mosaicml/mpt-7b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.28991596638655465,0.029472485833136098
mosaicml/mpt-7b,hendrycksTest-high_school_physics,5-shot,accuracy,0.26490066225165565,0.03603038545360384
mosaicml/mpt-7b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.26490066225165565,0.03603038545360384
mosaicml/mpt-7b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.26055045871559634,0.018819182034850068
mosaicml/mpt-7b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.26055045871559634,0.018819182034850068
mosaicml/mpt-7b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.3055555555555556,0.03141554629402544
mosaicml/mpt-7b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.3055555555555556,0.03141554629402544
mosaicml/mpt-7b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.2549019607843137,0.030587591351604257
mosaicml/mpt-7b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.2549019607843137,0.030587591351604257
mosaicml/mpt-7b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.028900721906293426
mosaicml/mpt-7b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.028900721906293426
mosaicml/mpt-7b,hendrycksTest-human_aging,5-shot,accuracy,0.3094170403587444,0.031024411740572203
mosaicml/mpt-7b,hendrycksTest-human_aging,5-shot,acc_norm,0.3094170403587444,0.031024411740572203
mosaicml/mpt-7b,hendrycksTest-human_sexuality,5-shot,accuracy,0.29770992366412213,0.04010358942462203
mosaicml/mpt-7b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.29770992366412213,0.04010358942462203
mosaicml/mpt-7b,hendrycksTest-international_law,5-shot,accuracy,0.2975206611570248,0.04173349148083498
mosaicml/mpt-7b,hendrycksTest-international_law,5-shot,acc_norm,0.2975206611570248,0.04173349148083498
mosaicml/mpt-7b,hendrycksTest-jurisprudence,5-shot,accuracy,0.2962962962962963,0.044143436668549335
mosaicml/mpt-7b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.2962962962962963,0.044143436668549335
mosaicml/mpt-7b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.25153374233128833,0.034089978868575295
mosaicml/mpt-7b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.25153374233128833,0.034089978868575295
mosaicml/mpt-7b,hendrycksTest-machine_learning,5-shot,accuracy,0.36607142857142855,0.0457237235873743
mosaicml/mpt-7b,hendrycksTest-machine_learning,5-shot,acc_norm,0.36607142857142855,0.0457237235873743
mosaicml/mpt-7b,hendrycksTest-management,5-shot,accuracy,0.23300970873786409,0.041858325989283136
mosaicml/mpt-7b,hendrycksTest-management,5-shot,acc_norm,0.23300970873786409,0.041858325989283136
mosaicml/mpt-7b,hendrycksTest-marketing,5-shot,accuracy,0.32051282051282054,0.03057281131029961
mosaicml/mpt-7b,hendrycksTest-marketing,5-shot,acc_norm,0.32051282051282054,0.03057281131029961
mosaicml/mpt-7b,hendrycksTest-medical_genetics,5-shot,accuracy,0.35,0.047937248544110196
mosaicml/mpt-7b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.35,0.047937248544110196
mosaicml/mpt-7b,hendrycksTest-miscellaneous,5-shot,accuracy,0.3001277139208174,0.016389249691317425
mosaicml/mpt-7b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.3001277139208174,0.016389249691317425
mosaicml/mpt-7b,hendrycksTest-moral_disputes,5-shot,accuracy,0.2630057803468208,0.023703099525258172
mosaicml/mpt-7b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.2630057803468208,0.023703099525258172
mosaicml/mpt-7b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2446927374301676,0.014378169884098423
mosaicml/mpt-7b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2446927374301676,0.014378169884098423
mosaicml/mpt-7b,hendrycksTest-nutrition,5-shot,accuracy,0.28104575163398693,0.025738854797818726
mosaicml/mpt-7b,hendrycksTest-nutrition,5-shot,acc_norm,0.28104575163398693,0.025738854797818726
mosaicml/mpt-7b,hendrycksTest-philosophy,5-shot,accuracy,0.2990353697749196,0.02600330111788513
mosaicml/mpt-7b,hendrycksTest-philosophy,5-shot,acc_norm,0.2990353697749196,0.02600330111788513
mosaicml/mpt-7b,hendrycksTest-prehistory,5-shot,accuracy,0.32098765432098764,0.025976566010862737
mosaicml/mpt-7b,hendrycksTest-prehistory,5-shot,acc_norm,0.32098765432098764,0.025976566010862737
mosaicml/mpt-7b,hendrycksTest-professional_accounting,5-shot,accuracy,0.24822695035460993,0.025770015644290392
mosaicml/mpt-7b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.24822695035460993,0.025770015644290392
mosaicml/mpt-7b,hendrycksTest-professional_law,5-shot,accuracy,0.2607561929595828,0.011213471559602325
mosaicml/mpt-7b,hendrycksTest-professional_law,5-shot,acc_norm,0.2607561929595828,0.011213471559602325
mosaicml/mpt-7b,hendrycksTest-professional_medicine,5-shot,accuracy,0.1948529411764706,0.024060599423487414
mosaicml/mpt-7b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1948529411764706,0.024060599423487414
mosaicml/mpt-7b,hendrycksTest-professional_psychology,5-shot,accuracy,0.25980392156862747,0.017740899509177788
mosaicml/mpt-7b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25980392156862747,0.017740899509177788
mosaicml/mpt-7b,hendrycksTest-public_relations,5-shot,accuracy,0.33636363636363636,0.04525393596302505
mosaicml/mpt-7b,hendrycksTest-public_relations,5-shot,acc_norm,0.33636363636363636,0.04525393596302505
mosaicml/mpt-7b,hendrycksTest-security_studies,5-shot,accuracy,0.3020408163265306,0.029393609319879818
mosaicml/mpt-7b,hendrycksTest-security_studies,5-shot,acc_norm,0.3020408163265306,0.029393609319879818
mosaicml/mpt-7b,hendrycksTest-sociology,5-shot,accuracy,0.23383084577114427,0.029929415408348384
mosaicml/mpt-7b,hendrycksTest-sociology,5-shot,acc_norm,0.23383084577114427,0.029929415408348384
mosaicml/mpt-7b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.4,0.04923659639173309
mosaicml/mpt-7b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.4,0.04923659639173309
mosaicml/mpt-7b,hendrycksTest-virology,5-shot,accuracy,0.3493975903614458,0.03711725190740749
mosaicml/mpt-7b,hendrycksTest-virology,5-shot,acc_norm,0.3493975903614458,0.03711725190740749
mosaicml/mpt-7b,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.03582529442573122
mosaicml/mpt-7b,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.03582529442573122
mosaicml/mpt-7b,truthfulqa:mc,0-shot,mc1,0.20563035495716034,0.014148482219460974
mosaicml/mpt-7b,truthfulqa:mc,0-shot,mc2,0.3354506043570123,0.013110323313593984
facebook/opt-6.7b,minerva_math_precalc,5-shot,accuracy,0.003663003663003663,0.002587757368193444
facebook/opt-6.7b,minerva_math_prealgebra,5-shot,accuracy,0.010332950631458095,0.0034284443646836666
facebook/opt-6.7b,minerva_math_num_theory,5-shot,accuracy,0.005555555555555556,0.0032015451273208887
facebook/opt-6.7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.004429678848283499,0.0022111531423787863
facebook/opt-6.7b,minerva_math_geometry,5-shot,accuracy,0.010438413361169102,0.004648627117184668
facebook/opt-6.7b,minerva_math_counting_and_prob,5-shot,accuracy,0.008438818565400843,0.004206007207713056
facebook/opt-6.7b,minerva_math_algebra,5-shot,accuracy,0.009267059814658803,0.002782319118488812
facebook/opt-6.7b,fld_default,0-shot,accuracy,0.0,
facebook/opt-6.7b,fld_star,0-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521483
facebook/opt-6.7b,arithmetic_3ds,5-shot,accuracy,0.002,0.0009992493430694893
facebook/opt-6.7b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
facebook/opt-6.7b,arithmetic_2ds,5-shot,accuracy,0.012,0.0024353573624298335
facebook/opt-6.7b,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_1dc,5-shot,accuracy,0.0205,0.00316936861988699
facebook/opt-6.7b,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-6.7b,arithmetic_2dm,5-shot,accuracy,0.036,0.004166614973833173
facebook/opt-6.7b,arithmetic_2da,5-shot,accuracy,0.0055,0.0016541593398342208
facebook/opt-6.7b,gsm8k_cot,5-shot,accuracy,0.02880970432145565,0.004607484283767461
facebook/opt-6.7b,mmlu_world_religions,0-shot,accuracy,0.2222222222222222,0.03188578017686398
facebook/opt-6.7b,mmlu_formal_logic,0-shot,accuracy,0.16666666666666666,0.03333333333333337
facebook/opt-6.7b,mmlu_prehistory,0-shot,accuracy,0.27469135802469136,0.024836057868294674
facebook/opt-6.7b,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
facebook/opt-6.7b,mmlu_high_school_world_history,0-shot,accuracy,0.25738396624472576,0.0284588209914603
facebook/opt-6.7b,mmlu_moral_disputes,0-shot,accuracy,0.23699421965317918,0.02289408248992599
facebook/opt-6.7b,mmlu_professional_law,0-shot,accuracy,0.23533246414602346,0.010834432543912238
facebook/opt-6.7b,mmlu_logical_fallacies,0-shot,accuracy,0.24539877300613497,0.03380939813943354
facebook/opt-6.7b,mmlu_high_school_us_history,0-shot,accuracy,0.24019607843137256,0.02998373305591361
facebook/opt-6.7b,mmlu_philosophy,0-shot,accuracy,0.2282958199356913,0.023839303311398215
facebook/opt-6.7b,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
facebook/opt-6.7b,mmlu_international_law,0-shot,accuracy,0.256198347107438,0.03984979653302872
facebook/opt-6.7b,mmlu_high_school_european_history,0-shot,accuracy,0.24242424242424243,0.033464098810559534
facebook/opt-6.7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.23316062176165803,0.03051611137147601
facebook/opt-6.7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.1722689075630252,0.024528664971305417
facebook/opt-6.7b,mmlu_high_school_geography,0-shot,accuracy,0.19696969696969696,0.02833560973246335
facebook/opt-6.7b,mmlu_high_school_psychology,0-shot,accuracy,0.26972477064220185,0.019028486711115452
facebook/opt-6.7b,mmlu_public_relations,0-shot,accuracy,0.32727272727272727,0.0449429086625209
facebook/opt-6.7b,mmlu_us_foreign_policy,0-shot,accuracy,0.22,0.041633319989322695
facebook/opt-6.7b,mmlu_sociology,0-shot,accuracy,0.23383084577114427,0.029929415408348377
facebook/opt-6.7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.24358974358974358,0.021763733684173926
facebook/opt-6.7b,mmlu_security_studies,0-shot,accuracy,0.19183673469387755,0.025206963154225392
facebook/opt-6.7b,mmlu_professional_psychology,0-shot,accuracy,0.25163398692810457,0.017555818091322267
facebook/opt-6.7b,mmlu_human_sexuality,0-shot,accuracy,0.22137404580152673,0.0364129708131373
facebook/opt-6.7b,mmlu_econometrics,0-shot,accuracy,0.23684210526315788,0.039994238792813344
facebook/opt-6.7b,mmlu_miscellaneous,0-shot,accuracy,0.2835249042145594,0.016117318166832276
facebook/opt-6.7b,mmlu_marketing,0-shot,accuracy,0.2948717948717949,0.02987257770889117
facebook/opt-6.7b,mmlu_management,0-shot,accuracy,0.24271844660194175,0.04245022486384495
facebook/opt-6.7b,mmlu_nutrition,0-shot,accuracy,0.2222222222222222,0.023805186524888135
facebook/opt-6.7b,mmlu_medical_genetics,0-shot,accuracy,0.32,0.046882617226215034
facebook/opt-6.7b,mmlu_human_aging,0-shot,accuracy,0.3542600896860987,0.032100621541349864
facebook/opt-6.7b,mmlu_professional_medicine,0-shot,accuracy,0.2647058823529412,0.02679956202488768
facebook/opt-6.7b,mmlu_college_medicine,0-shot,accuracy,0.2543352601156069,0.0332055644308557
facebook/opt-6.7b,mmlu_business_ethics,0-shot,accuracy,0.2,0.04020151261036845
facebook/opt-6.7b,mmlu_clinical_knowledge,0-shot,accuracy,0.18867924528301888,0.02407999513006224
facebook/opt-6.7b,mmlu_global_facts,0-shot,accuracy,0.29,0.045604802157206845
facebook/opt-6.7b,mmlu_virology,0-shot,accuracy,0.3313253012048193,0.03664314777288086
facebook/opt-6.7b,mmlu_professional_accounting,0-shot,accuracy,0.30141843971631205,0.027374128882631153
facebook/opt-6.7b,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237655
facebook/opt-6.7b,mmlu_high_school_physics,0-shot,accuracy,0.16556291390728478,0.03034818341030361
facebook/opt-6.7b,mmlu_high_school_biology,0-shot,accuracy,0.23225806451612904,0.02402225613030824
facebook/opt-6.7b,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.03476590104304134
facebook/opt-6.7b,mmlu_anatomy,0-shot,accuracy,0.3333333333333333,0.04072314811876837
facebook/opt-6.7b,mmlu_college_chemistry,0-shot,accuracy,0.24,0.042923469599092816
facebook/opt-6.7b,mmlu_computer_security,0-shot,accuracy,0.24,0.04292346959909283
facebook/opt-6.7b,mmlu_college_computer_science,0-shot,accuracy,0.3,0.046056618647183814
facebook/opt-6.7b,mmlu_astronomy,0-shot,accuracy,0.15789473684210525,0.029674167520101446
facebook/opt-6.7b,mmlu_college_mathematics,0-shot,accuracy,0.3,0.046056618647183814
facebook/opt-6.7b,mmlu_conceptual_physics,0-shot,accuracy,0.2553191489361702,0.02850485647051418
facebook/opt-6.7b,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909283
facebook/opt-6.7b,mmlu_high_school_computer_science,0-shot,accuracy,0.24,0.042923469599092816
facebook/opt-6.7b,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.04327040932578728
facebook/opt-6.7b,mmlu_high_school_chemistry,0-shot,accuracy,0.2512315270935961,0.030516530732694436
facebook/opt-6.7b,mmlu_high_school_statistics,0-shot,accuracy,0.17592592592592593,0.025967420958258526
facebook/opt-6.7b,mmlu_elementary_mathematics,0-shot,accuracy,0.24074074074074073,0.02201908001221789
facebook/opt-6.7b,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.03752833958003337
facebook/opt-6.7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2740740740740741,0.027195934804085626
facebook/opt-6.7b,arc_challenge,25-shot,accuracy,0.35665529010238906,0.013998056902620199
facebook/opt-6.7b,arc_challenge,25-shot,acc_norm,0.3856655290102389,0.014224250973257175
facebook/opt-6.7b,truthfulqa_mc2,0-shot,accuracy,0.35095596246578176,0.013568219502617192
facebook/opt-6.7b,truthfulqa_gen,0-shot,bleu_max,23.707553429599244,0.7450122059703083
facebook/opt-6.7b,truthfulqa_gen,0-shot,bleu_acc,0.2876376988984088,0.015846315101394785
facebook/opt-6.7b,truthfulqa_gen,0-shot,bleu_diff,-8.638098070838828,0.8005088298726831
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge1_max,48.60974313039175,0.858316293921586
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge1_acc,0.2594859241126071,0.015345409485558008
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge1_diff,-11.045582261019915,0.8882795627003046
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge2_max,31.52878460707801,0.9802016635484709
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge2_acc,0.20195838433292534,0.014053957441512376
facebook/opt-6.7b,truthfulqa_gen,0-shot,rouge2_diff,-13.46757758334418,1.035259887092465
facebook/opt-6.7b,truthfulqa_gen,0-shot,rougeL_max,45.74410864668455,0.8676424195118835
facebook/opt-6.7b,truthfulqa_gen,0-shot,rougeL_acc,0.2594859241126071,0.015345409485558011
facebook/opt-6.7b,truthfulqa_gen,0-shot,rougeL_diff,-11.266372071748116,0.8999068306781938
facebook/opt-6.7b,truthfulqa_mc1,0-shot,accuracy,0.2178702570379437,0.014450846714123899
AbacusResearch/haLLAwa2,minerva_math_precalc,5-shot,accuracy,0.06043956043956044,0.010207626216646904
AbacusResearch/haLLAwa2,minerva_math_prealgebra,5-shot,accuracy,0.3099885189437428,0.015679829530316276
AbacusResearch/haLLAwa2,minerva_math_num_theory,5-shot,accuracy,0.08888888888888889,0.012257870465567294
AbacusResearch/haLLAwa2,minerva_math_intermediate_algebra,5-shot,accuracy,0.07641196013289037,0.008845381151645904
AbacusResearch/haLLAwa2,minerva_math_geometry,5-shot,accuracy,0.1336116910229645,0.015561969995340364
AbacusResearch/haLLAwa2,minerva_math_counting_and_prob,5-shot,accuracy,0.13924050632911392,0.01591816995536744
AbacusResearch/haLLAwa2,minerva_math_algebra,5-shot,accuracy,0.22662173546756528,0.012156384192199525
AbacusResearch/haLLAwa2,fld_default,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa2,fld_star,0-shot,accuracy,0.0,
AbacusResearch/haLLAwa2,arithmetic_3da,5-shot,accuracy,0.9685,0.003906597720891784
AbacusResearch/haLLAwa2,arithmetic_3ds,5-shot,accuracy,0.97,0.003815400193861728
AbacusResearch/haLLAwa2,arithmetic_4da,5-shot,accuracy,0.9135,0.006287180554084638
AbacusResearch/haLLAwa2,arithmetic_2ds,5-shot,accuracy,0.9955,0.0014969954902233223
AbacusResearch/haLLAwa2,arithmetic_5ds,5-shot,accuracy,0.8495,0.00799730288451757
AbacusResearch/haLLAwa2,arithmetic_5da,5-shot,accuracy,0.8895,0.007012093819243022
AbacusResearch/haLLAwa2,arithmetic_1dc,5-shot,accuracy,0.6425,0.010719343597608066
AbacusResearch/haLLAwa2,arithmetic_4ds,5-shot,accuracy,0.9125,0.006319956164639155
AbacusResearch/haLLAwa2,arithmetic_2dm,5-shot,accuracy,0.617,0.010872654105766934
AbacusResearch/haLLAwa2,arithmetic_2da,5-shot,accuracy,0.986,0.0026278228110667933
AbacusResearch/haLLAwa2,gsm8k_cot,5-shot,accuracy,0.5701288855193328,0.013636344017393732
AbacusResearch/haLLAwa2,gsm8k,5-shot,accuracy,0.5276724791508719,0.013751375538801326
AbacusResearch/haLLAwa2,anli_r2,0-shot,brier_score,1.0924707676244783,
AbacusResearch/haLLAwa2,anli_r3,0-shot,brier_score,1.0236139706947502,
AbacusResearch/haLLAwa2,anli_r1,0-shot,brier_score,1.0837828290354423,
AbacusResearch/haLLAwa2,xnli_eu,0-shot,brier_score,1.1439712925050591,
AbacusResearch/haLLAwa2,xnli_vi,0-shot,brier_score,1.0246823649930676,
AbacusResearch/haLLAwa2,xnli_ru,0-shot,brier_score,0.9703116231053286,
AbacusResearch/haLLAwa2,xnli_zh,0-shot,brier_score,1.2234280607922878,
AbacusResearch/haLLAwa2,xnli_tr,0-shot,brier_score,1.0282731990983742,
AbacusResearch/haLLAwa2,xnli_fr,0-shot,brier_score,0.9242344176415858,
AbacusResearch/haLLAwa2,xnli_en,0-shot,brier_score,0.6881897992605256,
AbacusResearch/haLLAwa2,xnli_ur,0-shot,brier_score,1.2513617048946404,
AbacusResearch/haLLAwa2,xnli_ar,0-shot,brier_score,1.305527995518514,
AbacusResearch/haLLAwa2,xnli_de,0-shot,brier_score,1.018285594258703,
AbacusResearch/haLLAwa2,xnli_hi,0-shot,brier_score,1.023238689228211,
AbacusResearch/haLLAwa2,xnli_es,0-shot,brier_score,0.9770376302469205,
AbacusResearch/haLLAwa2,xnli_bg,0-shot,brier_score,0.9690174071509365,
AbacusResearch/haLLAwa2,xnli_sw,0-shot,brier_score,0.9981986621227679,
AbacusResearch/haLLAwa2,xnli_el,0-shot,brier_score,0.9569683975606962,
AbacusResearch/haLLAwa2,xnli_th,0-shot,brier_score,1.0281918906994378,
AbacusResearch/haLLAwa2,logiqa2,0-shot,brier_score,0.9982039832917935,
AbacusResearch/haLLAwa2,mathqa,5-shot,brier_score,0.9685303356773713,
AbacusResearch/haLLAwa2,lambada_standard,0-shot,perplexity,3.7062962855818853,0.11093935427091443
AbacusResearch/haLLAwa2,lambada_standard,0-shot,accuracy,0.6743644478944304,0.006528678957835463
AbacusResearch/haLLAwa2,lambada_openai,0-shot,perplexity,2.8051253790007746,0.07002379774910768
AbacusResearch/haLLAwa2,lambada_openai,0-shot,accuracy,0.74345041723268,0.006084483727167678
AbacusResearch/haLLAwa2,mmlu_world_religions,0-shot,accuracy,0.8362573099415205,0.028380919596145866
AbacusResearch/haLLAwa2,mmlu_formal_logic,0-shot,accuracy,0.40476190476190477,0.043902592653775614
AbacusResearch/haLLAwa2,mmlu_prehistory,0-shot,accuracy,0.7160493827160493,0.02508947852376513
AbacusResearch/haLLAwa2,mmlu_moral_scenarios,0-shot,accuracy,0.4223463687150838,0.016519594275297114
AbacusResearch/haLLAwa2,mmlu_high_school_world_history,0-shot,accuracy,0.810126582278481,0.025530100460233497
AbacusResearch/haLLAwa2,mmlu_moral_disputes,0-shot,accuracy,0.7369942196531792,0.023703099525258172
AbacusResearch/haLLAwa2,mmlu_professional_law,0-shot,accuracy,0.45827900912646674,0.012725701656953642
AbacusResearch/haLLAwa2,mmlu_logical_fallacies,0-shot,accuracy,0.7607361963190185,0.0335195387952127
AbacusResearch/haLLAwa2,mmlu_high_school_us_history,0-shot,accuracy,0.8137254901960784,0.027325470966716333
AbacusResearch/haLLAwa2,mmlu_philosophy,0-shot,accuracy,0.7266881028938906,0.025311765975426115
AbacusResearch/haLLAwa2,mmlu_jurisprudence,0-shot,accuracy,0.8055555555555556,0.038260763248848646
AbacusResearch/haLLAwa2,mmlu_international_law,0-shot,accuracy,0.8181818181818182,0.035208939510976506
AbacusResearch/haLLAwa2,mmlu_high_school_european_history,0-shot,accuracy,0.7636363636363637,0.03317505930009181
AbacusResearch/haLLAwa2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8704663212435233,0.02423353229775872
AbacusResearch/haLLAwa2,mmlu_high_school_microeconomics,0-shot,accuracy,0.6512605042016807,0.030956636328566545
AbacusResearch/haLLAwa2,mmlu_high_school_geography,0-shot,accuracy,0.7777777777777778,0.02962022787479047
AbacusResearch/haLLAwa2,mmlu_high_school_psychology,0-shot,accuracy,0.8256880733944955,0.01626567563201037
AbacusResearch/haLLAwa2,mmlu_public_relations,0-shot,accuracy,0.6454545454545455,0.045820048415054174
AbacusResearch/haLLAwa2,mmlu_us_foreign_policy,0-shot,accuracy,0.89,0.03144660377352203
AbacusResearch/haLLAwa2,mmlu_sociology,0-shot,accuracy,0.8606965174129353,0.02448448716291397
AbacusResearch/haLLAwa2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6128205128205129,0.024697216930878937
AbacusResearch/haLLAwa2,mmlu_security_studies,0-shot,accuracy,0.726530612244898,0.02853556033712844
AbacusResearch/haLLAwa2,mmlu_professional_psychology,0-shot,accuracy,0.6519607843137255,0.019270998708223977
AbacusResearch/haLLAwa2,mmlu_human_sexuality,0-shot,accuracy,0.7557251908396947,0.03768335959728744
AbacusResearch/haLLAwa2,mmlu_econometrics,0-shot,accuracy,0.4824561403508772,0.04700708033551038
AbacusResearch/haLLAwa2,mmlu_miscellaneous,0-shot,accuracy,0.8084291187739464,0.014072859310451949
AbacusResearch/haLLAwa2,mmlu_marketing,0-shot,accuracy,0.8803418803418803,0.021262719400406957
AbacusResearch/haLLAwa2,mmlu_management,0-shot,accuracy,0.7961165048543689,0.039891398595317706
AbacusResearch/haLLAwa2,mmlu_nutrition,0-shot,accuracy,0.7516339869281046,0.024739981355113592
AbacusResearch/haLLAwa2,mmlu_medical_genetics,0-shot,accuracy,0.67,0.04725815626252607
AbacusResearch/haLLAwa2,mmlu_human_aging,0-shot,accuracy,0.6995515695067265,0.030769352008229136
AbacusResearch/haLLAwa2,mmlu_professional_medicine,0-shot,accuracy,0.6948529411764706,0.0279715413701706
AbacusResearch/haLLAwa2,mmlu_college_medicine,0-shot,accuracy,0.6011560693641619,0.037336266553835096
AbacusResearch/haLLAwa2,mmlu_business_ethics,0-shot,accuracy,0.56,0.04988876515698589
AbacusResearch/haLLAwa2,mmlu_clinical_knowledge,0-shot,accuracy,0.6981132075471698,0.028254200344438662
AbacusResearch/haLLAwa2,mmlu_global_facts,0-shot,accuracy,0.4,0.04923659639173309
AbacusResearch/haLLAwa2,mmlu_virology,0-shot,accuracy,0.5421686746987951,0.038786267710023595
AbacusResearch/haLLAwa2,mmlu_professional_accounting,0-shot,accuracy,0.44680851063829785,0.029658235097666907
AbacusResearch/haLLAwa2,mmlu_college_physics,0-shot,accuracy,0.4411764705882353,0.04940635630605659
AbacusResearch/haLLAwa2,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.03684881521389024
AbacusResearch/haLLAwa2,mmlu_high_school_biology,0-shot,accuracy,0.7419354838709677,0.024892469172462836
AbacusResearch/haLLAwa2,mmlu_college_biology,0-shot,accuracy,0.7222222222222222,0.037455547914624576
AbacusResearch/haLLAwa2,mmlu_anatomy,0-shot,accuracy,0.6222222222222222,0.04188307537595853
AbacusResearch/haLLAwa2,mmlu_college_chemistry,0-shot,accuracy,0.46,0.05009082659620332
AbacusResearch/haLLAwa2,mmlu_computer_security,0-shot,accuracy,0.77,0.04229525846816506
AbacusResearch/haLLAwa2,mmlu_college_computer_science,0-shot,accuracy,0.52,0.050211673156867795
AbacusResearch/haLLAwa2,mmlu_astronomy,0-shot,accuracy,0.6710526315789473,0.03823428969926604
AbacusResearch/haLLAwa2,mmlu_college_mathematics,0-shot,accuracy,0.36,0.048241815132442176
AbacusResearch/haLLAwa2,mmlu_conceptual_physics,0-shot,accuracy,0.5872340425531914,0.03218471141400351
AbacusResearch/haLLAwa2,mmlu_abstract_algebra,0-shot,accuracy,0.35,0.047937248544110196
AbacusResearch/haLLAwa2,mmlu_high_school_computer_science,0-shot,accuracy,0.67,0.04725815626252607
AbacusResearch/haLLAwa2,mmlu_machine_learning,0-shot,accuracy,0.4732142857142857,0.047389751192741546
AbacusResearch/haLLAwa2,mmlu_high_school_chemistry,0-shot,accuracy,0.4876847290640394,0.035169204442208966
AbacusResearch/haLLAwa2,mmlu_high_school_statistics,0-shot,accuracy,0.48148148148148145,0.03407632093854051
AbacusResearch/haLLAwa2,mmlu_elementary_mathematics,0-shot,accuracy,0.3835978835978836,0.025043757318520196
AbacusResearch/haLLAwa2,mmlu_electrical_engineering,0-shot,accuracy,0.5310344827586206,0.04158632762097828
AbacusResearch/haLLAwa2,mmlu_high_school_mathematics,0-shot,accuracy,0.3814814814814815,0.029616718927497596
AbacusResearch/haLLAwa2,arc_challenge,25-shot,accuracy,0.6023890784982935,0.014301752223279538
AbacusResearch/haLLAwa2,arc_challenge,25-shot,acc_norm,0.6407849829351536,0.014020224155839154
AbacusResearch/haLLAwa2,hellaswag,10-shot,accuracy,0.68123879705238,0.004650438781745276
AbacusResearch/haLLAwa2,hellaswag,10-shot,acc_norm,0.8464449312885879,0.00359784913981503
AbacusResearch/haLLAwa2,truthfulqa_mc2,0-shot,accuracy,0.47657125753705704,0.01563309801578014
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,bleu_max,26.979448196778783,0.7893178897204924
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,bleu_acc,0.3929008567931457,0.017097248285233065
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,bleu_diff,-2.604511700508371,0.8015257472068844
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge1_max,52.66916062467333,0.8511635282334032
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge1_acc,0.3953488372093023,0.017115815632418204
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge1_diff,-4.016130326745816,0.974035197973408
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge2_max,37.88807807215924,0.9893255272073741
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge2_acc,0.35495716034271724,0.0167508623813759
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rouge2_diff,-4.403291370670143,1.1060638545857444
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rougeL_max,49.48769052738361,0.8679658445507986
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rougeL_acc,0.3708690330477356,0.01690969358024884
AbacusResearch/haLLAwa2,truthfulqa_gen,0-shot,rougeL_diff,-4.579117328573877,0.9886743725581556
AbacusResearch/haLLAwa2,truthfulqa_mc1,0-shot,accuracy,0.33414932680538556,0.01651253067715055
AbacusResearch/haLLAwa2,winogrande,5-shot,accuracy,0.760852407261247,0.011988541844843903
Qwen/Qwen2.5-32B,mmlu_world_religions,0-shot,accuracy,0.9064327485380117,0.02233599323116327
Qwen/Qwen2.5-32B,mmlu_formal_logic,0-shot,accuracy,0.7222222222222222,0.040061680838488774
Qwen/Qwen2.5-32B,mmlu_prehistory,0-shot,accuracy,0.9135802469135802,0.015634305710693557
Qwen/Qwen2.5-32B,mmlu_moral_scenarios,0-shot,accuracy,0.7541899441340782,0.014400296429225613
Qwen/Qwen2.5-32B,mmlu_high_school_world_history,0-shot,accuracy,0.9240506329113924,0.0172446332510657
Qwen/Qwen2.5-32B,mmlu_moral_disputes,0-shot,accuracy,0.8439306358381503,0.019539014685374036
Qwen/Qwen2.5-32B,mmlu_professional_law,0-shot,accuracy,0.6492829204693612,0.012187773370741518
Qwen/Qwen2.5-32B,mmlu_logical_fallacies,0-shot,accuracy,0.8895705521472392,0.024624937788941315
Qwen/Qwen2.5-32B,mmlu_high_school_us_history,0-shot,accuracy,0.9362745098039216,0.01714392165552496
Qwen/Qwen2.5-32B,mmlu_philosophy,0-shot,accuracy,0.8778135048231511,0.018600811252967916
Qwen/Qwen2.5-32B,mmlu_jurisprudence,0-shot,accuracy,0.8425925925925926,0.03520703990517965
Qwen/Qwen2.5-32B,mmlu_international_law,0-shot,accuracy,0.9338842975206612,0.022683403691723322
Qwen/Qwen2.5-32B,mmlu_high_school_european_history,0-shot,accuracy,0.8787878787878788,0.025485498373343223
Qwen/Qwen2.5-32B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9740932642487047,0.011464523356953162
Qwen/Qwen2.5-32B,mmlu_high_school_microeconomics,0-shot,accuracy,0.9453781512605042,0.014760864893498496
Qwen/Qwen2.5-32B,mmlu_high_school_geography,0-shot,accuracy,0.9444444444444444,0.016319950700767395
Qwen/Qwen2.5-32B,mmlu_high_school_psychology,0-shot,accuracy,0.9339449541284404,0.010649131487858935
Qwen/Qwen2.5-32B,mmlu_public_relations,0-shot,accuracy,0.7636363636363637,0.04069306319721377
Qwen/Qwen2.5-32B,mmlu_us_foreign_policy,0-shot,accuracy,0.96,0.019694638556693223
Qwen/Qwen2.5-32B,mmlu_sociology,0-shot,accuracy,0.9104477611940298,0.02019067053502794
Qwen/Qwen2.5-32B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.8769230769230769,0.016656903282068206
Qwen/Qwen2.5-32B,mmlu_security_studies,0-shot,accuracy,0.8489795918367347,0.022923004094736858
Qwen/Qwen2.5-32B,mmlu_professional_psychology,0-shot,accuracy,0.8709150326797386,0.01356454163404412
Qwen/Qwen2.5-32B,mmlu_human_sexuality,0-shot,accuracy,0.9236641221374046,0.023288939536173784
Qwen/Qwen2.5-32B,mmlu_econometrics,0-shot,accuracy,0.7807017543859649,0.038924311065187546
Qwen/Qwen2.5-32B,mmlu_miscellaneous,0-shot,accuracy,0.9195402298850575,0.009726831316141838
Qwen/Qwen2.5-32B,mmlu_marketing,0-shot,accuracy,0.9145299145299145,0.018315891685625838
Qwen/Qwen2.5-32B,mmlu_management,0-shot,accuracy,0.883495145631068,0.03176683948640407
Qwen/Qwen2.5-32B,mmlu_nutrition,0-shot,accuracy,0.869281045751634,0.01930187362421529
Qwen/Qwen2.5-32B,mmlu_medical_genetics,0-shot,accuracy,0.91,0.028762349126466153
Qwen/Qwen2.5-32B,mmlu_human_aging,0-shot,accuracy,0.8161434977578476,0.02599837909235651
Qwen/Qwen2.5-32B,mmlu_professional_medicine,0-shot,accuracy,0.8970588235294118,0.01845951895538869
Qwen/Qwen2.5-32B,mmlu_college_medicine,0-shot,accuracy,0.8265895953757225,0.028868107874970635
Qwen/Qwen2.5-32B,mmlu_business_ethics,0-shot,accuracy,0.82,0.03861229196653693
Qwen/Qwen2.5-32B,mmlu_clinical_knowledge,0-shot,accuracy,0.8754716981132076,0.020321376630696202
Qwen/Qwen2.5-32B,mmlu_global_facts,0-shot,accuracy,0.71,0.045604802157206845
Qwen/Qwen2.5-32B,mmlu_virology,0-shot,accuracy,0.572289156626506,0.038515976837185335
Qwen/Qwen2.5-32B,mmlu_professional_accounting,0-shot,accuracy,0.6843971631205674,0.027724989449509314
Qwen/Qwen2.5-32B,mmlu_college_physics,0-shot,accuracy,0.7450980392156863,0.0433643270799318
Qwen/Qwen2.5-32B,mmlu_high_school_physics,0-shot,accuracy,0.7615894039735099,0.0347918557259966
Qwen/Qwen2.5-32B,mmlu_high_school_biology,0-shot,accuracy,0.9516129032258065,0.012207189992293643
Qwen/Qwen2.5-32B,mmlu_college_biology,0-shot,accuracy,0.9375,0.02024219611347799
Qwen/Qwen2.5-32B,mmlu_anatomy,0-shot,accuracy,0.7851851851851852,0.03547854198560824
Qwen/Qwen2.5-32B,mmlu_college_chemistry,0-shot,accuracy,0.62,0.04878317312145632
Qwen/Qwen2.5-32B,mmlu_computer_security,0-shot,accuracy,0.86,0.03487350880197771
Qwen/Qwen2.5-32B,mmlu_college_computer_science,0-shot,accuracy,0.79,0.040936018074033256
Qwen/Qwen2.5-32B,mmlu_astronomy,0-shot,accuracy,0.9407894736842105,0.019206897196800306
Qwen/Qwen2.5-32B,mmlu_college_mathematics,0-shot,accuracy,0.74,0.044084400227680794
Qwen/Qwen2.5-32B,mmlu_conceptual_physics,0-shot,accuracy,0.902127659574468,0.019424777705573392
Qwen/Qwen2.5-32B,mmlu_abstract_algebra,0-shot,accuracy,0.72,0.04512608598542127
Qwen/Qwen2.5-32B,mmlu_high_school_computer_science,0-shot,accuracy,0.94,0.02386832565759419
Qwen/Qwen2.5-32B,mmlu_machine_learning,0-shot,accuracy,0.7946428571428571,0.03834241021419073
Qwen/Qwen2.5-32B,mmlu_high_school_chemistry,0-shot,accuracy,0.7684729064039408,0.029678333141444444
Qwen/Qwen2.5-32B,mmlu_high_school_statistics,0-shot,accuracy,0.8518518518518519,0.02422762927372837
Qwen/Qwen2.5-32B,mmlu_elementary_mathematics,0-shot,accuracy,0.9021164021164021,0.015304374225091419
Qwen/Qwen2.5-32B,mmlu_electrical_engineering,0-shot,accuracy,0.8206896551724138,0.03196766433373187
Qwen/Qwen2.5-32B,mmlu_high_school_mathematics,0-shot,accuracy,0.6518518518518519,0.029045600290616255
Qwen/Qwen2.5-32B,arc_challenge,25-shot,accuracy,0.6621160409556314,0.01382204792228352
Qwen/Qwen2.5-32B,arc_challenge,25-shot,acc_norm,0.6911262798634812,0.013501770929344003
Qwen/Qwen2.5-32B,hellaswag,10-shot,accuracy,0.6571400119498108,0.004736950810617823
Qwen/Qwen2.5-32B,hellaswag,10-shot,acc_norm,0.8515236008763195,0.003548449054286016
Qwen/Qwen2.5-32B,truthfulqa_mc2,0-shot,accuracy,0.5781834572377078,0.014797004312646012
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,bleu_max,6.969336348202127,0.5870541151739272
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,bleu_acc,0.13096695226438188,0.011810109581712577
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,bleu_diff,0.6563244381841533,0.422393788389518
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge1_max,14.003241923004378,0.9380420509133744
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge1_acc,0.13953488372093023,0.01213006008958147
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge1_diff,1.0093134943305602,0.5297814254859047
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge2_max,10.188591447651278,0.7985636803484863
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge2_acc,0.11260709914320685,0.011066130337399355
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rouge2_diff,0.6138278691566204,0.608593814594194
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rougeL_max,13.156036427349836,0.8996246678278078
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rougeL_acc,0.13953488372093023,0.01213006008958147
Qwen/Qwen2.5-32B,truthfulqa_gen,0-shot,rougeL_diff,0.9732115915170979,0.5279186710795332
Qwen/Qwen2.5-32B,truthfulqa_mc1,0-shot,accuracy,0.40024479804161567,0.01715160555574914
Qwen/Qwen2.5-32B,winogrande,5-shot,accuracy,0.8184688239936859,0.01083327651500751
Qwen/Qwen2.5-32B,gsm8k,5-shot,accuracy,0.8976497346474602,0.008349110996208827
rinna/bilingual-gpt-neox-4b,arc:challenge,25-shot,accuracy,0.23890784982935154,0.012461071376316623
rinna/bilingual-gpt-neox-4b,arc:challenge,25-shot,acc_norm,0.29180887372013653,0.013284525292403506
rinna/bilingual-gpt-neox-4b,hendrycksTest-abstract_algebra,5-shot,accuracy,0.22,0.04163331998932268
rinna/bilingual-gpt-neox-4b,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.22,0.04163331998932268
rinna/bilingual-gpt-neox-4b,hendrycksTest-anatomy,5-shot,accuracy,0.18518518518518517,0.03355677216313142
rinna/bilingual-gpt-neox-4b,hendrycksTest-anatomy,5-shot,acc_norm,0.18518518518518517,0.03355677216313142
rinna/bilingual-gpt-neox-4b,hendrycksTest-astronomy,5-shot,accuracy,0.17763157894736842,0.031103182383123398
rinna/bilingual-gpt-neox-4b,hendrycksTest-astronomy,5-shot,acc_norm,0.17763157894736842,0.031103182383123398
rinna/bilingual-gpt-neox-4b,hendrycksTest-business_ethics,5-shot,accuracy,0.3,0.046056618647183814
rinna/bilingual-gpt-neox-4b,hendrycksTest-business_ethics,5-shot,acc_norm,0.3,0.046056618647183814
rinna/bilingual-gpt-neox-4b,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.21509433962264152,0.02528839450289137
rinna/bilingual-gpt-neox-4b,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.21509433962264152,0.02528839450289137
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_biology,5-shot,accuracy,0.2569444444444444,0.03653946969442099
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_biology,5-shot,acc_norm,0.2569444444444444,0.03653946969442099
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_chemistry,5-shot,accuracy,0.2,0.04020151261036845
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_chemistry,5-shot,acc_norm,0.2,0.04020151261036845
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_computer_science,5-shot,accuracy,0.26,0.0440844002276808
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_computer_science,5-shot,acc_norm,0.26,0.0440844002276808
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_mathematics,5-shot,accuracy,0.2,0.04020151261036846
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_mathematics,5-shot,acc_norm,0.2,0.04020151261036846
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_medicine,5-shot,accuracy,0.20809248554913296,0.030952890217749874
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_medicine,5-shot,acc_norm,0.20809248554913296,0.030952890217749874
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237654
rinna/bilingual-gpt-neox-4b,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237654
rinna/bilingual-gpt-neox-4b,hendrycksTest-computer_security,5-shot,accuracy,0.28,0.045126085985421276
rinna/bilingual-gpt-neox-4b,hendrycksTest-computer_security,5-shot,acc_norm,0.28,0.045126085985421276
rinna/bilingual-gpt-neox-4b,hendrycksTest-conceptual_physics,5-shot,accuracy,0.26382978723404255,0.028809989854102973
rinna/bilingual-gpt-neox-4b,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.26382978723404255,0.028809989854102973
rinna/bilingual-gpt-neox-4b,hendrycksTest-econometrics,5-shot,accuracy,0.23684210526315788,0.039994238792813365
rinna/bilingual-gpt-neox-4b,hendrycksTest-econometrics,5-shot,acc_norm,0.23684210526315788,0.039994238792813365
rinna/bilingual-gpt-neox-4b,hendrycksTest-electrical_engineering,5-shot,accuracy,0.2413793103448276,0.03565998174135302
rinna/bilingual-gpt-neox-4b,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.2413793103448276,0.03565998174135302
rinna/bilingual-gpt-neox-4b,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.20899470899470898,0.02094048156533486
rinna/bilingual-gpt-neox-4b,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.20899470899470898,0.02094048156533486
rinna/bilingual-gpt-neox-4b,hendrycksTest-formal_logic,5-shot,accuracy,0.2857142857142857,0.04040610178208841
rinna/bilingual-gpt-neox-4b,hendrycksTest-formal_logic,5-shot,acc_norm,0.2857142857142857,0.04040610178208841
rinna/bilingual-gpt-neox-4b,hendrycksTest-global_facts,5-shot,accuracy,0.18,0.038612291966536934
rinna/bilingual-gpt-neox-4b,hendrycksTest-global_facts,5-shot,acc_norm,0.18,0.038612291966536934
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_biology,5-shot,accuracy,0.1774193548387097,0.02173254068932927
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_biology,5-shot,acc_norm,0.1774193548387097,0.02173254068932927
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.15270935960591134,0.02530890453938063
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.15270935960591134,0.02530890453938063
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.25,0.04351941398892446
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.25,0.04351941398892446
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_european_history,5-shot,accuracy,0.21818181818181817,0.03225078108306289
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.21818181818181817,0.03225078108306289
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_geography,5-shot,accuracy,0.17676767676767677,0.027178752639044915
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_geography,5-shot,acc_norm,0.17676767676767677,0.027178752639044915
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.19689119170984457,0.028697873971860664
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.19689119170984457,0.028697873971860664
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.20256410256410257,0.020377660970371372
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.20256410256410257,0.020377660970371372
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2111111111111111,0.024882116857655075
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2111111111111111,0.024882116857655075
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.21008403361344538,0.026461398717471874
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.21008403361344538,0.026461398717471874
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_physics,5-shot,accuracy,0.1986754966887417,0.03257847384436776
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_physics,5-shot,acc_norm,0.1986754966887417,0.03257847384436776
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_psychology,5-shot,accuracy,0.1926605504587156,0.016909276884936094
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.1926605504587156,0.016909276884936094
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_statistics,5-shot,accuracy,0.1527777777777778,0.024536326026134224
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.1527777777777778,0.024536326026134224
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_us_history,5-shot,accuracy,0.25,0.03039153369274154
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.25,0.03039153369274154
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_world_history,5-shot,accuracy,0.270042194092827,0.028900721906293426
rinna/bilingual-gpt-neox-4b,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.270042194092827,0.028900721906293426
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_aging,5-shot,accuracy,0.31390134529147984,0.031146796482972465
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_aging,5-shot,acc_norm,0.31390134529147984,0.031146796482972465
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_sexuality,5-shot,accuracy,0.2595419847328244,0.03844876139785271
rinna/bilingual-gpt-neox-4b,hendrycksTest-human_sexuality,5-shot,acc_norm,0.2595419847328244,0.03844876139785271
rinna/bilingual-gpt-neox-4b,hendrycksTest-international_law,5-shot,accuracy,0.2396694214876033,0.03896878985070417
rinna/bilingual-gpt-neox-4b,hendrycksTest-international_law,5-shot,acc_norm,0.2396694214876033,0.03896878985070417
rinna/bilingual-gpt-neox-4b,hendrycksTest-jurisprudence,5-shot,accuracy,0.25925925925925924,0.042365112580946336
rinna/bilingual-gpt-neox-4b,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25925925925925924,0.042365112580946336
rinna/bilingual-gpt-neox-4b,hendrycksTest-logical_fallacies,5-shot,accuracy,0.22085889570552147,0.032591773927421776
rinna/bilingual-gpt-neox-4b,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.22085889570552147,0.032591773927421776
rinna/bilingual-gpt-neox-4b,hendrycksTest-machine_learning,5-shot,accuracy,0.3125,0.043994650575715215
rinna/bilingual-gpt-neox-4b,hendrycksTest-machine_learning,5-shot,acc_norm,0.3125,0.043994650575715215
rinna/bilingual-gpt-neox-4b,hendrycksTest-management,5-shot,accuracy,0.17475728155339806,0.037601780060266224
rinna/bilingual-gpt-neox-4b,hendrycksTest-management,5-shot,acc_norm,0.17475728155339806,0.037601780060266224
rinna/bilingual-gpt-neox-4b,hendrycksTest-marketing,5-shot,accuracy,0.2905982905982906,0.02974504857267404
rinna/bilingual-gpt-neox-4b,hendrycksTest-marketing,5-shot,acc_norm,0.2905982905982906,0.02974504857267404
rinna/bilingual-gpt-neox-4b,hendrycksTest-medical_genetics,5-shot,accuracy,0.3,0.046056618647183814
rinna/bilingual-gpt-neox-4b,hendrycksTest-medical_genetics,5-shot,acc_norm,0.3,0.046056618647183814
rinna/bilingual-gpt-neox-4b,hendrycksTest-miscellaneous,5-shot,accuracy,0.23754789272030652,0.015218733046150193
rinna/bilingual-gpt-neox-4b,hendrycksTest-miscellaneous,5-shot,acc_norm,0.23754789272030652,0.015218733046150193
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_disputes,5-shot,accuracy,0.24855491329479767,0.023267528432100174
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_disputes,5-shot,acc_norm,0.24855491329479767,0.023267528432100174
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23798882681564246,0.014242630070574915
rinna/bilingual-gpt-neox-4b,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23798882681564246,0.014242630070574915
rinna/bilingual-gpt-neox-4b,hendrycksTest-nutrition,5-shot,accuracy,0.22549019607843138,0.023929155517351284
rinna/bilingual-gpt-neox-4b,hendrycksTest-nutrition,5-shot,acc_norm,0.22549019607843138,0.023929155517351284
rinna/bilingual-gpt-neox-4b,hendrycksTest-philosophy,5-shot,accuracy,0.1864951768488746,0.02212243977248077
rinna/bilingual-gpt-neox-4b,hendrycksTest-philosophy,5-shot,acc_norm,0.1864951768488746,0.02212243977248077
rinna/bilingual-gpt-neox-4b,hendrycksTest-prehistory,5-shot,accuracy,0.21604938271604937,0.022899162918445806
rinna/bilingual-gpt-neox-4b,hendrycksTest-prehistory,5-shot,acc_norm,0.21604938271604937,0.022899162918445806
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_accounting,5-shot,accuracy,0.23404255319148937,0.025257861359432417
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_accounting,5-shot,acc_norm,0.23404255319148937,0.025257861359432417
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_law,5-shot,accuracy,0.2457627118644068,0.010996156635142692
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_law,5-shot,acc_norm,0.2457627118644068,0.010996156635142692
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_medicine,5-shot,accuracy,0.18382352941176472,0.023529242185193106
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_medicine,5-shot,acc_norm,0.18382352941176472,0.023529242185193106
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_psychology,5-shot,accuracy,0.25,0.01751781884501444
rinna/bilingual-gpt-neox-4b,hendrycksTest-professional_psychology,5-shot,acc_norm,0.25,0.01751781884501444
rinna/bilingual-gpt-neox-4b,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
rinna/bilingual-gpt-neox-4b,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
rinna/bilingual-gpt-neox-4b,hendrycksTest-security_studies,5-shot,accuracy,0.18775510204081633,0.02500025603954621
rinna/bilingual-gpt-neox-4b,hendrycksTest-security_studies,5-shot,acc_norm,0.18775510204081633,0.02500025603954621
rinna/bilingual-gpt-neox-4b,hendrycksTest-sociology,5-shot,accuracy,0.24378109452736318,0.03036049015401465
rinna/bilingual-gpt-neox-4b,hendrycksTest-sociology,5-shot,acc_norm,0.24378109452736318,0.03036049015401465
rinna/bilingual-gpt-neox-4b,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.28,0.04512608598542128
rinna/bilingual-gpt-neox-4b,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.28,0.04512608598542128
rinna/bilingual-gpt-neox-4b,hendrycksTest-virology,5-shot,accuracy,0.28313253012048195,0.03507295431370518
rinna/bilingual-gpt-neox-4b,hendrycksTest-virology,5-shot,acc_norm,0.28313253012048195,0.03507295431370518
rinna/bilingual-gpt-neox-4b,hendrycksTest-world_religions,5-shot,accuracy,0.3216374269005848,0.03582529442573122
rinna/bilingual-gpt-neox-4b,hendrycksTest-world_religions,5-shot,acc_norm,0.3216374269005848,0.03582529442573122
rinna/bilingual-gpt-neox-4b,truthfulqa:mc,0-shot,mc1,0.2460220318237454,0.015077219200662573
rinna/bilingual-gpt-neox-4b,truthfulqa:mc,0-shot,mc2,0.4500298950094902,0.014997412839549137
rinna/bilingual-gpt-neox-4b,drop,3-shot,accuracy,0.0,
rinna/bilingual-gpt-neox-4b,drop,3-shot,f1,0.0019494546979865776,0.0001656985868155588
facebook/opt-2.7b,minerva_math_precalc,5-shot,accuracy,0.023809523809523808,0.006530469219761482
facebook/opt-2.7b,minerva_math_prealgebra,5-shot,accuracy,0.014925373134328358,0.004110905928505601
facebook/opt-2.7b,minerva_math_num_theory,5-shot,accuracy,0.014814814814814815,0.005203704987512652
facebook/opt-2.7b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0221483942414175,0.00490009308861579
facebook/opt-2.7b,minerva_math_geometry,5-shot,accuracy,0.012526096033402923,0.005086941389677978
facebook/opt-2.7b,minerva_math_counting_and_prob,5-shot,accuracy,0.010548523206751054,0.004697453735376162
facebook/opt-2.7b,minerva_math_algebra,5-shot,accuracy,0.015164279696714406,0.0035485460431325523
facebook/opt-2.7b,fld_default,0-shot,accuracy,0.0,
facebook/opt-2.7b,fld_star,0-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_3da,5-shot,accuracy,0.0015,0.0008655920660521438
facebook/opt-2.7b,arithmetic_3ds,5-shot,accuracy,0.001,0.0007069298939339508
facebook/opt-2.7b,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
facebook/opt-2.7b,arithmetic_2ds,5-shot,accuracy,0.0135,0.002581124968507327
facebook/opt-2.7b,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_1dc,5-shot,accuracy,0.001,0.0007069298939339562
facebook/opt-2.7b,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-2.7b,arithmetic_2dm,5-shot,accuracy,0.0215,0.0032440926417928078
facebook/opt-2.7b,arithmetic_2da,5-shot,accuracy,0.011,0.002332856855993375
facebook/opt-2.7b,gsm8k_cot,5-shot,accuracy,0.02122820318423048,0.003970449129848635
facebook/opt-2.7b,anli_r2,0-shot,brier_score,0.9806575981542668,
facebook/opt-2.7b,anli_r3,0-shot,brier_score,0.8989579823804109,
facebook/opt-2.7b,anli_r1,0-shot,brier_score,0.9835816196646929,
facebook/opt-2.7b,xnli_eu,0-shot,brier_score,1.0610127557895535,
facebook/opt-2.7b,xnli_vi,0-shot,brier_score,0.9078406353013023,
facebook/opt-2.7b,xnli_ru,0-shot,brier_score,0.8321734481428278,
facebook/opt-2.7b,xnli_zh,0-shot,brier_score,1.1784647036176363,
facebook/opt-2.7b,xnli_tr,0-shot,brier_score,0.8894841423618519,
facebook/opt-2.7b,xnli_fr,0-shot,brier_score,0.8313152226484499,
facebook/opt-2.7b,xnli_en,0-shot,brier_score,0.6116601747186954,
facebook/opt-2.7b,xnli_ur,0-shot,brier_score,1.322704035007194,
facebook/opt-2.7b,xnli_ar,0-shot,brier_score,0.9110827039215548,
facebook/opt-2.7b,xnli_de,0-shot,brier_score,0.8552219959870653,
facebook/opt-2.7b,xnli_hi,0-shot,brier_score,0.9737996476054792,
facebook/opt-2.7b,xnli_es,0-shot,brier_score,0.8705426231888244,
facebook/opt-2.7b,xnli_bg,0-shot,brier_score,0.9735045902629904,
facebook/opt-2.7b,xnli_sw,0-shot,brier_score,0.8732903791230823,
facebook/opt-2.7b,xnli_el,0-shot,brier_score,1.1872540651461676,
facebook/opt-2.7b,xnli_th,0-shot,brier_score,1.2687537528892334,
facebook/opt-2.7b,logiqa2,0-shot,brier_score,1.2134396565953744,
facebook/opt-2.7b,mathqa,5-shot,brier_score,0.9929214264531077,
facebook/opt-2.7b,lambada_standard,0-shot,perplexity,7.415927761731489,0.19675668972521876
facebook/opt-2.7b,lambada_standard,0-shot,accuracy,0.5592858529012226,0.0069168361138352225
facebook/opt-2.7b,lambada_openai,0-shot,perplexity,5.119225735809997,0.119744800718221
facebook/opt-2.7b,lambada_openai,0-shot,accuracy,0.6353580438579468,0.0067058627120832725
facebook/opt-2.7b,mmlu_world_religions,0-shot,accuracy,0.2222222222222222,0.03188578017686398
facebook/opt-2.7b,mmlu_formal_logic,0-shot,accuracy,0.19047619047619047,0.035122074123020534
facebook/opt-2.7b,mmlu_prehistory,0-shot,accuracy,0.25308641975308643,0.024191808600713002
facebook/opt-2.7b,mmlu_moral_scenarios,0-shot,accuracy,0.2346368715083799,0.014173044098303673
facebook/opt-2.7b,mmlu_high_school_world_history,0-shot,accuracy,0.24472573839662448,0.027985699387036423
facebook/opt-2.7b,mmlu_moral_disputes,0-shot,accuracy,0.2543352601156069,0.02344582627654554
facebook/opt-2.7b,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.01106415102716544
facebook/opt-2.7b,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.0332201579577674
facebook/opt-2.7b,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.030587591351604246
facebook/opt-2.7b,mmlu_philosophy,0-shot,accuracy,0.3247588424437299,0.026596782287697046
facebook/opt-2.7b,mmlu_jurisprudence,0-shot,accuracy,0.25,0.04186091791394607
facebook/opt-2.7b,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.039418975265163025
facebook/opt-2.7b,mmlu_high_school_european_history,0-shot,accuracy,0.23030303030303031,0.03287666758603488
facebook/opt-2.7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.35233160621761656,0.03447478286414357
facebook/opt-2.7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.2184873949579832,0.02684151432295895
facebook/opt-2.7b,mmlu_high_school_geography,0-shot,accuracy,0.21212121212121213,0.029126522834586815
facebook/opt-2.7b,mmlu_high_school_psychology,0-shot,accuracy,0.3155963302752294,0.01992611751386967
facebook/opt-2.7b,mmlu_public_relations,0-shot,accuracy,0.22727272727272727,0.04013964554072773
facebook/opt-2.7b,mmlu_us_foreign_policy,0-shot,accuracy,0.25,0.04351941398892446
facebook/opt-2.7b,mmlu_sociology,0-shot,accuracy,0.263681592039801,0.031157150869355568
facebook/opt-2.7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.358974358974359,0.024321738484602357
facebook/opt-2.7b,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904024
facebook/opt-2.7b,mmlu_professional_psychology,0-shot,accuracy,0.2679738562091503,0.017917974069594722
facebook/opt-2.7b,mmlu_human_sexuality,0-shot,accuracy,0.2366412213740458,0.03727673575596918
facebook/opt-2.7b,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.04049339297748141
facebook/opt-2.7b,mmlu_miscellaneous,0-shot,accuracy,0.23371647509578544,0.015133383278988829
facebook/opt-2.7b,mmlu_marketing,0-shot,accuracy,0.23076923076923078,0.027601921381417618
facebook/opt-2.7b,mmlu_management,0-shot,accuracy,0.3883495145631068,0.0482572933735639
facebook/opt-2.7b,mmlu_nutrition,0-shot,accuracy,0.23202614379084968,0.02417084087934101
facebook/opt-2.7b,mmlu_medical_genetics,0-shot,accuracy,0.33,0.04725815626252604
facebook/opt-2.7b,mmlu_human_aging,0-shot,accuracy,0.20179372197309417,0.026936111912802273
facebook/opt-2.7b,mmlu_professional_medicine,0-shot,accuracy,0.33455882352941174,0.02866199620233531
facebook/opt-2.7b,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
facebook/opt-2.7b,mmlu_business_ethics,0-shot,accuracy,0.19,0.03942772444036624
facebook/opt-2.7b,mmlu_clinical_knowledge,0-shot,accuracy,0.21132075471698114,0.025125766484827845
facebook/opt-2.7b,mmlu_global_facts,0-shot,accuracy,0.36,0.04824181513244218
facebook/opt-2.7b,mmlu_virology,0-shot,accuracy,0.21686746987951808,0.032082844503563655
facebook/opt-2.7b,mmlu_professional_accounting,0-shot,accuracy,0.2872340425531915,0.026992199173064356
facebook/opt-2.7b,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237656
facebook/opt-2.7b,mmlu_high_school_physics,0-shot,accuracy,0.31125827814569534,0.03780445850526733
facebook/opt-2.7b,mmlu_high_school_biology,0-shot,accuracy,0.2161290322580645,0.023415293433568532
facebook/opt-2.7b,mmlu_college_biology,0-shot,accuracy,0.2152777777777778,0.03437079344106135
facebook/opt-2.7b,mmlu_anatomy,0-shot,accuracy,0.2814814814814815,0.03885004245800254
facebook/opt-2.7b,mmlu_college_chemistry,0-shot,accuracy,0.21,0.040936018074033256
facebook/opt-2.7b,mmlu_computer_security,0-shot,accuracy,0.26,0.0440844002276808
facebook/opt-2.7b,mmlu_college_computer_science,0-shot,accuracy,0.3,0.046056618647183814
facebook/opt-2.7b,mmlu_astronomy,0-shot,accuracy,0.19078947368421054,0.031975658210325004
facebook/opt-2.7b,mmlu_college_mathematics,0-shot,accuracy,0.23,0.04229525846816505
facebook/opt-2.7b,mmlu_conceptual_physics,0-shot,accuracy,0.23829787234042554,0.0278512529738898
facebook/opt-2.7b,mmlu_abstract_algebra,0-shot,accuracy,0.21,0.040936018074033256
facebook/opt-2.7b,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.04760952285695236
facebook/opt-2.7b,mmlu_machine_learning,0-shot,accuracy,0.25892857142857145,0.04157751539865629
facebook/opt-2.7b,mmlu_high_school_chemistry,0-shot,accuracy,0.270935960591133,0.031270907132976984
facebook/opt-2.7b,mmlu_high_school_statistics,0-shot,accuracy,0.39814814814814814,0.03338473403207402
facebook/opt-2.7b,mmlu_elementary_mathematics,0-shot,accuracy,0.24074074074074073,0.022019080012217883
facebook/opt-2.7b,mmlu_electrical_engineering,0-shot,accuracy,0.2827586206896552,0.03752833958003336
facebook/opt-2.7b,mmlu_high_school_mathematics,0-shot,accuracy,0.2851851851851852,0.027528599210340496
facebook/opt-2.7b,arc_challenge,25-shot,accuracy,0.31399317406143346,0.01356269122472629
facebook/opt-2.7b,arc_challenge,25-shot,acc_norm,0.3455631399317406,0.01389693846114568
facebook/opt-2.7b,truthfulqa_mc2,0-shot,accuracy,0.3763787086134506,0.013809514742722533
facebook/opt-2.7b,truthfulqa_gen,0-shot,bleu_max,21.936813599784074,0.6915463833520875
facebook/opt-2.7b,truthfulqa_gen,0-shot,bleu_acc,0.2827417380660955,0.01576477083677731
facebook/opt-2.7b,truthfulqa_gen,0-shot,bleu_diff,-7.93706461416589,0.6942322145010175
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge1_max,46.56471474176105,0.8452800285319477
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge1_acc,0.26193390452876375,0.015392118805015044
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge1_diff,-10.43421160381809,0.7865219397875223
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge2_max,29.59058433337149,0.9351396896455382
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge2_acc,0.19216646266829865,0.013792870480628964
facebook/opt-2.7b,truthfulqa_gen,0-shot,rouge2_diff,-12.224344822664076,0.8785968206293998
facebook/opt-2.7b,truthfulqa_gen,0-shot,rougeL_max,43.69031715340036,0.8461881443382573
facebook/opt-2.7b,truthfulqa_gen,0-shot,rougeL_acc,0.24724602203182375,0.015102404797359654
facebook/opt-2.7b,truthfulqa_gen,0-shot,rougeL_diff,-10.797286242336181,0.7775003821253601
facebook/opt-2.7b,truthfulqa_mc1,0-shot,accuracy,0.22399020807833536,0.014594964329474205
Salesforce/codegen-16B-mono,anli_r2,0-shot,brier_score,0.8253824795424403,
Salesforce/codegen-16B-mono,anli_r3,0-shot,brier_score,0.7940354639454789,
Salesforce/codegen-16B-mono,anli_r1,0-shot,brier_score,0.8429583560477731,
Salesforce/codegen-16B-mono,xnli_eu,0-shot,brier_score,1.0481738865590222,
Salesforce/codegen-16B-mono,xnli_vi,0-shot,brier_score,1.0622085188319774,
Salesforce/codegen-16B-mono,xnli_ru,0-shot,brier_score,0.8720664022610117,
Salesforce/codegen-16B-mono,xnli_zh,0-shot,brier_score,0.9990022713619396,
Salesforce/codegen-16B-mono,xnli_tr,0-shot,brier_score,0.9070160533682529,
Salesforce/codegen-16B-mono,xnli_fr,0-shot,brier_score,0.938508643895241,
Salesforce/codegen-16B-mono,xnli_en,0-shot,brier_score,0.7700595581688592,
Salesforce/codegen-16B-mono,xnli_ur,0-shot,brier_score,1.3209693093229593,
Salesforce/codegen-16B-mono,xnli_ar,0-shot,brier_score,1.082490658523419,
Salesforce/codegen-16B-mono,xnli_de,0-shot,brier_score,1.043589609239351,
Salesforce/codegen-16B-mono,xnli_hi,0-shot,brier_score,0.9979601232052799,
Salesforce/codegen-16B-mono,xnli_es,0-shot,brier_score,0.9189766913317707,
Salesforce/codegen-16B-mono,xnli_bg,0-shot,brier_score,0.9981098092050139,
Salesforce/codegen-16B-mono,xnli_sw,0-shot,brier_score,0.9858592239662022,
Salesforce/codegen-16B-mono,xnli_el,0-shot,brier_score,0.8942569399409489,
Salesforce/codegen-16B-mono,xnli_th,0-shot,brier_score,0.8555323255757882,
Salesforce/codegen-16B-mono,logiqa2,0-shot,brier_score,1.0900452349971228,
Salesforce/codegen-16B-mono,mathqa,5-shot,brier_score,0.9631942925356739,
Salesforce/codegen-16B-mono,lambada_standard,0-shot,perplexity,40.81985306298772,1.485975941096083
Salesforce/codegen-16B-mono,lambada_standard,0-shot,accuracy,0.3052590723850184,0.006415903230922692
Salesforce/codegen-16B-mono,lambada_openai,0-shot,perplexity,27.000998548239398,0.9699976694126962
Salesforce/codegen-16B-mono,lambada_openai,0-shot,accuracy,0.3592082282165729,0.006684111319975825
Salesforce/codegen-16B-mono,mmlu_world_religions,0-shot,accuracy,0.27485380116959063,0.03424042924691583
Salesforce/codegen-16B-mono,mmlu_formal_logic,0-shot,accuracy,0.20634920634920634,0.0361960452412425
Salesforce/codegen-16B-mono,mmlu_prehistory,0-shot,accuracy,0.2839506172839506,0.025089478523765137
Salesforce/codegen-16B-mono,mmlu_moral_scenarios,0-shot,accuracy,0.23575418994413408,0.014196375686290804
Salesforce/codegen-16B-mono,mmlu_high_school_world_history,0-shot,accuracy,0.25316455696202533,0.02830465794303529
Salesforce/codegen-16B-mono,mmlu_moral_disputes,0-shot,accuracy,0.26011560693641617,0.023618678310069363
Salesforce/codegen-16B-mono,mmlu_professional_law,0-shot,accuracy,0.25358539765319427,0.011111715336101132
Salesforce/codegen-16B-mono,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.032591773927421776
Salesforce/codegen-16B-mono,mmlu_high_school_us_history,0-shot,accuracy,0.25,0.03039153369274154
Salesforce/codegen-16B-mono,mmlu_philosophy,0-shot,accuracy,0.27009646302250806,0.025218040373410605
Salesforce/codegen-16B-mono,mmlu_jurisprudence,0-shot,accuracy,0.26851851851851855,0.04284467968052191
Salesforce/codegen-16B-mono,mmlu_international_law,0-shot,accuracy,0.2644628099173554,0.04026187527591206
Salesforce/codegen-16B-mono,mmlu_high_school_european_history,0-shot,accuracy,0.22424242424242424,0.03256866661681102
Salesforce/codegen-16B-mono,mmlu_high_school_government_and_politics,0-shot,accuracy,0.20725388601036268,0.02925282329180364
Salesforce/codegen-16B-mono,mmlu_high_school_microeconomics,0-shot,accuracy,0.23529411764705882,0.02755361446786379
Salesforce/codegen-16B-mono,mmlu_high_school_geography,0-shot,accuracy,0.1919191919191919,0.028057791672989017
Salesforce/codegen-16B-mono,mmlu_high_school_psychology,0-shot,accuracy,0.23302752293577983,0.0181256691808615
Salesforce/codegen-16B-mono,mmlu_public_relations,0-shot,accuracy,0.2727272727272727,0.04265792110940589
Salesforce/codegen-16B-mono,mmlu_us_foreign_policy,0-shot,accuracy,0.24,0.04292346959909281
Salesforce/codegen-16B-mono,mmlu_sociology,0-shot,accuracy,0.29850746268656714,0.03235743789355041
Salesforce/codegen-16B-mono,mmlu_high_school_macroeconomics,0-shot,accuracy,0.22564102564102564,0.021193632525148522
Salesforce/codegen-16B-mono,mmlu_security_studies,0-shot,accuracy,0.22040816326530613,0.026537045312145284
Salesforce/codegen-16B-mono,mmlu_professional_psychology,0-shot,accuracy,0.25980392156862747,0.017740899509177795
Salesforce/codegen-16B-mono,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.03915345408847836
Salesforce/codegen-16B-mono,mmlu_econometrics,0-shot,accuracy,0.24561403508771928,0.040493392977481425
Salesforce/codegen-16B-mono,mmlu_miscellaneous,0-shot,accuracy,0.2656449553001277,0.01579430248788872
Salesforce/codegen-16B-mono,mmlu_marketing,0-shot,accuracy,0.2863247863247863,0.029614323690456648
Salesforce/codegen-16B-mono,mmlu_management,0-shot,accuracy,0.22330097087378642,0.04123553189891431
Salesforce/codegen-16B-mono,mmlu_nutrition,0-shot,accuracy,0.2549019607843137,0.02495418432487991
Salesforce/codegen-16B-mono,mmlu_medical_genetics,0-shot,accuracy,0.24,0.04292346959909281
Salesforce/codegen-16B-mono,mmlu_human_aging,0-shot,accuracy,0.3542600896860987,0.032100621541349864
Salesforce/codegen-16B-mono,mmlu_professional_medicine,0-shot,accuracy,0.20588235294117646,0.024562204314142317
Salesforce/codegen-16B-mono,mmlu_college_medicine,0-shot,accuracy,0.2254335260115607,0.03186209851641144
Salesforce/codegen-16B-mono,mmlu_business_ethics,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-16B-mono,mmlu_clinical_knowledge,0-shot,accuracy,0.2188679245283019,0.025447863825108614
Salesforce/codegen-16B-mono,mmlu_global_facts,0-shot,accuracy,0.31,0.046482319871173156
Salesforce/codegen-16B-mono,mmlu_virology,0-shot,accuracy,0.28313253012048195,0.03507295431370519
Salesforce/codegen-16B-mono,mmlu_professional_accounting,0-shot,accuracy,0.26595744680851063,0.026358065698880585
Salesforce/codegen-16B-mono,mmlu_college_physics,0-shot,accuracy,0.18627450980392157,0.038739587141493524
Salesforce/codegen-16B-mono,mmlu_high_school_physics,0-shot,accuracy,0.2847682119205298,0.03684881521389023
Salesforce/codegen-16B-mono,mmlu_high_school_biology,0-shot,accuracy,0.19032258064516128,0.02233170761182307
Salesforce/codegen-16B-mono,mmlu_college_biology,0-shot,accuracy,0.25,0.03621034121889507
Salesforce/codegen-16B-mono,mmlu_anatomy,0-shot,accuracy,0.2222222222222222,0.035914440841969694
Salesforce/codegen-16B-mono,mmlu_college_chemistry,0-shot,accuracy,0.2,0.040201512610368466
Salesforce/codegen-16B-mono,mmlu_computer_security,0-shot,accuracy,0.29,0.04560480215720684
Salesforce/codegen-16B-mono,mmlu_college_computer_science,0-shot,accuracy,0.2,0.040201512610368445
Salesforce/codegen-16B-mono,mmlu_astronomy,0-shot,accuracy,0.20394736842105263,0.03279000406310051
Salesforce/codegen-16B-mono,mmlu_college_mathematics,0-shot,accuracy,0.25,0.04351941398892446
Salesforce/codegen-16B-mono,mmlu_conceptual_physics,0-shot,accuracy,0.2978723404255319,0.029896145682095455
Salesforce/codegen-16B-mono,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909283
Salesforce/codegen-16B-mono,mmlu_high_school_computer_science,0-shot,accuracy,0.29,0.04560480215720684
Salesforce/codegen-16B-mono,mmlu_machine_learning,0-shot,accuracy,0.2857142857142857,0.04287858751340456
Salesforce/codegen-16B-mono,mmlu_high_school_chemistry,0-shot,accuracy,0.2019704433497537,0.02824735012218027
Salesforce/codegen-16B-mono,mmlu_high_school_statistics,0-shot,accuracy,0.2037037037037037,0.027467401804057982
Salesforce/codegen-16B-mono,mmlu_elementary_mathematics,0-shot,accuracy,0.23544973544973544,0.021851509822031722
Salesforce/codegen-16B-mono,mmlu_electrical_engineering,0-shot,accuracy,0.2896551724137931,0.03780019230438015
Salesforce/codegen-16B-mono,mmlu_high_school_mathematics,0-shot,accuracy,0.2074074074074074,0.024720713193952155
Salesforce/codegen-16B-mono,arc_challenge,25-shot,accuracy,0.2696245733788396,0.012968040686869166
Salesforce/codegen-16B-mono,arc_challenge,25-shot,acc_norm,0.295221843003413,0.013329750293382315
Salesforce/codegen-16B-mono,hellaswag,10-shot,accuracy,0.3579964150567616,0.004784312972495382
Salesforce/codegen-16B-mono,hellaswag,10-shot,acc_norm,0.4473212507468632,0.004962010338226348
Salesforce/codegen-16B-mono,truthfulqa_mc2,0-shot,accuracy,0.4061093580338541,0.015136437773534737
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,bleu_max,19.57158267860521,0.672039191352111
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,bleu_acc,0.3023255813953488,0.016077509266133036
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,bleu_diff,-5.301786631798468,0.658779186406353
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge1_max,43.18409379824872,0.8482447875377391
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge1_acc,0.2802937576499388,0.01572313952460875
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge1_diff,-7.480158262440118,0.8228031808206397
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge2_max,26.21877195377042,0.9378604257610167
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge2_acc,0.20563035495716034,0.014148482219460978
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rouge2_diff,-8.570941233813052,0.9197989857896314
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rougeL_max,40.08679096043078,0.8489879245723853
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rougeL_acc,0.2864137086903305,0.01582614243950234
Salesforce/codegen-16B-mono,truthfulqa_gen,0-shot,rougeL_diff,-7.518785257278961,0.8301172921913628
Salesforce/codegen-16B-mono,truthfulqa_mc1,0-shot,accuracy,0.24357405140758873,0.01502635482491078
Salesforce/codegen-16B-mono,winogrande,5-shot,accuracy,0.5674822415153907,0.013923911578623842
Salesforce/codegen-16B-mono,gsm8k,5-shot,accuracy,0.026535253980288095,0.004427045987265163
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_precalc,5-shot,accuracy,0.11172161172161173,0.013494130099732618
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_prealgebra,5-shot,accuracy,0.4764638346727899,0.016932796474939078
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_num_theory,5-shot,accuracy,0.2037037037037037,0.017347720963761987
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_intermediate_algebra,5-shot,accuracy,0.12070874861572536,0.010847570493593098
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_geometry,5-shot,accuracy,0.19624217118997914,0.018165394328850663
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_counting_and_prob,5-shot,accuracy,0.24472573839662448,0.019767948269352843
meta-llama/Meta-Llama-3-8B-Instruct,minerva_math_algebra,5-shot,accuracy,0.37826453243470937,0.014081803764022903
meta-llama/Meta-Llama-3-8B-Instruct,fld_default,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B-Instruct,fld_star,0-shot,accuracy,0.0,
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_3da,5-shot,accuracy,0.9995,0.0005000000000000156
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_3ds,5-shot,accuracy,0.8855,0.007121814032784058
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_4da,5-shot,accuracy,0.8945,0.006870842687736334
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_2ds,5-shot,accuracy,0.997,0.0012232122154647094
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_5ds,5-shot,accuracy,0.7225,0.010014840164064459
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_5da,5-shot,accuracy,0.795,0.009029300312431016
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_1dc,5-shot,accuracy,0.7925,0.009069895616998728
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_4ds,5-shot,accuracy,0.8155,0.008675684915577362
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_2dm,5-shot,accuracy,0.67,0.010516905564438917
meta-llama/Meta-Llama-3-8B-Instruct,arithmetic_2da,5-shot,accuracy,1.0,
meta-llama/Meta-Llama-3-8B-Instruct,gsm8k_cot,5-shot,accuracy,0.775587566338135,0.011491617756630552
meta-llama/Meta-Llama-3-8B-Instruct,anli_r2,0-shot,brier_score,0.7985456079098852,
meta-llama/Meta-Llama-3-8B-Instruct,anli_r3,0-shot,brier_score,0.8097953259921752,
meta-llama/Meta-Llama-3-8B-Instruct,anli_r1,0-shot,brier_score,0.7717221698804513,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_eu,0-shot,brier_score,0.8196432026111591,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_vi,0-shot,brier_score,0.7618000692371746,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_ru,0-shot,brier_score,0.8013434537191864,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_zh,0-shot,brier_score,0.9930200253772428,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_tr,0-shot,brier_score,0.8615816405837291,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_fr,0-shot,brier_score,0.6960972021301648,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_en,0-shot,brier_score,0.6514935237404581,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_ur,0-shot,brier_score,1.2113932574417665,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_ar,0-shot,brier_score,1.2690322872660167,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_de,0-shot,brier_score,0.8339566525731148,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_hi,0-shot,brier_score,0.8549027587669167,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_es,0-shot,brier_score,0.8028087096573583,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_bg,0-shot,brier_score,0.8658614845860327,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_sw,0-shot,brier_score,1.0179283740266234,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_el,0-shot,brier_score,0.8840107296653167,
meta-llama/Meta-Llama-3-8B-Instruct,xnli_th,0-shot,brier_score,0.8332666686094854,
meta-llama/Meta-Llama-3-8B-Instruct,logiqa2,0-shot,brier_score,0.9980174455769029,
meta-llama/Meta-Llama-3-8B-Instruct,mathqa,5-shot,brier_score,0.8366526946512588,
meta-llama/Meta-Llama-3-8B-Instruct,lambada_standard,0-shot,perplexity,4.010364094889167,0.10269598682376097
meta-llama/Meta-Llama-3-8B-Instruct,lambada_standard,0-shot,accuracy,0.6501067339413934,0.0066446513367465045
meta-llama/Meta-Llama-3-8B-Instruct,lambada_openai,0-shot,perplexity,3.104536403718183,0.07662347070866664
meta-llama/Meta-Llama-3-8B-Instruct,lambada_openai,0-shot,accuracy,0.7189986415680186,0.006262248789164311
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_world_religions,0-shot,accuracy,0.783625730994152,0.031581495393387324
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_formal_logic,0-shot,accuracy,0.49206349206349204,0.044715725362943486
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_prehistory,0-shot,accuracy,0.7407407407407407,0.02438366553103545
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_moral_scenarios,0-shot,accuracy,0.4346368715083799,0.01657899743549672
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_world_history,0-shot,accuracy,0.8438818565400844,0.023627159460318684
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_moral_disputes,0-shot,accuracy,0.7456647398843931,0.023445826276545546
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_law,0-shot,accuracy,0.4784876140808344,0.01275841094103892
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_logical_fallacies,0-shot,accuracy,0.7668711656441718,0.0332201579577674
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_us_history,0-shot,accuracy,0.8529411764705882,0.024857478080250465
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_philosophy,0-shot,accuracy,0.7202572347266881,0.0254942593506949
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_jurisprudence,0-shot,accuracy,0.7777777777777778,0.0401910747255735
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_international_law,0-shot,accuracy,0.8181818181818182,0.03520893951097653
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_european_history,0-shot,accuracy,0.7515151515151515,0.03374402644139405
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_government_and_politics,0-shot,accuracy,0.9119170984455959,0.02045374660160103
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_microeconomics,0-shot,accuracy,0.7647058823529411,0.027553614467863818
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_geography,0-shot,accuracy,0.8434343434343434,0.025890520358141454
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_psychology,0-shot,accuracy,0.8311926605504587,0.01606005626853035
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_public_relations,0-shot,accuracy,0.6363636363636364,0.046075820907199756
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_us_foreign_policy,0-shot,accuracy,0.85,0.0358870281282637
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_sociology,0-shot,accuracy,0.8706467661691543,0.023729830881018515
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_macroeconomics,0-shot,accuracy,0.658974358974359,0.024035489676335065
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_security_studies,0-shot,accuracy,0.7428571428571429,0.02797982353874455
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_psychology,0-shot,accuracy,0.7091503267973857,0.018373116915903973
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_human_sexuality,0-shot,accuracy,0.7786259541984732,0.03641297081313729
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_econometrics,0-shot,accuracy,0.6140350877192983,0.045796394220704334
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_miscellaneous,0-shot,accuracy,0.7994891443167306,0.014317653708594212
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_marketing,0-shot,accuracy,0.905982905982906,0.019119892798924974
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_management,0-shot,accuracy,0.7766990291262136,0.04123553189891431
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_nutrition,0-shot,accuracy,0.7516339869281046,0.024739981355113592
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_medical_genetics,0-shot,accuracy,0.8,0.04020151261036846
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_human_aging,0-shot,accuracy,0.7309417040358744,0.029763779406874972
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_medicine,0-shot,accuracy,0.7242647058823529,0.027146271936625166
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_medicine,0-shot,accuracy,0.6416184971098265,0.0365634365335316
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_business_ethics,0-shot,accuracy,0.69,0.04648231987117316
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_clinical_knowledge,0-shot,accuracy,0.7471698113207547,0.026749899771241224
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_global_facts,0-shot,accuracy,0.4,0.049236596391733084
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_virology,0-shot,accuracy,0.5060240963855421,0.03892212195333045
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_professional_accounting,0-shot,accuracy,0.5390070921985816,0.029736592526424438
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_physics,0-shot,accuracy,0.5098039215686274,0.04974229460422817
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_physics,0-shot,accuracy,0.44370860927152317,0.04056527902281732
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_biology,0-shot,accuracy,0.7806451612903226,0.023540799358723302
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_biology,0-shot,accuracy,0.7916666666666666,0.03396116205845334
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_anatomy,0-shot,accuracy,0.6370370370370371,0.041539484047424
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_chemistry,0-shot,accuracy,0.47,0.05016135580465919
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_computer_security,0-shot,accuracy,0.77,0.042295258468165065
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_computer_science,0-shot,accuracy,0.58,0.049604496374885836
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_astronomy,0-shot,accuracy,0.6973684210526315,0.03738520676119667
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_college_mathematics,0-shot,accuracy,0.38,0.04878317312145633
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_conceptual_physics,0-shot,accuracy,0.6042553191489362,0.031967586978353627
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_abstract_algebra,0-shot,accuracy,0.32,0.046882617226215034
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_computer_science,0-shot,accuracy,0.75,0.04351941398892446
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_machine_learning,0-shot,accuracy,0.5446428571428571,0.04726835553719097
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_chemistry,0-shot,accuracy,0.5024630541871922,0.03517945038691063
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_statistics,0-shot,accuracy,0.5370370370370371,0.03400603625538271
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_elementary_mathematics,0-shot,accuracy,0.4470899470899471,0.025606723995777025
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_electrical_engineering,0-shot,accuracy,0.6275862068965518,0.04028731532947559
meta-llama/Meta-Llama-3-8B-Instruct,mmlu_high_school_mathematics,0-shot,accuracy,0.3925925925925926,0.02977384701253297
meta-llama/Meta-Llama-3-8B-Instruct,arc_challenge,25-shot,accuracy,0.5767918088737202,0.014438036220848025
meta-llama/Meta-Llama-3-8B-Instruct,arc_challenge,25-shot,acc_norm,0.6186006825938567,0.01419438908668527
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_mc2,0-shot,accuracy,0.5165834108450839,0.015188480530101718
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,bleu_max,20.38284601416647,0.7259585799496531
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,bleu_acc,0.4749082007343941,0.017481446804104003
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,bleu_diff,-0.39825979672746564,0.6399548657857169
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge1_max,43.70636561691526,0.8683149503828073
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge1_acc,0.4969400244798042,0.017503173260960625
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge1_diff,-0.267087450778999,0.8748649880343018
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge2_max,27.583822895276878,0.9560493672304742
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge2_acc,0.36474908200734396,0.016850961061720113
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rouge2_diff,-1.7302074043155247,0.928098862686909
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rougeL_max,40.83179541506895,0.8672456167019427
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rougeL_acc,0.4847001223990208,0.017495304473187902
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_gen,0-shot,rougeL_diff,-0.6070224090190008,0.8794043276943261
meta-llama/Meta-Llama-3-8B-Instruct,truthfulqa_mc1,0-shot,accuracy,0.36474908200734396,0.016850961061720116
mosaicml/mpt-7b-storywriter,minerva_math_precalc,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_prealgebra,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_num_theory,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_geometry,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,minerva_math_algebra,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,fld_default,0-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,fld_star,0-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
mosaicml/mpt-7b-storywriter,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
mosaicml/mpt-7b-storywriter,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
mosaicml/mpt-7b-storywriter,arithmetic_2ds,5-shot,accuracy,0.0125,0.00248494717876267
mosaicml/mpt-7b-storywriter,arithmetic_5ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_5da,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_1dc,5-shot,accuracy,0.0055,0.0016541593398342208
mosaicml/mpt-7b-storywriter,arithmetic_4ds,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,arithmetic_2dm,5-shot,accuracy,0.0265,0.003592398594787676
mosaicml/mpt-7b-storywriter,arithmetic_2da,5-shot,accuracy,0.007,0.0018647355360237512
mosaicml/mpt-7b-storywriter,gsm8k_cot,5-shot,accuracy,0.0,
mosaicml/mpt-7b-storywriter,anli_r2,0-shot,brier_score,0.8579585824630889,
mosaicml/mpt-7b-storywriter,anli_r3,0-shot,brier_score,0.8532457528157426,
mosaicml/mpt-7b-storywriter,anli_r1,0-shot,brier_score,0.8446244791915563,
mosaicml/mpt-7b-storywriter,xnli_eu,0-shot,brier_score,1.2168588145018897,
mosaicml/mpt-7b-storywriter,xnli_vi,0-shot,brier_score,1.2991601321455832,
mosaicml/mpt-7b-storywriter,xnli_ru,0-shot,brier_score,1.2753960909711017,
mosaicml/mpt-7b-storywriter,xnli_zh,0-shot,brier_score,1.0743836903664954,
mosaicml/mpt-7b-storywriter,xnli_tr,0-shot,brier_score,1.1927998634673966,
mosaicml/mpt-7b-storywriter,xnli_fr,0-shot,brier_score,1.290251935664798,
mosaicml/mpt-7b-storywriter,xnli_en,0-shot,brier_score,0.8772257385108837,
mosaicml/mpt-7b-storywriter,xnli_ur,0-shot,brier_score,1.2785277549537262,
mosaicml/mpt-7b-storywriter,xnli_ar,0-shot,brier_score,0.9907536655140208,
mosaicml/mpt-7b-storywriter,xnli_de,0-shot,brier_score,1.2274392301166228,
mosaicml/mpt-7b-storywriter,xnli_hi,0-shot,brier_score,1.1088702072239738,
mosaicml/mpt-7b-storywriter,xnli_es,0-shot,brier_score,1.2244126501923114,
mosaicml/mpt-7b-storywriter,xnli_bg,0-shot,brier_score,1.0445857377325605,
mosaicml/mpt-7b-storywriter,xnli_sw,0-shot,brier_score,1.084639887151233,
mosaicml/mpt-7b-storywriter,xnli_el,0-shot,brier_score,1.1457153222036922,
mosaicml/mpt-7b-storywriter,xnli_th,0-shot,brier_score,1.301506120005153,
mosaicml/mpt-7b-storywriter,logiqa2,0-shot,brier_score,1.4689876462916187,
mosaicml/mpt-7b-storywriter,mathqa,5-shot,brier_score,1.0079977099532107,
mosaicml/mpt-7b-storywriter,lambada_standard,0-shot,perplexity,202.229010197755,9.46813064432611
mosaicml/mpt-7b-storywriter,lambada_standard,0-shot,accuracy,0.11449640985833495,0.004436118825118391
mosaicml/mpt-7b-storywriter,lambada_openai,0-shot,perplexity,176.61298095244155,9.78176772928432
mosaicml/mpt-7b-storywriter,lambada_openai,0-shot,accuracy,0.17659615757810984,0.005312624764825889
mosaicml/mpt-7b-storywriter,mmlu_world_religions,0-shot,accuracy,0.23976608187134502,0.03274485211946956
mosaicml/mpt-7b-storywriter,mmlu_formal_logic,0-shot,accuracy,0.21428571428571427,0.03670066451047181
mosaicml/mpt-7b-storywriter,mmlu_prehistory,0-shot,accuracy,0.26851851851851855,0.024659685185967294
mosaicml/mpt-7b-storywriter,mmlu_moral_scenarios,0-shot,accuracy,0.24916201117318434,0.014465893829859923
mosaicml/mpt-7b-storywriter,mmlu_high_school_world_history,0-shot,accuracy,0.3333333333333333,0.03068582059661081
mosaicml/mpt-7b-storywriter,mmlu_moral_disputes,0-shot,accuracy,0.29190751445086704,0.024476994076247323
mosaicml/mpt-7b-storywriter,mmlu_professional_law,0-shot,accuracy,0.25684485006518903,0.011158455853098862
mosaicml/mpt-7b-storywriter,mmlu_logical_fallacies,0-shot,accuracy,0.26993865030674846,0.034878251684978906
mosaicml/mpt-7b-storywriter,mmlu_high_school_us_history,0-shot,accuracy,0.3480392156862745,0.03343311240488418
mosaicml/mpt-7b-storywriter,mmlu_philosophy,0-shot,accuracy,0.26366559485530544,0.02502553850053234
mosaicml/mpt-7b-storywriter,mmlu_jurisprudence,0-shot,accuracy,0.25925925925925924,0.04236511258094632
mosaicml/mpt-7b-storywriter,mmlu_international_law,0-shot,accuracy,0.24793388429752067,0.03941897526516303
mosaicml/mpt-7b-storywriter,mmlu_high_school_european_history,0-shot,accuracy,0.24848484848484848,0.03374402644139405
mosaicml/mpt-7b-storywriter,mmlu_high_school_government_and_politics,0-shot,accuracy,0.2694300518134715,0.03201867122877794
mosaicml/mpt-7b-storywriter,mmlu_high_school_microeconomics,0-shot,accuracy,0.23949579831932774,0.027722065493361266
mosaicml/mpt-7b-storywriter,mmlu_high_school_geography,0-shot,accuracy,0.2474747474747475,0.030746300742124522
mosaicml/mpt-7b-storywriter,mmlu_high_school_psychology,0-shot,accuracy,0.23669724770642203,0.01822407811729907
mosaicml/mpt-7b-storywriter,mmlu_public_relations,0-shot,accuracy,0.38181818181818183,0.046534298079135075
mosaicml/mpt-7b-storywriter,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.04512608598542127
mosaicml/mpt-7b-storywriter,mmlu_sociology,0-shot,accuracy,0.2835820895522388,0.031871875379197966
mosaicml/mpt-7b-storywriter,mmlu_high_school_macroeconomics,0-shot,accuracy,0.25384615384615383,0.022066054378726257
mosaicml/mpt-7b-storywriter,mmlu_security_studies,0-shot,accuracy,0.2816326530612245,0.028795185574291282
mosaicml/mpt-7b-storywriter,mmlu_professional_psychology,0-shot,accuracy,0.27450980392156865,0.0180540274588152
mosaicml/mpt-7b-storywriter,mmlu_human_sexuality,0-shot,accuracy,0.2748091603053435,0.03915345408847836
mosaicml/mpt-7b-storywriter,mmlu_econometrics,0-shot,accuracy,0.2631578947368421,0.04142439719489359
mosaicml/mpt-7b-storywriter,mmlu_miscellaneous,0-shot,accuracy,0.30395913154533843,0.016448321686769043
mosaicml/mpt-7b-storywriter,mmlu_marketing,0-shot,accuracy,0.29914529914529914,0.029996951858349476
mosaicml/mpt-7b-storywriter,mmlu_management,0-shot,accuracy,0.24271844660194175,0.04245022486384495
mosaicml/mpt-7b-storywriter,mmlu_nutrition,0-shot,accuracy,0.30392156862745096,0.026336613469046626
mosaicml/mpt-7b-storywriter,mmlu_medical_genetics,0-shot,accuracy,0.26,0.0440844002276808
mosaicml/mpt-7b-storywriter,mmlu_human_aging,0-shot,accuracy,0.32286995515695066,0.03138147637575498
mosaicml/mpt-7b-storywriter,mmlu_professional_medicine,0-shot,accuracy,0.21691176470588236,0.025035845227711264
mosaicml/mpt-7b-storywriter,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
mosaicml/mpt-7b-storywriter,mmlu_business_ethics,0-shot,accuracy,0.28,0.045126085985421296
mosaicml/mpt-7b-storywriter,mmlu_clinical_knowledge,0-shot,accuracy,0.26037735849056604,0.0270087660907081
mosaicml/mpt-7b-storywriter,mmlu_global_facts,0-shot,accuracy,0.27,0.044619604333847394
mosaicml/mpt-7b-storywriter,mmlu_virology,0-shot,accuracy,0.3373493975903614,0.03680783690727581
mosaicml/mpt-7b-storywriter,mmlu_professional_accounting,0-shot,accuracy,0.2801418439716312,0.02678917235114025
mosaicml/mpt-7b-storywriter,mmlu_college_physics,0-shot,accuracy,0.20588235294117646,0.04023382273617747
mosaicml/mpt-7b-storywriter,mmlu_high_school_physics,0-shot,accuracy,0.2582781456953642,0.035737053147634576
mosaicml/mpt-7b-storywriter,mmlu_high_school_biology,0-shot,accuracy,0.25483870967741934,0.024790118459332204
mosaicml/mpt-7b-storywriter,mmlu_college_biology,0-shot,accuracy,0.2569444444444444,0.03653946969442099
mosaicml/mpt-7b-storywriter,mmlu_anatomy,0-shot,accuracy,0.2814814814814815,0.038850042458002554
mosaicml/mpt-7b-storywriter,mmlu_college_chemistry,0-shot,accuracy,0.18,0.03861229196653695
mosaicml/mpt-7b-storywriter,mmlu_computer_security,0-shot,accuracy,0.31,0.04648231987117316
mosaicml/mpt-7b-storywriter,mmlu_college_computer_science,0-shot,accuracy,0.23,0.04229525846816506
mosaicml/mpt-7b-storywriter,mmlu_astronomy,0-shot,accuracy,0.2236842105263158,0.033911609343436025
mosaicml/mpt-7b-storywriter,mmlu_college_mathematics,0-shot,accuracy,0.27,0.044619604333847415
mosaicml/mpt-7b-storywriter,mmlu_conceptual_physics,0-shot,accuracy,0.3191489361702128,0.03047297336338004
mosaicml/mpt-7b-storywriter,mmlu_abstract_algebra,0-shot,accuracy,0.27,0.04461960433384741
mosaicml/mpt-7b-storywriter,mmlu_high_school_computer_science,0-shot,accuracy,0.26,0.04408440022768077
mosaicml/mpt-7b-storywriter,mmlu_machine_learning,0-shot,accuracy,0.25892857142857145,0.04157751539865629
mosaicml/mpt-7b-storywriter,mmlu_high_school_chemistry,0-shot,accuracy,0.24630541871921183,0.03031509928561773
mosaicml/mpt-7b-storywriter,mmlu_high_school_statistics,0-shot,accuracy,0.2916666666666667,0.030998666304560534
mosaicml/mpt-7b-storywriter,mmlu_elementary_mathematics,0-shot,accuracy,0.2751322751322751,0.023000086859068642
mosaicml/mpt-7b-storywriter,mmlu_electrical_engineering,0-shot,accuracy,0.25517241379310346,0.03632984052707842
mosaicml/mpt-7b-storywriter,mmlu_high_school_mathematics,0-shot,accuracy,0.25925925925925924,0.02671924078371216
mosaicml/mpt-7b-storywriter,arc_challenge,25-shot,accuracy,0.4274744027303754,0.014456862944650659
mosaicml/mpt-7b-storywriter,arc_challenge,25-shot,acc_norm,0.46075085324232085,0.014566303676636584
mosaicml/mpt-7b-storywriter,truthfulqa_mc2,0-shot,accuracy,0.35969412975276555,0.013513306511175506
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,bleu_max,17.977525977108503,0.7695541132431905
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,bleu_acc,0.2350061199510404,0.014843061507731604
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,bleu_diff,-5.0966333287389975,0.721249793575583
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge1_max,35.04312245719028,1.0996506075148464
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge1_acc,0.19216646266829865,0.013792870480628949
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge1_diff,-6.663656074389997,0.8025282580401039
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge2_max,24.121071934633303,1.0180145363654554
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge2_acc,0.17135862913096694,0.013191409923739371
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rouge2_diff,-7.719271942054364,0.953796284275681
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rougeL_max,33.244148594030946,1.075159041310315
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rougeL_acc,0.19216646266829865,0.01379287048062895
mosaicml/mpt-7b-storywriter,truthfulqa_gen,0-shot,rougeL_diff,-6.790992534457851,0.8170761181979462
mosaicml/mpt-7b-storywriter,truthfulqa_mc1,0-shot,accuracy,0.21542227662178703,0.014391902652427683
Qwen/Qwen2.5-3B,mmlu_world_religions,0-shot,accuracy,0.8304093567251462,0.02878210810540171
Qwen/Qwen2.5-3B,mmlu_formal_logic,0-shot,accuracy,0.46825396825396826,0.04463112720677173
Qwen/Qwen2.5-3B,mmlu_prehistory,0-shot,accuracy,0.75,0.02409347123262133
Qwen/Qwen2.5-3B,mmlu_moral_scenarios,0-shot,accuracy,0.2636871508379888,0.01473692638376197
Qwen/Qwen2.5-3B,mmlu_high_school_world_history,0-shot,accuracy,0.8438818565400844,0.02362715946031869
Qwen/Qwen2.5-3B,mmlu_moral_disputes,0-shot,accuracy,0.7196531791907514,0.024182427496577615
Qwen/Qwen2.5-3B,mmlu_professional_law,0-shot,accuracy,0.4791395045632334,0.012759117066518019
Qwen/Qwen2.5-3B,mmlu_logical_fallacies,0-shot,accuracy,0.7791411042944786,0.03259177392742178
Qwen/Qwen2.5-3B,mmlu_high_school_us_history,0-shot,accuracy,0.8431372549019608,0.02552472232455335
Qwen/Qwen2.5-3B,mmlu_philosophy,0-shot,accuracy,0.7170418006430869,0.025583062489984834
Qwen/Qwen2.5-3B,mmlu_jurisprudence,0-shot,accuracy,0.7685185185185185,0.04077494709252626
Qwen/Qwen2.5-3B,mmlu_international_law,0-shot,accuracy,0.768595041322314,0.03849856098794088
Qwen/Qwen2.5-3B,mmlu_high_school_european_history,0-shot,accuracy,0.7757575757575758,0.032568666616811015
Qwen/Qwen2.5-3B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8549222797927462,0.025416343096306422
Qwen/Qwen2.5-3B,mmlu_high_school_microeconomics,0-shot,accuracy,0.7815126050420168,0.02684151432295893
Qwen/Qwen2.5-3B,mmlu_high_school_geography,0-shot,accuracy,0.8080808080808081,0.02805779167298901
Qwen/Qwen2.5-3B,mmlu_high_school_psychology,0-shot,accuracy,0.8587155963302753,0.01493386898702808
Qwen/Qwen2.5-3B,mmlu_public_relations,0-shot,accuracy,0.7181818181818181,0.04309118709946458
Qwen/Qwen2.5-3B,mmlu_us_foreign_policy,0-shot,accuracy,0.84,0.036845294917747094
Qwen/Qwen2.5-3B,mmlu_sociology,0-shot,accuracy,0.835820895522388,0.026193923544454115
Qwen/Qwen2.5-3B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.7076923076923077,0.02306043838085775
Qwen/Qwen2.5-3B,mmlu_security_studies,0-shot,accuracy,0.7346938775510204,0.028263889943784603
Qwen/Qwen2.5-3B,mmlu_professional_psychology,0-shot,accuracy,0.6977124183006536,0.018579232711113895
Qwen/Qwen2.5-3B,mmlu_human_sexuality,0-shot,accuracy,0.7709923664122137,0.036853466317118506
Qwen/Qwen2.5-3B,mmlu_econometrics,0-shot,accuracy,0.5614035087719298,0.04668000738510455
Qwen/Qwen2.5-3B,mmlu_miscellaneous,0-shot,accuracy,0.8007662835249042,0.014283378044296413
Qwen/Qwen2.5-3B,mmlu_marketing,0-shot,accuracy,0.8974358974358975,0.019875655027867447
Qwen/Qwen2.5-3B,mmlu_management,0-shot,accuracy,0.7961165048543689,0.03989139859531772
Qwen/Qwen2.5-3B,mmlu_nutrition,0-shot,accuracy,0.7516339869281046,0.024739981355113592
Qwen/Qwen2.5-3B,mmlu_medical_genetics,0-shot,accuracy,0.8,0.04020151261036845
Qwen/Qwen2.5-3B,mmlu_human_aging,0-shot,accuracy,0.7040358744394619,0.030636591348699813
Qwen/Qwen2.5-3B,mmlu_professional_medicine,0-shot,accuracy,0.6580882352941176,0.02881472242225418
Qwen/Qwen2.5-3B,mmlu_college_medicine,0-shot,accuracy,0.6820809248554913,0.035506839891655796
Qwen/Qwen2.5-3B,mmlu_business_ethics,0-shot,accuracy,0.71,0.045604802157206845
Qwen/Qwen2.5-3B,mmlu_clinical_knowledge,0-shot,accuracy,0.7245283018867924,0.027495663683724046
Qwen/Qwen2.5-3B,mmlu_global_facts,0-shot,accuracy,0.39,0.04902071300001974
Qwen/Qwen2.5-3B,mmlu_virology,0-shot,accuracy,0.4819277108433735,0.038899512528272166
Qwen/Qwen2.5-3B,mmlu_professional_accounting,0-shot,accuracy,0.4929078014184397,0.02982449855912901
Qwen/Qwen2.5-3B,mmlu_college_physics,0-shot,accuracy,0.49019607843137253,0.04974229460422817
Qwen/Qwen2.5-3B,mmlu_high_school_physics,0-shot,accuracy,0.45695364238410596,0.04067325174247443
Qwen/Qwen2.5-3B,mmlu_high_school_biology,0-shot,accuracy,0.8225806451612904,0.021732540689329272
Qwen/Qwen2.5-3B,mmlu_college_biology,0-shot,accuracy,0.7638888888888888,0.03551446610810826
Qwen/Qwen2.5-3B,mmlu_anatomy,0-shot,accuracy,0.6296296296296297,0.04171654161354544
Qwen/Qwen2.5-3B,mmlu_college_chemistry,0-shot,accuracy,0.48,0.050211673156867795
Qwen/Qwen2.5-3B,mmlu_computer_security,0-shot,accuracy,0.8,0.04020151261036846
Qwen/Qwen2.5-3B,mmlu_college_computer_science,0-shot,accuracy,0.6,0.04923659639173309
Qwen/Qwen2.5-3B,mmlu_astronomy,0-shot,accuracy,0.743421052631579,0.035541803680256896
Qwen/Qwen2.5-3B,mmlu_college_mathematics,0-shot,accuracy,0.44,0.04988876515698589
Qwen/Qwen2.5-3B,mmlu_conceptual_physics,0-shot,accuracy,0.6425531914893617,0.031329417894764254
Qwen/Qwen2.5-3B,mmlu_abstract_algebra,0-shot,accuracy,0.38,0.048783173121456316
Qwen/Qwen2.5-3B,mmlu_high_school_computer_science,0-shot,accuracy,0.76,0.042923469599092816
Qwen/Qwen2.5-3B,mmlu_machine_learning,0-shot,accuracy,0.4642857142857143,0.04733667890053756
Qwen/Qwen2.5-3B,mmlu_high_school_chemistry,0-shot,accuracy,0.6059113300492611,0.03438157967036545
Qwen/Qwen2.5-3B,mmlu_high_school_statistics,0-shot,accuracy,0.625,0.033016908987210894
Qwen/Qwen2.5-3B,mmlu_elementary_mathematics,0-shot,accuracy,0.5978835978835979,0.02525303255499769
Qwen/Qwen2.5-3B,mmlu_electrical_engineering,0-shot,accuracy,0.7034482758620689,0.03806142687309992
Qwen/Qwen2.5-3B,mmlu_high_school_mathematics,0-shot,accuracy,0.5,0.030485538042484616
Qwen/Qwen2.5-3B,arc_challenge,25-shot,accuracy,0.5332764505119454,0.014578995859605799
Qwen/Qwen2.5-3B,arc_challenge,25-shot,acc_norm,0.5750853242320819,0.014445698968520767
Qwen/Qwen2.5-3B,hellaswag,10-shot,accuracy,0.5538737303326031,0.004960732382255246
Qwen/Qwen2.5-3B,hellaswag,10-shot,acc_norm,0.7460665206134236,0.004343704512380098
Qwen/Qwen2.5-3B,truthfulqa_mc2,0-shot,accuracy,0.4893490828530172,0.014899604966354063
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,bleu_max,17.723506876309774,0.6221890551789969
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,bleu_acc,0.36474908200734396,0.016850961061720127
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,bleu_diff,-2.7917053834970753,0.5719675325277486
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge1_max,42.124187039438695,0.7609882906246943
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge1_acc,0.37209302325581395,0.016921090118814035
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge1_diff,-4.2070463723170395,0.7335338436606067
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge2_max,28.977364786765644,0.8202368269256133
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge2_acc,0.32313341493268055,0.016371836286454604
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rouge2_diff,-5.148974861232573,0.8363241355197695
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rougeL_max,39.37523397372098,0.7596826090671275
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rougeL_acc,0.3537331701346389,0.01673781435884615
Qwen/Qwen2.5-3B,truthfulqa_gen,0-shot,rougeL_diff,-4.5716762036035945,0.7325385085991697
Qwen/Qwen2.5-3B,truthfulqa_mc1,0-shot,accuracy,0.31701346389228885,0.016289203374403392
Qwen/Qwen2.5-3B,winogrande,5-shot,accuracy,0.7087608524072613,0.012769029305370702
Qwen/Qwen2.5-3B,gsm8k,5-shot,accuracy,0.7467778620166793,0.011978125194299678
EleutherAI/pythia-1.4b-deduped,arc:challenge,25-shot,accuracy,0.295221843003413,0.013329750293382316
EleutherAI/pythia-1.4b-deduped,arc:challenge,25-shot,acc_norm,0.3267918088737201,0.013706665975587333
EleutherAI/pythia-1.4b-deduped,hellaswag,10-shot,accuracy,0.41784505078669587,0.004921964133874023
EleutherAI/pythia-1.4b-deduped,hellaswag,10-shot,acc_norm,0.5495917147978491,0.004965177633049914
EleutherAI/pythia-1.4b-deduped,hendrycksTest-abstract_algebra,5-shot,accuracy,0.27,0.044619604333847415
EleutherAI/pythia-1.4b-deduped,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.27,0.044619604333847415
EleutherAI/pythia-1.4b-deduped,hendrycksTest-anatomy,5-shot,accuracy,0.1925925925925926,0.03406542058502652
EleutherAI/pythia-1.4b-deduped,hendrycksTest-anatomy,5-shot,acc_norm,0.1925925925925926,0.03406542058502652
EleutherAI/pythia-1.4b-deduped,hendrycksTest-astronomy,5-shot,accuracy,0.15789473684210525,0.029674167520101435
EleutherAI/pythia-1.4b-deduped,hendrycksTest-astronomy,5-shot,acc_norm,0.15789473684210525,0.029674167520101435
EleutherAI/pythia-1.4b-deduped,hendrycksTest-business_ethics,5-shot,accuracy,0.34,0.047609522856952344
EleutherAI/pythia-1.4b-deduped,hendrycksTest-business_ethics,5-shot,acc_norm,0.34,0.047609522856952344
EleutherAI/pythia-1.4b-deduped,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.2490566037735849,0.026616482980501715
EleutherAI/pythia-1.4b-deduped,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.2490566037735849,0.026616482980501715
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_biology,5-shot,accuracy,0.2986111111111111,0.03827052357950756
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_biology,5-shot,acc_norm,0.2986111111111111,0.03827052357950756
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_chemistry,5-shot,accuracy,0.24,0.04292346959909281
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_chemistry,5-shot,acc_norm,0.24,0.04292346959909281
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_computer_science,5-shot,accuracy,0.35,0.0479372485441102
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_computer_science,5-shot,acc_norm,0.35,0.0479372485441102
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_mathematics,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_mathematics,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_medicine,5-shot,accuracy,0.20809248554913296,0.030952890217749884
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_medicine,5-shot,acc_norm,0.20809248554913296,0.030952890217749884
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_physics,5-shot,accuracy,0.21568627450980393,0.04092563958237654
EleutherAI/pythia-1.4b-deduped,hendrycksTest-college_physics,5-shot,acc_norm,0.21568627450980393,0.04092563958237654
EleutherAI/pythia-1.4b-deduped,hendrycksTest-computer_security,5-shot,accuracy,0.27,0.0446196043338474
EleutherAI/pythia-1.4b-deduped,hendrycksTest-computer_security,5-shot,acc_norm,0.27,0.0446196043338474
EleutherAI/pythia-1.4b-deduped,hendrycksTest-conceptual_physics,5-shot,accuracy,0.2765957446808511,0.029241883869628827
EleutherAI/pythia-1.4b-deduped,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.2765957446808511,0.029241883869628827
EleutherAI/pythia-1.4b-deduped,hendrycksTest-econometrics,5-shot,accuracy,0.21929824561403508,0.03892431106518752
EleutherAI/pythia-1.4b-deduped,hendrycksTest-econometrics,5-shot,acc_norm,0.21929824561403508,0.03892431106518752
EleutherAI/pythia-1.4b-deduped,hendrycksTest-electrical_engineering,5-shot,accuracy,0.32413793103448274,0.03900432069185553
EleutherAI/pythia-1.4b-deduped,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.32413793103448274,0.03900432069185553
EleutherAI/pythia-1.4b-deduped,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.25925925925925924,0.022569897074918417
EleutherAI/pythia-1.4b-deduped,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.25925925925925924,0.022569897074918417
EleutherAI/pythia-1.4b-deduped,hendrycksTest-formal_logic,5-shot,accuracy,0.23809523809523808,0.03809523809523811
EleutherAI/pythia-1.4b-deduped,hendrycksTest-formal_logic,5-shot,acc_norm,0.23809523809523808,0.03809523809523811
EleutherAI/pythia-1.4b-deduped,hendrycksTest-global_facts,5-shot,accuracy,0.34,0.047609522856952365
EleutherAI/pythia-1.4b-deduped,hendrycksTest-global_facts,5-shot,acc_norm,0.34,0.047609522856952365
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_biology,5-shot,accuracy,0.24193548387096775,0.02436259969303109
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_biology,5-shot,acc_norm,0.24193548387096775,0.02436259969303109
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.23645320197044334,0.029896114291733545
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.23645320197044334,0.029896114291733545
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.33,0.04725815626252605
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.33,0.04725815626252605
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_european_history,5-shot,accuracy,0.24242424242424243,0.033464098810559534
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.24242424242424243,0.033464098810559534
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_geography,5-shot,accuracy,0.16666666666666666,0.02655220782821529
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_geography,5-shot,acc_norm,0.16666666666666666,0.02655220782821529
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.20725388601036268,0.029252823291803617
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.20725388601036268,0.029252823291803617
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.23846153846153847,0.021606294494647727
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.23846153846153847,0.021606294494647727
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2518518518518518,0.026466117538959916
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2518518518518518,0.026466117538959916
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.20588235294117646,0.026265024608275886
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.20588235294117646,0.026265024608275886
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_physics,5-shot,accuracy,0.271523178807947,0.03631329803969653
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_physics,5-shot,acc_norm,0.271523178807947,0.03631329803969653
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_psychology,5-shot,accuracy,0.20550458715596331,0.017324352325016
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.20550458715596331,0.017324352325016
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_statistics,5-shot,accuracy,0.4166666666666667,0.03362277436608043
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.4166666666666667,0.03362277436608043
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_us_history,5-shot,accuracy,0.24509803921568626,0.030190282453501936
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.24509803921568626,0.030190282453501936
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_world_history,5-shot,accuracy,0.2489451476793249,0.028146970599422644
EleutherAI/pythia-1.4b-deduped,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.2489451476793249,0.028146970599422644
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_aging,5-shot,accuracy,0.3183856502242152,0.03126580522513714
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_aging,5-shot,acc_norm,0.3183856502242152,0.03126580522513714
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_sexuality,5-shot,accuracy,0.22900763358778625,0.036853466317118506
EleutherAI/pythia-1.4b-deduped,hendrycksTest-human_sexuality,5-shot,acc_norm,0.22900763358778625,0.036853466317118506
EleutherAI/pythia-1.4b-deduped,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.03984979653302871
EleutherAI/pythia-1.4b-deduped,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.03984979653302871
EleutherAI/pythia-1.4b-deduped,hendrycksTest-jurisprudence,5-shot,accuracy,0.25925925925925924,0.042365112580946336
EleutherAI/pythia-1.4b-deduped,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25925925925925924,0.042365112580946336
EleutherAI/pythia-1.4b-deduped,hendrycksTest-logical_fallacies,5-shot,accuracy,0.24539877300613497,0.03380939813943353
EleutherAI/pythia-1.4b-deduped,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.24539877300613497,0.03380939813943353
EleutherAI/pythia-1.4b-deduped,hendrycksTest-machine_learning,5-shot,accuracy,0.25892857142857145,0.04157751539865629
EleutherAI/pythia-1.4b-deduped,hendrycksTest-machine_learning,5-shot,acc_norm,0.25892857142857145,0.04157751539865629
EleutherAI/pythia-1.4b-deduped,hendrycksTest-management,5-shot,accuracy,0.17475728155339806,0.037601780060266224
EleutherAI/pythia-1.4b-deduped,hendrycksTest-management,5-shot,acc_norm,0.17475728155339806,0.037601780060266224
EleutherAI/pythia-1.4b-deduped,hendrycksTest-marketing,5-shot,accuracy,0.2692307692307692,0.029058588303748842
EleutherAI/pythia-1.4b-deduped,hendrycksTest-marketing,5-shot,acc_norm,0.2692307692307692,0.029058588303748842
EleutherAI/pythia-1.4b-deduped,hendrycksTest-medical_genetics,5-shot,accuracy,0.26,0.0440844002276808
EleutherAI/pythia-1.4b-deduped,hendrycksTest-medical_genetics,5-shot,acc_norm,0.26,0.0440844002276808
EleutherAI/pythia-1.4b-deduped,hendrycksTest-miscellaneous,5-shot,accuracy,0.23499361430395913,0.015162024152278445
EleutherAI/pythia-1.4b-deduped,hendrycksTest-miscellaneous,5-shot,acc_norm,0.23499361430395913,0.015162024152278445
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_disputes,5-shot,accuracy,0.26878612716763006,0.023868003262500125
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_disputes,5-shot,acc_norm,0.26878612716763006,0.023868003262500125
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23798882681564246,0.014242630070574915
EleutherAI/pythia-1.4b-deduped,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23798882681564246,0.014242630070574915
EleutherAI/pythia-1.4b-deduped,hendrycksTest-nutrition,5-shot,accuracy,0.26143790849673204,0.025160998214292456
EleutherAI/pythia-1.4b-deduped,hendrycksTest-nutrition,5-shot,acc_norm,0.26143790849673204,0.025160998214292456
EleutherAI/pythia-1.4b-deduped,hendrycksTest-philosophy,5-shot,accuracy,0.24115755627009647,0.02429659403476343
EleutherAI/pythia-1.4b-deduped,hendrycksTest-philosophy,5-shot,acc_norm,0.24115755627009647,0.02429659403476343
EleutherAI/pythia-1.4b-deduped,hendrycksTest-prehistory,5-shot,accuracy,0.24074074074074073,0.023788583551658533
EleutherAI/pythia-1.4b-deduped,hendrycksTest-prehistory,5-shot,acc_norm,0.24074074074074073,0.023788583551658533
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_accounting,5-shot,accuracy,0.2872340425531915,0.026992199173064356
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2872340425531915,0.026992199173064356
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_law,5-shot,accuracy,0.24967405475880053,0.011054538377832317
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_law,5-shot,acc_norm,0.24967405475880053,0.011054538377832317
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_medicine,5-shot,accuracy,0.22058823529411764,0.02518778666022725
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_medicine,5-shot,acc_norm,0.22058823529411764,0.02518778666022725
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_psychology,5-shot,accuracy,0.2549019607843137,0.017630827375148383
EleutherAI/pythia-1.4b-deduped,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2549019607843137,0.017630827375148383
EleutherAI/pythia-1.4b-deduped,hendrycksTest-public_relations,5-shot,accuracy,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-1.4b-deduped,hendrycksTest-public_relations,5-shot,acc_norm,0.21818181818181817,0.03955932861795833
EleutherAI/pythia-1.4b-deduped,hendrycksTest-security_studies,5-shot,accuracy,0.20816326530612245,0.0259911176728133
EleutherAI/pythia-1.4b-deduped,hendrycksTest-security_studies,5-shot,acc_norm,0.20816326530612245,0.0259911176728133
EleutherAI/pythia-1.4b-deduped,hendrycksTest-sociology,5-shot,accuracy,0.23880597014925373,0.030147775935409217
EleutherAI/pythia-1.4b-deduped,hendrycksTest-sociology,5-shot,acc_norm,0.23880597014925373,0.030147775935409217
EleutherAI/pythia-1.4b-deduped,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.3,0.046056618647183814
EleutherAI/pythia-1.4b-deduped,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.3,0.046056618647183814
EleutherAI/pythia-1.4b-deduped,hendrycksTest-virology,5-shot,accuracy,0.25903614457831325,0.034106466140718564
EleutherAI/pythia-1.4b-deduped,hendrycksTest-virology,5-shot,acc_norm,0.25903614457831325,0.034106466140718564
EleutherAI/pythia-1.4b-deduped,hendrycksTest-world_religions,5-shot,accuracy,0.3157894736842105,0.035650796707083106
EleutherAI/pythia-1.4b-deduped,hendrycksTest-world_religions,5-shot,acc_norm,0.3157894736842105,0.035650796707083106
EleutherAI/pythia-1.4b-deduped,truthfulqa:mc,0-shot,mc1,0.23011015911872704,0.014734557959807763
EleutherAI/pythia-1.4b-deduped,truthfulqa:mc,0-shot,mc2,0.3865846445222244,0.0139993827663357
EleutherAI/pythia-1.4b-deduped,drop,3-shot,accuracy,0.0014681208053691276,0.0003921042190298455
EleutherAI/pythia-1.4b-deduped,drop,3-shot,f1,0.04330536912751699,0.0011661836886516016
EleutherAI/pythia-1.4b-deduped,gsm8k,5-shot,accuracy,0.008339651250947688,0.002504942226860525
EleutherAI/pythia-1.4b-deduped,winogrande,5-shot,accuracy,0.5730071033938438,0.013901878072575058
facebook/opt-125m,minerva_math_precalc,5-shot,accuracy,0.0018315018315018315,0.001831501831501841
facebook/opt-125m,minerva_math_prealgebra,5-shot,accuracy,0.004592422502870264,0.0022922488477037954
facebook/opt-125m,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/opt-125m,minerva_math_intermediate_algebra,5-shot,accuracy,0.0011074197120708748,0.001107419712070882
facebook/opt-125m,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/opt-125m,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/opt-125m,minerva_math_algebra,5-shot,accuracy,0.006739679865206402,0.0023757942810498857
facebook/opt-125m,fld_default,0-shot,accuracy,0.0,
facebook/opt-125m,fld_star,0-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_3da,5-shot,accuracy,0.001,0.0007069298939339458
facebook/opt-125m,arithmetic_3ds,5-shot,accuracy,0.0015,0.0008655920660521539
facebook/opt-125m,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
facebook/opt-125m,arithmetic_2ds,5-shot,accuracy,0.012,0.002435357362429822
facebook/opt-125m,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_5da,5-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_1dc,5-shot,accuracy,0.003,0.001223212215464699
facebook/opt-125m,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/opt-125m,arithmetic_2dm,5-shot,accuracy,0.023,0.003352778036238045
facebook/opt-125m,arithmetic_2da,5-shot,accuracy,0.008,0.0019924821184884632
facebook/opt-125m,gsm8k_cot,5-shot,accuracy,0.01592115238817286,0.0034478192723889985
facebook/opt-125m,anli_r2,0-shot,brier_score,0.7830871364529988,
facebook/opt-125m,anli_r3,0-shot,brier_score,0.7953205099615661,
facebook/opt-125m,anli_r1,0-shot,brier_score,0.8170221338251071,
facebook/opt-125m,xnli_eu,0-shot,brier_score,1.2285116904787872,
facebook/opt-125m,xnli_vi,0-shot,brier_score,1.0784582678662726,
facebook/opt-125m,xnli_ru,0-shot,brier_score,0.9507820309163365,
facebook/opt-125m,xnli_zh,0-shot,brier_score,0.8250929559751068,
facebook/opt-125m,xnli_tr,0-shot,brier_score,1.0901246908488005,
facebook/opt-125m,xnli_fr,0-shot,brier_score,0.8265299752835056,
facebook/opt-125m,xnli_en,0-shot,brier_score,0.7095494061851583,
facebook/opt-125m,xnli_ur,0-shot,brier_score,1.2931860597684186,
facebook/opt-125m,xnli_ar,0-shot,brier_score,0.8195340201469077,
facebook/opt-125m,xnli_de,0-shot,brier_score,0.8832758677383753,
facebook/opt-125m,xnli_hi,0-shot,brier_score,1.201948609364907,
facebook/opt-125m,xnli_es,0-shot,brier_score,1.0140681729559233,
facebook/opt-125m,xnli_bg,0-shot,brier_score,1.298722997987076,
facebook/opt-125m,xnli_sw,0-shot,brier_score,0.959071321033876,
facebook/opt-125m,xnli_el,0-shot,brier_score,0.9979876427435008,
facebook/opt-125m,xnli_th,0-shot,brier_score,1.2628707586880557,
facebook/opt-125m,logiqa2,0-shot,brier_score,1.1225169562376798,
facebook/opt-125m,mathqa,5-shot,brier_score,1.0330427505083621,
facebook/opt-125m,lambada_standard,0-shot,perplexity,73.09771501293442,3.093022125006431
facebook/opt-125m,lambada_standard,0-shot,accuracy,0.289928197166699,0.006321329576857221
facebook/opt-125m,lambada_openai,0-shot,perplexity,26.021596320893057,0.9407497764446304
facebook/opt-125m,lambada_openai,0-shot,accuracy,0.3784203376673782,0.006756903326773717
facebook/opt-125m,mmlu_world_religions,0-shot,accuracy,0.17543859649122806,0.029170885500727686
facebook/opt-125m,mmlu_formal_logic,0-shot,accuracy,0.1349206349206349,0.030557101589417508
facebook/opt-125m,mmlu_prehistory,0-shot,accuracy,0.2962962962962963,0.025407197798890162
facebook/opt-125m,mmlu_moral_scenarios,0-shot,accuracy,0.2435754189944134,0.01435591196476786
facebook/opt-125m,mmlu_high_school_world_history,0-shot,accuracy,0.270042194092827,0.028900721906293426
facebook/opt-125m,mmlu_moral_disputes,0-shot,accuracy,0.23699421965317918,0.02289408248992599
facebook/opt-125m,mmlu_professional_law,0-shot,accuracy,0.25488917861799215,0.011130509812662963
facebook/opt-125m,mmlu_logical_fallacies,0-shot,accuracy,0.22085889570552147,0.03259177392742178
facebook/opt-125m,mmlu_high_school_us_history,0-shot,accuracy,0.25980392156862747,0.03077855467869326
facebook/opt-125m,mmlu_philosophy,0-shot,accuracy,0.24115755627009647,0.024296594034763426
facebook/opt-125m,mmlu_jurisprudence,0-shot,accuracy,0.21296296296296297,0.03957835471980981
facebook/opt-125m,mmlu_international_law,0-shot,accuracy,0.38016528925619836,0.04431324501968431
facebook/opt-125m,mmlu_high_school_european_history,0-shot,accuracy,0.21212121212121213,0.031922715695483
facebook/opt-125m,mmlu_high_school_government_and_politics,0-shot,accuracy,0.36787564766839376,0.034801756684660366
facebook/opt-125m,mmlu_high_school_microeconomics,0-shot,accuracy,0.3445378151260504,0.030868682604121626
facebook/opt-125m,mmlu_high_school_geography,0-shot,accuracy,0.2828282828282828,0.03208779558786752
facebook/opt-125m,mmlu_high_school_psychology,0-shot,accuracy,0.22752293577981653,0.017974463578776502
facebook/opt-125m,mmlu_public_relations,0-shot,accuracy,0.22727272727272727,0.04013964554072775
facebook/opt-125m,mmlu_us_foreign_policy,0-shot,accuracy,0.3,0.046056618647183814
facebook/opt-125m,mmlu_sociology,0-shot,accuracy,0.23383084577114427,0.029929415408348366
facebook/opt-125m,mmlu_high_school_macroeconomics,0-shot,accuracy,0.358974358974359,0.024321738484602357
facebook/opt-125m,mmlu_security_studies,0-shot,accuracy,0.24897959183673468,0.027682979522960227
facebook/opt-125m,mmlu_professional_psychology,0-shot,accuracy,0.2222222222222222,0.016819028375736386
facebook/opt-125m,mmlu_human_sexuality,0-shot,accuracy,0.25190839694656486,0.03807387116306085
facebook/opt-125m,mmlu_econometrics,0-shot,accuracy,0.22807017543859648,0.03947152782669415
facebook/opt-125m,mmlu_miscellaneous,0-shot,accuracy,0.24265644955300128,0.015329888940899858
facebook/opt-125m,mmlu_marketing,0-shot,accuracy,0.19658119658119658,0.02603538609895129
facebook/opt-125m,mmlu_management,0-shot,accuracy,0.18446601941747573,0.03840423627288276
facebook/opt-125m,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.02526169121972948
facebook/opt-125m,mmlu_medical_genetics,0-shot,accuracy,0.34,0.047609522856952365
facebook/opt-125m,mmlu_human_aging,0-shot,accuracy,0.20179372197309417,0.02693611191280227
facebook/opt-125m,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.030211479609121593
facebook/opt-125m,mmlu_college_medicine,0-shot,accuracy,0.2138728323699422,0.03126511206173043
facebook/opt-125m,mmlu_business_ethics,0-shot,accuracy,0.21,0.040936018074033256
facebook/opt-125m,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.02528839450289137
facebook/opt-125m,mmlu_global_facts,0-shot,accuracy,0.19,0.039427724440366234
facebook/opt-125m,mmlu_virology,0-shot,accuracy,0.21084337349397592,0.031755547866299194
facebook/opt-125m,mmlu_professional_accounting,0-shot,accuracy,0.25886524822695034,0.026129572527180848
facebook/opt-125m,mmlu_college_physics,0-shot,accuracy,0.37254901960784315,0.04810840148082634
facebook/opt-125m,mmlu_high_school_physics,0-shot,accuracy,0.31788079470198677,0.038020397601079024
facebook/opt-125m,mmlu_high_school_biology,0-shot,accuracy,0.3161290322580645,0.026450874489042767
facebook/opt-125m,mmlu_college_biology,0-shot,accuracy,0.2222222222222222,0.03476590104304134
facebook/opt-125m,mmlu_anatomy,0-shot,accuracy,0.24444444444444444,0.037125378336148665
facebook/opt-125m,mmlu_college_chemistry,0-shot,accuracy,0.29,0.045604802157206845
facebook/opt-125m,mmlu_computer_security,0-shot,accuracy,0.18,0.038612291966536955
facebook/opt-125m,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526045
facebook/opt-125m,mmlu_astronomy,0-shot,accuracy,0.2565789473684211,0.0355418036802569
facebook/opt-125m,mmlu_college_mathematics,0-shot,accuracy,0.28,0.04512608598542127
facebook/opt-125m,mmlu_conceptual_physics,0-shot,accuracy,0.3021276595744681,0.030017554471880557
facebook/opt-125m,mmlu_abstract_algebra,0-shot,accuracy,0.29,0.04560480215720684
facebook/opt-125m,mmlu_high_school_computer_science,0-shot,accuracy,0.19,0.03942772444036625
facebook/opt-125m,mmlu_machine_learning,0-shot,accuracy,0.16964285714285715,0.0356236785009539
facebook/opt-125m,mmlu_high_school_chemistry,0-shot,accuracy,0.30049261083743845,0.032257994762334846
facebook/opt-125m,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
facebook/opt-125m,mmlu_elementary_mathematics,0-shot,accuracy,0.2566137566137566,0.022494510767503154
facebook/opt-125m,mmlu_electrical_engineering,0-shot,accuracy,0.25517241379310346,0.03632984052707842
facebook/opt-125m,mmlu_high_school_mathematics,0-shot,accuracy,0.26296296296296295,0.026842057873833706
facebook/opt-125m,arc_challenge,25-shot,accuracy,0.20733788395904437,0.011846905782971333
facebook/opt-125m,arc_challenge,25-shot,acc_norm,0.2226962457337884,0.012158314774829928
facebook/opt-125m,truthfulqa_mc2,0-shot,accuracy,0.428871922722071,0.015069821806716208
facebook/opt-125m,truthfulqa_gen,0-shot,bleu_max,15.720420721623517,0.5472384436992891
facebook/opt-125m,truthfulqa_gen,0-shot,bleu_acc,0.412484700122399,0.017233299399571207
facebook/opt-125m,truthfulqa_gen,0-shot,bleu_diff,-0.8644446662587123,0.5171860112672116
facebook/opt-125m,truthfulqa_gen,0-shot,rouge1_max,36.57421967061916,0.7883288595416582
facebook/opt-125m,truthfulqa_gen,0-shot,rouge1_acc,0.3488372093023256,0.01668441985998691
facebook/opt-125m,truthfulqa_gen,0-shot,rouge1_diff,-3.1802654121876426,0.7798761989232565
facebook/opt-125m,truthfulqa_gen,0-shot,rouge2_max,18.635541322636527,0.8253607772770336
facebook/opt-125m,truthfulqa_gen,0-shot,rouge2_acc,0.20807833537331702,0.01421050347357663
facebook/opt-125m,truthfulqa_gen,0-shot,rouge2_diff,-3.48758369439353,0.7563641375616611
facebook/opt-125m,truthfulqa_gen,0-shot,rougeL_max,34.10946249460258,0.7777451900951593
facebook/opt-125m,truthfulqa_gen,0-shot,rougeL_acc,0.3525091799265606,0.016724646380756554
facebook/opt-125m,truthfulqa_gen,0-shot,rougeL_diff,-2.817674709105612,0.7655327681953442
facebook/opt-125m,truthfulqa_mc1,0-shot,accuracy,0.24112607099143207,0.01497482727975234
Devio/test-3b,minerva_math_precalc,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_prealgebra,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_num_theory,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_geometry,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
Devio/test-3b,minerva_math_algebra,5-shot,accuracy,0.0,
Devio/test-3b,fld_default,0-shot,accuracy,0.0,
Devio/test-3b,fld_star,0-shot,accuracy,0.0,
Devio/test-3b,arithmetic_3da,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_3ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_4da,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_2ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_5ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_5da,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_1dc,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_4ds,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_2dm,5-shot,accuracy,0.0,
Devio/test-3b,arithmetic_2da,5-shot,accuracy,0.0,
Devio/test-3b,gsm8k_cot,5-shot,accuracy,0.0,
Devio/test-3b,gsm8k,5-shot,accuracy,0.0,
Devio/test-3b,anli_r2,0-shot,brier_score,1.2297922636861298,
Devio/test-3b,anli_r3,0-shot,brier_score,1.2333997188823227,
Devio/test-3b,anli_r1,0-shot,brier_score,1.2298489673650648,
Devio/test-3b,xnli_eu,0-shot,brier_score,1.0534244785984144,
Devio/test-3b,xnli_vi,0-shot,brier_score,1.3218456507070504,
Devio/test-3b,xnli_ru,0-shot,brier_score,0.8656071346235457,
Devio/test-3b,xnli_zh,0-shot,brier_score,0.9476036633633778,
Devio/test-3b,xnli_tr,0-shot,brier_score,0.9925860351979764,
Devio/test-3b,xnli_fr,0-shot,brier_score,0.957790185963517,
Devio/test-3b,xnli_en,0-shot,brier_score,0.7251422341685971,
Devio/test-3b,xnli_ur,0-shot,brier_score,1.0854404485149611,
Devio/test-3b,xnli_ar,0-shot,brier_score,0.8967637727978277,
Devio/test-3b,xnli_de,0-shot,brier_score,1.0362908584253308,
Devio/test-3b,xnli_hi,0-shot,brier_score,1.0781294620168924,
Devio/test-3b,xnli_es,0-shot,brier_score,1.2906893281594574,
Devio/test-3b,xnli_bg,0-shot,brier_score,0.9965475745381035,
Devio/test-3b,xnli_sw,0-shot,brier_score,1.115504637558115,
Devio/test-3b,xnli_el,0-shot,brier_score,1.155706973028575,
Devio/test-3b,xnli_th,0-shot,brier_score,1.1482107613489194,
Devio/test-3b,logiqa2,0-shot,brier_score,1.5177031880020304,
Devio/test-3b,mathqa,5-shot,brier_score,1.014650380983031,
Devio/test-3b,lambada_standard,0-shot,perplexity,538584815.1171707,54349236.53521809
Devio/test-3b,lambada_standard,0-shot,accuracy,0.00019406171162429653,0.0001940617116243046
Devio/test-3b,lambada_openai,0-shot,perplexity,254132266.18287274,26030411.523864448
Devio/test-3b,lambada_openai,0-shot,accuracy,0.0,
Devio/test-3b,mmlu_world_religions,0-shot,accuracy,0.3216374269005848,0.03582529442573122
Devio/test-3b,mmlu_formal_logic,0-shot,accuracy,0.36507936507936506,0.04306241259127153
Devio/test-3b,mmlu_prehistory,0-shot,accuracy,0.24074074074074073,0.023788583551658544
Devio/test-3b,mmlu_moral_scenarios,0-shot,accuracy,0.24692737430167597,0.01442229220480886
Devio/test-3b,mmlu_high_school_world_history,0-shot,accuracy,0.25738396624472576,0.028458820991460285
Devio/test-3b,mmlu_moral_disputes,0-shot,accuracy,0.21965317919075145,0.022289638852617893
Devio/test-3b,mmlu_professional_law,0-shot,accuracy,0.2503259452411995,0.011064151027165441
Devio/test-3b,mmlu_logical_fallacies,0-shot,accuracy,0.2331288343558282,0.03322015795776741
Devio/test-3b,mmlu_high_school_us_history,0-shot,accuracy,0.25980392156862747,0.030778554678693257
Devio/test-3b,mmlu_philosophy,0-shot,accuracy,0.21864951768488747,0.02347558141786111
Devio/test-3b,mmlu_jurisprudence,0-shot,accuracy,0.21296296296296297,0.03957835471980979
Devio/test-3b,mmlu_international_law,0-shot,accuracy,0.2396694214876033,0.03896878985070417
Devio/test-3b,mmlu_high_school_european_history,0-shot,accuracy,0.21212121212121213,0.03192271569548299
Devio/test-3b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.34196891191709844,0.03423465100104283
Devio/test-3b,mmlu_high_school_microeconomics,0-shot,accuracy,0.3445378151260504,0.030868682604121626
Devio/test-3b,mmlu_high_school_geography,0-shot,accuracy,0.25252525252525254,0.030954055470365907
Devio/test-3b,mmlu_high_school_psychology,0-shot,accuracy,0.3467889908256881,0.020406097104093027
Devio/test-3b,mmlu_public_relations,0-shot,accuracy,0.21818181818181817,0.03955932861795833
Devio/test-3b,mmlu_us_foreign_policy,0-shot,accuracy,0.28,0.04512608598542129
Devio/test-3b,mmlu_sociology,0-shot,accuracy,0.2537313432835821,0.030769444967296018
Devio/test-3b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.3641025641025641,0.024396672985094767
Devio/test-3b,mmlu_security_studies,0-shot,accuracy,0.24489795918367346,0.027529637440174934
Devio/test-3b,mmlu_professional_psychology,0-shot,accuracy,0.2173202614379085,0.016684820929148598
Devio/test-3b,mmlu_human_sexuality,0-shot,accuracy,0.26717557251908397,0.03880848301082396
Devio/test-3b,mmlu_econometrics,0-shot,accuracy,0.2719298245614035,0.04185774424022056
Devio/test-3b,mmlu_miscellaneous,0-shot,accuracy,0.23627075351213284,0.015190473717037497
Devio/test-3b,mmlu_marketing,0-shot,accuracy,0.19658119658119658,0.02603538609895129
Devio/test-3b,mmlu_management,0-shot,accuracy,0.32038834951456313,0.0462028408228004
Devio/test-3b,mmlu_nutrition,0-shot,accuracy,0.2647058823529412,0.02526169121972948
Devio/test-3b,mmlu_medical_genetics,0-shot,accuracy,0.23,0.04229525846816506
Devio/test-3b,mmlu_human_aging,0-shot,accuracy,0.27802690582959644,0.030069584874494043
Devio/test-3b,mmlu_professional_medicine,0-shot,accuracy,0.4485294117647059,0.0302114796091216
Devio/test-3b,mmlu_college_medicine,0-shot,accuracy,0.35260115606936415,0.03643037168958548
Devio/test-3b,mmlu_business_ethics,0-shot,accuracy,0.21,0.04093601807403326
Devio/test-3b,mmlu_clinical_knowledge,0-shot,accuracy,0.3018867924528302,0.028254200344438655
Devio/test-3b,mmlu_global_facts,0-shot,accuracy,0.2,0.040201512610368445
Devio/test-3b,mmlu_virology,0-shot,accuracy,0.2289156626506024,0.03270745277352477
Devio/test-3b,mmlu_professional_accounting,0-shot,accuracy,0.2375886524822695,0.0253895125527299
Devio/test-3b,mmlu_college_physics,0-shot,accuracy,0.23529411764705882,0.04220773659171453
Devio/test-3b,mmlu_high_school_physics,0-shot,accuracy,0.33774834437086093,0.0386155754625517
Devio/test-3b,mmlu_high_school_biology,0-shot,accuracy,0.3161290322580645,0.02645087448904276
Devio/test-3b,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.03716177437566017
Devio/test-3b,mmlu_anatomy,0-shot,accuracy,0.22962962962962963,0.036333844140734636
Devio/test-3b,mmlu_college_chemistry,0-shot,accuracy,0.4,0.049236596391733084
Devio/test-3b,mmlu_computer_security,0-shot,accuracy,0.2,0.040201512610368445
Devio/test-3b,mmlu_college_computer_science,0-shot,accuracy,0.33,0.047258156262526045
Devio/test-3b,mmlu_astronomy,0-shot,accuracy,0.32894736842105265,0.038234289699266046
Devio/test-3b,mmlu_college_mathematics,0-shot,accuracy,0.31,0.04648231987117316
Devio/test-3b,mmlu_conceptual_physics,0-shot,accuracy,0.20851063829787234,0.02655698211783872
Devio/test-3b,mmlu_abstract_algebra,0-shot,accuracy,0.26,0.04408440022768077
Devio/test-3b,mmlu_high_school_computer_science,0-shot,accuracy,0.2,0.040201512610368445
Devio/test-3b,mmlu_machine_learning,0-shot,accuracy,0.16071428571428573,0.0348594609647574
Devio/test-3b,mmlu_high_school_chemistry,0-shot,accuracy,0.26108374384236455,0.030903796952114482
Devio/test-3b,mmlu_high_school_statistics,0-shot,accuracy,0.4722222222222222,0.0340470532865388
Devio/test-3b,mmlu_elementary_mathematics,0-shot,accuracy,0.2222222222222222,0.02141168439369418
Devio/test-3b,mmlu_electrical_engineering,0-shot,accuracy,0.25517241379310346,0.03632984052707842
Devio/test-3b,mmlu_high_school_mathematics,0-shot,accuracy,0.25555555555555554,0.026593939101844058
Devio/test-3b,arc_challenge,25-shot,accuracy,0.20477815699658702,0.011792544338513414
Devio/test-3b,arc_challenge,25-shot,acc_norm,0.2619453924914676,0.01284905482685811
Devio/test-3b,hellaswag,10-shot,accuracy,0.25951005775741887,0.004374699189284862
Devio/test-3b,hellaswag,10-shot,acc_norm,0.26329416450906196,0.004395205528158076
Devio/test-3b,truthfulqa_mc2,0-shot,accuracy,0.4854291073268079,0.016647522216000306
Devio/test-3b,truthfulqa_gen,0-shot,bleu_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,bleu_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,bleu_diff,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge1_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge1_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge1_diff,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge2_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge2_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rouge2_diff,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rougeL_max,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rougeL_acc,0.0,
Devio/test-3b,truthfulqa_gen,0-shot,rougeL_diff,0.0,
Devio/test-3b,truthfulqa_mc1,0-shot,accuracy,0.2386780905752754,0.014922629695456416
Devio/test-3b,winogrande,5-shot,accuracy,0.489344909234412,0.014049294536290393
LLM360/Amber,arc:challenge,25-shot,accuracy,0.39761092150170646,0.014301752223279536
LLM360/Amber,arc:challenge,25-shot,acc_norm,0.40955631399317405,0.014370358632472437
LLM360/Amber,hellaswag,10-shot,accuracy,0.5478988249352719,0.004966832553245046
LLM360/Amber,hellaswag,10-shot,acc_norm,0.7379008165704043,0.00438877529821019
LLM360/Amber,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.04725815626252606
LLM360/Amber,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.04725815626252606
LLM360/Amber,hendrycksTest-anatomy,5-shot,accuracy,0.2222222222222222,0.035914440841969694
LLM360/Amber,hendrycksTest-anatomy,5-shot,acc_norm,0.2222222222222222,0.035914440841969694
LLM360/Amber,hendrycksTest-astronomy,5-shot,accuracy,0.2631578947368421,0.03583496176361065
LLM360/Amber,hendrycksTest-astronomy,5-shot,acc_norm,0.2631578947368421,0.03583496176361065
LLM360/Amber,hendrycksTest-business_ethics,5-shot,accuracy,0.31,0.04648231987117316
LLM360/Amber,hendrycksTest-business_ethics,5-shot,acc_norm,0.31,0.04648231987117316
LLM360/Amber,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.21132075471698114,0.025125766484827845
LLM360/Amber,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.21132075471698114,0.025125766484827845
LLM360/Amber,hendrycksTest-college_biology,5-shot,accuracy,0.2708333333333333,0.03716177437566018
LLM360/Amber,hendrycksTest-college_biology,5-shot,acc_norm,0.2708333333333333,0.03716177437566018
LLM360/Amber,hendrycksTest-college_chemistry,5-shot,accuracy,0.22,0.041633319989322695
LLM360/Amber,hendrycksTest-college_chemistry,5-shot,acc_norm,0.22,0.041633319989322695
LLM360/Amber,hendrycksTest-college_computer_science,5-shot,accuracy,0.4,0.049236596391733084
LLM360/Amber,hendrycksTest-college_computer_science,5-shot,acc_norm,0.4,0.049236596391733084
LLM360/Amber,hendrycksTest-college_mathematics,5-shot,accuracy,0.22,0.04163331998932269
LLM360/Amber,hendrycksTest-college_mathematics,5-shot,acc_norm,0.22,0.04163331998932269
LLM360/Amber,hendrycksTest-college_medicine,5-shot,accuracy,0.24855491329479767,0.03295304696818318
LLM360/Amber,hendrycksTest-college_medicine,5-shot,acc_norm,0.24855491329479767,0.03295304696818318
LLM360/Amber,hendrycksTest-college_physics,5-shot,accuracy,0.17647058823529413,0.03793281185307809
LLM360/Amber,hendrycksTest-college_physics,5-shot,acc_norm,0.17647058823529413,0.03793281185307809
LLM360/Amber,hendrycksTest-computer_security,5-shot,accuracy,0.3,0.046056618647183814
LLM360/Amber,hendrycksTest-computer_security,5-shot,acc_norm,0.3,0.046056618647183814
LLM360/Amber,hendrycksTest-conceptual_physics,5-shot,accuracy,0.251063829787234,0.02834696377716246
LLM360/Amber,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.251063829787234,0.02834696377716246
LLM360/Amber,hendrycksTest-econometrics,5-shot,accuracy,0.2631578947368421,0.0414243971948936
LLM360/Amber,hendrycksTest-econometrics,5-shot,acc_norm,0.2631578947368421,0.0414243971948936
LLM360/Amber,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3103448275862069,0.03855289616378947
LLM360/Amber,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3103448275862069,0.03855289616378947
LLM360/Amber,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2777777777777778,0.02306818884826111
LLM360/Amber,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2777777777777778,0.02306818884826111
LLM360/Amber,hendrycksTest-formal_logic,5-shot,accuracy,0.29365079365079366,0.040735243221471276
LLM360/Amber,hendrycksTest-formal_logic,5-shot,acc_norm,0.29365079365079366,0.040735243221471276
LLM360/Amber,hendrycksTest-global_facts,5-shot,accuracy,0.29,0.04560480215720684
LLM360/Amber,hendrycksTest-global_facts,5-shot,acc_norm,0.29,0.04560480215720684
LLM360/Amber,hendrycksTest-high_school_biology,5-shot,accuracy,0.23870967741935484,0.02425107126220884
LLM360/Amber,hendrycksTest-high_school_biology,5-shot,acc_norm,0.23870967741935484,0.02425107126220884
LLM360/Amber,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.16748768472906403,0.026273086047535414
LLM360/Amber,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.16748768472906403,0.026273086047535414
LLM360/Amber,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.32,0.046882617226215034
LLM360/Amber,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.32,0.046882617226215034
LLM360/Amber,hendrycksTest-high_school_european_history,5-shot,accuracy,0.28484848484848485,0.035243908445117836
LLM360/Amber,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.28484848484848485,0.035243908445117836
LLM360/Amber,hendrycksTest-high_school_geography,5-shot,accuracy,0.1919191919191919,0.028057791672989024
LLM360/Amber,hendrycksTest-high_school_geography,5-shot,acc_norm,0.1919191919191919,0.028057791672989024
LLM360/Amber,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.19689119170984457,0.028697873971860664
LLM360/Amber,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.19689119170984457,0.028697873971860664
LLM360/Amber,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.22564102564102564,0.021193632525148533
LLM360/Amber,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.22564102564102564,0.021193632525148533
LLM360/Amber,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.2074074074074074,0.024720713193952165
LLM360/Amber,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.2074074074074074,0.024720713193952165
LLM360/Amber,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.24789915966386555,0.028047967224176896
LLM360/Amber,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.24789915966386555,0.028047967224176896
LLM360/Amber,hendrycksTest-high_school_physics,5-shot,accuracy,0.2582781456953642,0.035737053147634576
LLM360/Amber,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2582781456953642,0.035737053147634576
LLM360/Amber,hendrycksTest-high_school_psychology,5-shot,accuracy,0.2018348623853211,0.01720857935778755
LLM360/Amber,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.2018348623853211,0.01720857935778755
LLM360/Amber,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2777777777777778,0.030546745264953202
LLM360/Amber,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2777777777777778,0.030546745264953202
LLM360/Amber,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3235294117647059,0.03283472056108567
LLM360/Amber,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3235294117647059,0.03283472056108567
LLM360/Amber,hendrycksTest-high_school_world_history,5-shot,accuracy,0.29957805907172996,0.029818024749753095
LLM360/Amber,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.29957805907172996,0.029818024749753095
LLM360/Amber,hendrycksTest-human_aging,5-shot,accuracy,0.3004484304932735,0.030769352008229143
LLM360/Amber,hendrycksTest-human_aging,5-shot,acc_norm,0.3004484304932735,0.030769352008229143
LLM360/Amber,hendrycksTest-human_sexuality,5-shot,accuracy,0.3511450381679389,0.04186445163013751
LLM360/Amber,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3511450381679389,0.04186445163013751
LLM360/Amber,hendrycksTest-international_law,5-shot,accuracy,0.256198347107438,0.03984979653302871
LLM360/Amber,hendrycksTest-international_law,5-shot,acc_norm,0.256198347107438,0.03984979653302871
LLM360/Amber,hendrycksTest-jurisprudence,5-shot,accuracy,0.25,0.04186091791394607
LLM360/Amber,hendrycksTest-jurisprudence,5-shot,acc_norm,0.25,0.04186091791394607
LLM360/Amber,hendrycksTest-logical_fallacies,5-shot,accuracy,0.19631901840490798,0.031207970394709215
LLM360/Amber,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.19631901840490798,0.031207970394709215
LLM360/Amber,hendrycksTest-machine_learning,5-shot,accuracy,0.29464285714285715,0.04327040932578728
LLM360/Amber,hendrycksTest-machine_learning,5-shot,acc_norm,0.29464285714285715,0.04327040932578728
LLM360/Amber,hendrycksTest-management,5-shot,accuracy,0.17475728155339806,0.037601780060266224
LLM360/Amber,hendrycksTest-management,5-shot,acc_norm,0.17475728155339806,0.037601780060266224
LLM360/Amber,hendrycksTest-marketing,5-shot,accuracy,0.31196581196581197,0.03035152732334496
LLM360/Amber,hendrycksTest-marketing,5-shot,acc_norm,0.31196581196581197,0.03035152732334496
LLM360/Amber,hendrycksTest-medical_genetics,5-shot,accuracy,0.4,0.049236596391733084
LLM360/Amber,hendrycksTest-medical_genetics,5-shot,acc_norm,0.4,0.049236596391733084
LLM360/Amber,hendrycksTest-miscellaneous,5-shot,accuracy,0.2796934865900383,0.016050792148036522
LLM360/Amber,hendrycksTest-miscellaneous,5-shot,acc_norm,0.2796934865900383,0.016050792148036522
LLM360/Amber,hendrycksTest-moral_disputes,5-shot,accuracy,0.3265895953757225,0.025248264774242832
LLM360/Amber,hendrycksTest-moral_disputes,5-shot,acc_norm,0.3265895953757225,0.025248264774242832
LLM360/Amber,hendrycksTest-moral_scenarios,5-shot,accuracy,0.23687150837988827,0.01421957078810399
LLM360/Amber,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.23687150837988827,0.01421957078810399
LLM360/Amber,hendrycksTest-nutrition,5-shot,accuracy,0.27124183006535946,0.02545775669666787
LLM360/Amber,hendrycksTest-nutrition,5-shot,acc_norm,0.27124183006535946,0.02545775669666787
LLM360/Amber,hendrycksTest-philosophy,5-shot,accuracy,0.3183279742765273,0.02645722506781103
LLM360/Amber,hendrycksTest-philosophy,5-shot,acc_norm,0.3183279742765273,0.02645722506781103
LLM360/Amber,hendrycksTest-prehistory,5-shot,accuracy,0.25308641975308643,0.024191808600713002
LLM360/Amber,hendrycksTest-prehistory,5-shot,acc_norm,0.25308641975308643,0.024191808600713002
LLM360/Amber,hendrycksTest-professional_accounting,5-shot,accuracy,0.30141843971631205,0.02737412888263115
LLM360/Amber,hendrycksTest-professional_accounting,5-shot,acc_norm,0.30141843971631205,0.02737412888263115
LLM360/Amber,hendrycksTest-professional_law,5-shot,accuracy,0.2790091264667536,0.011455208832803545
LLM360/Amber,hendrycksTest-professional_law,5-shot,acc_norm,0.2790091264667536,0.011455208832803545
LLM360/Amber,hendrycksTest-professional_medicine,5-shot,accuracy,0.1875,0.023709788253811766
LLM360/Amber,hendrycksTest-professional_medicine,5-shot,acc_norm,0.1875,0.023709788253811766
LLM360/Amber,hendrycksTest-professional_psychology,5-shot,accuracy,0.30718954248366015,0.018663359671463663
LLM360/Amber,hendrycksTest-professional_psychology,5-shot,acc_norm,0.30718954248366015,0.018663359671463663
LLM360/Amber,hendrycksTest-public_relations,5-shot,accuracy,0.2818181818181818,0.043091187099464585
LLM360/Amber,hendrycksTest-public_relations,5-shot,acc_norm,0.2818181818181818,0.043091187099464585
LLM360/Amber,hendrycksTest-security_studies,5-shot,accuracy,0.1836734693877551,0.02478907133200763
LLM360/Amber,hendrycksTest-security_studies,5-shot,acc_norm,0.1836734693877551,0.02478907133200763
LLM360/Amber,hendrycksTest-sociology,5-shot,accuracy,0.2885572139303483,0.03203841040213322
LLM360/Amber,hendrycksTest-sociology,5-shot,acc_norm,0.2885572139303483,0.03203841040213322
LLM360/Amber,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.3,0.046056618647183814
LLM360/Amber,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.3,0.046056618647183814
LLM360/Amber,hendrycksTest-virology,5-shot,accuracy,0.27710843373493976,0.034843315926805875
LLM360/Amber,hendrycksTest-virology,5-shot,acc_norm,0.27710843373493976,0.034843315926805875
LLM360/Amber,hendrycksTest-world_religions,5-shot,accuracy,0.3684210526315789,0.036996580176568775
LLM360/Amber,hendrycksTest-world_religions,5-shot,acc_norm,0.3684210526315789,0.036996580176568775
LLM360/Amber,truthfulqa:mc,0-shot,mc1,0.2141982864137087,0.014362148155690462
LLM360/Amber,truthfulqa:mc,0-shot,mc2,0.3355637385526089,0.013068282225164367
LLM360/Amber,winogrande,5-shot,accuracy,0.6787687450670876,0.013123599324558307
LLM360/Amber,gsm8k,5-shot,accuracy,0.028051554207733132,0.004548229533836332
facebook/xglm-2.9B,minerva_math_precalc,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_prealgebra,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_num_theory,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_intermediate_algebra,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_geometry,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_counting_and_prob,5-shot,accuracy,0.0,
facebook/xglm-2.9B,minerva_math_algebra,5-shot,accuracy,0.0,
facebook/xglm-2.9B,fld_default,0-shot,accuracy,0.0,
facebook/xglm-2.9B,fld_star,0-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_3da,5-shot,accuracy,0.0005,0.0005000000000000061
facebook/xglm-2.9B,arithmetic_3ds,5-shot,accuracy,0.0005,0.0005000000000000116
facebook/xglm-2.9B,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
facebook/xglm-2.9B,arithmetic_2ds,5-shot,accuracy,0.0125,0.00248494717876267
facebook/xglm-2.9B,arithmetic_5ds,5-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_5da,5-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_1dc,5-shot,accuracy,0.0025,0.0011169148353275223
facebook/xglm-2.9B,arithmetic_4ds,5-shot,accuracy,0.0,
facebook/xglm-2.9B,arithmetic_2dm,5-shot,accuracy,0.01,0.002225415969682742
facebook/xglm-2.9B,arithmetic_2da,5-shot,accuracy,0.0005,0.0005000000000000079
facebook/xglm-2.9B,gsm8k_cot,5-shot,accuracy,0.024260803639120546,0.004238007900001395
facebook/xglm-2.9B,gsm8k,5-shot,accuracy,0.015163002274450341,0.0033660229497263377
facebook/xglm-2.9B,anli_r2,0-shot,brier_score,0.8903282853468429,
facebook/xglm-2.9B,anli_r3,0-shot,brier_score,0.8828546319214271,
facebook/xglm-2.9B,anli_r1,0-shot,brier_score,0.9090076784540243,
facebook/xglm-2.9B,xnli_eu,0-shot,brier_score,0.6945318464480982,
facebook/xglm-2.9B,xnli_vi,0-shot,brier_score,0.7198974352337865,
facebook/xglm-2.9B,xnli_ru,0-shot,brier_score,0.7940689223362921,
facebook/xglm-2.9B,xnli_zh,0-shot,brier_score,0.9662319296220725,
facebook/xglm-2.9B,xnli_tr,0-shot,brier_score,0.7985713408191671,
facebook/xglm-2.9B,xnli_fr,0-shot,brier_score,0.8777604442850733,
facebook/xglm-2.9B,xnli_en,0-shot,brier_score,0.6546699998277438,
facebook/xglm-2.9B,xnli_ur,0-shot,brier_score,0.9005216252993709,
facebook/xglm-2.9B,xnli_ar,0-shot,brier_score,1.2213926002441413,
facebook/xglm-2.9B,xnli_de,0-shot,brier_score,0.8098229929420445,
facebook/xglm-2.9B,xnli_hi,0-shot,brier_score,0.7612249719321225,
facebook/xglm-2.9B,xnli_es,0-shot,brier_score,0.8391711908916202,
facebook/xglm-2.9B,xnli_bg,0-shot,brier_score,0.7784193725622509,
facebook/xglm-2.9B,xnli_sw,0-shot,brier_score,0.752512116394542,
facebook/xglm-2.9B,xnli_el,0-shot,brier_score,0.8014402208550904,
facebook/xglm-2.9B,xnli_th,0-shot,brier_score,0.8209309976197092,
facebook/xglm-2.9B,logiqa2,0-shot,brier_score,1.0440698590170623,
facebook/xglm-2.9B,mathqa,5-shot,brier_score,1.0015941907404673,
facebook/xglm-2.9B,lambada_standard,0-shot,perplexity,10.724228161680852,0.3113611480259543
facebook/xglm-2.9B,lambada_standard,0-shot,accuracy,0.5022317096836794,0.006965908268351023
facebook/xglm-2.9B,lambada_openai,0-shot,perplexity,9.850349597657685,0.28597160307157565
facebook/xglm-2.9B,lambada_openai,0-shot,accuracy,0.49408111779545894,0.0069654895595805955
facebook/xglm-2.9B,mmlu_world_religions,0-shot,accuracy,0.2631578947368421,0.03377310252209197
facebook/xglm-2.9B,mmlu_formal_logic,0-shot,accuracy,0.1746031746031746,0.03395490020856113
facebook/xglm-2.9B,mmlu_prehistory,0-shot,accuracy,0.27469135802469136,0.024836057868294677
facebook/xglm-2.9B,mmlu_moral_scenarios,0-shot,accuracy,0.2424581005586592,0.014333522059217887
facebook/xglm-2.9B,mmlu_high_school_world_history,0-shot,accuracy,0.24472573839662448,0.027985699387036416
facebook/xglm-2.9B,mmlu_moral_disputes,0-shot,accuracy,0.18208092485549132,0.02077676110251297
facebook/xglm-2.9B,mmlu_professional_law,0-shot,accuracy,0.2438070404172099,0.010966507972178475
facebook/xglm-2.9B,mmlu_logical_fallacies,0-shot,accuracy,0.27607361963190186,0.0351238528370505
facebook/xglm-2.9B,mmlu_high_school_us_history,0-shot,accuracy,0.2549019607843137,0.030587591351604243
facebook/xglm-2.9B,mmlu_philosophy,0-shot,accuracy,0.19614147909967847,0.022552447780478033
facebook/xglm-2.9B,mmlu_jurisprudence,0-shot,accuracy,0.2222222222222222,0.040191074725573483
facebook/xglm-2.9B,mmlu_international_law,0-shot,accuracy,0.35537190082644626,0.04369236326573981
facebook/xglm-2.9B,mmlu_high_school_european_history,0-shot,accuracy,0.21818181818181817,0.03225078108306289
facebook/xglm-2.9B,mmlu_high_school_government_and_politics,0-shot,accuracy,0.36787564766839376,0.034801756684660366
facebook/xglm-2.9B,mmlu_high_school_microeconomics,0-shot,accuracy,0.3277310924369748,0.030489911417673227
facebook/xglm-2.9B,mmlu_high_school_geography,0-shot,accuracy,0.2474747474747475,0.030746300742124515
facebook/xglm-2.9B,mmlu_high_school_psychology,0-shot,accuracy,0.22385321100917432,0.017871217767790226
facebook/xglm-2.9B,mmlu_public_relations,0-shot,accuracy,0.23636363636363636,0.040693063197213754
facebook/xglm-2.9B,mmlu_us_foreign_policy,0-shot,accuracy,0.27,0.044619604333847394
facebook/xglm-2.9B,mmlu_sociology,0-shot,accuracy,0.16417910447761194,0.02619392354445411
facebook/xglm-2.9B,mmlu_high_school_macroeconomics,0-shot,accuracy,0.2923076923076923,0.023060438380857747
facebook/xglm-2.9B,mmlu_security_studies,0-shot,accuracy,0.2163265306122449,0.026358916334904035
facebook/xglm-2.9B,mmlu_professional_psychology,0-shot,accuracy,0.2434640522875817,0.017362473762146644
facebook/xglm-2.9B,mmlu_human_sexuality,0-shot,accuracy,0.2595419847328244,0.03844876139785271
facebook/xglm-2.9B,mmlu_econometrics,0-shot,accuracy,0.2543859649122807,0.040969851398436716
facebook/xglm-2.9B,mmlu_miscellaneous,0-shot,accuracy,0.2707535121328225,0.01588988836256049
facebook/xglm-2.9B,mmlu_marketing,0-shot,accuracy,0.17094017094017094,0.024662496845209807
facebook/xglm-2.9B,mmlu_management,0-shot,accuracy,0.18446601941747573,0.03840423627288276
facebook/xglm-2.9B,mmlu_nutrition,0-shot,accuracy,0.24509803921568626,0.024630048979824775
facebook/xglm-2.9B,mmlu_medical_genetics,0-shot,accuracy,0.3,0.046056618647183814
facebook/xglm-2.9B,mmlu_human_aging,0-shot,accuracy,0.23766816143497757,0.028568079464714274
facebook/xglm-2.9B,mmlu_professional_medicine,0-shot,accuracy,0.31985294117647056,0.028332959514031236
facebook/xglm-2.9B,mmlu_college_medicine,0-shot,accuracy,0.23699421965317918,0.03242414757483098
facebook/xglm-2.9B,mmlu_business_ethics,0-shot,accuracy,0.19,0.03942772444036623
facebook/xglm-2.9B,mmlu_clinical_knowledge,0-shot,accuracy,0.21509433962264152,0.02528839450289137
facebook/xglm-2.9B,mmlu_global_facts,0-shot,accuracy,0.18,0.038612291966536955
facebook/xglm-2.9B,mmlu_virology,0-shot,accuracy,0.2289156626506024,0.03270745277352477
facebook/xglm-2.9B,mmlu_professional_accounting,0-shot,accuracy,0.22695035460992907,0.024987106365642962
facebook/xglm-2.9B,mmlu_college_physics,0-shot,accuracy,0.23529411764705882,0.04220773659171452
facebook/xglm-2.9B,mmlu_high_school_physics,0-shot,accuracy,0.304635761589404,0.03757949922943342
facebook/xglm-2.9B,mmlu_high_school_biology,0-shot,accuracy,0.31290322580645163,0.02637756702864586
facebook/xglm-2.9B,mmlu_college_biology,0-shot,accuracy,0.2708333333333333,0.03716177437566016
facebook/xglm-2.9B,mmlu_anatomy,0-shot,accuracy,0.32592592592592595,0.040491220417025055
facebook/xglm-2.9B,mmlu_college_chemistry,0-shot,accuracy,0.23,0.042295258468165065
facebook/xglm-2.9B,mmlu_computer_security,0-shot,accuracy,0.16,0.0368452949177471
facebook/xglm-2.9B,mmlu_college_computer_science,0-shot,accuracy,0.24,0.04292346959909283
facebook/xglm-2.9B,mmlu_astronomy,0-shot,accuracy,0.17763157894736842,0.031103182383123398
facebook/xglm-2.9B,mmlu_college_mathematics,0-shot,accuracy,0.24,0.04292346959909281
facebook/xglm-2.9B,mmlu_conceptual_physics,0-shot,accuracy,0.2297872340425532,0.027501752944412424
facebook/xglm-2.9B,mmlu_abstract_algebra,0-shot,accuracy,0.27,0.044619604333847394
facebook/xglm-2.9B,mmlu_high_school_computer_science,0-shot,accuracy,0.32,0.04688261722621504
facebook/xglm-2.9B,mmlu_machine_learning,0-shot,accuracy,0.29464285714285715,0.043270409325787317
facebook/xglm-2.9B,mmlu_high_school_chemistry,0-shot,accuracy,0.3054187192118227,0.032406615658684086
facebook/xglm-2.9B,mmlu_high_school_statistics,0-shot,accuracy,0.4675925925925926,0.03402801581358966
facebook/xglm-2.9B,mmlu_elementary_mathematics,0-shot,accuracy,0.2619047619047619,0.022644212615525218
facebook/xglm-2.9B,mmlu_electrical_engineering,0-shot,accuracy,0.2689655172413793,0.036951833116502325
facebook/xglm-2.9B,mmlu_high_school_mathematics,0-shot,accuracy,0.29259259259259257,0.02773896963217609
facebook/xglm-2.9B,arc_challenge,25-shot,accuracy,0.2764505119453925,0.013069662474252427
facebook/xglm-2.9B,arc_challenge,25-shot,acc_norm,0.30119453924914674,0.013406741767847632
facebook/xglm-2.9B,hellaswag,10-shot,accuracy,0.4106751643098984,0.004909509538525162
facebook/xglm-2.9B,hellaswag,10-shot,acc_norm,0.5415255925114519,0.0049725431277678695
facebook/xglm-2.9B,truthfulqa_mc2,0-shot,accuracy,0.3582356473001071,0.01375467497109615
facebook/xglm-2.9B,truthfulqa_gen,0-shot,bleu_max,3.6087558287053865,0.18670249668473277
facebook/xglm-2.9B,truthfulqa_gen,0-shot,bleu_acc,0.2631578947368421,0.015415241740237002
facebook/xglm-2.9B,truthfulqa_gen,0-shot,bleu_diff,-1.3731184892287656,0.14371374046717808
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge1_max,13.419021251081578,0.3823807582971274
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge1_acc,0.29498164014687883,0.015964400965589633
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge1_diff,-2.1593394128613332,0.22093851799676853
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge2_max,7.604441562464007,0.3269185048983413
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge2_acc,0.19951040391676866,0.013989929967559657
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rouge2_diff,-2.7368765900678316,0.24573293145782651
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rougeL_max,12.140731662361581,0.3609507196982488
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rougeL_acc,0.29008567931456547,0.015886236874209515
facebook/xglm-2.9B,truthfulqa_gen,0-shot,rougeL_diff,-2.3125797772891734,0.22079372238444525
facebook/xglm-2.9B,truthfulqa_mc1,0-shot,accuracy,0.2141982864137087,0.01436214815569047
facebook/xglm-2.9B,winogrande,5-shot,accuracy,0.5588003157063931,0.013954975072834734
openlm-research/open_llama_7b_v2,minerva_math_precalc,5-shot,accuracy,0.02197802197802198,0.006280154928252514
openlm-research/open_llama_7b_v2,minerva_math_prealgebra,5-shot,accuracy,0.04936854190585534,0.0073446586249585125
openlm-research/open_llama_7b_v2,minerva_math_num_theory,5-shot,accuracy,0.011111111111111112,0.004515003707694655
openlm-research/open_llama_7b_v2,minerva_math_intermediate_algebra,5-shot,accuracy,0.031007751937984496,0.005771544162113688
openlm-research/open_llama_7b_v2,minerva_math_geometry,5-shot,accuracy,0.025052192066805846,0.007148247838013829
openlm-research/open_llama_7b_v2,minerva_math_counting_and_prob,5-shot,accuracy,0.05063291139240506,0.010080984934213217
openlm-research/open_llama_7b_v2,minerva_math_algebra,5-shot,accuracy,0.030328559393428812,0.004979615942997417
openlm-research/open_llama_7b_v2,fld_default,0-shot,accuracy,0.0,
openlm-research/open_llama_7b_v2,fld_star,0-shot,accuracy,0.0,
openlm-research/open_llama_7b_v2,arithmetic_3da,5-shot,accuracy,0.71,0.010148965501486966
openlm-research/open_llama_7b_v2,arithmetic_3ds,5-shot,accuracy,0.3825,0.010869956438573788
openlm-research/open_llama_7b_v2,arithmetic_4da,5-shot,accuracy,0.3935,0.010926507643554026
openlm-research/open_llama_7b_v2,arithmetic_2ds,5-shot,accuracy,0.4185,0.011033573531383041
openlm-research/open_llama_7b_v2,arithmetic_5ds,5-shot,accuracy,0.1685,0.008371912532971794
openlm-research/open_llama_7b_v2,arithmetic_5da,5-shot,accuracy,0.1345,0.007631119969964948
openlm-research/open_llama_7b_v2,arithmetic_1dc,5-shot,accuracy,0.2,0.008946508816851677
openlm-research/open_llama_7b_v2,arithmetic_4ds,5-shot,accuracy,0.3295,0.010512855704685485
openlm-research/open_llama_7b_v2,arithmetic_2dm,5-shot,accuracy,0.291,0.01015928666554761
openlm-research/open_llama_7b_v2,arithmetic_2da,5-shot,accuracy,0.801,0.00892969034652623
openlm-research/open_llama_7b_v2,gsm8k_cot,5-shot,accuracy,0.07808946171341925,0.007390654481108221
openlm-research/open_llama_7b_v2,gsm8k,5-shot,accuracy,0.034874905231235785,0.0050534807650222295
openlm-research/open_llama_7b_v2,anli_r2,0-shot,brier_score,0.750784987299882,
openlm-research/open_llama_7b_v2,anli_r3,0-shot,brier_score,0.6964174783567271,
openlm-research/open_llama_7b_v2,anli_r1,0-shot,brier_score,0.757434803319279,
openlm-research/open_llama_7b_v2,xnli_eu,0-shot,brier_score,0.9882727127400761,
openlm-research/open_llama_7b_v2,xnli_vi,0-shot,brier_score,0.9074524363165406,
openlm-research/open_llama_7b_v2,xnli_ru,0-shot,brier_score,0.7900843853265546,
openlm-research/open_llama_7b_v2,xnli_zh,0-shot,brier_score,0.8965790535014185,
openlm-research/open_llama_7b_v2,xnli_tr,0-shot,brier_score,0.883110444362914,
openlm-research/open_llama_7b_v2,xnli_fr,0-shot,brier_score,0.7477033299175156,
openlm-research/open_llama_7b_v2,xnli_en,0-shot,brier_score,0.6792402837627817,
openlm-research/open_llama_7b_v2,xnli_ur,0-shot,brier_score,1.3278735930168233,
openlm-research/open_llama_7b_v2,xnli_ar,0-shot,brier_score,1.0709475690301518,
openlm-research/open_llama_7b_v2,xnli_de,0-shot,brier_score,0.7830298946319253,
openlm-research/open_llama_7b_v2,xnli_hi,0-shot,brier_score,0.9790361891860578,
openlm-research/open_llama_7b_v2,xnli_es,0-shot,brier_score,0.9468302931488434,
openlm-research/open_llama_7b_v2,xnli_bg,0-shot,brier_score,0.7759774458295697,
openlm-research/open_llama_7b_v2,xnli_sw,0-shot,brier_score,0.9755466790498818,
openlm-research/open_llama_7b_v2,xnli_el,0-shot,brier_score,1.056880149085425,
openlm-research/open_llama_7b_v2,xnli_th,0-shot,brier_score,0.9438295823457749,
openlm-research/open_llama_7b_v2,logiqa2,0-shot,brier_score,1.0170481725501903,
openlm-research/open_llama_7b_v2,mathqa,5-shot,brier_score,0.9136828407266988,
openlm-research/open_llama_7b_v2,lambada_standard,0-shot,perplexity,4.773750468568448,0.10096739079170974
openlm-research/open_llama_7b_v2,lambada_standard,0-shot,accuracy,0.6413739569183,0.00668172574341263
openlm-research/open_llama_7b_v2,lambada_openai,0-shot,perplexity,3.8225400947763557,0.07818826158317511
openlm-research/open_llama_7b_v2,lambada_openai,0-shot,accuracy,0.7151174073355328,0.006288306538252611
openlm-research/open_llama_7b_v2,mmlu_world_religions,0-shot,accuracy,0.52046783625731,0.0383161053282193
openlm-research/open_llama_7b_v2,mmlu_formal_logic,0-shot,accuracy,0.3253968253968254,0.041905964388711366
openlm-research/open_llama_7b_v2,mmlu_prehistory,0-shot,accuracy,0.4351851851851852,0.027586006221607694
openlm-research/open_llama_7b_v2,mmlu_moral_scenarios,0-shot,accuracy,0.26145251396648045,0.014696599650364552
openlm-research/open_llama_7b_v2,mmlu_high_school_world_history,0-shot,accuracy,0.48523206751054854,0.032533028078777386
openlm-research/open_llama_7b_v2,mmlu_moral_disputes,0-shot,accuracy,0.43352601156069365,0.026680134761679214
openlm-research/open_llama_7b_v2,mmlu_professional_law,0-shot,accuracy,0.33572359843546284,0.012061304157664605
openlm-research/open_llama_7b_v2,mmlu_logical_fallacies,0-shot,accuracy,0.39263803680981596,0.03836740907831029
openlm-research/open_llama_7b_v2,mmlu_high_school_us_history,0-shot,accuracy,0.4411764705882353,0.034849415144292316
openlm-research/open_llama_7b_v2,mmlu_philosophy,0-shot,accuracy,0.40514469453376206,0.02788238379132596
openlm-research/open_llama_7b_v2,mmlu_jurisprudence,0-shot,accuracy,0.49074074074074076,0.04832853553437055
openlm-research/open_llama_7b_v2,mmlu_international_law,0-shot,accuracy,0.5041322314049587,0.045641987674327526
openlm-research/open_llama_7b_v2,mmlu_high_school_european_history,0-shot,accuracy,0.43636363636363634,0.03872592983524753
openlm-research/open_llama_7b_v2,mmlu_high_school_government_and_politics,0-shot,accuracy,0.5803108808290155,0.035615873276858834
openlm-research/open_llama_7b_v2,mmlu_high_school_microeconomics,0-shot,accuracy,0.37815126050420167,0.03149930577784906
openlm-research/open_llama_7b_v2,mmlu_high_school_geography,0-shot,accuracy,0.46464646464646464,0.035534363688280626
openlm-research/open_llama_7b_v2,mmlu_high_school_psychology,0-shot,accuracy,0.5211009174311927,0.021418224754264643
openlm-research/open_llama_7b_v2,mmlu_public_relations,0-shot,accuracy,0.45454545454545453,0.04769300568972744
openlm-research/open_llama_7b_v2,mmlu_us_foreign_policy,0-shot,accuracy,0.55,0.05
openlm-research/open_llama_7b_v2,mmlu_sociology,0-shot,accuracy,0.5572139303482587,0.03512310964123937
openlm-research/open_llama_7b_v2,mmlu_high_school_macroeconomics,0-shot,accuracy,0.41025641025641024,0.024939313906940784
openlm-research/open_llama_7b_v2,mmlu_security_studies,0-shot,accuracy,0.45714285714285713,0.03189141832421397
openlm-research/open_llama_7b_v2,mmlu_professional_psychology,0-shot,accuracy,0.3839869281045752,0.01967580813528152
openlm-research/open_llama_7b_v2,mmlu_human_sexuality,0-shot,accuracy,0.4961832061068702,0.043851623256015534
openlm-research/open_llama_7b_v2,mmlu_econometrics,0-shot,accuracy,0.2982456140350877,0.04303684033537314
openlm-research/open_llama_7b_v2,mmlu_miscellaneous,0-shot,accuracy,0.5696040868454662,0.017705868776292398
openlm-research/open_llama_7b_v2,mmlu_marketing,0-shot,accuracy,0.6068376068376068,0.03199957924651047
openlm-research/open_llama_7b_v2,mmlu_management,0-shot,accuracy,0.5436893203883495,0.049318019942204146
openlm-research/open_llama_7b_v2,mmlu_nutrition,0-shot,accuracy,0.4215686274509804,0.028275490156791438
openlm-research/open_llama_7b_v2,mmlu_medical_genetics,0-shot,accuracy,0.53,0.05016135580465919
openlm-research/open_llama_7b_v2,mmlu_human_aging,0-shot,accuracy,0.3991031390134529,0.03286745312567961
openlm-research/open_llama_7b_v2,mmlu_professional_medicine,0-shot,accuracy,0.4375,0.030134614954403924
openlm-research/open_llama_7b_v2,mmlu_college_medicine,0-shot,accuracy,0.4046242774566474,0.03742461193887249
openlm-research/open_llama_7b_v2,mmlu_business_ethics,0-shot,accuracy,0.4,0.04923659639173309
openlm-research/open_llama_7b_v2,mmlu_clinical_knowledge,0-shot,accuracy,0.45660377358490567,0.030656748696739435
openlm-research/open_llama_7b_v2,mmlu_global_facts,0-shot,accuracy,0.33,0.047258156262526045
openlm-research/open_llama_7b_v2,mmlu_virology,0-shot,accuracy,0.41566265060240964,0.03836722176598053
openlm-research/open_llama_7b_v2,mmlu_professional_accounting,0-shot,accuracy,0.3120567375886525,0.027640120545169924
openlm-research/open_llama_7b_v2,mmlu_college_physics,0-shot,accuracy,0.21568627450980393,0.04092563958237655
openlm-research/open_llama_7b_v2,mmlu_high_school_physics,0-shot,accuracy,0.2913907284768212,0.03710185726119994
openlm-research/open_llama_7b_v2,mmlu_high_school_biology,0-shot,accuracy,0.43548387096774194,0.02820622559150274
openlm-research/open_llama_7b_v2,mmlu_college_biology,0-shot,accuracy,0.4236111111111111,0.041321250197233685
openlm-research/open_llama_7b_v2,mmlu_anatomy,0-shot,accuracy,0.43703703703703706,0.04284958639753399
openlm-research/open_llama_7b_v2,mmlu_college_chemistry,0-shot,accuracy,0.3,0.046056618647183814
openlm-research/open_llama_7b_v2,mmlu_computer_security,0-shot,accuracy,0.58,0.049604496374885836
openlm-research/open_llama_7b_v2,mmlu_college_computer_science,0-shot,accuracy,0.34,0.04760952285695235
openlm-research/open_llama_7b_v2,mmlu_astronomy,0-shot,accuracy,0.4276315789473684,0.04026097083296557
openlm-research/open_llama_7b_v2,mmlu_college_mathematics,0-shot,accuracy,0.33,0.04725815626252604
openlm-research/open_llama_7b_v2,mmlu_conceptual_physics,0-shot,accuracy,0.3404255319148936,0.030976692998534436
openlm-research/open_llama_7b_v2,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909283
openlm-research/open_llama_7b_v2,mmlu_high_school_computer_science,0-shot,accuracy,0.34,0.04760952285695235
openlm-research/open_llama_7b_v2,mmlu_machine_learning,0-shot,accuracy,0.3392857142857143,0.0449394906861354
openlm-research/open_llama_7b_v2,mmlu_high_school_chemistry,0-shot,accuracy,0.24630541871921183,0.030315099285617736
openlm-research/open_llama_7b_v2,mmlu_high_school_statistics,0-shot,accuracy,0.2638888888888889,0.030058202704309846
openlm-research/open_llama_7b_v2,mmlu_elementary_mathematics,0-shot,accuracy,0.2830687830687831,0.023201392938194978
openlm-research/open_llama_7b_v2,mmlu_electrical_engineering,0-shot,accuracy,0.4413793103448276,0.04137931034482758
openlm-research/open_llama_7b_v2,mmlu_high_school_mathematics,0-shot,accuracy,0.23703703703703705,0.025928876132766107
openlm-research/open_llama_7b_v2,arc_challenge,25-shot,accuracy,0.42235494880546076,0.014434138713379981
openlm-research/open_llama_7b_v2,arc_challenge,25-shot,acc_norm,0.44880546075085326,0.014534599585097662
openlm-research/open_llama_7b_v2,hellaswag,10-shot,accuracy,0.5456084445329615,0.004968979259738336
openlm-research/open_llama_7b_v2,hellaswag,10-shot,acc_norm,0.7219677355108544,0.004471137333619625
openlm-research/open_llama_7b_v2,truthfulqa_mc2,0-shot,accuracy,0.34571322971663265,0.01348292646455553
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,bleu_max,26.304961396549636,0.7825243698743464
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,bleu_acc,0.2974296205630355,0.016002651487360978
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,bleu_diff,-9.780023194536044,0.8239620546869166
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge1_max,50.831523166784905,0.8615610126946267
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge1_acc,0.2729498164014688,0.01559475363200654
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge1_diff,-12.094224542000415,0.8842926117979107
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge2_max,34.74146560299905,0.9969274635318671
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge2_acc,0.22888616891064872,0.014706994909055027
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rouge2_diff,-14.236043695877685,1.0648852083123828
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rougeL_max,47.945898558796365,0.8719715450118734
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rougeL_acc,0.25703794369645044,0.015298077509485088
openlm-research/open_llama_7b_v2,truthfulqa_gen,0-shot,rougeL_diff,-12.736671064103732,0.8845110469156796
openlm-research/open_llama_7b_v2,truthfulqa_mc1,0-shot,accuracy,0.22643818849449204,0.014651337324602587
openlm-research/open_llama_7b_v2,winogrande,5-shot,accuracy,0.6937647987371744,0.012954385972802462
Monero/WizardLM-13b-OpenAssistant-Uncensored,arc:challenge,25-shot,accuracy,0.4709897610921502,0.014586776355294321
Monero/WizardLM-13b-OpenAssistant-Uncensored,arc:challenge,25-shot,acc_norm,0.4854948805460751,0.014605241081370053
Monero/WizardLM-13b-OpenAssistant-Uncensored,hellaswag,10-shot,accuracy,0.569806811392153,0.004940911779273365
Monero/WizardLM-13b-OpenAssistant-Uncensored,hellaswag,10-shot,acc_norm,0.7603067118103963,0.004260238033657913
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-abstract_algebra,5-shot,accuracy,0.33,0.047258156262526045
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.33,0.047258156262526045
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-anatomy,5-shot,accuracy,0.4,0.042320736951515885
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-anatomy,5-shot,acc_norm,0.4,0.042320736951515885
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-astronomy,5-shot,accuracy,0.3157894736842105,0.0378272898086547
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-astronomy,5-shot,acc_norm,0.3157894736842105,0.0378272898086547
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-business_ethics,5-shot,accuracy,0.47,0.05016135580465919
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-business_ethics,5-shot,acc_norm,0.47,0.05016135580465919
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4528301886792453,0.030635627957961823
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4528301886792453,0.030635627957961823
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_biology,5-shot,accuracy,0.4097222222222222,0.04112490974670787
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_biology,5-shot,acc_norm,0.4097222222222222,0.04112490974670787
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_chemistry,5-shot,accuracy,0.26,0.04408440022768078
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_chemistry,5-shot,acc_norm,0.26,0.04408440022768078
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_computer_science,5-shot,accuracy,0.4,0.049236596391733084
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_computer_science,5-shot,acc_norm,0.4,0.049236596391733084
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_mathematics,5-shot,accuracy,0.41,0.04943110704237102
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_mathematics,5-shot,acc_norm,0.41,0.04943110704237102
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_medicine,5-shot,accuracy,0.3815028901734104,0.0370385119309952
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_medicine,5-shot,acc_norm,0.3815028901734104,0.0370385119309952
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_physics,5-shot,accuracy,0.18627450980392157,0.038739587141493524
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-college_physics,5-shot,acc_norm,0.18627450980392157,0.038739587141493524
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-computer_security,5-shot,accuracy,0.55,0.049999999999999996
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-computer_security,5-shot,acc_norm,0.55,0.049999999999999996
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-conceptual_physics,5-shot,accuracy,0.37872340425531914,0.03170995606040655
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.37872340425531914,0.03170995606040655
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-econometrics,5-shot,accuracy,0.2807017543859649,0.042270544512322004
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-econometrics,5-shot,acc_norm,0.2807017543859649,0.042270544512322004
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-electrical_engineering,5-shot,accuracy,0.3724137931034483,0.04028731532947559
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.3724137931034483,0.04028731532947559
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.30423280423280424,0.023695415009463087
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.30423280423280424,0.023695415009463087
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-formal_logic,5-shot,accuracy,0.2698412698412698,0.03970158273235172
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-formal_logic,5-shot,acc_norm,0.2698412698412698,0.03970158273235172
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-global_facts,5-shot,accuracy,0.37,0.04852365870939099
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-global_facts,5-shot,acc_norm,0.37,0.04852365870939099
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_biology,5-shot,accuracy,0.4161290322580645,0.028040981380761543
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_biology,5-shot,acc_norm,0.4161290322580645,0.028040981380761543
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2857142857142857,0.03178529710642749
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2857142857142857,0.03178529710642749
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.46,0.05009082659620332
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.46,0.05009082659620332
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_european_history,5-shot,accuracy,0.5151515151515151,0.03902551007374449
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.5151515151515151,0.03902551007374449
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_geography,5-shot,accuracy,0.5202020202020202,0.035594435655639176
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_geography,5-shot,acc_norm,0.5202020202020202,0.035594435655639176
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.6113989637305699,0.03517739796373131
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.6113989637305699,0.03517739796373131
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.3974358974358974,0.024811920017903836
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.3974358974358974,0.024811920017903836
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.27037037037037037,0.027080372815145668
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.27037037037037037,0.027080372815145668
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.41596638655462187,0.03201650100739615
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.41596638655462187,0.03201650100739615
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_physics,5-shot,accuracy,0.2052980132450331,0.03297986648473836
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2052980132450331,0.03297986648473836
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_psychology,5-shot,accuracy,0.5394495412844037,0.021370494609995093
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.5394495412844037,0.021370494609995093
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_statistics,5-shot,accuracy,0.2777777777777778,0.030546745264953178
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.2777777777777778,0.030546745264953178
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_us_history,5-shot,accuracy,0.5784313725490197,0.03465868196380761
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.5784313725490197,0.03465868196380761
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_world_history,5-shot,accuracy,0.5738396624472574,0.032190357031317736
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.5738396624472574,0.032190357031317736
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_aging,5-shot,accuracy,0.5650224215246636,0.03327283370271344
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_aging,5-shot,acc_norm,0.5650224215246636,0.03327283370271344
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_sexuality,5-shot,accuracy,0.4198473282442748,0.04328577215262972
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-human_sexuality,5-shot,acc_norm,0.4198473282442748,0.04328577215262972
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-international_law,5-shot,accuracy,0.5371900826446281,0.04551711196104218
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-international_law,5-shot,acc_norm,0.5371900826446281,0.04551711196104218
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-jurisprudence,5-shot,accuracy,0.5277777777777778,0.048262172941398944
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5277777777777778,0.048262172941398944
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-logical_fallacies,5-shot,accuracy,0.43558282208588955,0.03895632464138936
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.43558282208588955,0.03895632464138936
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-machine_learning,5-shot,accuracy,0.4017857142857143,0.04653333146973646
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-machine_learning,5-shot,acc_norm,0.4017857142857143,0.04653333146973646
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-management,5-shot,accuracy,0.5728155339805825,0.04897957737781168
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-management,5-shot,acc_norm,0.5728155339805825,0.04897957737781168
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-marketing,5-shot,accuracy,0.6837606837606838,0.03046365674734026
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-marketing,5-shot,acc_norm,0.6837606837606838,0.03046365674734026
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-medical_genetics,5-shot,accuracy,0.52,0.05021167315686779
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-medical_genetics,5-shot,acc_norm,0.52,0.05021167315686779
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-miscellaneous,5-shot,accuracy,0.6091954022988506,0.017448366067062526
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-miscellaneous,5-shot,acc_norm,0.6091954022988506,0.017448366067062526
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_disputes,5-shot,accuracy,0.4277456647398844,0.026636539741116072
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_disputes,5-shot,acc_norm,0.4277456647398844,0.026636539741116072
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_scenarios,5-shot,accuracy,0.2670391061452514,0.014796502622562551
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.2670391061452514,0.014796502622562551
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-nutrition,5-shot,accuracy,0.4411764705882353,0.02843109544417664
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-nutrition,5-shot,acc_norm,0.4411764705882353,0.02843109544417664
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-philosophy,5-shot,accuracy,0.47266881028938906,0.02835563356832818
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-philosophy,5-shot,acc_norm,0.47266881028938906,0.02835563356832818
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-prehistory,5-shot,accuracy,0.4660493827160494,0.02775653525734767
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-prehistory,5-shot,acc_norm,0.4660493827160494,0.02775653525734767
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_accounting,5-shot,accuracy,0.3475177304964539,0.028406627809590954
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_accounting,5-shot,acc_norm,0.3475177304964539,0.028406627809590954
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_law,5-shot,accuracy,0.3376792698826597,0.012078563777145572
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_law,5-shot,acc_norm,0.3376792698826597,0.012078563777145572
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_medicine,5-shot,accuracy,0.41544117647058826,0.029935342707877743
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_medicine,5-shot,acc_norm,0.41544117647058826,0.029935342707877743
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_psychology,5-shot,accuracy,0.434640522875817,0.02005426920072646
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-professional_psychology,5-shot,acc_norm,0.434640522875817,0.02005426920072646
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-public_relations,5-shot,accuracy,0.5181818181818182,0.04785964010794916
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-public_relations,5-shot,acc_norm,0.5181818181818182,0.04785964010794916
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-security_studies,5-shot,accuracy,0.42857142857142855,0.031680911612338825
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-security_studies,5-shot,acc_norm,0.42857142857142855,0.031680911612338825
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-sociology,5-shot,accuracy,0.5074626865671642,0.035351400842767194
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-sociology,5-shot,acc_norm,0.5074626865671642,0.035351400842767194
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.63,0.04852365870939099
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.63,0.04852365870939099
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-virology,5-shot,accuracy,0.3855421686746988,0.0378913442461155
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-virology,5-shot,acc_norm,0.3855421686746988,0.0378913442461155
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-world_religions,5-shot,accuracy,0.631578947368421,0.036996580176568775
Monero/WizardLM-13b-OpenAssistant-Uncensored,hendrycksTest-world_religions,5-shot,acc_norm,0.631578947368421,0.036996580176568775
Monero/WizardLM-13b-OpenAssistant-Uncensored,truthfulqa:mc,0-shot,mc1,0.3329253365973072,0.01649740238201206
Monero/WizardLM-13b-OpenAssistant-Uncensored,truthfulqa:mc,0-shot,mc2,0.4940407873488723,0.01598359583562834
Monero/WizardLM-13b-OpenAssistant-Uncensored,drop,3-shot,accuracy,0.0803271812080537,0.002783476701010582
Monero/WizardLM-13b-OpenAssistant-Uncensored,drop,3-shot,f1,0.17449454697986574,0.0031261159442318247
Monero/WizardLM-13b-OpenAssistant-Uncensored,gsm8k,5-shot,accuracy,0.16906747536012132,0.010324171445497349
Monero/WizardLM-13b-OpenAssistant-Uncensored,winogrande,5-shot,accuracy,0.6977111286503551,0.012907200361627532
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_precalc,5-shot,accuracy,0.01282051282051282,0.004818950982487621
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_prealgebra,5-shot,accuracy,0.04822043628013777,0.007263135212103671
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_num_theory,5-shot,accuracy,0.009259259259259259,0.0041254730154902395
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_intermediate_algebra,5-shot,accuracy,0.009966777408637873,0.003307493466972023
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_geometry,5-shot,accuracy,0.010438413361169102,0.00464862711718465
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_counting_and_prob,5-shot,accuracy,0.0189873417721519,0.006275362513989625
Monero/WizardLM-13b-OpenAssistant-Uncensored,minerva_math_algebra,5-shot,accuracy,0.02106149957877001,0.004169461854206072
Monero/WizardLM-13b-OpenAssistant-Uncensored,fld_default,0-shot,accuracy,0.0,
Monero/WizardLM-13b-OpenAssistant-Uncensored,fld_star,0-shot,accuracy,0.0,
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_3da,5-shot,accuracy,0.494,0.01118233080628221
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_3ds,5-shot,accuracy,0.0925,0.0064801906943945005
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_4da,5-shot,accuracy,0.143,0.007829824587852599
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_2ds,5-shot,accuracy,0.092,0.006464433033702528
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_5ds,5-shot,accuracy,0.0105,0.002279796863070978
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_5da,5-shot,accuracy,0.0185,0.003013870718586685
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_1dc,5-shot,accuracy,0.0,
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_4ds,5-shot,accuracy,0.0655,0.005533550857500552
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_2dm,5-shot,accuracy,0.1065,0.006899469279456988
Monero/WizardLM-13b-OpenAssistant-Uncensored,arithmetic_2da,5-shot,accuracy,0.569,0.011076138335187558
Monero/WizardLM-13b-OpenAssistant-Uncensored,gsm8k_cot,5-shot,accuracy,0.19939347990902198,0.011005438029475638
Monero/WizardLM-13b-OpenAssistant-Uncensored,anli_r2,0-shot,brier_score,0.9518990734725757,
Monero/WizardLM-13b-OpenAssistant-Uncensored,anli_r3,0-shot,brier_score,0.9050219163099841,
Monero/WizardLM-13b-OpenAssistant-Uncensored,anli_r1,0-shot,brier_score,0.9499203600796313,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_eu,0-shot,brier_score,1.2116651846902302,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_vi,0-shot,brier_score,1.1118470416477884,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_ru,0-shot,brier_score,0.8328410171110645,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_zh,0-shot,brier_score,1.1789746567492252,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_tr,0-shot,brier_score,1.153573017626289,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_fr,0-shot,brier_score,1.008165615172993,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_en,0-shot,brier_score,0.9233846980099405,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_ur,0-shot,brier_score,1.3155356160757106,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_ar,0-shot,brier_score,1.274329293402633,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_de,0-shot,brier_score,0.9779918916363576,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_hi,0-shot,brier_score,0.920937714318964,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_es,0-shot,brier_score,0.9890453652911487,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_bg,0-shot,brier_score,0.9024154835286141,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_sw,0-shot,brier_score,0.9466655510728087,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_el,0-shot,brier_score,0.9289485390405853,
Monero/WizardLM-13b-OpenAssistant-Uncensored,xnli_th,0-shot,brier_score,1.0414528699854,
Monero/WizardLM-13b-OpenAssistant-Uncensored,logiqa2,0-shot,brier_score,1.1507590248894561,
Monero/WizardLM-13b-OpenAssistant-Uncensored,mathqa,5-shot,brier_score,1.0865117207773218,
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_standard,0-shot,perplexity,4.612695206892958,0.16254474947632105
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_standard,0-shot,accuracy,0.6541820298855036,0.006626514558805178
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_openai,0-shot,perplexity,3.3610202083230787,0.1123279051077298
Monero/WizardLM-13b-OpenAssistant-Uncensored,lambada_openai,0-shot,accuracy,0.7240442460702503,0.006227508845545615
google/gemma-7b,mmlu_world_religions,0-shot,accuracy,0.8421052631578947,0.027966785859160865
google/gemma-7b,mmlu_formal_logic,0-shot,accuracy,0.5,0.04472135954999579
google/gemma-7b,mmlu_prehistory,0-shot,accuracy,0.7314814814814815,0.024659685185967273
google/gemma-7b,mmlu_moral_scenarios,0-shot,accuracy,0.31508379888268156,0.015536850852473635
google/gemma-7b,mmlu_high_school_world_history,0-shot,accuracy,0.8227848101265823,0.024856364184503228
google/gemma-7b,mmlu_moral_disputes,0-shot,accuracy,0.6676300578034682,0.02536116874968822
google/gemma-7b,mmlu_professional_law,0-shot,accuracy,0.4661016949152542,0.012740853872949832
google/gemma-7b,mmlu_logical_fallacies,0-shot,accuracy,0.7484662576687117,0.03408997886857529
google/gemma-7b,mmlu_high_school_us_history,0-shot,accuracy,0.7843137254901961,0.028867431449849313
google/gemma-7b,mmlu_philosophy,0-shot,accuracy,0.7009646302250804,0.026003301117885128
google/gemma-7b,mmlu_jurisprudence,0-shot,accuracy,0.75,0.04186091791394607
google/gemma-7b,mmlu_international_law,0-shot,accuracy,0.8512396694214877,0.03248470083807195
google/gemma-7b,mmlu_high_school_european_history,0-shot,accuracy,0.7575757575757576,0.03346409881055953
google/gemma-7b,mmlu_high_school_government_and_politics,0-shot,accuracy,0.8860103626943006,0.022935144053919436
google/gemma-7b,mmlu_high_school_microeconomics,0-shot,accuracy,0.6470588235294118,0.031041941304059278
google/gemma-7b,mmlu_high_school_geography,0-shot,accuracy,0.803030303030303,0.028335609732463362
google/gemma-7b,mmlu_high_school_psychology,0-shot,accuracy,0.8128440366972477,0.01672268452620016
google/gemma-7b,mmlu_public_relations,0-shot,accuracy,0.6545454545454545,0.04554619617541054
google/gemma-7b,mmlu_us_foreign_policy,0-shot,accuracy,0.87,0.03379976689896308
google/gemma-7b,mmlu_sociology,0-shot,accuracy,0.835820895522388,0.026193923544454125
google/gemma-7b,mmlu_high_school_macroeconomics,0-shot,accuracy,0.6307692307692307,0.02446861524147891
google/gemma-7b,mmlu_security_studies,0-shot,accuracy,0.7183673469387755,0.028795185574291296
google/gemma-7b,mmlu_professional_psychology,0-shot,accuracy,0.6895424836601307,0.018718067052623213
google/gemma-7b,mmlu_human_sexuality,0-shot,accuracy,0.7251908396946565,0.03915345408847836
google/gemma-7b,mmlu_econometrics,0-shot,accuracy,0.45614035087719296,0.04685473041907789
google/gemma-7b,mmlu_miscellaneous,0-shot,accuracy,0.8416347381864623,0.013055346753516743
google/gemma-7b,mmlu_marketing,0-shot,accuracy,0.8931623931623932,0.020237149008990943
google/gemma-7b,mmlu_management,0-shot,accuracy,0.8252427184466019,0.03760178006026621
google/gemma-7b,mmlu_nutrition,0-shot,accuracy,0.7418300653594772,0.025058503316958147
google/gemma-7b,mmlu_medical_genetics,0-shot,accuracy,0.75,0.04351941398892446
google/gemma-7b,mmlu_human_aging,0-shot,accuracy,0.7085201793721974,0.03050028317654584
google/gemma-7b,mmlu_professional_medicine,0-shot,accuracy,0.5772058823529411,0.03000856284500347
google/gemma-7b,mmlu_college_medicine,0-shot,accuracy,0.6473988439306358,0.03643037168958548
google/gemma-7b,mmlu_business_ethics,0-shot,accuracy,0.64,0.048241815132442176
google/gemma-7b,mmlu_clinical_knowledge,0-shot,accuracy,0.660377358490566,0.029146904747798328
google/gemma-7b,mmlu_global_facts,0-shot,accuracy,0.35,0.0479372485441102
google/gemma-7b,mmlu_virology,0-shot,accuracy,0.5240963855421686,0.03887971849597264
google/gemma-7b,mmlu_professional_accounting,0-shot,accuracy,0.46099290780141844,0.02973659252642444
google/gemma-7b,mmlu_college_physics,0-shot,accuracy,0.38235294117647056,0.04835503696107223
google/gemma-7b,mmlu_high_school_physics,0-shot,accuracy,0.3973509933774834,0.03995524007681681
google/gemma-7b,mmlu_high_school_biology,0-shot,accuracy,0.8,0.02275520495954294
google/gemma-7b,mmlu_college_biology,0-shot,accuracy,0.7291666666666666,0.03716177437566017
google/gemma-7b,mmlu_anatomy,0-shot,accuracy,0.5259259259259259,0.04313531696750575
google/gemma-7b,mmlu_college_chemistry,0-shot,accuracy,0.48,0.050211673156867795
google/gemma-7b,mmlu_computer_security,0-shot,accuracy,0.75,0.04351941398892446
google/gemma-7b,mmlu_college_computer_science,0-shot,accuracy,0.49,0.050241839379569095
google/gemma-7b,mmlu_astronomy,0-shot,accuracy,0.6907894736842105,0.03761070869867479
google/gemma-7b,mmlu_college_mathematics,0-shot,accuracy,0.38,0.04878317312145632
google/gemma-7b,mmlu_conceptual_physics,0-shot,accuracy,0.5829787234042553,0.03223276266711712
google/gemma-7b,mmlu_abstract_algebra,0-shot,accuracy,0.24,0.04292346959909282
google/gemma-7b,mmlu_high_school_computer_science,0-shot,accuracy,0.65,0.047937248544110196
google/gemma-7b,mmlu_machine_learning,0-shot,accuracy,0.5535714285714286,0.047184714852195865
google/gemma-7b,mmlu_high_school_chemistry,0-shot,accuracy,0.5024630541871922,0.03517945038691063
google/gemma-7b,mmlu_high_school_statistics,0-shot,accuracy,0.48148148148148145,0.034076320938540516
google/gemma-7b,mmlu_elementary_mathematics,0-shot,accuracy,0.4603174603174603,0.025670080636909186
google/gemma-7b,mmlu_electrical_engineering,0-shot,accuracy,0.6344827586206897,0.04013124195424386
google/gemma-7b,mmlu_high_school_mathematics,0-shot,accuracy,0.362962962962963,0.02931820364520686
google/gemma-7b,arc_challenge,25-shot,accuracy,0.5708191126279863,0.014464085894870657
google/gemma-7b,arc_challenge,25-shot,acc_norm,0.6049488054607508,0.014285898292938169
google/gemma-7b,hellaswag,10-shot,accuracy,0.6207926707827126,0.00484198197351532
google/gemma-7b,hellaswag,10-shot,acc_norm,0.8228440549691296,0.003810203308901054
google/gemma-7b,truthfulqa_mc2,0-shot,accuracy,0.45109982374873114,0.014690361721297288
google/gemma-7b,truthfulqa_gen,0-shot,bleu_max,27.01069206803005,0.8195476991507818
google/gemma-7b,truthfulqa_gen,0-shot,bleu_acc,0.4394124847001224,0.01737452048251371
google/gemma-7b,truthfulqa_gen,0-shot,bleu_diff,0.7951375918699185,0.9114193943215877
google/gemma-7b,truthfulqa_gen,0-shot,rouge1_max,52.929721104206294,0.8888037925222474
google/gemma-7b,truthfulqa_gen,0-shot,rouge1_acc,0.44063647490820074,0.017379697555437446
google/gemma-7b,truthfulqa_gen,0-shot,rouge1_diff,1.0282214491941382,1.1490330858956697
google/gemma-7b,truthfulqa_gen,0-shot,rouge2_max,37.759767751388836,1.0663490449012212
google/gemma-7b,truthfulqa_gen,0-shot,rouge2_acc,0.3671970624235006,0.01687480500145318
google/gemma-7b,truthfulqa_gen,0-shot,rouge2_diff,-0.45206688687486213,1.3041898041246565
google/gemma-7b,truthfulqa_gen,0-shot,rougeL_max,49.90048034353921,0.9075990783414511
google/gemma-7b,truthfulqa_gen,0-shot,rougeL_acc,0.42105263157894735,0.017283936248136483
google/gemma-7b,truthfulqa_gen,0-shot,rougeL_diff,0.7823563764856337,1.1650307856683864
google/gemma-7b,truthfulqa_mc1,0-shot,accuracy,0.3108935128518972,0.016203316673559693
google/gemma-7b,winogrande,5-shot,accuracy,0.760852407261247,0.011988541844843909
google/gemma-7b,gsm8k,5-shot,accuracy,0.5382865807429871,0.013732048227016685
meta-llama/Llama-2-70b-hf,drop,3-shot,accuracy,0.0017827181208053692,0.00043200973460388544
meta-llama/Llama-2-70b-hf,drop,3-shot,f1,0.06615562080536916,0.0013739852117668813
meta-llama/Llama-2-70b-hf,gsm8k,5-shot,accuracy,0.5405610310841547,0.013727093010429788
meta-llama/Llama-2-70b-hf,winogrande,5-shot,accuracy,0.8374112075769534,0.010370455551343326
openlm-research/open_llama_7b_v2,arc:challenge,25-shot,accuracy,0.4232081911262799,0.014438036220848029
openlm-research/open_llama_7b_v2,arc:challenge,25-shot,acc_norm,0.43686006825938567,0.014494421584256513
openlm-research/open_llama_7b_v2,hendrycksTest-abstract_algebra,5-shot,accuracy,0.24,0.04292346959909283
openlm-research/open_llama_7b_v2,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.24,0.04292346959909283
openlm-research/open_llama_7b_v2,hendrycksTest-anatomy,5-shot,accuracy,0.43703703703703706,0.04284958639753399
openlm-research/open_llama_7b_v2,hendrycksTest-anatomy,5-shot,acc_norm,0.43703703703703706,0.04284958639753399
openlm-research/open_llama_7b_v2,hendrycksTest-astronomy,5-shot,accuracy,0.4407894736842105,0.04040311062490436
openlm-research/open_llama_7b_v2,hendrycksTest-astronomy,5-shot,acc_norm,0.4407894736842105,0.04040311062490436
openlm-research/open_llama_7b_v2,hendrycksTest-business_ethics,5-shot,accuracy,0.41,0.049431107042371025
openlm-research/open_llama_7b_v2,hendrycksTest-business_ethics,5-shot,acc_norm,0.41,0.049431107042371025
openlm-research/open_llama_7b_v2,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.4641509433962264,0.030693675018458006
openlm-research/open_llama_7b_v2,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.4641509433962264,0.030693675018458006
openlm-research/open_llama_7b_v2,hendrycksTest-college_biology,5-shot,accuracy,0.4305555555555556,0.04140685639111502
openlm-research/open_llama_7b_v2,hendrycksTest-college_biology,5-shot,acc_norm,0.4305555555555556,0.04140685639111502
openlm-research/open_llama_7b_v2,hendrycksTest-college_chemistry,5-shot,accuracy,0.3,0.046056618647183814
openlm-research/open_llama_7b_v2,hendrycksTest-college_chemistry,5-shot,acc_norm,0.3,0.046056618647183814
openlm-research/open_llama_7b_v2,hendrycksTest-college_computer_science,5-shot,accuracy,0.34,0.04760952285695236
openlm-research/open_llama_7b_v2,hendrycksTest-college_computer_science,5-shot,acc_norm,0.34,0.04760952285695236
openlm-research/open_llama_7b_v2,hendrycksTest-college_mathematics,5-shot,accuracy,0.33,0.047258156262526045
openlm-research/open_llama_7b_v2,hendrycksTest-college_mathematics,5-shot,acc_norm,0.33,0.047258156262526045
openlm-research/open_llama_7b_v2,hendrycksTest-college_medicine,5-shot,accuracy,0.3988439306358382,0.037336266553835096
openlm-research/open_llama_7b_v2,hendrycksTest-college_medicine,5-shot,acc_norm,0.3988439306358382,0.037336266553835096
openlm-research/open_llama_7b_v2,hendrycksTest-college_physics,5-shot,accuracy,0.22549019607843138,0.041583075330832865
openlm-research/open_llama_7b_v2,hendrycksTest-college_physics,5-shot,acc_norm,0.22549019607843138,0.041583075330832865
openlm-research/open_llama_7b_v2,hendrycksTest-computer_security,5-shot,accuracy,0.56,0.04988876515698589
openlm-research/open_llama_7b_v2,hendrycksTest-computer_security,5-shot,acc_norm,0.56,0.04988876515698589
openlm-research/open_llama_7b_v2,hendrycksTest-conceptual_physics,5-shot,accuracy,0.3446808510638298,0.03106898596312215
openlm-research/open_llama_7b_v2,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.3446808510638298,0.03106898596312215
openlm-research/open_llama_7b_v2,hendrycksTest-econometrics,5-shot,accuracy,0.30701754385964913,0.043391383225798615
openlm-research/open_llama_7b_v2,hendrycksTest-econometrics,5-shot,acc_norm,0.30701754385964913,0.043391383225798615
openlm-research/open_llama_7b_v2,hendrycksTest-electrical_engineering,5-shot,accuracy,0.43448275862068964,0.041307408795554966
openlm-research/open_llama_7b_v2,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.43448275862068964,0.041307408795554966
openlm-research/open_llama_7b_v2,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.2804232804232804,0.023135287974325635
openlm-research/open_llama_7b_v2,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.2804232804232804,0.023135287974325635
openlm-research/open_llama_7b_v2,hendrycksTest-formal_logic,5-shot,accuracy,0.35714285714285715,0.04285714285714281
openlm-research/open_llama_7b_v2,hendrycksTest-formal_logic,5-shot,acc_norm,0.35714285714285715,0.04285714285714281
openlm-research/open_llama_7b_v2,hendrycksTest-global_facts,5-shot,accuracy,0.34,0.04760952285695235
openlm-research/open_llama_7b_v2,hendrycksTest-global_facts,5-shot,acc_norm,0.34,0.04760952285695235
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_biology,5-shot,accuracy,0.44516129032258067,0.028272410186214906
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_biology,5-shot,acc_norm,0.44516129032258067,0.028272410186214906
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.2660098522167488,0.031089826002937523
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.2660098522167488,0.031089826002937523
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.34,0.04760952285695236
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.34,0.04760952285695236
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_european_history,5-shot,accuracy,0.42424242424242425,0.038592681420702615
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.42424242424242425,0.038592681420702615
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_geography,5-shot,accuracy,0.4696969696969697,0.03555804051763929
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_geography,5-shot,acc_norm,0.4696969696969697,0.03555804051763929
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.5803108808290155,0.035615873276858834
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.5803108808290155,0.035615873276858834
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.40512820512820513,0.024890471769938145
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.40512820512820513,0.024890471769938145
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.23703703703703705,0.025928876132766114
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.23703703703703705,0.025928876132766114
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.38235294117647056,0.031566630992154156
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.38235294117647056,0.031566630992154156
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_physics,5-shot,accuracy,0.304635761589404,0.03757949922943343
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_physics,5-shot,acc_norm,0.304635761589404,0.03757949922943343
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_psychology,5-shot,accuracy,0.5284403669724771,0.021402615697348054
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.5284403669724771,0.021402615697348054
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_statistics,5-shot,accuracy,0.27314814814814814,0.030388051301678116
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.27314814814814814,0.030388051301678116
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_us_history,5-shot,accuracy,0.45098039215686275,0.03492406104163613
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.45098039215686275,0.03492406104163613
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_world_history,5-shot,accuracy,0.4767932489451477,0.032512152011410174
openlm-research/open_llama_7b_v2,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.4767932489451477,0.032512152011410174
openlm-research/open_llama_7b_v2,hendrycksTest-human_aging,5-shot,accuracy,0.4170403587443946,0.03309266936071721
openlm-research/open_llama_7b_v2,hendrycksTest-human_aging,5-shot,acc_norm,0.4170403587443946,0.03309266936071721
openlm-research/open_llama_7b_v2,hendrycksTest-human_sexuality,5-shot,accuracy,0.5114503816793893,0.04384140024078016
openlm-research/open_llama_7b_v2,hendrycksTest-human_sexuality,5-shot,acc_norm,0.5114503816793893,0.04384140024078016
openlm-research/open_llama_7b_v2,hendrycksTest-international_law,5-shot,accuracy,0.48760330578512395,0.045629515481807666
openlm-research/open_llama_7b_v2,hendrycksTest-international_law,5-shot,acc_norm,0.48760330578512395,0.045629515481807666
openlm-research/open_llama_7b_v2,hendrycksTest-jurisprudence,5-shot,accuracy,0.5185185185185185,0.04830366024635331
openlm-research/open_llama_7b_v2,hendrycksTest-jurisprudence,5-shot,acc_norm,0.5185185185185185,0.04830366024635331
openlm-research/open_llama_7b_v2,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3803680981595092,0.03814269893261837
openlm-research/open_llama_7b_v2,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3803680981595092,0.03814269893261837
openlm-research/open_llama_7b_v2,hendrycksTest-machine_learning,5-shot,accuracy,0.32142857142857145,0.0443280405529152
openlm-research/open_llama_7b_v2,hendrycksTest-machine_learning,5-shot,acc_norm,0.32142857142857145,0.0443280405529152
openlm-research/open_llama_7b_v2,hendrycksTest-management,5-shot,accuracy,0.5728155339805825,0.04897957737781168
openlm-research/open_llama_7b_v2,hendrycksTest-management,5-shot,acc_norm,0.5728155339805825,0.04897957737781168
openlm-research/open_llama_7b_v2,hendrycksTest-marketing,5-shot,accuracy,0.6111111111111112,0.031937057262002924
openlm-research/open_llama_7b_v2,hendrycksTest-marketing,5-shot,acc_norm,0.6111111111111112,0.031937057262002924
openlm-research/open_llama_7b_v2,hendrycksTest-medical_genetics,5-shot,accuracy,0.53,0.050161355804659205
openlm-research/open_llama_7b_v2,hendrycksTest-medical_genetics,5-shot,acc_norm,0.53,0.050161355804659205
openlm-research/open_llama_7b_v2,hendrycksTest-miscellaneous,5-shot,accuracy,0.5683269476372924,0.0177122289392998
openlm-research/open_llama_7b_v2,hendrycksTest-miscellaneous,5-shot,acc_norm,0.5683269476372924,0.0177122289392998
openlm-research/open_llama_7b_v2,hendrycksTest-moral_disputes,5-shot,accuracy,0.4508670520231214,0.02678881193156276
openlm-research/open_llama_7b_v2,hendrycksTest-moral_disputes,5-shot,acc_norm,0.4508670520231214,0.02678881193156276
openlm-research/open_llama_7b_v2,hendrycksTest-moral_scenarios,5-shot,accuracy,0.24804469273743016,0.014444157808261433
openlm-research/open_llama_7b_v2,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.24804469273743016,0.014444157808261433
openlm-research/open_llama_7b_v2,hendrycksTest-nutrition,5-shot,accuracy,0.42810457516339867,0.02833239748366427
openlm-research/open_llama_7b_v2,hendrycksTest-nutrition,5-shot,acc_norm,0.42810457516339867,0.02833239748366427
openlm-research/open_llama_7b_v2,hendrycksTest-philosophy,5-shot,accuracy,0.40192926045016075,0.027846476005930477
openlm-research/open_llama_7b_v2,hendrycksTest-philosophy,5-shot,acc_norm,0.40192926045016075,0.027846476005930477
openlm-research/open_llama_7b_v2,hendrycksTest-prehistory,5-shot,accuracy,0.4382716049382716,0.02760791408740047
openlm-research/open_llama_7b_v2,hendrycksTest-prehistory,5-shot,acc_norm,0.4382716049382716,0.02760791408740047
openlm-research/open_llama_7b_v2,hendrycksTest-professional_accounting,5-shot,accuracy,0.31560283687943264,0.027724989449509317
openlm-research/open_llama_7b_v2,hendrycksTest-professional_accounting,5-shot,acc_norm,0.31560283687943264,0.027724989449509317
openlm-research/open_llama_7b_v2,hendrycksTest-professional_law,5-shot,accuracy,0.3363754889178618,0.01206708307945222
openlm-research/open_llama_7b_v2,hendrycksTest-professional_law,5-shot,acc_norm,0.3363754889178618,0.01206708307945222
openlm-research/open_llama_7b_v2,hendrycksTest-professional_medicine,5-shot,accuracy,0.44485294117647056,0.030187532060329387
openlm-research/open_llama_7b_v2,hendrycksTest-professional_medicine,5-shot,acc_norm,0.44485294117647056,0.030187532060329387
openlm-research/open_llama_7b_v2,hendrycksTest-professional_psychology,5-shot,accuracy,0.3627450980392157,0.01945076843250551
openlm-research/open_llama_7b_v2,hendrycksTest-professional_psychology,5-shot,acc_norm,0.3627450980392157,0.01945076843250551
openlm-research/open_llama_7b_v2,hendrycksTest-public_relations,5-shot,accuracy,0.4636363636363636,0.047764491623961985
openlm-research/open_llama_7b_v2,hendrycksTest-public_relations,5-shot,acc_norm,0.4636363636363636,0.047764491623961985
openlm-research/open_llama_7b_v2,hendrycksTest-security_studies,5-shot,accuracy,0.4489795918367347,0.03184213866687579
openlm-research/open_llama_7b_v2,hendrycksTest-security_studies,5-shot,acc_norm,0.4489795918367347,0.03184213866687579
openlm-research/open_llama_7b_v2,hendrycksTest-sociology,5-shot,accuracy,0.5771144278606966,0.034932317774212816
openlm-research/open_llama_7b_v2,hendrycksTest-sociology,5-shot,acc_norm,0.5771144278606966,0.034932317774212816
openlm-research/open_llama_7b_v2,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.53,0.050161355804659205
openlm-research/open_llama_7b_v2,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.53,0.050161355804659205
openlm-research/open_llama_7b_v2,hendrycksTest-virology,5-shot,accuracy,0.41566265060240964,0.038367221765980515
openlm-research/open_llama_7b_v2,hendrycksTest-virology,5-shot,acc_norm,0.41566265060240964,0.038367221765980515
openlm-research/open_llama_7b_v2,hendrycksTest-world_religions,5-shot,accuracy,0.5321637426900585,0.03826882417660368
openlm-research/open_llama_7b_v2,hendrycksTest-world_religions,5-shot,acc_norm,0.5321637426900585,0.03826882417660368
openlm-research/open_llama_7b_v2,truthfulqa:mc,0-shot,mc1,0.23745410036719705,0.014896277441041836
openlm-research/open_llama_7b_v2,truthfulqa:mc,0-shot,mc2,0.35539215429408955,0.013611390479523594
openlm-research/open_llama_7b_v2,drop,3-shot,accuracy,0.001153523489932886,0.0003476179896857104
openlm-research/open_llama_7b_v2,drop,3-shot,f1,0.05493078859060421,0.0013198629767466948
jisukim8873/falcon-7B-case-3,arc:challenge,25-shot,accuracy,0.44112627986348124,0.014509747749064664
jisukim8873/falcon-7B-case-3,arc:challenge,25-shot,acc_norm,0.4778156996587031,0.014597001927076136
jisukim8873/falcon-7B-case-3,hellaswag,10-shot,accuracy,0.5955984863572994,0.004897728370737241
jisukim8873/falcon-7B-case-3,hellaswag,10-shot,acc_norm,0.783011352320255,0.0041135241598451115
jisukim8873/falcon-7B-case-3,hendrycksTest-abstract_algebra,5-shot,accuracy,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-3,hendrycksTest-abstract_algebra,5-shot,acc_norm,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-3,hendrycksTest-anatomy,5-shot,accuracy,0.3333333333333333,0.04072314811876837
jisukim8873/falcon-7B-case-3,hendrycksTest-anatomy,5-shot,acc_norm,0.3333333333333333,0.04072314811876837
jisukim8873/falcon-7B-case-3,hendrycksTest-astronomy,5-shot,accuracy,0.29605263157894735,0.03715062154998905
jisukim8873/falcon-7B-case-3,hendrycksTest-astronomy,5-shot,acc_norm,0.29605263157894735,0.03715062154998905
jisukim8873/falcon-7B-case-3,hendrycksTest-business_ethics,5-shot,accuracy,0.22,0.0416333199893227
jisukim8873/falcon-7B-case-3,hendrycksTest-business_ethics,5-shot,acc_norm,0.22,0.0416333199893227
jisukim8873/falcon-7B-case-3,hendrycksTest-clinical_knowledge,5-shot,accuracy,0.35094339622641507,0.029373646253234686
jisukim8873/falcon-7B-case-3,hendrycksTest-clinical_knowledge,5-shot,acc_norm,0.35094339622641507,0.029373646253234686
jisukim8873/falcon-7B-case-3,hendrycksTest-college_biology,5-shot,accuracy,0.2638888888888889,0.03685651095897532
jisukim8873/falcon-7B-case-3,hendrycksTest-college_biology,5-shot,acc_norm,0.2638888888888889,0.03685651095897532
jisukim8873/falcon-7B-case-3,hendrycksTest-college_chemistry,5-shot,accuracy,0.18,0.03861229196653697
jisukim8873/falcon-7B-case-3,hendrycksTest-college_chemistry,5-shot,acc_norm,0.18,0.03861229196653697
jisukim8873/falcon-7B-case-3,hendrycksTest-college_computer_science,5-shot,accuracy,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-3,hendrycksTest-college_computer_science,5-shot,acc_norm,0.3,0.046056618647183814
jisukim8873/falcon-7B-case-3,hendrycksTest-college_mathematics,5-shot,accuracy,0.28,0.04512608598542127
jisukim8873/falcon-7B-case-3,hendrycksTest-college_mathematics,5-shot,acc_norm,0.28,0.04512608598542127
jisukim8873/falcon-7B-case-3,hendrycksTest-college_medicine,5-shot,accuracy,0.35260115606936415,0.03643037168958548
jisukim8873/falcon-7B-case-3,hendrycksTest-college_medicine,5-shot,acc_norm,0.35260115606936415,0.03643037168958548
jisukim8873/falcon-7B-case-3,hendrycksTest-college_physics,5-shot,accuracy,0.20588235294117646,0.04023382273617748
jisukim8873/falcon-7B-case-3,hendrycksTest-college_physics,5-shot,acc_norm,0.20588235294117646,0.04023382273617748
jisukim8873/falcon-7B-case-3,hendrycksTest-computer_security,5-shot,accuracy,0.42,0.04960449637488584
jisukim8873/falcon-7B-case-3,hendrycksTest-computer_security,5-shot,acc_norm,0.42,0.04960449637488584
jisukim8873/falcon-7B-case-3,hendrycksTest-conceptual_physics,5-shot,accuracy,0.35319148936170214,0.031245325202761926
jisukim8873/falcon-7B-case-3,hendrycksTest-conceptual_physics,5-shot,acc_norm,0.35319148936170214,0.031245325202761926
jisukim8873/falcon-7B-case-3,hendrycksTest-econometrics,5-shot,accuracy,0.2894736842105263,0.042663394431593935
jisukim8873/falcon-7B-case-3,hendrycksTest-econometrics,5-shot,acc_norm,0.2894736842105263,0.042663394431593935
jisukim8873/falcon-7B-case-3,hendrycksTest-electrical_engineering,5-shot,accuracy,0.296551724137931,0.03806142687309994
jisukim8873/falcon-7B-case-3,hendrycksTest-electrical_engineering,5-shot,acc_norm,0.296551724137931,0.03806142687309994
jisukim8873/falcon-7B-case-3,hendrycksTest-elementary_mathematics,5-shot,accuracy,0.24074074074074073,0.0220190800122179
jisukim8873/falcon-7B-case-3,hendrycksTest-elementary_mathematics,5-shot,acc_norm,0.24074074074074073,0.0220190800122179
jisukim8873/falcon-7B-case-3,hendrycksTest-formal_logic,5-shot,accuracy,0.20634920634920634,0.036196045241242515
jisukim8873/falcon-7B-case-3,hendrycksTest-formal_logic,5-shot,acc_norm,0.20634920634920634,0.036196045241242515
jisukim8873/falcon-7B-case-3,hendrycksTest-global_facts,5-shot,accuracy,0.33,0.047258156262526045
jisukim8873/falcon-7B-case-3,hendrycksTest-global_facts,5-shot,acc_norm,0.33,0.047258156262526045
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_biology,5-shot,accuracy,0.33548387096774196,0.026860206444724356
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_biology,5-shot,acc_norm,0.33548387096774196,0.026860206444724356
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_chemistry,5-shot,accuracy,0.30049261083743845,0.03225799476233484
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_chemistry,5-shot,acc_norm,0.30049261083743845,0.03225799476233484
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_computer_science,5-shot,accuracy,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_computer_science,5-shot,acc_norm,0.36,0.04824181513244218
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_european_history,5-shot,accuracy,0.296969696969697,0.03567969772268048
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_european_history,5-shot,acc_norm,0.296969696969697,0.03567969772268048
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_geography,5-shot,accuracy,0.3939393939393939,0.03481285338232963
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_geography,5-shot,acc_norm,0.3939393939393939,0.03481285338232963
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_government_and_politics,5-shot,accuracy,0.35233160621761656,0.03447478286414357
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_government_and_politics,5-shot,acc_norm,0.35233160621761656,0.03447478286414357
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_macroeconomics,5-shot,accuracy,0.30512820512820515,0.023346335293325887
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_macroeconomics,5-shot,acc_norm,0.30512820512820515,0.023346335293325887
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_mathematics,5-shot,accuracy,0.24814814814814815,0.0263357394040558
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_mathematics,5-shot,acc_norm,0.24814814814814815,0.0263357394040558
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_microeconomics,5-shot,accuracy,0.3067226890756303,0.02995382389188704
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_microeconomics,5-shot,acc_norm,0.3067226890756303,0.02995382389188704
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_physics,5-shot,accuracy,0.2847682119205298,0.03684881521389023
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_physics,5-shot,acc_norm,0.2847682119205298,0.03684881521389023
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_psychology,5-shot,accuracy,0.3192660550458716,0.019987829069750006
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_psychology,5-shot,acc_norm,0.3192660550458716,0.019987829069750006
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_statistics,5-shot,accuracy,0.18518518518518517,0.026491914727355157
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_statistics,5-shot,acc_norm,0.18518518518518517,0.026491914727355157
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_us_history,5-shot,accuracy,0.3235294117647059,0.03283472056108567
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_us_history,5-shot,acc_norm,0.3235294117647059,0.03283472056108567
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_world_history,5-shot,accuracy,0.3628691983122363,0.03129920825530213
jisukim8873/falcon-7B-case-3,hendrycksTest-high_school_world_history,5-shot,acc_norm,0.3628691983122363,0.03129920825530213
jisukim8873/falcon-7B-case-3,hendrycksTest-human_aging,5-shot,accuracy,0.3721973094170404,0.032443052830087304
jisukim8873/falcon-7B-case-3,hendrycksTest-human_aging,5-shot,acc_norm,0.3721973094170404,0.032443052830087304
jisukim8873/falcon-7B-case-3,hendrycksTest-human_sexuality,5-shot,accuracy,0.3511450381679389,0.041864451630137495
jisukim8873/falcon-7B-case-3,hendrycksTest-human_sexuality,5-shot,acc_norm,0.3511450381679389,0.041864451630137495
jisukim8873/falcon-7B-case-3,hendrycksTest-international_law,5-shot,accuracy,0.38016528925619836,0.04431324501968432
jisukim8873/falcon-7B-case-3,hendrycksTest-international_law,5-shot,acc_norm,0.38016528925619836,0.04431324501968432
jisukim8873/falcon-7B-case-3,hendrycksTest-jurisprudence,5-shot,accuracy,0.3333333333333333,0.04557239513497752
jisukim8873/falcon-7B-case-3,hendrycksTest-jurisprudence,5-shot,acc_norm,0.3333333333333333,0.04557239513497752
jisukim8873/falcon-7B-case-3,hendrycksTest-logical_fallacies,5-shot,accuracy,0.3067484662576687,0.036230899157241474
jisukim8873/falcon-7B-case-3,hendrycksTest-logical_fallacies,5-shot,acc_norm,0.3067484662576687,0.036230899157241474
jisukim8873/falcon-7B-case-3,hendrycksTest-machine_learning,5-shot,accuracy,0.35714285714285715,0.04547960999764376
jisukim8873/falcon-7B-case-3,hendrycksTest-machine_learning,5-shot,acc_norm,0.35714285714285715,0.04547960999764376
jisukim8873/falcon-7B-case-3,hendrycksTest-management,5-shot,accuracy,0.30097087378640774,0.045416094465039476
jisukim8873/falcon-7B-case-3,hendrycksTest-management,5-shot,acc_norm,0.30097087378640774,0.045416094465039476
jisukim8873/falcon-7B-case-3,hendrycksTest-marketing,5-shot,accuracy,0.4017094017094017,0.032116937510516204
jisukim8873/falcon-7B-case-3,hendrycksTest-marketing,5-shot,acc_norm,0.4017094017094017,0.032116937510516204
jisukim8873/falcon-7B-case-3,hendrycksTest-medical_genetics,5-shot,accuracy,0.4,0.049236596391733084
jisukim8873/falcon-7B-case-3,hendrycksTest-medical_genetics,5-shot,acc_norm,0.4,0.049236596391733084
jisukim8873/falcon-7B-case-3,hendrycksTest-miscellaneous,5-shot,accuracy,0.40485312899106,0.017553246467720263
jisukim8873/falcon-7B-case-3,hendrycksTest-miscellaneous,5-shot,acc_norm,0.40485312899106,0.017553246467720263
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_disputes,5-shot,accuracy,0.33815028901734107,0.025469770149400175
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_disputes,5-shot,acc_norm,0.33815028901734107,0.025469770149400175
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_scenarios,5-shot,accuracy,0.25027932960893856,0.014487500852850409
jisukim8873/falcon-7B-case-3,hendrycksTest-moral_scenarios,5-shot,acc_norm,0.25027932960893856,0.014487500852850409
jisukim8873/falcon-7B-case-3,hendrycksTest-nutrition,5-shot,accuracy,0.3235294117647059,0.026787453111906532
jisukim8873/falcon-7B-case-3,hendrycksTest-nutrition,5-shot,acc_norm,0.3235294117647059,0.026787453111906532
jisukim8873/falcon-7B-case-3,hendrycksTest-philosophy,5-shot,accuracy,0.3279742765273312,0.026664410886937613
jisukim8873/falcon-7B-case-3,hendrycksTest-philosophy,5-shot,acc_norm,0.3279742765273312,0.026664410886937613
jisukim8873/falcon-7B-case-3,hendrycksTest-prehistory,5-shot,accuracy,0.3148148148148148,0.02584224870090217
jisukim8873/falcon-7B-case-3,hendrycksTest-prehistory,5-shot,acc_norm,0.3148148148148148,0.02584224870090217
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_accounting,5-shot,accuracy,0.2553191489361702,0.02601199293090202
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_accounting,5-shot,acc_norm,0.2553191489361702,0.02601199293090202
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_law,5-shot,accuracy,0.26597131681877445,0.011285033165551288
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_law,5-shot,acc_norm,0.26597131681877445,0.011285033165551288
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_medicine,5-shot,accuracy,0.39705882352941174,0.02972215209928006
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_medicine,5-shot,acc_norm,0.39705882352941174,0.02972215209928006
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_psychology,5-shot,accuracy,0.2875816993464052,0.018311653053648222
jisukim8873/falcon-7B-case-3,hendrycksTest-professional_psychology,5-shot,acc_norm,0.2875816993464052,0.018311653053648222
jisukim8873/falcon-7B-case-3,hendrycksTest-public_relations,5-shot,accuracy,0.3090909090909091,0.044262946482000985
jisukim8873/falcon-7B-case-3,hendrycksTest-public_relations,5-shot,acc_norm,0.3090909090909091,0.044262946482000985
jisukim8873/falcon-7B-case-3,hendrycksTest-security_studies,5-shot,accuracy,0.35918367346938773,0.03071356045510849
jisukim8873/falcon-7B-case-3,hendrycksTest-security_studies,5-shot,acc_norm,0.35918367346938773,0.03071356045510849
jisukim8873/falcon-7B-case-3,hendrycksTest-sociology,5-shot,accuracy,0.3681592039800995,0.03410410565495301
jisukim8873/falcon-7B-case-3,hendrycksTest-sociology,5-shot,acc_norm,0.3681592039800995,0.03410410565495301
jisukim8873/falcon-7B-case-3,hendrycksTest-us_foreign_policy,5-shot,accuracy,0.47,0.05016135580465919
jisukim8873/falcon-7B-case-3,hendrycksTest-us_foreign_policy,5-shot,acc_norm,0.47,0.05016135580465919
jisukim8873/falcon-7B-case-3,hendrycksTest-virology,5-shot,accuracy,0.3313253012048193,0.036643147772880864
jisukim8873/falcon-7B-case-3,hendrycksTest-virology,5-shot,acc_norm,0.3313253012048193,0.036643147772880864
jisukim8873/falcon-7B-case-3,hendrycksTest-world_religions,5-shot,accuracy,0.4152046783625731,0.03779275945503201
jisukim8873/falcon-7B-case-3,hendrycksTest-world_religions,5-shot,acc_norm,0.4152046783625731,0.03779275945503201
jisukim8873/falcon-7B-case-3,truthfulqa:mc,0-shot,mc1,0.24724602203182375,0.01510240479735965
jisukim8873/falcon-7B-case-3,truthfulqa:mc,0-shot,mc2,0.36433197582570465,0.014190689156837067
jisukim8873/falcon-7B-case-3,winogrande,5-shot,accuracy,0.7103393843725335,0.012748550807638256
jisukim8873/falcon-7B-case-3,gsm8k,5-shot,accuracy,0.06595905989385899,0.006836951192034178
jisukim8873/falcon-7B-case-3,minerva_math_precalc,5-shot,accuracy,0.020146520146520148,0.00601841788965395
jisukim8873/falcon-7B-case-3,minerva_math_prealgebra,5-shot,accuracy,0.030998851894374284,0.005875912555745253
jisukim8873/falcon-7B-case-3,minerva_math_num_theory,5-shot,accuracy,0.018518518518518517,0.005806972807912266
jisukim8873/falcon-7B-case-3,minerva_math_intermediate_algebra,5-shot,accuracy,0.016611295681063124,0.004255602872194622
jisukim8873/falcon-7B-case-3,minerva_math_geometry,5-shot,accuracy,0.010438413361169102,0.004648627117184685
jisukim8873/falcon-7B-case-3,minerva_math_counting_and_prob,5-shot,accuracy,0.016877637130801686,0.005922826894852686
jisukim8873/falcon-7B-case-3,minerva_math_algebra,5-shot,accuracy,0.012636899747262006,0.0032435184443521804
jisukim8873/falcon-7B-case-3,fld_default,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-3,fld_star,0-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-3,arithmetic_3da,5-shot,accuracy,0.0225,0.0033169829948455245
jisukim8873/falcon-7B-case-3,arithmetic_3ds,5-shot,accuracy,0.0445,0.004611996341621292
jisukim8873/falcon-7B-case-3,arithmetic_4da,5-shot,accuracy,0.0005,0.0005000000000000151
jisukim8873/falcon-7B-case-3,arithmetic_2ds,5-shot,accuracy,0.3365,0.010568335718547789
jisukim8873/falcon-7B-case-3,arithmetic_5ds,5-shot,accuracy,0.003,0.0012232122154646975
jisukim8873/falcon-7B-case-3,arithmetic_5da,5-shot,accuracy,0.0,
jisukim8873/falcon-7B-case-3,arithmetic_1dc,5-shot,accuracy,0.074,0.005854838987520069
jisukim8873/falcon-7B-case-3,arithmetic_4ds,5-shot,accuracy,0.002,0.0009992493430694784
jisukim8873/falcon-7B-case-3,arithmetic_2dm,5-shot,accuracy,0.1455,0.007886442352955771
jisukim8873/falcon-7B-case-3,arithmetic_2da,5-shot,accuracy,0.28,0.01004243124012325
jisukim8873/falcon-7B-case-3,gsm8k_cot,5-shot,accuracy,0.10765731614859743,0.008537484003023336
jisukim8873/falcon-7B-case-3,anli_r2,0-shot,brier_score,0.9498963226859567,
jisukim8873/falcon-7B-case-3,anli_r3,0-shot,brier_score,0.9049030040265976,
jisukim8873/falcon-7B-case-3,anli_r1,0-shot,brier_score,0.9935104969984004,
jisukim8873/falcon-7B-case-3,xnli_eu,0-shot,brier_score,1.0601168919059503,
jisukim8873/falcon-7B-case-3,xnli_vi,0-shot,brier_score,1.0474950786006254,
jisukim8873/falcon-7B-case-3,xnli_ru,0-shot,brier_score,0.830398964603772,
jisukim8873/falcon-7B-case-3,xnli_zh,0-shot,brier_score,0.9872301490924532,
jisukim8873/falcon-7B-case-3,xnli_tr,0-shot,brier_score,0.9775824306933933,
jisukim8873/falcon-7B-case-3,xnli_fr,0-shot,brier_score,0.7460284259966345,
jisukim8873/falcon-7B-case-3,xnli_en,0-shot,brier_score,0.6732677740933464,
jisukim8873/falcon-7B-case-3,xnli_ur,0-shot,brier_score,1.2965021008598239,
jisukim8873/falcon-7B-case-3,xnli_ar,0-shot,brier_score,1.2688541556577642,
jisukim8873/falcon-7B-case-3,xnli_de,0-shot,brier_score,0.8333599994517225,
jisukim8873/falcon-7B-case-3,xnli_hi,0-shot,brier_score,1.125339879413296,
jisukim8873/falcon-7B-case-3,xnli_es,0-shot,brier_score,0.8171417939353351,
jisukim8873/falcon-7B-case-3,xnli_bg,0-shot,brier_score,1.0120503254545998,
jisukim8873/falcon-7B-case-3,xnli_sw,0-shot,brier_score,1.1011185031976496,
jisukim8873/falcon-7B-case-3,xnli_el,0-shot,brier_score,0.8884252615275592,
jisukim8873/falcon-7B-case-3,xnli_th,0-shot,brier_score,0.9584810095383591,
jisukim8873/falcon-7B-case-3,logiqa2,0-shot,brier_score,1.1023888188527122,
jisukim8873/falcon-7B-case-3,mathqa,5-shot,brier_score,0.9436004260298938,
jisukim8873/falcon-7B-case-3,lambada_standard,0-shot,perplexity,4.165215660982916,0.0923976679815377
jisukim8873/falcon-7B-case-3,lambada_standard,0-shot,accuracy,0.6712594605084417,0.006544612151352773
jisukim8873/falcon-7B-case-3,lambada_openai,0-shot,perplexity,3.340843946340243,0.06994859508074691
jisukim8873/falcon-7B-case-3,lambada_openai,0-shot,accuracy,0.736270133902581,0.0061391793635698555
