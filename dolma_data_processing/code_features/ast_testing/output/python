Node id : 93958103920448, Node type: module, Node text: b'import requests\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport argparse\nimport random\nimport json\nimport gzip\nfrom io import BytesIO\nimport pandas as pd\nimport os\nimport logging\nfrom tqdm import tqdm\nimport sys\nimport gc\n\nLLAMA_DIR = "/data/datasets/models/huggingface/meta-llama/Llama-2-70b-hf/"\n\nTOKENS_TO_FETCH_10B = {\n    "common-crawl": 5_186_000_000,\n    "c4": 1_396_000_000,\n    "peS2o": 796_000_000,\n    "stack-code": 2_188_000_000,\n    "gutenberg-books": 231_000_000,\n    "wiki-en-simple": 200_000_000,\n}\n\nMAX_DUMP_SIZE = 500_000_000\n\n\ndef parse_num(val: str) -> int:\n    if val.lower().endswith("b"):\n        return int(val[:-1]) * 1_000_000_000\n    elif val.lower().endswith("m"):\n        return int(val[:-1]) * 1_000_000\n    else:\n        try:\n            return int(float(val))\n        except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )\n\n\ndef process_zipped_file(content: bytes, file_ind: int) -> list:\n    if file_ind % 50 == 0:\n        print(f"Processing file {file_ind}")\n    with gzip.open(BytesIO(content), "rt", errors="ignore") as f:\n        try:\n            lines = f.readlines()\n            lines = [line.strip() for line in lines]\n            return lines\n        except Exception as e:\n            print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []\n\n\ndef fetch_tokens(\n    num_tokens: int,\n    domain: str,\n    output_dir: str or None,\n    all_files_lst: list,\n    seed: int = 42,\n):\n    current_tokens = 0\n    output_dir = output_dir if output_dir else f"./dolma/{domain}_{num_tokens}"\n    logging.info(f"Fetching {num_tokens} tokens from {domain}")\n\n    # shuffle\n    random.seed(seed)\n    random.shuffle(all_files_lst)\n    texts_to_dump = []\n\n    # filter out non-gz files\n    all_files_lst = [f for f in all_files_lst if f.endswith(".gz")]\n\n    # filter by top level domain\n    all_files_lst = [f for f in all_files_lst if domain in f]\n    tokenizer = AutoTokenizer.from_pretrained(LLAMA_DIR)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    file_ind = 0\n    part_ind = 0\n    with tqdm(total=num_tokens) as pbar:\n        while current_tokens < num_tokens and file_ind < len(all_files_lst):\n            response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )\n\n            if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue\n\n            file_ind += 1\n\n            docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]\n\n            # just keep main data\n            fields_to_keep = ["text", "id", "lang"]\n            docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]\n\n            # tokenizing individually to avoid oom\n            for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break\n\n    logging.info(f"Saved all output ({current_tokens} tokens)")\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--num_tokens",\n        help="Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )\n    parser.add_argument(\n        "--num_total_tokens",\n        help="Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )\n    parser.add_argument("--output", help="Output dir", type=str)\n    parser.add_argument(\n        "--domain",\n        help="Domains to fetch",\n        type=str,\n        choices=[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ],\n    )\n    parser.add_argument("--seed", help="Random seed", type=int, default=42)\n    args = parser.parse_args()\n    logging.basicConfig(level=logging.INFO)\n\n    if args.num_tokens and args.num_total_tokens:\n        raise ValueError(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )\n\n    if args.num_tokens:\n        logging.info(f"Fetching {args.num_tokens} tokens")\n        num_tokens = parse_num(args.num_tokens)\n    # Calculate num_tokens from domain and num_total_tokens\n    elif args.num_total_tokens and args.domain:\n        logging.info("Total domain tokens not specified, using 10B ratio mix")\n        num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000\n    # Calculate num_tokens from domain and num_total_tokens=10B\n    elif args.domain:\n        logging.info("Total tokens/domain tokens not specified, using 10B mix")\n        num_tokens = TOKENS_TO_FETCH_10B[args.domain]\n\n    # the flask server has to be up on clio\n    all_files_lst = requests.get("http://128.2.209.71:5000/list-all").json()\n    if args.domain:\n        fetch_tokens(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )\n    else:\n        logging.info("Fetching from all domains following the 10B ratio mix")\n        for domain in TOKENS_TO_FETCH_10B.keys():\n            logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )\n'
  Node id : 93958104423216, Node type: import_statement, Node text: b'import requests'
    Node id : 93958104416128, Node type: dotted_name, Node text: b'requests'
      Node id : 93958104441536, Node type: identifier, Node text: b'requests'
  Node id : 93958103518096, Node type: import_from_statement, Node text: b'from transformers import AutoTokenizer'
    Node id : 93958104413880, Node type: dotted_name, Node text: b'transformers'
      Node id : 93958104399264, Node type: identifier, Node text: b'transformers'
    Node id : 93958103010416, Node type: dotted_name, Node text: b'AutoTokenizer'
      Node id : 93958103010320, Node type: identifier, Node text: b'AutoTokenizer'
  Node id : 93958102910432, Node type: import_from_statement, Node text: b'from datasets import load_dataset'
    Node id : 93958102947416, Node type: dotted_name, Node text: b'datasets'
      Node id : 93958103561952, Node type: identifier, Node text: b'datasets'
    Node id : 93958103480000, Node type: dotted_name, Node text: b'load_dataset'
      Node id : 93958103479904, Node type: identifier, Node text: b'load_dataset'
  Node id : 93958103685456, Node type: import_statement, Node text: b'import argparse'
    Node id : 93958104524496, Node type: dotted_name, Node text: b'argparse'
      Node id : 93958104524400, Node type: identifier, Node text: b'argparse'
  Node id : 93958104017168, Node type: import_statement, Node text: b'import random'
    Node id : 93958104014256, Node type: dotted_name, Node text: b'random'
      Node id : 93958104014160, Node type: identifier, Node text: b'random'
  Node id : 93958103321408, Node type: import_statement, Node text: b'import json'
    Node id : 93958104500528, Node type: dotted_name, Node text: b'json'
      Node id : 93958104000928, Node type: identifier, Node text: b'json'
  Node id : 93958104596624, Node type: import_statement, Node text: b'import gzip'
    Node id : 93958103430832, Node type: dotted_name, Node text: b'gzip'
      Node id : 93958103430736, Node type: identifier, Node text: b'gzip'
  Node id : 93958103876048, Node type: import_from_statement, Node text: b'from io import BytesIO'
    Node id : 93958103882792, Node type: dotted_name, Node text: b'io'
      Node id : 93958103217696, Node type: identifier, Node text: b'io'
    Node id : 93958103882688, Node type: dotted_name, Node text: b'BytesIO'
      Node id : 93958103882592, Node type: identifier, Node text: b'BytesIO'
  Node id : 93958103730400, Node type: import_statement, Node text: b'import pandas as pd'
    Node id : 93958103317760, Node type: aliased_import, Node text: b'pandas as pd'
      Node id : 93958103317648, Node type: dotted_name, Node text: b'pandas'
        Node id : 93958103876272, Node type: identifier, Node text: b'pandas'
      Node id : 93958103317664, Node type: identifier, Node text: b'pd'
  Node id : 93958104566000, Node type: import_statement, Node text: b'import os'
    Node id : 93958103349168, Node type: dotted_name, Node text: b'os'
      Node id : 93958103349072, Node type: identifier, Node text: b'os'
  Node id : 93958103657328, Node type: import_statement, Node text: b'import logging'
    Node id : 93958103542320, Node type: dotted_name, Node text: b'logging'
      Node id : 93958103542224, Node type: identifier, Node text: b'logging'
  Node id : 93958103362816, Node type: import_from_statement, Node text: b'from tqdm import tqdm'
    Node id : 93958103362696, Node type: dotted_name, Node text: b'tqdm'
      Node id : 93958103702672, Node type: identifier, Node text: b'tqdm'
    Node id : 93958103362592, Node type: dotted_name, Node text: b'tqdm'
      Node id : 93958103702864, Node type: identifier, Node text: b'tqdm'
  Node id : 93958103408752, Node type: import_statement, Node text: b'import sys'
    Node id : 93958103408544, Node type: dotted_name, Node text: b'sys'
      Node id : 93958103046016, Node type: identifier, Node text: b'sys'
  Node id : 93958104002352, Node type: import_statement, Node text: b'import gc'
    Node id : 93958103426976, Node type: dotted_name, Node text: b'gc'
      Node id : 93958103426880, Node type: identifier, Node text: b'gc'
  Node id : 93958103173952, Node type: expression_statement, Node text: b'LLAMA_DIR = "/data/datasets/models/huggingface/meta-llama/Llama-2-70b-hf/"'
    Node id : 93958103173856, Node type: assignment, Node text: b'LLAMA_DIR = "/data/datasets/models/huggingface/meta-llama/Llama-2-70b-hf/"'
      Node id : 93958104002576, Node type: identifier, Node text: b'LLAMA_DIR'
      Node id : 93958103327120, Node type: string, Node text: b'"/data/datasets/models/huggingface/meta-llama/Llama-2-70b-hf/"'
        Node id : 93958103433280, Node type: string_start, Node text: b'"'
        Node id : 93958103433288, Node type: string_content, Node text: b'/data/datasets/models/huggingface/meta-llama/Llama-2-70b-hf/'
        Node id : 93958103433296, Node type: string_end, Node text: b'"'
  Node id : 93958104716352, Node type: expression_statement, Node text: b'TOKENS_TO_FETCH_10B = {\n    "common-crawl": 5_186_000_000,\n    "c4": 1_396_000_000,\n    "peS2o": 796_000_000,\n    "stack-code": 2_188_000_000,\n    "gutenberg-books": 231_000_000,\n    "wiki-en-simple": 200_000_000,\n}'
    Node id : 93958104716256, Node type: assignment, Node text: b'TOKENS_TO_FETCH_10B = {\n    "common-crawl": 5_186_000_000,\n    "c4": 1_396_000_000,\n    "peS2o": 796_000_000,\n    "stack-code": 2_188_000_000,\n    "gutenberg-books": 231_000_000,\n    "wiki-en-simple": 200_000_000,\n}'
      Node id : 93958104504048, Node type: identifier, Node text: b'TOKENS_TO_FETCH_10B'
      Node id : 93958104715856, Node type: dictionary, Node text: b'{\n    "common-crawl": 5_186_000_000,\n    "c4": 1_396_000_000,\n    "peS2o": 796_000_000,\n    "stack-code": 2_188_000_000,\n    "gutenberg-books": 231_000_000,\n    "wiki-en-simple": 200_000_000,\n}'
        Node id : 93958104715736, Node type: pair, Node text: b'"common-crawl": 5_186_000_000'
          Node id : 93958104571648, Node type: string, Node text: b'"common-crawl"'
            Node id : 93958103458224, Node type: string_start, Node text: b'"'
            Node id : 93958103458232, Node type: string_content, Node text: b'common-crawl'
            Node id : 93958103458240, Node type: string_end, Node text: b'"'
          Node id : 93958104571840, Node type: integer, Node text: b'5_186_000_000'
        Node id : 93958103305944, Node type: pair, Node text: b'"c4": 1_396_000_000'
          Node id : 93958103738384, Node type: string, Node text: b'"c4"'
            Node id : 93958103142288, Node type: string_start, Node text: b'"'
            Node id : 93958103142296, Node type: string_content, Node text: b'c4'
            Node id : 93958103142304, Node type: string_end, Node text: b'"'
          Node id : 93958103018672, Node type: integer, Node text: b'1_396_000_000'
        Node id : 93958104576728, Node type: pair, Node text: b'"peS2o": 796_000_000'
          Node id : 93958104576224, Node type: string, Node text: b'"peS2o"'
            Node id : 93958104433376, Node type: string_start, Node text: b'"'
            Node id : 93958104433384, Node type: string_content, Node text: b'peS2o'
            Node id : 93958104433392, Node type: string_end, Node text: b'"'
          Node id : 93958104576416, Node type: integer, Node text: b'796_000_000'
        Node id : 93958104577944, Node type: pair, Node text: b'"stack-code": 2_188_000_000'
          Node id : 93958104577440, Node type: string, Node text: b'"stack-code"'
            Node id : 93958104577328, Node type: string_start, Node text: b'"'
            Node id : 93958104577336, Node type: string_content, Node text: b'stack-code'
            Node id : 93958104577344, Node type: string_end, Node text: b'"'
          Node id : 93958104577632, Node type: integer, Node text: b'2_188_000_000'
        Node id : 93958104579160, Node type: pair, Node text: b'"gutenberg-books": 231_000_000'
          Node id : 93958104578656, Node type: string, Node text: b'"gutenberg-books"'
            Node id : 93958104578544, Node type: string_start, Node text: b'"'
            Node id : 93958104578552, Node type: string_content, Node text: b'gutenberg-books'
            Node id : 93958104578560, Node type: string_end, Node text: b'"'
          Node id : 93958104578848, Node type: integer, Node text: b'231_000_000'
        Node id : 93958104715416, Node type: pair, Node text: b'"wiki-en-simple": 200_000_000'
          Node id : 93958104714912, Node type: string, Node text: b'"wiki-en-simple"'
            Node id : 93958104714800, Node type: string_start, Node text: b'"'
            Node id : 93958104714808, Node type: string_content, Node text: b'wiki-en-simple'
            Node id : 93958104714816, Node type: string_end, Node text: b'"'
          Node id : 93958104715104, Node type: integer, Node text: b'200_000_000'
  Node id : 93958104717264, Node type: expression_statement, Node text: b'MAX_DUMP_SIZE = 500_000_000'
    Node id : 93958104717168, Node type: assignment, Node text: b'MAX_DUMP_SIZE = 500_000_000'
      Node id : 93958104716576, Node type: identifier, Node text: b'MAX_DUMP_SIZE'
      Node id : 93958104716768, Node type: integer, Node text: b'500_000_000'
  Node id : 93958104737080, Node type: function_definition, Node text: b'def parse_num(val: str) -> int:\n    if val.lower().endswith("b"):\n        return int(val[:-1]) * 1_000_000_000\n    elif val.lower().endswith("m"):\n        return int(val[:-1]) * 1_000_000\n    else:\n        try:\n            return int(float(val))\n        except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
    Node id : 93958104736920, Node type: identifier, Node text: b'parse_num'
    Node id : 93958104736928, Node type: parameters, Node text: b'(val: str)'
      Node id : 93958104717888, Node type: typed_parameter, Node text: b'val: str'
        Node id : 93958104717776, Node type: identifier, Node text: b'val'
        Node id : 93958104717792, Node type: type, Node text: b'str'
          Node id : 93958104717488, Node type: identifier, Node text: b'str'
    Node id : 93958104736944, Node type: type, Node text: b'int'
      Node id : 93958104718192, Node type: identifier, Node text: b'int'
    Node id : 93958104736968, Node type: block, Node text: b'if val.lower().endswith("b"):\n        return int(val[:-1]) * 1_000_000_000\n    elif val.lower().endswith("m"):\n        return int(val[:-1]) * 1_000_000\n    else:\n        try:\n            return int(float(val))\n        except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
      Node id : 93958104736800, Node type: if_statement, Node text: b'if val.lower().endswith("b"):\n        return int(val[:-1]) * 1_000_000_000\n    elif val.lower().endswith("m"):\n        return int(val[:-1]) * 1_000_000\n    else:\n        try:\n            return int(float(val))\n        except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
        Node id : 93958104721280, Node type: call, Node text: b'val.lower().endswith("b")'
          Node id : 93958104719792, Node type: attribute, Node text: b'val.lower().endswith'
            Node id : 93958104719584, Node type: call, Node text: b'val.lower()'
              Node id : 93958104719264, Node type: attribute, Node text: b'val.lower'
                Node id : 93958104718576, Node type: identifier, Node text: b'val'
                Node id : 93958104718928, Node type: identifier, Node text: b'lower'
              Node id : 93958104719480, Node type: argument_list, Node text: b'()'
            Node id : 93958104719696, Node type: identifier, Node text: b'endswith'
          Node id : 93958104721176, Node type: argument_list, Node text: b'("b")'
            Node id : 93958104720864, Node type: string, Node text: b'"b"'
              Node id : 93958104720512, Node type: string_start, Node text: b'"'
              Node id : 93958104720520, Node type: string_content, Node text: b'b'
              Node id : 93958104720528, Node type: string_end, Node text: b'"'
        Node id : 93958104736688, Node type: block, Node text: b'return int(val[:-1]) * 1_000_000_000'
          Node id : 93958104725056, Node type: return_statement, Node text: b'return int(val[:-1]) * 1_000_000_000'
            Node id : 93958104724656, Node type: binary_operator, Node text: b'int(val[:-1]) * 1_000_000_000'
              Node id : 93958104724256, Node type: call, Node text: b'int(val[:-1])'
                Node id : 93958104721568, Node type: identifier, Node text: b'int'
                Node id : 93958104724152, Node type: argument_list, Node text: b'(val[:-1])'
                  Node id : 93958104723840, Node type: subscript, Node text: b'val[:-1]'
                    Node id : 93958104721904, Node type: identifier, Node text: b'val'
                    Node id : 93958104723728, Node type: slice, Node text: b':-1'
                      Node id : 93958104723408, Node type: unary_operator, Node text: b'-1'
                        Node id : 93958104722960, Node type: integer, Node text: b'1'
              Node id : 93958104724448, Node type: integer, Node text: b'1_000_000_000'
        Node id : 93958104730912, Node type: elif_clause, Node text: b'elif val.lower().endswith("m"):\n        return int(val[:-1]) * 1_000_000'
          Node id : 93958104727024, Node type: call, Node text: b'val.lower().endswith("m")'
            Node id : 93958104726016, Node type: attribute, Node text: b'val.lower().endswith'
              Node id : 93958104725808, Node type: call, Node text: b'val.lower()'
                Node id : 93958104725488, Node type: attribute, Node text: b'val.lower'
                  Node id : 93958104725280, Node type: identifier, Node text: b'val'
                  Node id : 93958104725392, Node type: identifier, Node text: b'lower'
                Node id : 93958104725704, Node type: argument_list, Node text: b'()'
              Node id : 93958104725920, Node type: identifier, Node text: b'endswith'
            Node id : 93958104726920, Node type: argument_list, Node text: b'("m")'
              Node id : 93958104726608, Node type: string, Node text: b'"m"'
                Node id : 93958104726496, Node type: string_start, Node text: b'"'
                Node id : 93958104726504, Node type: string_content, Node text: b'm'
                Node id : 93958104726512, Node type: string_end, Node text: b'"'
          Node id : 93958104730816, Node type: block, Node text: b'return int(val[:-1]) * 1_000_000'
            Node id : 93958104730560, Node type: return_statement, Node text: b'return int(val[:-1]) * 1_000_000'
              Node id : 93958104730160, Node type: binary_operator, Node text: b'int(val[:-1]) * 1_000_000'
                Node id : 93958104729760, Node type: call, Node text: b'int(val[:-1])'
                  Node id : 93958104727312, Node type: identifier, Node text: b'int'
                  Node id : 93958104729656, Node type: argument_list, Node text: b'(val[:-1])'
                    Node id : 93958104729344, Node type: subscript, Node text: b'val[:-1]'
                      Node id : 93958104727408, Node type: identifier, Node text: b'val'
                      Node id : 93958104729232, Node type: slice, Node text: b':-1'
                        Node id : 93958104728912, Node type: unary_operator, Node text: b'-1'
                          Node id : 93958104728464, Node type: integer, Node text: b'1'
                Node id : 93958104729952, Node type: integer, Node text: b'1_000_000'
        Node id : 93958104736704, Node type: else_clause, Node text: b'else:\n        try:\n            return int(float(val))\n        except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
          Node id : 93958104736552, Node type: block, Node text: b'try:\n            return int(float(val))\n        except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
            Node id : 93958104736416, Node type: try_statement, Node text: b'try:\n            return int(float(val))\n        except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
              Node id : 93958104736216, Node type: block, Node text: b'return int(float(val))'
                Node id : 93958104733200, Node type: return_statement, Node text: b'return int(float(val))'
                  Node id : 93958104732800, Node type: call, Node text: b'int(float(val))'
                    Node id : 93958104731200, Node type: identifier, Node text: b'int'
                    Node id : 93958104732696, Node type: argument_list, Node text: b'(float(val))'
                      Node id : 93958104732288, Node type: call, Node text: b'float(val)'
                        Node id : 93958104731296, Node type: identifier, Node text: b'float'
                        Node id : 93958104732184, Node type: argument_list, Node text: b'(val)'
                          Node id : 93958104731392, Node type: identifier, Node text: b'val'
              Node id : 93958104736224, Node type: except_clause, Node text: b'except:\n            raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
                Node id : 93958104736088, Node type: block, Node text: b'raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
                  Node id : 93958104735744, Node type: raise_statement, Node text: b'raise ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
                    Node id : 93958104735344, Node type: call, Node text: b'ValueError(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
                      Node id : 93958104733520, Node type: identifier, Node text: b'ValueError'
                      Node id : 93958104735240, Node type: argument_list, Node text: b'(\n                "You must pass either an integer, scientific notation, or xB/xM for num tokens"\n            )'
                        Node id : 93958104734832, Node type: string, Node text: b'"You must pass either an integer, scientific notation, or xB/xM for num tokens"'
                          Node id : 93958104734480, Node type: string_start, Node text: b'"'
                          Node id : 93958104734488, Node type: string_content, Node text: b'You must pass either an integer, scientific notation, or xB/xM for num tokens'
                          Node id : 93958104734496, Node type: string_end, Node text: b'"'
  Node id : 93958104878544, Node type: function_definition, Node text: b'def process_zipped_file(content: bytes, file_ind: int) -> list:\n    if file_ind % 50 == 0:\n        print(f"Processing file {file_ind}")\n    with gzip.open(BytesIO(content), "rt", errors="ignore") as f:\n        try:\n            lines = f.readlines()\n            lines = [line.strip() for line in lines]\n            return lines\n        except Exception as e:\n            print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []'
    Node id : 93958104757672, Node type: identifier, Node text: b'process_zipped_file'
    Node id : 93958104757680, Node type: parameters, Node text: b'(content: bytes, file_ind: int)'
      Node id : 93958104737584, Node type: typed_parameter, Node text: b'content: bytes'
        Node id : 93958104737472, Node type: identifier, Node text: b'content'
        Node id : 93958104737488, Node type: type, Node text: b'bytes'
          Node id : 93958104737184, Node type: identifier, Node text: b'bytes'
      Node id : 93958104738080, Node type: typed_parameter, Node text: b'file_ind: int'
        Node id : 93958104737968, Node type: identifier, Node text: b'file_ind'
        Node id : 93958104737984, Node type: type, Node text: b'int'
          Node id : 93958104737680, Node type: identifier, Node text: b'int'
    Node id : 93958104757696, Node type: type, Node text: b'list'
      Node id : 93958104738512, Node type: identifier, Node text: b'list'
    Node id : 93958104757720, Node type: block, Node text: b'if file_ind % 50 == 0:\n        print(f"Processing file {file_ind}")\n    with gzip.open(BytesIO(content), "rt", errors="ignore") as f:\n        try:\n            lines = f.readlines()\n            lines = [line.strip() for line in lines]\n            return lines\n        except Exception as e:\n            print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []'
      Node id : 93958104757440, Node type: if_statement, Node text: b'if file_ind % 50 == 0:\n        print(f"Processing file {file_ind}")'
        Node id : 93958104739616, Node type: comparison_operator, Node text: b'file_ind % 50 == 0'
          Node id : 93958104739200, Node type: binary_operator, Node text: b'file_ind % 50'
            Node id : 93958104738896, Node type: identifier, Node text: b'file_ind'
            Node id : 93958104738992, Node type: integer, Node text: b'50'
          Node id : 93958104739296, Node type: integer, Node text: b'0'
        Node id : 93958104743424, Node type: block, Node text: b'print(f"Processing file {file_ind}")'
          Node id : 93958104743168, Node type: expression_statement, Node text: b'print(f"Processing file {file_ind}")'
            Node id : 93958104742784, Node type: call, Node text: b'print(f"Processing file {file_ind}")'
              Node id : 93958104739808, Node type: identifier, Node text: b'print'
              Node id : 93958104742680, Node type: argument_list, Node text: b'(f"Processing file {file_ind}")'
                Node id : 93958104741216, Node type: string, Node text: b'f"Processing file {file_ind}"'
                  Node id : 93958104741008, Node type: string_start, Node text: b'f"'
                  Node id : 93958104740784, Node type: string_content, Node text: b'Processing file '
                  Node id : 93958104740792, Node type: interpolation, Node text: b'{file_ind}'
                    Node id : 93958104740288, Node type: identifier, Node text: b'file_ind'
                  Node id : 93958104741024, Node type: string_end, Node text: b'"'
      Node id : 93958104757448, Node type: with_statement, Node text: b'with gzip.open(BytesIO(content), "rt", errors="ignore") as f:\n        try:\n            lines = f.readlines()\n            lines = [line.strip() for line in lines]\n            return lines\n        except Exception as e:\n            print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []'
        Node id : 93958104757320, Node type: with_clause, Node text: b'gzip.open(BytesIO(content), "rt", errors="ignore") as f'
          Node id : 93958104745920, Node type: with_item, Node text: b'gzip.open(BytesIO(content), "rt", errors="ignore") as f'
            Node id : 93958104745728, Node type: as_pattern, Node text: b'gzip.open(BytesIO(content), "rt", errors="ignore") as f'
              Node id : 93958104745232, Node type: call, Node text: b'gzip.open(BytesIO(content), "rt", errors="ignore")'
                Node id : 93958104741312, Node type: attribute, Node text: b'gzip.open'
                  Node id : 93958104741120, Node type: identifier, Node text: b'gzip'
                  Node id : 93958104740912, Node type: identifier, Node text: b'open'
                Node id : 93958104745128, Node type: argument_list, Node text: b'(BytesIO(content), "rt", errors="ignore")'
                  Node id : 93958104743616, Node type: call, Node text: b'BytesIO(content)'
                    Node id : 93958104741712, Node type: identifier, Node text: b'BytesIO'
                    Node id : 93958104741912, Node type: argument_list, Node text: b'(content)'
                      Node id : 93958104741808, Node type: identifier, Node text: b'content'
                  Node id : 93958104743904, Node type: string, Node text: b'"rt"'
                    Node id : 93958104742112, Node type: string_start, Node text: b'"'
                    Node id : 93958104742120, Node type: string_content, Node text: b'rt'
                    Node id : 93958104742128, Node type: string_end, Node text: b'"'
                  Node id : 93958104744904, Node type: keyword_argument, Node text: b'errors="ignore"'
                    Node id : 93958104744784, Node type: identifier, Node text: b'errors'
                    Node id : 93958104744592, Node type: string, Node text: b'"ignore"'
                      Node id : 93958104744480, Node type: string_start, Node text: b'"'
                      Node id : 93958104744488, Node type: string_content, Node text: b'ignore'
                      Node id : 93958104744496, Node type: string_end, Node text: b'"'
              Node id : 93958104745632, Node type: as_pattern_target, Node text: b'f'
                Node id : 93958104745424, Node type: identifier, Node text: b'f'
        Node id : 93958104757344, Node type: block, Node text: b'try:\n            lines = f.readlines()\n            lines = [line.strip() for line in lines]\n            return lines\n        except Exception as e:\n            print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []'
          Node id : 93958104757200, Node type: try_statement, Node text: b'try:\n            lines = f.readlines()\n            lines = [line.strip() for line in lines]\n            return lines\n        except Exception as e:\n            print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []'
            Node id : 93958104757096, Node type: block, Node text: b'lines = f.readlines()\n            lines = [line.strip() for line in lines]\n            return lines'
              Node id : 93958104747424, Node type: expression_statement, Node text: b'lines = f.readlines()'
                Node id : 93958104747328, Node type: assignment, Node text: b'lines = f.readlines()'
                  Node id : 93958104746208, Node type: identifier, Node text: b'lines'
                  Node id : 93958104746928, Node type: call, Node text: b'f.readlines()'
                    Node id : 93958104746512, Node type: attribute, Node text: b'f.readlines'
                      Node id : 93958104746304, Node type: identifier, Node text: b'f'
                      Node id : 93958104746416, Node type: identifier, Node text: b'readlines'
                    Node id : 93958104746824, Node type: argument_list, Node text: b'()'
              Node id : 93958104749680, Node type: expression_statement, Node text: b'lines = [line.strip() for line in lines]'
                Node id : 93958104749584, Node type: assignment, Node text: b'lines = [line.strip() for line in lines]'
                  Node id : 93958104747536, Node type: identifier, Node text: b'lines'
                  Node id : 93958104749184, Node type: list_comprehension, Node text: b'[line.strip() for line in lines]'
                    Node id : 93958104748160, Node type: call, Node text: b'line.strip()'
                      Node id : 93958104747840, Node type: attribute, Node text: b'line.strip'
                        Node id : 93958104747632, Node type: identifier, Node text: b'line'
                        Node id : 93958104747744, Node type: identifier, Node text: b'strip'
                      Node id : 93958104748056, Node type: argument_list, Node text: b'()'
                    Node id : 93958104748864, Node type: for_in_clause, Node text: b'for line in lines'
                      Node id : 93958104748352, Node type: identifier, Node text: b'line'
                      Node id : 93958104748448, Node type: identifier, Node text: b'lines'
              Node id : 93958104750400, Node type: return_statement, Node text: b'return lines'
                Node id : 93958104750000, Node type: identifier, Node text: b'lines'
            Node id : 93958104757104, Node type: except_clause, Node text: b'except Exception as e:\n            print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []'
              Node id : 93958104751232, Node type: as_pattern, Node text: b'Exception as e'
                Node id : 93958104750736, Node type: identifier, Node text: b'Exception'
                Node id : 93958104751136, Node type: as_pattern_target, Node text: b'e'
                  Node id : 93958104750928, Node type: identifier, Node text: b'e'
              Node id : 93958104756976, Node type: block, Node text: b'print(f"Error occured while reading gzip: {e}")\n            print(f"Skipping file {file_ind}")\n            return []'
                Node id : 93958104754352, Node type: expression_statement, Node text: b'print(f"Error occured while reading gzip: {e}")'
                  Node id : 93958104754064, Node type: call, Node text: b'print(f"Error occured while reading gzip: {e}")'
                    Node id : 93958104751424, Node type: identifier, Node text: b'print'
                    Node id : 93958104753960, Node type: argument_list, Node text: b'(f"Error occured while reading gzip: {e}")'
                      Node id : 93958104752832, Node type: string, Node text: b'f"Error occured while reading gzip: {e}"'
                        Node id : 93958104752624, Node type: string_start, Node text: b'f"'
                        Node id : 93958104752400, Node type: string_content, Node text: b'Error occured while reading gzip: '
                        Node id : 93958104752408, Node type: interpolation, Node text: b'{e}'
                          Node id : 93958104751904, Node type: identifier, Node text: b'e'
                        Node id : 93958104752640, Node type: string_end, Node text: b'"'
                Node id : 93958104756720, Node type: expression_statement, Node text: b'print(f"Skipping file {file_ind}")'
                  Node id : 93958104756432, Node type: call, Node text: b'print(f"Skipping file {file_ind}")'
                    Node id : 93958104752736, Node type: identifier, Node text: b'print'
                    Node id : 93958104756328, Node type: argument_list, Node text: b'(f"Skipping file {file_ind}")'
                      Node id : 93958104755200, Node type: string, Node text: b'f"Skipping file {file_ind}"'
                        Node id : 93958104754992, Node type: string_start, Node text: b'f"'
                        Node id : 93958104753216, Node type: string_content, Node text: b'Skipping file '
                        Node id : 93958104753224, Node type: interpolation, Node text: b'{file_ind}'
                          Node id : 93958104754464, Node type: identifier, Node text: b'file_ind'
                        Node id : 93958104755008, Node type: string_end, Node text: b'"'
                Node id : 93958104755776, Node type: return_statement, Node text: b'return []'
                  Node id : 93958104755104, Node type: list, Node text: b'[]'
  Node id : 93958104878552, Node type: function_definition, Node text: b'def fetch_tokens(\n    num_tokens: int,\n    domain: str,\n    output_dir: str or None,\n    all_files_lst: list,\n    seed: int = 42,\n):\n    current_tokens = 0\n    output_dir = output_dir if output_dir else f"./dolma/{domain}_{num_tokens}"\n    logging.info(f"Fetching {num_tokens} tokens from {domain}")\n\n    # shuffle\n    random.seed(seed)\n    random.shuffle(all_files_lst)\n    texts_to_dump = []\n\n    # filter out non-gz files\n    all_files_lst = [f for f in all_files_lst if f.endswith(".gz")]\n\n    # filter by top level domain\n    all_files_lst = [f for f in all_files_lst if domain in f]\n    tokenizer = AutoTokenizer.from_pretrained(LLAMA_DIR)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    file_ind = 0\n    part_ind = 0\n    with tqdm(total=num_tokens) as pbar:\n        while current_tokens < num_tokens and file_ind < len(all_files_lst):\n            response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )\n\n            if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue\n\n            file_ind += 1\n\n            docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]\n\n            # just keep main data\n            fields_to_keep = ["text", "id", "lang"]\n            docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]\n\n            # tokenizing individually to avoid oom\n            for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break\n\n    logging.info(f"Saved all output ({current_tokens} tokens)")'
    Node id : 93958104878408, Node type: identifier, Node text: b'fetch_tokens'
    Node id : 93958104878416, Node type: parameters, Node text: b'(\n    num_tokens: int,\n    domain: str,\n    output_dir: str or None,\n    all_files_lst: list,\n    seed: int = 42,\n)'
      Node id : 93958104758336, Node type: typed_parameter, Node text: b'num_tokens: int'
        Node id : 93958104758224, Node type: identifier, Node text: b'num_tokens'
        Node id : 93958104758240, Node type: type, Node text: b'int'
          Node id : 93958104757936, Node type: identifier, Node text: b'int'
      Node id : 93958104758832, Node type: typed_parameter, Node text: b'domain: str'
        Node id : 93958104758720, Node type: identifier, Node text: b'domain'
        Node id : 93958104758736, Node type: type, Node text: b'str'
          Node id : 93958104758432, Node type: identifier, Node text: b'str'
      Node id : 93958104759840, Node type: typed_parameter, Node text: b'output_dir: str or None'
        Node id : 93958104759728, Node type: identifier, Node text: b'output_dir'
        Node id : 93958104759744, Node type: type, Node text: b'str or None'
          Node id : 93958104759536, Node type: boolean_operator, Node text: b'str or None'
            Node id : 93958104759040, Node type: identifier, Node text: b'str'
            Node id : 93958104759232, Node type: none, Node text: b'None'
      Node id : 93958104760560, Node type: typed_parameter, Node text: b'all_files_lst: list'
        Node id : 93958104760448, Node type: identifier, Node text: b'all_files_lst'
        Node id : 93958104760464, Node type: type, Node text: b'list'
          Node id : 93958104760160, Node type: identifier, Node text: b'list'
      Node id : 93958104761488, Node type: typed_default_parameter, Node text: b'seed: int = 42'
        Node id : 93958104761360, Node type: identifier, Node text: b'seed'
        Node id : 93958104761376, Node type: type, Node text: b'int'
          Node id : 93958104760880, Node type: identifier, Node text: b'int'
        Node id : 93958104761168, Node type: integer, Node text: b'42'
    Node id : 93958104878440, Node type: block, Node text: b'current_tokens = 0\n    output_dir = output_dir if output_dir else f"./dolma/{domain}_{num_tokens}"\n    logging.info(f"Fetching {num_tokens} tokens from {domain}")\n\n    # shuffle\n    random.seed(seed)\n    random.shuffle(all_files_lst)\n    texts_to_dump = []\n\n    # filter out non-gz files\n    all_files_lst = [f for f in all_files_lst if f.endswith(".gz")]\n\n    # filter by top level domain\n    all_files_lst = [f for f in all_files_lst if domain in f]\n    tokenizer = AutoTokenizer.from_pretrained(LLAMA_DIR)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    file_ind = 0\n    part_ind = 0\n    with tqdm(total=num_tokens) as pbar:\n        while current_tokens < num_tokens and file_ind < len(all_files_lst):\n            response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )\n\n            if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue\n\n            file_ind += 1\n\n            docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]\n\n            # just keep main data\n            fields_to_keep = ["text", "id", "lang"]\n            docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]\n\n            # tokenizing individually to avoid oom\n            for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break\n\n    logging.info(f"Saved all output ({current_tokens} tokens)")'
      Node id : 93958104762816, Node type: expression_statement, Node text: b'current_tokens = 0'
        Node id : 93958104762720, Node type: assignment, Node text: b'current_tokens = 0'
          Node id : 93958104762128, Node type: identifier, Node text: b'current_tokens'
          Node id : 93958104762320, Node type: integer, Node text: b'0'
      Node id : 93958104766048, Node type: expression_statement, Node text: b'output_dir = output_dir if output_dir else f"./dolma/{domain}_{num_tokens}"'
        Node id : 93958104765952, Node type: assignment, Node text: b'output_dir = output_dir if output_dir else f"./dolma/{domain}_{num_tokens}"'
          Node id : 93958104762928, Node type: identifier, Node text: b'output_dir'
          Node id : 93958104765648, Node type: conditional_expression, Node text: b'output_dir if output_dir else f"./dolma/{domain}_{num_tokens}"'
            Node id : 93958104763024, Node type: identifier, Node text: b'output_dir'
            Node id : 93958104763216, Node type: identifier, Node text: b'output_dir'
            Node id : 93958104765328, Node type: string, Node text: b'f"./dolma/{domain}_{num_tokens}"'
              Node id : 93958104765216, Node type: string_start, Node text: b'f"'
              Node id : 93958104764192, Node type: string_content, Node text: b'./dolma/'
              Node id : 93958104764200, Node type: interpolation, Node text: b'{domain}'
                Node id : 93958104763696, Node type: identifier, Node text: b'domain'
              Node id : 93958104764408, Node type: string_content, Node text: b'_'
              Node id : 93958104765016, Node type: interpolation, Node text: b'{num_tokens}'
                Node id : 93958104764512, Node type: identifier, Node text: b'num_tokens'
              Node id : 93958104765232, Node type: string_end, Node text: b'"'
      Node id : 93958104769200, Node type: expression_statement, Node text: b'logging.info(f"Fetching {num_tokens} tokens from {domain}")'
        Node id : 93958104768912, Node type: call, Node text: b'logging.info(f"Fetching {num_tokens} tokens from {domain}")'
          Node id : 93958104766480, Node type: attribute, Node text: b'logging.info'
            Node id : 93958104766272, Node type: identifier, Node text: b'logging'
            Node id : 93958104766384, Node type: identifier, Node text: b'info'
          Node id : 93958104768808, Node type: argument_list, Node text: b'(f"Fetching {num_tokens} tokens from {domain}")'
            Node id : 93958104768400, Node type: string, Node text: b'f"Fetching {num_tokens} tokens from {domain}"'
              Node id : 93958104768288, Node type: string_start, Node text: b'f"'
              Node id : 93958104767360, Node type: string_content, Node text: b'Fetching '
              Node id : 93958104767368, Node type: interpolation, Node text: b'{num_tokens}'
                Node id : 93958104766864, Node type: identifier, Node text: b'num_tokens'
              Node id : 93958104767576, Node type: string_content, Node text: b' tokens from '
              Node id : 93958104768184, Node type: interpolation, Node text: b'{domain}'
                Node id : 93958104767680, Node type: identifier, Node text: b'domain'
              Node id : 93958104768304, Node type: string_end, Node text: b'"'
      Node id : 93958104770648, Node type: comment, Node text: b'# shuffle'
      Node id : 93958104770528, Node type: expression_statement, Node text: b'random.seed(seed)'
        Node id : 93958104770240, Node type: call, Node text: b'random.seed(seed)'
          Node id : 93958104769632, Node type: attribute, Node text: b'random.seed'
            Node id : 93958104769424, Node type: identifier, Node text: b'random'
            Node id : 93958104769536, Node type: identifier, Node text: b'seed'
          Node id : 93958104770136, Node type: argument_list, Node text: b'(seed)'
            Node id : 93958104769728, Node type: identifier, Node text: b'seed'
      Node id : 93958104771856, Node type: expression_statement, Node text: b'random.shuffle(all_files_lst)'
        Node id : 93958104771568, Node type: call, Node text: b'random.shuffle(all_files_lst)'
          Node id : 93958104770960, Node type: attribute, Node text: b'random.shuffle'
            Node id : 93958104770752, Node type: identifier, Node text: b'random'
            Node id : 93958104770864, Node type: identifier, Node text: b'shuffle'
          Node id : 93958104771464, Node type: argument_list, Node text: b'(all_files_lst)'
            Node id : 93958104771056, Node type: identifier, Node text: b'all_files_lst'
      Node id : 93958104772880, Node type: expression_statement, Node text: b'texts_to_dump = []'
        Node id : 93958104772784, Node type: assignment, Node text: b'texts_to_dump = []'
          Node id : 93958104772080, Node type: identifier, Node text: b'texts_to_dump'
          Node id : 93958104772384, Node type: list, Node text: b'[]'
      Node id : 93958104776376, Node type: comment, Node text: b'# filter out non-gz files'
      Node id : 93958104776256, Node type: expression_statement, Node text: b'all_files_lst = [f for f in all_files_lst if f.endswith(".gz")]'
        Node id : 93958104776160, Node type: assignment, Node text: b'all_files_lst = [f for f in all_files_lst if f.endswith(".gz")]'
          Node id : 93958104773104, Node type: identifier, Node text: b'all_files_lst'
          Node id : 93958104775760, Node type: list_comprehension, Node text: b'[f for f in all_files_lst if f.endswith(".gz")]'
            Node id : 93958104773200, Node type: identifier, Node text: b'f'
            Node id : 93958104775424, Node type: for_in_clause, Node text: b'for f in all_files_lst'
              Node id : 93958104773392, Node type: identifier, Node text: b'f'
              Node id : 93958104773488, Node type: identifier, Node text: b'all_files_lst'
            Node id : 93958104775432, Node type: if_clause, Node text: b'if f.endswith(".gz")'
              Node id : 93958104775120, Node type: call, Node text: b'f.endswith(".gz")'
                Node id : 93958104774112, Node type: attribute, Node text: b'f.endswith'
                  Node id : 93958104773904, Node type: identifier, Node text: b'f'
                  Node id : 93958104774016, Node type: identifier, Node text: b'endswith'
                Node id : 93958104775016, Node type: argument_list, Node text: b'(".gz")'
                  Node id : 93958104774704, Node type: string, Node text: b'".gz"'
                    Node id : 93958104774592, Node type: string_start, Node text: b'"'
                    Node id : 93958104774600, Node type: string_content, Node text: b'.gz'
                    Node id : 93958104774608, Node type: string_end, Node text: b'"'
      Node id : 93958104778856, Node type: comment, Node text: b'# filter by top level domain'
      Node id : 93958104778736, Node type: expression_statement, Node text: b'all_files_lst = [f for f in all_files_lst if domain in f]'
        Node id : 93958104778640, Node type: assignment, Node text: b'all_files_lst = [f for f in all_files_lst if domain in f]'
          Node id : 93958104776480, Node type: identifier, Node text: b'all_files_lst'
          Node id : 93958104778240, Node type: list_comprehension, Node text: b'[f for f in all_files_lst if domain in f]'
            Node id : 93958104776576, Node type: identifier, Node text: b'f'
            Node id : 93958104777904, Node type: for_in_clause, Node text: b'for f in all_files_lst'
              Node id : 93958104776768, Node type: identifier, Node text: b'f'
              Node id : 93958104776864, Node type: identifier, Node text: b'all_files_lst'
            Node id : 93958104777912, Node type: if_clause, Node text: b'if domain in f'
              Node id : 93958104777696, Node type: comparison_operator, Node text: b'domain in f'
                Node id : 93958104777280, Node type: identifier, Node text: b'domain'
                Node id : 93958104777376, Node type: identifier, Node text: b'f'
      Node id : 93958104780368, Node type: expression_statement, Node text: b'tokenizer = AutoTokenizer.from_pretrained(LLAMA_DIR)'
        Node id : 93958104780272, Node type: assignment, Node text: b'tokenizer = AutoTokenizer.from_pretrained(LLAMA_DIR)'
          Node id : 93958104778960, Node type: identifier, Node text: b'tokenizer'
          Node id : 93958104779872, Node type: call, Node text: b'AutoTokenizer.from_pretrained(LLAMA_DIR)'
            Node id : 93958104779264, Node type: attribute, Node text: b'AutoTokenizer.from_pretrained'
              Node id : 93958104779056, Node type: identifier, Node text: b'AutoTokenizer'
              Node id : 93958104779168, Node type: identifier, Node text: b'from_pretrained'
            Node id : 93958104779768, Node type: argument_list, Node text: b'(LLAMA_DIR)'
              Node id : 93958104779360, Node type: identifier, Node text: b'LLAMA_DIR'
      Node id : 93958104781696, Node type: expression_statement, Node text: b'tokenizer.pad_token = tokenizer.eos_token'
        Node id : 93958104781600, Node type: assignment, Node text: b'tokenizer.pad_token = tokenizer.eos_token'
          Node id : 93958104780800, Node type: attribute, Node text: b'tokenizer.pad_token'
            Node id : 93958104780592, Node type: identifier, Node text: b'tokenizer'
            Node id : 93958104780704, Node type: identifier, Node text: b'pad_token'
          Node id : 93958104781200, Node type: attribute, Node text: b'tokenizer.eos_token'
            Node id : 93958104780896, Node type: identifier, Node text: b'tokenizer'
            Node id : 93958104781104, Node type: identifier, Node text: b'eos_token'
      Node id : 93958104782608, Node type: expression_statement, Node text: b'file_ind = 0'
        Node id : 93958104782512, Node type: assignment, Node text: b'file_ind = 0'
          Node id : 93958104781920, Node type: identifier, Node text: b'file_ind'
          Node id : 93958104782112, Node type: integer, Node text: b'0'
      Node id : 93958104783520, Node type: expression_statement, Node text: b'part_ind = 0'
        Node id : 93958104783424, Node type: assignment, Node text: b'part_ind = 0'
          Node id : 93958104782832, Node type: identifier, Node text: b'part_ind'
          Node id : 93958104783024, Node type: integer, Node text: b'0'
      Node id : 93958104875448, Node type: with_statement, Node text: b'with tqdm(total=num_tokens) as pbar:\n        while current_tokens < num_tokens and file_ind < len(all_files_lst):\n            response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )\n\n            if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue\n\n            file_ind += 1\n\n            docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]\n\n            # just keep main data\n            fields_to_keep = ["text", "id", "lang"]\n            docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]\n\n            # tokenizing individually to avoid oom\n            for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
        Node id : 93958104875320, Node type: with_clause, Node text: b'tqdm(total=num_tokens) as pbar'
          Node id : 93958104785056, Node type: with_item, Node text: b'tqdm(total=num_tokens) as pbar'
            Node id : 93958104784864, Node type: as_pattern, Node text: b'tqdm(total=num_tokens) as pbar'
              Node id : 93958104784368, Node type: call, Node text: b'tqdm(total=num_tokens)'
                Node id : 93958104783744, Node type: identifier, Node text: b'tqdm'
                Node id : 93958104784264, Node type: argument_list, Node text: b'(total=num_tokens)'
                  Node id : 93958104784152, Node type: keyword_argument, Node text: b'total=num_tokens'
                    Node id : 93958104784032, Node type: identifier, Node text: b'total'
                    Node id : 93958104783840, Node type: identifier, Node text: b'num_tokens'
              Node id : 93958104784768, Node type: as_pattern_target, Node text: b'pbar'
                Node id : 93958104784560, Node type: identifier, Node text: b'pbar'
        Node id : 93958104875344, Node type: block, Node text: b'while current_tokens < num_tokens and file_ind < len(all_files_lst):\n            response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )\n\n            if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue\n\n            file_ind += 1\n\n            docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]\n\n            # just keep main data\n            fields_to_keep = ["text", "id", "lang"]\n            docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]\n\n            # tokenizing individually to avoid oom\n            for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
          Node id : 93958104875200, Node type: while_statement, Node text: b'while current_tokens < num_tokens and file_ind < len(all_files_lst):\n            response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )\n\n            if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue\n\n            file_ind += 1\n\n            docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]\n\n            # just keep main data\n            fields_to_keep = ["text", "id", "lang"]\n            docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]\n\n            # tokenizing individually to avoid oom\n            for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
            Node id : 93958104786896, Node type: boolean_operator, Node text: b'current_tokens < num_tokens and file_ind < len(all_files_lst)'
              Node id : 93958104785664, Node type: comparison_operator, Node text: b'current_tokens < num_tokens'
                Node id : 93958104785248, Node type: identifier, Node text: b'current_tokens'
                Node id : 93958104785344, Node type: identifier, Node text: b'num_tokens'
              Node id : 93958104786688, Node type: comparison_operator, Node text: b'file_ind < len(all_files_lst)'
                Node id : 93958104785760, Node type: identifier, Node text: b'file_ind'
                Node id : 93958104786368, Node type: call, Node text: b'len(all_files_lst)'
                  Node id : 93958104785856, Node type: identifier, Node text: b'len'
                  Node id : 93958104786264, Node type: argument_list, Node text: b'(all_files_lst)'
                    Node id : 93958104785952, Node type: identifier, Node text: b'all_files_lst'
            Node id : 93958104875104, Node type: block, Node text: b'response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )\n\n            if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue\n\n            file_ind += 1\n\n            docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]\n\n            # just keep main data\n            fields_to_keep = ["text", "id", "lang"]\n            docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]\n\n            # tokenizing individually to avoid oom\n            for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
              Node id : 93958104789920, Node type: expression_statement, Node text: b'response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )'
                Node id : 93958104789824, Node type: assignment, Node text: b'response = requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )'
                  Node id : 93958104787088, Node type: identifier, Node text: b'response'
                  Node id : 93958104789424, Node type: call, Node text: b'requests.get(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )'
                    Node id : 93958104787392, Node type: attribute, Node text: b'requests.get'
                      Node id : 93958104787184, Node type: identifier, Node text: b'requests'
                      Node id : 93958104787296, Node type: identifier, Node text: b'get'
                    Node id : 93958104789320, Node type: argument_list, Node text: b'(\n                f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"\n            )'
                      Node id : 93958104788912, Node type: string, Node text: b'f"http://128.2.209.71:5000/{all_files_lst[file_ind]}"'
                        Node id : 93958104788800, Node type: string_start, Node text: b'f"'
                        Node id : 93958104788688, Node type: string_content, Node text: b'http://128.2.209.71:5000/'
                        Node id : 93958104788696, Node type: interpolation, Node text: b'{all_files_lst[file_ind]}'
                          Node id : 93958104788192, Node type: subscript, Node text: b'all_files_lst[file_ind]'
                            Node id : 93958104787776, Node type: identifier, Node text: b'all_files_lst'
                            Node id : 93958104787872, Node type: identifier, Node text: b'file_ind'
                        Node id : 93958104788816, Node type: string_end, Node text: b'"'
              Node id : 93958104794248, Node type: if_statement, Node text: b'if response.status_code != 200:\n                logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue'
                Node id : 93958104790656, Node type: comparison_operator, Node text: b'response.status_code != 200'
                  Node id : 93958104790240, Node type: attribute, Node text: b'response.status_code'
                    Node id : 93958104790032, Node type: identifier, Node text: b'response'
                    Node id : 93958104790144, Node type: identifier, Node text: b'status_code'
                  Node id : 93958104790336, Node type: integer, Node text: b'200'
                Node id : 93958104794144, Node type: block, Node text: b'logging.info(f"Error fetching {all_files_lst[file_ind]}")\n                continue'
                  Node id : 93958104793376, Node type: expression_statement, Node text: b'logging.info(f"Error fetching {all_files_lst[file_ind]}")'
                    Node id : 93958104793088, Node type: call, Node text: b'logging.info(f"Error fetching {all_files_lst[file_ind]}")'
                      Node id : 93958104791056, Node type: attribute, Node text: b'logging.info'
                        Node id : 93958104790848, Node type: identifier, Node text: b'logging'
                        Node id : 93958104790960, Node type: identifier, Node text: b'info'
                      Node id : 93958104792984, Node type: argument_list, Node text: b'(f"Error fetching {all_files_lst[file_ind]}")'
                        Node id : 93958104792576, Node type: string, Node text: b'f"Error fetching {all_files_lst[file_ind]}"'
                          Node id : 93958104792464, Node type: string_start, Node text: b'f"'
                          Node id : 93958104792352, Node type: string_content, Node text: b'Error fetching '
                          Node id : 93958104792360, Node type: interpolation, Node text: b'{all_files_lst[file_ind]}'
                            Node id : 93958104791856, Node type: subscript, Node text: b'all_files_lst[file_ind]'
                              Node id : 93958104791440, Node type: identifier, Node text: b'all_files_lst'
                              Node id : 93958104791536, Node type: identifier, Node text: b'file_ind'
                          Node id : 93958104792480, Node type: string_end, Node text: b'"'
                  Node id : 93958104793776, Node type: continue_statement, Node text: b'continue'
              Node id : 93958104795040, Node type: expression_statement, Node text: b'file_ind += 1'
                Node id : 93958104794944, Node type: augmented_assignment, Node text: b'file_ind += 1'
                  Node id : 93958104794352, Node type: identifier, Node text: b'file_ind'
                  Node id : 93958104794544, Node type: integer, Node text: b'1'
              Node id : 93958104798640, Node type: expression_statement, Node text: b'docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]'
                Node id : 93958104798544, Node type: assignment, Node text: b'docs = [\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]'
                  Node id : 93958104795264, Node type: identifier, Node text: b'docs'
                  Node id : 93958104798144, Node type: list_comprehension, Node text: b'[\n                json.loads(l) for l in process_zipped_file(response.content, file_ind)\n            ]'
                    Node id : 93958104796080, Node type: call, Node text: b'json.loads(l)'
                      Node id : 93958104795568, Node type: attribute, Node text: b'json.loads'
                        Node id : 93958104795360, Node type: identifier, Node text: b'json'
                        Node id : 93958104795472, Node type: identifier, Node text: b'loads'
                      Node id : 93958104795976, Node type: argument_list, Node text: b'(l)'
                        Node id : 93958104795664, Node type: identifier, Node text: b'l'
                    Node id : 93958104797824, Node type: for_in_clause, Node text: b'for l in process_zipped_file(response.content, file_ind)'
                      Node id : 93958104796272, Node type: identifier, Node text: b'l'
                      Node id : 93958104797408, Node type: call, Node text: b'process_zipped_file(response.content, file_ind)'
                        Node id : 93958104796368, Node type: identifier, Node text: b'process_zipped_file'
                        Node id : 93958104797304, Node type: argument_list, Node text: b'(response.content, file_ind)'
                          Node id : 93958104796672, Node type: attribute, Node text: b'response.content'
                            Node id : 93958104796464, Node type: identifier, Node text: b'response'
                            Node id : 93958104796576, Node type: identifier, Node text: b'content'
                          Node id : 93958104796864, Node type: identifier, Node text: b'file_ind'
              Node id : 93958104802296, Node type: comment, Node text: b'# just keep main data'
              Node id : 93958104802176, Node type: expression_statement, Node text: b'fields_to_keep = ["text", "id", "lang"]'
                Node id : 93958104802080, Node type: assignment, Node text: b'fields_to_keep = ["text", "id", "lang"]'
                  Node id : 93958104798864, Node type: identifier, Node text: b'fields_to_keep'
                  Node id : 93958104801680, Node type: list, Node text: b'["text", "id", "lang"]'
                    Node id : 93958104799456, Node type: string, Node text: b'"text"'
                      Node id : 93958104799344, Node type: string_start, Node text: b'"'
                      Node id : 93958104799352, Node type: string_content, Node text: b'text'
                      Node id : 93958104799360, Node type: string_end, Node text: b'"'
                    Node id : 93958104800144, Node type: string, Node text: b'"id"'
                      Node id : 93958104800032, Node type: string_start, Node text: b'"'
                      Node id : 93958104800040, Node type: string_content, Node text: b'id'
                      Node id : 93958104800048, Node type: string_end, Node text: b'"'
                    Node id : 93958104800944, Node type: string, Node text: b'"lang"'
                      Node id : 93958104800832, Node type: string_start, Node text: b'"'
                      Node id : 93958104800840, Node type: string_content, Node text: b'lang'
                      Node id : 93958104800848, Node type: string_end, Node text: b'"'
              Node id : 93958104806736, Node type: expression_statement, Node text: b'docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]'
                Node id : 93958104806640, Node type: assignment, Node text: b'docs = [{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]'
                  Node id : 93958104802400, Node type: identifier, Node text: b'docs'
                  Node id : 93958104806240, Node type: list_comprehension, Node text: b'[{k: v for k, v in d.items() if k in fields_to_keep} for d in docs]'
                    Node id : 93958104805216, Node type: dictionary_comprehension, Node text: b'{k: v for k, v in d.items() if k in fields_to_keep}'
                      Node id : 93958104805096, Node type: pair, Node text: b'k: v'
                        Node id : 93958104802496, Node type: identifier, Node text: b'k'
                        Node id : 93958104802688, Node type: identifier, Node text: b'v'
                      Node id : 93958104804976, Node type: for_in_clause, Node text: b'for k, v in d.items()'
                        Node id : 93958104804232, Node type: pattern_list, Node text: b'k, v'
                          Node id : 93958104802992, Node type: identifier, Node text: b'k'
                          Node id : 93958104803088, Node type: identifier, Node text: b'v'
                        Node id : 93958104803936, Node type: call, Node text: b'd.items()'
                          Node id : 93958104803616, Node type: attribute, Node text: b'd.items'
                            Node id : 93958104803408, Node type: identifier, Node text: b'd'
                            Node id : 93958104803520, Node type: identifier, Node text: b'items'
                          Node id : 93958104803832, Node type: argument_list, Node text: b'()'
                      Node id : 93958104804984, Node type: if_clause, Node text: b'if k in fields_to_keep'
                        Node id : 93958104804768, Node type: comparison_operator, Node text: b'k in fields_to_keep'
                          Node id : 93958104804352, Node type: identifier, Node text: b'k'
                          Node id : 93958104804448, Node type: identifier, Node text: b'fields_to_keep'
                    Node id : 93958104805920, Node type: for_in_clause, Node text: b'for d in docs'
                      Node id : 93958104805408, Node type: identifier, Node text: b'd'
                      Node id : 93958104805504, Node type: identifier, Node text: b'docs'
              Node id : 93958104874760, Node type: comment, Node text: b'# tokenizing individually to avoid oom'
              Node id : 93958104874768, Node type: for_statement, Node text: b'for _, doc in enumerate(docs):\n                encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
                Node id : 93958104874616, Node type: pattern_list, Node text: b'_, doc'
                  Node id : 93958104806960, Node type: identifier, Node text: b'_'
                  Node id : 93958104807056, Node type: identifier, Node text: b'doc'
                Node id : 93958104807888, Node type: call, Node text: b'enumerate(docs)'
                  Node id : 93958104807376, Node type: identifier, Node text: b'enumerate'
                  Node id : 93958104807784, Node type: argument_list, Node text: b'(docs)'
                    Node id : 93958104807472, Node type: identifier, Node text: b'docs'
                Node id : 93958104874656, Node type: block, Node text: b'encoded_inputs = tokenizer(doc["text"], return_tensors="pt")\n                num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )\n                current_tokens += sum(num_non_padding_toks)\n                pbar.update(sum(num_non_padding_toks))\n\n                texts_to_dump.append(doc)\n                # save the reduced dataset as a <= 500 MB arrow file\n                ## for table of random strings each with length 2000,\n                ## parquet file size is roughly 500 * size in memory\n                if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
                  Node id : 93958104811936, Node type: expression_statement, Node text: b'encoded_inputs = tokenizer(doc["text"], return_tensors="pt")'
                    Node id : 93958104811840, Node type: assignment, Node text: b'encoded_inputs = tokenizer(doc["text"], return_tensors="pt")'
                      Node id : 93958104808176, Node type: identifier, Node text: b'encoded_inputs'
                      Node id : 93958104811440, Node type: call, Node text: b'tokenizer(doc["text"], return_tensors="pt")'
                        Node id : 93958104808272, Node type: identifier, Node text: b'tokenizer'
                        Node id : 93958104811336, Node type: argument_list, Node text: b'(doc["text"], return_tensors="pt")'
                          Node id : 93958104809520, Node type: subscript, Node text: b'doc["text"]'
                            Node id : 93958104808368, Node type: identifier, Node text: b'doc'
                            Node id : 93958104809200, Node type: string, Node text: b'"text"'
                              Node id : 93958104808848, Node type: string_start, Node text: b'"'
                              Node id : 93958104808856, Node type: string_content, Node text: b'text'
                              Node id : 93958104808864, Node type: string_end, Node text: b'"'
                          Node id : 93958104811000, Node type: keyword_argument, Node text: b'return_tensors="pt"'
                            Node id : 93958104810880, Node type: identifier, Node text: b'return_tensors'
                            Node id : 93958104810688, Node type: string, Node text: b'"pt"'
                              Node id : 93958104810336, Node type: string_start, Node text: b'"'
                              Node id : 93958104810344, Node type: string_content, Node text: b'pt'
                              Node id : 93958104810352, Node type: string_end, Node text: b'"'
                  Node id : 93958104815936, Node type: expression_statement, Node text: b'num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )'
                    Node id : 93958104815840, Node type: assignment, Node text: b'num_non_padding_toks = (\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )'
                      Node id : 93958104812048, Node type: identifier, Node text: b'num_non_padding_toks'
                      Node id : 93958104815440, Node type: parenthesized_expression, Node text: b'(\n                    (encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()\n                )'
                        Node id : 93958104815040, Node type: call, Node text: b'(encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist()'
                          Node id : 93958104814720, Node type: attribute, Node text: b'(encoded_inputs["attention_mask"] == 1).sum(dim=1).tolist'
                            Node id : 93958104814512, Node type: call, Node text: b'(encoded_inputs["attention_mask"] == 1).sum(dim=1)'
                              Node id : 93958104813888, Node type: attribute, Node text: b'(encoded_inputs["attention_mask"] == 1).sum'
                                Node id : 93958104813680, Node type: parenthesized_expression, Node text: b'(encoded_inputs["attention_mask"] == 1)'
                                  Node id : 93958104813472, Node type: comparison_operator, Node text: b'encoded_inputs["attention_mask"] == 1'
                                    Node id : 93958104813056, Node type: subscript, Node text: b'encoded_inputs["attention_mask"]'
                                      Node id : 93958104812144, Node type: identifier, Node text: b'encoded_inputs'
                                      Node id : 93958104812736, Node type: string, Node text: b'"attention_mask"'
                                        Node id : 93958104812624, Node type: string_start, Node text: b'"'
                                        Node id : 93958104812632, Node type: string_content, Node text: b'attention_mask'
                                        Node id : 93958104812640, Node type: string_end, Node text: b'"'
                                    Node id : 93958104813152, Node type: integer, Node text: b'1'
                                Node id : 93958104813792, Node type: identifier, Node text: b'sum'
                              Node id : 93958104814408, Node type: argument_list, Node text: b'(dim=1)'
                                Node id : 93958104814296, Node type: keyword_argument, Node text: b'dim=1'
                                  Node id : 93958104814176, Node type: identifier, Node text: b'dim'
                                  Node id : 93958104813984, Node type: integer, Node text: b'1'
                            Node id : 93958104814624, Node type: identifier, Node text: b'tolist'
                          Node id : 93958104814936, Node type: argument_list, Node text: b'()'
                  Node id : 93958104817360, Node type: expression_statement, Node text: b'current_tokens += sum(num_non_padding_toks)'
                    Node id : 93958104817264, Node type: augmented_assignment, Node text: b'current_tokens += sum(num_non_padding_toks)'
                      Node id : 93958104816160, Node type: identifier, Node text: b'current_tokens'
                      Node id : 93958104816864, Node type: call, Node text: b'sum(num_non_padding_toks)'
                        Node id : 93958104816256, Node type: identifier, Node text: b'sum'
                        Node id : 93958104816760, Node type: argument_list, Node text: b'(num_non_padding_toks)'
                          Node id : 93958104816352, Node type: identifier, Node text: b'num_non_padding_toks'
                  Node id : 93958104819200, Node type: expression_statement, Node text: b'pbar.update(sum(num_non_padding_toks))'
                    Node id : 93958104818912, Node type: call, Node text: b'pbar.update(sum(num_non_padding_toks))'
                      Node id : 93958104817792, Node type: attribute, Node text: b'pbar.update'
                        Node id : 93958104817584, Node type: identifier, Node text: b'pbar'
                        Node id : 93958104817696, Node type: identifier, Node text: b'update'
                      Node id : 93958104818808, Node type: argument_list, Node text: b'(sum(num_non_padding_toks))'
                        Node id : 93958104818400, Node type: call, Node text: b'sum(num_non_padding_toks)'
                          Node id : 93958104817888, Node type: identifier, Node text: b'sum'
                          Node id : 93958104818296, Node type: argument_list, Node text: b'(num_non_padding_toks)'
                            Node id : 93958104817984, Node type: identifier, Node text: b'num_non_padding_toks'
                  Node id : 93958104821008, Node type: expression_statement, Node text: b'texts_to_dump.append(doc)'
                    Node id : 93958104820240, Node type: call, Node text: b'texts_to_dump.append(doc)'
                      Node id : 93958104819632, Node type: attribute, Node text: b'texts_to_dump.append'
                        Node id : 93958104819424, Node type: identifier, Node text: b'texts_to_dump'
                        Node id : 93958104819536, Node type: identifier, Node text: b'append'
                      Node id : 93958104820136, Node type: argument_list, Node text: b'(doc)'
                        Node id : 93958104819728, Node type: identifier, Node text: b'doc'
                  Node id : 93958104874280, Node type: comment, Node text: b'# save the reduced dataset as a <= 500 MB arrow file'
                  Node id : 93958104874288, Node type: comment, Node text: b'## for table of random strings each with length 2000,'
                  Node id : 93958104874296, Node type: comment, Node text: b'## parquet file size is roughly 500 * size in memory'
                  Node id : 93958104874304, Node type: if_statement, Node text: b'if (\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                ):\n                    part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
                    Node id : 93958104823840, Node type: parenthesized_expression, Node text: b'(\n                    current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE\n                )'
                      Node id : 93958104823632, Node type: boolean_operator, Node text: b'current_tokens >= num_tokens\n                    or sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE'
                        Node id : 93958104821648, Node type: comparison_operator, Node text: b'current_tokens >= num_tokens'
                          Node id : 93958104821232, Node type: identifier, Node text: b'current_tokens'
                          Node id : 93958104821328, Node type: identifier, Node text: b'num_tokens'
                        Node id : 93958104823424, Node type: comparison_operator, Node text: b'sys.getsizeof(texts_to_dump) * 500 >= MAX_DUMP_SIZE'
                          Node id : 93958104823008, Node type: binary_operator, Node text: b'sys.getsizeof(texts_to_dump) * 500'
                            Node id : 93958104822704, Node type: call, Node text: b'sys.getsizeof(texts_to_dump)'
                              Node id : 93958104821952, Node type: attribute, Node text: b'sys.getsizeof'
                                Node id : 93958104821744, Node type: identifier, Node text: b'sys'
                                Node id : 93958104821856, Node type: identifier, Node text: b'getsizeof'
                              Node id : 93958104822600, Node type: argument_list, Node text: b'(texts_to_dump)'
                                Node id : 93958104822048, Node type: identifier, Node text: b'texts_to_dump'
                            Node id : 93958104822800, Node type: integer, Node text: b'500'
                          Node id : 93958104823104, Node type: identifier, Node text: b'MAX_DUMP_SIZE'
                    Node id : 93958104874176, Node type: block, Node text: b'part_ind += 1\n                    output_file = f"{output_dir}/part_{part_ind}.arrow"\n                    logging.info(f"Output file is: {output_file}")\n\n                    # mkdir -p\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)\n                    df.to_parquet(output_file)\n\n                    # if the output file is too large, recursively split it in half\n                    # can\'t accurately predict parquet compression rate so this is necessary\n                    def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()\n\n                    split_file(output_file, df)\n                    texts_to_dump = []\n\n                    if current_tokens >= num_tokens:\n                        break\n\n                    if current_tokens >= num_tokens:\n                        break'
                      Node id : 93958104824624, Node type: expression_statement, Node text: b'part_ind += 1'
                        Node id : 93958104824528, Node type: augmented_assignment, Node text: b'part_ind += 1'
                          Node id : 93958104824032, Node type: identifier, Node text: b'part_ind'
                          Node id : 93958104824128, Node type: integer, Node text: b'1'
                      Node id : 93958104828208, Node type: expression_statement, Node text: b'output_file = f"{output_dir}/part_{part_ind}.arrow"'
                        Node id : 93958104828112, Node type: assignment, Node text: b'output_file = f"{output_dir}/part_{part_ind}.arrow"'
                          Node id : 93958104824736, Node type: identifier, Node text: b'output_file'
                          Node id : 93958104827712, Node type: string, Node text: b'f"{output_dir}/part_{part_ind}.arrow"'
                            Node id : 93958104827600, Node type: string_start, Node text: b'f"'
                            Node id : 93958104826240, Node type: interpolation, Node text: b'{output_dir}'
                              Node id : 93958104825168, Node type: identifier, Node text: b'output_dir'
                            Node id : 93958104826248, Node type: string_content, Node text: b'/part_'
                            Node id : 93958104827096, Node type: interpolation, Node text: b'{part_ind}'
                              Node id : 93958104826352, Node type: identifier, Node text: b'part_ind'
                            Node id : 93958104827400, Node type: string_content, Node text: b'.arrow'
                            Node id : 93958104827616, Node type: string_end, Node text: b'"'
                      Node id : 93958104830736, Node type: expression_statement, Node text: b'logging.info(f"Output file is: {output_file}")'
                        Node id : 93958104830256, Node type: call, Node text: b'logging.info(f"Output file is: {output_file}")'
                          Node id : 93958104828640, Node type: attribute, Node text: b'logging.info'
                            Node id : 93958104828432, Node type: identifier, Node text: b'logging'
                            Node id : 93958104828544, Node type: identifier, Node text: b'info'
                          Node id : 93958104830152, Node type: argument_list, Node text: b'(f"Output file is: {output_file}")'
                            Node id : 93958104829744, Node type: string, Node text: b'f"Output file is: {output_file}"'
                              Node id : 93958104829632, Node type: string_start, Node text: b'f"'
                              Node id : 93958104829520, Node type: string_content, Node text: b'Output file is: '
                              Node id : 93958104829528, Node type: interpolation, Node text: b'{output_file}'
                                Node id : 93958104829024, Node type: identifier, Node text: b'output_file'
                              Node id : 93958104829648, Node type: string_end, Node text: b'"'
                      Node id : 93958104833688, Node type: comment, Node text: b'# mkdir -p'
                      Node id : 93958104833568, Node type: expression_statement, Node text: b'os.makedirs(os.path.dirname(output_file), exist_ok=True)'
                        Node id : 93958104833280, Node type: call, Node text: b'os.makedirs(os.path.dirname(output_file), exist_ok=True)'
                          Node id : 93958104831168, Node type: attribute, Node text: b'os.makedirs'
                            Node id : 93958104830960, Node type: identifier, Node text: b'os'
                            Node id : 93958104831072, Node type: identifier, Node text: b'makedirs'
                          Node id : 93958104833176, Node type: argument_list, Node text: b'(os.path.dirname(output_file), exist_ok=True)'
                            Node id : 93958104832192, Node type: call, Node text: b'os.path.dirname(output_file)'
                              Node id : 93958104831680, Node type: attribute, Node text: b'os.path.dirname'
                                Node id : 93958104831472, Node type: attribute, Node text: b'os.path'
                                  Node id : 93958104831264, Node type: identifier, Node text: b'os'
                                  Node id : 93958104831376, Node type: identifier, Node text: b'path'
                                Node id : 93958104831584, Node type: identifier, Node text: b'dirname'
                              Node id : 93958104832088, Node type: argument_list, Node text: b'(output_file)'
                                Node id : 93958104831776, Node type: identifier, Node text: b'output_file'
                            Node id : 93958104832936, Node type: keyword_argument, Node text: b'exist_ok=True'
                              Node id : 93958104832816, Node type: identifier, Node text: b'exist_ok'
                              Node id : 93958104832384, Node type: true, Node text: b'True'
                      Node id : 93958104835872, Node type: expression_statement, Node text: b'df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)'
                        Node id : 93958104835776, Node type: assignment, Node text: b'df = pd.DataFrame(texts_to_dump, columns=fields_to_keep)'
                          Node id : 93958104833792, Node type: identifier, Node text: b'df'
                          Node id : 93958104835376, Node type: call, Node text: b'pd.DataFrame(texts_to_dump, columns=fields_to_keep)'
                            Node id : 93958104834096, Node type: attribute, Node text: b'pd.DataFrame'
                              Node id : 93958104833888, Node type: identifier, Node text: b'pd'
                              Node id : 93958104834000, Node type: identifier, Node text: b'DataFrame'
                            Node id : 93958104835272, Node type: argument_list, Node text: b'(texts_to_dump, columns=fields_to_keep)'
                              Node id : 93958104834192, Node type: identifier, Node text: b'texts_to_dump'
                              Node id : 93958104834936, Node type: keyword_argument, Node text: b'columns=fields_to_keep'
                                Node id : 93958104834816, Node type: identifier, Node text: b'columns'
                                Node id : 93958104834384, Node type: identifier, Node text: b'fields_to_keep'
                      Node id : 93958104837584, Node type: expression_statement, Node text: b'df.to_parquet(output_file)'
                        Node id : 93958104836912, Node type: call, Node text: b'df.to_parquet(output_file)'
                          Node id : 93958104836304, Node type: attribute, Node text: b'df.to_parquet'
                            Node id : 93958104836096, Node type: identifier, Node text: b'df'
                            Node id : 93958104836208, Node type: identifier, Node text: b'to_parquet'
                          Node id : 93958104836808, Node type: argument_list, Node text: b'(output_file)'
                            Node id : 93958104836400, Node type: identifier, Node text: b'output_file'
                      Node id : 93958104868536, Node type: comment, Node text: b'# if the output file is too large, recursively split it in half'
                      Node id : 93958104868544, Node type: comment, Node text: b"# can't accurately predict parquet compression rate so this is necessary"
                      Node id : 93958104869984, Node type: function_definition, Node text: b'def split_file(output_file, df):\n                        if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()'
                        Node id : 93958104868392, Node type: identifier, Node text: b'split_file'
                        Node id : 93958104868400, Node type: parameters, Node text: b'(output_file, df)'
                          Node id : 93958104837808, Node type: identifier, Node text: b'output_file'
                          Node id : 93958104837904, Node type: identifier, Node text: b'df'
                        Node id : 93958104868424, Node type: block, Node text: b'if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2\n                        del df\n                        gc.collect()'
                          Node id : 93958104866928, Node type: if_statement, Node text: b'if os.path.getsize(output_file) > MAX_DUMP_SIZE:\n                            os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2'
                            Node id : 93958104840400, Node type: comparison_operator, Node text: b'os.path.getsize(output_file) > MAX_DUMP_SIZE'
                              Node id : 93958104839984, Node type: call, Node text: b'os.path.getsize(output_file)'
                                Node id : 93958104839232, Node type: attribute, Node text: b'os.path.getsize'
                                  Node id : 93958104839024, Node type: attribute, Node text: b'os.path'
                                    Node id : 93958104838336, Node type: identifier, Node text: b'os'
                                    Node id : 93958104838688, Node type: identifier, Node text: b'path'
                                  Node id : 93958104839136, Node type: identifier, Node text: b'getsize'
                                Node id : 93958104839880, Node type: argument_list, Node text: b'(output_file)'
                                  Node id : 93958104839328, Node type: identifier, Node text: b'output_file'
                              Node id : 93958104840080, Node type: identifier, Node text: b'MAX_DUMP_SIZE'
                            Node id : 93958104866320, Node type: block, Node text: b'os.remove(output_file)\n                            output_file_1 = f"{output_file[:-6]}_1.arrow"\n                            output_file_2 = f"{output_file[:-6]}_2.arrow"\n                            os.makedirs(os.path.dirname(output_file_1), exist_ok=True)\n                            os.makedirs(os.path.dirname(output_file_2), exist_ok=True)\n                            df1 = df[: df.shape[0] // 2]\n                            df2 = df[df.shape[0] // 2 :]\n                            df1.to_parquet(output_file_1)\n                            df2.to_parquet(output_file_2)\n                            split_file(output_file_1, df1)\n                            split_file(output_file_2, df2)\n                            del df1\n                            del df2'
                              Node id : 93958104842320, Node type: expression_statement, Node text: b'os.remove(output_file)'
                                Node id : 93958104842032, Node type: call, Node text: b'os.remove(output_file)'
                                  Node id : 93958104841184, Node type: attribute, Node text: b'os.remove'
                                    Node id : 93958104840496, Node type: identifier, Node text: b'os'
                                    Node id : 93958104840848, Node type: identifier, Node text: b'remove'
                                  Node id : 93958104841928, Node type: argument_list, Node text: b'(output_file)'
                                    Node id : 93958104841280, Node type: identifier, Node text: b'output_file'
                              Node id : 93958104846304, Node type: expression_statement, Node text: b'output_file_1 = f"{output_file[:-6]}_1.arrow"'
                                Node id : 93958104846208, Node type: assignment, Node text: b'output_file_1 = f"{output_file[:-6]}_1.arrow"'
                                  Node id : 93958104842432, Node type: identifier, Node text: b'output_file_1'
                                  Node id : 93958104845808, Node type: string, Node text: b'f"{output_file[:-6]}_1.arrow"'
                                    Node id : 93958104845696, Node type: string_start, Node text: b'f"'
                                    Node id : 93958104845488, Node type: interpolation, Node text: b'{output_file[:-6]}'
                                      Node id : 93958104844800, Node type: subscript, Node text: b'output_file[:-6]'
                                        Node id : 93958104842864, Node type: identifier, Node text: b'output_file'
                                        Node id : 93958104844688, Node type: slice, Node text: b':-6'
                                          Node id : 93958104844368, Node type: unary_operator, Node text: b'-6'
                                            Node id : 93958104843920, Node type: integer, Node text: b'6'
                                    Node id : 93958104845496, Node type: string_content, Node text: b'_1.arrow'
                                    Node id : 93958104845712, Node type: string_end, Node text: b'"'
                              Node id : 93958104848960, Node type: expression_statement, Node text: b'output_file_2 = f"{output_file[:-6]}_2.arrow"'
                                Node id : 93958104848864, Node type: assignment, Node text: b'output_file_2 = f"{output_file[:-6]}_2.arrow"'
                                  Node id : 93958104846528, Node type: identifier, Node text: b'output_file_2'
                                  Node id : 93958104848464, Node type: string, Node text: b'f"{output_file[:-6]}_2.arrow"'
                                    Node id : 93958104848352, Node type: string_start, Node text: b'f"'
                                    Node id : 93958104848144, Node type: interpolation, Node text: b'{output_file[:-6]}'
                                      Node id : 93958104847456, Node type: subscript, Node text: b'output_file[:-6]'
                                        Node id : 93958104846720, Node type: identifier, Node text: b'output_file'
                                        Node id : 93958104847344, Node type: slice, Node text: b':-6'
                                          Node id : 93958104847024, Node type: unary_operator, Node text: b'-6'
                                            Node id : 93958104846816, Node type: integer, Node text: b'6'
                                    Node id : 93958104848152, Node type: string_content, Node text: b'_2.arrow'
                                    Node id : 93958104848368, Node type: string_end, Node text: b'"'
                              Node id : 93958104851648, Node type: expression_statement, Node text: b'os.makedirs(os.path.dirname(output_file_1), exist_ok=True)'
                                Node id : 93958104851360, Node type: call, Node text: b'os.makedirs(os.path.dirname(output_file_1), exist_ok=True)'
                                  Node id : 93958104849392, Node type: attribute, Node text: b'os.makedirs'
                                    Node id : 93958104849184, Node type: identifier, Node text: b'os'
                                    Node id : 93958104849296, Node type: identifier, Node text: b'makedirs'
                                  Node id : 93958104851256, Node type: argument_list, Node text: b'(os.path.dirname(output_file_1), exist_ok=True)'
                                    Node id : 93958104850416, Node type: call, Node text: b'os.path.dirname(output_file_1)'
                                      Node id : 93958104849904, Node type: attribute, Node text: b'os.path.dirname'
                                        Node id : 93958104849696, Node type: attribute, Node text: b'os.path'
                                          Node id : 93958104849488, Node type: identifier, Node text: b'os'
                                          Node id : 93958104849600, Node type: identifier, Node text: b'path'
                                        Node id : 93958104849808, Node type: identifier, Node text: b'dirname'
                                      Node id : 93958104850312, Node type: argument_list, Node text: b'(output_file_1)'
                                        Node id : 93958104850000, Node type: identifier, Node text: b'output_file_1'
                                    Node id : 93958104850920, Node type: keyword_argument, Node text: b'exist_ok=True'
                                      Node id : 93958104850800, Node type: identifier, Node text: b'exist_ok'
                                      Node id : 93958104850608, Node type: true, Node text: b'True'
                              Node id : 93958104854336, Node type: expression_statement, Node text: b'os.makedirs(os.path.dirname(output_file_2), exist_ok=True)'
                                Node id : 93958104854048, Node type: call, Node text: b'os.makedirs(os.path.dirname(output_file_2), exist_ok=True)'
                                  Node id : 93958104852080, Node type: attribute, Node text: b'os.makedirs'
                                    Node id : 93958104851872, Node type: identifier, Node text: b'os'
                                    Node id : 93958104851984, Node type: identifier, Node text: b'makedirs'
                                  Node id : 93958104853944, Node type: argument_list, Node text: b'(os.path.dirname(output_file_2), exist_ok=True)'
                                    Node id : 93958104853104, Node type: call, Node text: b'os.path.dirname(output_file_2)'
                                      Node id : 93958104852592, Node type: attribute, Node text: b'os.path.dirname'
                                        Node id : 93958104852384, Node type: attribute, Node text: b'os.path'
                                          Node id : 93958104852176, Node type: identifier, Node text: b'os'
                                          Node id : 93958104852288, Node type: identifier, Node text: b'path'
                                        Node id : 93958104852496, Node type: identifier, Node text: b'dirname'
                                      Node id : 93958104853000, Node type: argument_list, Node text: b'(output_file_2)'
                                        Node id : 93958104852688, Node type: identifier, Node text: b'output_file_2'
                                    Node id : 93958104853608, Node type: keyword_argument, Node text: b'exist_ok=True'
                                      Node id : 93958104853488, Node type: identifier, Node text: b'exist_ok'
                                      Node id : 93958104853296, Node type: true, Node text: b'True'
                              Node id : 93958104856704, Node type: expression_statement, Node text: b'df1 = df[: df.shape[0] // 2]'
                                Node id : 93958104856608, Node type: assignment, Node text: b'df1 = df[: df.shape[0] // 2]'
                                  Node id : 93958104854560, Node type: identifier, Node text: b'df1'
                                  Node id : 93958104856208, Node type: subscript, Node text: b'df[: df.shape[0] // 2]'
                                    Node id : 93958104854656, Node type: identifier, Node text: b'df'
                                    Node id : 93958104856096, Node type: slice, Node text: b': df.shape[0] // 2'
                                      Node id : 93958104855680, Node type: binary_operator, Node text: b'df.shape[0] // 2'
                                        Node id : 93958104855376, Node type: subscript, Node text: b'df.shape[0]'
                                          Node id : 93958104854960, Node type: attribute, Node text: b'df.shape'
                                            Node id : 93958104854752, Node type: identifier, Node text: b'df'
                                            Node id : 93958104854864, Node type: identifier, Node text: b'shape'
                                          Node id : 93958104855056, Node type: integer, Node text: b'0'
                                        Node id : 93958104855472, Node type: integer, Node text: b'2'
                              Node id : 93958104859072, Node type: expression_statement, Node text: b'df2 = df[df.shape[0] // 2 :]'
                                Node id : 93958104858976, Node type: assignment, Node text: b'df2 = df[df.shape[0] // 2 :]'
                                  Node id : 93958104856928, Node type: identifier, Node text: b'df2'
                                  Node id : 93958104858576, Node type: subscript, Node text: b'df[df.shape[0] // 2 :]'
                                    Node id : 93958104857024, Node type: identifier, Node text: b'df'
                                    Node id : 93958104858464, Node type: slice, Node text: b'df.shape[0] // 2 :'
                                      Node id : 93958104858048, Node type: binary_operator, Node text: b'df.shape[0] // 2'
                                        Node id : 93958104857744, Node type: subscript, Node text: b'df.shape[0]'
                                          Node id : 93958104857328, Node type: attribute, Node text: b'df.shape'
                                            Node id : 93958104857120, Node type: identifier, Node text: b'df'
                                            Node id : 93958104857232, Node type: identifier, Node text: b'shape'
                                          Node id : 93958104857424, Node type: integer, Node text: b'0'
                                        Node id : 93958104857840, Node type: integer, Node text: b'2'
                              Node id : 93958104860400, Node type: expression_statement, Node text: b'df1.to_parquet(output_file_1)'
                                Node id : 93958104860112, Node type: call, Node text: b'df1.to_parquet(output_file_1)'
                                  Node id : 93958104859504, Node type: attribute, Node text: b'df1.to_parquet'
                                    Node id : 93958104859296, Node type: identifier, Node text: b'df1'
                                    Node id : 93958104859408, Node type: identifier, Node text: b'to_parquet'
                                  Node id : 93958104860008, Node type: argument_list, Node text: b'(output_file_1)'
                                    Node id : 93958104859600, Node type: identifier, Node text: b'output_file_1'
                              Node id : 93958104861728, Node type: expression_statement, Node text: b'df2.to_parquet(output_file_2)'
                                Node id : 93958104861440, Node type: call, Node text: b'df2.to_parquet(output_file_2)'
                                  Node id : 93958104860832, Node type: attribute, Node text: b'df2.to_parquet'
                                    Node id : 93958104860624, Node type: identifier, Node text: b'df2'
                                    Node id : 93958104860736, Node type: identifier, Node text: b'to_parquet'
                                  Node id : 93958104861336, Node type: argument_list, Node text: b'(output_file_2)'
                                    Node id : 93958104860928, Node type: identifier, Node text: b'output_file_2'
                              Node id : 93958104863168, Node type: expression_statement, Node text: b'split_file(output_file_1, df1)'
                                Node id : 93958104862880, Node type: call, Node text: b'split_file(output_file_1, df1)'
                                  Node id : 93958104861952, Node type: identifier, Node text: b'split_file'
                                  Node id : 93958104862776, Node type: argument_list, Node text: b'(output_file_1, df1)'
                                    Node id : 93958104862048, Node type: identifier, Node text: b'output_file_1'
                                    Node id : 93958104862240, Node type: identifier, Node text: b'df1'
                              Node id : 93958104864608, Node type: expression_statement, Node text: b'split_file(output_file_2, df2)'
                                Node id : 93958104864320, Node type: call, Node text: b'split_file(output_file_2, df2)'
                                  Node id : 93958104863392, Node type: identifier, Node text: b'split_file'
                                  Node id : 93958104864216, Node type: argument_list, Node text: b'(output_file_2, df2)'
                                    Node id : 93958104863488, Node type: identifier, Node text: b'output_file_2'
                                    Node id : 93958104863680, Node type: identifier, Node text: b'df2'
                              Node id : 93958104865232, Node type: delete_statement, Node text: b'del df1'
                                Node id : 93958104864928, Node type: identifier, Node text: b'df1'
                              Node id : 93958104865952, Node type: delete_statement, Node text: b'del df2'
                                Node id : 93958104865552, Node type: identifier, Node text: b'df2'
                          Node id : 93958104866816, Node type: delete_statement, Node text: b'del df'
                            Node id : 93958104866512, Node type: identifier, Node text: b'df'
                          Node id : 93958104868048, Node type: expression_statement, Node text: b'gc.collect()'
                            Node id : 93958104867664, Node type: call, Node text: b'gc.collect()'
                              Node id : 93958104867248, Node type: attribute, Node text: b'gc.collect'
                                Node id : 93958104867040, Node type: identifier, Node text: b'gc'
                                Node id : 93958104867152, Node type: identifier, Node text: b'collect'
                              Node id : 93958104867560, Node type: argument_list, Node text: b'()'
                      Node id : 93958104869872, Node type: expression_statement, Node text: b'split_file(output_file, df)'
                        Node id : 93958104869584, Node type: call, Node text: b'split_file(output_file, df)'
                          Node id : 93958104868656, Node type: identifier, Node text: b'split_file'
                          Node id : 93958104869480, Node type: argument_list, Node text: b'(output_file, df)'
                            Node id : 93958104868752, Node type: identifier, Node text: b'output_file'
                            Node id : 93958104868944, Node type: identifier, Node text: b'df'
                      Node id : 93958104870896, Node type: expression_statement, Node text: b'texts_to_dump = []'
                        Node id : 93958104870800, Node type: assignment, Node text: b'texts_to_dump = []'
                          Node id : 93958104870096, Node type: identifier, Node text: b'texts_to_dump'
                          Node id : 93958104870400, Node type: list, Node text: b'[]'
                      Node id : 93958104872376, Node type: if_statement, Node text: b'if current_tokens >= num_tokens:\n                        break'
                        Node id : 93958104871536, Node type: comparison_operator, Node text: b'current_tokens >= num_tokens'
                          Node id : 93958104871120, Node type: identifier, Node text: b'current_tokens'
                          Node id : 93958104871216, Node type: identifier, Node text: b'num_tokens'
                        Node id : 93958104872272, Node type: block, Node text: b'break'
                          Node id : 93958104872016, Node type: break_statement, Node text: b'break'
                      Node id : 93958104873832, Node type: if_statement, Node text: b'if current_tokens >= num_tokens:\n                        break'
                        Node id : 93958104872896, Node type: comparison_operator, Node text: b'current_tokens >= num_tokens'
                          Node id : 93958104872480, Node type: identifier, Node text: b'current_tokens'
                          Node id : 93958104872576, Node type: identifier, Node text: b'num_tokens'
                        Node id : 93958104873728, Node type: block, Node text: b'break'
                          Node id : 93958104873376, Node type: break_statement, Node text: b'break'
      Node id : 93958104878064, Node type: expression_statement, Node text: b'logging.info(f"Saved all output ({current_tokens} tokens)")'
        Node id : 93958104877680, Node type: call, Node text: b'logging.info(f"Saved all output ({current_tokens} tokens)")'
          Node id : 93958104875760, Node type: attribute, Node text: b'logging.info'
            Node id : 93958104875552, Node type: identifier, Node text: b'logging'
            Node id : 93958104875664, Node type: identifier, Node text: b'info'
          Node id : 93958104877576, Node type: argument_list, Node text: b'(f"Saved all output ({current_tokens} tokens)")'
            Node id : 93958104877168, Node type: string, Node text: b'f"Saved all output ({current_tokens} tokens)"'
              Node id : 93958104877056, Node type: string_start, Node text: b'f"'
              Node id : 93958104876640, Node type: string_content, Node text: b'Saved all output ('
              Node id : 93958104876648, Node type: interpolation, Node text: b'{current_tokens}'
                Node id : 93958104876144, Node type: identifier, Node text: b'current_tokens'
              Node id : 93958104876952, Node type: string_content, Node text: b' tokens)'
              Node id : 93958104877072, Node type: string_end, Node text: b'"'
  Node id : 93958104956360, Node type: if_statement, Node text: b'if __name__ == "__main__":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--num_tokens",\n        help="Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )\n    parser.add_argument(\n        "--num_total_tokens",\n        help="Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )\n    parser.add_argument("--output", help="Output dir", type=str)\n    parser.add_argument(\n        "--domain",\n        help="Domains to fetch",\n        type=str,\n        choices=[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ],\n    )\n    parser.add_argument("--seed", help="Random seed", type=int, default=42)\n    args = parser.parse_args()\n    logging.basicConfig(level=logging.INFO)\n\n    if args.num_tokens and args.num_total_tokens:\n        raise ValueError(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )\n\n    if args.num_tokens:\n        logging.info(f"Fetching {args.num_tokens} tokens")\n        num_tokens = parse_num(args.num_tokens)\n    # Calculate num_tokens from domain and num_total_tokens\n    elif args.num_total_tokens and args.domain:\n        logging.info("Total domain tokens not specified, using 10B ratio mix")\n        num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000\n    # Calculate num_tokens from domain and num_total_tokens=10B\n    elif args.domain:\n        logging.info("Total tokens/domain tokens not specified, using 10B mix")\n        num_tokens = TOKENS_TO_FETCH_10B[args.domain]\n\n    # the flask server has to be up on clio\n    all_files_lst = requests.get("http://128.2.209.71:5000/list-all").json()\n    if args.domain:\n        fetch_tokens(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )\n    else:\n        logging.info("Fetching from all domains following the 10B ratio mix")\n        for domain in TOKENS_TO_FETCH_10B.keys():\n            logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
    Node id : 93958104879568, Node type: comparison_operator, Node text: b'__name__ == "__main__"'
      Node id : 93958104878656, Node type: identifier, Node text: b'__name__'
      Node id : 93958104879248, Node type: string, Node text: b'"__main__"'
        Node id : 93958104879136, Node type: string_start, Node text: b'"'
        Node id : 93958104879144, Node type: string_content, Node text: b'__main__'
        Node id : 93958104879152, Node type: string_end, Node text: b'"'
    Node id : 93958104956256, Node type: block, Node text: b'parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--num_tokens",\n        help="Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )\n    parser.add_argument(\n        "--num_total_tokens",\n        help="Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )\n    parser.add_argument("--output", help="Output dir", type=str)\n    parser.add_argument(\n        "--domain",\n        help="Domains to fetch",\n        type=str,\n        choices=[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ],\n    )\n    parser.add_argument("--seed", help="Random seed", type=int, default=42)\n    args = parser.parse_args()\n    logging.basicConfig(level=logging.INFO)\n\n    if args.num_tokens and args.num_total_tokens:\n        raise ValueError(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )\n\n    if args.num_tokens:\n        logging.info(f"Fetching {args.num_tokens} tokens")\n        num_tokens = parse_num(args.num_tokens)\n    # Calculate num_tokens from domain and num_total_tokens\n    elif args.num_total_tokens and args.domain:\n        logging.info("Total domain tokens not specified, using 10B ratio mix")\n        num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000\n    # Calculate num_tokens from domain and num_total_tokens=10B\n    elif args.domain:\n        logging.info("Total tokens/domain tokens not specified, using 10B mix")\n        num_tokens = TOKENS_TO_FETCH_10B[args.domain]\n\n    # the flask server has to be up on clio\n    all_files_lst = requests.get("http://128.2.209.71:5000/list-all").json()\n    if args.domain:\n        fetch_tokens(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )\n    else:\n        logging.info("Fetching from all domains following the 10B ratio mix")\n        for domain in TOKENS_TO_FETCH_10B.keys():\n            logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
      Node id : 93958104880976, Node type: expression_statement, Node text: b'parser = argparse.ArgumentParser()'
        Node id : 93958104880880, Node type: assignment, Node text: b'parser = argparse.ArgumentParser()'
          Node id : 93958104879760, Node type: identifier, Node text: b'parser'
          Node id : 93958104880480, Node type: call, Node text: b'argparse.ArgumentParser()'
            Node id : 93958104880064, Node type: attribute, Node text: b'argparse.ArgumentParser'
              Node id : 93958104879856, Node type: identifier, Node text: b'argparse'
              Node id : 93958104879968, Node type: identifier, Node text: b'ArgumentParser'
            Node id : 93958104880376, Node type: argument_list, Node text: b'()'
      Node id : 93958104884144, Node type: expression_statement, Node text: b'parser.add_argument(\n        "--num_tokens",\n        help="Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )'
        Node id : 93958104883856, Node type: call, Node text: b'parser.add_argument(\n        "--num_tokens",\n        help="Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )'
          Node id : 93958104881296, Node type: attribute, Node text: b'parser.add_argument'
            Node id : 93958104881088, Node type: identifier, Node text: b'parser'
            Node id : 93958104881200, Node type: identifier, Node text: b'add_argument'
          Node id : 93958104883752, Node type: argument_list, Node text: b'(\n        "--num_tokens",\n        help="Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )'
            Node id : 93958104881888, Node type: string, Node text: b'"--num_tokens"'
              Node id : 93958104881776, Node type: string_start, Node text: b'"'
              Node id : 93958104881784, Node type: string_content, Node text: b'--num_tokens'
              Node id : 93958104881792, Node type: string_end, Node text: b'"'
            Node id : 93958104882888, Node type: keyword_argument, Node text: b'help="Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions"'
              Node id : 93958104882768, Node type: identifier, Node text: b'help'
              Node id : 93958104882576, Node type: string, Node text: b'"Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions"'
                Node id : 93958104882464, Node type: string_start, Node text: b'"'
                Node id : 93958104882472, Node type: string_content, Node text: b'Number of tokens to fetch. You can also write xB/xM to fetch x billions/millions'
                Node id : 93958104882480, Node type: string_end, Node text: b'"'
            Node id : 93958104883304, Node type: keyword_argument, Node text: b'type=str'
              Node id : 93958104883184, Node type: identifier, Node text: b'type'
              Node id : 93958104882992, Node type: identifier, Node text: b'str'
      Node id : 93958104887424, Node type: expression_statement, Node text: b'parser.add_argument(\n        "--num_total_tokens",\n        help="Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )'
        Node id : 93958104887136, Node type: call, Node text: b'parser.add_argument(\n        "--num_total_tokens",\n        help="Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )'
          Node id : 93958104884576, Node type: attribute, Node text: b'parser.add_argument'
            Node id : 93958104884368, Node type: identifier, Node text: b'parser'
            Node id : 93958104884480, Node type: identifier, Node text: b'add_argument'
          Node id : 93958104887032, Node type: argument_list, Node text: b'(\n        "--num_total_tokens",\n        help="Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions",\n        type=str,\n    )'
            Node id : 93958104885168, Node type: string, Node text: b'"--num_total_tokens"'
              Node id : 93958104885056, Node type: string_start, Node text: b'"'
              Node id : 93958104885064, Node type: string_content, Node text: b'--num_total_tokens'
              Node id : 93958104885072, Node type: string_end, Node text: b'"'
            Node id : 93958104886168, Node type: keyword_argument, Node text: b'help="Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions"'
              Node id : 93958104886048, Node type: identifier, Node text: b'help'
              Node id : 93958104885856, Node type: string, Node text: b'"Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions"'
                Node id : 93958104885744, Node type: string_start, Node text: b'"'
                Node id : 93958104885752, Node type: string_content, Node text: b'Total number of tokens to fetch. You can also write xB/xM to fetch x billions/millions'
                Node id : 93958104885760, Node type: string_end, Node text: b'"'
            Node id : 93958104886584, Node type: keyword_argument, Node text: b'type=str'
              Node id : 93958104886464, Node type: identifier, Node text: b'type'
              Node id : 93958104886272, Node type: identifier, Node text: b'str'
      Node id : 93958104890704, Node type: expression_statement, Node text: b'parser.add_argument("--output", help="Output dir", type=str)'
        Node id : 93958104890416, Node type: call, Node text: b'parser.add_argument("--output", help="Output dir", type=str)'
          Node id : 93958104887856, Node type: attribute, Node text: b'parser.add_argument'
            Node id : 93958104887648, Node type: identifier, Node text: b'parser'
            Node id : 93958104887760, Node type: identifier, Node text: b'add_argument'
          Node id : 93958104890312, Node type: argument_list, Node text: b'("--output", help="Output dir", type=str)'
            Node id : 93958104888448, Node type: string, Node text: b'"--output"'
              Node id : 93958104888336, Node type: string_start, Node text: b'"'
              Node id : 93958104888344, Node type: string_content, Node text: b'--output'
              Node id : 93958104888352, Node type: string_end, Node text: b'"'
            Node id : 93958104889448, Node type: keyword_argument, Node text: b'help="Output dir"'
              Node id : 93958104889328, Node type: identifier, Node text: b'help'
              Node id : 93958104889136, Node type: string, Node text: b'"Output dir"'
                Node id : 93958104889024, Node type: string_start, Node text: b'"'
                Node id : 93958104889032, Node type: string_content, Node text: b'Output dir'
                Node id : 93958104889040, Node type: string_end, Node text: b'"'
            Node id : 93958104889864, Node type: keyword_argument, Node text: b'type=str'
              Node id : 93958104889744, Node type: identifier, Node text: b'type'
              Node id : 93958104889552, Node type: identifier, Node text: b'str'
      Node id : 93958104899872, Node type: expression_statement, Node text: b'parser.add_argument(\n        "--domain",\n        help="Domains to fetch",\n        type=str,\n        choices=[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ],\n    )'
        Node id : 93958104899584, Node type: call, Node text: b'parser.add_argument(\n        "--domain",\n        help="Domains to fetch",\n        type=str,\n        choices=[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ],\n    )'
          Node id : 93958104891136, Node type: attribute, Node text: b'parser.add_argument'
            Node id : 93958104890928, Node type: identifier, Node text: b'parser'
            Node id : 93958104891040, Node type: identifier, Node text: b'add_argument'
          Node id : 93958104899480, Node type: argument_list, Node text: b'(\n        "--domain",\n        help="Domains to fetch",\n        type=str,\n        choices=[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ],\n    )'
            Node id : 93958104891728, Node type: string, Node text: b'"--domain"'
              Node id : 93958104891616, Node type: string_start, Node text: b'"'
              Node id : 93958104891624, Node type: string_content, Node text: b'--domain'
              Node id : 93958104891632, Node type: string_end, Node text: b'"'
            Node id : 93958104892728, Node type: keyword_argument, Node text: b'help="Domains to fetch"'
              Node id : 93958104892608, Node type: identifier, Node text: b'help'
              Node id : 93958104892416, Node type: string, Node text: b'"Domains to fetch"'
                Node id : 93958104892304, Node type: string_start, Node text: b'"'
                Node id : 93958104892312, Node type: string_content, Node text: b'Domains to fetch'
                Node id : 93958104892320, Node type: string_end, Node text: b'"'
            Node id : 93958104893144, Node type: keyword_argument, Node text: b'type=str'
              Node id : 93958104893024, Node type: identifier, Node text: b'type'
              Node id : 93958104892832, Node type: identifier, Node text: b'str'
            Node id : 93958104899032, Node type: keyword_argument, Node text: b'choices=[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ]'
              Node id : 93958104898912, Node type: identifier, Node text: b'choices'
              Node id : 93958104898720, Node type: list, Node text: b'[\n            "peS2o",\n            "common-crawl",\n            "stack-code",\n            "wiki-en-simple",\n            "c4",\n            "gutenberg-books",\n        ]'
                Node id : 93958104893856, Node type: string, Node text: b'"peS2o"'
                  Node id : 93958104893744, Node type: string_start, Node text: b'"'
                  Node id : 93958104893752, Node type: string_content, Node text: b'peS2o'
                  Node id : 93958104893760, Node type: string_end, Node text: b'"'
                Node id : 93958104894544, Node type: string, Node text: b'"common-crawl"'
                  Node id : 93958104894432, Node type: string_start, Node text: b'"'
                  Node id : 93958104894440, Node type: string_content, Node text: b'common-crawl'
                  Node id : 93958104894448, Node type: string_end, Node text: b'"'
                Node id : 93958104895344, Node type: string, Node text: b'"stack-code"'
                  Node id : 93958104895232, Node type: string_start, Node text: b'"'
                  Node id : 93958104895240, Node type: string_content, Node text: b'stack-code'
                  Node id : 93958104895248, Node type: string_end, Node text: b'"'
                Node id : 93958104896256, Node type: string, Node text: b'"wiki-en-simple"'
                  Node id : 93958104896144, Node type: string_start, Node text: b'"'
                  Node id : 93958104896152, Node type: string_content, Node text: b'wiki-en-simple'
                  Node id : 93958104896160, Node type: string_end, Node text: b'"'
                Node id : 93958104897168, Node type: string, Node text: b'"c4"'
                  Node id : 93958104897056, Node type: string_start, Node text: b'"'
                  Node id : 93958104897064, Node type: string_content, Node text: b'c4'
                  Node id : 93958104897072, Node type: string_end, Node text: b'"'
                Node id : 93958104898080, Node type: string, Node text: b'"gutenberg-books"'
                  Node id : 93958104897968, Node type: string_start, Node text: b'"'
                  Node id : 93958104897976, Node type: string_content, Node text: b'gutenberg-books'
                  Node id : 93958104897984, Node type: string_end, Node text: b'"'
      Node id : 93958104903680, Node type: expression_statement, Node text: b'parser.add_argument("--seed", help="Random seed", type=int, default=42)'
        Node id : 93958104903392, Node type: call, Node text: b'parser.add_argument("--seed", help="Random seed", type=int, default=42)'
          Node id : 93958104900304, Node type: attribute, Node text: b'parser.add_argument'
            Node id : 93958104900096, Node type: identifier, Node text: b'parser'
            Node id : 93958104900208, Node type: identifier, Node text: b'add_argument'
          Node id : 93958104903288, Node type: argument_list, Node text: b'("--seed", help="Random seed", type=int, default=42)'
            Node id : 93958104900896, Node type: string, Node text: b'"--seed"'
              Node id : 93958104900784, Node type: string_start, Node text: b'"'
              Node id : 93958104900792, Node type: string_content, Node text: b'--seed'
              Node id : 93958104900800, Node type: string_end, Node text: b'"'
            Node id : 93958104901896, Node type: keyword_argument, Node text: b'help="Random seed"'
              Node id : 93958104901776, Node type: identifier, Node text: b'help'
              Node id : 93958104901584, Node type: string, Node text: b'"Random seed"'
                Node id : 93958104901472, Node type: string_start, Node text: b'"'
                Node id : 93958104901480, Node type: string_content, Node text: b'Random seed'
                Node id : 93958104901488, Node type: string_end, Node text: b'"'
            Node id : 93958104902312, Node type: keyword_argument, Node text: b'type=int'
              Node id : 93958104902192, Node type: identifier, Node text: b'type'
              Node id : 93958104902000, Node type: identifier, Node text: b'int'
            Node id : 93958104902840, Node type: keyword_argument, Node text: b'default=42'
              Node id : 93958104902720, Node type: identifier, Node text: b'default'
              Node id : 93958104902528, Node type: integer, Node text: b'42'
      Node id : 93958104905120, Node type: expression_statement, Node text: b'args = parser.parse_args()'
        Node id : 93958104905024, Node type: assignment, Node text: b'args = parser.parse_args()'
          Node id : 93958104903904, Node type: identifier, Node text: b'args'
          Node id : 93958104904624, Node type: call, Node text: b'parser.parse_args()'
            Node id : 93958104904208, Node type: attribute, Node text: b'parser.parse_args'
              Node id : 93958104904000, Node type: identifier, Node text: b'parser'
              Node id : 93958104904112, Node type: identifier, Node text: b'parse_args'
            Node id : 93958104904520, Node type: argument_list, Node text: b'()'
      Node id : 93958104906768, Node type: expression_statement, Node text: b'logging.basicConfig(level=logging.INFO)'
        Node id : 93958104906480, Node type: call, Node text: b'logging.basicConfig(level=logging.INFO)'
          Node id : 93958104905552, Node type: attribute, Node text: b'logging.basicConfig'
            Node id : 93958104905344, Node type: identifier, Node text: b'logging'
            Node id : 93958104905456, Node type: identifier, Node text: b'basicConfig'
          Node id : 93958104906376, Node type: argument_list, Node text: b'(level=logging.INFO)'
            Node id : 93958104906264, Node type: keyword_argument, Node text: b'level=logging.INFO'
              Node id : 93958104906048, Node type: identifier, Node text: b'level'
              Node id : 93958104905856, Node type: attribute, Node text: b'logging.INFO'
                Node id : 93958104905648, Node type: identifier, Node text: b'logging'
                Node id : 93958104905760, Node type: identifier, Node text: b'INFO'
      Node id : 93958104909960, Node type: if_statement, Node text: b'if args.num_tokens and args.num_total_tokens:\n        raise ValueError(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )'
        Node id : 93958104907904, Node type: boolean_operator, Node text: b'args.num_tokens and args.num_total_tokens'
          Node id : 93958104907200, Node type: attribute, Node text: b'args.num_tokens'
            Node id : 93958104906992, Node type: identifier, Node text: b'args'
            Node id : 93958104907104, Node type: identifier, Node text: b'num_tokens'
          Node id : 93958104907600, Node type: attribute, Node text: b'args.num_total_tokens'
            Node id : 93958104907392, Node type: identifier, Node text: b'args'
            Node id : 93958104907504, Node type: identifier, Node text: b'num_total_tokens'
        Node id : 93958104909856, Node type: block, Node text: b'raise ValueError(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )'
          Node id : 93958104909600, Node type: raise_statement, Node text: b'raise ValueError(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )'
            Node id : 93958104909200, Node type: call, Node text: b'ValueError(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )'
              Node id : 93958104908096, Node type: identifier, Node text: b'ValueError'
              Node id : 93958104909096, Node type: argument_list, Node text: b'(\n            "Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"\n        )'
                Node id : 93958104908688, Node type: string, Node text: b'"Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)"'
                  Node id : 93958104908576, Node type: string_start, Node text: b'"'
                  Node id : 93958104908584, Node type: string_content, Node text: b'Please specify only one of --num_tokens or --num_total_tokens (num_tokens is per domain, while num_total will calculate the appropriate number for a domain based on the total)'
                  Node id : 93958104908592, Node type: string_end, Node text: b'"'
      Node id : 93958104930368, Node type: if_statement, Node text: b'if args.num_tokens:\n        logging.info(f"Fetching {args.num_tokens} tokens")\n        num_tokens = parse_num(args.num_tokens)\n    # Calculate num_tokens from domain and num_total_tokens\n    elif args.num_total_tokens and args.domain:\n        logging.info("Total domain tokens not specified, using 10B ratio mix")\n        num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000\n    # Calculate num_tokens from domain and num_total_tokens=10B\n    elif args.domain:\n        logging.info("Total tokens/domain tokens not specified, using 10B mix")\n        num_tokens = TOKENS_TO_FETCH_10B[args.domain]'
        Node id : 93958104910272, Node type: attribute, Node text: b'args.num_tokens'
          Node id : 93958104910064, Node type: identifier, Node text: b'args'
          Node id : 93958104910176, Node type: identifier, Node text: b'num_tokens'
        Node id : 93958104927600, Node type: block, Node text: b'logging.info(f"Fetching {args.num_tokens} tokens")\n        num_tokens = parse_num(args.num_tokens)'
          Node id : 93958104913184, Node type: expression_statement, Node text: b'logging.info(f"Fetching {args.num_tokens} tokens")'
            Node id : 93958104912896, Node type: call, Node text: b'logging.info(f"Fetching {args.num_tokens} tokens")'
              Node id : 93958104910768, Node type: attribute, Node text: b'logging.info'
                Node id : 93958104910560, Node type: identifier, Node text: b'logging'
                Node id : 93958104910672, Node type: identifier, Node text: b'info'
              Node id : 93958104912792, Node type: argument_list, Node text: b'(f"Fetching {args.num_tokens} tokens")'
                Node id : 93958104912384, Node type: string, Node text: b'f"Fetching {args.num_tokens} tokens"'
                  Node id : 93958104912272, Node type: string_start, Node text: b'f"'
                  Node id : 93958104911856, Node type: string_content, Node text: b'Fetching '
                  Node id : 93958104911864, Node type: interpolation, Node text: b'{args.num_tokens}'
                    Node id : 93958104911360, Node type: attribute, Node text: b'args.num_tokens'
                      Node id : 93958104911152, Node type: identifier, Node text: b'args'
                      Node id : 93958104911264, Node type: identifier, Node text: b'num_tokens'
                  Node id : 93958104912168, Node type: string_content, Node text: b' tokens'
                  Node id : 93958104912288, Node type: string_end, Node text: b'"'
          Node id : 93958104914800, Node type: expression_statement, Node text: b'num_tokens = parse_num(args.num_tokens)'
            Node id : 93958104914608, Node type: assignment, Node text: b'num_tokens = parse_num(args.num_tokens)'
              Node id : 93958104913296, Node type: identifier, Node text: b'num_tokens'
              Node id : 93958104914208, Node type: call, Node text: b'parse_num(args.num_tokens)'
                Node id : 93958104913392, Node type: identifier, Node text: b'parse_num'
                Node id : 93958104914104, Node type: argument_list, Node text: b'(args.num_tokens)'
                  Node id : 93958104913696, Node type: attribute, Node text: b'args.num_tokens'
                    Node id : 93958104913488, Node type: identifier, Node text: b'args'
                    Node id : 93958104913600, Node type: identifier, Node text: b'num_tokens'
        Node id : 93958104927608, Node type: comment, Node text: b'# Calculate num_tokens from domain and num_total_tokens'
        Node id : 93958104923184, Node type: elif_clause, Node text: b'elif args.num_total_tokens and args.domain:\n        logging.info("Total domain tokens not specified, using 10B ratio mix")\n        num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000'
          Node id : 93958104916048, Node type: boolean_operator, Node text: b'args.num_total_tokens and args.domain'
            Node id : 93958104915344, Node type: attribute, Node text: b'args.num_total_tokens'
              Node id : 93958104915136, Node type: identifier, Node text: b'args'
              Node id : 93958104915248, Node type: identifier, Node text: b'num_total_tokens'
            Node id : 93958104915744, Node type: attribute, Node text: b'args.domain'
              Node id : 93958104915536, Node type: identifier, Node text: b'args'
              Node id : 93958104915648, Node type: identifier, Node text: b'domain'
          Node id : 93958104923088, Node type: block, Node text: b'logging.info("Total domain tokens not specified, using 10B ratio mix")\n        num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000'
            Node id : 93958104917840, Node type: expression_statement, Node text: b'logging.info("Total domain tokens not specified, using 10B ratio mix")'
              Node id : 93958104917552, Node type: call, Node text: b'logging.info("Total domain tokens not specified, using 10B ratio mix")'
                Node id : 93958104916448, Node type: attribute, Node text: b'logging.info'
                  Node id : 93958104916240, Node type: identifier, Node text: b'logging'
                  Node id : 93958104916352, Node type: identifier, Node text: b'info'
                Node id : 93958104917448, Node type: argument_list, Node text: b'("Total domain tokens not specified, using 10B ratio mix")'
                  Node id : 93958104917040, Node type: string, Node text: b'"Total domain tokens not specified, using 10B ratio mix"'
                    Node id : 93958104916928, Node type: string_start, Node text: b'"'
                    Node id : 93958104916936, Node type: string_content, Node text: b'Total domain tokens not specified, using 10B ratio mix'
                    Node id : 93958104916944, Node type: string_end, Node text: b'"'
            Node id : 93958104922720, Node type: expression_statement, Node text: b'num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000'
              Node id : 93958104922528, Node type: assignment, Node text: b'num_tokens = (\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000'
                Node id : 93958104917952, Node type: identifier, Node text: b'num_tokens'
                Node id : 93958104922128, Node type: binary_operator, Node text: b'(\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        ) * 1_000_000'
                  Node id : 93958104921728, Node type: parenthesized_expression, Node text: b'(\n            int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000\n        )'
                    Node id : 93958104921424, Node type: binary_operator, Node text: b'int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )\n            // 1_000_000'
                      Node id : 93958104921120, Node type: call, Node text: b'int(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )'
                        Node id : 93958104918048, Node type: identifier, Node text: b'int'
                        Node id : 93958104921016, Node type: argument_list, Node text: b'(\n                (\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )\n            )'
                          Node id : 93958104920704, Node type: parenthesized_expression, Node text: b'(\n                    (parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]\n                )'
                            Node id : 93958104920400, Node type: binary_operator, Node text: b'(parse_num(args.num_total_tokens) / 10_000_000_000)\n                    * TOKENS_TO_FETCH_10B[args.domain]'
                              Node id : 93958104919472, Node type: parenthesized_expression, Node text: b'(parse_num(args.num_total_tokens) / 10_000_000_000)'
                                Node id : 93958104919168, Node type: binary_operator, Node text: b'parse_num(args.num_total_tokens) / 10_000_000_000'
                                  Node id : 93958104918864, Node type: call, Node text: b'parse_num(args.num_total_tokens)'
                                    Node id : 93958104918144, Node type: identifier, Node text: b'parse_num'
                                    Node id : 93958104918760, Node type: argument_list, Node text: b'(args.num_total_tokens)'
                                      Node id : 93958104918448, Node type: attribute, Node text: b'args.num_total_tokens'
                                        Node id : 93958104918240, Node type: identifier, Node text: b'args'
                                        Node id : 93958104918352, Node type: identifier, Node text: b'num_total_tokens'
                                  Node id : 93958104918960, Node type: integer, Node text: b'10_000_000_000'
                              Node id : 93958104920192, Node type: subscript, Node text: b'TOKENS_TO_FETCH_10B[args.domain]'
                                Node id : 93958104919568, Node type: identifier, Node text: b'TOKENS_TO_FETCH_10B'
                                Node id : 93958104919872, Node type: attribute, Node text: b'args.domain'
                                  Node id : 93958104919664, Node type: identifier, Node text: b'args'
                                  Node id : 93958104919776, Node type: identifier, Node text: b'domain'
                      Node id : 93958104921216, Node type: integer, Node text: b'1_000_000'
                  Node id : 93958104921920, Node type: integer, Node text: b'1_000_000'
        Node id : 93958104927464, Node type: comment, Node text: b'# Calculate num_tokens from domain and num_total_tokens=10B'
        Node id : 93958104927360, Node type: elif_clause, Node text: b'elif args.domain:\n        logging.info("Total tokens/domain tokens not specified, using 10B mix")\n        num_tokens = TOKENS_TO_FETCH_10B[args.domain]'
          Node id : 93958104923488, Node type: attribute, Node text: b'args.domain'
            Node id : 93958104923280, Node type: identifier, Node text: b'args'
            Node id : 93958104923392, Node type: identifier, Node text: b'domain'
          Node id : 93958104927264, Node type: block, Node text: b'logging.info("Total tokens/domain tokens not specified, using 10B mix")\n        num_tokens = TOKENS_TO_FETCH_10B[args.domain]'
            Node id : 93958104925376, Node type: expression_statement, Node text: b'logging.info("Total tokens/domain tokens not specified, using 10B mix")'
              Node id : 93958104925088, Node type: call, Node text: b'logging.info("Total tokens/domain tokens not specified, using 10B mix")'
                Node id : 93958104923984, Node type: attribute, Node text: b'logging.info'
                  Node id : 93958104923776, Node type: identifier, Node text: b'logging'
                  Node id : 93958104923888, Node type: identifier, Node text: b'info'
                Node id : 93958104924984, Node type: argument_list, Node text: b'("Total tokens/domain tokens not specified, using 10B mix")'
                  Node id : 93958104924576, Node type: string, Node text: b'"Total tokens/domain tokens not specified, using 10B mix"'
                    Node id : 93958104924464, Node type: string_start, Node text: b'"'
                    Node id : 93958104924472, Node type: string_content, Node text: b'Total tokens/domain tokens not specified, using 10B mix'
                    Node id : 93958104924480, Node type: string_end, Node text: b'"'
            Node id : 93958104926896, Node type: expression_statement, Node text: b'num_tokens = TOKENS_TO_FETCH_10B[args.domain]'
              Node id : 93958104926704, Node type: assignment, Node text: b'num_tokens = TOKENS_TO_FETCH_10B[args.domain]'
                Node id : 93958104925488, Node type: identifier, Node text: b'num_tokens'
                Node id : 93958104926304, Node type: subscript, Node text: b'TOKENS_TO_FETCH_10B[args.domain]'
                  Node id : 93958104925584, Node type: identifier, Node text: b'TOKENS_TO_FETCH_10B'
                  Node id : 93958104925888, Node type: attribute, Node text: b'args.domain'
                    Node id : 93958104925680, Node type: identifier, Node text: b'args'
                    Node id : 93958104925792, Node type: identifier, Node text: b'domain'
      Node id : 93958104930376, Node type: comment, Node text: b'# the flask server has to be up on clio'
      Node id : 93958104930256, Node type: expression_statement, Node text: b'all_files_lst = requests.get("http://128.2.209.71:5000/list-all").json()'
        Node id : 93958104930160, Node type: assignment, Node text: b'all_files_lst = requests.get("http://128.2.209.71:5000/list-all").json()'
          Node id : 93958104927824, Node type: identifier, Node text: b'all_files_lst'
          Node id : 93958104929760, Node type: call, Node text: b'requests.get("http://128.2.209.71:5000/list-all").json()'
            Node id : 93958104929344, Node type: attribute, Node text: b'requests.get("http://128.2.209.71:5000/list-all").json'
              Node id : 93958104929136, Node type: call, Node text: b'requests.get("http://128.2.209.71:5000/list-all")'
                Node id : 93958104928128, Node type: attribute, Node text: b'requests.get'
                  Node id : 93958104927920, Node type: identifier, Node text: b'requests'
                  Node id : 93958104928032, Node type: identifier, Node text: b'get'
                Node id : 93958104929032, Node type: argument_list, Node text: b'("http://128.2.209.71:5000/list-all")'
                  Node id : 93958104928720, Node type: string, Node text: b'"http://128.2.209.71:5000/list-all"'
                    Node id : 93958104928608, Node type: string_start, Node text: b'"'
                    Node id : 93958104928616, Node type: string_content, Node text: b'http://128.2.209.71:5000/list-all'
                    Node id : 93958104928624, Node type: string_end, Node text: b'"'
              Node id : 93958104929248, Node type: identifier, Node text: b'json'
            Node id : 93958104929656, Node type: argument_list, Node text: b'()'
      Node id : 93958104956008, Node type: if_statement, Node text: b'if args.domain:\n        fetch_tokens(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )\n    else:\n        logging.info("Fetching from all domains following the 10B ratio mix")\n        for domain in TOKENS_TO_FETCH_10B.keys():\n            logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
        Node id : 93958104930688, Node type: attribute, Node text: b'args.domain'
          Node id : 93958104930480, Node type: identifier, Node text: b'args'
          Node id : 93958104930592, Node type: identifier, Node text: b'domain'
        Node id : 93958104955888, Node type: block, Node text: b'fetch_tokens(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )'
          Node id : 93958104934720, Node type: expression_statement, Node text: b'fetch_tokens(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )'
            Node id : 93958104934336, Node type: call, Node text: b'fetch_tokens(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )'
              Node id : 93958104930976, Node type: identifier, Node text: b'fetch_tokens'
              Node id : 93958104934232, Node type: argument_list, Node text: b'(\n            num_tokens=num_tokens,\n            domain=args.domain,\n            output_dir=args.output,\n            all_files_lst=all_files_lst,\n            seed=args.seed,\n        )'
                Node id : 93958104934104, Node type: keyword_argument, Node text: b'num_tokens=num_tokens'
                  Node id : 93958104931264, Node type: identifier, Node text: b'num_tokens'
                  Node id : 93958104931072, Node type: identifier, Node text: b'num_tokens'
                Node id : 93958104931896, Node type: keyword_argument, Node text: b'domain=args.domain'
                  Node id : 93958104931776, Node type: identifier, Node text: b'domain'
                  Node id : 93958104931584, Node type: attribute, Node text: b'args.domain'
                    Node id : 93958104931376, Node type: identifier, Node text: b'args'
                    Node id : 93958104931488, Node type: identifier, Node text: b'domain'
                Node id : 93958104932520, Node type: keyword_argument, Node text: b'output_dir=args.output'
                  Node id : 93958104932400, Node type: identifier, Node text: b'output_dir'
                  Node id : 93958104932208, Node type: attribute, Node text: b'args.output'
                    Node id : 93958104932000, Node type: identifier, Node text: b'args'
                    Node id : 93958104932112, Node type: identifier, Node text: b'output'
                Node id : 93958104933048, Node type: keyword_argument, Node text: b'all_files_lst=all_files_lst'
                  Node id : 93958104932928, Node type: identifier, Node text: b'all_files_lst'
                  Node id : 93958104932736, Node type: identifier, Node text: b'all_files_lst'
                Node id : 93958104933784, Node type: keyword_argument, Node text: b'seed=args.seed'
                  Node id : 93958104933664, Node type: identifier, Node text: b'seed'
                  Node id : 93958104933472, Node type: attribute, Node text: b'args.seed'
                    Node id : 93958104933264, Node type: identifier, Node text: b'args'
                    Node id : 93958104933376, Node type: identifier, Node text: b'seed'
        Node id : 93958104955896, Node type: else_clause, Node text: b'else:\n        logging.info("Fetching from all domains following the 10B ratio mix")\n        for domain in TOKENS_TO_FETCH_10B.keys():\n            logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
          Node id : 93958104955752, Node type: block, Node text: b'logging.info("Fetching from all domains following the 10B ratio mix")\n        for domain in TOKENS_TO_FETCH_10B.keys():\n            logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
            Node id : 93958104936640, Node type: expression_statement, Node text: b'logging.info("Fetching from all domains following the 10B ratio mix")'
              Node id : 93958104936352, Node type: call, Node text: b'logging.info("Fetching from all domains following the 10B ratio mix")'
                Node id : 93958104935248, Node type: attribute, Node text: b'logging.info'
                  Node id : 93958104935040, Node type: identifier, Node text: b'logging'
                  Node id : 93958104935152, Node type: identifier, Node text: b'info'
                Node id : 93958104936248, Node type: argument_list, Node text: b'("Fetching from all domains following the 10B ratio mix")'
                  Node id : 93958104935840, Node type: string, Node text: b'"Fetching from all domains following the 10B ratio mix"'
                    Node id : 93958104935728, Node type: string_start, Node text: b'"'
                    Node id : 93958104935736, Node type: string_content, Node text: b'Fetching from all domains following the 10B ratio mix'
                    Node id : 93958104935744, Node type: string_end, Node text: b'"'
            Node id : 93958104955416, Node type: for_statement, Node text: b'for domain in TOKENS_TO_FETCH_10B.keys():\n            logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
              Node id : 93958104936752, Node type: identifier, Node text: b'domain'
              Node id : 93958104937376, Node type: call, Node text: b'TOKENS_TO_FETCH_10B.keys()'
                Node id : 93958104937056, Node type: attribute, Node text: b'TOKENS_TO_FETCH_10B.keys'
                  Node id : 93958104936848, Node type: identifier, Node text: b'TOKENS_TO_FETCH_10B'
                  Node id : 93958104936960, Node type: identifier, Node text: b'keys'
                Node id : 93958104937272, Node type: argument_list, Node text: b'()'
              Node id : 93958104955312, Node type: block, Node text: b'logging.info(f"Fetching {domain}")\n            if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]\n            fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
                Node id : 93958104939776, Node type: expression_statement, Node text: b'logging.info(f"Fetching {domain}")'
                  Node id : 93958104939488, Node type: call, Node text: b'logging.info(f"Fetching {domain}")'
                    Node id : 93958104937872, Node type: attribute, Node text: b'logging.info'
                      Node id : 93958104937664, Node type: identifier, Node text: b'logging'
                      Node id : 93958104937776, Node type: identifier, Node text: b'info'
                    Node id : 93958104939384, Node type: argument_list, Node text: b'(f"Fetching {domain}")'
                      Node id : 93958104938976, Node type: string, Node text: b'f"Fetching {domain}"'
                        Node id : 93958104938864, Node type: string_start, Node text: b'f"'
                        Node id : 93958104938752, Node type: string_content, Node text: b'Fetching '
                        Node id : 93958104938760, Node type: interpolation, Node text: b'{domain}'
                          Node id : 93958104938256, Node type: identifier, Node text: b'domain'
                        Node id : 93958104938880, Node type: string_end, Node text: b'"'
                Node id : 93958104950616, Node type: if_statement, Node text: b'if args.num_total_tokens:\n                logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000\n            else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]'
                  Node id : 93958104940096, Node type: attribute, Node text: b'args.num_total_tokens'
                    Node id : 93958104939888, Node type: identifier, Node text: b'args'
                    Node id : 93958104940000, Node type: identifier, Node text: b'num_total_tokens'
                  Node id : 93958104950496, Node type: block, Node text: b'logging.info("Calculating num_tokens from given args.num_total_tokens")\n                num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000'
                    Node id : 93958104941984, Node type: expression_statement, Node text: b'logging.info("Calculating num_tokens from given args.num_total_tokens")'
                      Node id : 93958104941696, Node type: call, Node text: b'logging.info("Calculating num_tokens from given args.num_total_tokens")'
                        Node id : 93958104940592, Node type: attribute, Node text: b'logging.info'
                          Node id : 93958104940384, Node type: identifier, Node text: b'logging'
                          Node id : 93958104940496, Node type: identifier, Node text: b'info'
                        Node id : 93958104941592, Node type: argument_list, Node text: b'("Calculating num_tokens from given args.num_total_tokens")'
                          Node id : 93958104941184, Node type: string, Node text: b'"Calculating num_tokens from given args.num_total_tokens"'
                            Node id : 93958104941072, Node type: string_start, Node text: b'"'
                            Node id : 93958104941080, Node type: string_content, Node text: b'Calculating num_tokens from given args.num_total_tokens'
                            Node id : 93958104941088, Node type: string_end, Node text: b'"'
                    Node id : 93958104946656, Node type: expression_statement, Node text: b'num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000'
                      Node id : 93958104946464, Node type: assignment, Node text: b'num_tokens = (\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000'
                        Node id : 93958104942096, Node type: identifier, Node text: b'num_tokens'
                        Node id : 93958104946064, Node type: binary_operator, Node text: b'(\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                ) * 1_000_000'
                          Node id : 93958104945664, Node type: parenthesized_expression, Node text: b'(\n                    int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000\n                )'
                            Node id : 93958104945360, Node type: binary_operator, Node text: b'int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )\n                    // 1_000_000'
                              Node id : 93958104945056, Node type: call, Node text: b'int(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )'
                                Node id : 93958104942192, Node type: identifier, Node text: b'int'
                                Node id : 93958104944952, Node type: argument_list, Node text: b'(\n                        (\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )\n                    )'
                                  Node id : 93958104944640, Node type: parenthesized_expression, Node text: b'(\n                            (parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]\n                        )'
                                    Node id : 93958104944336, Node type: binary_operator, Node text: b'(parse_num(args.num_total_tokens) / 10_000_000_000)\n                            * TOKENS_TO_FETCH_10B[domain]'
                                      Node id : 93958104943616, Node type: parenthesized_expression, Node text: b'(parse_num(args.num_total_tokens) / 10_000_000_000)'
                                        Node id : 93958104943312, Node type: binary_operator, Node text: b'parse_num(args.num_total_tokens) / 10_000_000_000'
                                          Node id : 93958104943008, Node type: call, Node text: b'parse_num(args.num_total_tokens)'
                                            Node id : 93958104942288, Node type: identifier, Node text: b'parse_num'
                                            Node id : 93958104942904, Node type: argument_list, Node text: b'(args.num_total_tokens)'
                                              Node id : 93958104942592, Node type: attribute, Node text: b'args.num_total_tokens'
                                                Node id : 93958104942384, Node type: identifier, Node text: b'args'
                                                Node id : 93958104942496, Node type: identifier, Node text: b'num_total_tokens'
                                          Node id : 93958104943104, Node type: integer, Node text: b'10_000_000_000'
                                      Node id : 93958104944128, Node type: subscript, Node text: b'TOKENS_TO_FETCH_10B[domain]'
                                        Node id : 93958104943712, Node type: identifier, Node text: b'TOKENS_TO_FETCH_10B'
                                        Node id : 93958104943808, Node type: identifier, Node text: b'domain'
                              Node id : 93958104945152, Node type: integer, Node text: b'1_000_000'
                          Node id : 93958104945856, Node type: integer, Node text: b'1_000_000'
                  Node id : 93958104950504, Node type: else_clause, Node text: b'else:\n                logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]'
                    Node id : 93958104950360, Node type: block, Node text: b'logging.info("Calculating num_tokens from args.num_total_tokens = 10B")\n                num_tokens = TOKENS_TO_FETCH_10B[domain]'
                      Node id : 93958104948688, Node type: expression_statement, Node text: b'logging.info("Calculating num_tokens from args.num_total_tokens = 10B")'
                        Node id : 93958104948400, Node type: call, Node text: b'logging.info("Calculating num_tokens from args.num_total_tokens = 10B")'
                          Node id : 93958104947296, Node type: attribute, Node text: b'logging.info'
                            Node id : 93958104947088, Node type: identifier, Node text: b'logging'
                            Node id : 93958104947200, Node type: identifier, Node text: b'info'
                          Node id : 93958104948296, Node type: argument_list, Node text: b'("Calculating num_tokens from args.num_total_tokens = 10B")'
                            Node id : 93958104947888, Node type: string, Node text: b'"Calculating num_tokens from args.num_total_tokens = 10B"'
                              Node id : 93958104947776, Node type: string_start, Node text: b'"'
                              Node id : 93958104947784, Node type: string_content, Node text: b'Calculating num_tokens from args.num_total_tokens = 10B'
                              Node id : 93958104947792, Node type: string_end, Node text: b'"'
                      Node id : 93958104950000, Node type: expression_statement, Node text: b'num_tokens = TOKENS_TO_FETCH_10B[domain]'
                        Node id : 93958104949808, Node type: assignment, Node text: b'num_tokens = TOKENS_TO_FETCH_10B[domain]'
                          Node id : 93958104948800, Node type: identifier, Node text: b'num_tokens'
                          Node id : 93958104949408, Node type: subscript, Node text: b'TOKENS_TO_FETCH_10B[domain]'
                            Node id : 93958104948896, Node type: identifier, Node text: b'TOKENS_TO_FETCH_10B'
                            Node id : 93958104948992, Node type: identifier, Node text: b'domain'
                Node id : 93958104954832, Node type: expression_statement, Node text: b'fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
                  Node id : 93958104954448, Node type: call, Node text: b'fetch_tokens(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
                    Node id : 93958104950720, Node type: identifier, Node text: b'fetch_tokens'
                    Node id : 93958104954344, Node type: argument_list, Node text: b'(\n                num_tokens=num_tokens,\n                domain=domain,\n                # Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains\n                output_dir=args.output + f"/{domain}",\n                all_files_lst=all_files_lst,\n            )'
                      Node id : 93958104954216, Node type: keyword_argument, Node text: b'num_tokens=num_tokens'
                        Node id : 93958104951008, Node type: identifier, Node text: b'num_tokens'
                        Node id : 93958104950816, Node type: identifier, Node text: b'num_tokens'
                      Node id : 93958104951432, Node type: keyword_argument, Node text: b'domain=domain'
                        Node id : 93958104951312, Node type: identifier, Node text: b'domain'
                        Node id : 93958104951120, Node type: identifier, Node text: b'domain'
                      Node id : 93958104953368, Node type: comment, Node text: b'# Slightly jank - append domain dir here instead of in get_tokens.sh if running on all domains'
                      Node id : 93958104953376, Node type: keyword_argument, Node text: b'output_dir=args.output + f"/{domain}"'
                        Node id : 93958104953248, Node type: identifier, Node text: b'output_dir'
                        Node id : 93958104953056, Node type: binary_operator, Node text: b'args.output + f"/{domain}"'
                          Node id : 93958104951744, Node type: attribute, Node text: b'args.output'
                            Node id : 93958104951536, Node type: identifier, Node text: b'args'
                            Node id : 93958104951648, Node type: identifier, Node text: b'output'
                          Node id : 93958104952848, Node type: string, Node text: b'f"/{domain}"'
                            Node id : 93958104952736, Node type: string_start, Node text: b'f"'
                            Node id : 93958104952624, Node type: string_content, Node text: b'/'
                            Node id : 93958104952632, Node type: interpolation, Node text: b'{domain}'
                              Node id : 93958104952128, Node type: identifier, Node text: b'domain'
                            Node id : 93958104952752, Node type: string_end, Node text: b'"'
                      Node id : 93958104953896, Node type: keyword_argument, Node text: b'all_files_lst=all_files_lst'
                        Node id : 93958104953776, Node type: identifier, Node text: b'all_files_lst'
                        Node id : 93958104953584, Node type: identifier, Node text: b'all_files_lst'

--------------------------------------------------
Word depths:
93958103920448 (b'impor...): 0
   93958104423216 (b'impor...): 1
      93958104416128 (b'reque...): 2
         93958104441536 (b'reque...): 3
   93958103518096 (b'from ...): 1
      93958104413880 (b'trans...): 2
         93958104399264 (b'trans...): 3
      93958103010416 (b'AutoT...): 2
         93958103010320 (b'AutoT...): 3
   93958102910432 (b'from ...): 1
      93958102947416 (b'datas...): 2
         93958103561952 (b'datas...): 3
      93958103480000 (b'load_...): 2
         93958103479904 (b'load_...): 3
   93958103685456 (b'impor...): 1
      93958104524496 (b'argpa...): 2
         93958104524400 (b'argpa...): 3
   93958104017168 (b'impor...): 1
      93958104014256 (b'random'): 2
         93958104014160 (b'random'): 3
   93958103321408 (b'impor...): 1
      93958104500528 (b'json'): 2
         93958104000928 (b'json'): 3
   93958104596624 (b'impor...): 1
      93958103430832 (b'gzip'): 2
         93958103430736 (b'gzip'): 3
   93958103876048 (b'from ...): 1
      93958103882792 (b'io'): 2
         93958103217696 (b'io'): 3
      93958103882688 (b'Bytes...): 2
         93958103882592 (b'Bytes...): 3
   93958103730400 (b'impor...): 1
      93958103317760 (b'panda...): 2
         93958103317648 (b'pandas'): 3
            93958103876272 (b'pandas'): 4
         93958103317664 (b'pd'): 3
   93958104566000 (b'impor...): 1
      93958103349168 (b'os'): 2
         93958103349072 (b'os'): 3
   93958103657328 (b'impor...): 1
      93958103542320 (b'loggi...): 2
         93958103542224 (b'loggi...): 3
   93958103362816 (b'from ...): 1
      93958103362696 (b'tqdm'): 2
         93958103702672 (b'tqdm'): 3
      93958103362592 (b'tqdm'): 2
         93958103702864 (b'tqdm'): 3
   93958103408752 (b'impor...): 1
      93958103408544 (b'sys'): 2
         93958103046016 (b'sys'): 3
   93958104002352 (b'impor...): 1
      93958103426976 (b'gc'): 2
         93958103426880 (b'gc'): 3
   93958103173952 (b'LLAMA...): 1
      93958103173856 (b'LLAMA...): 2
         93958104002576 (b'LLAMA...): 3
         93958103327120 (b'"/dat...): 3
            93958103433280 (b'"'): 4
            93958103433288 (b'/data...): 4
            93958103433296 (b'"'): 4
   93958104716352 (b'TOKEN...): 1
      93958104716256 (b'TOKEN...): 2
         93958104504048 (b'TOKEN...): 3
         93958104715856 (b'{\n  ...): 3
            93958104715736 (b'"comm...): 4
               93958104571648 (b'"comm...): 5
                  93958103458224 (b'"'): 6
                  93958103458232 (b'commo...): 6
                  93958103458240 (b'"'): 6
               93958104571840 (b'5_186...): 5
            93958103305944 (b'"c4":...): 4
               93958103738384 (b'"c4"'): 5
                  93958103142288 (b'"'): 6
                  93958103142296 (b'c4'): 6
                  93958103142304 (b'"'): 6
               93958103018672 (b'1_396...): 5
            93958104576728 (b'"peS2...): 4
               93958104576224 (b'"peS2...): 5
                  93958104433376 (b'"'): 6
                  93958104433384 (b'peS2o'): 6
                  93958104433392 (b'"'): 6
               93958104576416 (b'796_0...): 5
            93958104577944 (b'"stac...): 4
               93958104577440 (b'"stac...): 5
                  93958104577328 (b'"'): 6
                  93958104577336 (b'stack...): 6
                  93958104577344 (b'"'): 6
               93958104577632 (b'2_188...): 5
            93958104579160 (b'"gute...): 4
               93958104578656 (b'"gute...): 5
                  93958104578544 (b'"'): 6
                  93958104578552 (b'guten...): 6
                  93958104578560 (b'"'): 6
               93958104578848 (b'231_0...): 5
            93958104715416 (b'"wiki...): 4
               93958104714912 (b'"wiki...): 5
                  93958104714800 (b'"'): 6
                  93958104714808 (b'wiki-...): 6
                  93958104714816 (b'"'): 6
               93958104715104 (b'200_0...): 5
   93958104717264 (b'MAX_D...): 1
      93958104717168 (b'MAX_D...): 2
         93958104716576 (b'MAX_D...): 3
         93958104716768 (b'500_0...): 3
   93958104737080 (b'def p...): 1
      93958104736920 (b'parse...): 2
      93958104736928 (b'(val:...): 2
         93958104717888 (b'val: ...): 3
            93958104717776 (b'val'): 4
            93958104717792 (b'str'): 4
               93958104717488 (b'str'): 5
      93958104736944 (b'int'): 2
         93958104718192 (b'int'): 3
      93958104736968 (b'if va...): 2
         93958104736800 (b'if va...): 3
            93958104721280 (b'val.l...): 4
               93958104719792 (b'val.l...): 5
                  93958104719584 (b'val.l...): 6
                     93958104719264 (b'val.l...): 7
                        93958104718576 (b'val'): 8
                        93958104718928 (b'lower'): 8
                     93958104719480 (b'()'): 7
                  93958104719696 (b'endsw...): 6
               93958104721176 (b'("b")'): 5
                  93958104720864 (b'"b"'): 6
                     93958104720512 (b'"'): 7
                     93958104720520 (b'b'): 7
                     93958104720528 (b'"'): 7
            93958104736688 (b'retur...): 4
               93958104725056 (b'retur...): 5
                  93958104724656 (b'int(v...): 6
                     93958104724256 (b'int(v...): 7
                        93958104721568 (b'int'): 8
                        93958104724152 (b'(val[...): 8
                           93958104723840 (b'val[:...): 9
                              93958104721904 (b'val'): 10
                              93958104723728 (b':-1'): 10
                                 93958104723408 (b'-1'): 11
                                    93958104722960 (b'1'): 12
                     93958104724448 (b'1_000...): 7
            93958104730912 (b'elif ...): 4
               93958104727024 (b'val.l...): 5
                  93958104726016 (b'val.l...): 6
                     93958104725808 (b'val.l...): 7
                        93958104725488 (b'val.l...): 8
                           93958104725280 (b'val'): 9
                           93958104725392 (b'lower'): 9
                        93958104725704 (b'()'): 8
                     93958104725920 (b'endsw...): 7
                  93958104726920 (b'("m")'): 6
                     93958104726608 (b'"m"'): 7
                        93958104726496 (b'"'): 8
                        93958104726504 (b'm'): 8
                        93958104726512 (b'"'): 8
               93958104730816 (b'retur...): 5
                  93958104730560 (b'retur...): 6
                     93958104730160 (b'int(v...): 7
                        93958104729760 (b'int(v...): 8
                           93958104727312 (b'int'): 9
                           93958104729656 (b'(val[...): 9
                              93958104729344 (b'val[:...): 10
                                 93958104727408 (b'val'): 11
                                 93958104729232 (b':-1'): 11
                                    93958104728912 (b'-1'): 12
                                       93958104728464 (b'1'): 13
                        93958104729952 (b'1_000...): 8
            93958104736704 (b'else:...): 4
               93958104736552 (b'try:\...): 5
                  93958104736416 (b'try:\...): 6
                     93958104736216 (b'retur...): 7
                        93958104733200 (b'retur...): 8
                           93958104732800 (b'int(f...): 9
                              93958104731200 (b'int'): 10
                              93958104732696 (b'(floa...): 10
                                 93958104732288 (b'float...): 11
                                    93958104731296 (b'float'): 12
                                    93958104732184 (b'(val)'): 12
                                       93958104731392 (b'val'): 13
                     93958104736224 (b'excep...): 7
                        93958104736088 (b'raise...): 8
                           93958104735744 (b'raise...): 9
                              93958104735344 (b'Value...): 10
                                 93958104733520 (b'Value...): 11
                                 93958104735240 (b'(\n  ...): 11
                                    93958104734832 (b'"You ...): 12
                                       93958104734480 (b'"'): 13
                                       93958104734488 (b'You m...): 13
                                       93958104734496 (b'"'): 13
   93958104878544 (b'def p...): 1
      93958104757672 (b'proce...): 2
      93958104757680 (b'(cont...): 2
         93958104737584 (b'conte...): 3
            93958104737472 (b'conte...): 4
            93958104737488 (b'bytes'): 4
               93958104737184 (b'bytes'): 5
         93958104738080 (b'file_...): 3
            93958104737968 (b'file_...): 4
            93958104737984 (b'int'): 4
               93958104737680 (b'int'): 5
      93958104757696 (b'list'): 2
         93958104738512 (b'list'): 3
      93958104757720 (b'if fi...): 2
         93958104757440 (b'if fi...): 3
            93958104739616 (b'file_...): 4
               93958104739200 (b'file_...): 5
                  93958104738896 (b'file_...): 6
                  93958104738992 (b'50'): 6
               93958104739296 (b'0'): 5
            93958104743424 (b'print...): 4
               93958104743168 (b'print...): 5
                  93958104742784 (b'print...): 6
                     93958104739808 (b'print'): 7
                     93958104742680 (b'(f"Pr...): 7
                        93958104741216 (b'f"Pro...): 8
                           93958104741008 (b'f"'): 9
                           93958104740784 (b'Proce...): 9
                           93958104740792 (b'{file...): 9
                              93958104740288 (b'file_...): 10
                           93958104741024 (b'"'): 9
         93958104757448 (b'with ...): 3
            93958104757320 (b'gzip....): 4
               93958104745920 (b'gzip....): 5
                  93958104745728 (b'gzip....): 6
                     93958104745232 (b'gzip....): 7
                        93958104741312 (b'gzip....): 8
                           93958104741120 (b'gzip'): 9
                           93958104740912 (b'open'): 9
                        93958104745128 (b'(Byte...): 8
                           93958104743616 (b'Bytes...): 9
                              93958104741712 (b'Bytes...): 10
                              93958104741912 (b'(cont...): 10
                                 93958104741808 (b'conte...): 11
                           93958104743904 (b'"rt"'): 9
                              93958104742112 (b'"'): 10
                              93958104742120 (b'rt'): 10
                              93958104742128 (b'"'): 10
                           93958104744904 (b'error...): 9
                              93958104744784 (b'errors'): 10
                              93958104744592 (b'"igno...): 10
                                 93958104744480 (b'"'): 11
                                 93958104744488 (b'ignore'): 11
                                 93958104744496 (b'"'): 11
                     93958104745632 (b'f'): 7
                        93958104745424 (b'f'): 8
            93958104757344 (b'try:\...): 4
               93958104757200 (b'try:\...): 5
                  93958104757096 (b'lines...): 6
                     93958104747424 (b'lines...): 7
                        93958104747328 (b'lines...): 8
                           93958104746208 (b'lines'): 9
                           93958104746928 (b'f.rea...): 9
                              93958104746512 (b'f.rea...): 10
                                 93958104746304 (b'f'): 11
                                 93958104746416 (b'readl...): 11
                              93958104746824 (b'()'): 10
                     93958104749680 (b'lines...): 7
                        93958104749584 (b'lines...): 8
                           93958104747536 (b'lines'): 9
                           93958104749184 (b'[line...): 9
                              93958104748160 (b'line....): 10
                                 93958104747840 (b'line....): 11
                                    93958104747632 (b'line'): 12
                                    93958104747744 (b'strip'): 12
                                 93958104748056 (b'()'): 11
                              93958104748864 (b'for l...): 10
                                 93958104748352 (b'line'): 11
                                 93958104748448 (b'lines'): 11
                     93958104750400 (b'retur...): 7
                        93958104750000 (b'lines'): 8
                  93958104757104 (b'excep...): 6
                     93958104751232 (b'Excep...): 7
                        93958104750736 (b'Excep...): 8
                        93958104751136 (b'e'): 8
                           93958104750928 (b'e'): 9
                     93958104756976 (b'print...): 7
                        93958104754352 (b'print...): 8
                           93958104754064 (b'print...): 9
                              93958104751424 (b'print'): 10
                              93958104753960 (b'(f"Er...): 10
                                 93958104752832 (b'f"Err...): 11
                                    93958104752624 (b'f"'): 12
                                    93958104752400 (b'Error...): 12
                                    93958104752408 (b'{e}'): 12
                                       93958104751904 (b'e'): 13
                                    93958104752640 (b'"'): 12
                        93958104756720 (b'print...): 8
                           93958104756432 (b'print...): 9
                              93958104752736 (b'print'): 10
                              93958104756328 (b'(f"Sk...): 10
                                 93958104755200 (b'f"Ski...): 11
                                    93958104754992 (b'f"'): 12
                                    93958104753216 (b'Skipp...): 12
                                    93958104753224 (b'{file...): 12
                                       93958104754464 (b'file_...): 13
                                    93958104755008 (b'"'): 12
                        93958104755776 (b'retur...): 8
                           93958104755104 (b'[]'): 9
   93958104878552 (b'def f...): 1
      93958104878408 (b'fetch...): 2
      93958104878416 (b'(\n  ...): 2
         93958104758336 (b'num_t...): 3
            93958104758224 (b'num_t...): 4
            93958104758240 (b'int'): 4
               93958104757936 (b'int'): 5
         93958104758832 (b'domai...): 3
            93958104758720 (b'domain'): 4
            93958104758736 (b'str'): 4
               93958104758432 (b'str'): 5
         93958104759840 (b'outpu...): 3
            93958104759728 (b'outpu...): 4
            93958104759744 (b'str o...): 4
               93958104759536 (b'str o...): 5
                  93958104759040 (b'str'): 6
                  93958104759232 (b'None'): 6
         93958104760560 (b'all_f...): 3
            93958104760448 (b'all_f...): 4
            93958104760464 (b'list'): 4
               93958104760160 (b'list'): 5
         93958104761488 (b'seed:...): 3
            93958104761360 (b'seed'): 4
            93958104761376 (b'int'): 4
               93958104760880 (b'int'): 5
            93958104761168 (b'42'): 4
      93958104878440 (b'curre...): 2
         93958104762816 (b'curre...): 3
            93958104762720 (b'curre...): 4
               93958104762128 (b'curre...): 5
               93958104762320 (b'0'): 5
         93958104766048 (b'outpu...): 3
            93958104765952 (b'outpu...): 4
               93958104762928 (b'outpu...): 5
               93958104765648 (b'outpu...): 5
                  93958104763024 (b'outpu...): 6
                  93958104763216 (b'outpu...): 6
                  93958104765328 (b'f"./d...): 6
                     93958104765216 (b'f"'): 7
                     93958104764192 (b'./dol...): 7
                     93958104764200 (b'{doma...): 7
                        93958104763696 (b'domain'): 8
                     93958104764408 (b'_'): 7
                     93958104765016 (b'{num_...): 7
                        93958104764512 (b'num_t...): 8
                     93958104765232 (b'"'): 7
         93958104769200 (b'loggi...): 3
            93958104768912 (b'loggi...): 4
               93958104766480 (b'loggi...): 5
                  93958104766272 (b'loggi...): 6
                  93958104766384 (b'info'): 6
               93958104768808 (b'(f"Fe...): 5
                  93958104768400 (b'f"Fet...): 6
                     93958104768288 (b'f"'): 7
                     93958104767360 (b'Fetch...): 7
                     93958104767368 (b'{num_...): 7
                        93958104766864 (b'num_t...): 8
                     93958104767576 (b' toke...): 7
                     93958104768184 (b'{doma...): 7
                        93958104767680 (b'domain'): 8
                     93958104768304 (b'"'): 7
         93958104770648 (b'# shu...): 3
         93958104770528 (b'rando...): 3
            93958104770240 (b'rando...): 4
               93958104769632 (b'rando...): 5
                  93958104769424 (b'random'): 6
                  93958104769536 (b'seed'): 6
               93958104770136 (b'(seed)'): 5
                  93958104769728 (b'seed'): 6
         93958104771856 (b'rando...): 3
            93958104771568 (b'rando...): 4
               93958104770960 (b'rando...): 5
                  93958104770752 (b'random'): 6
                  93958104770864 (b'shuff...): 6
               93958104771464 (b'(all_...): 5
                  93958104771056 (b'all_f...): 6
         93958104772880 (b'texts...): 3
            93958104772784 (b'texts...): 4
               93958104772080 (b'texts...): 5
               93958104772384 (b'[]'): 5
         93958104776376 (b'# fil...): 3
         93958104776256 (b'all_f...): 3
            93958104776160 (b'all_f...): 4
               93958104773104 (b'all_f...): 5
               93958104775760 (b'[f fo...): 5
                  93958104773200 (b'f'): 6
                  93958104775424 (b'for f...): 6
                     93958104773392 (b'f'): 7
                     93958104773488 (b'all_f...): 7
                  93958104775432 (b'if f....): 6
                     93958104775120 (b'f.end...): 7
                        93958104774112 (b'f.end...): 8
                           93958104773904 (b'f'): 9
                           93958104774016 (b'endsw...): 9
                        93958104775016 (b'(".gz...): 8
                           93958104774704 (b'".gz"'): 9
                              93958104774592 (b'"'): 10
                              93958104774600 (b'.gz'): 10
                              93958104774608 (b'"'): 10
         93958104778856 (b'# fil...): 3
         93958104778736 (b'all_f...): 3
            93958104778640 (b'all_f...): 4
               93958104776480 (b'all_f...): 5
               93958104778240 (b'[f fo...): 5
                  93958104776576 (b'f'): 6
                  93958104777904 (b'for f...): 6
                     93958104776768 (b'f'): 7
                     93958104776864 (b'all_f...): 7
                  93958104777912 (b'if do...): 6
                     93958104777696 (b'domai...): 7
                        93958104777280 (b'domain'): 8
                        93958104777376 (b'f'): 8
         93958104780368 (b'token...): 3
            93958104780272 (b'token...): 4
               93958104778960 (b'token...): 5
               93958104779872 (b'AutoT...): 5
                  93958104779264 (b'AutoT...): 6
                     93958104779056 (b'AutoT...): 7
                     93958104779168 (b'from_...): 7
                  93958104779768 (b'(LLAM...): 6
                     93958104779360 (b'LLAMA...): 7
         93958104781696 (b'token...): 3
            93958104781600 (b'token...): 4
               93958104780800 (b'token...): 5
                  93958104780592 (b'token...): 6
                  93958104780704 (b'pad_t...): 6
               93958104781200 (b'token...): 5
                  93958104780896 (b'token...): 6
                  93958104781104 (b'eos_t...): 6
         93958104782608 (b'file_...): 3
            93958104782512 (b'file_...): 4
               93958104781920 (b'file_...): 5
               93958104782112 (b'0'): 5
         93958104783520 (b'part_...): 3
            93958104783424 (b'part_...): 4
               93958104782832 (b'part_...): 5
               93958104783024 (b'0'): 5
         93958104875448 (b'with ...): 3
            93958104875320 (b'tqdm(...): 4
               93958104785056 (b'tqdm(...): 5
                  93958104784864 (b'tqdm(...): 6
                     93958104784368 (b'tqdm(...): 7
                        93958104783744 (b'tqdm'): 8
                        93958104784264 (b'(tota...): 8
                           93958104784152 (b'total...): 9
                              93958104784032 (b'total'): 10
                              93958104783840 (b'num_t...): 10
                     93958104784768 (b'pbar'): 7
                        93958104784560 (b'pbar'): 8
            93958104875344 (b'while...): 4
               93958104875200 (b'while...): 5
                  93958104786896 (b'curre...): 6
                     93958104785664 (b'curre...): 7
                        93958104785248 (b'curre...): 8
                        93958104785344 (b'num_t...): 8
                     93958104786688 (b'file_...): 7
                        93958104785760 (b'file_...): 8
                        93958104786368 (b'len(a...): 8
                           93958104785856 (b'len'): 9
                           93958104786264 (b'(all_...): 9
                              93958104785952 (b'all_f...): 10
                  93958104875104 (b'respo...): 6
                     93958104789920 (b'respo...): 7
                        93958104789824 (b'respo...): 8
                           93958104787088 (b'respo...): 9
                           93958104789424 (b'reque...): 9
                              93958104787392 (b'reque...): 10
                                 93958104787184 (b'reque...): 11
                                 93958104787296 (b'get'): 11
                              93958104789320 (b'(\n  ...): 10
                                 93958104788912 (b'f"htt...): 11
                                    93958104788800 (b'f"'): 12
                                    93958104788688 (b'http:...): 12
                                    93958104788696 (b'{all_...): 12
                                       93958104788192 (b'all_f...): 13
                                          93958104787776 (b'all_f...): 14
                                          93958104787872 (b'file_...): 14
                                    93958104788816 (b'"'): 12
                     93958104794248 (b'if re...): 7
                        93958104790656 (b'respo...): 8
                           93958104790240 (b'respo...): 9
                              93958104790032 (b'respo...): 10
                              93958104790144 (b'statu...): 10
                           93958104790336 (b'200'): 9
                        93958104794144 (b'loggi...): 8
                           93958104793376 (b'loggi...): 9
                              93958104793088 (b'loggi...): 10
                                 93958104791056 (b'loggi...): 11
                                    93958104790848 (b'loggi...): 12
                                    93958104790960 (b'info'): 12
                                 93958104792984 (b'(f"Er...): 11
                                    93958104792576 (b'f"Err...): 12
                                       93958104792464 (b'f"'): 13
                                       93958104792352 (b'Error...): 13
                                       93958104792360 (b'{all_...): 13
                                          93958104791856 (b'all_f...): 14
                                             93958104791440 (b'all_f...): 15
                                             93958104791536 (b'file_...): 15
                                       93958104792480 (b'"'): 13
                           93958104793776 (b'conti...): 9
                     93958104795040 (b'file_...): 7
                        93958104794944 (b'file_...): 8
                           93958104794352 (b'file_...): 9
                           93958104794544 (b'1'): 9
                     93958104798640 (b'docs ...): 7
                        93958104798544 (b'docs ...): 8
                           93958104795264 (b'docs'): 9
                           93958104798144 (b'[\n  ...): 9
                              93958104796080 (b'json....): 10
                                 93958104795568 (b'json....): 11
                                    93958104795360 (b'json'): 12
                                    93958104795472 (b'loads'): 12
                                 93958104795976 (b'(l)'): 11
                                    93958104795664 (b'l'): 12
                              93958104797824 (b'for l...): 10
                                 93958104796272 (b'l'): 11
                                 93958104797408 (b'proce...): 11
                                    93958104796368 (b'proce...): 12
                                    93958104797304 (b'(resp...): 12
                                       93958104796672 (b'respo...): 13
                                          93958104796464 (b'respo...): 14
                                          93958104796576 (b'conte...): 14
                                       93958104796864 (b'file_...): 13
                     93958104802296 (b'# jus...): 7
                     93958104802176 (b'field...): 7
                        93958104802080 (b'field...): 8
                           93958104798864 (b'field...): 9
                           93958104801680 (b'["tex...): 9
                              93958104799456 (b'"text"'): 10
                                 93958104799344 (b'"'): 11
                                 93958104799352 (b'text'): 11
                                 93958104799360 (b'"'): 11
                              93958104800144 (b'"id"'): 10
                                 93958104800032 (b'"'): 11
                                 93958104800040 (b'id'): 11
                                 93958104800048 (b'"'): 11
                              93958104800944 (b'"lang"'): 10
                                 93958104800832 (b'"'): 11
                                 93958104800840 (b'lang'): 11
                                 93958104800848 (b'"'): 11
                     93958104806736 (b'docs ...): 7
                        93958104806640 (b'docs ...): 8
                           93958104802400 (b'docs'): 9
                           93958104806240 (b'[{k: ...): 9
                              93958104805216 (b'{k: v...): 10
                                 93958104805096 (b'k: v'): 11
                                    93958104802496 (b'k'): 12
                                    93958104802688 (b'v'): 12
                                 93958104804976 (b'for k...): 11
                                    93958104804232 (b'k, v'): 12
                                       93958104802992 (b'k'): 13
                                       93958104803088 (b'v'): 13
                                    93958104803936 (b'd.ite...): 12
                                       93958104803616 (b'd.ite...): 13
                                          93958104803408 (b'd'): 14
                                          93958104803520 (b'items'): 14
                                       93958104803832 (b'()'): 13
                                 93958104804984 (b'if k ...): 11
                                    93958104804768 (b'k in ...): 12
                                       93958104804352 (b'k'): 13
                                       93958104804448 (b'field...): 13
                              93958104805920 (b'for d...): 10
                                 93958104805408 (b'd'): 11
                                 93958104805504 (b'docs'): 11
                     93958104874760 (b'# tok...): 7
                     93958104874768 (b'for _...): 7
                        93958104874616 (b'_, doc'): 8
                           93958104806960 (b'_'): 9
                           93958104807056 (b'doc'): 9
                        93958104807888 (b'enume...): 8
                           93958104807376 (b'enume...): 9
                           93958104807784 (b'(docs)'): 9
                              93958104807472 (b'docs'): 10
                        93958104874656 (b'encod...): 8
                           93958104811936 (b'encod...): 9
                              93958104811840 (b'encod...): 10
                                 93958104808176 (b'encod...): 11
                                 93958104811440 (b'token...): 11
                                    93958104808272 (b'token...): 12
                                    93958104811336 (b'(doc[...): 12
                                       93958104809520 (b'doc["...): 13
                                          93958104808368 (b'doc'): 14
                                          93958104809200 (b'"text"'): 14
                                             93958104808848 (b'"'): 15
                                             93958104808856 (b'text'): 15
                                             93958104808864 (b'"'): 15
                                       93958104811000 (b'retur...): 13
                                          93958104810880 (b'retur...): 14
                                          93958104810688 (b'"pt"'): 14
                                             93958104810336 (b'"'): 15
                                             93958104810344 (b'pt'): 15
                                             93958104810352 (b'"'): 15
                           93958104815936 (b'num_n...): 9
                              93958104815840 (b'num_n...): 10
                                 93958104812048 (b'num_n...): 11
                                 93958104815440 (b'(\n  ...): 11
                                    93958104815040 (b'(enco...): 12
                                       93958104814720 (b'(enco...): 13
                                          93958104814512 (b'(enco...): 14
                                             93958104813888 (b'(enco...): 15
                                                93958104813680 (b'(enco...): 16
                                                   93958104813472 (b'encod...): 17
                                                      93958104813056 (b'encod...): 18
                                                         93958104812144 (b'encod...): 19
                                                         93958104812736 (b'"atte...): 19
                                                            93958104812624 (b'"'): 20
                                                            93958104812632 (b'atten...): 20
                                                            93958104812640 (b'"'): 20
                                                      93958104813152 (b'1'): 18
                                                93958104813792 (b'sum'): 16
                                             93958104814408 (b'(dim=...): 15
                                                93958104814296 (b'dim=1'): 16
                                                   93958104814176 (b'dim'): 17
                                                   93958104813984 (b'1'): 17
                                          93958104814624 (b'tolist'): 14
                                       93958104814936 (b'()'): 13
                           93958104817360 (b'curre...): 9
                              93958104817264 (b'curre...): 10
                                 93958104816160 (b'curre...): 11
                                 93958104816864 (b'sum(n...): 11
                                    93958104816256 (b'sum'): 12
                                    93958104816760 (b'(num_...): 12
                                       93958104816352 (b'num_n...): 13
                           93958104819200 (b'pbar....): 9
                              93958104818912 (b'pbar....): 10
                                 93958104817792 (b'pbar....): 11
                                    93958104817584 (b'pbar'): 12
                                    93958104817696 (b'update'): 12
                                 93958104818808 (b'(sum(...): 11
                                    93958104818400 (b'sum(n...): 12
                                       93958104817888 (b'sum'): 13
                                       93958104818296 (b'(num_...): 13
                                          93958104817984 (b'num_n...): 14
                           93958104821008 (b'texts...): 9
                              93958104820240 (b'texts...): 10
                                 93958104819632 (b'texts...): 11
                                    93958104819424 (b'texts...): 12
                                    93958104819536 (b'append'): 12
                                 93958104820136 (b'(doc)'): 11
                                    93958104819728 (b'doc'): 12
                           93958104874280 (b'# sav...): 9
                           93958104874288 (b'## fo...): 9
                           93958104874296 (b'## pa...): 9
                           93958104874304 (b'if (\...): 9
                              93958104823840 (b'(\n  ...): 10
                                 93958104823632 (b'curre...): 11
                                    93958104821648 (b'curre...): 12
                                       93958104821232 (b'curre...): 13
                                       93958104821328 (b'num_t...): 13
                                    93958104823424 (b'sys.g...): 12
                                       93958104823008 (b'sys.g...): 13
                                          93958104822704 (b'sys.g...): 14
                                             93958104821952 (b'sys.g...): 15
                                                93958104821744 (b'sys'): 16
                                                93958104821856 (b'getsi...): 16
                                             93958104822600 (b'(text...): 15
                                                93958104822048 (b'texts...): 16
                                          93958104822800 (b'500'): 14
                                       93958104823104 (b'MAX_D...): 13
                              93958104874176 (b'part_...): 10
                                 93958104824624 (b'part_...): 11
                                    93958104824528 (b'part_...): 12
                                       93958104824032 (b'part_...): 13
                                       93958104824128 (b'1'): 13
                                 93958104828208 (b'outpu...): 11
                                    93958104828112 (b'outpu...): 12
                                       93958104824736 (b'outpu...): 13
                                       93958104827712 (b'f"{ou...): 13
                                          93958104827600 (b'f"'): 14
                                          93958104826240 (b'{outp...): 14
                                             93958104825168 (b'outpu...): 15
                                          93958104826248 (b'/part_'): 14
                                          93958104827096 (b'{part...): 14
                                             93958104826352 (b'part_...): 15
                                          93958104827400 (b'.arrow'): 14
                                          93958104827616 (b'"'): 14
                                 93958104830736 (b'loggi...): 11
                                    93958104830256 (b'loggi...): 12
                                       93958104828640 (b'loggi...): 13
                                          93958104828432 (b'loggi...): 14
                                          93958104828544 (b'info'): 14
                                       93958104830152 (b'(f"Ou...): 13
                                          93958104829744 (b'f"Out...): 14
                                             93958104829632 (b'f"'): 15
                                             93958104829520 (b'Outpu...): 15
                                             93958104829528 (b'{outp...): 15
                                                93958104829024 (b'outpu...): 16
                                             93958104829648 (b'"'): 15
                                 93958104833688 (b'# mkd...): 11
                                 93958104833568 (b'os.ma...): 11
                                    93958104833280 (b'os.ma...): 12
                                       93958104831168 (b'os.ma...): 13
                                          93958104830960 (b'os'): 14
                                          93958104831072 (b'maked...): 14
                                       93958104833176 (b'(os.p...): 13
                                          93958104832192 (b'os.pa...): 14
                                             93958104831680 (b'os.pa...): 15
                                                93958104831472 (b'os.pa...): 16
                                                   93958104831264 (b'os'): 17
                                                   93958104831376 (b'path'): 17
                                                93958104831584 (b'dirna...): 16
                                             93958104832088 (b'(outp...): 15
                                                93958104831776 (b'outpu...): 16
                                          93958104832936 (b'exist...): 14
                                             93958104832816 (b'exist...): 15
                                             93958104832384 (b'True'): 15
                                 93958104835872 (b'df = ...): 11
                                    93958104835776 (b'df = ...): 12
                                       93958104833792 (b'df'): 13
                                       93958104835376 (b'pd.Da...): 13
                                          93958104834096 (b'pd.Da...): 14
                                             93958104833888 (b'pd'): 15
                                             93958104834000 (b'DataF...): 15
                                          93958104835272 (b'(text...): 14
                                             93958104834192 (b'texts...): 15
                                             93958104834936 (b'colum...): 15
                                                93958104834816 (b'colum...): 16
                                                93958104834384 (b'field...): 16
                                 93958104837584 (b'df.to...): 11
                                    93958104836912 (b'df.to...): 12
                                       93958104836304 (b'df.to...): 13
                                          93958104836096 (b'df'): 14
                                          93958104836208 (b'to_pa...): 14
                                       93958104836808 (b'(outp...): 13
                                          93958104836400 (b'outpu...): 14
                                 93958104868536 (b'# if ...): 11
                                 93958104868544 (b"# can...): 11
                                 93958104869984 (b'def s...): 11
                                    93958104868392 (b'split...): 12
                                    93958104868400 (b'(outp...): 12
                                       93958104837808 (b'outpu...): 13
                                       93958104837904 (b'df'): 13
                                    93958104868424 (b'if os...): 12
                                       93958104866928 (b'if os...): 13
                                          93958104840400 (b'os.pa...): 14
                                             93958104839984 (b'os.pa...): 15
                                                93958104839232 (b'os.pa...): 16
                                                   93958104839024 (b'os.pa...): 17
                                                      93958104838336 (b'os'): 18
                                                      93958104838688 (b'path'): 18
                                                   93958104839136 (b'getsi...): 17
                                                93958104839880 (b'(outp...): 16
                                                   93958104839328 (b'outpu...): 17
                                             93958104840080 (b'MAX_D...): 15
                                          93958104866320 (b'os.re...): 14
                                             93958104842320 (b'os.re...): 15
                                                93958104842032 (b'os.re...): 16
                                                   93958104841184 (b'os.re...): 17
                                                      93958104840496 (b'os'): 18
                                                      93958104840848 (b'remove'): 18
                                                   93958104841928 (b'(outp...): 17
                                                      93958104841280 (b'outpu...): 18
                                             93958104846304 (b'outpu...): 15
                                                93958104846208 (b'outpu...): 16
                                                   93958104842432 (b'outpu...): 17
                                                   93958104845808 (b'f"{ou...): 17
                                                      93958104845696 (b'f"'): 18
                                                      93958104845488 (b'{outp...): 18
                                                         93958104844800 (b'outpu...): 19
                                                            93958104842864 (b'outpu...): 20
                                                            93958104844688 (b':-6'): 20
                                                               93958104844368 (b'-6'): 21
                                                                  93958104843920 (b'6'): 22
                                                      93958104845496 (b'_1.ar...): 18
                                                      93958104845712 (b'"'): 18
                                             93958104848960 (b'outpu...): 15
                                                93958104848864 (b'outpu...): 16
                                                   93958104846528 (b'outpu...): 17
                                                   93958104848464 (b'f"{ou...): 17
                                                      93958104848352 (b'f"'): 18
                                                      93958104848144 (b'{outp...): 18
                                                         93958104847456 (b'outpu...): 19
                                                            93958104846720 (b'outpu...): 20
                                                            93958104847344 (b':-6'): 20
                                                               93958104847024 (b'-6'): 21
                                                                  93958104846816 (b'6'): 22
                                                      93958104848152 (b'_2.ar...): 18
                                                      93958104848368 (b'"'): 18
                                             93958104851648 (b'os.ma...): 15
                                                93958104851360 (b'os.ma...): 16
                                                   93958104849392 (b'os.ma...): 17
                                                      93958104849184 (b'os'): 18
                                                      93958104849296 (b'maked...): 18
                                                   93958104851256 (b'(os.p...): 17
                                                      93958104850416 (b'os.pa...): 18
                                                         93958104849904 (b'os.pa...): 19
                                                            93958104849696 (b'os.pa...): 20
                                                               93958104849488 (b'os'): 21
                                                               93958104849600 (b'path'): 21
                                                            93958104849808 (b'dirna...): 20
                                                         93958104850312 (b'(outp...): 19
                                                            93958104850000 (b'outpu...): 20
                                                      93958104850920 (b'exist...): 18
                                                         93958104850800 (b'exist...): 19
                                                         93958104850608 (b'True'): 19
                                             93958104854336 (b'os.ma...): 15
                                                93958104854048 (b'os.ma...): 16
                                                   93958104852080 (b'os.ma...): 17
                                                      93958104851872 (b'os'): 18
                                                      93958104851984 (b'maked...): 18
                                                   93958104853944 (b'(os.p...): 17
                                                      93958104853104 (b'os.pa...): 18
                                                         93958104852592 (b'os.pa...): 19
                                                            93958104852384 (b'os.pa...): 20
                                                               93958104852176 (b'os'): 21
                                                               93958104852288 (b'path'): 21
                                                            93958104852496 (b'dirna...): 20
                                                         93958104853000 (b'(outp...): 19
                                                            93958104852688 (b'outpu...): 20
                                                      93958104853608 (b'exist...): 18
                                                         93958104853488 (b'exist...): 19
                                                         93958104853296 (b'True'): 19
                                             93958104856704 (b'df1 =...): 15
                                                93958104856608 (b'df1 =...): 16
                                                   93958104854560 (b'df1'): 17
                                                   93958104856208 (b'df[: ...): 17
                                                      93958104854656 (b'df'): 18
                                                      93958104856096 (b': df....): 18
                                                         93958104855680 (b'df.sh...): 19
                                                            93958104855376 (b'df.sh...): 20
                                                               93958104854960 (b'df.sh...): 21
                                                                  93958104854752 (b'df'): 22
                                                                  93958104854864 (b'shape'): 22
                                                               93958104855056 (b'0'): 21
                                                            93958104855472 (b'2'): 20
                                             93958104859072 (b'df2 =...): 15
                                                93958104858976 (b'df2 =...): 16
                                                   93958104856928 (b'df2'): 17
                                                   93958104858576 (b'df[df...): 17
                                                      93958104857024 (b'df'): 18
                                                      93958104858464 (b'df.sh...): 18
                                                         93958104858048 (b'df.sh...): 19
                                                            93958104857744 (b'df.sh...): 20
                                                               93958104857328 (b'df.sh...): 21
                                                                  93958104857120 (b'df'): 22
                                                                  93958104857232 (b'shape'): 22
                                                               93958104857424 (b'0'): 21
                                                            93958104857840 (b'2'): 20
                                             93958104860400 (b'df1.t...): 15
                                                93958104860112 (b'df1.t...): 16
                                                   93958104859504 (b'df1.t...): 17
                                                      93958104859296 (b'df1'): 18
                                                      93958104859408 (b'to_pa...): 18
                                                   93958104860008 (b'(outp...): 17
                                                      93958104859600 (b'outpu...): 18
                                             93958104861728 (b'df2.t...): 15
                                                93958104861440 (b'df2.t...): 16
                                                   93958104860832 (b'df2.t...): 17
                                                      93958104860624 (b'df2'): 18
                                                      93958104860736 (b'to_pa...): 18
                                                   93958104861336 (b'(outp...): 17
                                                      93958104860928 (b'outpu...): 18
                                             93958104863168 (b'split...): 15
                                                93958104862880 (b'split...): 16
                                                   93958104861952 (b'split...): 17
                                                   93958104862776 (b'(outp...): 17
                                                      93958104862048 (b'outpu...): 18
                                                      93958104862240 (b'df1'): 18
                                             93958104864608 (b'split...): 15
                                                93958104864320 (b'split...): 16
                                                   93958104863392 (b'split...): 17
                                                   93958104864216 (b'(outp...): 17
                                                      93958104863488 (b'outpu...): 18
                                                      93958104863680 (b'df2'): 18
                                             93958104865232 (b'del d...): 15
                                                93958104864928 (b'df1'): 16
                                             93958104865952 (b'del d...): 15
                                                93958104865552 (b'df2'): 16
                                       93958104866816 (b'del df'): 13
                                          93958104866512 (b'df'): 14
                                       93958104868048 (b'gc.co...): 13
                                          93958104867664 (b'gc.co...): 14
                                             93958104867248 (b'gc.co...): 15
                                                93958104867040 (b'gc'): 16
                                                93958104867152 (b'colle...): 16
                                             93958104867560 (b'()'): 15
                                 93958104869872 (b'split...): 11
                                    93958104869584 (b'split...): 12
                                       93958104868656 (b'split...): 13
                                       93958104869480 (b'(outp...): 13
                                          93958104868752 (b'outpu...): 14
                                          93958104868944 (b'df'): 14
                                 93958104870896 (b'texts...): 11
                                    93958104870800 (b'texts...): 12
                                       93958104870096 (b'texts...): 13
                                       93958104870400 (b'[]'): 13
                                 93958104872376 (b'if cu...): 11
                                    93958104871536 (b'curre...): 12
                                       93958104871120 (b'curre...): 13
                                       93958104871216 (b'num_t...): 13
                                    93958104872272 (b'break'): 12
                                       93958104872016 (b'break'): 13
                                 93958104873832 (b'if cu...): 11
                                    93958104872896 (b'curre...): 12
                                       93958104872480 (b'curre...): 13
                                       93958104872576 (b'num_t...): 13
                                    93958104873728 (b'break'): 12
                                       93958104873376 (b'break'): 13
         93958104878064 (b'loggi...): 3
            93958104877680 (b'loggi...): 4
               93958104875760 (b'loggi...): 5
                  93958104875552 (b'loggi...): 6
                  93958104875664 (b'info'): 6
               93958104877576 (b'(f"Sa...): 5
                  93958104877168 (b'f"Sav...): 6
                     93958104877056 (b'f"'): 7
                     93958104876640 (b'Saved...): 7
                     93958104876648 (b'{curr...): 7
                        93958104876144 (b'curre...): 8
                     93958104876952 (b' toke...): 7
                     93958104877072 (b'"'): 7
   93958104956360 (b'if __...): 1
      93958104879568 (b'__nam...): 2
         93958104878656 (b'__nam...): 3
         93958104879248 (b'"__ma...): 3
            93958104879136 (b'"'): 4
            93958104879144 (b'__mai...): 4
            93958104879152 (b'"'): 4
      93958104956256 (b'parse...): 2
         93958104880976 (b'parse...): 3
            93958104880880 (b'parse...): 4
               93958104879760 (b'parser'): 5
               93958104880480 (b'argpa...): 5
                  93958104880064 (b'argpa...): 6
                     93958104879856 (b'argpa...): 7
                     93958104879968 (b'Argum...): 7
                  93958104880376 (b'()'): 6
         93958104884144 (b'parse...): 3
            93958104883856 (b'parse...): 4
               93958104881296 (b'parse...): 5
                  93958104881088 (b'parser'): 6
                  93958104881200 (b'add_a...): 6
               93958104883752 (b'(\n  ...): 5
                  93958104881888 (b'"--nu...): 6
                     93958104881776 (b'"'): 7
                     93958104881784 (b'--num...): 7
                     93958104881792 (b'"'): 7
                  93958104882888 (b'help=...): 6
                     93958104882768 (b'help'): 7
                     93958104882576 (b'"Numb...): 7
                        93958104882464 (b'"'): 8
                        93958104882472 (b'Numbe...): 8
                        93958104882480 (b'"'): 8
                  93958104883304 (b'type=...): 6
                     93958104883184 (b'type'): 7
                     93958104882992 (b'str'): 7
         93958104887424 (b'parse...): 3
            93958104887136 (b'parse...): 4
               93958104884576 (b'parse...): 5
                  93958104884368 (b'parser'): 6
                  93958104884480 (b'add_a...): 6
               93958104887032 (b'(\n  ...): 5
                  93958104885168 (b'"--nu...): 6
                     93958104885056 (b'"'): 7
                     93958104885064 (b'--num...): 7
                     93958104885072 (b'"'): 7
                  93958104886168 (b'help=...): 6
                     93958104886048 (b'help'): 7
                     93958104885856 (b'"Tota...): 7
                        93958104885744 (b'"'): 8
                        93958104885752 (b'Total...): 8
                        93958104885760 (b'"'): 8
                  93958104886584 (b'type=...): 6
                     93958104886464 (b'type'): 7
                     93958104886272 (b'str'): 7
         93958104890704 (b'parse...): 3
            93958104890416 (b'parse...): 4
               93958104887856 (b'parse...): 5
                  93958104887648 (b'parser'): 6
                  93958104887760 (b'add_a...): 6
               93958104890312 (b'("--o...): 5
                  93958104888448 (b'"--ou...): 6
                     93958104888336 (b'"'): 7
                     93958104888344 (b'--out...): 7
                     93958104888352 (b'"'): 7
                  93958104889448 (b'help=...): 6
                     93958104889328 (b'help'): 7
                     93958104889136 (b'"Outp...): 7
                        93958104889024 (b'"'): 8
                        93958104889032 (b'Outpu...): 8
                        93958104889040 (b'"'): 8
                  93958104889864 (b'type=...): 6
                     93958104889744 (b'type'): 7
                     93958104889552 (b'str'): 7
         93958104899872 (b'parse...): 3
            93958104899584 (b'parse...): 4
               93958104891136 (b'parse...): 5
                  93958104890928 (b'parser'): 6
                  93958104891040 (b'add_a...): 6
               93958104899480 (b'(\n  ...): 5
                  93958104891728 (b'"--do...): 6
                     93958104891616 (b'"'): 7
                     93958104891624 (b'--dom...): 7
                     93958104891632 (b'"'): 7
                  93958104892728 (b'help=...): 6
                     93958104892608 (b'help'): 7
                     93958104892416 (b'"Doma...): 7
                        93958104892304 (b'"'): 8
                        93958104892312 (b'Domai...): 8
                        93958104892320 (b'"'): 8
                  93958104893144 (b'type=...): 6
                     93958104893024 (b'type'): 7
                     93958104892832 (b'str'): 7
                  93958104899032 (b'choic...): 6
                     93958104898912 (b'choic...): 7
                     93958104898720 (b'[\n  ...): 7
                        93958104893856 (b'"peS2...): 8
                           93958104893744 (b'"'): 9
                           93958104893752 (b'peS2o'): 9
                           93958104893760 (b'"'): 9
                        93958104894544 (b'"comm...): 8
                           93958104894432 (b'"'): 9
                           93958104894440 (b'commo...): 9
                           93958104894448 (b'"'): 9
                        93958104895344 (b'"stac...): 8
                           93958104895232 (b'"'): 9
                           93958104895240 (b'stack...): 9
                           93958104895248 (b'"'): 9
                        93958104896256 (b'"wiki...): 8
                           93958104896144 (b'"'): 9
                           93958104896152 (b'wiki-...): 9
                           93958104896160 (b'"'): 9
                        93958104897168 (b'"c4"'): 8
                           93958104897056 (b'"'): 9
                           93958104897064 (b'c4'): 9
                           93958104897072 (b'"'): 9
                        93958104898080 (b'"gute...): 8
                           93958104897968 (b'"'): 9
                           93958104897976 (b'guten...): 9
                           93958104897984 (b'"'): 9
         93958104903680 (b'parse...): 3
            93958104903392 (b'parse...): 4
               93958104900304 (b'parse...): 5
                  93958104900096 (b'parser'): 6
                  93958104900208 (b'add_a...): 6
               93958104903288 (b'("--s...): 5
                  93958104900896 (b'"--se...): 6
                     93958104900784 (b'"'): 7
                     93958104900792 (b'--seed'): 7
                     93958104900800 (b'"'): 7
                  93958104901896 (b'help=...): 6
                     93958104901776 (b'help'): 7
                     93958104901584 (b'"Rand...): 7
                        93958104901472 (b'"'): 8
                        93958104901480 (b'Rando...): 8
                        93958104901488 (b'"'): 8
                  93958104902312 (b'type=...): 6
                     93958104902192 (b'type'): 7
                     93958104902000 (b'int'): 7
                  93958104902840 (b'defau...): 6
                     93958104902720 (b'defau...): 7
                     93958104902528 (b'42'): 7
         93958104905120 (b'args ...): 3
            93958104905024 (b'args ...): 4
               93958104903904 (b'args'): 5
               93958104904624 (b'parse...): 5
                  93958104904208 (b'parse...): 6
                     93958104904000 (b'parser'): 7
                     93958104904112 (b'parse...): 7
                  93958104904520 (b'()'): 6
         93958104906768 (b'loggi...): 3
            93958104906480 (b'loggi...): 4
               93958104905552 (b'loggi...): 5
                  93958104905344 (b'loggi...): 6
                  93958104905456 (b'basic...): 6
               93958104906376 (b'(leve...): 5
                  93958104906264 (b'level...): 6
                     93958104906048 (b'level'): 7
                     93958104905856 (b'loggi...): 7
                        93958104905648 (b'loggi...): 8
                        93958104905760 (b'INFO'): 8
         93958104909960 (b'if ar...): 3
            93958104907904 (b'args....): 4
               93958104907200 (b'args....): 5
                  93958104906992 (b'args'): 6
                  93958104907104 (b'num_t...): 6
               93958104907600 (b'args....): 5
                  93958104907392 (b'args'): 6
                  93958104907504 (b'num_t...): 6
            93958104909856 (b'raise...): 4
               93958104909600 (b'raise...): 5
                  93958104909200 (b'Value...): 6
                     93958104908096 (b'Value...): 7
                     93958104909096 (b'(\n  ...): 7
                        93958104908688 (b'"Plea...): 8
                           93958104908576 (b'"'): 9
                           93958104908584 (b'Pleas...): 9
                           93958104908592 (b'"'): 9
         93958104930368 (b'if ar...): 3
            93958104910272 (b'args....): 4
               93958104910064 (b'args'): 5
               93958104910176 (b'num_t...): 5
            93958104927600 (b'loggi...): 4
               93958104913184 (b'loggi...): 5
                  93958104912896 (b'loggi...): 6
                     93958104910768 (b'loggi...): 7
                        93958104910560 (b'loggi...): 8
                        93958104910672 (b'info'): 8
                     93958104912792 (b'(f"Fe...): 7
                        93958104912384 (b'f"Fet...): 8
                           93958104912272 (b'f"'): 9
                           93958104911856 (b'Fetch...): 9
                           93958104911864 (b'{args...): 9
                              93958104911360 (b'args....): 10
                                 93958104911152 (b'args'): 11
                                 93958104911264 (b'num_t...): 11
                           93958104912168 (b' toke...): 9
                           93958104912288 (b'"'): 9
               93958104914800 (b'num_t...): 5
                  93958104914608 (b'num_t...): 6
                     93958104913296 (b'num_t...): 7
                     93958104914208 (b'parse...): 7
                        93958104913392 (b'parse...): 8
                        93958104914104 (b'(args...): 8
                           93958104913696 (b'args....): 9
                              93958104913488 (b'args'): 10
                              93958104913600 (b'num_t...): 10
            93958104927608 (b'# Cal...): 4
            93958104923184 (b'elif ...): 4
               93958104916048 (b'args....): 5
                  93958104915344 (b'args....): 6
                     93958104915136 (b'args'): 7
                     93958104915248 (b'num_t...): 7
                  93958104915744 (b'args....): 6
                     93958104915536 (b'args'): 7
                     93958104915648 (b'domain'): 7
               93958104923088 (b'loggi...): 5
                  93958104917840 (b'loggi...): 6
                     93958104917552 (b'loggi...): 7
                        93958104916448 (b'loggi...): 8
                           93958104916240 (b'loggi...): 9
                           93958104916352 (b'info'): 9
                        93958104917448 (b'("Tot...): 8
                           93958104917040 (b'"Tota...): 9
                              93958104916928 (b'"'): 10
                              93958104916936 (b'Total...): 10
                              93958104916944 (b'"'): 10
                  93958104922720 (b'num_t...): 6
                     93958104922528 (b'num_t...): 7
                        93958104917952 (b'num_t...): 8
                        93958104922128 (b'(\n  ...): 8
                           93958104921728 (b'(\n  ...): 9
                              93958104921424 (b'int(\...): 10
                                 93958104921120 (b'int(\...): 11
                                    93958104918048 (b'int'): 12
                                    93958104921016 (b'(\n  ...): 12
                                       93958104920704 (b'(\n  ...): 13
                                          93958104920400 (b'(pars...): 14
                                             93958104919472 (b'(pars...): 15
                                                93958104919168 (b'parse...): 16
                                                   93958104918864 (b'parse...): 17
                                                      93958104918144 (b'parse...): 18
                                                      93958104918760 (b'(args...): 18
                                                         93958104918448 (b'args....): 19
                                                            93958104918240 (b'args'): 20
                                                            93958104918352 (b'num_t...): 20
                                                   93958104918960 (b'10_00...): 17
                                             93958104920192 (b'TOKEN...): 15
                                                93958104919568 (b'TOKEN...): 16
                                                93958104919872 (b'args....): 16
                                                   93958104919664 (b'args'): 17
                                                   93958104919776 (b'domain'): 17
                                 93958104921216 (b'1_000...): 11
                           93958104921920 (b'1_000...): 9
            93958104927464 (b'# Cal...): 4
            93958104927360 (b'elif ...): 4
               93958104923488 (b'args....): 5
                  93958104923280 (b'args'): 6
                  93958104923392 (b'domain'): 6
               93958104927264 (b'loggi...): 5
                  93958104925376 (b'loggi...): 6
                     93958104925088 (b'loggi...): 7
                        93958104923984 (b'loggi...): 8
                           93958104923776 (b'loggi...): 9
                           93958104923888 (b'info'): 9
                        93958104924984 (b'("Tot...): 8
                           93958104924576 (b'"Tota...): 9
                              93958104924464 (b'"'): 10
                              93958104924472 (b'Total...): 10
                              93958104924480 (b'"'): 10
                  93958104926896 (b'num_t...): 6
                     93958104926704 (b'num_t...): 7
                        93958104925488 (b'num_t...): 8
                        93958104926304 (b'TOKEN...): 8
                           93958104925584 (b'TOKEN...): 9
                           93958104925888 (b'args....): 9
                              93958104925680 (b'args'): 10
                              93958104925792 (b'domain'): 10
         93958104930376 (b'# the...): 3
         93958104930256 (b'all_f...): 3
            93958104930160 (b'all_f...): 4
               93958104927824 (b'all_f...): 5
               93958104929760 (b'reque...): 5
                  93958104929344 (b'reque...): 6
                     93958104929136 (b'reque...): 7
                        93958104928128 (b'reque...): 8
                           93958104927920 (b'reque...): 9
                           93958104928032 (b'get'): 9
                        93958104929032 (b'("htt...): 8
                           93958104928720 (b'"http...): 9
                              93958104928608 (b'"'): 10
                              93958104928616 (b'http:...): 10
                              93958104928624 (b'"'): 10
                     93958104929248 (b'json'): 7
                  93958104929656 (b'()'): 6
         93958104956008 (b'if ar...): 3
            93958104930688 (b'args....): 4
               93958104930480 (b'args'): 5
               93958104930592 (b'domain'): 5
            93958104955888 (b'fetch...): 4
               93958104934720 (b'fetch...): 5
                  93958104934336 (b'fetch...): 6
                     93958104930976 (b'fetch...): 7
                     93958104934232 (b'(\n  ...): 7
                        93958104934104 (b'num_t...): 8
                           93958104931264 (b'num_t...): 9
                           93958104931072 (b'num_t...): 9
                        93958104931896 (b'domai...): 8
                           93958104931776 (b'domain'): 9
                           93958104931584 (b'args....): 9
                              93958104931376 (b'args'): 10
                              93958104931488 (b'domain'): 10
                        93958104932520 (b'outpu...): 8
                           93958104932400 (b'outpu...): 9
                           93958104932208 (b'args....): 9
                              93958104932000 (b'args'): 10
                              93958104932112 (b'output'): 10
                        93958104933048 (b'all_f...): 8
                           93958104932928 (b'all_f...): 9
                           93958104932736 (b'all_f...): 9
                        93958104933784 (b'seed=...): 8
                           93958104933664 (b'seed'): 9
                           93958104933472 (b'args....): 9
                              93958104933264 (b'args'): 10
                              93958104933376 (b'seed'): 10
            93958104955896 (b'else:...): 4
               93958104955752 (b'loggi...): 5
                  93958104936640 (b'loggi...): 6
                     93958104936352 (b'loggi...): 7
                        93958104935248 (b'loggi...): 8
                           93958104935040 (b'loggi...): 9
                           93958104935152 (b'info'): 9
                        93958104936248 (b'("Fet...): 8
                           93958104935840 (b'"Fetc...): 9
                              93958104935728 (b'"'): 10
                              93958104935736 (b'Fetch...): 10
                              93958104935744 (b'"'): 10
                  93958104955416 (b'for d...): 6
                     93958104936752 (b'domain'): 7
                     93958104937376 (b'TOKEN...): 7
                        93958104937056 (b'TOKEN...): 8
                           93958104936848 (b'TOKEN...): 9
                           93958104936960 (b'keys'): 9
                        93958104937272 (b'()'): 8
                     93958104955312 (b'loggi...): 7
                        93958104939776 (b'loggi...): 8
                           93958104939488 (b'loggi...): 9
                              93958104937872 (b'loggi...): 10
                                 93958104937664 (b'loggi...): 11
                                 93958104937776 (b'info'): 11
                              93958104939384 (b'(f"Fe...): 10
                                 93958104938976 (b'f"Fet...): 11
                                    93958104938864 (b'f"'): 12
                                    93958104938752 (b'Fetch...): 12
                                    93958104938760 (b'{doma...): 12
                                       93958104938256 (b'domain'): 13
                                    93958104938880 (b'"'): 12
                        93958104950616 (b'if ar...): 8
                           93958104940096 (b'args....): 9
                              93958104939888 (b'args'): 10
                              93958104940000 (b'num_t...): 10
                           93958104950496 (b'loggi...): 9
                              93958104941984 (b'loggi...): 10
                                 93958104941696 (b'loggi...): 11
                                    93958104940592 (b'loggi...): 12
                                       93958104940384 (b'loggi...): 13
                                       93958104940496 (b'info'): 13
                                    93958104941592 (b'("Cal...): 12
                                       93958104941184 (b'"Calc...): 13
                                          93958104941072 (b'"'): 14
                                          93958104941080 (b'Calcu...): 14
                                          93958104941088 (b'"'): 14
                              93958104946656 (b'num_t...): 10
                                 93958104946464 (b'num_t...): 11
                                    93958104942096 (b'num_t...): 12
                                    93958104946064 (b'(\n  ...): 12
                                       93958104945664 (b'(\n  ...): 13
                                          93958104945360 (b'int(\...): 14
                                             93958104945056 (b'int(\...): 15
                                                93958104942192 (b'int'): 16
                                                93958104944952 (b'(\n  ...): 16
                                                   93958104944640 (b'(\n  ...): 17
                                                      93958104944336 (b'(pars...): 18
                                                         93958104943616 (b'(pars...): 19
                                                            93958104943312 (b'parse...): 20
                                                               93958104943008 (b'parse...): 21
                                                                  93958104942288 (b'parse...): 22
                                                                  93958104942904 (b'(args...): 22
                                                                     93958104942592 (b'args....): 23
                                                                        93958104942384 (b'args'): 24
                                                                        93958104942496 (b'num_t...): 24
                                                               93958104943104 (b'10_00...): 21
                                                         93958104944128 (b'TOKEN...): 19
                                                            93958104943712 (b'TOKEN...): 20
                                                            93958104943808 (b'domain'): 20
                                             93958104945152 (b'1_000...): 15
                                       93958104945856 (b'1_000...): 13
                           93958104950504 (b'else:...): 9
                              93958104950360 (b'loggi...): 10
                                 93958104948688 (b'loggi...): 11
                                    93958104948400 (b'loggi...): 12
                                       93958104947296 (b'loggi...): 13
                                          93958104947088 (b'loggi...): 14
                                          93958104947200 (b'info'): 14
                                       93958104948296 (b'("Cal...): 13
                                          93958104947888 (b'"Calc...): 14
                                             93958104947776 (b'"'): 15
                                             93958104947784 (b'Calcu...): 15
                                             93958104947792 (b'"'): 15
                                 93958104950000 (b'num_t...): 11
                                    93958104949808 (b'num_t...): 12
                                       93958104948800 (b'num_t...): 13
                                       93958104949408 (b'TOKEN...): 13
                                          93958104948896 (b'TOKEN...): 14
                                          93958104948992 (b'domain'): 14
                        93958104954832 (b'fetch...): 8
                           93958104954448 (b'fetch...): 9
                              93958104950720 (b'fetch...): 10
                              93958104954344 (b'(\n  ...): 10
                                 93958104954216 (b'num_t...): 11
                                    93958104951008 (b'num_t...): 12
                                    93958104950816 (b'num_t...): 12
                                 93958104951432 (b'domai...): 11
                                    93958104951312 (b'domain'): 12
                                    93958104951120 (b'domain'): 12
                                 93958104953368 (b'# Sli...): 11
                                 93958104953376 (b'outpu...): 11
                                    93958104953248 (b'outpu...): 12
                                    93958104953056 (b'args....): 12
                                       93958104951744 (b'args....): 13
                                          93958104951536 (b'args'): 14
                                          93958104951648 (b'output'): 14
                                       93958104952848 (b'f"/{d...): 13
                                          93958104952736 (b'f"'): 14
                                          93958104952624 (b'/'): 14
                                          93958104952632 (b'{doma...): 14
                                             93958104952128 (b'domain'): 15
                                          93958104952752 (b'"'): 14
                                 93958104953896 (b'all_f...): 11
                                    93958104953776 (b'all_f...): 12
                                    93958104953584 (b'all_f...): 12
--------------------------------------------------
Tree depth: 24
--------------------------------------------------
Num words: 1348
