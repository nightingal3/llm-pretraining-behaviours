{
    "hendrycksTest-econometrics_5-shot_acc": [
        [
            "mistralai/Mixtral-8x7B-v0.1",
            0.2088066365635186
        ],
        [
            "jb723/cross_lingual_epoch2",
            0.1859302055417445
        ]
    ],
    "hendrycksTest-virology_5-shot_acc": [
        [
            "openai-community/gpt2",
            0.1792234110544963
        ]
    ],
    "hendrycksTest-high_school_statistics_5-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.3296465189368637
        ]
    ],
    "hendrycksTest-conceptual_physics_5-shot_acc": [
        [
            "kevin009/flyingllama-v2",
            0.2020491783923291
        ]
    ],
    "hendrycksTest-professional_medicine_5-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.364224440911237
        ]
    ],
    "hendrycksTest-professional_accounting_5-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.163067248273403
        ]
    ],
    "hendrycksTest-public_relations_5-shot_acc": [
        [
            "openai-community/gpt2",
            0.2681790297681636
        ]
    ],
    "hendrycksTest-abstract_algebra_5-shot_acc": [
        [
            "kevin009/flyingllama-v2",
            0.1125764226913452
        ],
        [
            "Monero/WizardLM-13b-OpenAssistant-Uncensored",
            0.0823343116044998
        ]
    ],
    "hendrycksTest-human_aging_5-shot_acc": [
        [
            "kevin009/flyingllama-v2",
            0.3355154608931777
        ]
    ],
    "hendrycksTest-moral_scenarios_5-shot_acc": [
        [
            "allenai/tulu-2-dpo-70b",
            0.1639072104539285
        ]
    ],
    "arc:challenge_25-shot_acc": [
        [
            "kevin009/flyingllama-v2",
            0.2433072464864815
        ]
    ],
    "hellaswag_10-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.2640680843462295
        ],
        [
            "kevin009/flyingllama-v2",
            0.2514298466827926
        ]
    ],
    "hendrycksTest-machine_learning_5-shot_acc": [
        [
            "kevin009/flyingllama-v2",
            0.1984105706214904
        ]
    ],
    "gsm8k_5-shot_acc": [
        [
            "mistralai/Mixtral-8x7B-v0.1",
            0.4792183277573524
        ]
    ],
    "hendrycksTest-high_school_physics_5-shot_acc": [
        [
            "mistralai/Mixtral-8x7B-v0.1",
            0.0935464450065663
        ],
        [
            "Monero/WizardLM-13b-OpenAssistant-Uncensored",
            0.1145205132613908
        ]
    ],
    "hendrycksTest-global_facts_5-shot_acc": [
        [
            "kevin009/flyingllama-v2",
            0.1297256433963775
        ],
        [
            "openai-community/gpt2",
            0.2043407618999481
        ]
    ],
    "hendrycksTest-high_school_mathematics_5-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.1122745052531913
        ]
    ],
    "hendrycksTest-college_mathematics_5-shot_acc": [
        [
            "jisukim8873/falcon-7B-case-0",
            0.1035371804237365
        ]
    ],
    "hendrycksTest-college_physics_5-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.1524829741786508
        ]
    ],
    "winogrande_5-shot_acc": [
        [
            "mistralai/Mixtral-8x7B-v0.1",
            0.1808972881831987
        ],
        [
            "kevin009/flyingllama-v2",
            0.1919098555712409
        ]
    ],
    "hendrycksTest-high_school_chemistry_5-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.2219981722937429
        ]
    ],
    "hendrycksTest-world_religions_5-shot_acc": [
        [
            "kevin009/flyingllama-v2",
            0.4087327739648652
        ]
    ],
    "hendrycksTest-college_chemistry_5-shot_acc": [
        [
            "jb723/cross_lingual_epoch2",
            0.2556562197208404
        ]
    ]
}