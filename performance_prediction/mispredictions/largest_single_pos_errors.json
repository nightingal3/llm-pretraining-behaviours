[
  [
    "LLM360/Amber",
    "hendrycksTest-high_school_biology_5-shot_acc",
    0.1670281398680902
  ],
  [
    "facebook/xglm-564M",
    "hendrycksTest-high_school_geography_5-shot_acc",
    0.1681697576334982
  ],
  [
    "openlm-research/open_llama_3b_v2",
    "hendrycksTest-marketing_5-shot_acc",
    0.1682663091227539
  ],
  [
    "Deci/DeciCoder-1b",
    "hendrycksTest-security_studies_5-shot_acc",
    0.1687779863269962
  ],
  [
    "facebook/xglm-7.5B",
    "hendrycksTest-marketing_5-shot_acc",
    0.1704327655144226
  ],
  [
    "facebook/xglm-564M",
    "hendrycksTest-formal_logic_5-shot_acc",
    0.1717321489538466
  ],
  [
    "facebook/opt-6.7b",
    "hendrycksTest-high_school_statistics_5-shot_acc",
    0.1729237415172435
  ],
  [
    "cerebras/Cerebras-GPT-13B",
    "hendrycksTest-professional_medicine_5-shot_acc",
    0.1738989984287936
  ],
  [
    "Deci/DeciCoder-1b",
    "hendrycksTest-high_school_biology_5-shot_acc",
    0.174627495581104
  ],
  [
    "Deci/DeciCoder-1b",
    "hendrycksTest-professional_medicine_5-shot_acc",
    0.1748481848660637
  ],
  [
    "cerebras/Cerebras-GPT-13B",
    "hendrycksTest-high_school_geography_5-shot_acc",
    0.1750555059524498
  ],
  [
    "EleutherAI/gpt-neo-1.3B",
    "hendrycksTest-high_school_geography_5-shot_acc",
    0.1757799790363119
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-marketing_5-shot_acc",
    0.1759511940499656
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-logical_fallacies_5-shot_acc",
    0.1762704207487633
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-sociology_5-shot_acc",
    0.1770277794320784
  ],
  [
    "facebook/xglm-7.5B",
    "hendrycksTest-management_5-shot_acc",
    0.1818595534389459
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-jurisprudence_5-shot_acc",
    0.1827517449855804
  ],
  [
    "facebook/xglm-7.5B",
    "hendrycksTest-human_sexuality_5-shot_acc",
    0.1863865622582326
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-miscellaneous_5-shot_acc",
    0.1865949309877052
  ],
  [
    "EleutherAI/pythia-1b-deduped",
    "hendrycksTest-high_school_statistics_5-shot_acc",
    0.1880388855934143
  ],
  [
    "Deci/DeciCoder-1b",
    "hendrycksTest-high_school_statistics_5-shot_acc",
    0.1895267985485218
  ],
  [
    "facebook/xglm-7.5B",
    "hendrycksTest-jurisprudence_5-shot_acc",
    0.1899166625958902
  ],
  [
    "mosaicml/mpt-7b",
    "hendrycksTest-high_school_geography_5-shot_acc",
    0.1902070211039649
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-professional_medicine_5-shot_acc",
    0.1905549466609954
  ],
  [
    "bigscience/bloom-1b7",
    "hendrycksTest-human_aging_5-shot_acc",
    0.1909484910056195
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-clinical_knowledge_5-shot_acc",
    0.1918932230967396
  ],
  [
    "cerebras/Cerebras-GPT-13B",
    "hendrycksTest-high_school_government_and_politics_5-shot_acc",
    0.1938502161614018
  ],
  [
    "cerebras/Cerebras-GPT-13B",
    "hendrycksTest-human_sexuality_5-shot_acc",
    0.1940201500444922
  ],
  [
    "openlm-research/open_llama_3b_v2",
    "hendrycksTest-professional_medicine_5-shot_acc",
    0.194779271588606
  ],
  [
    "openlm-research/open_llama_3b_v2",
    "hendrycksTest-medical_genetics_5-shot_acc",
    0.1964273941516876
  ],
  [
    "cerebras/Cerebras-GPT-13B",
    "hendrycksTest-management_5-shot_acc",
    0.2012770291671012
  ],
  [
    "EleutherAI/gpt-j-6b",
    "hendrycksTest-high_school_statistics_5-shot_acc",
    0.2049987393396872
  ],
  [
    "cerebras/Cerebras-GPT-13B",
    "hendrycksTest-us_foreign_policy_5-shot_acc",
    0.2059953606128692
  ],
  [
    "facebook/xglm-7.5B",
    "hendrycksTest-us_foreign_policy_5-shot_acc",
    0.2059953606128692
  ],
  [
    "tiiuae/falcon-7b",
    "hendrycksTest-high_school_statistics_5-shot_acc",
    0.2087417675389183
  ],
  [
    "rinna/bilingual-gpt-neox-4b",
    "hendrycksTest-high_school_statistics_5-shot_acc",
    0.2137059999836815
  ],
  [
    "Dampish/StellarX-4B-V0",
    "hendrycksTest-human_aging_5-shot_acc",
    0.2329894325658345
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-security_studies_5-shot_acc",
    0.2330332714684155
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-high_school_geography_5-shot_acc",
    0.2333083011285223
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-high_school_psychology_5-shot_acc",
    0.2507374535459991
  ],
  [
    "EleutherAI/gpt-neo-1.3B",
    "hendrycksTest-professional_medicine_5-shot_acc",
    0.2550722746288075
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-management_5-shot_acc",
    0.2799179458502427
  ],
  [
    "LLM360/Amber",
    "hendrycksTest-high_school_government_and_politics_5-shot_acc",
    0.2905342548004704
  ]
]