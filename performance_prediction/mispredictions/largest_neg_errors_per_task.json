{
  "arc:challenge_25-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1066419449682528
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2475755127216768
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2758276804315352
    ]
  ],
  "gsm8k_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.2314079490866599
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.5610920767652166
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2952049629899749
    ]
  ],
  "hellaswag_10-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.0650000573866539
    ],
    [
      "Monero/WizardLM-13b-OpenAssistant-Uncensored",
      -0.1275807293055898
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2608580233637162
    ]
  ],
  "hendrycksTest-abstract_algebra_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.0495820009708404
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2075961327552795
    ],
    [
      "jisukim8873/falcon-7B-case-0",
      -0.0449354720115661
    ]
  ],
  "hendrycksTest-anatomy_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2025791607521198
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1746269694081059
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3099640033863209
    ]
  ],
  "hendrycksTest-astronomy_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.352641717383736
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.485287501623756
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2611958447255587
    ]
  ],
  "hendrycksTest-business_ethics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.256412456035614
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4766156637668609
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2310422766208648
    ]
  ],
  "hendrycksTest-clinical_knowledge_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2788100893767374
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1669937255247584
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4777190414239776
    ]
  ],
  "hendrycksTest-college_biology_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3411826822492811
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.2145155999395582
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.5555923382441202
    ]
  ],
  "hendrycksTest-college_chemistry_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2259510266780853
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2474781858921051
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2400904095172882
    ]
  ],
  "hendrycksTest-college_computer_science_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.1963288474082947
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.309867912530899
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.158524956703186
    ]
  ],
  "hendrycksTest-college_mathematics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.0796195185184479
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.0716920912265777
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.0793728971481323
    ]
  ],
  "hendrycksTest-college_medicine_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2759231831986091
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4070532330543319
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.295347445962057
    ]
  ],
  "hendrycksTest-college_physics_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.1376431061356675
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1024189866056628
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.281558514810076
    ]
  ],
  "hendrycksTest-computer_security_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1599206304550171
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4140849769115448
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2356782865524291
    ]
  ],
  "hendrycksTest-conceptual_physics_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.1765991487401597
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1643372393668966
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3868956882902916
    ]
  ],
  "hendrycksTest-econometrics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.2088066365635186
    ],
    [
      "mistralai/Mistral-7B-v0.1",
      -0.0913053750991821
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2656942266121245
    ]
  ],
  "hendrycksTest-electrical_engineering_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.276504794482527
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1735689516725211
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4329474969156857
    ]
  ],
  "hendrycksTest-elementary_mathematics_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.1389186339718954
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1263803133258112
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4589020256642941
    ]
  ],
  "hendrycksTest-formal_logic_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.178507412709887
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1922261454756297
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2625181698609912
    ]
  ],
  "hendrycksTest-global_facts_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1541395878791809
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2004503786563873
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.0833769929409027
    ]
  ],
  "hendrycksTest-high_school_biology_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3509382313297641
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.5336749822862685
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2105216203197356
    ]
  ],
  "hendrycksTest-high_school_chemistry_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2070407843942125
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1671643892826118
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3003368693325907
    ]
  ],
  "hendrycksTest-high_school_computer_science_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3211474251747132
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4408657729625702
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3542191076278687
    ]
  ],
  "hendrycksTest-high_school_european_history_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.307527224222819
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.422163394906304
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.4264398444782603
    ]
  ],
  "hendrycksTest-high_school_geography_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3915695373458091
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.5506238368424503
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.4329846733146243
    ]
  ],
  "hendrycksTest-high_school_government_and_politics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.188298346773948
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4988492440065571
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.4462680540245431
    ]
  ],
  "hendrycksTest-high_school_macroeconomics_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2708146339807755
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4914094990644699
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2273286422093708
    ]
  ],
  "hendrycksTest-high_school_mathematics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.0655032725245864
    ],
    [
      "mistralai/Mistral-7B-v0.1",
      -0.0307762724381905
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.186937904799426
    ]
  ],
  "hendrycksTest-high_school_microeconomics_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2507508447190293
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.494491956314119
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2355348648143416
    ]
  ],
  "hendrycksTest-high_school_physics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1709478446189931
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.237581956662879
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.0697075570655973
    ]
  ],
  "hendrycksTest-high_school_psychology_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.4032035454697565
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4470009307248876
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.4058002120857938
    ]
  ],
  "hendrycksTest-high_school_statistics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1286868143964696
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3329598958845492
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.1754526683577785
    ]
  ],
  "hendrycksTest-high_school_us_history_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1444134770655164
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.5410688472729104
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.5016414026419321
    ]
  ],
  "hendrycksTest-high_school_world_history_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.2038679442325222
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4702378286339562
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3213467567781859
    ]
  ],
  "hendrycksTest-human_aging_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1497370320050706
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3351395056653985
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3563447041361856
    ]
  ],
  "hendrycksTest-human_sexuality_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3555277171935743
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4681004481461212
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.4052844807391859
    ]
  ],
  "hendrycksTest-international_law_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3848624052095019
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3974554671728906
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3308850024357315
    ]
  ],
  "hendrycksTest-jurisprudence_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3556749765519742
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4579125929761816
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3767739788249686
    ]
  ],
  "hendrycksTest-logical_fallacies_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3156922874640833
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4256933318691019
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.4072053280710442
    ]
  ],
  "hendrycksTest-machine_learning_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.0893547407218388
    ],
    [
      "mistralai/Mistral-7B-v0.1",
      -0.0709116373743329
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2895335342202868
    ]
  ],
  "hendrycksTest-management_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1975901659252574
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.5205379770797433
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.4054153411133775
    ]
  ],
  "hendrycksTest-marketing_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1855682980301033
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4389923163968272
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3082899390122829
    ]
  ],
  "hendrycksTest-medical_genetics_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1422884154319763
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4850639545917511
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2155015492439269
    ]
  ],
  "hendrycksTest-miscellaneous_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1681790736290991
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4512444052507441
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3650252138898442
    ]
  ],
  "hendrycksTest-moral_disputes_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3073594384110731
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3899788663566457
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2567761450833667
    ]
  ],
  "hendrycksTest-moral_scenarios_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.0800876851521391
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3954796279941857
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.064343075379313
    ]
  ],
  "hendrycksTest-nutrition_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3224504993242376
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4275639857731613
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2565466633030012
    ]
  ],
  "hendrycksTest-philosophy_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3286651225335345
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3984929143999167
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3125023317682015
    ]
  ],
  "hendrycksTest-prehistory_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3265966405103235
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.5034693582558337
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.256885243050846
    ]
  ],
  "hendrycksTest-professional_accounting_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.1058847919000801
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3360171172213047
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.1361623793206317
    ]
  ],
  "hendrycksTest-professional_law_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.1250293484878042
    ],
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1237786092845215
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2651676228257792
    ]
  ],
  "hendrycksTest-professional_medicine_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2430755390840417
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4943859138909507
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.2059359550476074
    ]
  ],
  "hendrycksTest-professional_psychology_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.2516362871609482
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4193061179195354
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.1999061411502314
    ]
  ],
  "hendrycksTest-public_relations_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.0984572887420653
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3384762525558471
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3534329452297904
    ]
  ],
  "hendrycksTest-security_studies_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.3155617623913045
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4600613192636139
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3298791848883337
    ]
  ],
  "hendrycksTest-sociology_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.4017610122908407
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4100538671609774
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3905951435589672
    ]
  ],
  "hendrycksTest-us_foreign_policy_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1879861783981323
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.4264028906822205
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3314954686164856
    ]
  ],
  "hendrycksTest-virology_5-shot_acc": [
    [
      "Qwen/Qwen-7B",
      -0.0914703787091266
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.208921348833176
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.1221961160022092
    ]
  ],
  "hendrycksTest-world_religions_5-shot_acc": [
    [
      "mistralai/Mixtral-8x7B-v0.1",
      -0.1582980239600465
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.3974283717529118
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.3960687676716966
    ]
  ],
  "winogrande_5-shot_acc": [
    [
      "AbacusResearch/Jallabi-34B",
      -0.1434045326737973
    ],
    [
      "allenai/tulu-2-dpo-70b",
      -0.0656499621335596
    ],
    [
      "meta-llama/Llama-2-70b-hf",
      -0.0695638483965029
    ]
  ]
}