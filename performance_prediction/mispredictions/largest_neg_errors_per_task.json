{
  "arc:challenge_25-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1408738599702359
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1787855289495031
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1536202939297152
    ]
  ],
  "gsm8k_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-8B-Instruct",
      -0.3514847329749222
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3953530069234428
    ],
    [
      "Qwen/Qwen-7B",
      -0.4078655887234156
    ]
  ],
  "hellaswag_10-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1130487249087964
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1469011363625122
    ],
    [
      "openlm-research/open_llama_3b_v2",
      -0.1172991304301664
    ]
  ],
  "hendrycksTest-abstract_algebra_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1832585740089416
    ],
    [
      "Dampish/StellarX-4B-V0",
      -0.1043686383962631
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1444371438026428
    ]
  ],
  "hendrycksTest-anatomy_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.224915270893662
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2490040333182723
    ],
    [
      "openlm-research/open_llama_7b_v2",
      -0.1711380046826821
    ]
  ],
  "hendrycksTest-astronomy_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.3133806994086817
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4811711876015913
    ],
    [
      "Qwen/Qwen-7B",
      -0.275585369059914
    ]
  ],
  "hendrycksTest-business_ethics_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3950857591629028
    ],
    [
      "Qwen/Qwen-7B",
      -0.2566232311725617
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2826189661026
    ]
  ],
  "hendrycksTest-clinical_knowledge_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3757751307397519
    ],
    [
      "Qwen/Qwen-7B",
      -0.2192155177863138
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2393732693960082
    ]
  ],
  "hendrycksTest-college_biology_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4328165650367737
    ],
    [
      "Qwen/Qwen-7B",
      -0.2526586684915755
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2918807665506998
    ]
  ],
  "hendrycksTest-college_chemistry_5-shot_acc": [
    [
      "EleutherAI/pythia-70m",
      -0.1439765334129333
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2176284742355346
    ],
    [
      "Qwen/Qwen-7B",
      -0.2077188897132873
    ]
  ],
  "hendrycksTest-college_computer_science_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-8B-Instruct",
      -0.1453739964962005
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.305625479221344
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1386859655380249
    ]
  ],
  "hendrycksTest-college_mathematics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.0884062159061431
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1421787333488464
    ],
    [
      "Dampish/StellarX-4B-V0",
      -0.1025557374954223
    ]
  ],
  "hendrycksTest-college_medicine_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1828000331889687
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4072842031200497
    ],
    [
      "Qwen/Qwen-7B",
      -0.2069702353436133
    ]
  ],
  "hendrycksTest-college_physics_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-8B",
      -0.1247642642142725
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1345681857829
    ],
    [
      "facebook/opt-125m",
      -0.132596604379953
    ]
  ],
  "hendrycksTest-computer_security_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2475217151641845
    ],
    [
      "meta-llama/Meta-Llama-3-8B",
      -0.2263475036621094
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2663475036621093
    ]
  ],
  "hendrycksTest-conceptual_physics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.208402391697498
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3710731715598004
    ],
    [
      "Qwen/Qwen-7B",
      -0.2194057145017258
    ]
  ],
  "hendrycksTest-econometrics_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-8B-Instruct",
      -0.1809246383215251
    ],
    [
      "01-ai/Yi-34B",
      -0.0961334924948843
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2898667810256021
    ]
  ],
  "hendrycksTest-electrical_engineering_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.3042188465595246
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3361754855205272
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2212858898886318
    ]
  ],
  "hendrycksTest-elementary_mathematics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.167474477852463
    ],
    [
      "Qwen/Qwen-7B",
      -0.1679834680897848
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.284837747061694
    ]
  ],
  "hendrycksTest-formal_logic_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1349802353079356
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.299347856688121
    ],
    [
      "Qwen/Qwen-7B",
      -0.1213315976044487
    ]
  ],
  "hendrycksTest-global_facts_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1321255755424499
    ],
    [
      "Dampish/StellarX-4B-V0",
      -0.109185026884079
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1718355298042297
    ]
  ],
  "hendrycksTest-high_school_biology_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4521769852407515
    ],
    [
      "Qwen/Qwen-7B",
      -0.2855786686943423
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2879733712442458
    ]
  ],
  "hendrycksTest-high_school_chemistry_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1747841736659627
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.355683167579726
    ],
    [
      "Qwen/Qwen-7B",
      -0.2805225229615649
    ]
  ],
  "hendrycksTest-high_school_computer_science_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2374333810806274
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.42466144323349
    ],
    [
      "Qwen/Qwen-7B",
      -0.2388148140907288
    ]
  ],
  "hendrycksTest-high_school_european_history_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2662826100985209
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2633799831072489
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2820516207001425
    ]
  ],
  "hendrycksTest-high_school_geography_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2607595505136432
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.387526633161487
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2758686867627231
    ]
  ],
  "hendrycksTest-high_school_government_and_politics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2897232579443739
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4064335035536573
    ],
    [
      "Qwen/Qwen-7B",
      -0.2973258847399697
    ]
  ],
  "hendrycksTest-high_school_macroeconomics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2251857399940491
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3762648299718514
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2415512091074234
    ]
  ],
  "hendrycksTest-high_school_mathematics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.0774078932073381
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1912050739482597
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.0722878043298367
    ]
  ],
  "hendrycksTest-high_school_microeconomics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2638656411852155
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2915165654751432
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.261528453907045
    ]
  ],
  "hendrycksTest-high_school_physics_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1005531833661312
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1675604941039685
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.0819524410149908
    ]
  ],
  "hendrycksTest-high_school_psychology_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2687642217776097
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3783005927680829
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2815305690152929
    ]
  ],
  "hendrycksTest-high_school_statistics_5-shot_acc": [
    [
      "facebook/xglm-7.5B",
      -0.123768953261552
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2266923078784236
    ],
    [
      "cerebras/Cerebras-GPT-1.3B",
      -0.141384912861718
    ]
  ],
  "hendrycksTest-high_school_us_history_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.272528330485026
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3068712774445028
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2822420550327675
    ]
  ],
  "hendrycksTest-high_school_world_history_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.283815152785949
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3449190751912724
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2844337893940728
    ]
  ],
  "hendrycksTest-human_aging_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.217156111392205
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2791866692192352
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1958213673044213
    ]
  ],
  "hendrycksTest-human_sexuality_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2488319682710953
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3664334853186862
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2462610798027679
    ]
  ],
  "hendrycksTest-international_law_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2562828985127536
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2336938494493153
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2801306976759729
    ]
  ],
  "hendrycksTest-jurisprudence_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3746371037430234
    ],
    [
      "Qwen/Qwen-7B",
      -0.3130791728143338
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2893426109243322
    ]
  ],
  "hendrycksTest-logical_fallacies_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.28390339583707
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4208297663671108
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2561992096023325
    ]
  ],
  "hendrycksTest-machine_learning_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1311317256518772
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.165167361497879
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1828796352658953
    ]
  ],
  "hendrycksTest-management_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3537594479264565
    ],
    [
      "Qwen/Qwen-7B",
      -0.3032935785437093
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2902137683433236
    ]
  ],
  "hendrycksTest-marketing_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2712507207169492
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3357233965498769
    ],
    [
      "openlm-research/open_llama_7b_v2",
      -0.2890137798256345
    ]
  ],
  "hendrycksTest-medical_genetics_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2790909743309021
    ],
    [
      "Qwen/Qwen-7B",
      -0.2835726058483123
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2582122230529785
    ]
  ],
  "hendrycksTest-miscellaneous_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2641911395512625
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3220834766311208
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2895241889764827
    ]
  ],
  "hendrycksTest-moral_disputes_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2435680179926701
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4105763214861038
    ],
    [
      "Qwen/Qwen-7B",
      -0.2491282634652418
    ]
  ],
  "hendrycksTest-moral_scenarios_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1869679575858835
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.144725054835474
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2230296815906822
    ]
  ],
  "hendrycksTest-nutrition_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2615029134002387
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4110205989647535
    ],
    [
      "Qwen/Qwen-7B",
      -0.2549075887483709
    ]
  ],
  "hendrycksTest-philosophy_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2503801818062638
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3679295569370797
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.237427306136901
    ]
  ],
  "hendrycksTest-prehistory_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2763792173362073
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4078968564669291
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.258263181021184
    ]
  ],
  "hendrycksTest-professional_accounting_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2075315916791875
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1749716863987294
    ],
    [
      "Qwen/Qwen-7B",
      -0.1572778239740547
    ]
  ],
  "hendrycksTest-professional_law_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1651302633571501
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2187717813401054
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.1313663413736002
    ]
  ],
  "hendrycksTest-professional_medicine_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.199316143989563
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.2587481807259952
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2188160279217887
    ]
  ],
  "hendrycksTest-professional_psychology_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2358851175682217
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3590035352831572
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2306530362640331
    ]
  ],
  "hendrycksTest-public_relations_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2271728981624949
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3017537935213609
    ],
    [
      "Qwen/Qwen-7B",
      -0.1861433966593308
    ]
  ],
  "hendrycksTest-security_studies_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2446993326654239
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3667932147882422
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2493032292443878
    ]
  ],
  "hendrycksTest-sociology_5-shot_acc": [
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.4778464095509468
    ],
    [
      "Qwen/Qwen-7B",
      -0.3586111535776907
    ],
    [
      "openlm-research/open_llama_7b_v2",
      -0.2626312621197297
    ]
  ],
  "hendrycksTest-us_foreign_policy_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.245086932182312
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3444514894485473
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2556389808654785
    ]
  ],
  "hendrycksTest-virology_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.1307857848075498
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1408007101840284
    ],
    [
      "openlm-research/open_llama_7b_v2",
      -0.1115855915718767
    ]
  ],
  "hendrycksTest-world_religions_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.2377420747489259
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.3321545727071705
    ],
    [
      "AbacusResearch/Jallabi-34B",
      -0.2868466161147892
    ]
  ],
  "winogrande_5-shot_acc": [
    [
      "01-ai/Yi-34B",
      -0.0972574077540716
    ],
    [
      "meta-llama/Meta-Llama-3-70B",
      -0.1176880853046276
    ],
    [
      "meta-llama/Llama-2-70b-hf",
      -0.0993577665292849
    ]
  ]
}