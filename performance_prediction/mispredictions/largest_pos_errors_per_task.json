{
  "arc:challenge_25-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.326432815794245
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2013785328474467
    ],
    [
      "openai-community/gpt2",
      0.1366747560761487
    ]
  ],
  "gsm8k_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2216829508543014
    ],
    [
      "tiiuae/falcon-7b",
      0.3411062931228172
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2771378092416947
    ]
  ],
  "hellaswag_10-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3484633961086656
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2653126293291397
    ],
    [
      "openai-community/gpt2",
      0.113463763090463
    ]
  ],
  "hendrycksTest-abstract_algebra_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.0987768185138702
    ],
    [
      "kevin009/flyingllama-v2",
      0.1087265288829803
    ],
    [
      "allenai/tulu-2-dpo-70b",
      0.0842652440071106
    ]
  ],
  "hendrycksTest-anatomy_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2920396972585607
    ],
    [
      "tiiuae/falcon-7b",
      0.1937397003173828
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3644491924179925
    ]
  ],
  "hendrycksTest-astronomy_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3301138721014324
    ],
    [
      "tiiuae/falcon-7b",
      0.3261258257062812
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3858441741842973
    ]
  ],
  "hendrycksTest-business_ethics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3364022874832153
    ],
    [
      "openlm-research/open_llama_7b",
      0.1255961191654205
    ],
    [
      "openai-community/gpt2",
      0.2244353461265563
    ]
  ],
  "hendrycksTest-clinical_knowledge_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4481006140978832
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1839496707016567
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2040751767608354
    ]
  ],
  "hendrycksTest-college_biology_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4590955111715529
    ],
    [
      "tiiuae/falcon-7b",
      0.3428049617343479
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3820451166894701
    ]
  ],
  "hendrycksTest-college_chemistry_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2756562197208404
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2556562197208404
    ],
    [
      "huggyllama/llama-7b",
      0.1194399118423462
    ]
  ],
  "hendrycksTest-college_computer_science_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1823035407066345
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1581870245933532
    ],
    [
      "tiiuae/falcon-7b",
      0.2214226233959197
    ]
  ],
  "hendrycksTest-college_mathematics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1855522072315216
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.0903804814815521
    ],
    [
      "jisukim8873/falcon-7B-case-0",
      0.0866456115245819
    ]
  ],
  "hendrycksTest-college_medicine_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3559558567284159
    ],
    [
      "kevin009/flyingllama-v2",
      0.3166421524362068
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3879883692443716
    ]
  ],
  "hendrycksTest-college_physics_5-shot_acc": [
    [
      "jb723/cross_lingual_epoch2",
      0.1524829741786508
    ],
    [
      "kevin009/babyllama-v0.6",
      0.1583894076300602
    ],
    [
      "Monero/WizardLM-13b-OpenAssistant-Uncensored",
      0.1678267010286743
    ]
  ],
  "hendrycksTest-computer_security_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4000793695449829
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2193608903884887
    ],
    [
      "openai-community/gpt2",
      0.407884624004364
    ]
  ],
  "hendrycksTest-conceptual_physics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2975432190489261
    ],
    [
      "kevin009/flyingllama-v2",
      0.209065598756709
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2378909610687418
    ]
  ],
  "hendrycksTest-econometrics_5-shot_acc": [
    [
      "jb723/cross_lingual_epoch2",
      0.1859302055417445
    ],
    [
      "kevin009/flyingllama-v2",
      0.2137407864394941
    ],
    [
      "kevin009/babyllama-v0.6",
      0.1874249969658099
    ]
  ],
  "hendrycksTest-electrical_engineering_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2271454136947105
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1298793241895479
    ],
    [
      "huggyllama/llama-7b",
      0.1148882537052549
    ]
  ],
  "hendrycksTest-elementary_mathematics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1325199548844938
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.0984874115419135
    ],
    [
      "tiiuae/falcon-7b",
      0.142703895846372
    ]
  ],
  "hendrycksTest-formal_logic_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1621146060171582
    ],
    [
      "tiiuae/falcon-7b",
      0.1281984916755132
    ],
    [
      "openai-community/gpt2",
      0.1195886049951826
    ]
  ],
  "hendrycksTest-global_facts_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1767676973342895
    ],
    [
      "kevin009/flyingllama-v2",
      0.0746877574920654
    ],
    [
      "openai-community/gpt2",
      0.170183128118515
    ]
  ],
  "hendrycksTest-high_school_biology_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4523283596961729
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2620057790510116
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3067356805647573
    ]
  ],
  "hendrycksTest-high_school_chemistry_5-shot_acc": [
    [
      "jb723/cross_lingual_epoch2",
      0.2219981722937429
    ],
    [
      "tiiuae/falcon-7b",
      0.2142089846098951
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2264115317114468
    ]
  ],
  "hendrycksTest-high_school_computer_science_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4208372807502746
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2608372807502747
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2409685838222503
    ]
  ],
  "hendrycksTest-high_school_european_history_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3593311284527634
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.3047856739073089
    ],
    [
      "kevin009/babyllama-v0.6",
      0.307913741198453
    ]
  ],
  "hendrycksTest-high_school_geography_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4702342266988273
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.3086180650826656
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2412736879454718
    ]
  ],
  "hendrycksTest-high_school_government_and_politics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4904581299099898
    ],
    [
      "kevin009/flyingllama-v2",
      0.3576888608808963
    ],
    [
      "openai-community/gpt2",
      0.3013720391945518
    ]
  ],
  "hendrycksTest-high_school_macroeconomics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3855611257064036
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.3060739462192242
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3237036319879385
    ]
  ],
  "hendrycksTest-high_school_mathematics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1182169477144877
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1122745052531913
    ],
    [
      "kevin009/babyllama-v0.6",
      0.0517921288808187
    ]
  ],
  "hendrycksTest-high_school_microeconomics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3571775991375707
    ],
    [
      "kevin009/flyingllama-v2",
      0.2544300127931002
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2948998668614556
    ]
  ],
  "hendrycksTest-high_school_physics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1382416344636323
    ],
    [
      "kevin009/babyllama-v0.6",
      0.0900081627021562
    ],
    [
      "Monero/WizardLM-13b-OpenAssistant-Uncensored",
      0.0757965378413926
    ]
  ],
  "hendrycksTest-high_school_psychology_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4962143286652521
    ],
    [
      "kevin009/flyingllama-v2",
      0.342167378893686
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3302890886954212
    ]
  ],
  "hendrycksTest-high_school_statistics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2602020744924193
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.319777837506047
    ],
    [
      "jisukim8873/falcon-7B-case-0",
      0.3040386648089798
    ]
  ],
  "hendrycksTest-high_school_us_history_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4242139739148757
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.3261747582286012
    ],
    [
      "openai-community/gpt2",
      0.3881885409355163
    ]
  ],
  "hendrycksTest-high_school_world_history_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4796763595649462
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2476088490164229
    ],
    [
      "openai-community/gpt2",
      0.3388806835508549
    ]
  ],
  "hendrycksTest-human_aging_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3480208155285617
    ],
    [
      "kevin009/flyingllama-v2",
      0.3481796001639602
    ],
    [
      "kevin009/babyllama-v0.6",
      0.1734058648481497
    ]
  ],
  "hendrycksTest-human_sexuality_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3989933556272784
    ],
    [
      "kevin009/flyingllama-v2",
      0.3887102044265689
    ],
    [
      "kevin009/babyllama-v0.6",
      0.4486602613034139
    ]
  ],
  "hendrycksTest-international_law_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.442391991122695
    ],
    [
      "kevin009/flyingllama-v2",
      0.2129161101727447
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2681650514445029
    ]
  ],
  "hendrycksTest-jurisprudence_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3492963115374247
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2381852004263136
    ],
    [
      "openai-community/gpt2",
      0.2981832248193247
    ]
  ],
  "hendrycksTest-logical_fallacies_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4486996293799278
    ],
    [
      "kevin009/flyingllama-v2",
      0.3979537881956511
    ],
    [
      "kevin009/babyllama-v0.6",
      0.4587002158896324
    ]
  ],
  "hendrycksTest-machine_learning_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2347369236605508
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1338595449924469
    ],
    [
      "kevin009/flyingllama-v2",
      0.2010433077812194
    ]
  ],
  "hendrycksTest-management_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.5111476981524126
    ],
    [
      "kevin009/flyingllama-v2",
      0.5036813223246233
    ],
    [
      "openai-community/gpt2",
      0.3316417145497591
    ]
  ],
  "hendrycksTest-marketing_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4981923857305804
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2720535229413938
    ],
    [
      "openai-community/gpt2",
      0.2177336407013428
    ]
  ],
  "hendrycksTest-medical_genetics_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4477115845680237
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2381346082687377
    ],
    [
      "openai-community/gpt2",
      0.238888840675354
    ]
  ],
  "hendrycksTest-miscellaneous_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4244135189634934
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2935387564497128
    ],
    [
      "openai-community/gpt2",
      0.4894590961674316
    ]
  ],
  "hendrycksTest-moral_disputes_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3603924082193761
    ],
    [
      "kevin009/flyingllama-v2",
      0.2583166011151551
    ],
    [
      "kevin009/babyllama-v0.6",
      0.4006100917138116
    ]
  ],
  "hendrycksTest-moral_scenarios_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2200423787759003
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.0830408064679727
    ],
    [
      "kevin009/babyllama-v0.6",
      0.1000053867281482
    ]
  ],
  "hendrycksTest-nutrition_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3805121023670521
    ],
    [
      "kevin009/flyingllama-v2",
      0.2993392480744256
    ],
    [
      "kevin009/babyllama-v0.6",
      0.4168648575645646
    ]
  ],
  "hendrycksTest-philosophy_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3930033891913975
    ],
    [
      "kevin009/flyingllama-v2",
      0.1881200694194561
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2140408653729982
    ]
  ],
  "hendrycksTest-prehistory_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3990247249603271
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1953210212566234
    ],
    [
      "openai-community/gpt2",
      0.1815125791378963
    ]
  ],
  "hendrycksTest-professional_accounting_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2340478903435647
    ],
    [
      "kevin009/flyingllama-v2",
      0.1903829477357526
    ],
    [
      "kevin009/babyllama-v0.6",
      0.1797446498634122
    ]
  ],
  "hendrycksTest-professional_law_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2119658032172351
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.109598183414305
    ],
    [
      "kevin009/babyllama-v0.6",
      0.1287168788940997
    ]
  ],
  "hendrycksTest-professional_medicine_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3789303232641781
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.364224440911237
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3957164427813362
    ]
  ],
  "hendrycksTest-professional_psychology_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3644514087758033
    ],
    [
      "kevin009/flyingllama-v2",
      0.2442907702689078
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2762498547828276
    ]
  ],
  "hendrycksTest-public_relations_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3197245294397527
    ],
    [
      "kevin009/babyllama-v0.6",
      0.1511068587953394
    ],
    [
      "openai-community/gpt2",
      0.2823444908315486
    ]
  ],
  "hendrycksTest-security_studies_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.3741529430661883
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.2190509022498617
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3166745500905173
    ]
  ],
  "hendrycksTest-sociology_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4588429284332997
    ],
    [
      "openlm-research/open_llama_7b",
      0.2796165456819297
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3784358735701338
    ]
  ],
  "hendrycksTest-us_foreign_policy_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.5520138216018677
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1620138216018677
    ],
    [
      "kevin009/babyllama-v0.6",
      0.2607867205142975
    ]
  ],
  "hendrycksTest-virology_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.2153751304350704
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.0948932027242269
    ],
    [
      "openai-community/gpt2",
      0.1836679804037852
    ]
  ],
  "hendrycksTest-world_religions_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.4674329701920002
    ],
    [
      "kevin009/babyllama-v0.6",
      0.3788530251436066
    ],
    [
      "openai-community/gpt2",
      0.4370199159572
    ]
  ],
  "winogrande_5-shot_acc": [
    [
      "playdev7/theseed-v0.3",
      0.1940434699747255
    ],
    [
      "jb723/cross_lingual_epoch2",
      0.1425778400079416
    ],
    [
      "kevin009/flyingllama-v2",
      0.1103872120051093
    ]
  ]
}