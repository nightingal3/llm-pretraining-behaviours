{
  "arc:challenge_25-shot_acc": [
    [
      "rinna/bilingual-gpt-neox-4b",
      0.09265261288389
    ],
    [
      "cerebras/Cerebras-GPT-13B",
      0.0955216488943979
    ],
    [
      "Deci/DeciCoder-1b",
      0.147715522364138
    ]
  ],
  "gsm8k_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.0577780592613278
    ],
    [
      "facebook/xglm-7.5B",
      0.0596122253146743
    ],
    [
      "LLM360/Amber",
      0.0546633685184212
    ]
  ],
  "hellaswag_10-shot_acc": [
    [
      "EleutherAI/pythia-70m",
      0.0896961158417867
    ],
    [
      "EleutherAI/pythia-70m-deduped",
      0.0903383381597978
    ],
    [
      "Deci/DeciCoder-1b",
      0.1348585623641622
    ]
  ],
  "hendrycksTest-abstract_algebra_5-shot_acc": [
    [
      "mosaicml/mpt-7b",
      0.1059609627723693
    ],
    [
      "facebook/xglm-564M",
      0.0748355865478515
    ],
    [
      "facebook/xglm-4.5B",
      0.0997862994670867
    ]
  ],
  "hendrycksTest-anatomy_5-shot_acc": [
    [
      "rinna/bilingual-gpt-neox-4b",
      0.1279270737259477
    ],
    [
      "EleutherAI/gpt-neo-1.3B",
      0.1316371869157862
    ],
    [
      "LLM360/Amber",
      0.1628982292281257
    ]
  ],
  "hendrycksTest-astronomy_5-shot_acc": [
    [
      "facebook/opt-6.7b",
      0.1623588047529522
    ],
    [
      "cerebras/Cerebras-GPT-13B",
      0.1496776405133699
    ],
    [
      "facebook/xglm-7.5B",
      0.1430986931449488
    ]
  ],
  "hendrycksTest-business_ethics_5-shot_acc": [
    [
      "tiiuae/falcon-7b",
      0.0955514693260193
    ],
    [
      "EleutherAI/pythia-160m-deduped",
      0.1347070205211639
    ],
    [
      "cerebras/Cerebras-GPT-6.7B",
      0.1634380173683166
    ]
  ],
  "hendrycksTest-clinical_knowledge_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1358025558714596
    ],
    [
      "facebook/xglm-564M",
      0.123234900888407
    ],
    [
      "LLM360/Amber",
      0.1918932230967396
    ]
  ],
  "hendrycksTest-college_biology_5-shot_acc": [
    [
      "tiiuae/falcon-7b",
      0.103996776872211
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.1268252697255877
    ],
    [
      "LLM360/Amber",
      0.1286602715651194
    ]
  ],
  "hendrycksTest-college_chemistry_5-shot_acc": [
    [
      "EleutherAI/pythia-1b-deduped",
      0.0956517934799194
    ],
    [
      "tiiuae/falcon-7b",
      0.1006034100055694
    ],
    [
      "facebook/xglm-564M",
      0.1334291875362396
    ]
  ],
  "hendrycksTest-college_computer_science_5-shot_acc": [
    [
      "EleutherAI/gpt-j-6b",
      0.1209890437126159
    ],
    [
      "facebook/xglm-564M",
      0.1588674247264862
    ],
    [
      "Deci/DeciCoder-1b",
      0.1289620375633239
    ]
  ],
  "hendrycksTest-college_mathematics_5-shot_acc": [
    [
      "rinna/bilingual-gpt-neox-4b",
      0.1031869828701019
    ],
    [
      "EleutherAI/pythia-1b-deduped",
      0.0892627418041229
    ],
    [
      "LLM360/Amber",
      0.0996530640125274
    ]
  ],
  "hendrycksTest-college_medicine_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.0968771151380043
    ],
    [
      "EleutherAI/pythia-70m-deduped",
      0.0970367222507564
    ],
    [
      "LLM360/Amber",
      0.1116082985277121
    ]
  ],
  "hendrycksTest-college_physics_5-shot_acc": [
    [
      "facebook/opt-350m",
      0.081244008213866
    ],
    [
      "cerebras/Cerebras-GPT-2.7B",
      0.0654061629491694
    ],
    [
      "EleutherAI/pythia-2.8b-deduped",
      0.0861015083158718
    ]
  ],
  "hendrycksTest-computer_security_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1405253815650939
    ],
    [
      "facebook/xglm-7.5B",
      0.1305253815650939
    ],
    [
      "LLM360/Amber",
      0.1536361098289489
    ]
  ],
  "hendrycksTest-conceptual_physics_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.0912236266947807
    ],
    [
      "facebook/opt-2.7b",
      0.0808657913766008
    ],
    [
      "LLM360/Amber",
      0.0931212245149815
    ]
  ],
  "hendrycksTest-econometrics_5-shot_acc": [
    [
      "EleutherAI/pythia-70m",
      0.0666256301235735
    ],
    [
      "EleutherAI/pythia-1b-deduped",
      0.0685519403532932
    ],
    [
      "Deci/DeciCoder-1b",
      0.070408617718178
    ]
  ],
  "hendrycksTest-electrical_engineering_5-shot_acc": [
    [
      "facebook/xglm-7.5B",
      0.1375846780579666
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.1579967356961349
    ],
    [
      "huggyllama/llama-7b",
      0.1535809962913908
    ]
  ],
  "hendrycksTest-elementary_mathematics_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-2.7B",
      0.0621151108905752
    ],
    [
      "rinna/bilingual-gpt-neox-4b",
      0.0648608579837456
    ],
    [
      "Deci/DeciCoder-1b",
      0.0622424543219268
    ]
  ],
  "hendrycksTest-formal_logic_5-shot_acc": [
    [
      "facebook/xglm-564M",
      0.1717321489538466
    ],
    [
      "facebook/opt-125m",
      0.1158871863569532
    ],
    [
      "facebook/xglm-4.5B",
      0.1399861172078148
    ]
  ],
  "hendrycksTest-global_facts_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-2.7B",
      0.1192407977581024
    ],
    [
      "rinna/bilingual-gpt-neox-4b",
      0.1235120368003845
    ],
    [
      "cerebras/Cerebras-GPT-13B",
      0.1237675726413726
    ]
  ],
  "hendrycksTest-high_school_biology_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1186901644352944
    ],
    [
      "Deci/DeciCoder-1b",
      0.174627495581104
    ],
    [
      "LLM360/Amber",
      0.1670281398680902
    ]
  ],
  "hendrycksTest-high_school_chemistry_5-shot_acc": [
    [
      "rinna/bilingual-gpt-neox-4b",
      0.0959313117665023
    ],
    [
      "Deci/DeciCoder-1b",
      0.1232154905208814
    ],
    [
      "LLM360/Amber",
      0.1204727551913614
    ]
  ],
  "hendrycksTest-high_school_computer_science_5-shot_acc": [
    [
      "facebook/opt-6.7b",
      0.1098027718067169
    ],
    [
      "EleutherAI/gpt-j-6b",
      0.143030172586441
    ],
    [
      "cerebras/Cerebras-GPT-13B",
      0.1340282452106475
    ]
  ],
  "hendrycksTest-high_school_european_history_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.1329087927485958
    ],
    [
      "facebook/xglm-4.5B",
      0.1040304126161518
    ],
    [
      "LLM360/Amber",
      0.1078028346552993
    ]
  ],
  "hendrycksTest-high_school_geography_5-shot_acc": [
    [
      "mosaicml/mpt-7b",
      0.1902070211039649
    ],
    [
      "EleutherAI/gpt-neo-1.3B",
      0.1757799790363119
    ],
    [
      "LLM360/Amber",
      0.2333083011285223
    ]
  ],
  "hendrycksTest-high_school_government_and_politics_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1938502161614018
    ],
    [
      "LLM360/Amber",
      0.2905342548004704
    ],
    [
      "cerebras/Cerebras-GPT-6.7B",
      0.1589616612758043
    ]
  ],
  "hendrycksTest-high_school_macroeconomics_5-shot_acc": [
    [
      "facebook/xglm-564M",
      0.1510186962592296
    ],
    [
      "Deci/DeciCoder-1b",
      0.1532516065316323
    ],
    [
      "LLM360/Amber",
      0.1406030818437919
    ]
  ],
  "hendrycksTest-high_school_mathematics_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1118207130167219
    ],
    [
      "mosaicml/mpt-7b-instruct",
      0.0473331060674456
    ],
    [
      "LLM360/Amber",
      0.0551300719932273
    ]
  ],
  "hendrycksTest-high_school_microeconomics_5-shot_acc": [
    [
      "Deci/DeciCoder-1b",
      0.1297110203935319
    ],
    [
      "facebook/xglm-4.5B",
      0.1015933619326904
    ],
    [
      "LLM360/Amber",
      0.0984250159323716
    ]
  ],
  "hendrycksTest-high_school_physics_5-shot_acc": [
    [
      "facebook/opt-6.7b",
      0.1122364139320045
    ],
    [
      "rinna/bilingual-gpt-neox-4b",
      0.1148208843556461
    ],
    [
      "facebook/xglm-564M",
      0.1160696016636905
    ]
  ],
  "hendrycksTest-high_school_psychology_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.1398197921044236
    ],
    [
      "Deci/DeciCoder-1b",
      0.1442304976489565
    ],
    [
      "LLM360/Amber",
      0.2507374535459991
    ]
  ],
  "hendrycksTest-high_school_statistics_5-shot_acc": [
    [
      "EleutherAI/gpt-j-6b",
      0.2049987393396872
    ],
    [
      "rinna/bilingual-gpt-neox-4b",
      0.2137059999836815
    ],
    [
      "tiiuae/falcon-7b",
      0.2087417675389183
    ]
  ],
  "hendrycksTest-high_school_us_history_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.1528430738869835
    ],
    [
      "EleutherAI/pythia-160m-deduped",
      0.1028266578328376
    ],
    [
      "LLM360/Amber",
      0.0938713059705846
    ]
  ],
  "hendrycksTest-high_school_world_history_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.148194047469127
    ],
    [
      "EleutherAI/gpt-neo-125m",
      0.0895923716860984
    ],
    [
      "LLM360/Amber",
      0.1287693496997849
    ]
  ],
  "hendrycksTest-human_aging_5-shot_acc": [
    [
      "EleutherAI/gpt-neo-2.7B",
      0.1492968349980667
    ],
    [
      "bigscience/bloom-1b7",
      0.1909484910056195
    ],
    [
      "Dampish/StellarX-4B-V0",
      0.2329894325658345
    ]
  ],
  "hendrycksTest-human_sexuality_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1940201500444922
    ],
    [
      "facebook/xglm-7.5B",
      0.1863865622582326
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.1560334212907398
    ]
  ],
  "hendrycksTest-international_law_5-shot_acc": [
    [
      "tiiuae/falcon-7b",
      0.118314555114951
    ],
    [
      "Deci/DeciCoder-1b",
      0.1252261991343222
    ],
    [
      "cerebras/Cerebras-GPT-6.7B",
      0.1205843874245635
    ]
  ],
  "hendrycksTest-jurisprudence_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1528796255588531
    ],
    [
      "facebook/xglm-7.5B",
      0.1899166625958902
    ],
    [
      "LLM360/Amber",
      0.1827517449855804
    ]
  ],
  "hendrycksTest-logical_fallacies_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1086484956229391
    ],
    [
      "mosaicml/mpt-7b",
      0.1025135262977858
    ],
    [
      "LLM360/Amber",
      0.1762704207487633
    ]
  ],
  "hendrycksTest-machine_learning_5-shot_acc": [
    [
      "facebook/opt-350m",
      0.1346607846873148
    ],
    [
      "bigscience/bloom-1b7",
      0.1209514779703958
    ],
    [
      "cerebras/Cerebras-GPT-6.7B",
      0.1140432442937579
    ]
  ],
  "hendrycksTest-management_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.2012770291671012
    ],
    [
      "facebook/xglm-7.5B",
      0.1818595534389459
    ],
    [
      "LLM360/Amber",
      0.2799179458502427
    ]
  ],
  "hendrycksTest-marketing_5-shot_acc": [
    [
      "facebook/xglm-7.5B",
      0.1704327655144226
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.1682663091227539
    ],
    [
      "LLM360/Amber",
      0.1759511940499656
    ]
  ],
  "hendrycksTest-medical_genetics_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1614594769477844
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.1964273941516876
    ],
    [
      "Deci/DeciCoder-1b",
      0.1455240726470947
    ]
  ],
  "hendrycksTest-miscellaneous_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.1561972273187771
    ],
    [
      "facebook/xglm-4.5B",
      0.1190142955055334
    ],
    [
      "LLM360/Amber",
      0.1865949309877052
    ]
  ],
  "hendrycksTest-moral_disputes_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.1209443414831437
    ],
    [
      "Deci/DeciCoder-1b",
      0.0666418208207698
    ],
    [
      "LLM360/Amber",
      0.0742978939431251
    ]
  ],
  "hendrycksTest-moral_scenarios_5-shot_acc": [
    [
      "EleutherAI/pythia-70m",
      0.0441450780996397
    ],
    [
      "cerebras/Cerebras-GPT-1.3B",
      0.046870114783335
    ],
    [
      "facebook/xglm-4.5B",
      0.0630983751246383
    ]
  ],
  "hendrycksTest-nutrition_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.1047380408819984
    ],
    [
      "facebook/opt-2.7b",
      0.1034764234536614
    ],
    [
      "LLM360/Amber",
      0.1237722175573212
    ]
  ],
  "hendrycksTest-philosophy_5-shot_acc": [
    [
      "rinna/bilingual-gpt-neox-4b",
      0.1135202726750511
    ],
    [
      "facebook/xglm-7.5B",
      0.1253390047711192
    ],
    [
      "EleutherAI/gpt-neo-1.3B",
      0.1454357585339684
    ]
  ],
  "hendrycksTest-prehistory_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1438776887493369
    ],
    [
      "facebook/xglm-7.5B",
      0.1130134912184727
    ],
    [
      "LLM360/Amber",
      0.1500197034559132
    ]
  ],
  "hendrycksTest-professional_accounting_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.0495001874494214
    ],
    [
      "EleutherAI/pythia-70m",
      0.0596451402133238
    ],
    [
      "Deci/DeciCoder-1b",
      0.0565013386679034
    ]
  ],
  "hendrycksTest-professional_law_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.0440200407159996
    ],
    [
      "facebook/xglm-7.5B",
      0.0424417384367101
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.0633369623604467
    ]
  ],
  "hendrycksTest-professional_medicine_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.194779271588606
    ],
    [
      "EleutherAI/gpt-neo-1.3B",
      0.2550722746288075
    ],
    [
      "LLM360/Amber",
      0.1905549466609954
    ]
  ],
  "hendrycksTest-professional_psychology_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.1013431702953538
    ],
    [
      "tiiuae/falcon-7b",
      0.0751993794456806
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.0790992817457984
    ]
  ],
  "hendrycksTest-public_relations_5-shot_acc": [
    [
      "EleutherAI/gpt-neo-2.7B",
      0.1354013052853671
    ],
    [
      "rinna/bilingual-gpt-neox-4b",
      0.1312504416162318
    ],
    [
      "LLM360/Amber",
      0.1197032028978521
    ]
  ],
  "hendrycksTest-security_studies_5-shot_acc": [
    [
      "Deci/DeciCoder-1b",
      0.1687779863269962
    ],
    [
      "LLM360/Amber",
      0.2330332714684155
    ],
    [
      "cerebras/Cerebras-GPT-6.7B",
      0.1613238334655761
    ]
  ],
  "hendrycksTest-sociology_5-shot_acc": [
    [
      "openlm-research/open_llama_3b_v2",
      0.1551145175796243
    ],
    [
      "Deci/DeciCoder-1b",
      0.131445897307562
    ],
    [
      "LLM360/Amber",
      0.1770277794320784
    ]
  ],
  "hendrycksTest-us_foreign_policy_5-shot_acc": [
    [
      "cerebras/Cerebras-GPT-13B",
      0.2059953606128692
    ],
    [
      "facebook/xglm-7.5B",
      0.2059953606128692
    ],
    [
      "LLM360/Amber",
      0.157799345254898
    ]
  ],
  "hendrycksTest-virology_5-shot_acc": [
    [
      "EleutherAI/pythia-160m",
      0.0966393358736153
    ],
    [
      "bigscience/bloom-1b7",
      0.0941181965621121
    ],
    [
      "facebook/xglm-4.5B",
      0.1248502400984247
    ]
  ],
  "hendrycksTest-world_religions_5-shot_acc": [
    [
      "EleutherAI/pythia-70m",
      0.1218883858786689
    ],
    [
      "openlm-research/open_llama_3b_v2",
      0.1343441255259932
    ],
    [
      "facebook/opt-2.7b",
      0.1312175257164136
    ]
  ],
  "winogrande_5-shot_acc": [
    [
      "rinna/bilingual-gpt-neox-4b",
      0.0963834751847229
    ],
    [
      "EleutherAI/gpt-neo-125m",
      0.0790852348731085
    ],
    [
      "Deci/DeciCoder-1b",
      0.1006354867424095
    ]
  ]
}