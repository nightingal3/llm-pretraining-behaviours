["AbacusResearch/Jallabi-34B", "gsm8k_5-shot_acc", -0.5610920767652166]
["AbacusResearch/Jallabi-34B", "hendrycksTest-college_biology_5-shot_acc", -0.5555923382441202]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_geography_5-shot_acc", -0.5506238368424503]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_us_history_5-shot_acc", -0.5410688472729104]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_biology_5-shot_acc", -0.5336749822862685]
["AbacusResearch/Jallabi-34B", "hendrycksTest-management_5-shot_acc", -0.5205379770797433]
["AbacusResearch/Jallabi-34B", "hendrycksTest-prehistory_5-shot_acc", -0.5034693582558337]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_us_history_5-shot_acc", -0.5016414026419321]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_government_and_politics_5-shot_acc", -0.4988492440065571]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_microeconomics_5-shot_acc", -0.494491956314119]
["AbacusResearch/Jallabi-34B", "hendrycksTest-professional_medicine_5-shot_acc", -0.4943859138909507]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_macroeconomics_5-shot_acc", -0.4914094990644699]
["AbacusResearch/Jallabi-34B", "hendrycksTest-astronomy_5-shot_acc", -0.485287501623756]
["AbacusResearch/Jallabi-34B", "hendrycksTest-medical_genetics_5-shot_acc", -0.4850639545917511]
["AbacusResearch/Jallabi-34B", "hendrycksTest-clinical_knowledge_5-shot_acc", -0.4777190414239776]
["AbacusResearch/Jallabi-34B", "hendrycksTest-business_ethics_5-shot_acc", -0.4766156637668609]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_world_history_5-shot_acc", -0.4702378286339562]
["AbacusResearch/Jallabi-34B", "hendrycksTest-human_sexuality_5-shot_acc", -0.4681004481461212]
["AbacusResearch/Jallabi-34B", "hendrycksTest-security_studies_5-shot_acc", -0.4600613192636139]
["AbacusResearch/Jallabi-34B", "hendrycksTest-elementary_mathematics_5-shot_acc", -0.4589020256642941]
["AbacusResearch/Jallabi-34B", "hendrycksTest-jurisprudence_5-shot_acc", -0.4579125929761816]
["AbacusResearch/Jallabi-34B", "hendrycksTest-miscellaneous_5-shot_acc", -0.4512444052507441]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_psychology_5-shot_acc", -0.4470009307248876]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_government_and_politics_5-shot_acc", -0.4462680540245431]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_computer_science_5-shot_acc", -0.4408657729625702]
["AbacusResearch/Jallabi-34B", "hendrycksTest-marketing_5-shot_acc", -0.4389923163968272]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_geography_5-shot_acc", -0.4329846733146243]
["AbacusResearch/Jallabi-34B", "hendrycksTest-electrical_engineering_5-shot_acc", -0.4329474969156857]
["AbacusResearch/Jallabi-34B", "hendrycksTest-nutrition_5-shot_acc", -0.4275639857731613]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_european_history_5-shot_acc", -0.4264398444782603]
["AbacusResearch/Jallabi-34B", "hendrycksTest-us_foreign_policy_5-shot_acc", -0.4264028906822205]
["AbacusResearch/Jallabi-34B", "hendrycksTest-logical_fallacies_5-shot_acc", -0.4256933318691019]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_european_history_5-shot_acc", -0.422163394906304]
["AbacusResearch/Jallabi-34B", "hendrycksTest-professional_psychology_5-shot_acc", -0.4193061179195354]
["AbacusResearch/Jallabi-34B", "hendrycksTest-computer_security_5-shot_acc", -0.4140849769115448]
["AbacusResearch/Jallabi-34B", "hendrycksTest-sociology_5-shot_acc", -0.4100538671609774]
["allenai/tulu-2-dpo-70b", "hendrycksTest-logical_fallacies_5-shot_acc", -0.4072053280710442]
["AbacusResearch/Jallabi-34B", "hendrycksTest-college_medicine_5-shot_acc", -0.4070532330543319]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_psychology_5-shot_acc", -0.4058002120857938]
["allenai/tulu-2-dpo-70b", "hendrycksTest-management_5-shot_acc", -0.4054153411133775]
["allenai/tulu-2-dpo-70b", "hendrycksTest-human_sexuality_5-shot_acc", -0.4052844807391859]
["Qwen/Qwen-7B", "hendrycksTest-high_school_psychology_5-shot_acc", -0.4032035454697565]
["Qwen/Qwen-7B", "hendrycksTest-sociology_5-shot_acc", -0.4017610122908407]
["AbacusResearch/Jallabi-34B", "hendrycksTest-philosophy_5-shot_acc", -0.3984929143999167]
["AbacusResearch/Jallabi-34B", "hendrycksTest-international_law_5-shot_acc", -0.3974554671728906]
["AbacusResearch/Jallabi-34B", "hendrycksTest-world_religions_5-shot_acc", -0.3974283717529118]
["allenai/tulu-2-dpo-70b", "hendrycksTest-world_religions_5-shot_acc", -0.3960687676716966]
["AbacusResearch/Jallabi-34B", "hendrycksTest-moral_scenarios_5-shot_acc", -0.3954796279941857]
["Qwen/Qwen-7B", "hendrycksTest-high_school_geography_5-shot_acc", -0.3915695373458091]
["allenai/tulu-2-dpo-70b", "hendrycksTest-sociology_5-shot_acc", -0.3905951435589672]
["AbacusResearch/Jallabi-34B", "hendrycksTest-moral_disputes_5-shot_acc", -0.3899788663566457]
["AbacusResearch/Jallabi-34B", "hendrycksTest-conceptual_physics_5-shot_acc", -0.3868956882902916]
["Qwen/Qwen-7B", "hendrycksTest-international_law_5-shot_acc", -0.3848624052095019]
["allenai/tulu-2-dpo-70b", "hendrycksTest-jurisprudence_5-shot_acc", -0.3767739788249686]
["allenai/tulu-2-dpo-70b", "hendrycksTest-miscellaneous_5-shot_acc", -0.3650252138898442]
["allenai/tulu-2-dpo-70b", "hendrycksTest-human_aging_5-shot_acc", -0.3563447041361856]
["Qwen/Qwen-7B", "hendrycksTest-jurisprudence_5-shot_acc", -0.3556749765519742]
["Qwen/Qwen-7B", "hendrycksTest-human_sexuality_5-shot_acc", -0.3555277171935743]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_computer_science_5-shot_acc", -0.3542191076278687]
["allenai/tulu-2-dpo-70b", "hendrycksTest-public_relations_5-shot_acc", -0.3534329452297904]
["Qwen/Qwen-7B", "hendrycksTest-astronomy_5-shot_acc", -0.352641717383736]
["Qwen/Qwen-7B", "hendrycksTest-high_school_biology_5-shot_acc", -0.3509382313297641]
["Qwen/Qwen-7B", "hendrycksTest-college_biology_5-shot_acc", -0.3411826822492811]
["AbacusResearch/Jallabi-34B", "hendrycksTest-public_relations_5-shot_acc", -0.3384762525558471]
["AbacusResearch/Jallabi-34B", "hendrycksTest-professional_accounting_5-shot_acc", -0.3360171172213047]
["AbacusResearch/Jallabi-34B", "hendrycksTest-human_aging_5-shot_acc", -0.3351395056653985]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_statistics_5-shot_acc", -0.3329598958845492]
["allenai/tulu-2-dpo-70b", "hendrycksTest-us_foreign_policy_5-shot_acc", -0.3314954686164856]
["allenai/tulu-2-dpo-70b", "hendrycksTest-international_law_5-shot_acc", -0.3308850024357315]
["allenai/tulu-2-dpo-70b", "hendrycksTest-security_studies_5-shot_acc", -0.3298791848883337]
["Qwen/Qwen-7B", "hendrycksTest-philosophy_5-shot_acc", -0.3286651225335345]
["Qwen/Qwen-7B", "hendrycksTest-prehistory_5-shot_acc", -0.3265966405103235]
["Qwen/Qwen-7B", "hendrycksTest-nutrition_5-shot_acc", -0.3224504993242376]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_world_history_5-shot_acc", -0.3213467567781859]
["Qwen/Qwen-7B", "hendrycksTest-high_school_computer_science_5-shot_acc", -0.3211474251747132]
["Qwen/Qwen-7B", "hendrycksTest-logical_fallacies_5-shot_acc", -0.3156922874640833]
["Qwen/Qwen-7B", "hendrycksTest-security_studies_5-shot_acc", -0.3155617623913045]
["allenai/tulu-2-dpo-70b", "hendrycksTest-philosophy_5-shot_acc", -0.3125023317682015]
["AbacusResearch/Jallabi-34B", "hendrycksTest-anatomy_5-shot_acc", -0.3099640033863209]
["AbacusResearch/Jallabi-34B", "hendrycksTest-college_computer_science_5-shot_acc", -0.309867912530899]
["allenai/tulu-2-dpo-70b", "hendrycksTest-marketing_5-shot_acc", -0.3082899390122829]
["Qwen/Qwen-7B", "hendrycksTest-high_school_european_history_5-shot_acc", -0.307527224222819]
["Qwen/Qwen-7B", "hendrycksTest-moral_disputes_5-shot_acc", -0.3073594384110731]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_chemistry_5-shot_acc", -0.3003368693325907]
["allenai/tulu-2-dpo-70b", "hendrycksTest-college_medicine_5-shot_acc", -0.295347445962057]
["allenai/tulu-2-dpo-70b", "gsm8k_5-shot_acc", -0.2952049629899749]
["AbacusResearch/Jallabi-34B", "hendrycksTest-machine_learning_5-shot_acc", -0.2895335342202868]
["AbacusResearch/Jallabi-34B", "hendrycksTest-college_physics_5-shot_acc", -0.281558514810076]
["Qwen/Qwen-7B", "hendrycksTest-clinical_knowledge_5-shot_acc", -0.2788100893767374]
["Qwen/Qwen-7B", "hendrycksTest-electrical_engineering_5-shot_acc", -0.276504794482527]
["Qwen/Qwen-7B", "hendrycksTest-college_medicine_5-shot_acc", -0.2759231831986091]
["allenai/tulu-2-dpo-70b", "arc:challenge_25-shot_acc", -0.2758276804315352]
["Qwen/Qwen-7B", "hendrycksTest-high_school_macroeconomics_5-shot_acc", -0.2708146339807755]
["AbacusResearch/Jallabi-34B", "hendrycksTest-econometrics_5-shot_acc", -0.2656942266121245]
["AbacusResearch/Jallabi-34B", "hendrycksTest-professional_law_5-shot_acc", -0.2651676228257792]
["AbacusResearch/Jallabi-34B", "hendrycksTest-formal_logic_5-shot_acc", -0.2625181698609912]
["allenai/tulu-2-dpo-70b", "hendrycksTest-astronomy_5-shot_acc", -0.2611958447255587]
["allenai/tulu-2-dpo-70b", "hellaswag_10-shot_acc", -0.2608580233637162]
["allenai/tulu-2-dpo-70b", "hendrycksTest-prehistory_5-shot_acc", -0.256885243050846]
["allenai/tulu-2-dpo-70b", "hendrycksTest-moral_disputes_5-shot_acc", -0.2567761450833667]
["allenai/tulu-2-dpo-70b", "hendrycksTest-nutrition_5-shot_acc", -0.2565466633030012]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-business_ethics_5-shot_acc", -0.256412456035614]
["Qwen/Qwen-7B", "hendrycksTest-professional_psychology_5-shot_acc", -0.2516362871609482]
["Qwen/Qwen-7B", "hendrycksTest-high_school_microeconomics_5-shot_acc", -0.2507508447190293]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-astronomy_5-shot_acc", -0.2498715551275956]
["AbacusResearch/Jallabi-34B", "arc:challenge_25-shot_acc", -0.2475755127216768]
["AbacusResearch/Jallabi-34B", "hendrycksTest-college_chemistry_5-shot_acc", -0.2474781858921051]
["Qwen/Qwen-7B", "hendrycksTest-professional_medicine_5-shot_acc", -0.2430755390840417]
["allenai/tulu-2-dpo-70b", "hendrycksTest-college_chemistry_5-shot_acc", -0.2400904095172882]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_physics_5-shot_acc", -0.237581956662879]
["allenai/tulu-2-dpo-70b", "hendrycksTest-computer_security_5-shot_acc", -0.2356782865524291]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_microeconomics_5-shot_acc", -0.2355348648143416]
["mistralai/Mixtral-8x7B-v0.1", "gsm8k_5-shot_acc", -0.2314079490866599]
["allenai/tulu-2-dpo-70b", "hendrycksTest-business_ethics_5-shot_acc", -0.2310422766208648]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_macroeconomics_5-shot_acc", -0.2273286422093708]
["Qwen/Qwen-7B", "hendrycksTest-college_chemistry_5-shot_acc", -0.2259510266780853]
["allenai/tulu-2-dpo-70b", "hendrycksTest-medical_genetics_5-shot_acc", -0.2155015492439269]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-college_biology_5-shot_acc", -0.2145155999395582]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_biology_5-shot_acc", -0.2105216203197356]
["AbacusResearch/Jallabi-34B", "hendrycksTest-virology_5-shot_acc", -0.208921348833176]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-econometrics_5-shot_acc", -0.2088066365635186]
["AbacusResearch/Jallabi-34B", "hendrycksTest-abstract_algebra_5-shot_acc", -0.2075961327552795]
["Qwen/Qwen-7B", "hendrycksTest-high_school_chemistry_5-shot_acc", -0.2070407843942125]
["allenai/tulu-2-dpo-70b", "hendrycksTest-professional_medicine_5-shot_acc", -0.2059359550476074]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_world_history_5-shot_acc", -0.2038679442325222]
["Qwen/Qwen-7B", "hendrycksTest-anatomy_5-shot_acc", -0.2025791607521198]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_microeconomics_5-shot_acc", -0.201174395424979]
["allenai/tulu-2-dpo-70b", "hendrycksTest-college_biology_5-shot_acc", -0.2005190187030368]
["AbacusResearch/Jallabi-34B", "hendrycksTest-global_facts_5-shot_acc", -0.2004503786563873]
["allenai/tulu-2-dpo-70b", "hendrycksTest-professional_psychology_5-shot_acc", -0.1999061411502314]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-management_5-shot_acc", -0.1975901659252574]
["Qwen/Qwen-7B", "hendrycksTest-college_computer_science_5-shot_acc", -0.1963288474082947]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-formal_logic_5-shot_acc", -0.1922261454756297]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-professional_medicine_5-shot_acc", -0.1909226179122924]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-prehistory_5-shot_acc", -0.1904814478791789]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_government_and_politics_5-shot_acc", -0.188298346773948]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-us_foreign_policy_5-shot_acc", -0.1879861783981323]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-sociology_5-shot_acc", -0.1879232407209292]
["AbacusResearch/Jallabi-34B", "hendrycksTest-high_school_mathematics_5-shot_acc", -0.186937904799426]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-marketing_5-shot_acc", -0.1855682980301033]
["Qwen/Qwen-7B", "hendrycksTest-formal_logic_5-shot_acc", -0.178507412709887]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-international_law_5-shot_acc", -0.1774427196211065]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-moral_disputes_5-shot_acc", -0.1771798461158841]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_biology_5-shot_acc", -0.1767038983683432]
["Qwen/Qwen-7B", "hendrycksTest-conceptual_physics_5-shot_acc", -0.1765991487401597]
["Qwen/Qwen-7B", "hendrycksTest-business_ethics_5-shot_acc", -0.176412456035614]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-professional_psychology_5-shot_acc", -0.176398264426811]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_geography_5-shot_acc", -0.1762304197658192]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_statistics_5-shot_acc", -0.1754526683577785]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-nutrition_5-shot_acc", -0.1750434531885034]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-anatomy_5-shot_acc", -0.1746269694081059]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-electrical_engineering_5-shot_acc", -0.1735689516725211]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_physics_5-shot_acc", -0.1709478446189931]
["meta-llama/Llama-2-70b-hf", "gsm8k_5-shot_acc", -0.169088880996798]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-miscellaneous_5-shot_acc", -0.1681790736290991]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_chemistry_5-shot_acc", -0.1671643892826118]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-clinical_knowledge_5-shot_acc", -0.1669937255247584]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-philosophy_5-shot_acc", -0.1664821413552263]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_psychology_5-shot_acc", -0.1661709924356653]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-conceptual_physics_5-shot_acc", -0.1643372393668966]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-computer_security_5-shot_acc", -0.1599206304550171]
["allenai/tulu-2-dpo-70b", "hendrycksTest-college_computer_science_5-shot_acc", -0.158524956703186]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-logical_fallacies_5-shot_acc", -0.1584674127262794]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-world_religions_5-shot_acc", -0.1582980239600465]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-security_studies_5-shot_acc", -0.1564593018317709]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-global_facts_5-shot_acc", -0.1541395878791809]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_biology_5-shot_acc", -0.1518157920529765]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-college_computer_science_5-shot_acc", -0.1518129754066467]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-jurisprudence_5-shot_acc", -0.1507036884625753]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-human_aging_5-shot_acc", -0.1497370320050706]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-human_sexuality_5-shot_acc", -0.1484448623111229]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-college_biology_5-shot_acc", -0.1471878091494242]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_us_history_5-shot_acc", -0.1444134770655164]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-nutrition_5-shot_acc", -0.1437763427597245]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_european_history_5-shot_acc", -0.1436991745775396]
["AbacusResearch/Jallabi-34B", "winogrande_5-shot_acc", -0.1434045326737973]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-human_sexuality_5-shot_acc", -0.1429913771971491]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-professional_medicine_5-shot_acc", -0.1429483259425443]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-medical_genetics_5-shot_acc", -0.1422884154319763]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-marketing_5-shot_acc", -0.1420884055969042]
["Qwen/Qwen-7B", "hendrycksTest-elementary_mathematics_5-shot_acc", -0.1389186339718954]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-clinical_knowledge_5-shot_acc", -0.1387317077168878]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-us_foreign_policy_5-shot_acc", -0.1378874635696411]
["Qwen/Qwen-7B", "hendrycksTest-college_physics_5-shot_acc", -0.1376431061356675]
["Qwen/Qwen-7B", "gsm8k_5-shot_acc", -0.136978739111418]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-professional_psychology_5-shot_acc", -0.1367669362647861]
["allenai/tulu-2-dpo-70b", "hendrycksTest-professional_accounting_5-shot_acc", -0.1361623793206317]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-management_5-shot_acc", -0.134377702925969]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-anatomy_5-shot_acc", -0.1328752217469392]
["allenai/tulu-2-dpo-70b", "hendrycksTest-conceptual_physics_5-shot_acc", -0.1325866577473092]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-college_medicine_5-shot_acc", -0.13212547446951]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_psychology_5-shot_acc", -0.1314042373534736]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-sociology_5-shot_acc", -0.1304322460397559]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-college_medicine_5-shot_acc", -0.1295932762195609]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_statistics_5-shot_acc", -0.1286868143964696]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hellaswag_10-shot_acc", -0.1275807293055898]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-prehistory_5-shot_acc", -0.1264863816308387]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-elementary_mathematics_5-shot_acc", -0.1263803133258112]
["allenai/tulu-2-dpo-70b", "hendrycksTest-clinical_knowledge_5-shot_acc", -0.1257492042937369]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_macroeconomics_5-shot_acc", -0.1257196466128031]
["Qwen/Qwen-7B", "hendrycksTest-professional_law_5-shot_acc", -0.1250293484878042]
["openai-community/gpt2", "hendrycksTest-high_school_statistics_5-shot_acc", -0.1249176992310417]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_government_and_politics_5-shot_acc", -0.1246631092975794]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_microeconomics_5-shot_acc", -0.1245137548246303]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-professional_law_5-shot_acc", -0.1237786092845215]
["allenai/tulu-2-dpo-70b", "hendrycksTest-virology_5-shot_acc", -0.1221961160022092]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_european_history_5-shot_acc", -0.1221940820867365]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-world_religions_5-shot_acc", -0.121922322881152]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-world_religions_5-shot_acc", -0.1210693371923345]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-moral_disputes_5-shot_acc", -0.1193367029201089]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_us_history_5-shot_acc", -0.1182644098412757]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-management_5-shot_acc", -0.115268912419532]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-medical_genetics_5-shot_acc", -0.1147212290763854]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_geography_5-shot_acc", -0.1141315915367819]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_world_history_5-shot_acc", -0.1130679362936865]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-computer_security_5-shot_acc", -0.1128379297256469]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-miscellaneous_5-shot_acc", -0.1120734708823768]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-astronomy_5-shot_acc", -0.1104979577817415]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_computer_science_5-shot_acc", -0.1091627192497253]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-international_law_5-shot_acc", -0.1069044889497362]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-security_studies_5-shot_acc", -0.1067954309132634]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_macroeconomics_5-shot_acc", -0.1067465666012886]
["mistralai/Mixtral-8x7B-v0.1", "arc:challenge_25-shot_acc", -0.1066419449682528]
["Qwen/Qwen-7B", "hendrycksTest-professional_accounting_5-shot_acc", -0.1058847919000801]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-college_chemistry_5-shot_acc", -0.1043437802791595]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-logical_fallacies_5-shot_acc", -0.1034476098838759]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_us_history_5-shot_acc", -0.1034049438495262]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-college_physics_5-shot_acc", -0.1024189866056628]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-philosophy_5-shot_acc", -0.1011148677761532]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-jurisprudence_5-shot_acc", -0.101024137602912]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_computer_science_5-shot_acc", -0.1004925894737244]
["Qwen/Qwen-7B", "hendrycksTest-marketing_5-shot_acc", -0.1000982125600179]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_statistics_5-shot_acc", -0.099797084375664]
["allenai/tulu-2-dpo-70b", "hendrycksTest-professional_law_5-shot_acc", -0.0985285298609827]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-public_relations_5-shot_acc", -0.0984572887420653]
["huggyllama/llama-7b", "hendrycksTest-international_law_5-shot_acc", -0.0981945809253976]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-college_chemistry_5-shot_acc", -0.0971787762641906]
["Qwen/Qwen-7B", "hendrycksTest-high_school_government_and_politics_5-shot_acc", -0.0950340980692848]
["allenai/tulu-2-dpo-70b", "hendrycksTest-electrical_engineering_5-shot_acc", -0.094541060102397]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-conceptual_physics_5-shot_acc", -0.0940144448838335]
["huggyllama/llama-7b", "hendrycksTest-professional_medicine_5-shot_acc", -0.0939332807765287]
["mistralai/Mistral-7B-v0.1", "gsm8k_5-shot_acc", -0.09384298198055]
["kevin009/flyingllama-v2", "hendrycksTest-professional_medicine_5-shot_acc", -0.0920882119851954]
["Qwen/Qwen-7B", "hendrycksTest-virology_5-shot_acc", -0.0914703787091266]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-econometrics_5-shot_acc", -0.0913053750991821]
["Qwen/Qwen-7B", "hendrycksTest-management_5-shot_acc", -0.090794049420403]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-machine_learning_5-shot_acc", -0.0893547407218388]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-marketing_5-shot_acc", -0.0883118917799403]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-miscellaneous_5-shot_acc", -0.0855385572060771]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-public_relations_5-shot_acc", -0.0850772120735862]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_chemistry_5-shot_acc", -0.0838333733856971]
["allenai/tulu-2-dpo-70b", "hendrycksTest-global_facts_5-shot_acc", -0.0833769929409027]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-virology_5-shot_acc", -0.0817293582192386]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-moral_scenarios_5-shot_acc", -0.0800876851521391]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_chemistry_5-shot_acc", -0.0799958177388008]
["Qwen/Qwen-7B", "hendrycksTest-computer_security_5-shot_acc", -0.079920630455017]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-college_mathematics_5-shot_acc", -0.0796195185184479]
["allenai/tulu-2-dpo-70b", "hendrycksTest-college_mathematics_5-shot_acc", -0.0793728971481323]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-professional_accounting_5-shot_acc", -0.0780675034996465]
["Qwen/Qwen-7B", "hendrycksTest-miscellaneous_5-shot_acc", -0.0775021898487671]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_world_history_5-shot_acc", -0.0773045102503732]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-professional_accounting_5-shot_acc", -0.076677549181255]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-human_aging_5-shot_acc", -0.0756103955042202]
["huggyllama/llama-7b", "hendrycksTest-sociology_5-shot_acc", -0.0732869633394686]
["Qwen/Qwen-7B", "hendrycksTest-high_school_world_history_5-shot_acc", -0.0730662564688091]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-business_ethics_5-shot_acc", -0.0729814183712005]
["Qwen/Qwen-7B", "hendrycksTest-medical_genetics_5-shot_acc", -0.0722884154319762]
["huggyllama/llama-7b", "hendrycksTest-high_school_european_history_5-shot_acc", -0.0720633853565562]
["AbacusResearch/Jallabi-34B", "hendrycksTest-college_mathematics_5-shot_acc", -0.0716920912265777]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-machine_learning_5-shot_acc", -0.0709116373743329]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_physics_5-shot_acc", -0.0697075570655973]
["meta-llama/Llama-2-70b-hf", "winogrande_5-shot_acc", -0.0695638483965029]
["Qwen/Qwen-7B", "hendrycksTest-us_foreign_policy_5-shot_acc", -0.0679861783981323]
["allenai/tulu-2-dpo-70b", "winogrande_5-shot_acc", -0.0656499621335596]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-high_school_mathematics_5-shot_acc", -0.0655032725245864]
["huggyllama/llama-7b", "hendrycksTest-high_school_psychology_5-shot_acc", -0.065280389949816]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-anatomy_5-shot_acc", -0.0652494759471328]
["mistralai/Mixtral-8x7B-v0.1", "hellaswag_10-shot_acc", -0.0650000573866539]
["allenai/tulu-2-dpo-70b", "hendrycksTest-moral_scenarios_5-shot_acc", -0.064343075379313]
["huggyllama/llama-7b", "hendrycksTest-logical_fallacies_5-shot_acc", -0.0640582672657411]
["allenai/tulu-2-dpo-70b", "hendrycksTest-college_physics_5-shot_acc", -0.0629778761489718]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-electrical_engineering_5-shot_acc", -0.0614420085117735]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-professional_law_5-shot_acc", -0.0603610220835136]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-business_ethics_5-shot_acc", -0.0587973070144652]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_government_and_politics_5-shot_acc", -0.0585574791839085]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-human_aging_5-shot_acc", -0.0581177598692376]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_european_history_5-shot_acc", -0.057528122807994]
["mistralai/Mixtral-8x7B-v0.1", "winogrande_5-shot_acc", -0.0559714363612993]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-us_foreign_policy_5-shot_acc", -0.0549337911605835]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-college_physics_5-shot_acc", -0.0544690723512686]
["kevin009/babyllama-v0.6", "hellaswag_10-shot_acc", -0.0541548545534831]
["allenai/tulu-2-dpo-70b", "hendrycksTest-anatomy_5-shot_acc", -0.0531868903725235]
["Qwen/Qwen-7B", "hendrycksTest-public_relations_5-shot_acc", -0.0530027432875199]
["allenai/tulu-2-dpo-70b", "hendrycksTest-formal_logic_5-shot_acc", -0.0524303903655399]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-college_mathematics_5-shot_acc", -0.0521226859092712]
["AbacusResearch/Jallabi-34B", "hellaswag_10-shot_acc", -0.0502090478555967]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-abstract_algebra_5-shot_acc", -0.0495820009708404]
["huggyllama/llama-7b", "hellaswag_10-shot_acc", -0.0495230205575205]
["jisukim8873/falcon-7B-case-0", "hellaswag_10-shot_acc", -0.0493497199163187]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-formal_logic_5-shot_acc", -0.0482446797310359]
["Qwen/Qwen-7B", "hendrycksTest-high_school_statistics_5-shot_acc", -0.0461845375873424]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-econometrics_5-shot_acc", -0.045365479954502]
["Qwen/Qwen-7B", "hendrycksTest-high_school_physics_5-shot_acc", -0.0451200300494565]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_geography_5-shot_acc", -0.0451138410905395]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-abstract_algebra_5-shot_acc", -0.0449354720115661]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-college_computer_science_5-shot_acc", -0.0445077908039093]
["huggyllama/llama-7b", "hendrycksTest-high_school_world_history_5-shot_acc", -0.0435121938397612]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-abstract_algebra_5-shot_acc", -0.043091492652893]
["mistralai/Mistral-7B-v0.1", "hellaswag_10-shot_acc", -0.0423821230329761]
["Qwen/Qwen-7B", "hendrycksTest-moral_scenarios_5-shot_acc", -0.0414969296428743]
["Qwen/Qwen-7B", "hendrycksTest-econometrics_5-shot_acc", -0.0414420005522275]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "arc:challenge_25-shot_acc", -0.0413901183385491]
["huggyllama/llama-7b", "hendrycksTest-marketing_5-shot_acc", -0.039890437044649]
["mistralai/Mistral-7B-v0.1", "arc:challenge_25-shot_acc", -0.0379410658680131]
["meta-llama/Llama-2-70b-chat-hf", "winogrande_5-shot_acc", -0.0372565740097575]
["huggyllama/llama-7b", "arc:challenge_25-shot_acc", -0.0367994708209313]
["Qwen/Qwen-7B", "hendrycksTest-high_school_us_history_5-shot_acc", -0.0365703398106145]
["Qwen/Qwen-7B", "hendrycksTest-world_religions_5-shot_acc", -0.0354910064161869]
["tiiuae/falcon-7b", "hellaswag_10-shot_acc", -0.0345770369200942]
["tiiuae/falcon-7b", "hendrycksTest-virology_5-shot_acc", -0.0342225536524531]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-public_relations_5-shot_acc", -0.0336105996912176]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_computer_science_5-shot_acc", -0.0330413794517517]
["huggyllama/llama-7b", "hendrycksTest-business_ethics_5-shot_acc", -0.0316132342815398]
["mistralai/Mixtral-8x7B-v0.1", "hendrycksTest-virology_5-shot_acc", -0.0316128213721585]
["huggyllama/llama-7b", "hendrycksTest-world_religions_5-shot_acc", -0.0310206153587987]
["Qwen/Qwen-7B", "arc:challenge_25-shot_acc", -0.0309568570335569]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_mathematics_5-shot_acc", -0.0307762724381905]
["tiiuae/falcon-7b", "hendrycksTest-machine_learning_5-shot_acc", -0.0306049627917153]
["mistralai/Mistral-7B-v0.1", "winogrande_5-shot_acc", -0.0304469542890999]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_psychology_5-shot_acc", -0.0296774789827679]
["huggyllama/llama-7b", "hendrycksTest-astronomy_5-shot_acc", -0.0296159919939542]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_statistics_5-shot_acc", -0.0280815958976745]
["Qwen/Qwen-7B", "hendrycksTest-high_school_mathematics_5-shot_acc", -0.0277468740940093]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-medical_genetics_5-shot_acc", -0.0263198506832122]
["openlm-research/open_llama_7b", "hendrycksTest-abstract_algebra_5-shot_acc", -0.025473403930664]
["kevin009/babyllama-v0.6", "hendrycksTest-global_facts_5-shot_acc", -0.0253122425079345]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-jurisprudence_5-shot_acc", -0.0244830714331732]
["Qwen/Qwen-7B", "hendrycksTest-global_facts_5-shot_acc", -0.0241395878791809]
["allenai/tulu-2-dpo-70b", "hendrycksTest-machine_learning_5-shot_acc", -0.0230257213115692]
["huggyllama/llama-7b", "hendrycksTest-philosophy_5-shot_acc", -0.0228231224408103]
["huggyllama/llama-7b", "hendrycksTest-high_school_government_and_politics_5-shot_acc", -0.0212175790510028]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-elementary_mathematics_5-shot_acc", -0.0210788770327492]
["huggyllama/llama-7b", "winogrande_5-shot_acc", -0.0207747050694057]
["huggyllama/llama-7b", "hendrycksTest-computer_security_5-shot_acc", -0.0199846744537353]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-college_mathematics_5-shot_acc", -0.0196487843990326]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "winogrande_5-shot_acc", -0.0174126214367625]
["tiiuae/falcon-7b", "hendrycksTest-human_aging_5-shot_acc", -0.0167538554411831]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-college_computer_science_5-shot_acc", -0.0161993741989135]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-moral_scenarios_5-shot_acc", -0.0146635107154952]
["huggyllama/llama-7b", "hendrycksTest-moral_disputes_5-shot_acc", -0.0145658082355653]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-global_facts_5-shot_acc", -0.0132085335254669]
["Qwen/Qwen-7B", "hellaswag_10-shot_acc", -0.0126938009684695]
["kevin009/flyingllama-v2", "hendrycksTest-college_mathematics_5-shot_acc", -0.0125689089298248]
["huggyllama/llama-7b", "hendrycksTest-jurisprudence_5-shot_acc", -0.012273887793223]
["huggyllama/llama-7b", "hendrycksTest-professional_psychology_5-shot_acc", -0.0108116631414375]
["kevin009/babyllama-v0.6", "winogrande_5-shot_acc", -0.0107323634125815]
["kevin009/flyingllama-v2", "hendrycksTest-college_physics_5-shot_acc", -0.0106626339987212]
["tiiuae/falcon-7b", "winogrande_5-shot_acc", -0.0105943050832379]
["huggyllama/llama-7b", "hendrycksTest-anatomy_5-shot_acc", -0.0103870959193618]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-global_facts_5-shot_acc", -0.010188992023468]
["allenai/tulu-2-dpo-70b", "hendrycksTest-high_school_mathematics_5-shot_acc", -0.0099623914118166]
["jisukim8873/falcon-7B-case-0", "arc:challenge_25-shot_acc", -0.0098968576449175]
["openai-community/gpt2", "hendrycksTest-professional_medicine_5-shot_acc", -0.0097092021914088]
["kevin009/babyllama-v0.6", "arc:challenge_25-shot_acc", -0.0086657813791528]
["huggyllama/llama-7b", "hendrycksTest-nutrition_5-shot_acc", -0.0085015826755099]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_physics_5-shot_acc", -0.0075486324086094]
["openai-community/gpt2", "hendrycksTest-high_school_mathematics_5-shot_acc", -0.0069010215776937]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-high_school_physics_5-shot_acc", -0.0064364162501909]
["Qwen/Qwen-7B", "hendrycksTest-human_aging_5-shot_acc", -0.0062392741575368]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-computer_security_5-shot_acc", -0.0061567902565002]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-conceptual_physics_5-shot_acc", -0.0052714699126304]
["jisukim8873/falcon-7B-case-0", "winogrande_5-shot_acc", -0.0050694432047849]
["tiiuae/falcon-7b", "hendrycksTest-sociology_5-shot_acc", -0.0049119757775643]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-professional_accounting_5-shot_acc", -0.0048876421671387]
["tiiuae/falcon-7b", "hendrycksTest-public_relations_5-shot_acc", -0.0044837995008988]
["huggyllama/llama-7b", "hendrycksTest-conceptual_physics_5-shot_acc", -0.0039669299379307]
["huggyllama/llama-7b", "hendrycksTest-miscellaneous_5-shot_acc", -0.0038784646439825]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_macroeconomics_5-shot_acc", -0.0037013202141492]
["tiiuae/falcon-7b", "hendrycksTest-college_mathematics_5-shot_acc", -0.0031103980541229]
["openlm-research/open_llama_7b", "hendrycksTest-global_facts_5-shot_acc", -0.0007396805286407]
["kevin009/flyingllama-v2", "hendrycksTest-college_computer_science_5-shot_acc", -0.0006657004356384]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-professional_psychology_5-shot_acc", -3.724900725621261e-05]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.0006086466489015]
["huggyllama/llama-7b", "hendrycksTest-professional_law_5-shot_acc", 0.0015905813023817]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-professional_law_5-shot_acc", 0.0017491251502229]
["jb723/cross_lingual_epoch2", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0027288806438445]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.0028097690680088]
["mistralai/Mistral-7B-v0.1", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0029695844650268]
["huggyllama/llama-7b", "hendrycksTest-medical_genetics_5-shot_acc", 0.0030866014957428]
["openai-community/gpt2", "hendrycksTest-college_mathematics_5-shot_acc", 0.0037396073341369]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.003742845600875]
["huggyllama/llama-7b", "hendrycksTest-human_aging_5-shot_acc", 0.0037639190530563]
["openai-community/gpt2", "hendrycksTest-high_school_physics_5-shot_acc", 0.0037783074457913]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-professional_medicine_5-shot_acc", 0.003901341382195]
["tiiuae/falcon-7b", "arc:challenge_25-shot_acc", 0.0039225997778336]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-conceptual_physics_5-shot_acc", 0.0057229186626191]
["huggyllama/llama-7b", "hendrycksTest-formal_logic_5-shot_acc", 0.0061405443009875]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-international_law_5-shot_acc", 0.0066897839554085]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-logical_fallacies_5-shot_acc", 0.010380888460604]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-machine_learning_5-shot_acc", 0.0114909282752445]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-college_biology_5-shot_acc", 0.0121393965350257]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.0122176458855637]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_european_history_5-shot_acc", 0.0127850994919285]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-sociology_5-shot_acc", 0.01306390139594]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-medical_genetics_5-shot_acc", 0.0130866014957428]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-philosophy_5-shot_acc", 0.0144704147167145]
["openlm-research/open_llama_7b", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.0146513192158824]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-machine_learning_5-shot_acc", 0.0158010167734964]
["huggyllama/llama-7b", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.0159637131610838]
["huggyllama/llama-7b", "hendrycksTest-high_school_physics_5-shot_acc", 0.0161938888347701]
["tiiuae/falcon-7b", "hendrycksTest-electrical_engineering_5-shot_acc", 0.0165607396898598]
["tiiuae/falcon-7b", "hendrycksTest-prehistory_5-shot_acc", 0.0166658634020959]
["huggyllama/llama-7b", "hendrycksTest-security_studies_5-shot_acc", 0.0167168472494398]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.0174560316328732]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-moral_scenarios_5-shot_acc", 0.0178797653267503]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-moral_scenarios_5-shot_acc", 0.0187921068521851]
["huggyllama/llama-7b", "hendrycksTest-college_physics_5-shot_acc", 0.0192815521184136]
["huggyllama/llama-7b", "hendrycksTest-college_biology_5-shot_acc", 0.0198375582695007]
["tiiuae/falcon-7b", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.0204706531983835]
["tiiuae/falcon-7b", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.0204798007011413]
["tiiuae/falcon-7b", "hendrycksTest-high_school_physics_5-shot_acc", 0.0206127843714708]
["huggyllama/llama-7b", "hendrycksTest-human_sexuality_5-shot_acc", 0.0207655040817406]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-professional_law_5-shot_acc", 0.0211472957743504]
["openlm-research/open_llama_7b", "hendrycksTest-virology_5-shot_acc", 0.0244432483092847]
["kevin009/flyingllama-v2", "hendrycksTest-formal_logic_5-shot_acc", 0.0251484669390179]
["openlm-research/open_llama_7b", "winogrande_5-shot_acc", 0.0251752414756057]
["openlm-research/open_llama_7b", "hendrycksTest-econometrics_5-shot_acc", 0.0256736560871726]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-prehistory_5-shot_acc", 0.0261769795123441]
["tiiuae/falcon-7b", "hendrycksTest-college_chemistry_5-shot_acc", 0.0269185090065002]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-electrical_engineering_5-shot_acc", 0.0269717485740266]
["huggyllama/llama-7b", "hendrycksTest-public_relations_5-shot_acc", 0.0273970549756831]
["huggyllama/llama-7b", "hendrycksTest-college_mathematics_5-shot_acc", 0.0274827015399932]
["openlm-research/open_llama_7b", "hellaswag_10-shot_acc", 0.0276646674617165]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-anatomy_5-shot_acc", 0.0277789294719695]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-college_chemistry_5-shot_acc", 0.0281152629852294]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_physics_5-shot_acc", 0.0284152717779804]
["allenai/tulu-2-dpo-70b", "hendrycksTest-econometrics_5-shot_acc", 0.0285994332087667]
["openlm-research/open_llama_7b", "hendrycksTest-college_computer_science_5-shot_acc", 0.0293342995643615]
["openlm-research/open_llama_7b", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0295826035832602]
["huggyllama/llama-7b", "hendrycksTest-high_school_us_history_5-shot_acc", 0.0300662155244865]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-formal_logic_5-shot_acc", 0.0300718385075766]
["tiiuae/falcon-7b", "hendrycksTest-moral_scenarios_5-shot_acc", 0.0303973587864604]
["kevin009/babyllama-v0.6", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0306582844257354]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_statistics_5-shot_acc", 0.0308133231268988]
["kevin009/babyllama-v0.6", "hendrycksTest-college_mathematics_5-shot_acc", 0.0309319734573364]
["huggyllama/llama-7b", "hendrycksTest-global_facts_5-shot_acc", 0.0311121368408203]
["openai-community/gpt2", "hendrycksTest-moral_scenarios_5-shot_acc", 0.0311127946363481]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-moral_disputes_5-shot_acc", 0.0311882549283132]
["huggyllama/llama-7b", "hendrycksTest-management_5-shot_acc", 0.0312920131729644]
["openlm-research/open_llama_7b", "hendrycksTest-moral_scenarios_5-shot_acc", 0.0316156211512049]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-professional_psychology_5-shot_acc", 0.0316719969892813]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.0322982523176405]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-college_medicine_5-shot_acc", 0.0330518249831447]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.0337876736236911]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.0342153067942019]
["tiiuae/falcon-7b", "hendrycksTest-high_school_us_history_5-shot_acc", 0.0344044525249332]
["tiiuae/falcon-7b", "hendrycksTest-professional_accounting_5-shot_acc", 0.0345084208545956]
["huggyllama/llama-7b", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.0348963682298307]
["huggyllama/llama-7b", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0349337661266326]
["openai-community/gpt2", "hendrycksTest-security_studies_5-shot_acc", 0.0351433515548705]
["huggyllama/llama-7b", "hendrycksTest-moral_scenarios_5-shot_acc", 0.035656218288997]
["huggyllama/llama-7b", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.0358836781978607]
["openai-community/gpt2", "hendrycksTest-professional_accounting_5-shot_acc", 0.0361129622510139]
["openai-community/gpt2", "hendrycksTest-college_physics_5-shot_acc", 0.0366472449957155]
["openlm-research/open_llama_7b", "hendrycksTest-electrical_engineering_5-shot_acc", 0.0367584366222908]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0376400594358091]
["openlm-research/open_llama_7b", "hendrycksTest-college_chemistry_5-shot_acc", 0.038220385313034]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-nutrition_5-shot_acc", 0.0385537182583528]
["tiiuae/falcon-7b", "hendrycksTest-global_facts_5-shot_acc", 0.0396025800704956]
["allenai/tulu-2-dpo-70b", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0400159584466742]
["huggyllama/llama-7b", "hendrycksTest-college_medicine_5-shot_acc", 0.0404382422135744]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-management_5-shot_acc", 0.0410007510370421]
["huggyllama/llama-7b", "hendrycksTest-prehistory_5-shot_acc", 0.041240068865411]
["openlm-research/open_llama_7b", "arc:challenge_25-shot_acc", 0.0414831557981797]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.0420792796672919]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-security_studies_5-shot_acc", 0.0425132257597787]
["tiiuae/falcon-7b", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.0430356755571545]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-computer_security_5-shot_acc", 0.0433463096618652]
["openlm-research/open_llama_7b", "hendrycksTest-formal_logic_5-shot_acc", 0.0436645008268811]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_physics_5-shot_acc", 0.0440168639287254]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.0445344722270965]
["openai-community/gpt2", "hendrycksTest-machine_learning_5-shot_acc", 0.0447525637490409]
["openai-community/gpt2", "hendrycksTest-professional_law_5-shot_acc", 0.0451338826599767]
["openlm-research/open_llama_7b", "hendrycksTest-public_relations_5-shot_acc", 0.0454926973039454]
["huggyllama/llama-7b", "hendrycksTest-virology_5-shot_acc", 0.0460860564048031]
["huggyllama/llama-7b", "hendrycksTest-college_computer_science_5-shot_acc", 0.0464245998859405]
["huggyllama/llama-7b", "hendrycksTest-high_school_biology_5-shot_acc", 0.0466510167044978]
["huggyllama/llama-7b", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.0470292959457788]
["tiiuae/falcon-7b", "hendrycksTest-human_sexuality_5-shot_acc", 0.0474173519902557]
["openai-community/gpt2", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0476370239257812]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.048088425177115]
["openlm-research/open_llama_7b", "hendrycksTest-college_medicine_5-shot_acc", 0.0480899809068338]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-international_law_5-shot_acc", 0.0483416831197818]
["Qwen/Qwen-7B", "winogrande_5-shot_acc", 0.0489074773645438]
["tiiuae/falcon-7b", "hendrycksTest-college_medicine_5-shot_acc", 0.0495941657895987]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-moral_disputes_5-shot_acc", 0.0500569271214435]
["openlm-research/open_llama_7b", "hendrycksTest-college_biology_5-shot_acc", 0.0504480964607662]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.0506086775234767]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-virology_5-shot_acc", 0.0510922824761953]
["huggyllama/llama-7b", "hendrycksTest-machine_learning_5-shot_acc", 0.051555859191077]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.0517921288808187]
["tiiuae/falcon-7b", "hendrycksTest-philosophy_5-shot_acc", 0.0523485864282037]
["tiiuae/falcon-7b", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0524038672447204]
["kevin009/flyingllama-v2", "hendrycksTest-professional_law_5-shot_acc", 0.0528650697238131]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_biology_5-shot_acc", 0.0531026296077236]
["huggyllama/llama-7b", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.0540576315865728]
["huggyllama/llama-7b", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.0545344722270965]
["openai-community/gpt2", "hendrycksTest-conceptual_physics_5-shot_acc", 0.0545449170660465]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-human_sexuality_5-shot_acc", 0.0546967796696961]
["openlm-research/open_llama_7b", "hendrycksTest-college_mathematics_5-shot_acc", 0.055066258907318]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-global_facts_5-shot_acc", 0.0559959816932678]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.0568669726108683]
["openlm-research/open_llama_7b", "hendrycksTest-professional_law_5-shot_acc", 0.05711915039954]
["tiiuae/falcon-7b", "hendrycksTest-security_studies_5-shot_acc", 0.0579611328183388]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-miscellaneous_5-shot_acc", 0.0587013565565282]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-astronomy_5-shot_acc", 0.0591160827561428]
["openlm-research/open_llama_7b", "hendrycksTest-conceptual_physics_5-shot_acc", 0.0598059999181869]
["openai-community/gpt2", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.0601426786234399]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_biology_5-shot_acc", 0.0601931716165234]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_physics_5-shot_acc", 0.0603084189212875]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-professional_accounting_5-shot_acc", 0.0610774736455146]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-jurisprudence_5-shot_acc", 0.0618710319201151]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-world_religions_5-shot_acc", 0.0625466361031895]
["openlm-research/open_llama_7b", "hendrycksTest-college_physics_5-shot_acc", 0.0628173985317642]
["openlm-research/open_llama_7b", "hendrycksTest-professional_accounting_5-shot_acc", 0.0645983212805809]
["openai-community/gpt2", "hendrycksTest-international_law_5-shot_acc", 0.0653214198498687]
["jb723/cross_lingual_epoch2", "hendrycksTest-global_facts_5-shot_acc", 0.0658604121208191]
["kevin009/flyingllama-v2", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0659861019048742]
["kevin009/flyingllama-v2", "hendrycksTest-virology_5-shot_acc", 0.0662658803434257]
["kevin009/babyllama-v0.6", "hendrycksTest-virology_5-shot_acc", 0.0662658803434257]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-nutrition_5-shot_acc", 0.0666618160173004]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-philosophy_5-shot_acc", 0.0672090319000256]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-human_aging_5-shot_acc", 0.0682834577934624]
["Qwen/Qwen-7B", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0687768185138702]
["jb723/cross_lingual_epoch2", "hendrycksTest-formal_logic_5-shot_acc", 0.0696786164291322]
["openai-community/gpt2", "hendrycksTest-econometrics_5-shot_acc", 0.0697929953273974]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-business_ethics_5-shot_acc", 0.069869726896286]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-college_biology_5-shot_acc", 0.0699566437138451]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_world_history_5-shot_acc", 0.0700222439906768]
["huggyllama/llama-7b", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0706417702493213]
["huggyllama/llama-7b", "hendrycksTest-professional_accounting_5-shot_acc", 0.0706853445962811]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-econometrics_5-shot_acc", 0.0711027702741455]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.071883523699098]
["openai-community/gpt2", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0721863654870836]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_geography_5-shot_acc", 0.0725560338810236]
["tiiuae/falcon-7b", "hendrycksTest-jurisprudence_5-shot_acc", 0.0733488411815078]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-electrical_engineering_5-shot_acc", 0.0735089433604273]
["huggyllama/llama-7b", "hendrycksTest-high_school_geography_5-shot_acc", 0.0740518073240916]
["tiiuae/falcon-7b", "hendrycksTest-logical_fallacies_5-shot_acc", 0.0744324247895574]
["kevin009/flyingllama-v2", "hendrycksTest-global_facts_5-shot_acc", 0.0746877574920654]
["huggyllama/llama-7b", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.0751352959084061]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_physics_5-shot_acc", 0.0757965378413926]
["openai-community/gpt2", "hendrycksTest-human_aging_5-shot_acc", 0.0766269022039233]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-virology_5-shot_acc", 0.0774827333817999]
["openai-community/gpt2", "hendrycksTest-anatomy_5-shot_acc", 0.0777740838351073]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.0778734004497528]
["openlm-research/open_llama_7b", "hendrycksTest-machine_learning_5-shot_acc", 0.077901005744934]
["kevin009/flyingllama-v2", "hendrycksTest-moral_scenarios_5-shot_acc", 0.0798936548845728]
["tiiuae/falcon-7b", "hendrycksTest-econometrics_5-shot_acc", 0.0803250466522417]
["Qwen/Qwen-7B", "hendrycksTest-college_mathematics_5-shot_acc", 0.0803804814815521]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-college_physics_5-shot_acc", 0.0805531635003931]
["openai-community/gpt2", "hendrycksTest-college_chemistry_5-shot_acc", 0.080607521533966]
["kevin009/flyingllama-v2", "hellaswag_10-shot_acc", 0.081873824992424]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-astronomy_5-shot_acc", 0.0822261132692036]
["tiiuae/falcon-7b", "hendrycksTest-moral_disputes_5-shot_acc", 0.082278915740162]
["huggyllama/llama-7b", "hendrycksTest-econometrics_5-shot_acc", 0.0825515483555041]
["jb723/cross_lingual_epoch2", "hendrycksTest-moral_scenarios_5-shot_acc", 0.0830408064679727]
["tiiuae/falcon-7b", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.0835971093177795]
["tiiuae/falcon-7b", "hendrycksTest-nutrition_5-shot_acc", 0.0839624237390905]
["allenai/tulu-2-dpo-70b", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0842652440071106]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-college_computer_science_5-shot_acc", 0.0843344044685363]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-college_mathematics_5-shot_acc", 0.0866456115245819]
["tiiuae/falcon-7b", "hendrycksTest-professional_psychology_5-shot_acc", 0.0868916649834003]
["openlm-research/open_llama_7b", "hendrycksTest-prehistory_5-shot_acc", 0.0874393810460597]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.088275225162506]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_us_history_5-shot_acc", 0.0895918712896459]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_physics_5-shot_acc", 0.0900081627021562]
["jb723/cross_lingual_epoch2", "hendrycksTest-college_mathematics_5-shot_acc", 0.0903804814815521]
["tiiuae/falcon-7b", "hendrycksTest-marketing_5-shot_acc", 0.0907663815041893]
["kevin009/flyingllama-v2", "gsm8k_5-shot_acc", 0.090917021036148]
["openlm-research/open_llama_7b", "hendrycksTest-anatomy_5-shot_acc", 0.091351310412089]
["openai-community/gpt2", "gsm8k_5-shot_acc", 0.0915786797425076]
["kevin009/flyingllama-v2", "hendrycksTest-electrical_engineering_5-shot_acc", 0.0919308504153942]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-college_chemistry_5-shot_acc", 0.09255464553833]
["openai-community/gpt2", "hendrycksTest-high_school_biology_5-shot_acc", 0.0929578394659104]
["openai-community/gpt2", "hendrycksTest-philosophy_5-shot_acc", 0.0929585709640834]
["kevin009/babyllama-v0.6", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0938710217122679]
["kevin009/babyllama-v0.6", "hendrycksTest-machine_learning_5-shot_acc", 0.0939004506383623]
["openai-community/gpt2", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.094134125732026]
["Qwen/Qwen-7B", "hendrycksTest-machine_learning_5-shot_acc", 0.0948559939861297]
["jb723/cross_lingual_epoch2", "hendrycksTest-virology_5-shot_acc", 0.0948932027242269]
["kevin009/babyllama-v0.6", "hendrycksTest-business_ethics_5-shot_acc", 0.0964717364311218]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.0982242053797265]
["tiiuae/falcon-7b", "hendrycksTest-world_religions_5-shot_acc", 0.0983080815153513]
["jb723/cross_lingual_epoch2", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.0984874115419135]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_biology_5-shot_acc", 0.0985934165216261]
["tiiuae/falcon-7b", "hendrycksTest-high_school_world_history_5-shot_acc", 0.0986451042603843]
["playdev7/theseed-v0.3", "hendrycksTest-abstract_algebra_5-shot_acc", 0.0987768185138702]
["openai-community/gpt2", "hendrycksTest-professional_psychology_5-shot_acc", 0.0992168926335628]
["tiiuae/falcon-7b", "hendrycksTest-professional_law_5-shot_acc", 0.0996485324928627]
["kevin009/babyllama-v0.6", "hendrycksTest-moral_scenarios_5-shot_acc", 0.1000053867281482]
["kevin009/flyingllama-v2", "hendrycksTest-anatomy_5-shot_acc", 0.1005993326505025]
["tiiuae/falcon-7b", "hendrycksTest-business_ethics_5-shot_acc", 0.100750275850296]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-formal_logic_5-shot_acc", 0.1013786395390829]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-logical_fallacies_5-shot_acc", 0.1015859045134001]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_geography_5-shot_acc", 0.1022617302157662]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.1033635258674621]
["jb723/cross_lingual_epoch2", "hendrycksTest-business_ethics_5-shot_acc", 0.1035875439643859]
["playdev7/theseed-v0.3", "hendrycksTest-college_physics_5-shot_acc", 0.1039526298934338]
["openai-community/gpt2", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.1044380772113799]
["kevin009/flyingllama-v2", "hendrycksTest-college_biology_5-shot_acc", 0.1048957440588209]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-marketing_5-shot_acc", 0.1054087082544962]
["kevin009/babyllama-v0.6", "hendrycksTest-electrical_engineering_5-shot_acc", 0.1057239538636701]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-prehistory_5-shot_acc", 0.1060548836802258]
["huggyllama/llama-7b", "hendrycksTest-high_school_statistics_5-shot_acc", 0.1068825092580583]
["openlm-research/open_llama_7b", "gsm8k_5-shot_acc", 0.1071726165661042]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.1084006879509998]
["openai-community/gpt2", "winogrande_5-shot_acc", 0.1086427407087824]
["kevin009/flyingllama-v2", "hendrycksTest-abstract_algebra_5-shot_acc", 0.1087265288829803]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.1092675074091497]
["jb723/cross_lingual_epoch2", "hendrycksTest-professional_law_5-shot_acc", 0.109598183414305]
["kevin009/flyingllama-v2", "winogrande_5-shot_acc", 0.1103872120051093]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_us_history_5-shot_acc", 0.1105269176118514]
["openai-community/gpt2", "hendrycksTest-college_computer_science_5-shot_acc", 0.1107198905944824]
["openai-community/gpt2", "hendrycksTest-electrical_engineering_5-shot_acc", 0.11082933380686]
["tiiuae/falcon-7b", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.1110099689299319]
["openlm-research/open_llama_7b", "hendrycksTest-miscellaneous_5-shot_acc", 0.1117799959572491]
["openlm-research/open_llama_7b", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.1118242597579955]
["tiiuae/falcon-7b", "hendrycksTest-miscellaneous_5-shot_acc", 0.1120795496807244]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.1122745052531913]
["openai-community/gpt2", "hellaswag_10-shot_acc", 0.113463763090463]
["jb723/cross_lingual_epoch2", "hendrycksTest-marketing_5-shot_acc", 0.1135770011151957]
["openlm-research/open_llama_7b", "hendrycksTest-astronomy_5-shot_acc", 0.1139099252851386]
["huggyllama/llama-7b", "hendrycksTest-electrical_engineering_5-shot_acc", 0.1148882537052549]
["tiiuae/falcon-7b", "hendrycksTest-medical_genetics_5-shot_acc", 0.1149360454082489]
["kevin009/flyingllama-v2", "hendrycksTest-college_chemistry_5-shot_acc", 0.1149700760841369]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_geography_5-shot_acc", 0.1150110616828455]
["kevin009/babyllama-v0.6", "hendrycksTest-formal_logic_5-shot_acc", 0.116361010169226]
["kevin009/flyingllama-v2", "hendrycksTest-business_ethics_5-shot_acc", 0.1164717364311218]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_psychology_5-shot_acc", 0.1167910675390051]
["jb723/cross_lingual_epoch2", "gsm8k_5-shot_acc", 0.1169260069727897]
["openlm-research/open_llama_7b", "hendrycksTest-professional_medicine_5-shot_acc", 0.1174706115442164]
["tiiuae/falcon-7b", "hendrycksTest-high_school_biology_5-shot_acc", 0.1179379209395378]
["kevin009/babyllama-v0.6", "hendrycksTest-college_chemistry_5-shot_acc", 0.1180682778358459]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_mathematics_5-shot_acc", 0.1182169477144877]
["openai-community/gpt2", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.1186765949127122]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_statistics_5-shot_acc", 0.1187762860898619]
["openlm-research/open_llama_7b", "hendrycksTest-jurisprudence_5-shot_acc", 0.1192962814260412]
["huggyllama/llama-7b", "hendrycksTest-college_chemistry_5-shot_acc", 0.1194399118423462]
["openai-community/gpt2", "hendrycksTest-formal_logic_5-shot_acc", 0.1195886049951826]
["openai-community/gpt2", "hendrycksTest-college_medicine_5-shot_acc", 0.1196576451635085]
["tiiuae/falcon-7b", "hendrycksTest-computer_security_5-shot_acc", 0.1205178213119506]
["openai-community/gpt2", "hendrycksTest-college_biology_5-shot_acc", 0.1208900080786811]
["openlm-research/open_llama_7b", "hendrycksTest-professional_psychology_5-shot_acc", 0.1215213227505777]
["tiiuae/falcon-7b", "hendrycksTest-college_physics_5-shot_acc", 0.1225917210765913]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-security_studies_5-shot_acc", 0.1228392962290316]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.1242080790874285]
["openlm-research/open_llama_7b", "hendrycksTest-computer_security_5-shot_acc", 0.1245841336250305]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-college_medicine_5-shot_acc", 0.1249807723340271]
["kevin009/babyllama-v0.6", "hendrycksTest-college_computer_science_5-shot_acc", 0.1250172960758209]
["openlm-research/open_llama_7b", "hendrycksTest-business_ethics_5-shot_acc", 0.1255961191654205]
["kevin009/babyllama-v0.6", "hendrycksTest-prehistory_5-shot_acc", 0.1256807652520544]
["openlm-research/open_llama_7b", "hendrycksTest-moral_disputes_5-shot_acc", 0.1262161285546474]
["tiiuae/falcon-7b", "hendrycksTest-management_5-shot_acc", 0.1264571608270256]
["openlm-research/open_llama_7b", "hendrycksTest-nutrition_5-shot_acc", 0.1265097661735186]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-human_sexuality_5-shot_acc", 0.1276357330893742]
["kevin009/flyingllama-v2", "arc:challenge_25-shot_acc", 0.1278529899519051]
["huggyllama/llama-7b", "gsm8k_5-shot_acc", 0.1279359632048126]
["tiiuae/falcon-7b", "hendrycksTest-formal_logic_5-shot_acc", 0.1281984916755132]
["kevin009/babyllama-v0.6", "hendrycksTest-professional_law_5-shot_acc", 0.1287168788940997]
["openai-community/gpt2", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.1288402812603192]
["openai-community/gpt2", "hendrycksTest-high_school_geography_5-shot_acc", 0.1290209895432598]
["jb723/cross_lingual_epoch2", "hendrycksTest-anatomy_5-shot_acc", 0.1290767342955978]
["tiiuae/falcon-7b", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.1295655062470412]
["openlm-research/open_llama_7b", "hendrycksTest-world_religions_5-shot_acc", 0.129770751236475]
["jb723/cross_lingual_epoch2", "hendrycksTest-electrical_engineering_5-shot_acc", 0.1298793241895479]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-sociology_5-shot_acc", 0.1317175032191015]
["kevin009/flyingllama-v2", "hendrycksTest-public_relations_5-shot_acc", 0.1322853223844008]
["jisukim8873/falcon-7B-case-0", "gsm8k_5-shot_acc", 0.1324723325415576]
["playdev7/theseed-v0.3", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.1325199548844938]
["jb723/cross_lingual_epoch2", "hendrycksTest-machine_learning_5-shot_acc", 0.1338595449924469]
["openai-community/gpt2", "arc:challenge_25-shot_acc", 0.1366747560761487]
["tiiuae/falcon-7b", "hendrycksTest-high_school_psychology_5-shot_acc", 0.1371918935294545]
["jb723/cross_lingual_epoch2", "hendrycksTest-human_aging_5-shot_acc", 0.1372584836899967]
["jb723/cross_lingual_epoch2", "hendrycksTest-public_relations_5-shot_acc", 0.1379063476215709]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_physics_5-shot_acc", 0.1382416344636323]
["jb723/cross_lingual_epoch2", "hendrycksTest-philosophy_5-shot_acc", 0.1421995306704971]
["jb723/cross_lingual_epoch2", "winogrande_5-shot_acc", 0.1425778400079416]
["tiiuae/falcon-7b", "hendrycksTest-elementary_mathematics_5-shot_acc", 0.142703895846372]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-high_school_statistics_5-shot_acc", 0.143077982796563]
["kevin009/flyingllama-v2", "hendrycksTest-world_religions_5-shot_acc", 0.1434267184190583]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-public_relations_5-shot_acc", 0.1465348509224978]
["kevin009/flyingllama-v2", "hendrycksTest-astronomy_5-shot_acc", 0.1468597870124014]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_psychology_5-shot_acc", 0.1469255205688126]
["kevin009/flyingllama-v2", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.1469930922985077]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_world_history_5-shot_acc", 0.1473738244314233]
["openlm-research/open_llama_7b", "hendrycksTest-medical_genetics_5-shot_acc", 0.1503872382640838]
["tiiuae/falcon-7b", "hendrycksTest-high_school_european_history_5-shot_acc", 0.1509094906575752]
["kevin009/babyllama-v0.6", "hendrycksTest-public_relations_5-shot_acc", 0.1511068587953394]
["kevin009/flyingllama-v2", "hendrycksTest-jurisprudence_5-shot_acc", 0.1521284171828517]
["openlm-research/open_llama_7b", "hendrycksTest-human_sexuality_5-shot_acc", 0.1523108905508318]
["jb723/cross_lingual_epoch2", "hendrycksTest-college_physics_5-shot_acc", 0.1524829741786508]
["playdev7/theseed-v0.3", "hendrycksTest-econometrics_5-shot_acc", 0.1539135669407091]
["openai-community/gpt2", "hendrycksTest-astronomy_5-shot_acc", 0.1539659782459861]
["openai-community/gpt2", "hendrycksTest-moral_disputes_5-shot_acc", 0.154107490715953]
["openai-community/gpt2", "hendrycksTest-high_school_psychology_5-shot_acc", 0.1542410440401199]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "gsm8k_5-shot_acc", 0.1550161135951347]
["openai-community/gpt2", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.1562113070487976]
["jb723/cross_lingual_epoch2", "hendrycksTest-college_computer_science_5-shot_acc", 0.1581870245933532]
["kevin009/babyllama-v0.6", "hendrycksTest-college_physics_5-shot_acc", 0.1583894076300602]
["openlm-research/open_llama_7b", "hendrycksTest-philosophy_5-shot_acc", 0.1603913687624731]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.1611700629323258]
["tiiuae/falcon-7b", "hendrycksTest-high_school_geography_5-shot_acc", 0.1614973752787619]
["jb723/cross_lingual_epoch2", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.1620138216018677]
["playdev7/theseed-v0.3", "hendrycksTest-formal_logic_5-shot_acc", 0.1621146060171582]
["kevin009/flyingllama-v2", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.1625547445045327]
["jb723/cross_lingual_epoch2", "hendrycksTest-professional_accounting_5-shot_acc", 0.163067248273403]
["kevin009/flyingllama-v2", "hendrycksTest-miscellaneous_5-shot_acc", 0.1644522670280583]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.1653109183583235]
["openlm-research/open_llama_7b", "hendrycksTest-human_aging_5-shot_acc", 0.1655577267499248]
["tiiuae/falcon-7b", "hendrycksTest-international_law_5-shot_acc", 0.1670577732984684]
["Monero/WizardLM-13b-OpenAssistant-Uncensored", "hendrycksTest-college_physics_5-shot_acc", 0.1678267010286743]
["kevin009/flyingllama-v2", "hendrycksTest-prehistory_5-shot_acc", 0.1688906417952643]
["jb723/cross_lingual_epoch2", "hendrycksTest-computer_security_5-shot_acc", 0.1700793695449829]
["openai-community/gpt2", "hendrycksTest-global_facts_5-shot_acc", 0.170183128118515]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_world_history_5-shot_acc", 0.1704709260775571]
["openai-community/gpt2", "hendrycksTest-logical_fallacies_5-shot_acc", 0.1714765228376799]
["openai-community/gpt2", "hendrycksTest-human_sexuality_5-shot_acc", 0.1730632907106676]
["openlm-research/open_llama_7b", "hendrycksTest-high_school_european_history_5-shot_acc", 0.1730754581364718]
["openlm-research/open_llama_7b", "hendrycksTest-security_studies_5-shot_acc", 0.1733364785204129]
["kevin009/babyllama-v0.6", "hendrycksTest-human_aging_5-shot_acc", 0.1734058648481497]
["kevin009/flyingllama-v2", "hendrycksTest-computer_security_5-shot_acc", 0.1739328789710998]
["kevin009/flyingllama-v2", "hendrycksTest-medical_genetics_5-shot_acc", 0.1749882984161377]
["openlm-research/open_llama_7b", "hendrycksTest-marketing_5-shot_acc", 0.1761826353195386]
["playdev7/theseed-v0.3", "hendrycksTest-global_facts_5-shot_acc", 0.1767676973342895]
["jb723/cross_lingual_epoch2", "hendrycksTest-international_law_5-shot_acc", 0.1779291812053396]
["openlm-research/open_llama_7b", "hendrycksTest-management_5-shot_acc", 0.1780651570523826]
["kevin009/babyllama-v0.6", "hendrycksTest-professional_accounting_5-shot_acc", 0.1797446498634122]
["openlm-research/open_llama_7b", "hendrycksTest-logical_fallacies_5-shot_acc", 0.1811183595949886]
["openai-community/gpt2", "hendrycksTest-prehistory_5-shot_acc", 0.1815125791378963]
["playdev7/theseed-v0.3", "hendrycksTest-college_computer_science_5-shot_acc", 0.1823035407066345]
["meta-llama/Llama-2-70b-chat-hf", "gsm8k_5-shot_acc", 0.1831363634551628]
["openai-community/gpt2", "hendrycksTest-virology_5-shot_acc", 0.1836679804037852]
["jb723/cross_lingual_epoch2", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.1839496707016567]
["playdev7/theseed-v0.3", "hendrycksTest-college_mathematics_5-shot_acc", 0.1855522072315216]
["kevin009/babyllama-v0.6", "hendrycksTest-jurisprudence_5-shot_acc", 0.1856533829812651]
["jb723/cross_lingual_epoch2", "hendrycksTest-econometrics_5-shot_acc", 0.1859302055417445]
["kevin009/babyllama-v0.6", "hendrycksTest-econometrics_5-shot_acc", 0.1874249969658099]
["kevin009/flyingllama-v2", "hendrycksTest-philosophy_5-shot_acc", 0.1881200694194561]
["kevin009/flyingllama-v2", "hendrycksTest-professional_accounting_5-shot_acc", 0.1903829477357526]
["tiiuae/falcon-7b", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.1918275211101924]
["tiiuae/falcon-7b", "hendrycksTest-anatomy_5-shot_acc", 0.1937397003173828]
["playdev7/theseed-v0.3", "winogrande_5-shot_acc", 0.1940434699747255]
["jb723/cross_lingual_epoch2", "hendrycksTest-prehistory_5-shot_acc", 0.1953210212566234]
["openai-community/gpt2", "hendrycksTest-nutrition_5-shot_acc", 0.1963927913335414]
["tiiuae/falcon-7b", "hendrycksTest-conceptual_physics_5-shot_acc", 0.1968036669365903]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.1973676304218217]
["openlm-research/open_llama_7b", "hendrycksTest-international_law_5-shot_acc", 0.1990402449261059]
["kevin009/flyingllama-v2", "hendrycksTest-machine_learning_5-shot_acc", 0.2010433077812194]
["jb723/cross_lingual_epoch2", "arc:challenge_25-shot_acc", 0.2013785328474467]
["jb723/cross_lingual_epoch2", "hendrycksTest-conceptual_physics_5-shot_acc", 0.201620207441614]
["kevin009/flyingllama-v2", "hendrycksTest-marketing_5-shot_acc", 0.2018717692957984]
["tiiuae/falcon-7b", "hendrycksTest-professional_medicine_5-shot_acc", 0.2031115573995254]
["kevin009/babyllama-v0.6", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.2040751767608354]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-professional_medicine_5-shot_acc", 0.2048550774069393]
["kevin009/flyingllama-v2", "hendrycksTest-conceptual_physics_5-shot_acc", 0.209065598756709]
["jb723/cross_lingual_epoch2", "hendrycksTest-college_medicine_5-shot_acc", 0.2114471862081847]
["playdev7/theseed-v0.3", "hendrycksTest-professional_law_5-shot_acc", 0.2119658032172351]
["kevin009/flyingllama-v2", "hendrycksTest-security_studies_5-shot_acc", 0.2123020522448482]
["kevin009/flyingllama-v2", "hendrycksTest-international_law_5-shot_acc", 0.2129161101727447]
["kevin009/flyingllama-v2", "hendrycksTest-econometrics_5-shot_acc", 0.2137407864394941]
["kevin009/babyllama-v0.6", "hendrycksTest-philosophy_5-shot_acc", 0.2140408653729982]
["tiiuae/falcon-7b", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.2142089846098951]
["kevin009/babyllama-v0.6", "hendrycksTest-medical_genetics_5-shot_acc", 0.214579712152481]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_biology_5-shot_acc", 0.2152711364530747]
["playdev7/theseed-v0.3", "hendrycksTest-virology_5-shot_acc", 0.2153751304350704]
["jb723/cross_lingual_epoch2", "hendrycksTest-moral_disputes_5-shot_acc", 0.2158837376991448]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_us_history_5-shot_acc", 0.2174853694205191]
["openai-community/gpt2", "hendrycksTest-marketing_5-shot_acc", 0.2177336407013428]
["jb723/cross_lingual_epoch2", "hendrycksTest-security_studies_5-shot_acc", 0.2190509022498617]
["kevin009/babyllama-v0.6", "hendrycksTest-computer_security_5-shot_acc", 0.2193608903884887]
["playdev7/theseed-v0.3", "hendrycksTest-moral_scenarios_5-shot_acc", 0.2200423787759003]
["jb723/cross_lingual_epoch2", "hendrycksTest-nutrition_5-shot_acc", 0.2203813834128037]
["tiiuae/falcon-7b", "hendrycksTest-college_computer_science_5-shot_acc", 0.2214226233959197]
["playdev7/theseed-v0.3", "gsm8k_5-shot_acc", 0.2216829508543014]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.2219981722937429]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.2231953524741806]
["openai-community/gpt2", "hendrycksTest-business_ethics_5-shot_acc", 0.2244353461265563]
["kevin009/flyingllama-v2", "hendrycksTest-sociology_5-shot_acc", 0.2251658300262186]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_us_history_5-shot_acc", 0.2260362511756374]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_chemistry_5-shot_acc", 0.2264115317114468]
["playdev7/theseed-v0.3", "hendrycksTest-electrical_engineering_5-shot_acc", 0.2271454136947105]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_world_history_5-shot_acc", 0.2283150580110429]
["jb723/cross_lingual_epoch2", "hendrycksTest-sociology_5-shot_acc", 0.2299872070402648]
["playdev7/theseed-v0.3", "hendrycksTest-professional_accounting_5-shot_acc", 0.2340478903435647]
["jb723/cross_lingual_epoch2", "hendrycksTest-miscellaneous_5-shot_acc", 0.2341197769456134]
["playdev7/theseed-v0.3", "hendrycksTest-machine_learning_5-shot_acc", 0.2347369236605508]
["kevin009/babyllama-v0.6", "hendrycksTest-conceptual_physics_5-shot_acc", 0.2378909610687418]
["jb723/cross_lingual_epoch2", "hendrycksTest-medical_genetics_5-shot_acc", 0.2381346082687377]
["jb723/cross_lingual_epoch2", "hendrycksTest-jurisprudence_5-shot_acc", 0.2381852004263136]
["jb723/cross_lingual_epoch2", "hendrycksTest-professional_psychology_5-shot_acc", 0.238634415311751]
["openai-community/gpt2", "hendrycksTest-medical_genetics_5-shot_acc", 0.238888840675354]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.2409685838222503]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_geography_5-shot_acc", 0.2412736879454718]
["kevin009/flyingllama-v2", "hendrycksTest-professional_psychology_5-shot_acc", 0.2442907702689078]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_world_history_5-shot_acc", 0.2476088490164229]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.2544300127931002]
["jb723/cross_lingual_epoch2", "hendrycksTest-college_chemistry_5-shot_acc", 0.2556562197208404]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.2572975081483318]
["tiiuae/falcon-7b", "hendrycksTest-high_school_statistics_5-shot_acc", 0.2573181523217095]
["kevin009/flyingllama-v2", "hendrycksTest-moral_disputes_5-shot_acc", 0.2583166011151551]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_statistics_5-shot_acc", 0.2602020744924193]
["kevin009/babyllama-v0.6", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.2607867205142975]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.2608372807502747]
["jb723/cross_lingual_epoch2", "hendrycksTest-human_sexuality_5-shot_acc", 0.2615887754746066]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_biology_5-shot_acc", 0.2620057790510116]
["jb723/cross_lingual_epoch2", "hellaswag_10-shot_acc", 0.2653126293291397]
["kevin009/babyllama-v0.6", "hendrycksTest-international_law_5-shot_acc", 0.2681650514445029]
["kevin009/babyllama-v0.6", "hendrycksTest-marketing_5-shot_acc", 0.2720535229413938]
["openai-community/gpt2", "hendrycksTest-high_school_european_history_5-shot_acc", 0.273243517225439]
["openai-community/gpt2", "hendrycksTest-sociology_5-shot_acc", 0.2740243154378673]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.2747640935559348]
["playdev7/theseed-v0.3", "hendrycksTest-college_chemistry_5-shot_acc", 0.2756562197208404]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_psychology_5-shot_acc", 0.27603084242672]
["kevin009/babyllama-v0.6", "hendrycksTest-professional_psychology_5-shot_acc", 0.2762498547828276]
["jb723/cross_lingual_epoch2", "hendrycksTest-astronomy_5-shot_acc", 0.2764442343460886]
["kevin009/babyllama-v0.6", "gsm8k_5-shot_acc", 0.2771378092416947]
["openlm-research/open_llama_7b", "hendrycksTest-sociology_5-shot_acc", 0.2796165456819297]
["jb723/cross_lingual_epoch2", "hendrycksTest-world_religions_5-shot_acc", 0.2802984672680236]
["openai-community/gpt2", "hendrycksTest-public_relations_5-shot_acc", 0.2823444908315486]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_european_history_5-shot_acc", 0.2861257383317658]
["kevin009/babyllama-v0.6", "hendrycksTest-management_5-shot_acc", 0.2902216917102777]
["playdev7/theseed-v0.3", "hendrycksTest-anatomy_5-shot_acc", 0.2920396972585607]
["kevin009/babyllama-v0.6", "hendrycksTest-miscellaneous_5-shot_acc", 0.2935387564497128]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.2948998668614556]
["playdev7/theseed-v0.3", "hendrycksTest-conceptual_physics_5-shot_acc", 0.2975432190489261]
["openai-community/gpt2", "hendrycksTest-jurisprudence_5-shot_acc", 0.2981832248193247]
["kevin009/flyingllama-v2", "hendrycksTest-nutrition_5-shot_acc", 0.2993392480744256]
["openai-community/gpt2", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.3013720391945518]
["jisukim8873/falcon-7B-case-0", "hendrycksTest-high_school_statistics_5-shot_acc", 0.3040386648089798]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_european_history_5-shot_acc", 0.3047856739073089]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.3060739462192242]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_biology_5-shot_acc", 0.3067356805647573]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_european_history_5-shot_acc", 0.307913741198453]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_geography_5-shot_acc", 0.3086180650826656]
["jb723/cross_lingual_epoch2", "hendrycksTest-logical_fallacies_5-shot_acc", 0.3137303042265535]
["kevin009/flyingllama-v2", "hendrycksTest-college_medicine_5-shot_acc", 0.3166421524362068]
["kevin009/babyllama-v0.6", "hendrycksTest-security_studies_5-shot_acc", 0.3166745500905173]
["jb723/cross_lingual_epoch2", "hendrycksTest-management_5-shot_acc", 0.3169729408708591]
["playdev7/theseed-v0.3", "hendrycksTest-public_relations_5-shot_acc", 0.3197245294397527]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_statistics_5-shot_acc", 0.319777837506047]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.3237036319879385]
["tiiuae/falcon-7b", "hendrycksTest-astronomy_5-shot_acc", 0.3261258257062812]
["jb723/cross_lingual_epoch2", "hendrycksTest-high_school_us_history_5-shot_acc", 0.3261747582286012]
["playdev7/theseed-v0.3", "arc:challenge_25-shot_acc", 0.326432815794245]
["playdev7/theseed-v0.3", "hendrycksTest-astronomy_5-shot_acc", 0.3301138721014324]
["kevin009/babyllama-v0.6", "hendrycksTest-high_school_psychology_5-shot_acc", 0.3302890886954212]
["openai-community/gpt2", "hendrycksTest-management_5-shot_acc", 0.3316417145497591]
["playdev7/theseed-v0.3", "hendrycksTest-business_ethics_5-shot_acc", 0.3364022874832153]
["openai-community/gpt2", "hendrycksTest-high_school_world_history_5-shot_acc", 0.3388806835508549]
["jb723/cross_lingual_epoch2", "hendrycksTest-college_biology_5-shot_acc", 0.3410399556159973]
["tiiuae/falcon-7b", "gsm8k_5-shot_acc", 0.3411062931228172]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_psychology_5-shot_acc", 0.342167378893686]
["tiiuae/falcon-7b", "hendrycksTest-college_biology_5-shot_acc", 0.3428049617343479]
["playdev7/theseed-v0.3", "hendrycksTest-human_aging_5-shot_acc", 0.3480208155285617]
["kevin009/flyingllama-v2", "hendrycksTest-human_aging_5-shot_acc", 0.3481796001639602]
["playdev7/theseed-v0.3", "hellaswag_10-shot_acc", 0.3484633961086656]
["playdev7/theseed-v0.3", "hendrycksTest-jurisprudence_5-shot_acc", 0.3492963115374247]
["playdev7/theseed-v0.3", "hendrycksTest-college_medicine_5-shot_acc", 0.3559558567284159]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_microeconomics_5-shot_acc", 0.3571775991375707]
["kevin009/flyingllama-v2", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.3576888608808963]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_european_history_5-shot_acc", 0.3593311284527634]
["playdev7/theseed-v0.3", "hendrycksTest-moral_disputes_5-shot_acc", 0.3603924082193761]
["jb723/cross_lingual_epoch2", "hendrycksTest-professional_medicine_5-shot_acc", 0.364224440911237]
["kevin009/babyllama-v0.6", "hendrycksTest-anatomy_5-shot_acc", 0.3644491924179925]
["playdev7/theseed-v0.3", "hendrycksTest-professional_psychology_5-shot_acc", 0.3644514087758033]
["playdev7/theseed-v0.3", "hendrycksTest-security_studies_5-shot_acc", 0.3741529430661883]
["kevin009/babyllama-v0.6", "hendrycksTest-sociology_5-shot_acc", 0.3784358735701338]
["kevin009/babyllama-v0.6", "hendrycksTest-world_religions_5-shot_acc", 0.3788530251436066]
["playdev7/theseed-v0.3", "hendrycksTest-professional_medicine_5-shot_acc", 0.3789303232641781]
["playdev7/theseed-v0.3", "hendrycksTest-nutrition_5-shot_acc", 0.3805121023670521]
["kevin009/babyllama-v0.6", "hendrycksTest-college_biology_5-shot_acc", 0.3820451166894701]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_macroeconomics_5-shot_acc", 0.3855611257064036]
["kevin009/babyllama-v0.6", "hendrycksTest-astronomy_5-shot_acc", 0.3858441741842973]
["kevin009/babyllama-v0.6", "hendrycksTest-college_medicine_5-shot_acc", 0.3879883692443716]
["openai-community/gpt2", "hendrycksTest-high_school_us_history_5-shot_acc", 0.3881885409355163]
["kevin009/flyingllama-v2", "hendrycksTest-human_sexuality_5-shot_acc", 0.3887102044265689]
["playdev7/theseed-v0.3", "hendrycksTest-philosophy_5-shot_acc", 0.3930033891913975]
["kevin009/babyllama-v0.6", "hendrycksTest-professional_medicine_5-shot_acc", 0.3957164427813362]
["kevin009/flyingllama-v2", "hendrycksTest-logical_fallacies_5-shot_acc", 0.3979537881956511]
["playdev7/theseed-v0.3", "hendrycksTest-human_sexuality_5-shot_acc", 0.3989933556272784]
["playdev7/theseed-v0.3", "hendrycksTest-prehistory_5-shot_acc", 0.3990247249603271]
["playdev7/theseed-v0.3", "hendrycksTest-computer_security_5-shot_acc", 0.4000793695449829]
["kevin009/babyllama-v0.6", "hendrycksTest-moral_disputes_5-shot_acc", 0.4006100917138116]
["openai-community/gpt2", "hendrycksTest-computer_security_5-shot_acc", 0.407884624004364]
["kevin009/babyllama-v0.6", "hendrycksTest-nutrition_5-shot_acc", 0.4168648575645646]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_computer_science_5-shot_acc", 0.4208372807502746]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_us_history_5-shot_acc", 0.4242139739148757]
["playdev7/theseed-v0.3", "hendrycksTest-miscellaneous_5-shot_acc", 0.4244135189634934]
["openai-community/gpt2", "hendrycksTest-world_religions_5-shot_acc", 0.4370199159572]
["playdev7/theseed-v0.3", "hendrycksTest-international_law_5-shot_acc", 0.442391991122695]
["playdev7/theseed-v0.3", "hendrycksTest-medical_genetics_5-shot_acc", 0.4477115845680237]
["playdev7/theseed-v0.3", "hendrycksTest-clinical_knowledge_5-shot_acc", 0.4481006140978832]
["kevin009/babyllama-v0.6", "hendrycksTest-human_sexuality_5-shot_acc", 0.4486602613034139]
["playdev7/theseed-v0.3", "hendrycksTest-logical_fallacies_5-shot_acc", 0.4486996293799278]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_biology_5-shot_acc", 0.4523283596961729]
["kevin009/babyllama-v0.6", "hendrycksTest-logical_fallacies_5-shot_acc", 0.4587002158896324]
["playdev7/theseed-v0.3", "hendrycksTest-sociology_5-shot_acc", 0.4588429284332997]
["playdev7/theseed-v0.3", "hendrycksTest-college_biology_5-shot_acc", 0.4590955111715529]
["playdev7/theseed-v0.3", "hendrycksTest-world_religions_5-shot_acc", 0.4674329701920002]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_geography_5-shot_acc", 0.4702342266988273]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_world_history_5-shot_acc", 0.4796763595649462]
["openai-community/gpt2", "hendrycksTest-miscellaneous_5-shot_acc", 0.4894590961674316]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_government_and_politics_5-shot_acc", 0.4904581299099898]
["playdev7/theseed-v0.3", "hendrycksTest-high_school_psychology_5-shot_acc", 0.4962143286652521]
["playdev7/theseed-v0.3", "hendrycksTest-marketing_5-shot_acc", 0.4981923857305804]
["kevin009/flyingllama-v2", "hendrycksTest-management_5-shot_acc", 0.5036813223246233]
["playdev7/theseed-v0.3", "hendrycksTest-management_5-shot_acc", 0.5111476981524126]
["playdev7/theseed-v0.3", "hendrycksTest-us_foreign_policy_5-shot_acc", 0.5520138216018677]
