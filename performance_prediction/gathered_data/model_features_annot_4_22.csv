id,base model,merged,dimension,num_heads,num_layers,mlp_ratio,intermediate_size,layer_norm_type,positional_embeddings,attention_variant,biases,block_type,activation,sequence_length,batch_instances,batch_tokens,batch_tokens,weight_tying,total_params,vocab_size,deep_key,total_params,vocab_size,,is_instruction_tuned,RLHF?,instruction tuning data,finetuning data
AbacusResearch/Jallabi-34B,yi-34b,no,7168,56,60,2.857142857,20480,rmsnorm,rope,gqa,none,sequential,swiglu,4096,-1,#N/A,-1,FALSE,34388917248,64000,Lara,based on Llama,,,unknown,,,
allenai/OLMo-7B,olmo-7b,no,4096,32,32,5.375,22016,non-parametric,rope,full,none,sequential,swiglu,2048,2160,#N/A,4000000,FALSE,,,DONE,,,,no,,,
EleutherAI/pythia-6.9b,pythia-6.9b,no,4096,32,32,2.666667,10922.66803,-1,rotary,-1,-1,-1,gelu,2048,1024,#N/A,2000000,FALSE,"6,857,302,106",,Lara,,,,no,,,
huggyllama/llama-7b,llama1-7b,no,4096,32,32,2.666667,10922.66803,rmsnorm,rope,full,none,sequential,swiglu,2048,1024,#N/A,4000000,FALSE,6738417664,,DONE,,,,,,,
jb723/cross_lingual_epoch2,llama2-7b,no,4096,32,32,2.6875,11008,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,32,4000000,FALSE,-1,32000,Lara,says in Korean on the model page that it was initialized from llama2-7b-hf,,,yes,no,,
jb723/LLaMA2-en-ko-7B-model,llama2-7b,no,4096,32,32,2.6875,11008,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,32,4000000,FALSE,-1,32000,Emmy,,,,yes,no,,
jisukim8873/falcon-7B-case-0,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,DONE,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-1,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,Lindia,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-2,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,DONE,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-3,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,DONE,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-4,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,DONE,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-5,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,Emmy,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-6,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,DONE,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-8,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,DONE,,,,yes,no,sample from Open-Orca,
jisukim8873/falcon-7B-case-c,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,DONE,,,,yes,no,sample from Open-Orca,
kevin009/babyllama-v0.6,tinyllama-1.1b,no,2048,32,22,2.75,5632,rmsnorm,rope,gqa,none,sequential,swiglu,2048,1024,#N/A,2000000,FALSE,1261529088,32000,Shreya,based on tinyllama -> which adopts llama2 architecture,,,,,,
kevin009/flyingllama-v2,flyingllama-v2,no,1024,16,4,rmsnorm,rope,gqa,-1,sequential,silu,1024,1024,4000000,FALSE,-1,6921720704,,,,Shreya,based on Llama,,,,,,
meta-llama/Llama-2-70b-chat-hf,llama2-70b,no,8192,64,80,3.5,28672,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,80,4000000,FALSE,68976653312,32000,DONE,,,,,,,
meta-llama/Llama-2-70b-hf,llama2-70b,no,8192,64,80,3.5,28672,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,80,4000000,FALSE,68976653312,32000,DONE,,,,no,no,n/a,n/a
meta-llama/Llama-2-7b,llama2-7b,no,4096,32,32,2.6875,11008,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,32,4000000,FALSE,-1,32000,,,,,no,no,n/a,n/a
mistralai/Mistral-7B-v0.1,mistral-7b-1,no,4096,32,32,3.5,14336,rmsnorm,rope,gqa,none,sequential,silu,4096,-1,#N/A,-1,FALSE,7241732096,,DONE,,,,no,no,n/a,n/a
mistralai/Mixtral-8x7B-v0.1,mixtral-8x7B-v0.1 ,no,4096,32,32,,14336,rmsnorm,rope,gqa,,,swiglu,32768,-1,#N/A,,FALSE,,32000,DONE,,,,no,no,n/a,n/a
Monero/WizardLM-13b-OpenAssistant-Uncensored,llama2-13b,no,5120,40,40,2.666667,13653.33504,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,40,4000000,FALSE,,32000,DONE,based in kkana13b,,,,,,
openai-community/gpt2,gpt2,no,768,12,12,-1,,-1,-1,-1,-1,-1,gelu,1024,-1,#N/A,-1,-1,137022720,,Shreya,,,,no,no,n/a,n/a
openlm-research/open_llama_7b,openlm-7b,no,4096,32,32,2.666667,10922.66803,parametric,rope,full,ln_only,sequential,swiglu,2048,2048,#N/A,4000000,FALSE,,,DONE,,,,,,,
playdev7/theseed-v0.3,mixtral-8x7B-v0.1 ,no,4096,32,32,,14336,rmsnorm,rope,gqa,,,swiglu,32768,-1,#N/A,,FALSE,,32000,Emmy,,,,,,,
Qwen/Qwen-7B,qwen-7b,no,4096,32,32,5.375,22016,-1,rotary,-1,none,-1,-1,8192,-1,32,-1,FALSE,-1,151851,Shreya,,,,no,no,n/a,n/a
tiiuae/falcon-7b,falcon-7b,no,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,32,4000000,FALSE,6921720704,65024,,,,,no,no,n/a,n/a
cerebras/Cerebras-GPT-13B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cerebras/Cerebras-GPT-6.7B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cerebras/Cerebras-GPT-2.7B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cerebras/Cerebras-GPT-1.3B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T5,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeBERTa,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mT5,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/gpt-neo-125M,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/gpt-neo-1.3B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/gpt-neo-2.7B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/gpt-j-6b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IEITYuan/Yuan2-2B-hf,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IEITYuan/Yuan2-51B-hf,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/xglm-564M,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/xglm-2.9B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/xglm-4.5B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/xglm-7.5B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/gpt-neox-20b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NinedayWang/PolyCoder-2.7B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-350M-multi,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-350M-mono,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-2B-multi,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-2B-mono,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-6B-multi,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-6B-mono,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-16B-multi,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen-16B-mono,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/opt-125m,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/opt-350m,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/opt-2.7b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
facebook/opt-6.7b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-1b7,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen25-7b-multi_P,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen25-7b-mono_P,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Salesforce/codegen25-7b-instruct_P,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mosaicml/mpt-7b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mosaicml/mpt-7b-instruct,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mosaicml/mpt-7b-storywriter,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dampish/StellarX-4B-V0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mosaicml/mpt-30b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mosaicml/mpt-30b-instruct,,,,,,,,,,,,,,,,,,,,,,,,,,,,
openlm-research/open_llama_3b_v2,,,,,,,,,,,,,,,,,,,,,,,,,,,,
openlm-research/open_llama_7b_v2,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cerebras/btlm-3b-8k-base,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rinna/bilingual-gpt-neox-4b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stabilityai/japanese-stablelm-base-alpha-7b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deci/DeciCoder-1b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deci/DeciCoder-6B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
aisingapore/sea-lion-7b,,,,,,,,,,,,,,,,,,,,,,,,,,,,
aisingapore/sea-lion-7b-instruct,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LLM360/CrystalCoder,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LLM360/Amber,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pile-t5-large,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3-8B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3-8B-Instruct,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3-70B,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3-8B-Instruct,,,,,,,,,,,,,,,,,,,,,,,,,,,,