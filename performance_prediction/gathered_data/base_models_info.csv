model name,dimension,num_heads,num_layers,mlp_ratio,intermediate_size,layer_norm_type,positional_embeddings,attention_variant,biases,block_type,activation,sequence_length,batch_instances,batch_tokens,weight_tying,deep_key,total_params,vocab_size,,link to pretraining data,pretraining data,total tokens trained on (in billions),percentage web data,percentage book data,percentage reference data,percentage academic paper data,percentage code data,percentage english data
meta-llama/Llama-2-7b-chat-hf,4096,32,32,2.6875,11008,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,4000000,FALSE,-1,-1,32000,,not public,Llama2 mix,"2,000",not known,not known,not known,not known,8.38,89.7
meta-llama/Llama-2-13b-chat-hf,5120,40,40,2.666667,13653.33504,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,4000000,FALSE,-1,,32000,,not public,Llama2 mix,"2,000",not known,not known,not known,not known,8.38,89.7
meta-llama/Llama-2-70b-chat-hf,8192,64,80,3.5,28672,rmsnorm,rope,gqa,none,sequential,swiglu,4096,1024,4000000,FALSE,-1,68976653312,32000,,not public,Llama2 mix,"2,000",not known,not known,not known,not known,8.38,89.7
falcon-7b,4544,71,32,4,18176,parametric,rope,mqa,ln_only,parallel,gelu,2048,2304,4000000,FALSE,-1,6921720704,65024,,https://huggingface.co/datasets/tiiuae/falcon-refinedweb,Falcon-refinedweb,1500,82,7,not known,2,3,97
Qwen/Qwen-7B,4096,32,32,5.375,22016,-1,rotary,-1,none,-1,-1,8192,-1,-1,FALSE,-1,-1,151851,,,unknown?,3000,unknown?,unknown?,unknown?,unknown?,unknown?,unknown?
pythia-6.9b,4096,32,32,2.666667,10922.66803,-1,rotary,-1,-1,-1,gelu,2048,1024,2000000,FALSE,-1,"6,857,302,106",,,,already documented,,,,,,,
mistral-7b-1,4096,32,32,3.5,14336,rmsnorm,rope,gqa,none,sequential,silu,4096,-1,-1,FALSE,-1,7241732096,,,not public,unknown,,,,,,,
llama1-7b,4096,32,32,2.666667,10922.66803,rmsnorm,rope,full,none,sequential,swiglu,2048,1024,4000000,FALSE,-1,6738417664,,,,Llama1 mix,1000,82,4.5,4.5,2.5,6.5,high
llama1-13b,5120,40,40,,0,rmsnorm,rope,full,none,sequential,swiglu,2048,-1,4000000,FALSE,,,,,,Llama1 mix,1000,82,4.5,4.5,2.5,6.5,high
olmo-7b,4096,32,32,5.375,22016,non-parametric,rope,full,none,sequential,swiglu,2048,2160,4000000,FALSE,,,,,https://huggingface.co/datasets/allenai/dolma,Dolma,2000,83.95,0.2,0.14,2.29,13.43,100
openlm-7b,4096,32,32,2.666667,10922.66803,parametric,rope,full,ln_only,sequential,swiglu,2048,2048,4000000,FALSE,,,,,,RedPyjama,1000,,,,,,
kevin009/flyingllama-v2,1024,16,24,,,,,,,,,,,,,,,,,,unknown,,,,,,,
gpt2,768,12,12,-1,,-1,-1,-1,-1,-1,gelu,1024,-1,-1,-1,-1,137022720,,,not public; OpenWebText is a replication: https://huggingface.co/datasets/Skylion007/openwebtext,WebText,,,,,,,
orca-mini3b,3200,32,26,-1,,rmsnorm,rope,full,none,sequential,silu,2048,-1,-1,FALSE,-1,-1,,,,unknown,,,,,,,
yi-34b,7168,56,60,2.857142857,20480,rmsnorm,rope,gqa,none,sequential,swiglu,4096,-1,-1,FALSE,-1,34388917248,64000,,,already documented,,,,,,,
mixtral-8x7B-v0.1 ,4096,32,32,,14336,rmsnorm,rope,gqa,,,swiglu,32768,-1,,FALSE,,,32000,,not public,unknown,,,,,,,
tinyllama-1.1b,2048,32,22,2.75,5632,rmsnorm,rope,gqa,none,sequential,swiglu,2048,1024,2000000,FALSE,-1,1261529088,32000,,https://github.com/jzhang38/TinyLlama?tab=readme-ov-file#training-details,mix of SlimPajama (minus Github split) and Starcoderdata (70/30),3000,58.26,3.1,2.81,3.4,33.48,unknown