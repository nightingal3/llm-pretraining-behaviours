activation,attention_variant,batch_instances,batch_tokens,biases,block_type,deep_key,dimension,id,layer_norm_type,mlp_ratio,num_heads,positional_embeddings,sequence_length,weight_tying
swiglu,gqa,-1,-1,none,sequential,-1,4096,mistralai/Mixtral-8x7B-v0.1,rmsnorm,3.5,32,rope,4096,False
swiglu,full,1024,4000000,none,sequential,-1,4096,huggyllama/llama-7b,rmsnorm,2.666667,32,rope,2048,False
swiglu,gqa,-1,-1,none,sequential,-1,4096,mistralai/Mistral-7B-v0.1,rmsnorm,3.5,32,rope,4096,False
swiglu,full,2048,4000000,ln_only,sequential,-1,4096,openlm-research/open_llama_7b,parametric,2.666667,32,rope,2048,False
-1,-1,-1,-1,-1,-1,-1,-1,openai-community/gpt2,-1,-1.0,-1,-1,-1,-1
swiglu,gqa,1024,4000000,none,sequential,-1,4096,meta-llama/Llama-2-7b,rmsnorm,2.666667,32,rope,4096,False
gelu,mqa,2304,4000000,ln_only,parallel,-1,4544,tiiuae/falcon-7b,parametric,4.0,71,rope,2048,False
swiglu,full,2160,4000000,none,sequential,-1,4096,allenai/OLMo-7B,non-paramteric,2.666667,32,rope,2048,False
swiglu,gqa,1024,4000000,none,sequential,-1,8192,meta-llama/Llama-2-70b-hf,rmsnorm,2.666667,64,rope,4096,False
swiglu,gqa,1024,4000000,none,sequential,-1,8192,meta-llama/Llama-2-70b-chat-hf,rmsnorm,2.666667,64,rope,4096,False
swiglu,full,1024,2000000,none,sequential,-1,4096,EleutherAI/pythia-6.9b,parametric,2.666667,32,rope,2048,False
swiglu,full,1024,4000000,attn_only,sequential,-1,4096,Qwen/Qwen-7B,rmsnorm,2.666667,32,rope,4096,False
