selected_features,improvement_scores,selection_history,final_mae,baseline_mae,task,setting
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_reference', 'pretraining_summary_percentage_code', 'biases', 'positional_embeddings', 'pretraining_summary_percentage_web']","{'pretraining_summary_percentage_reference': {'mae_improvement': 0.003996458289657438}, 'pretraining_summary_percentage_code': {'mae_improvement': 0.0029474276309585934}, 'biases': {'mae_improvement': 0.0010798973245646423}, 'positional_embeddings': {'mae_improvement': 0.00041712769577580133}, 'pretraining_summary_percentage_web': {'mae_improvement': 0.0005203789162806643}}","[{'feature': 'pretraining_summary_percentage_reference', 'mae_before': 0.0347556953495335, 'mae_after': 0.030759237059876064, 'improvement': 0.003996458289657438}, {'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.030759237059876064, 'mae_after': 0.02781180942891747, 'improvement': 0.0029474276309585934}, {'feature': 'biases', 'mae_before': 0.02781180942891747, 'mae_after': 0.02673191210435283, 'improvement': 0.0010798973245646423}, {'feature': 'positional_embeddings', 'mae_before': 0.02673191210435283, 'mae_after': 0.026314784408577027, 'improvement': 0.00041712769577580133}, {'feature': 'pretraining_summary_percentage_web', 'mae_before': 0.026314784408577027, 'mae_after': 0.025794405492296363, 'improvement': 0.0005203789162806643}]",0.0347556953495335,0.0347556953495335,arc_challenge,25-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_code', 'positional_embeddings', 'layer_norm_type', 'pretraining_summary_percentage_english', 'dimension']","{'pretraining_summary_percentage_code': {'mae_improvement': 0.004483296528298375}, 'positional_embeddings': {'mae_improvement': 0.0019194772086476714}, 'layer_norm_type': {'mae_improvement': 0.00031534728877860574}, 'pretraining_summary_percentage_english': {'mae_improvement': 0.00014729219151654593}, 'dimension': {'mae_improvement': 0.0004661373238623451}}","[{'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.03204926866410453, 'mae_after': 0.027565972135806152, 'improvement': 0.004483296528298375}, {'feature': 'positional_embeddings', 'mae_before': 0.027565972135806152, 'mae_after': 0.02564649492715848, 'improvement': 0.0019194772086476714}, {'feature': 'layer_norm_type', 'mae_before': 0.02564649492715848, 'mae_after': 0.025331147638379875, 'improvement': 0.00031534728877860574}, {'feature': 'pretraining_summary_percentage_english', 'mae_before': 0.025331147638379875, 'mae_after': 0.02518385544686333, 'improvement': 0.00014729219151654593}, {'feature': 'dimension', 'mae_before': 0.02518385544686333, 'mae_after': 0.024717718123000984, 'improvement': 0.0004661373238623451}]",0.03204926866410453,0.03204926866410453,hellaswag,10-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_code', 'question_words_ratio', 'pretraining_summary_percentage_books', 'layer_norm_type']","{'pretraining_summary_percentage_code': {'mae_improvement': 0.021631575200746794}, 'question_words_ratio': {'mae_improvement': 0.002509161504953561}, 'pretraining_summary_percentage_books': {'mae_improvement': 0.0010400522890163522}, 'layer_norm_type': {'mae_improvement': 0.0006128607562152361}}","[{'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.06585132468842289, 'mae_after': 0.0442197494876761, 'improvement': 0.021631575200746794}, {'feature': 'question_words_ratio', 'mae_before': 0.0442197494876761, 'mae_after': 0.041710587982722536, 'improvement': 0.002509161504953561}, {'feature': 'pretraining_summary_percentage_books', 'mae_before': 0.041710587982722536, 'mae_after': 0.040670535693706183, 'improvement': 0.0010400522890163522}, {'feature': 'layer_norm_type', 'mae_before': 0.040670535693706183, 'mae_after': 0.04005767493749095, 'improvement': 0.0006128607562152361}]",0.06585132468842289,0.06585132468842289,lambada,0-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pct_english_mean', 'pretraining_summary_percentage_reference', 'imperative_verbs_ratio', 'block_type']","{'pct_english_mean': {'mae_improvement': 0.004351034292610745}, 'pretraining_summary_percentage_reference': {'mae_improvement': 0.001513759947805851}, 'imperative_verbs_ratio': {'mae_improvement': 0.0006807068438940134}, 'block_type': {'mae_improvement': 0.0006823852193611896}}","[{'feature': 'pct_english_mean', 'mae_before': 0.0408353972146588, 'mae_after': 0.036484362922048055, 'improvement': 0.004351034292610745}, {'feature': 'pretraining_summary_percentage_reference', 'mae_before': 0.036484362922048055, 'mae_after': 0.034970602974242204, 'improvement': 0.001513759947805851}, {'feature': 'imperative_verbs_ratio', 'mae_before': 0.034970602974242204, 'mae_after': 0.03428989613034819, 'improvement': 0.0006807068438940134}, {'feature': 'block_type', 'mae_before': 0.03428989613034819, 'mae_after': 0.033607510910987, 'improvement': 0.0006823852193611896}]",0.0408353972146588,0.0408353972146588,mmlu_0-shot,0-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_academic', 'pretraining_summary_percentage_reference']","{'pretraining_summary_percentage_academic': {'mae_improvement': 0.0027672366290536765}, 'pretraining_summary_percentage_reference': {'mae_improvement': 0.0019544116474394385}}","[{'feature': 'pretraining_summary_percentage_academic', 'mae_before': 0.03512003467234659, 'mae_after': 0.03235279804329291, 'improvement': 0.0027672366290536765}, {'feature': 'pretraining_summary_percentage_reference', 'mae_before': 0.03235279804329291, 'mae_after': 0.030398386395853474, 'improvement': 0.0019544116474394385}]",0.03512003467234659,0.03512003467234659,mmlu_5-shot,5-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'domain_web_pct_mean', 'pretraining_summary_percentage_english', 'positional_embeddings']","{'domain_web_pct_mean': {'mae_improvement': 0.005200119424557337}, 'pretraining_summary_percentage_english': {'mae_improvement': 0.0004673094421193229}, 'positional_embeddings': {'mae_improvement': 0.00011956644230994712}}","[{'feature': 'domain_web_pct_mean', 'mae_before': 0.02270449805950773, 'mae_after': 0.017504378634950392, 'improvement': 0.005200119424557337}, {'feature': 'pretraining_summary_percentage_english', 'mae_before': 0.017504378634950392, 'mae_after': 0.01703706919283107, 'improvement': 0.0004673094421193229}, {'feature': 'positional_embeddings', 'mae_before': 0.01703706919283107, 'mae_after': 0.016917502750521122, 'improvement': 0.00011956644230994712}]",0.02270449805950773,0.02270449805950773,truthfulqa,0-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'domain_reference_pct_mean', 'biases', 'pretraining_summary_percentage_books', 'question_words_ratio', 'positional_embeddings', 'pretraining_summary_percentage_academic']","{'domain_reference_pct_mean': {'mae_improvement': 0.0008496233350914484}, 'biases': {'mae_improvement': 0.0011229283408024718}, 'pretraining_summary_percentage_books': {'mae_improvement': 0.0005842081225046619}, 'question_words_ratio': {'mae_improvement': 0.0001661381520678025}, 'positional_embeddings': {'mae_improvement': 0.0006463796626709761}, 'pretraining_summary_percentage_academic': {'mae_improvement': 0.0001149167189888542}}","[{'feature': 'domain_reference_pct_mean', 'mae_before': 0.029922162757979504, 'mae_after': 0.029072539422888056, 'improvement': 0.0008496233350914484}, {'feature': 'biases', 'mae_before': 0.029072539422888056, 'mae_after': 0.027949611082085584, 'improvement': 0.0011229283408024718}, {'feature': 'pretraining_summary_percentage_books', 'mae_before': 0.027949611082085584, 'mae_after': 0.027365402959580922, 'improvement': 0.0005842081225046619}, {'feature': 'question_words_ratio', 'mae_before': 0.027365402959580922, 'mae_after': 0.02719926480751312, 'improvement': 0.0001661381520678025}, {'feature': 'positional_embeddings', 'mae_before': 0.02719926480751312, 'mae_after': 0.026552885144842144, 'improvement': 0.0006463796626709761}, {'feature': 'pretraining_summary_percentage_academic', 'mae_before': 0.026552885144842144, 'mae_after': 0.02643796842585329, 'improvement': 0.0001149167189888542}]",0.029922162757979504,0.029922162757979504,winogrande,5-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_code', 'biases']","{'pretraining_summary_percentage_code': {'mae_improvement': 0.005277481604190094}, 'biases': {'mae_improvement': 0.00011733557496753505}}","[{'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.061168137053438376, 'mae_after': 0.05589065544924828, 'improvement': 0.005277481604190094}, {'feature': 'biases', 'mae_before': 0.05589065544924828, 'mae_after': 0.05577331987428075, 'improvement': 0.00011733557496753505}]",0.061168137053438376,0.061168137053438376,anli,0-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_code', 'biases', 'pretraining_summary_percentage_academic']","{'pretraining_summary_percentage_code': {'mae_improvement': 0.003495028228671454}, 'biases': {'mae_improvement': 0.00010502447999974351}, 'pretraining_summary_percentage_academic': {'mae_improvement': 0.0004087445202958784}}","[{'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.06305969346452642, 'mae_after': 0.05956466523585496, 'improvement': 0.003495028228671454}, {'feature': 'biases', 'mae_before': 0.05956466523585496, 'mae_after': 0.05945964075585522, 'improvement': 0.00010502447999974351}, {'feature': 'pretraining_summary_percentage_academic', 'mae_before': 0.05945964075585522, 'mae_after': 0.05905089623555934, 'improvement': 0.0004087445202958784}]",0.06305969346452642,0.06305969346452642,logiqa2,0-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_code', 'biases']","{'pretraining_summary_percentage_code': {'mae_improvement': 0.002473002538163835}, 'biases': {'mae_improvement': 0.0003583517996512131}}","[{'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.0437538473142518, 'mae_after': 0.04128084477608796, 'improvement': 0.002473002538163835}, {'feature': 'biases', 'mae_before': 0.04128084477608796, 'mae_after': 0.04092249297643675, 'improvement': 0.0003583517996512131}]",0.0437538473142518,0.0437538473142518,xnli,0-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_code', 'block_type']","{'pretraining_summary_percentage_code': {'mae_improvement': 0.0008019489507195832}, 'block_type': {'mae_improvement': 0.0005786663772147371}}","[{'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.026428931914729836, 'mae_after': 0.025626982964010253, 'improvement': 0.0008019489507195832}, {'feature': 'block_type', 'mae_before': 0.025626982964010253, 'mae_after': 0.025048316586795516, 'improvement': 0.0005786663772147371}]",0.026428931914729836,0.026428931914729836,mathqa,0-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'instructions_words_ratio', 'pretraining_summary_percentage_books', 'layer_norm_type', 'unique_tokens_mean', 'domain_web_pct_mean', 'imperative_verbs_ratio', 'attention_variant', 'char_len_std']","{'instructions_words_ratio': {'mae_improvement': 0.012702206517494603}, 'pretraining_summary_percentage_books': {'mae_improvement': 0.00042708013205260825}, 'layer_norm_type': {'mae_improvement': 0.000294483037160391}, 'unique_tokens_mean': {'mae_improvement': 0.00014644303554055965}, 'domain_web_pct_mean': {'mae_improvement': 0.0005160859919543792}, 'imperative_verbs_ratio': {'mae_improvement': 0.0004359589405901476}, 'attention_variant': {'mae_improvement': 0.0002344753746434322}, 'char_len_std': {'mae_improvement': 0.00022779881405601832}}","[{'feature': 'instructions_words_ratio', 'mae_before': 0.046090759862885904, 'mae_after': 0.0333885533453913, 'improvement': 0.012702206517494603}, {'feature': 'pretraining_summary_percentage_books', 'mae_before': 0.0333885533453913, 'mae_after': 0.03296147321333869, 'improvement': 0.00042708013205260825}, {'feature': 'layer_norm_type', 'mae_before': 0.03296147321333869, 'mae_after': 0.0326669901761783, 'improvement': 0.000294483037160391}, {'feature': 'unique_tokens_mean', 'mae_before': 0.0326669901761783, 'mae_after': 0.03252054714063774, 'improvement': 0.00014644303554055965}, {'feature': 'domain_web_pct_mean', 'mae_before': 0.03252054714063774, 'mae_after': 0.03200446114868336, 'improvement': 0.0005160859919543792}, {'feature': 'imperative_verbs_ratio', 'mae_before': 0.03200446114868336, 'mae_after': 0.031568502208093215, 'improvement': 0.0004359589405901476}, {'feature': 'attention_variant', 'mae_before': 0.031568502208093215, 'mae_after': 0.03133402683344978, 'improvement': 0.0002344753746434322}, {'feature': 'char_len_std', 'mae_before': 0.03133402683344978, 'mae_after': 0.031106228019393764, 'improvement': 0.00022779881405601832}]",0.046090759862885904,0.046090759862885904,gsm8k,5-shot
"['total_params', 'pretraining_summary_total_tokens_billions', 'pretraining_summary_percentage_english', 'sequence_length', 'pretraining_summary_percentage_code', 'layer_norm_type']","{'pretraining_summary_percentage_english': {'mae_improvement': 0.010910157187492234}, 'biases': {'mae_improvement': 0.0060219748330696266}, 'sequence_length': {'mae_improvement': 0.004060351939776628}, 'pretraining_summary_percentage_code': {'mae_improvement': 0.0015648376640200676}, 'layer_norm_type': {'mae_improvement': 0.0009752886254720816}}","[{'feature': 'pretraining_summary_percentage_english', 'mae_before': 0.05859273554459915, 'mae_after': 0.04768257835710692, 'improvement': 0.010910157187492234}, {'feature': 'biases', 'mae_before': 0.04768257835710692, 'mae_after': 0.04166060352403729, 'improvement': 0.0060219748330696266}, {'feature': 'sequence_length', 'mae_before': 0.04166060352403729, 'mae_after': 0.03760025158426066, 'improvement': 0.004060351939776628}, {'feature': 'pretraining_summary_percentage_code', 'mae_before': 0.03760025158426066, 'mae_after': 0.036035413920240594, 'improvement': 0.0015648376640200676}, {'feature': 'layer_norm_type', 'mae_before': 0.036035413920240594, 'mae_after': 0.03506012529476851, 'improvement': 0.0009752886254720816}]",0.05859273554459915,0.05859273554459915,humaneval,0-shot