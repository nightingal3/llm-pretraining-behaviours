training:
  num_layers: 14
  num_attention_heads: 12
  seq_length: 2048
  num_kv_heads: 12
  hidden_size: 1536
  ffn_hidden_size: 4128
  tune_steps: 1000
  lr: 0.00015
  min_lr: 1.0e-5
  weight_decay: 1e-2
  grad_clip: 1.0
  lr_warmup_steps: 100
  save_interval: 10000
  eval_interval: 10000
  train_epochs: 1
  tp: 1
  micro_batch_size: 1
  seed: 42
