global {
    # WARNING: change these variables to your own paths
    ducttape_output=/mnt/data/patrick/towerllm-outs/llama2_c4small
    repo=/home/patrick/repos/tower-llm-training

    # set this to "" to save only in ducttape_output 
    external_model_dir=(
        Size: 
            base="/mnt/data/shared/tower_llm_training/models/llama2_c4small"
            small1="/mnt/data/patrick/towerllm_models/llama2_c4small_test"
            small2=""
            small3=""
    )
    # set this to true if you want to restart from a previous checkpoint saved in external_model_dir
    # WARNING: setting this to false will delete the contents of external_model_dir
    external_resume=true

    # dataset pamaeters
    dataset_name=datablations/c4-filter-small
    tokenizer=NousResearch/Llama-2-7b-hf
    filter=false
    n_tokens=50000000000

    # eval 
    model_checkpoint=/mnt/data/shared/tower_llm_training/models/llama2_c4small

    eval_set=/mnt/data/shared/tower_llm_training/data/wikitext-103/wiki.test.tokens
    eval_metric=loss
    
    # model parameters
    model_config=(
        Size: 
            base=llama2_1b
            small1=llama2_50m
            small2=llama2_150m
            small3=llama2_500m
    )

    # training parameters
    train_steps=10000
    batch_size=32
    lr=3e-4
    min_lr=3e-5
    lr_warmup_steps=1000
    weight_decay=0.1
    grad_clip=1.0
    save_interval=500
    eval_interval=500

    # distributed training parameters
    gpus=2
    tp=1
    pp=1
    zero_stage=2
    master_addr=localhost
    master_port=6666
    rdzv_port=29666
    cpu_workers=8
}
