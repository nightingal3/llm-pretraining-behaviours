global {
    # WARNING: change these variables to your own paths
    ducttape_output=/projects/tir5/users/pfernand/towerllm-outs/llama2_c4small
    repo=/home/pfernand/repos/tower-llm-training
    # set this to "" to save only in ducttape_output 
    external_model_dir="/projects/tir4/users/pfernand/towerllm_models/llama2_c4small"
    # set this to true if you want to restart from a previous checkpoint saved in external_model_dir
    # WARNING: setting this to false will delete the contents of external_model_dir
    external_restart=false 

    # dataset pamaeters
    dataset_name=datablations/c4-filter-small
    tokenizer=NousResearch/Llama-2-7b-hf
    
    # model parameters
    hidden_size=2048
    ffn_hidden_size=5504
    num_layers=24
    num_attention_heads=16
    seq_length=2048
    num_kv_heads=4

    # training parameters
    train_steps=10000
    batch_size=32
    lr=3e-4
    min_lr=3e-5
    lr_warmup_steps=1000
    weight_decay=0.1
    grad_clip=1.0
    save_interval=500
    eval_interval=500

    # distributed training parameters
    gpus=4
    tp=2
    pp=1
    zero_stage=2
    master_addr=localhost
    master_port=6000
    cpu_workers=8
}
