import "tower_llm_uservars.tconf"

global {
    # WARNING: change these variables to your own paths

    ducttape_experimental_imports=true
    ducttape_experimental_submitters=true
    ducttape_experimental_multiproc=true

    # set this to true if you want to restart from a previous checkpoint saved in external_model_dir
    # WARNING: setting this to false will delete the contents of external_model_dir
    external_resume=true

    # dataset pamaeters
    # unused since we pass the dataset path directly to the pipeline
    dataset_name="towerllm-data"
    
    dataset_path=(
        Dataset:
            en=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/en/data
            en_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/en/lid/
            en_gutenberg=/gpfsstore/rech/hxz/ued79zb/towerllm-data/project_gutenberg/v1/en/lid/
            en_arxiv=/gpfsstore/rech/hxz/ued79zb/towerllm-data/arxiv/v1/en/lid/
            en_bookcorpus=/gpfsstore/rech/hxz/ued79zb/towerllm-data/bookcorpus/v1/en/lid/
            de=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/de/
            de_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/de/lid/
            es=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/es/
            es_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/es/lid/
            fr=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/fr/
            fr_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/fr/lid/
            it=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/it/
            it_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/it/lid/
            nl=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/nl/
            nl_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/nl/lid/
            pt=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/pt/
            pt_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/pt/lid/
            pl=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/pl/
            pl_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/pl/lid/
            ru=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/ru/
            ru_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/ru/lid/
            sv=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/sv/
            sv_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/sv/lid/
            ko=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/ko/
            ko_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/ko/lid/
            zh=/gpfsstore/rech/hxz/ued79zb/towerllm-data/v1/zh/
            zh_wiki=/gpfsstore/rech/hxz/ued79zb/towerllm-data/wikipedia/v1/zh/lid/
            code=/gpfsstore/rech/hxz/ued79zb/towerllm-data/code/data
    )

    dataset_dirs=""
    dataset_stream=false
    filter=true

    hf_dataset=(Dataset:
            en=true
            en_wiki=false
            en_gutenberg=false
            en_arxiv=false
            en_bookcorpus=false
            de=false
            de_wiki=false
            es=false
            es_wiki=false
            fr=false
            fr_wiki=false
            it=false
            it_wiki=false
            nl=false
            nl_wiki=false
            pt=false
            pt_wiki=false
            pl=false
            pl_wiki=false
            ru=false
            ru_wiki=false
            sv=false
            sv_wiki=false
            ko=false
            ko_wiki=false
            zh=false
            zh_wiki=false
            code=true
    )

    bilingual=(Dataset:
            en=false
            en_wiki=false
            en_gutenberg=false
            en_arxiv=false
            en_bookcorpus=false
            de=false
            de_wiki=false
            es=false
            es_wiki=false
            fr=false
            fr_wiki=false
            it=false
            it_wiki=false
            nl=false
            nl_wiki=false
            pt=false
            pt_wiki=false
            pl=false
            pl_wiki=false
            ru=false
            ru_wiki=false
            sv=false
            sv_wiki=false
            ko=false
            ko_wiki=false
            zh=false
            zh_wiki=false
            code=false
    )

    code=(Dataset:
            en=false
            en_wiki=false
            en_gutenberg=false
            en_arxiv=false
            en_bookcorpus=false
            de=false
            de_wiki=false
            es=false
            es_wiki=false
            fr=false
            fr_wiki=false
            it=false
            it_wiki=false
            nl=false
            nl_wiki=false
            pt=false
            pt_wiki=false
            pl=false
            pl_wiki=false
            ru=false
            ru_wiki=false
            sv=false
            sv_wiki=false
            ko=false
            ko_wiki=false
            zh=false
            zh_wiki=false
            code=true
    )

    percentile=(
        Dataset:
            en=50
            en_wiki=""
            en_gutenberg=""
            en_arxiv=""
            en_bookcorpus=""
            de=50
            de_wiki=""
            es=50
            es_wiki=""
            fr=50
            fr_wiki=""
            it=50
            it_wiki=""
            nl=50
            nl_wiki=""
            pt=50
            pt_wiki=""
            pl=50
            pl_wiki=""
            ru=50
            ru_wiki=""
            sv=50
            sv_wiki=""
            ko=50
            ko_wiki=""
            zh=50
            zh_wiki=""
            code=50
    )


    n_tokens=(
        Dataset:
            en=63000000000
            en_wiki=5000000000
            en_gutenberg=2000000000
            en_arxiv=14000000000
            en_bookcorpus=1000000000
            de=5000000000
            de_wiki=5000000000
            es=5000000000
            es_wiki=2000000000
            fr=5000000000
            fr_wiki=2000000000
            it=5000000000
            it_wiki=1000000000
            nl=5000000000
            nl_wiki=1000000000
            pt=5000000000
            pt_wiki=1000000000
            pl=5000000000
            pl_wiki=1000000000
            ru=5000000000
            ru_wiki=2000000000
            sv=5000000000
            sv_wiki=1000000000
            ko=5000000000
            ko_wiki=1000000000
            zh=20000000000
            zh_wiki=2000000000
            code=3000000000
    )

    n_tokens_test=(
        Dataset:
            en=500000
            en_wiki=500000
            en_gutenberg=500000
            en_arxiv=500000
            en_bookcorpus=500000
            de=500000
            de_wiki=500000
            es=500000
            es_wiki=500000
            fr=500000
            fr_wiki=500000
            it=500000
            it_wiki=500000
            nl=500000
            nl_wiki=500000
            pt=500000
            pt_wiki=500000
            pl=500000
            pl_wiki=500000
            ru=500000
            ru_wiki=500000
            sv=500000
            sv_wiki=500000
            ko=500000
            ko_wiki=500000
            zh=500000
            zh_wiki=500000
            code=500000
    )

    datamix_weights=(
        DataMix:
            enplus=(Dataset: 
                en=484 en_wiki=43 en_gutenberg=10 en_arxiv=100 en_bookcorpus=3 
                de=10 de_wiki=20 
                es=18 es_wiki=12 
                fr=14 fr_wiki=16 
                it=20 it_wiki=10 
                nl=24 nl_wiki=6 
                pt=24 pt_wiki=6 
                pl=24 pl_wiki=6 
                ru=16 ru_wiki=14 
                sv=25 sv_wiki=5 
                ko=25 ko_wiki=5 
                zh=23 zh_wiki=7
                code=30
                )
            enmid=(Dataset: 
                en=374 en_wiki=43 en_gutenberg=10 en_arxiv=100 en_bookcorpus=3 
                de=20 de_wiki=20
                es=28 es_wiki=12 
                fr=24 fr_wiki=16 
                it=30 it_wiki=10 
                nl=34 nl_wiki=6 
                pt=34 pt_wiki=6 
                pl=34 pl_wiki=6 
                ru=26 ru_wiki=14
                sv=35 sv_wiki=5 
                ko=35 ko_wiki=5 
                zh=33 zh_wiki=7 
                code=30
                )
            enless=(Dataset: 
                en=264 en_wiki=43 en_gutenberg=10 en_arxiv=100 en_bookcorpus=3 
                de=30 de_wiki=20
                es=38 es_wiki=12
                fr=34 fr_wiki=16
                it=40 it_wiki=10
                nl=44 nl_wiki=6
                pt=44 pt_wiki=6
                pl=44 pl_wiki=6
                ru=36 ru_wiki=14
                sv=45 sv_wiki=5
                ko=45 ko_wiki=5  
                zh=43 zh_wiki=7
                code=30
                )
            enmid_twowiki=(Dataset: 
                en=331 en_wiki=86 en_gutenberg=10 en_arxiv=100 en_bookcorpus=3 
                de=0 de_wiki=40
                es=16 es_wiki=24 
                fr=8 fr_wiki=32 
                it=20 it_wiki=20
                nl=28 nl_wiki=12
                pt=28 pt_wiki=12
                pl=28 pl_wiki=12
                ru=12 ru_wiki=28
                sv=30 sv_wiki=10
                ko=30 ko_wiki=10 
                zh=26 zh_wiki=14
                code=30
                )
    )

    external_tokenizer="/gpfswork/rech/hxz/ued79zb/towerllm-tokenizer"

    # training tokenizer parameters
    # WARNING: not passed to the rest of the pipeline
    # export and pass as external_tokenizer for now
    vocab_size=128000
    tokenizer_words_per_source=(
        Dataset:
        en=250000000
        en_wiki=250000000
        en_gutenberg=250000000
        en_arxiv=250000000
        en_bookcorpus=250000000
        de=250000000
        de_wiki=250000000
        es=250000000
        es_wiki=250000000
        fr=250000000
        fr_wiki=250000000
        it=250000000
        it_wiki=250000000
        nl=250000000
        nl_wiki=250000000
        pt=250000000
        pt_wiki=250000000
        pl=250000000
        pl_wiki=250000000
        ru=250000000
        ru_wiki=250000000
        sv=250000000
        sv_wiki=250000000
        ko=250000000
        ko_wiki=250000000
        zh=500000000
        zh_wiki=500000000
        code=250000000
    )
    extra_tokens=2000

    pre_tokenizer=(
        Dataset:
            en='whitespace'
            en_wiki='whitespace'
            en_gutenberg='whitespace'
            en_arxiv='whitespace'
            en_bookcorpus='whitespace'
            de='whitespace'
            de_wiki='whitespace'
            es='whitespace'
            es_wiki='whitespace'
            fr='whitespace'
            fr_wiki='whitespace'
            it='whitespace'
            it_wiki='whitespace'
            nl='whitespace'
            nl_wiki='whitespace'
            pt='whitespace'
            pt_wiki='whitespace'
            pl='whitespace'
            pl_wiki='whitespace'
            ru='whitespace'
            ru_wiki='whitespace'
            sv='whitespace'
            sv_wiki='whitespace'
            ko='whitespace'
            ko_wiki='whitespace'
            zh='characters'
            zh_wiki='characters'
            code='whitespace'
    )

    eval_metric=loss
    eval_iteration=(
        EvalIteration:
            latest=""
            100k=100000
            200k=200000
            300k=300000
            400k=400000
    )
    
    # model parameters
    model_config=(
        Size: 
            base=llama2_1b
            small4=llama2_440m
            small3=llama2_268m
            small2=llama2_166m
            small1=llama2_58m
    )

    # training parameters
    train_steps=(
        Size:
            base=500000
            small4=500000
            small3=500000
            small2=750000
            small1=750000
    )
    batch_size=(
        Size:
            base=128
            small4=96
            small3=96
            small2=64
            small1=64 
    )
    grad_accum_steps=1

    lr=3e-4
    min_lr=3e-5
    lr_warmup_steps=2000
    weight_decay=0.1
    grad_clip=1.0
    save_interval=5000
    eval_interval=500

    # distributed training parameters
    gpus=(
        Size:
            base=8
            small4=8
            small3=6
            small2=4
            small1=4
    )
    tp=1
    pp=1
    zero_stage=2
    master_addr=localhost
    master_port=(
        Size:
            base=62200
            small4=62202
            small3=62203
            small2=62204
            small1=62205
    )
    rdzv_port=(
        Size:
            base=29800
            small4=29802
            small3=29803
            small2=29804
            small1=29805
    )
    cpu_workers=16
    seed=911
}