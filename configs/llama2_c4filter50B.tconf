import "user_vars.tconf"

global {
    # WARNING: change these variables to your own paths
    # ducttape_output=/mnt/data/shared/tower_llm_training/experiments/1B_c4filter_50B_2
    # ducttape_output=/mnt/data/patrick/towerllm-outs/llama2_c4filter50B
    # repo=/mnt/data/pmartins/code/tower-llm-training
    
    # set this to "" to save only in ducttape_output
    external_model_dir=(
        Size: 
            base="/mnt/data_2/shared/tower_llm_training/models/llama2_c4filter50B_1B"
            small5="/mnt/data_2/shared/tower_llm_training/models/llama2_c4filter50B_554M"
            small4="/mnt/data_2/shared/tower_llm_training/models/llama2_c4filter50B_402M"
            small3="/mnt/data_2/shared/tower_llm_training/models/llama2_c4filter50B_255M"
            small2="/mnt/data_2/shared/tower_llm_training/models/llama2_c4filter50B_156M"
            small1="/mnt/data_2/shared/tower_llm_training/models/llama2_c4filter50B_56M"
    )
    # set this to true if you want to restart from a previous checkpoint saved in external_model_dir
    # WARNING: setting this to false will delete the contents of external_model_dir
    external_resume=true

    # dataset pamaeters
    dataset_name=datablations/c4-filter
    tokenizer=NousResearch/Llama-2-7b-hf
    filter=true
    n_tokens=50000000000

    eval_set=/mnt/data/shared/tower_llm_training/data/wikitext-103/wiki.test.tokens
    eval_metric=loss
    
    # model parameters
    model_config=(
        Size: 
            base=llama2_1b
            small5=llama2_554m
            small4=llama2_402m
            small3=llama2_255m
            small2=llama2_156m
            small1=llama2_56m
    )

    # training parameters
    train_steps=(
        Size:
            base=555000
            small5=400000
            small4=400000
            small3=300000
            small2=300000
            small1=300000
    )
    batch_size=(
        Size:
            base=44
            small5=30
            small4=30
            small3=30
            small2=30
            small1=30
    )

    lr=3e-4
    min_lr=3e-5
    lr_warmup_steps=1000
    weight_decay=0.1
    grad_clip=1.0
    save_interval=20000
    eval_interval=10000

    # distributed training parameters
    gpu_ids=(
        Size:
            base=0,1,2,3,4,5,6
            small5=0,1
            small4=2,3
            small3=4
            small2=5
            small1=6

    )
    gpus=(
        Size:
            base=4
            small5=2
            small4=2
            small3=1
            small2=1
            small1=1
    )
    tp=1
    pp=1
    zero_stage=2
    master_addr=localhost
    master_port=(
        Size:
            base=61200
            small5=61201
            small4=61202
            small3=61203
            small2=61204
            small1=61205
    )
    rdzv_port=(
        Size:
            base=29700
            small5=29701
            small4=29702
            small3=29703
            small2=29704
            small1=29705
    )
    cpu_workers=(
        Size: 
            base=16
            small5=4
            small4=4
            small3=4
            small2=4
            small1=4
    )
}
