import "croissant_llm_uservars.tconf"

global {
    ducttape_experimental_imports=true
    ducttape_experimental_submitters=true
    ducttape_experimental_multiproc=true

    # set this to true if you want to restart from a previous checkpoint saved in external_model_dir
    # WARNING: setting this to false will delete the contents of external_model_dir
    external_resume=true

    # dataset pamaeters
    dataset_name=(
        Dataset:
            french=manu/french-30b
            english=manu/english-60b
            code=manu/code-20b
    )
    dataset_path=(
        Dataset:
            french=/gpfsstore/rech/hxz/ued79zb/croissllm-data/manu/french-30b
            english=/gpfsstore/rech/hxz/ued79zb/croissllm-data/manu/english-60b
            code=/gpfsstore/rech/hxz/ued79zb/croissllm-data/manu/code-20b
    )
    hf_dataset=true
    dataset_dirs=""
    dataset_stream=false
    filter=false
    percentile=50
    n_tokens=""
    n_tokens_test=""
    bilingual=false
    code=false

    datamix_weights=(
        DataMix:
            equal=(Dataset: french=40 english=40 code=20)
            frplus=(Dataset: french=60 english=20 code=20)
            enplus=(Dataset: french=20 english=60 code=20)
    )

    # tokenization arguments
    external_tokenizer=manu/tok-fr-en-code

    # training tokenizer parameters
    # WARNING: not passed to the rest of the pipeline
    # (since this tape uses manu's tokenizer)
    # export and pass as external_tokenizer for now
    vocab_size=32000
    tokenizer_words_per_source=(
        Dataset:
            french=3000000000
            english=2000000000
            code=1000000000
    )
    extra_tokens=1000
    pre_tokenizer=whitespace

    eval_metric=loss
    eval_iteration=(
        EvalIteration:
            latest=""
            100k=100000
            200k=200000
            340k=340000
            360k=360000
            420k=420000
    )
    # model parameters
    model_config=(
        Size: 
            base=llama2_1b3
            small4=llama2_440m
            small3=llama2_268m
            small2=llama2_166m
            small1=llama2_58m
    )

    # training parameters
    train_steps=(
        Size:
            base=1000000
            small4=500000
            small3=500000
            small2=500000
            small1=500000
    )
    batch_size=(
        Size:
            base=64
            small4=128
            small3=126
            small2=128
            small1=128
    )
    grad_accum_steps=1

    lr=3e-4
    min_lr=1e-5
    lr_warmup_steps=2000
    weight_decay=0.1
    grad_clip=1.0
    save_interval=5000
    eval_interval=500

    # distributed training parameters
    gpus=(
        Size:
            base=8
            small4=8
            small3=6
            small2=4
            small1=4
    )
    tp=1
    pp=1
    zero_stage=2
    master_addr=localhost
    master_port=(
        Size:
            base=61200
            small4=61202
            small3=61203
            small2=61204
            small1=61205
    )
    rdzv_port=(
        Size:
            base=29700
            small4=29702
            small3=29703
            small2=29704
            small1=29705
    )
    cpu_workers=16
    seed=911
}